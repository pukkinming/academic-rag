{
  "paper_id": "2207.12248v3",
  "title": "Date Of Publication Xxxx 00, 0000, Date Of Current Version Xxxx 00, 0000",
  "published": "2022-07-07T02:53:39Z",
  "authors": [
    "Thejan Rajapakshe",
    "Rajib Rana",
    "Sara Khalifa",
    "Bjorn W. Schuller"
  ],
  "keywords": [
    "INDEX TERMS Reinforcement Learning",
    "Speech Emotion Recognition",
    "Domain Adaptation Domain Adapting Deep Reinforcement Learning for Real-world Speech Emotion Recognition Step 2 Step 1 Reinforcement Learning for Domain Adaptation Source Dataset Data Feed Feedback Base Model"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech-emotion recognition (SER) enables computers to engage with people in an emotionally intelligent way. The inability to adapt an existing model to a new domain is one of the significant limitations of SER methods. To overcome this challenge, domain adaptation techniques have been developed to transfer the knowledge learnt by a model across domains. Although existing domain adaptation techniques have improved the performance of SER models across domains, there is a need to improve their ability to adapt to real-world situations where models can self-tune while deployed. This paper presents a deep reinforcement learning-based strategy (RL-DA) for adapting a pre-trained SER model to a real-world setting by interacting with the environment and collecting continuous feedback. The proposed RL-DA technique is evaluated on SER tasks, including cross-corpus and cross-language domain adaptation scenarios. Our evaluation results show that RL-DA achieves significant improvements of 11% and 14% in testing accuracy over a fully supervised baseline for cross-corpus and cross-language scenarios, respectively, in the real-world setting. This technique also outperforms the baseline model's performance for both speaker independent and speaker dependent SER tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E motion-aware interaction has been identified as a key factor in improving human-computer interaction. To this end, much research has been devoted to automatic speechemotion recognition (SER). Although SER within the same corpus has shown reasonable accuracy, cross-corpus SER performance remains a challenge  [1] ,  [2] ,  [3] . The ability to perform cross-corpus SER is critical to achieving emotionaware interactions in real-world applications. This is because speech signals obtained from different devices, recording backgrounds, spoken languages, and acoustic signal conditions can differ from those in the training dataset and realworld implementation  [1] .\n\nResearchers have investigated domain adaptation techniques to enhance cross-corpus SER performance  [4] ,  [5] ,  [6] ,  [7] . Domain adaptation is a transfer learning method that involves training a model to optimise for a different distribution than the one used in the initial training. Researchers now use different types of domain adaptation strategies like, adversarial based, knowledge distillation based, and ensemble based  [8] ,  [9] ,  [10] ,  [11] . However, current domain adaptation approaches have a significant drawback: they cannot adapt to constantly changing real-world settings. In such settings, an intelligent agent interacting with customers/users could benefit from dynamically updating itself when it misclassifies speech emotion.\n\nIn this study, we address the drawback of the inability to adapt to the constantly changing real-world, by proposing a domain adaptation technique based on reinforcement learning (RL), which is uniquely suited for the dynamic requirements of real-world SER applications. Our approach leverages the dynamic updating capability of RL to develop a cross-corpus SER technique that can adapt in real-time.\n\nReinforcement learning has been utilised to optimise realworld interactions with robots and machines, following training in a simulated environment  [12] ,  [13] . However, the application of RL in the field of SER has not been explored. In a typical RL scenario, an agent takes actions in an environment, which are interpreted as a reward and a representation of the state, and fed back to the agent. This framework does not straightforwardly apply to the domain adaptation scenario of SER. As far as we are aware, our work is the first to propose FIGURE 1. Overarching design for incorporating reinforcement learning in domain adaptation to enhance the accuracy of speech emotion recognition. Initially, the Base Model is pre-trained on a source dataset and subsequently optimised for the target domain using reinforcement learning, aided by user feedback and produced a Domain Adapted Model. and evaluate techniques for using RL in this task.\n\nFigure  1  illustrates a potential use case for the proposed technique. Initially, a base deep learning model (''Base Model'') is trained on a labelled source dataset before undergoing reinforcement learning-based domain adaptation (RL-DA). The pre-trained model parameters are then transferred to the RL agent, optimised for the target domain using RL. During this optimisation, target data are provided as states to the RL algorithm, and the user provides feedback based on the RL output. The RL-DA approach employs this feedback to calculate the loss value and optimise the RL model. To facilitate effective domain adaptation, the RL-DA approach leverages continuous feedback from the environment. This interaction allows the RL agent to refine its emotion recognition predictions based on user or environment-provided feedback, enabling ongoing adaptation to real-world conditions without requiring manual retraining.\n\nThe proposed methodology aims to enhance real-world adaptability of speech emotion recognition by leveraging RL for continuous self-tuning, minimising dependence on labelled data in new domains, and providing consistent, reliable emotion detection across diverse users and environments.\n\nOur approach leverages a RL agent to play a speech emotion recognition game. We initialise the agent's model with a pre-trained SER model, rather than random initialisation. During real-world deployment, the agent receives a speech utterance and predicts the embedded emotion. The prediction is then validated, and feedback is provided to the RL agent, serving as a reward to guide future predictions. This framework also enables someone to deploy a working SER model in a live environment by training on a dataset available at hand rather than waiting for labelled data in the deployed environment.\n\nThere are many practical uses for speech emotion recognition, such as improving customer service interactions  [14]  to assisting in medical diagnosis and tracking emotional wellbeing  [15] . SER improves customer satisfaction in customer service by allowing systems to react adaptively to a user's emotional state. It can enable physicians in managing mental health disorders by monitoring and identifying changes in patients' emotional well-being. SER models must, however, dynamically adjust to shifting conditions, such as different accents, varying noise levels, and different speaking patterns, in order to perform at their best in these real-world applications. In contrast to classic supervised models, which necessitate retraining with fresh labelled data, self-tuning models-like those based on RL-are significant because they can continually update and improve their performance based on real-time feedback. Self-tuning models can adapt to these changes in dynamic situations offering accurate and dependable emotion detection in constantly changing real-world contexts. This feature is essential for maintaining SER applications' stability and efficacy by providing constant performance across various user groups and environments.\n\nIn order to make models more resilient to changes in input data, multi-condition training is a technique used in machine learning, mostly in speech processing. Deep Learning models are trained on data from multiple conditions (e.g., varying noise levels, settings, or speakers)  [16] ,  [17] . By subjecting the model to a range of situations throughout training, multicondition training aims to increase the model's generalisation to unknown conditions. This study addresses domain adaptation that is particularly applicable to the real-world SER scenario. Compared to the more general approach of multi-condition training, this can lead to more targeted performance gains. Domain adaptation methods can be designed to transfer knowledge from a source domain (e.g., a controlled environment) to a target domain (e.g., real-world conditions), potentially leading to better performance in the target domain. While multi-condition training aims for robustness across an extensive range of conditions, domain adaptation concentrates on specific domains and may not generalise effectively to completely new or unforeseen conditions outside the target domain.\n\nWe developed an example application to demonstrate the above use case  1  . This application consumes recorded audio and infers the emotion from that audio utterance while users can provide feedback on the inferred emotion. This feedback is applied for RL optimisation.\n\nWe evaluate our proposed approach considering widely used speech corpora in cross-corpus and cross-language scenarios. Our results demonstrate that our model achieves better SER performance than fully supervised benchmark models with an accuracy improvement of at least 11%. Additionally, we simulate a real-world data feed scenario and show that our RL-based model significantly outperforms the fully supervised benchmark models. We also focus on evaluating the be-haviour of our approach under speaker dependent and speaker independent settings. The results show that both speaker dependent and speaker independent settings outperform the fully supervised benchmark models.\n\nThis study focuses on, 1) Building a domain adapting speech emotion recognition model which can be used in the real-world. 2) Developing a RL based domain adapting framework which can be easily deployed in the real-word without waiting for the labelled data from the new domain.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Speech emotion recognition aims to bridge the gap between human-machine interactions by enabling systems to understand and respond to emotional cues. Traditional approaches relied on handcrafted features such as pitch, energy, and spectral properties, but recent advancements in deep learning have revolutionised SER by leveraging automatic feature extraction and robust modelling techniques  [18] ,  [19] ,  [20] ,  [21] ,  [22] ,  [23] .\n\nDeep Reinforcement Learning (Deep RL) is a novel approach that combines the techniques of RL and Deep Neural Networks (DNN) and has gained popularity with advancements in deep learning. Primarily, Deep RL has been implemented in gaming applications  [24] ,  [25] ,  [26] , recommendation systems  [27] , and robotics  [28] . However, the application of RL in speech-based domains is gradually gaining momentum  [29] .\n\nThis section presents an overview of the existing literature in two distinct groups: one group combines the literature on Domain Adaptation in SER, while the other group consolidates the previous works that employed RL for domain adaptation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Domain Adaptation In Speech Emotion Recognition",
      "text": "Domain adaptation was introduced to overcome the ''corpus bias'' issue. Since deep learning methods are powerful in extracting non-linear features from the input, domain adaptation can easily be implemented on deep learning-based platforms, yielding more robust and better performing models  [30] ,  [31] .\n\nMao et al.  [4]  conducted experiments on domain adaptation using a two-class task, where they shared priors between the source and target domains. They pre-trained a two-layer neural network using unsupervised learning and shared the standard classifier parameters between the source and target domains. In contrast, our approach follows the RL paradigm for domain adaptation and involves pre-training the source model with a labelled source dataset. However, we cannot compare our results with Mao et al.'s study on SER and domain adaptation since they used a different dataset and a different number of classes for prediction.\n\nGharib et al.  [47]  proposed unsupervised adversarial domain adaptation and pre-trained the model with two conditional sets, resulting in a nearly 10% increase in accuracy. Ahn et al.  [41]  proposed a few-shot learning methodology in unsupervised domain adaptation for cross-corpus SER tasks, where they used multiple corpora to optimise the emotion recognition robustness to unseen samples.\n\nAbdelwahab and Busso  [48]  studied the best adaptation technique for speech emotion recognition models through supervised domain adaptation or online training. They also investigated how the size of the labelled data affects the performance of the resulting model.\n\nAdversarial Domain Adaptation is a stream of research in domain adaptation  [49] ,  [50] ,  [51] . Latif et al.  [52]  proposed using adversarial training techniques to improve cross-lingual domain adaptation. This unsupervised learning method employs two distinct auto-encoders for the source and target and a single discriminator to determine whether an audio utterance is fake or real. Although we focus on cross-lingual domain adaptation, our main objective is enabling domain adaptation in real-world applications.\n\nZhang et al.  [53]  introduces a novel approach for crosscorpus SER using unsupervised domain adaptation, combining transformers with mutual information maximisation to align feature distributions across domains effectively. This approach demonstrates significant improvements in emotion recognition accuracy across multiple cross-corpus benchmarks, outperforming several baseline models and showcasing enhanced adaptability in SER tasks using IEMOCAP and MSP-Improv datasets.\n\nMote et al.  [54]  introduces unsupervised domain adaptation technique which involves a k-nearest neighbour-based voice conversion technique to adapt unlabelled speech data for emotion recognition by transforming it to match the labelled data domain. Their methodology avoids model re-training requirements for the unlabelled data. Results obtained using MSP-Improv and MSP-Podcast datasets show an 8.2% performance increment for valence detection over their baseline.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Reinforcement Learning For Domain Adaptation",
      "text": "The RL agent acquires knowledge through experiences gained by exploring and exploiting the environment. The agent selects the best action based on the policies, known as exploitation, given the current state. However, in the initial phase, the agent lacks sufficient experience to update the policy, so it explores the environment by randomly selecting actions and storing the experience in the memory buffer with a reward. Exploration poses a challenge since some actions in certain states may be unsafe  [40] .\n\nPrior knowledge can be provided to the RL agent in various ways, such as demonstration-based learning  [55] ,  [56] , pretraining, and domain adaptation  [40] . These methods reduce the exploration time and the risk of unsafe exploration. Previous research has demonstrated that pre-training reduces the training time while achieving higher performance  [57] ,  [58] ,  [59] .\n\nKoo et al.  [39]  proposed using adversarial training to enhance the coherent training of the target domain feature extractor, which is considered a generator for the target domain feature extractor. The parameters are similar to the latent Hazara and Kyrki  [60]  proposed a transfer approach that captures the core features of a simulated system and rationalises the dynamics combined with incremental learning. They demonstrated their approach in a basketball task with a robot and showed that the target model has improved task generalisation capability compared to direct usage.\n\nWhile these previous studies have employed an RL-based methodology for domain adaptation, none have investigated speech emotion recognition. In summary:\n\n1) The extensive literature on domain adaptation for SER employs various deep learning techniques. 2) While RL has been used in a few studies, including EmoRL  [35] , for speech emotion recognition tasks, no research has yet explored the application of RL for domain adaptation in SER. 3) Table  1  provides a brief overview of the literature on RL for domain adaptation and speech emotion classification, highlighting the absence of research that combines RL with domain adaptation for SER, which is the primary focus of this paper.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this section, we outline our approach for domain adaptation using RL for SER. We first provide a brief introduction to RL, followed by a description of our proposed approach and the baselines used to compare its performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Background",
      "text": "Reinforcement Learning is a machine learning paradigm that draws inspiration from behaviourist psychology and mimics the learning process of a child acquiring new skills. An RL problem typically comprises two fundamental components, namely the Environment and the Agent. In this context, the Agent interacts with the Environment by performing actions, and the Environment responds by providing a reward that corresponds to the action taken along with the updated state of the Environment. Figure  2    represents the current state of the environment, action a ∈ A is a single task that can be performed on the environment, and reward r ∈ R is the feedback value returned from the environment after executing the task a for the state s. The RL agent learns a policy π(s, a) that represents the probability of selecting action a for a given state s. The Q-value, also known as the quality value, represents the expected reward when action a is performed on an environment with state s. The discount factor γ is used to weigh the immediate reward against the expected long-term reward. The optimum Q-value Q * (s, a) can be written as in Equation  1 ;\n\nQ-values are estimated using Q-tables in q-learning, while a deep neural network called Deep Q Network (DQN) is used to estimate the Q values in deep q-learning. The loss function in Equation 2 is used to calculate the loss and Q target can be calculated using Equation  3 .\n\nCombining the two equations 2 and 3, the loss function can be rewritten as in Equation  4 .\n\nThe loss L is minimised in the training phase of RL.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Domain Adapting Reinforcement Learning For Speech Emotion Recognition",
      "text": "This study employs RL as a game for recognising emotions.\n\nThe RL agent aims to predict the correct emotion (action a, in RL terms) for a given audio utterance (state s, in RL terms). The environment returns a reward r, obtained through feedback. The RL agent learns a policy π to maximise the reward obtained at each episode. The state space S is defined as the distribution of speech audio, the action space A as the discrete emotion classes, and action selection involves inferring an emotion from a given audio utterance. According to Equation  4 , minimising L requires maximising Q(s t , a t ) while also maximising the reward R t+1 at optimum Loss (L * ).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "1) Selection Of Reinforcement Algorithm",
      "text": "The implementation of RL can be achieved through various algorithms, each characterised by unique properties. When selecting an RL algorithm, it is essential to consider the type of state space (discrete or continuous) and action space (discrete or continuous). Our SER problem is defined by a discrete state space and discrete action space, making Q-Learning-based algorithms an appropriate choice  [61] . We therefore utilise Deep Q-Learning, which exploits the capabilities of Deep Neural Networks to approximate Q-values from states, enabling efficient learning in complex environments.\n\n2) Pre-training Before RL Before using an RL agent, pre-training the DQN model has been shown to enhance its performance  [58] ,  [59] . This study uses RL to optimise the pre-trained model for the target domain. The RL agent with a pre-trained DQN model interacts with the environment and attempts to adapt the DQN to the target domain.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Baseline",
      "text": "RL methodologies typically do not report performance in terms of accuracy, due to the absence of labelled data in RL experiments. Therefore, we employed a supervised learning (SL) approach as a baseline to facilitate a comparative evaluation of our methodology's performance. To this end, we adopt the DNN architecture defined in the DQN of the RL agent (as depicted in Figure  2 ) as the architecture for the SL approach. First, we train the model using the Source Dataset to align with the pre-training in the RL approach, resulting in a Base Model as shown in Figure  1 . Next, the Base Model is trained using the Target dataset to match the RL-DA. This enables us to compare our RL-DA and Supervised Learningbased Domain Adaptation using similar parameters.\n\nTo evaluate the performance of our proposed RL-DA approach, we utilise four labelled datasets, with 20% of each dataset reserved for testing, while the remainder is used for either pre-training or RL domain optimisation. We provide more details about the used datasets in Section IV-A, while Section V outlines the various types of experiments conducted.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experimental Setup",
      "text": "In this section, we elaborate on the datasets, input features, and model configurations. Figure  2  depicts the detailed RL architecture and the DQN model employed in our study.\n\nTo simulate the RL environment, we receive the emotion (action) from the RL agent and output the reward and subsequent audio utterance as the state. The environment determines the reward by comparing the inferred class (agent's action) with the ground truth label in the dataset. Reward is positive if the action is inferred correctly and negative otherwise. In commercial or real-world applications, this environment can be replaced by integrating it into a feedback system where the feedback is generated considering the interaction of the user. . The convergence criteria is measured using the loss difference between the inferred and target Q-Values. The model is considered converged, when the loss difference is less than 0.2 for 100 iterations.\n\nEach feedback from the environment is stored in a memory database along with the audio utterance (state), reward, and inferred emotion (action), which are then utilised to optimise the DQN model after a specified number of iterations. The DQN model is a Deep Neural Network that comprises a combination of Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and dense layers. It ingests the Mel Frequency Cepstral Coefficients (MFCC) feature matrix and outputs the estimated Q-values required for the RL policy to determine the emotion (action). Therefore, the DQN model estimates the Q-values for a given audio utterance (state), and these Q-values are used by the RL policy to determine the emotion (action).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "A. Datasets",
      "text": "We utilise four publicly available datasets widely used in the field of SER: MSP-IMPROV  [42] , IEMOCAP  [36] , ESD  [46] , and EmoDB  [33] . These datasets enable the use of Cross-Corpus (CC) and Cross-Language (CL) experiments.\n\nDue to the unbalanced number of utterances under each emotion class for all datasets, we use an audio augmentation technique to generate audio utterances. We use Vocal Tract Length Perturbation (VLTP)  [62]  to augment audio utterances, which balances the number of utterances under each emotion class when preparing the testing subsets. All four speech emotion datasets utilised in this study contain categorical emotional labels, and we selected four emotions widely used in the literature: happiness, sadness, anger, and neutral.\n\nAdditionally, we use the DEMAND  [63]  dataset's kitchen environment audio as the background sound to evaluate the proposed RL-DA approach in real-world scenarios. Section V-C describes the usage of the DEMAND dataset in the experiments. Below, we provide more details on the considered datasets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "1) Iemocap (Iem)",
      "text": "IEMOCAP is a widely used dataset that comprises 12 hours of acted multi-speaker audio-visual data. The IEMOCAP dataset consists of dyadic sessions featuring both scripted and improvised scenarios. It contains categorical emotion labels of happiness, sadness, anger, and neutrality, as well as dimensional labels of valence, dominance, and activation  [36] . In this study, we utilise the audio data modality from the improvised scenarios and restrict our analysis to the categorical emotion labels.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "2) Msp-Improv (Msp)",
      "text": "MSP-IMPROV is an audio-visual database comprising acted performances that is widely used in multi-modal speech emotion recognition research. While originally designed for an audio-visual emotional perception study, it has also been utilised in various speech emotion recognition studies  [64] ,  [65] ,  [41] ,  [66] , rendering it a suitable dataset for our research purposes. The database was recorded in a controlled environment, with 20 pre-determined scripts that encompass the primary emotions of happiness, sadness, anger, and neutrality  [42] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "3) Emotional Speech Dataset (Esd)",
      "text": "The Emotional Speech Database (ESD) is a publicly available database of speech data originally designed for the purposes of speech synthesis and voice conversion. The database includes utterances from 20 speakers, consisting of 10 Mandarin and 10 English speakers, and each utterance has been categorised into one of five emotion classes: happy, surprise, neutral, angry, and sad  [46] . For our study, we exclusively selected utterances from 10 English speakers that correspond to the emotions of happiness, sadness, anger, and neutrality.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "4) Berlin Emodb (Emodb)",
      "text": "EmoDB is a database of emotional speech utterances in the German language that features recordings from 10 actors (5 male and 5 female) between the ages of 21 and 35. Each actor recorded 10 scripted texts, covering 7 different emotions, including anger, boredom, disgust, fear, happiness, sadness, and neutrality  [33] . In our cross-language experimental analysis, we selected EmoDB as the target dataset and focused solely on the utterances from the four emotion classes of anger, happiness, sadness, and neutrality.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "5) Diverse Environments Multichannel Acoustic Noise Database (Demand)",
      "text": "The DEMAND dataset is a widely utilised collection of realworld background noises. This dataset includes background noises from 18 different environments, categorised into six categories. The audio recordings from each environment were captured using an array of 16 microphones and stored in 16 channels. The DEMAND dataset offers versions of both 16 kHz and 48 kHz sampling rates; in this study, we employed the 48 kHz version and down-sampled it to 22 kHz to align with the other datasets used in our analysis.\n\nA summary of the datasets employed in this study is presented in Table  2 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Input Features",
      "text": "In this study, we employed MFCC as the input features for our analysis congruent to previous studies  [67] ,  [68] ,  [69] ,  [70] . Specifically, we set the frame length to 2, 048 and the hop length to 512, and extracted 40 MFCCs using the Librosa python library for audio and music analysis  [71] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Model Configuration",
      "text": "In this study, we employ a combined model of CNN and LSTM as it facilitates learning both frequency and temporal components in the speech signal  [72] ,  [59] . The discriminative features of the model are learnt by stacking CNN, LSTM, and fully connected layers, respectively, as depicted in Figure  2 . The feature matrix is processed through two layers of 2D convolution with filter sizes of 5 and 3. The output of the first 2D convolution layer is batch-normalised, and a hyperparameter batch size of 128 is used. The output from the second 2D convolution layer is flattened and fed into an LSTM layer with 16 cells, followed by a fully connected layer with 256 units. A dropout rate of 0.3 is applied before the last Dense output layer. The number of units in the output layer is set to four, which is the number of emotions to be classified. The linear activation function of the output layer is used in the RL agent as it outputs Q-values for a specific state, which should not be normalised. The input shape of the model is 40 × 87, where 40 MFCCs are used in the input, and 87 is the number of MFCC frames.\n\nThe model is optimised using the Adam Optimiser with a 2.5 × 10 -4 learning rate. The Deep Learning API Keras  [73]  with Tensorflow  [74]  (version 2.1.0) is used as the back-end for modelling and training purposes in this study.\n\nThe model consists of 42,966 trainable parameters and 87,327 floating-point operations (FLOPs) which gives a perspective on the complexity of the model. Considering the FLOPs, it indicates that the model is lightweight, making it scalable and suitable for deployment in resource-constrained environments as well. Low parameter count leads to a faster convergence and reduced memory requirements.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "V. Evaluation",
      "text": "We devised and executed several experiments to evaluate the performance of the proposed RL-DA approach. We formulated three scenarios: (1) pre-training with a source dataset and domain optimisation with a target dataset separately, (2) pre-training with a subset of the source dataset and domain optimisation with a mix of the remaining source and target datasets, and (3) domain adaptation in a recreated real-world setting. Each scenario's specifics and findings are outlined in Sections V-A, V-B, and V-C, respectively. We also evaluated the proposed RL-DA method for speaker-independent SER in Section V-E.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "1) Selection Of Source And Target Datasets",
      "text": "Since deep RL algorithms learn from feedback instead of directly labelled data, they necessitate more data instances than supervised learning algorithms  [75] ,  [76] . Therefore, we opted for IEMOCAP and MSP-Improv as the source datasets and ESD and EmoDB as the target datasets in crosscorpus and cross-language experiments, respectively. In the RL agent, the Max Boltzman Policy (Max.B)  [77]  serves as the RL policy.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "2) Evaluation Metrics",
      "text": "We use the Unweighted Average Recall Rate (UAR,%), which is a well-known metric researchers use to assess the performance of speech-based machine learning models  [4] ,  [41] ,  [43] ,  [44] . The recall is computed for each classification problem label, and the UAR is calculated as the unweighted average of each label's recall values.\n\nThe Python code repository for the experiments and data preparation is available in a public GitHub repository called ''RL-DomainAdaptation'' 2 . corpus and cross-language schemas. Table  3  and Figure  3 (a)  show the datasets used for each schema. Initially, the model is pre-trained using the training subset of the source dataset, resulting in the ''Base model''. Subsequently, the pre-trained parameters of the Base model are transferred to the DQN of the RL agent. The RL deep Q learning algorithm is then executed to optimise the DQN model using the target dataset in the environment, leading to the domain-optimised DNN model. We employ a Supervised Learning approach to evaluate the domain-optimised model's performance. Specifically, we utilise the testing subset of the Target dataset to infer the emotion of each testing utterance and record both the inferred emotion and the labelled emotion for the utterance. We calculate the UAR value for each experiment based on the recorded data about inferred emotions and ground truth.\n\nWe also measure the performance of our methodology using a Supervised Learning approach as the baseline (SL-DAsep). We develop a DNN model with an identical architecture to the one used in DQN (as depicted in Figure  2 ), which is pre-trained using the Source dataset. Next, the same model is trained using the training subset of the Target dataset. Finally, the resulting model is tested using the testing subset of the Target dataset to obtain the testing accuracy.\n\nWe report the UAR of the baseline (SL-DA -sep) model after training with Supervised Learning and the UAR of the model after training with the RL-DA approach (RL-DA) in Table  4 .\n\nOur findings indicate that the RL-DA approach outperforms the baseline UAR in most scenarios. Furthermore, when comparing the SL-DA method and the RL-DA method using the results in Table  4 . We observe that the standard deviation of the cross-language schema has decreased from 8.26 to 0.71. This demonstrates that the model performances are more consistent using the RL-DA approach. This outcome is significant because, unlike SL approaches that rely on labelled data to train the model, RL methods only provide feedback rewards indicating the accuracy of inference made during the RL optimisation phase. Therefore, achieving SL accuracy with the RL-DA approach implies that the method has successfully trained the model to the SL standard.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Experiment Mixing The Source And Target Datasets",
      "text": "This subsection assesses the effectiveness of our proposed methodology, where a subset of the source dataset is included in the RL optimisation phase to preserve the generalisable features of the source dataset.\n\nWe conduct this experiment similar to the previous experiment mentioned in Section V-A, with the following modifications: only a subset of the training set of the source dataset is employed in the pre-training phase. In contrast, a combination of the remaining training set of the source dataset and the training set of the target dataset is utilised in the RL optimisation phase. Table  5  and Figure  3  (b) depict how the datasets are employed in this experiment. We employ SL as a baseline (SL-DA -mix) and measure the performance to compare our proposed approach. Results of the experiments done by mixing source data into the target dataset as mentioned above are presented in Table  6 . The accuracy of the RL-DA approach is compared with the Supervised Learning approach -SL-DA -mix. The RL-DA approach demonstrates superior performance over SL-DA -mix by at least 8%, in every experiment. Since the target dataset includes elements from the source dataset, the model representations learned during the pretraining phase are maintained while the model adapts to the new domain.\n\nWe compare the performance of RL-DA in two scenarios: where the target dataset is isolated from the source dataset (S) (as discussed in Section V-A), and where the target dataset is mixed (M) with utterances from the source dataset (as discussed in Section V-B). These comparisons are shown in Figure  4 .  From the above visualisation, we note that the mixing of source and target datasets yields better performance than experiments with separate datasets. The mixing of datasets increases the diversity of the training utterances which leads the model to learn generalised feature representations when training.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "C. Experiments Simulating Live Data Feed And Real-World Data Scenarios",
      "text": "This study aims to compare the performance of supervised learning and reinforcement learning approaches in a simulated live data feed scenario. To avoid the difficulty of testing in a real-world environment, we continually push data from the target dataset and simulate a live scenario. The RL-DA approach continuously updates the model based on the incoming data, while the SL-DA method uses a static model trained on the source dataset.\n\nThe methodology of RL optimisation is similar to the experiment described in Section V-A. First, the baseline model (SL-DA -rw) is trained with a subset of the source dataset. Then, the testing accuracy is measured using testing subsets of the target dataset.\n\nWe also evaluate the performance of RL-DA on real-world audio data. To achieve this, we mix background audio with the target datasets to create a new dataset with background noise. The DEMAND dataset's kitchen environment audio is used as the background sound to mix with the ESD and EmoDB datasets, with a Signal-to-Noise ratio of -5 dB.\n\nTable  7  shows the testing accuracy of the models trained using three different approaches (SL-DA -rw, RL-DA -target dataset without background noise (RL-DA w/o noise), and RL-DA -target dataset with background noise (RL-DA w/ noise)). The increment of accuracy in the adjacent RL approach compared to the SL approach is represented in the ∆ columns.\n\nOur findings indicate that RL-DA outperforms the baseline SL strategy in the live data feed scenario. In supervised learning methodologies, without a labelled dataset to retrain during inference time, the model cannot adapt to the deviation of a domain distribution. On the other hand, RL-based learning methodologies constantly receive feedback from the environment and optimise the agent model for domain deviations on the go. Retraining the model using supervised learning is feasible, but this would necessitate manual intervention. In contrast, RL-based methods incorporate this capability by design. Thus, RL is more suitable for domain adaptation in realworld scenarios due to its adaptability during optimisation.\n\nFigure  5  presents a comparison of the UAR achieved by the Baseline, RL-DA without noise, and RL-DA with noise in the simulated live data feed experiments. The results indicate that both RL-DA approaches outperform the baseline method.\n\nTo further evaluate the performance of RL-DA, we calculate the average improvement gained by using RL compared to the SL approach (µ ∆ ) for each Cross-Corpus (µ ∆ CC ) and  Cross-Language (µ ∆ CL ) schema using the equation 5:\n\n; where X = CC or CL  (5)  Here, n = 4, as there are 4 experiments for each CC and CL schema. The results show that using RL over the supervised learning approach yields an average increment (µ ∆ ) of 11.77% and 14.46% for CC and CL schemas, respectively.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "D. Experiments With Wav2Vec2 Input Features",
      "text": "We conducted an additional experiment using the IEMOCAP dataset as the source and the ESD dataset as the target, employing wav2vec 2.0  [78]  as the feature extractor for raw audio instead of the MFCC input. The results are tabulated on Table  8 . We compare the results of the corresponding experiments with MFCC as feature. The average improvement of RL-DA over SL-DA (µ ∆ ) was 10.44, which aligns with the performance gains observed for the same source/target combination when using MFCC as the feature set as mentioned in Table  7 . While wav2vec 2.0 increases computational resource demands and extends processing time, the primary focus of this study is on the relative performance difference between RL-DA and SL-DA, rather than on absolute accuracy. For researchers primarily interested in optimising accuracy, feature extractors like wav2vec 2.0 or similar alternatives may be preferable over MFCC.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "E. Speaker Independent Experiments",
      "text": "This study extends our investigation by evaluating the proposed method for speaker-independent speech emotion recognition. We conduct two separate experiments using IEMOCAP and MSP-Improv as source datasets and ESD as the target dataset. To achieve speaker independence, we pretrain the base model using the source dataset and optimise it using the target dataset for the new domain. We evaluate the speaker-independent performance of the model by using a subset of the target dataset recorded by a specific speaker as the testing data and the remaining data as the training data.\n\nWe optimise the model with the training data and measure the testing accuracy. The experimental results are presented in Figure  6 . Comparing the results to Table  7 , we observe that the RL-DA approach outperforms the baseline testing accuracy in both speaker-dependent and speaker-independent scenarios.\n\nAdditionally, it is apparent that in both SL-DA and RL-DA scenarios, speaker dependent performance is higher than speaker independent performance, a finding that is further supported by the literature.  [79] .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vi. Discussion",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A. The Limitations Of Traditional Supervised Learning In Real Time Adaptation",
      "text": "Traditional SL models typically assume that the underlying data distribution remains static after training. However, in The fundamental difference between SL and RL lies in their approach to learning. SL operates in a single-step process, where the model predicts outputs based on the training data it has seen, without adjusting to new data after deployment. In contrast, RL requires multiple iterative steps, where the agent interacts with the environment, receives feedback, and updates its policy continuously. While this makes RL more adaptive, it also leads to increased computational complexity and training time.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B. Impact Of Feedback Frequency On Model Adaptability",
      "text": "In this study, we found that the frequency of feedback plays a critical role in the model's ability to adapt effectively to new domains. Optimal performance is achieved when the model receives feedback for each prediction, as this continuous reinforcement allows for accurate adaptation to the shifting contexts encountered in real-world applications. However, we recognise that this setup may not always be feasible due to practical constraints. As such, periodic feedback can also be employed, although this may slow the rate of convergence and reduce the model's adaptability over time. Ultimately, the decision on feedback frequency can be adjusted according to implementation needs, allowing for a balance between computational efficiency and model performance. Our findings reinforce the adaptability of reinforcement learning-based domain adaptation, highlighting how feedback frequency influences convergence rates and the overall adaptability of speech emotion recognition models in dynamic settings.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C. Limitations Of Reinforcement Learning",
      "text": "Despite the advantages of RL in handling dynamic environments, it has its own limitations. One major concern is stability. While RL is generally more robust to changing input distributions than SL, ensuring stability during training can be challenging. RL algorithms, especially deep RL methods, are known for their sensitivity to hyperparameters and exploration strategies. Poor exploration can lead to sub optimal policies, and over-exploration may result in instability during the training phase  [75] .\n\nAn additional limitation in this approach is the observed variability in performance outcomes, as indicated by a higher standard deviation in the results. This higher variability suggests that the model may lack stability, which can hinder its reliability in certain applications. Such fluctuations in performance underscore the sensitivity of RL to specific conditions and further highlight the potential instability introduced during training. This issue needs to be addressed to ensure consistent model behaviour and to reduce the risk of unreliable performance in real-world applications.\n\nAdditionally, while RL's adaptability is beneficial, it often requires many iterations to converge, increasing computational costs and training time  [80] . This presents a tradeoff between the model's ability to adapt and the resources required to maintain such flexibility. Although RL shows promise for domain adaptation, particularly in real-world SER tasks, these limitations must be carefully managed to fully realise its potential.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "This study investigated the use of Reinforcement Learning (RL) for domain adaptation in Speech Emotion Recognition (SER), marking the first attempt to apply RL for this purpose in SER. Previous approaches have primarily relied on supervised learning (SL) techniques, but the findings of this study suggest that RL offers superior domain adaptability, particularly in real-world scenarios. The proposed RL-based domain adaptation (RL-DA) framework demonstrated an increase in accuracy by 11% and 14% in cross-corpus and cross-language setups, respectively, compared to baseline SL approaches.\n\nA key factor contributing to this improved performance is the RL agent's ability to continually incorporate feedback, eliminating the need for manual retraining that SL models require. Furthermore, the study introduced a simulated environment for training, given the challenges of implementing a human-in-the-middle system during the training phase. Additionally, a human-in-the-middle application has been developed and publicly hosted at https://rlemotion.cloud.edu.au,  VOLUME 11, 2023  showcasing practical implementation in dynamic real-world scenarios.\n\nFuture directions will focus on enhancing the framework through continuous learning and scenario-specific testing. By enabling online the model can dynamically adjust to new conditions without full retraining. Scenario-specific protocols involving real-time interactions in varied settings will also be explored, enabling robust performance across different environments such as noisy versus quiet spaces or indoor versus outdoor contexts.\n\nThe outcomes of this research set a new direction for the community and pave the way for developing emotionaware applications that can flexibly adapt to users' emotional expressions across diverse domains.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overarching design for incorporating reinforcement learning in",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates a potential use case for the proposed",
      "page": 2
    },
    {
      "caption": "Figure 2: depicts the general architecture",
      "page": 4
    },
    {
      "caption": "Figure 2: The architecture of a Reinforcement Learning system featuring a Deep Q Network. The RL Agent comprises several constituent parts: Memory,",
      "page": 5
    },
    {
      "caption": "Figure 2: ) as the architecture for the SL",
      "page": 6
    },
    {
      "caption": "Figure 1: Next, the Base Model",
      "page": 6
    },
    {
      "caption": "Figure 2: depicts the detailed RL",
      "page": 6
    },
    {
      "caption": "Figure 2: The feature matrix is processed through two layers",
      "page": 7
    },
    {
      "caption": "Figure 2: ), which is",
      "page": 8
    },
    {
      "caption": "Figure 3: (b) depict",
      "page": 8
    },
    {
      "caption": "Figure 3: Composition of datasets used in Target and Source: (a) the Target dataset is not mixed with Source data and (b) the Target dataset is mixed",
      "page": 9
    },
    {
      "caption": "Figure 4: Comparison of accuracy of each RL-DA experiment with",
      "page": 9
    },
    {
      "caption": "Figure 5: presents a comparison of the UAR achieved by the",
      "page": 9
    },
    {
      "caption": "Figure 5: Comparison of UAR of SL-DA - rw and the proposed RL-DA",
      "page": 10
    },
    {
      "caption": "Figure 6: Comparing the results to Table 7, we observe that the",
      "page": 10
    },
    {
      "caption": "Figure 6: Comparison of UAR of Speaker Dependent SL-DA, Speaker",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 1: provides a brief overview of the literature on oftheRLmethodologyinaconcisemanner.",
      "data": [
        {
          "Paper": "",
          "Focus": "RL",
          "Brief Experimental Set-up & Results": "",
          "Methodology": ""
        },
        {
          "Paper": "Mao et al, 2016 [4]",
          "Focus": "✗",
          "Brief Experimental Set-up & Results": "61.54% UAR for ABC [32] & 57.58% UAR for\nEmo-DB [33]",
          "Methodology": "Sharing priors between source & target"
        },
        {
          "Paper": "Carr et al, 2018 [26]",
          "Focus": "✓",
          "Brief Experimental Set-up & Results": "Converges solution 800000 steps faster\nin Pong\nGame using Arcade Learning Environment [34]",
          "Methodology": "Adversarial\nautoencoder\naligns\nsource\nand target domains via feature space"
        },
        {
          "Paper": "Lakomkin et al, 2018 [35]",
          "Focus": "✓",
          "Brief Experimental Set-up & Results": "84.9% Accuracy with 1.82x speed-up for IEMO-\nCAP [36]",
          "Methodology": "Action-based\nemotion\ndetection with\nGRU"
        },
        {
          "Paper": "Hossain and Muhammad,\n2019 [37]",
          "Focus": "✗",
          "Brief Experimental Set-up & Results": "86.4% Accuracy for ELM fusion using eNTER-\nFACE 05 [38] & 99.9% for ELM fusion using\nRecorded Data",
          "Methodology": "Multimodal\nfusion of audio-visual data\nusing deep convolutional networks"
        },
        {
          "Paper": "Koo et al, 2019 [39]",
          "Focus": "✓",
          "Brief Experimental Set-up & Results": "Task\nSuccess Rate\nof\n90.1% for\nSan\nFran-\ncisco Restaurant dataset to Cambridge Restaurant\ndataset",
          "Methodology": "Adversarial calibrator aligns feature ex-\ntractors across domains using RL"
        },
        {
          "Paper": "Arndt et al, 2020 [40]",
          "Focus": "✓",
          "Brief Experimental Set-up & Results": "Policy trained with meta-learning exhibited less\nvariance & improved adaptation compared to do-\nmain randomisation",
          "Methodology": "Meta-policy\ntraining\nwith\ntrajectory\ngeneration\nfor\nrapid\nsim-to-real\nadaptation"
        },
        {
          "Paper": "Ahn et al, 2021 [41]",
          "Focus": "✗",
          "Brief Experimental Set-up & Results": "50.8% UA for\ndatasets MSP-IMPROV [42],\nEMO-DB [33], and KME-Korean) using IEMO-\nCAP as source",
          "Methodology": "Few-shot\nlearning with DA for\ncross-\ncorpus emotion classification"
        },
        {
          "Paper": "Ishaq et al, 2023 [43]",
          "Focus": "✗",
          "Brief Experimental Set-up & Results": "78.34% UA for\nIEMOCAP & 91.61% UA for\nEMO-DB",
          "Methodology": "Extract\ntemporal speech features using\nTCN & classify emotions via FCN"
        },
        {
          "Paper": "Khan et al, 2024 [44]",
          "Focus": "✗",
          "Brief Experimental Set-up & Results": "72.30% UA for IEMOCAP and MELD [45]",
          "Methodology": "Cross-attention, deep fusion, CNNs &\nspeech-text alignment based SER"
        },
        {
          "Paper": "This paper",
          "Focus": "✓",
          "Brief Experimental Set-up & Results": "UAR% is measured for datasets: MSP-IMPROV,\nIEMOCAP,\nESD [46],\nand\nEmoDB. Results\nshown in Section V",
          "Methodology": "Pre-train model, RL optimises DA using\nfeedback"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "IEMOCAP\nMSP-Improv\nESD\nEmoDB",
          "# Training": "2780\n432\n7744\n344",
          "# Testing": "852\n148\n2428\n100",
          "# Classes": "4\n4\n5\n7",
          "Language": "English\nEnglish\nEnglish\nGerman"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: and Figure 3 (b) depict",
      "data": [
        {
          "Training Dataset\nSource\nTarget": "IEM\nESD",
          "Testing Dataset": "IEM + ESD"
        },
        {
          "Training Dataset\nSource\nTarget": "MSP\nESD",
          "Testing Dataset": "MSP + ESD"
        },
        {
          "Training Dataset\nSource\nTarget": "IEM\nEmoDB",
          "Testing Dataset": "IEM + EmoDB"
        },
        {
          "Training Dataset\nSource\nTarget": "MSP\nEmoDB",
          "Testing Dataset": "MSP + EmoDB"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: and Figure 3 (b) depict",
      "data": [
        {
          "Training Dataset\nSource\nTarget": "50% IEM\n50% IEM + ESD",
          "Testing Dataset": "IEM + ESD"
        },
        {
          "Training Dataset\nSource\nTarget": "50% MSP\n50% MSP + ESD",
          "Testing Dataset": "MSP + ESD"
        },
        {
          "Training Dataset\nSource\nTarget": "50% IEM\n50% IEM + EmoDB",
          "Testing Dataset": "IEM + EmoDB"
        },
        {
          "Training Dataset\nSource\nTarget": "50% MSP\n50% MSP + EmoDB",
          "Testing Dataset": "MSP + EmoDB"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: and Figure 3 (b) depict",
      "data": [
        {
          "Dataset\nSource\nTarget": "IEM\nIEM + ESD\nMSP\nMSP + ESD",
          "UAR(%)\nSL-DA - mix\nRL-DA": "77.86 ± 0.43\n56.22 ± 1.15\n63.51 ± 2.40\n55.52 ± 0.97"
        },
        {
          "Dataset\nSource\nTarget": "IEM\nIEM + EmoDB\nMSP\nMSP + EmoDB",
          "UAR(%)\nSL-DA - mix\nRL-DA": "85.67 ± 2.72\n61.33 ± 5.44\n77.33 ± 1.43\n59.17 ± 1.43"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: and Figure 3 (b) depict",
      "data": [
        {
          "Dataset\nSource\nTarget": "IEM\nESD\nMSP\nESD",
          "UAR (%)\nSL-DA - sep\nRL-DA\nLiterature": "63.99 ± 1.95\n63.54 ± 2.11\n-\n66.10 ± 0.84\n63.18 ± 3.34\n-"
        },
        {
          "Dataset\nSource\nTarget": "IEM\nEmoDB\nMSP\nEmoDB",
          "UAR (%)\nSL-DA - sep\nRL-DA\nLiterature": "74.67 ± 1.03\n73.17 ± 0.62\n62.91 [53]\n65.50 ± 0.71\n56.67 ± 8.26\n-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 7: shows the testing accuracy of the models trained",
      "data": [
        {
          "Mixed\n(b)\nTarget\nTesting\nSource\nDomain\nPre-training\nAdaptation": "",
          "OO\nO\nX\nX\nO\nX\nX": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 7: shows the testing accuracy of the models trained",
      "data": [
        {
          "Separate\n(a)\nTarget\nTesting\nSource\nDomain\nPre-training\nAdaptation": "",
          "OO\nO\nX\nX\nO\nX\nX": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nSource\nTarget": "IEM\nESD\nMSP\nESD\nIEM\nEmoDB\nMSP\nEmoDB",
          "UAR(%)\nSL-DA - rw\nRL-DA\nµ∆": "62.89 ± 0.51\n53.11 ± 1.45\n9.78\n63.18 ± 2.15\n47.07 ± 2.56\n16.11\n73.83 ± 1.55\n60.67 ± 1.70\n13.16\n66.17 ± 0.24\n46.50 ± 1.08\n19.67"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Cross-Corpus Acoustic Emotion Recognition: Variances and Strategies",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "M Wollmer",
        "A Stuhlsatz",
        "A Wendemuth",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "A Cross-Corpus Study on Speech Emotion Recognition,'' 2019 IEEE Automatic Speech Recognition and Understanding Workshop",
      "authors": [
        "R Milner",
        "M Jalal",
        "R Ng",
        "T Hain"
      ],
      "venue": "A Cross-Corpus Study on Speech Emotion Recognition,'' 2019 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "3",
      "title": "Improving Cross-Corpus Speech Emotion Recognition with Adversarial Discriminative Domain Generalization (ADDoG)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Domain adaptation for speech emotion recognition by sharing priors between related source and target classes",
      "authors": [
        "Q Mao",
        "W Xue",
        "Q Rao",
        "F Zhang",
        "Y Zhan"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "5",
      "title": "Unsupervised Cross-Corpus Speech Emotion Recognition Using Domain-Adaptive Subspace Learning",
      "authors": [
        "N Liu",
        "Y Zong",
        "B Zhang",
        "L Liu",
        "J Chen",
        "G Zhao",
        "J Zhu"
      ],
      "year": "2018",
      "venue": "ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Speaker to emotion: Domain adaptation for speech emotion recognition with residual adapters",
      "authors": [
        "Y Xi",
        "P Li",
        "Y Song",
        "Y Jiang",
        "L Dai"
      ],
      "year": "2019",
      "venue": "Speaker to emotion: Domain adaptation for speech emotion recognition with residual adapters"
    },
    {
      "citation_id": "7",
      "title": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2019",
      "year": "2019",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2019"
    },
    {
      "citation_id": "8",
      "title": "Cross lingual speech emotion recognition via triple attentive asymmetric convolutional neural network",
      "authors": [
        "E Ocquaye",
        "Q Mao",
        "Y Xue",
        "H Song"
      ],
      "venue": "International Journal of Intelligent Systems"
    },
    {
      "citation_id": "9",
      "title": "Unsupervised Domain Adaptation Schemes for Building ASR in Low-Resource Languages,'' 2021 IEEE Automatic Speech Recognition and Understanding Workshop",
      "authors": [
        "A Ramakrishnan"
      ],
      "year": "2021",
      "venue": "Unsupervised Domain Adaptation Schemes for Building ASR in Low-Resource Languages,'' 2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "10",
      "title": "Unsupervised Speech Domain Adaptation Based on Disentangled Representation Learning for Robust Speech Recognition",
      "authors": [
        "J.-H Park",
        "M Oh",
        "H.-M Park"
      ],
      "year": "2019",
      "venue": "Unsupervised Speech Domain Adaptation Based on Disentangled Representation Learning for Robust Speech Recognition"
    },
    {
      "citation_id": "11",
      "title": "Domain Adaptation via Teacher-Student Learning for End-To-End Speech Recognition,'' 2019 IEEE Automatic Speech Recognition and Understanding Workshop",
      "authors": [
        "Z Meng",
        "J Li",
        "Y Gaur",
        "Y Gong"
      ],
      "venue": "Domain Adaptation via Teacher-Student Learning for End-To-End Speech Recognition,'' 2019 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "12",
      "title": "Domain Adaptation: Challenges, Methods, Datasets, and Applications",
      "authors": [
        "P Singhal",
        "R Walambe",
        "S Ramanna",
        "K Kotecha"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "13",
      "title": "Sim-to-Real: Learning Agile Locomotion For Quadruped Robots",
      "authors": [
        "J Tan",
        "T Zhang",
        "E Coumans",
        "A Iscen",
        "Y Bai",
        "D Hafner",
        "S Bohez",
        "V Vanhoucke"
      ],
      "year": "2018",
      "venue": "Robotics: Science and Systems"
    },
    {
      "citation_id": "14",
      "title": "DeepRacer: Autonomous Racing Platform for Experimentation with Sim2Real Reinforcement Learning",
      "authors": [
        "B Balaji",
        "S Mallya",
        "S Genc",
        "S Gupta",
        "L Dirac",
        "V Khare",
        "G Roy",
        "T Sun",
        "Y Tao",
        "B Townsend",
        "E Calleja",
        "S Muralidhara",
        "D Karuppasamy"
      ],
      "venue": "DeepRacer: Autonomous Racing Platform for Experimentation with Sim2Real Reinforcement Learning"
    },
    {
      "citation_id": "15",
      "title": "End-to-End Continuous Speech Emotion Recognition in Real-life Customer Service Call Center Conversations",
      "authors": [
        "Y Feng",
        "L Devillers"
      ],
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"
    },
    {
      "citation_id": "16",
      "title": "Emotion Recognition for Healthcare Surveillance Systems Using Neural Networks: A Survey",
      "authors": [
        "M Dhuheir",
        "A Albaseer",
        "E Baccour",
        "A Erbad",
        "M Abdallah",
        "M Hamdi"
      ],
      "year": "2021",
      "venue": "Emotion Recognition for Healthcare Surveillance Systems Using Neural Networks: A Survey"
    },
    {
      "citation_id": "17",
      "title": "pMCT: Patched Multi-Condition Training for Robust Speech Recognition",
      "authors": [
        "P Parada",
        "A Dobrowolska",
        "K Saravanan",
        "M Ozay"
      ],
      "year": "2022",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH"
    },
    {
      "citation_id": "18",
      "title": "Robust speech recognition with speech enhanced deep neural networks",
      "authors": [
        "J Du",
        "Q Wang",
        "T Gao",
        "Y Xu",
        "L Dai",
        "C Lee"
      ],
      "year": "2014",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "19",
      "title": "A robust feature selection method based on meta-heuristic optimization for speech emotion recognition",
      "authors": [
        "K Bagadi",
        "C Sivappagari"
      ],
      "year": "2024",
      "venue": "Evolutionary Intelligence",
      "doi": "10.1007/s12065-022-00772-5"
    },
    {
      "citation_id": "20",
      "title": "Improvement of emotion classification performance using multi-resolution variational mode decomposition method",
      "authors": [
        "S Mishra",
        "P Warule",
        "S Deb"
      ],
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "21",
      "title": "Improving speech emotion recognition by fusing self-supervised learning and spectral features via mixture of experts",
      "authors": [
        "J Hyeon",
        "Y Oh",
        "Y Lee",
        "H Choi"
      ],
      "venue": "Data & Knowledge Engineering"
    },
    {
      "citation_id": "22",
      "title": "Squeeze-andexcitation 3D convolutional attention recurrent network for end-to-end speech emotion recognition",
      "authors": [
        "N Saleem",
        "H Elmannai",
        "S Bourouis",
        "A Trigui"
      ],
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "23",
      "title": "DeepCNN: Spectro-temporal feature representation for speech emotion recognition",
      "authors": [
        "N Saleem",
        "J Gao",
        "R Irfan",
        "A Almadhor",
        "H Rauf",
        "Y Zhang",
        "S Kadry"
      ],
      "venue": "CAAI Transactions on Intelligence Technology"
    },
    {
      "citation_id": "24",
      "title": "An evolutionary optimization method for selecting features for speech emotion recognition",
      "authors": [
        "K Bagadi",
        "C Sivappagari"
      ],
      "year": "2023",
      "venue": "TELKOMNIKA (Telecommunication Computing Electronics and Control)"
    },
    {
      "citation_id": "25",
      "title": "Reinforcement learning in games",
      "authors": [
        "I Szita"
      ],
      "year": "2012",
      "venue": "Adaptation, Learning, and Optimization"
    },
    {
      "citation_id": "26",
      "title": "Playing Atari with Deep Reinforcement Learning",
      "authors": [
        "V Mnih",
        "K Kavukcuoglu",
        "D Silver",
        "A Graves",
        "I Antonoglou",
        "D Wierstra",
        "M Riedmiller"
      ],
      "venue": "Playing Atari with Deep Reinforcement Learning"
    },
    {
      "citation_id": "27",
      "title": "Domain Adaptation for Reinforcement Learning on the Atari",
      "authors": [
        "T Carr",
        "M Chli",
        "G Vogiatzis"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems"
    },
    {
      "citation_id": "28",
      "title": "Deep reinforcement learning for recommender systems",
      "authors": [
        "I Munemasa",
        "Y Tomomatsu",
        "K Hayashi",
        "T Takagi"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Information and Communications Technology, ICOIACT 2018"
    },
    {
      "citation_id": "29",
      "title": "Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: A Survey",
      "authors": [
        "W Zhao",
        "J Queralta",
        "T Westerlund"
      ],
      "year": "2020",
      "venue": "IEEE Symposium Series on Computational Intelligence"
    },
    {
      "citation_id": "30",
      "title": "A survey on deep reinforcement learning for audiobased applications",
      "authors": [
        "S Latif",
        "H Cuayáhuitl",
        "F Pervez",
        "F Shamshad",
        "H Shehbaz",
        "E Ali",
        "S Cambria",
        "Latif"
      ],
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "31",
      "title": "Feature Learning in Deep Neural Networks -Studies on Speech Recognition Tasks",
      "authors": [
        "D Yu",
        "M Seltzer",
        "J Li",
        "J.-T Huang",
        "F Seide"
      ],
      "year": "2013",
      "venue": "Feature Learning in Deep Neural Networks -Studies on Speech Recognition Tasks"
    },
    {
      "citation_id": "32",
      "title": "Coupled Unsupervised Deep Convolutional Domain Adaptation for Speech Emotion Recognition",
      "authors": [
        "O Nii Noi",
        "M Qirong",
        "G Xu",
        "Y Xue"
      ],
      "year": "2018",
      "venue": "2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)"
    },
    {
      "citation_id": "33",
      "title": "Audiovisual behavior modeling by combined feature spaces",
      "authors": [
        "B Schuller",
        "D Arsic",
        "G Rigoll",
        "M Wimmer",
        "B Radig"
      ],
      "year": "2007",
      "venue": "ICASSP"
    },
    {
      "citation_id": "34",
      "title": "A Database of German Emotional Speech,'' in Interspeech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A Database of German Emotional Speech,'' in Interspeech"
    },
    {
      "citation_id": "35",
      "title": "The Arcade Learning Environment: An Evaluation Platform General Agents",
      "authors": [
        "M Bellemare",
        "Y Naddaf",
        "J Veness",
        "M Bowling"
      ],
      "year": "2013",
      "venue": "Journal of Artificial Intelligence Research"
    },
    {
      "citation_id": "36",
      "title": "EmoRL: Continuous Acoustic Emotion Classification using Deep Reinforcement Learning",
      "authors": [
        "E Lakomkin",
        "M Zamani",
        "C Weber",
        "S Magg",
        "S Wermter"
      ],
      "year": "2018",
      "venue": "EmoRL: Continuous Acoustic Emotion Classification using Deep Reinforcement Learning"
    },
    {
      "citation_id": "37",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "38",
      "title": "Emotion recognition using deep learning approach from audio-visual emotional big data",
      "authors": [
        "M Hossain",
        "G Muhammad"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "39",
      "title": "The eNTERFACE'05 Audio-Visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "ICDEW 2006 -Proceedings of the 22nd International Conference on Data Engineering Workshops",
      "doi": "10.1109/ICDEW.2006.145"
    },
    {
      "citation_id": "40",
      "title": "Adversarial approach to domain adaptation for reinforcement learning on dialog systems",
      "authors": [
        "S Koo",
        "H Yu",
        "G Lee"
      ],
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "41",
      "title": "Meta Reinforcement Learning for Sim-to-real Domain Adaptation",
      "authors": [
        "K Arndt",
        "M Hazara",
        "A Ghadirzadeh",
        "V Kyrki"
      ],
      "venue": "Proceedings -IEEE"
    },
    {
      "citation_id": "42",
      "title": "Cross-Corpus Speech Emotion Recognition Based on Few-Shot Learning and Domain Adaptation",
      "authors": [
        "Y Ahn",
        "S Lee",
        "J Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "43",
      "title": "MSP-IMPROV: An Acted Corpus of Dyadic Interactions to Study Emotion Perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "TC-Net: A Modest & Lightweight Emotion Recognition System Using Temporal Convolution Network",
      "authors": [
        "M Ishaq",
        "M Khan",
        "S Kwon"
      ],
      "year": "2023",
      "venue": "Computer Systems Science and Engineering"
    },
    {
      "citation_id": "45",
      "title": "MSER: Multimodal speech emotion recognition using cross-attention with deep fusion",
      "authors": [
        "M Khan",
        "W Gueaieb",
        "A Saddik",
        "S Kwon"
      ],
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "46",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "ACL 2019 -57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "47",
      "title": "Emotional voice conversion: Theory, databases and ESD",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "48",
      "title": "Detection and Classification of Acoustic Scenes and Events",
      "authors": [
        "S Gharib",
        "K Drossos",
        "C Akir",
        "D Serdyuk",
        "T Virtanen"
      ],
      "year": "2018",
      "venue": "Detection and Classification of Acoustic Scenes and Events"
    },
    {
      "citation_id": "49",
      "title": "Supervised domain adaptation for emotion recognition from speech",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2015",
      "venue": "ICASSP"
    },
    {
      "citation_id": "50",
      "title": "Speaker-Invariant Adversarial Domain Adaptation for Emotion Recognition",
      "authors": [
        "Y Yin",
        "B Huang",
        "Y Wu",
        "M Soleymani"
      ],
      "year": "2020",
      "venue": "ICMI 2020 -Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "51",
      "title": "Unsupervised domain adaptation under label space mismatch for speech classification",
      "authors": [
        "A Mathur",
        "N Berthouze",
        "N Lane"
      ],
      "year": "2020",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH"
    },
    {
      "citation_id": "52",
      "title": "Domain Invariant Feature Learning for Speaker-Independent Speech Emotion Recognition",
      "authors": [
        "C Lu",
        "Y Zong",
        "W Zheng",
        "Y Li",
        "C Tang",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing"
    },
    {
      "citation_id": "53",
      "title": "Unsupervised Adversarial Domain Adaptation for Cross-Lingual Speech Emotion Recognition",
      "authors": [
        "S Latif",
        "J Qadir",
        "M Bilal"
      ],
      "year": "2019",
      "venue": "' 2019 8th International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "54",
      "title": "Unsupervised Domain Adaptation Integrating Transformer and Mutual Information for Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "S Zhang",
        "R Liu",
        "Y Yang",
        "X Zhao",
        "J Yu"
      ],
      "year": "2022",
      "venue": "MM 2022 -Proceedings of the 30th ACM International Conference on Multimedia",
      "doi": "10.1145/3503161.3548328"
    },
    {
      "citation_id": "55",
      "title": "Unsupervised Domain Adaptation for Speech Emotion Recognition using K-Nearest Neighbors Voice Conversion",
      "authors": [
        "P Mote",
        "B Sisman",
        "C Busso"
      ],
      "venue": "Unsupervised Domain Adaptation for Speech Emotion Recognition using K-Nearest Neighbors Voice Conversion"
    },
    {
      "citation_id": "56",
      "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations",
      "authors": [
        "A Nair",
        "B Mcgrew",
        "M Andrychowicz",
        "W Zaremba",
        "P Abbeel"
      ],
      "year": "2018",
      "venue": "Overcoming Exploration in Reinforcement Learning with Demonstrations"
    },
    {
      "citation_id": "57",
      "title": "Transfer Learning in Deep Reinforcement Learning: A Survey",
      "authors": [
        "Z Zhu",
        "K Lin",
        "J Zhou"
      ],
      "year": "2020",
      "venue": "Transfer Learning in Deep Reinforcement Learning: A Survey"
    },
    {
      "citation_id": "58",
      "title": "Pretraining in Deep Reinforcement Learning for Automatic Speech Recognition",
      "authors": [
        "T Rajapakshe",
        "R Rana",
        "S Latif",
        "S Khalifa",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Pretraining in Deep Reinforcement Learning for Automatic Speech Recognition",
      "arxiv": "arXiv:1910.11256"
    },
    {
      "citation_id": "59",
      "title": "Deep Reinforcement Learning with Pre-training for Time-efficient Training of Automatic Speech Recognition",
      "authors": [
        "T Rajapakshe",
        "S Latif",
        "R Rana",
        "S Khalifa",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Deep Reinforcement Learning with Pre-training for Time-efficient Training of Automatic Speech Recognition",
      "arxiv": "arXiv:2005.11172"
    },
    {
      "citation_id": "60",
      "title": "A Novel Policy for Pre-trained Deep Reinforcement Learning for Speech Emotion Recognition",
      "authors": [
        "T Rajapakshe",
        "R Rana",
        "S Khalifa",
        "J Liu",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Australasian Computer Science Week 2022"
    },
    {
      "citation_id": "61",
      "title": "Transferring Generalizable Motor Primitives From Simulation to Real World",
      "authors": [
        "M Hazara",
        "V Kyrki"
      ],
      "year": "2019",
      "venue": "IEEE Robotics and Automation Letters"
    },
    {
      "citation_id": "62",
      "title": "Reinforcement learning model, algorithms and its application",
      "authors": [
        "Q Wang",
        "Z Zhan"
      ],
      "year": "2011",
      "venue": "Proceedings 2011 International Conference on Mechatronic Science, Electric Engineering and Computer"
    },
    {
      "citation_id": "63",
      "title": "Vocal Tract Length Perturbation (VTLP) improves speech recognition",
      "authors": [
        "N Jaitly",
        "E Hinton"
      ],
      "year": "2013",
      "venue": "30th International Conference on Machine Learning"
    },
    {
      "citation_id": "64",
      "title": "DEMAND: a collection of multichannel recordings of acoustic noise in diverse environments",
      "authors": [
        "J Thiemann",
        "N Ito",
        "E Vincent"
      ],
      "year": "2013",
      "venue": "DEMAND: a collection of multichannel recordings of acoustic noise in diverse environments"
    },
    {
      "citation_id": "65",
      "title": "Speech Emotion Recognition Using 3D Convolutions and Attention-Based Sliding Recurrent Networks with Auditory Front-Ends",
      "authors": [
        "Z Peng",
        "X Li",
        "Z Zhu",
        "M Unoki",
        "J Dang",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "66",
      "title": "Speech Emotion Recognition Model with Time-Scale-Invariance MFCCs as Input,'' 2021 International Wireless Communications and Mobile Computing, IWCMC 2021",
      "authors": [
        "X Xie",
        "J Lou",
        "L Zhang"
      ],
      "year": "2021",
      "venue": "Speech Emotion Recognition Model with Time-Scale-Invariance MFCCs as Input,'' 2021 International Wireless Communications and Mobile Computing, IWCMC 2021"
    },
    {
      "citation_id": "67",
      "title": "Domain-Invariant Feature Learning for Cross Corpus Speech Emotion Recognition",
      "authors": [
        "Y Gao",
        "S Okada",
        "L Wang",
        "J Liu",
        "J Dang"
      ],
      "venue": "Domain-Invariant Feature Learning for Cross Corpus Speech Emotion Recognition"
    },
    {
      "citation_id": "68",
      "title": "Speech based human emotion recognition using MFCC",
      "authors": [
        "M Likitha",
        "S Gupta",
        "K Hasitha",
        "A Raju"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 International Conference on Wireless Communications, Signal Processing and Networking"
    },
    {
      "citation_id": "69",
      "title": "Direct Modelling of Speech Emotion from Raw Speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, IN-TERSPEECH"
    },
    {
      "citation_id": "70",
      "title": "Speech Emotion Recognition using MFCC, GFCC, Chromagram and RMSE features",
      "authors": [
        "H Patni",
        "A Jagtap",
        "V Bhoyar",
        "A Gupta"
      ],
      "year": "2021",
      "venue": "Proceedings of the 8th International Conference on Signal Processing and Integrated Networks, SPIN 2021"
    },
    {
      "citation_id": "71",
      "title": "Speech emotion recognition using ANN on MFCC features",
      "authors": [
        "H Dolka",
        "M Xavier",
        "S Juliet"
      ],
      "venue": "' 2021 3rd International Conference on Signal Processing and Communication"
    },
    {
      "citation_id": "72",
      "title": "'librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "73",
      "title": "Deep Representation Learning in Speech Processing: Challenges, Recent Advances, and Future Trends",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Deep Representation Learning in Speech Processing: Challenges, Recent Advances, and Future Trends",
      "arxiv": "arXiv:2001.00378"
    },
    {
      "citation_id": "74",
      "title": "",
      "authors": [
        "F Chollet"
      ],
      "year": "2015",
      "venue": ""
    },
    {
      "citation_id": "75",
      "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems",
      "authors": [
        "M Abadi",
        "A Agarwal",
        "P Barham",
        "E Brevdo",
        "Z Chen",
        "C Citro",
        "G Corrado",
        "A Davis",
        "J Dean",
        "M Devin",
        "S Ghemawat",
        "I Goodfellow",
        "A Harp",
        "G Irving",
        "M Isard",
        "Y Jia",
        "R Jozefowicz",
        "L Kaiser",
        "M Kudlur",
        "J Levenberg",
        "D Mané",
        "R Monga",
        "S Moore",
        "D Murray",
        "C Olah",
        "M Schuster",
        "J Shlens",
        "B Steiner",
        "I Sutskever",
        "K Talwar",
        "P Tucker",
        "V Vanhoucke",
        "V Vasudevan",
        "F Viégas",
        "O Vinyals",
        "P Warden",
        "M Wattenberg",
        "M Wicke",
        "Y Yu",
        "X Zheng"
      ],
      "year": "2015",
      "venue": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems"
    },
    {
      "citation_id": "76",
      "title": "Human-level control through deep reinforcement learning",
      "authors": [
        "V Mnih",
        "K Kavukcuoglu",
        "D Silver",
        "A Rusu",
        "J Veness",
        "M Belle",
        "-A Graves",
        "M Riedmiller",
        "A Fidjeland",
        "G Ostrovski",
        "S Petersen",
        "C Beattie",
        "A Sadik",
        "I Antonoglou",
        "H King",
        "D Kumaran",
        "D Wierstra",
        "S Legg",
        "D Hassabis"
      ],
      "year": "2015",
      "venue": "Nature"
    },
    {
      "citation_id": "77",
      "title": "Reinforcement learning : an introduction",
      "authors": [
        "R Sutton",
        "A Barto"
      ],
      "year": "2018",
      "venue": "Reinforcement learning : an introduction"
    },
    {
      "citation_id": "78",
      "title": "Explorations in Efficient Reinforcement Learning",
      "authors": [
        "M Wiering"
      ],
      "year": "1999",
      "venue": "Explorations in Efficient Reinforcement Learning"
    },
    {
      "citation_id": "79",
      "title": "'wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proceedings of the 34th International Conference on Neural Information Processing Systems",
      "doi": "10.5555/3495724.3496768"
    },
    {
      "citation_id": "80",
      "title": "Comparison of speaker dependent and speaker independent emotion recognition",
      "authors": [
        "J Rybka",
        "A Janicki"
      ],
      "year": "2013",
      "venue": "International Journal of Applied Mathematics and Computer Science"
    },
    {
      "citation_id": "81",
      "title": "Deep Reinforcement Learning: An Overview",
      "authors": [
        "Y Li"
      ],
      "year": "2017",
      "venue": "Deep Reinforcement Learning: An Overview"
    }
  ]
}