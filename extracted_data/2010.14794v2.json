{
  "paper_id": "2010.14794v2",
  "title": "Seen And Unseen Emotional Style Transfer For Voice Conversion With A New Emotional Speech Dataset",
  "published": "2020-10-28T07:16:18Z",
  "authors": [
    "Kun Zhou",
    "Berrak Sisman",
    "Rui Liu",
    "Haizhou Li"
  ],
  "keywords": [
    "emotional voice conversion",
    "speech emotion recognition (SER)",
    "emotional speech dataset"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotional voice conversion aims to transform emotional prosody in speech while preserving the linguistic content and speaker identity. Prior studies show that it is possible to disentangle emotional prosody using an encoder-decoder network conditioned on discrete representation, such as one-hot emotion labels. Such networks learn to remember a fixed set of emotional styles. In this paper, we propose a novel framework based on variational auto-encoding Wasserstein generative adversarial network (VAW-GAN), which makes use of a pre-trained speech emotion recognition (SER) model to transfer emotional style during training and at run-time inference. In this way, the network is able to transfer both seen and unseen emotional style to a new utterance. We show that the proposed framework achieves remarkable performance by consistently outperforming the baseline framework. This paper also marks the release of an emotional speech dataset (ESD) for voice conversion, which has multiple speakers and languages.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech conveys information with words and also through its prosody. Speech prosody can affect the syntactic and semantic interpretation of an utterance (linguistic prosody), and also displays one's emotional state (emotional prosody)  [1] . Emotional prosody reflects the intent, mood and temperament of the speaker and plays an important role in daily communication  [2] . Emotional voice conversion is a voice conversion (VC) technique, which aims to transfer the emotional style of an utterance from one to another. Emotional voice conversion enables various applications such as expressive text-to-speech (TTS)  [3]  and conversational agents.\n\nEmotional voice conversion and speech voice conversion  [4]  differs in many ways. Speech voice conversion aims to change the speaker identity, whereas emotional voice conversion focuses on the emotional state transfer. Traditional VC research includes modelling spectral mapping with statistical methods such as Gaussian mixture model (GMM)  [5] , partial least square regression  [6]  and sparse representation  [7] . Recent deep learning approaches such as deep neural network (DNN)  [8] , recurrent neural network (RNN)  [9]  and generative adversarial network (GAN)  [10]  have advanced the stateof-the-art. We note that these frameworks have motivated the studies in emotional voice conversion.\n\nEarly studies on emotional voice conversion handle both spectrum and prosody conversion with GMM  [11]  and sparse repre-sentation  [12] . Recent deep learning methods, such as deep belief network (DBN)  [13] , deep bi-directional long-short-term memory (DBLSTM)  [14] , sequence-to-sequence  [15]  and rule-based model  [16]  have shown the effectiveness on emotion conversion. We note that these frameworks are relied on the parallel training data. However, such data is not widely available in real-life applications.\n\nThere have been studies on deep learning approaches for emotional voice conversion that do not require parallel training data, such as cycle-consistent adversarial network (CycleGAN)-based  [17, 18]  and autoencoder-based frameworks  [19, 20] . However, they are typically designed for a fixed set of conversion pairs. In this paper, we would like to propose a novel technique, that is referred to as emotional style transfer (EST), that learns to transfer an emotional style to any input utterance. The emotional style is given to the network as a control condition. Therefore, the network supports oneto-many emotion conversion and for unseen emotion at run-time.\n\nAuto-encoder is a suitable computational model that allows for the control of output generation through the latent variables  [21] .\n\nRecent studies  [20]  on disentangling and recomposing the emotional elements in the speech with VAW-GAN represent one of the successful attempts in emotional voice conversion. However, emotional prosody is the result of the interplay of multiple signal attributes, hence it is not easy to define emotional prosody by a simple labeling scheme  [22] . In TTS studies, there are techniques to learn a latent prosody embedding, i.e. style token, from the training data, in order to predict or transfer the speech prosody  [23, 24] . These studies motivate us to investigate the use of deep emotional features that reflect the global speech style and describe the emotional prosody in a continuous space.\n\nIn this paper, we propose an emotional style transfer framework based on VAW-GAN. We use deep emotional features as a condition to enable the encoder-decoder training for seen and unseen emotion generation. Furthermore, we release an EVC dataset that consists of multi-speaker and multi-lingual emotional speech. We aim to tackle the lack of open-source emotional speech data in voice conversion research community. This dataset can be easily applied to other speech synthesis tasks, such as cross-lingual voice conversion and emotional TTS.\n\nThe main contributions of this paper include: 1) we propose to build a one-to-many emotional style transfer framework that does not require parallel training data; 2) we propose to use SER that is pre-trained with publicly available large speech corpus to describe the emotional style; 3) we propose to disentangle and recompose the emotional elements through deep emotional features; and 4) we release a multi-lingual and multi-speaker emotional speech corpus, denoted as ESD, that can be used for various speech synthesis tasks. To our best knowledge, this paper is the first reported study on emotional style transfer for unseen emotion.\n\nThis paper is organized as follows: In Section 2, we motivate our study and analyze the deep emotional features. In Section 3, we introduce the proposed one-to-many EST framework. In Section 4, we report the experiments. Section 5 concludes the study.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Analysis Of Deep Emotional Features",
      "text": "The computational analysis of emotion has been the focus of SER  [25] . Recent advances of deep learning have led to a shift from traditional human-crafted representations of acoustic features, to the deep features automatically learnt by neural networks  [25, 26] . Deep features are data-driven, less dependent on human knowledge and more suitable for emotional style transfer  [25] .\n\nEmotional prosody is prominently exhibited in emotional speech databases, which can be characterized by discrete categories, such as Ekmans's six basic emotions  [27] , and continuous representation, such as Russell's circumplex model  [28] . Recent studies seek to characterize emotions over a continuum rather than a finite set of discrete categorical labels with the feature representation learnt by deep neural networks in a continuous space  [29] . Following the findings in speech analysis, studies are conducted to use emotional prosody modelling to improve the expressiveness of speech synthesis systems  [30, 31, 32] . In  [30] , an emotion recognizer is used to generate the style embedding for speech style transfer. Um et al.  [31]  apply the style embedding to a Tacotron system with the aim to control the intensity of emotional expressiveness. These successful attempts have revealed the fact that deep emotional features serve as the excellent prosody descriptor, which motivates this study.\n\nWe are interested in the use of deep emotional features for voice conversion, to describe emotional prosody in a continuous space. The idea is to use deep emotional features of a reference speech to transfer its emotional style to an output target speech. To motivate the idea, we visualize the deep emotional features of 4 speakers (2 male and 2 female) using the t-SNE algorithm  [33]  in a two-dimensional plane, as shown in Fig.  1 . It is observed that deep emotional features form clear emotion groups in terms of feature distributions. Fig.  1  suggests that we may use deep emotional features as a style embedding to encode an emotion class. Encouraged by this observation, we propose a one-to-many emotional style transfer framework through deep emotional features.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "One-To-Many Emotional Style Transfer",
      "text": "We propose a novel one-to-many emotional style transfer framework, that is based on VAW-GAN  [21]  with its decoder conditioning on deep emotional features. The proposed one-to-many EST framework is referred as DeepEST in short. We next discuss DeepEST in three stages: 1) emotion descriptor training, 2) encoder-decoder training with VAW-GAN, and 3) run-time conversion interface. In Stage I, we train an auxiliary SER network to serve as the emotion descriptor for input utterances. In Stage II, the proposed encoderdecoder training is implemented to learn the disentanglement and recomposition of the emotional elements. In Stage III, DeepEST takes input utterance and target deep emotional features to generate the utterance with target emotional style.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Stage I: Emotion Descriptor Training",
      "text": "Emotional prosody is complex with multiple acoustic attributes which makes it difficult to model. There have been studies to label emotion manually into discrete categories, such as one-hot emotion label  [34, 20] . As emotional prosody naturally spreads over a continuum that is hard to force-fit into a few categories, we propose  to use deep emotional features that are learned from large animated and emotive speech data.\n\nWe propose to use a SER model as an emotion descriptor D, which extracts deep emotional features Φ from the input utterance X, or Φ = D(X). The SER architecture is as the same as that in  [35] , which includes: 1) a three-dimensional (3-D) CNN layer; 2) a BLSTM layer; 3) an attention layer; 4) a fully connected (FC) layer. The 3-D CNN first projects the input Mel-spectrum with its delta and delta-deltas features into a fixed size latent representation, that preserves the effective emotional information while reducing the influence of emotional irrelevant factors. Then the following BLSTM and attention layer summarize the temporal information from the previous layer and produce discriminative utterance-level feature Φ for emotion prediction, as visualized in Fig.  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Stage Ii: Encoder-Decoder Training With Vaw-Gan",
      "text": "Encoder-decoder structure has been used to effectively learn disentangled representations in previous studies  [20, 21] . We propose an encoder-decoder training procedure as shown in Fig.  2 , where the encoder (E) learns to disentangle the emotional elements from the input features and generate a latent representation z. The resulting representation z is assumed to contain phonetic and speaker information, but emotion-independent. Then the decoder/generator (G) learns to reconstruct the input features with the emotionindependent representation z and other controllable emotion-related attributes.\n\nIn practice, we use WORLD vocoder  [36]  to extract spectral features (SP) and fundamental frequency (F0) from the waveform. The encoder (E θ ) with parameter set θ is exposed to the input spectral frames x with different emotion types and learns an emotionindependent representation z: z = E θ (x). Since the latent representation z extracted from the source spectrum still contains the source F0 information, and the conversion performance can suffer from this flaw  [20] . Therefore, the decoder/generator (G ψ ) with parameter set ψ takes emotion-independent representation z and the emotion-related features: the deep emotional features Φ that reflect the global emotional variance of the input utterance X from Stage I and the corresponding F0 that contains source pitch information to recompose the emotional elements of the spectrum. The reconstructed feature x can be formulated as:\n\nWe then train a generative model for spectrum through an adversarial training: The discriminator (Yµ) with parameter set µ tries to maximize the loss between the real features x and reconstructed features x, while the generator (G ψ ) tries to minimize it. The parameter sets θ, ψ and µ are optimized through this min-max game, which allows us to generate high-quality speech samples.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Stage Iii: Run-Time Conversion",
      "text": "During the run-time conversion, we have a source utterance that is expressed in neutral emotion, we would like to convert it to a target emotion following the reference emotion style from the reference utterances. Suppose that we have a set of reference utterances Xt belonging to an emotion category. We first use the pre-trained SER to generate the deep emotional features Φt = mean(D(Xt)) for all the reference utterances, i.e. all the utterances of our dataset with the same reference emotion. We then concatenate Φt with the converted F0 ( F0) and emotion-independent z from the source utterance to compose a latent representation for the target utterance SP. The converted SP can be formulated as:\n\nFinally, the converted speech with target emotion is synthesised by WORLD vocoder with converted spectral features and converted F0.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotional Speech Dataset (Esd)",
      "text": "With this paper, we introduce and publicly release a new multilingual and multi-speaker emotional speech dataset that can be used for various speech synthesis and voice conversion tasks  1  . The dataset consists of 350 parallel utterances with an average duration of 2.9 seconds spoken by 10 native English and 10 native Mandarin speakers. For each language, the dataset consists of 5 male and 5 female speakers in five emotions summarized as follows: 1) happy, 2) sad, 3) neutral, 4) angry, and 5) surprise. Speech data are sampled at 16 kHz and saved in 16 bits.\n\nTo our best knowledge, this is the first parallel voice conversion dataset that provides emotion labels in a multi-lingual and multispeaker setup. As a future work, we will report an in-depth investigation of this dataset for cross-lingual and mono-lingual emotional voice conversion applications.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "We conduct objective and subjective evaluation to assess the performance of our proposed DeepEST model for seen and unseen emotional style transfer. We use four English speakers (2 male and 2 female) from ESD dataset. For each speaker, we conduct seen emotion conversion from neutral to happy (N2H: neutral-tohappy) and neutral to sad (N2S: neutral-to-sad). We choose angry as the unseen emotion, and conduct experiments from neutral to angry (N2A: neutral-to-angry) to assess the performance of our proposed model for unseen emotion style transfer.\n\nWe split the 350 utterances in ESD dataset for a speaker into training set (300 utterances), reference set (30 utterances), and test set (20 utterances). During training, we propose to have one universal model that takes 300 utterances from neutral, happy and sad emotion states respectively. To obtain the deep emotional features, we train the SER  [30, 32]  on a subset of IEMOCAP  [34]  with four emotion types (happy, angry, sad and neutral). At runtime, we evaluate our framework with 20 utterances both from seen (happy, sad) and unseen (angry) emotion states. We obtain the reference emotion style by using the 30 utterances from the reference set, as formulated in Eq. (2). For each emotion conversion, we generate the deep emotional features from SER module by calculating the mean of the features that are in the same emotion category as the emotion reference.\n\nAs the baseline framework, we adapted a state-of-the-art VAW-GAN-EVC  [20]  that can perform conversion from one emotional state to another. We note that for each emotion conversion pair, we need to train a new VAW-GAN-EVC model as it is not capable of performing one-to-many conversion. In contrast, DeepEST provides more flexible manipulation and generation of the output emotion, as it can perform an emotional conversion from one emotional state to In DeepEST, 513-dimensional SP, F0 and APs are extracted every 5 ms with the FFT length of 1024 using the WORLD vocoder. The frame length is 25 ms with a frame shift of 5 ms. We normalize every input frame of SP to unit sum and then re-scale it to logarithm. The encoder is a 5-layer 1D CNN with the kernel size of 7 and a stride of 3 followed by a FC layer. Its output channel is {16,32,64,128,256}. The latent representation z is 128dimensional and assumed to have a Gaussian distribution. The deep emotional feature is 256-dimensional, and concatenated with the 128-dimensional latent representation z and 1-dimensional F0 contour to merge as the input to the decoder. The decoder is 4-layer 1D CNN with kernel size of {9,7,7,1025} and strides of {3,3,3,1}. Its output channel is {32,16,8,1}. The discriminator is a 3-layer 1D CNN with kernel size of {7,7,115} and strides of {3,3,3} followed by a FC layer. The networks are trained by using RMSProp with a learning rate of 1e-5. The batch size is set as 256 for 45 epochs.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Objective Evaluation",
      "text": "We conduct objective evaluation to assess the performance of our proposed model. We calculate Mel-cepstral distortion (MCD)  [7, 4]  to measure the spectral distortion between the converted and reference Mel-spectrum for two male and two female speakers for three emotion combinations.\n\nAs shown in Table  1 , the proposed DeepEST outperforms the baseline framework VAW-GAN-EVC for all the seen emotions (N2H and N2S). We note that for the unseen emotion (angry), DeepEST still achieves comparable results to that of VAW-GAN-EVC baseline, which is trained with angry speech samples. These encouraging observations validate the effectiveness of our proposed DeepEST for seen and unseen emotion transfer. Last but not least, we require three VAW-GAN-EVC models to be trained for all conversion pairs, whereas the proposed DeepEST can perform all the emotion mapping pairs within one model, which we believe is remarkable.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Subjective Evaluation",
      "text": "We further conduct three listening experiments to assess the proposed DeepEST in terms of speech quality and emotion similarity. 15 subjects participated in all the experiments and each listened to 108 converted utterances in total.\n\nWe first report the mean opinion score (MOS) of the reference speech samples, baseline VAW-GAN-EVC and the proposed Deep-EST. As shown in Table  2 , DeepEST achieves better results than the baseline for both seen and unseen emotion combinations, which we believe is remarkable. Secondly, we conduct AB preference test to further evaluate the speech quality, where the subjects are asked to choose the speech samples with higher speech quality. As shown in Fig.  3 , we observe that proposed framework DeepEST consistently outperforms the baseline framework VAW-GAN-EVC for all the emotion combinations, which is also consistent with the MOS results. These observations validates the effectiveness of our proposed model in terms of voice quality.  We further conduct XAB emotion similarity test to assess the emotion conversion performance, where subjects are asked to choose the speech samples which sound closer to the reference in terms of the emotional expression. Consistent with previous experiments, the baseline is one-to-one conversion and all emotions are seen emotions. As shown in Fig.  4 , DeepEST outperforms the baseline for neutral-to-sad conversion. We observe that baseline achieves better performance for neutral-to-happy conversion due to the poor performance of SER on happy (29.95%) compared with 84.32% on sad, and 70.47% on angry. For unseen emotion (angry), DeepEST still achieves comparable results with the baseline in terms of emotion similarity, which is very encouraging. This result validates the effectiveness of DeepEST for unseen emotion transfer. As a future work, we will improve the SER performance for all emotional states, and report an in-depth investigation.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a one-to-many emotional style transfer framework based on VAW-GAN without the need for parallel data. We propose to leverage deep emotional features from SER to describe emotional prosody in a continuous space. By conditioning the decoder with controllable attributes such as deep emotional features and F0 values, we achieve competitive results for both seen and unseen emotions over the baseline framework, which validates the effectiveness of our proposed framework. Moreover, we also introduce a new emotional speech dataset, ESD, that can be used in speech synthesis and voice conversion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgment",
      "text": "",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: It is observed",
      "page": 2
    },
    {
      "caption": "Figure 1: suggests that we may use deep",
      "page": 2
    },
    {
      "caption": "Figure 1: t-SNE plot of deep emotional features for 20 utterances with",
      "page": 2
    },
    {
      "caption": "Figure 1: 3.2. Stage II: Encoder-Decoder Training with VAW-GAN",
      "page": 2
    },
    {
      "caption": "Figure 2: The training phase of the proposed DeepEST framework. Blue boxes represent the networks that involved in the training and red",
      "page": 3
    },
    {
      "caption": "Figure 3: , we observe that proposed framework DeepEST",
      "page": 4
    },
    {
      "caption": "Figure 3: AB preference test results for the speech quality.",
      "page": 4
    },
    {
      "caption": "Figure 4: XAB preference test results for the emotion similarity.",
      "page": 4
    },
    {
      "caption": "Figure 4: , DeepEST outperforms the baseline",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "sentation [12]. Recent deep learning methods, such as deep belief"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "network (DBN)\n[13], deep bi-directional\nlong-short-term memory"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "(DBLSTM) [14], sequence-to-sequence [15] and rule-based model"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "[16] have\nshown the\neffectiveness on emotion conversion.\nWe"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "note that\nthese frameworks are relied on the parallel\ntraining data."
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "However, such data is not widely available in real-life applications."
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "There have been studies on deep learning approaches for emo-"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "tional voice conversion that do not require parallel training data, such"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "as cycle-consistent adversarial network (CycleGAN)-based [17, 18]"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "and autoencoder-based frameworks\n[19, 20].\nHowever,\nthey are"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "typically designed for a ﬁxed set of conversion pairs.\nIn this paper,"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "we would like to propose a novel\ntechnique,\nthat\nis referred to as"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "emotional style transfer (EST),\nthat\nlearns to transfer an emotional"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "style to any input utterance.\nThe emotional\nstyle is given to the"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "network as a control condition. Therefore, the network supports one-"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "to-many emotion conversion and for unseen emotion at run-time."
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "Auto-encoder is a suitable computational model\nthat allows for"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "the control of output generation through the latent variables [21]."
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "Recent studies [20] on disentangling and recomposing the emotional"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "elements in the speech with VAW-GAN represent one of\nthe suc-"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "cessful attempts in emotional voice conversion. However, emotional"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "prosody is the result of\nthe interplay of multiple signal attributes,"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "hence it is not easy to deﬁne emotional prosody by a simple labeling"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "scheme [22].\nIn TTS studies,\nthere are techniques to learn a latent"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "prosody embedding, i.e. style token, from the training data, in order"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "to predict or\ntransfer\nthe speech prosody [23, 24].\nThese studies"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "motivate us to investigate the use of deep emotional\nfeatures that"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "reﬂect the global speech style and describe the emotional prosody in"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "a continuous space."
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "In this paper, we propose an emotional style transfer framework"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "based on VAW-GAN. We use deep emotional features as a condition"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "to enable the encoder-decoder training for seen and unseen emotion"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "generation. Furthermore, we release an EVC dataset that consists of"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "multi-speaker and multi-lingual emotional speech. We aim to tackle"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "the lack of open-source emotional speech data in voice conversion"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "research community.\nThis dataset can be easily applied to other"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "speech synthesis tasks, such as cross-lingual voice conversion and"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "emotional TTS."
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "The main contributions of this paper include: 1) we propose to"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "build a one-to-many emotional style transfer\nframework that does"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "not require parallel\ntraining data; 2) we propose to use SER that\nis"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "pre-trained with publicly available large speech corpus to describe"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "the emotional style; 3) we propose to disentangle and recompose"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "the emotional elements through deep emotional features; and 4) we"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "release a multi-lingual and multi-speaker emotional speech corpus,"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "denoted as ESD, that can be used for various speech synthesis tasks."
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": ""
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "To our best knowledge,\nthis paper\nis\nthe ﬁrst\nreported study on"
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "emotional style transfer for unseen emotion."
        },
        {
          "2 Information Systems Technology and Design, Singapore University of Technology and Design": "This paper\nis organized as follows:\nIn Section 2, we motivate"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Zero Effort\nVAW-GAN-EVC": "neutral-to-happy\n6.769\n4.738",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "4.569\n7.088\n4.284\n4.260"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "neutral-to-sad\n6.306\n4.284",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "4.127\n8.287\n5.464\n4.916"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "neutral-to-angry\n6.649\n4.482",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "4.564 (unseen)\n6.690\n4.204\n4.451 (unseen)"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "The reconstructed feature x can be formulated as:",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "female speakers in ﬁve emotions summarized as follows: 1) happy,"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "2) sad, 3) neutral, 4) angry, and 5) surprise. Speech data are sampled"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "(1)\nx = Gψ(z, Φ, F0) = Gψ(Eθ(x), D(X), F0)",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "at 16 kHz and saved in 16 bits."
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "To our best knowledge, this is the ﬁrst parallel voice conversion"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "We\nthen\ntrain\na\ngenerative model\nfor\nspectrum through\nan",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "dataset\nthat provides emotion labels\nin a multi-lingual and multi-"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "adversarial\ntraining:\nThe\ndiscriminator\nparameter\nset\n(Yµ) with",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "speaker\nsetup.\nAs\na\nfuture work, we will\nreport\nan\nin-depth"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "µ tries\nto maximize\nthe\nloss\nbetween\nthe\nreal\nfeatures x and",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "investigation\nof\nthis\ndataset\nfor\ncross-lingual\nand mono-lingual"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "reconstructed features x, while the generator (Gψ) tries to minimize",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "emotional voice conversion applications."
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "it. The parameter sets θ, ψ and µ are optimized through this min-max",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "game, which allows us to generate high-quality speech samples.",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "4.2. Experimental Setup"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "3.3.\nStage III: Run-time Conversion",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "We conduct objective and subjective evaluation to assess the per-"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "During the run-time conversion, we have a source utterance that\nis",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "formance of our proposed DeepEST model\nfor\nseen and unseen"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "expressed in neutral emotion, we would like to convert it to a target",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "emotional\nstyle\ntransfer. We use\nfour English speakers\n(2 male"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "emotion following the reference emotion style from the reference",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "and 2 female)\nfrom ESD dataset.\nFor each speaker, we conduct"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "seen emotion conversion from neutral\nto happy (N2H: neutral-to-"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "utterances.\nSuppose that we have a set of\nreference utterances Xt",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "belonging to an emotion category. We ﬁrst use the pre-trained SER",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "happy) and neutral to sad (N2S: neutral-to-sad). We choose angry as"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "the unseen emotion, and conduct experiments from neutral to angry"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "to generate the deep emotional\nfeatures Φt = mean(D(Xt)) for",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "all\nthe reference utterances,\ni.e.\nall\nthe utterances of our dataset",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "(N2A: neutral-to-angry) to assess the performance of our proposed"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "model for unseen emotion style transfer."
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "with the same reference emotion. We then concatenate Φt with",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "the converted F0 ( ˆF0) and emotion-independent z from the source",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "We split\nthe 350 utterances in ESD dataset\nfor a speaker\ninto"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "utterance to compose a latent representation for the target utterance",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "training\nset\n(300\nutterances),\nreference\nset\n(30\nutterances),\nand"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "SP. The converted SP can be formulated as:",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "test set\n(20 utterances). During training, we propose to have one"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "universal model\nthat\ntakes\n300\nutterances\nfrom neutral,\nhappy"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "(2)\nx = Gψ(z, Φt, ˆF0) = Gψ(Eθ(x), mean(D(Xt)), ˆF0)",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "and sad emotion states respectively. To obtain the deep emotional"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "features, we train the SER [30, 32] on a subset of IEMOCAP [34]"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "Finally,\nthe converted speech with target emotion is synthesised by",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "with four emotion types (happy, angry,\nsad and neutral). At\nrun-"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "WORLD vocoder with converted spectral features and converted F0.",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "time, we\nevaluate our\nframework with 20 utterances both from"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "seen (happy,\nsad) and unseen (angry) emotion states. We obtain"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "4. EXPERIMENTS",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "the\nreference\nemotion style by using the 30 utterances\nfrom the"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "reference set, as formulated in Eq. (2). For each emotion conversion,"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "4.1. Emotional Speech Dataset (ESD)",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "we\ngenerate\nthe\ndeep\nemotional\nfeatures\nfrom SER module\nby"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "calculating the mean of\nthe features that are in the same emotion"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "With this paper, we introduce and publicly release a new multi-",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "category as the emotion reference."
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "lingual and multi-speaker emotional speech dataset that can be used",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "As the baseline framework, we adapted a state-of-the-art VAW-"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "for\nvarious\nspeech\nsynthesis\nand\nvoice\nconversion\ntasks1.\nThe",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "GAN-EVC [20]\nthat can perform conversion from one emotional"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "dataset consists of 350 parallel utterances with an average duration",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "state to another. We note that for each emotion conversion pair, we"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "of 2.9 seconds spoken by 10 native English and 10 native Mandarin",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "need to train a new VAW-GAN-EVC model as it\nis not capable of"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "speakers.\nFor each language,\nthe dataset consists of 5 male and 5",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": ""
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "performing one-to-many conversion. In contrast, DeepEST provides"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "https://github.com/\n1Emotional\nSpeech\nDataset\n(ESD):",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "more ﬂexible manipulation and generation of the output emotion, as"
        },
        {
          "Zero Effort\nVAW-GAN-EVC": "HLTSingapore/Emotional-Speech-Data",
          "DeepEST\nDeepEST\nZero Effort\nVAW-GAN-EVC": "it can perform an emotional conversion from one emotional state to"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: , the proposed DeepEST outperforms the futurework,wewillimprovetheSERperformanceforallemotional",
      "data": [
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "Fig. 4. XAB preference test results for the emotion similarity."
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "We further conduct XAB emotion similarity test\nto assess the"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "emotion conversion performance, where subjects are asked to choose"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "the speech samples which sound closer to the reference in terms of"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "the emotional expression.\nConsistent with previous experiments,"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "the baseline\nis one-to-one\nconversion and all\nemotions\nare\nseen"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "emotions. As shown in Fig. 4, DeepEST outperforms the baseline"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "for neutral-to-sad conversion. We observe that baseline achieves"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "better performance for neutral-to-happy conversion due to the poor"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "performance of SER on happy (29.95%) compared with 84.32% on"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "sad, and 70.47% on angry. For unseen emotion (angry), DeepEST"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "still\nachieves\ncomparable\nresults with\nthe\nbaseline\nin\nterms\nof"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "emotion similarity, which is very encouraging. This result validates"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "the effectiveness of DeepEST for unseen emotion transfer.\nAs a"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "future work, we will improve the SER performance for all emotional"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "states, and report an in-depth investigation."
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "5. CONCLUSION"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "In this paper, we propose a one-to-many emotional\nstyle transfer"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "framework based on VAW-GAN without\nthe need for parallel data."
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "We\npropose\nto\nleverage\ndeep\nemotional\nfeatures\nfrom SER to"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "describe emotional prosody in a continuous space. By conditioning"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "the decoder with controllable\nattributes\nsuch as deep emotional"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "features and F0 values, we achieve competitive results for both seen"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "and unseen emotions over the baseline framework, which validates"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "the effectiveness of our proposed framework. Moreover, we also"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "introduce a new emotional speech dataset, ESD, that can be used in"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "speech synthesis and voice conversion."
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "6. ACKNOWLEDGMENT"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "The\nresearch is\nsupported by the National Research Foundation,"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "Singapore under\nits AI Singapore Programme (Award No: AISG-"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "GC-2019-002)\nand\n(Award No:\nAISG-100E-2018-006),\nand\nits"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "National Robotics Programme (Grant No.\n192 25 00054), and by"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "RIE2020 Advanced Manufacturing and Engineering Programmatic"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "Grants A1687b0033,\nand A18A2b0046.\nThe research by Berrak"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "Sisman and Rui Liu is funded by SUTD Start-up Grant Artiﬁcial"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "Intelligence for Human Voice Conversion (SRG ISTD 2020 158)"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "and SUTD AI Grant,\ntitled ’The Understanding and Synthesis of"
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": "Expressive Speech by AI’."
        },
        {
          "VAW-GAN-EVC\nNo Difference\nDeepEST (Proposed)": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Proc.\n“Nonparallel emotional speech conversion,”\nInterspeech 2019,"
        },
        {
          "7. REFERENCES": "The handbook of\n[1]\nJulia Hirschberg,\n“Pragmatics\nand intonation,”",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "pp. 2858–2862, 2019."
        },
        {
          "7. REFERENCES": "pragmatics, pp. 515–537, 2004.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[20] Kun Zhou, Berrak Sisman, Mingyang Zhang, and Haizhou Li,\n“Con-"
        },
        {
          "7. REFERENCES": "[2] Magda B Arnold, “Emotion and personality.,” 1960.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "verting Anyone’s Emotion: Towards Speaker-Independent Emotional"
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Voice Conversion,” in Proc. Interspeech 2020, 2020, pp. 3416–3420."
        },
        {
          "7. REFERENCES": "[3] Rui Liu, Berrak Sisman, Jingdong Li, Feilong Bao, Guanglai Gao, and",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[21] Chin-Cheng Hsu, Hsin-Te Hwang, Yi-Chiao Wu, Yu Tsao, and Hsin-"
        },
        {
          "7. REFERENCES": "Haizhou Li,\n“Teacher-student\ntraining for\nrobust\ntacotron-based tts,”",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Min Wang, “Voice conversion from unaligned corpora using variational"
        },
        {
          "7. REFERENCES": "in ICASSP 2020-2020 IEEE International Conference on Acoustics,",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "arXiv\nautoencoding wasserstein\ngenerative\nadversarial\nnetworks,”"
        },
        {
          "7. REFERENCES": "Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6274–6278.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "preprint arXiv:1704.00849, 2017."
        },
        {
          "7. REFERENCES": "[4] Berrak Sisman,\nJunichi Yamagishi,\nSimon King,\nand Haizhou Li,",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[22] Hieu-Thi Luong,\nShinji Takaki, Gustav Eje Henter,\nand\nJunichi"
        },
        {
          "7. REFERENCES": "“An overview of voice conversion and its challenges: From statistical",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Yamagishi,\n“Adapting and controlling dnn-based speech synthesis"
        },
        {
          "7. REFERENCES": "modeling to deep learning,” IEEE/ACM Transactions on Audio, Speech,",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "2017\nIEEE International Conference\non\nusing\ninput\ncodes,”\nin"
        },
        {
          "7. REFERENCES": "and Language Processing, 2020.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2017, pp."
        },
        {
          "7. REFERENCES": "[5]\nTomoki Toda, Alan W Black, and Keiichi Tokuda,\n“Voice conversion",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "4905–4909."
        },
        {
          "7. REFERENCES": "based\non maximum-likelihood\nestimation\nof\nspectral\nparameter",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[23] Yuxuan Wang, Daisy\nStanton, Yu Zhang,\nRJ-Skerry Ryan,\nEric"
        },
        {
          "7. REFERENCES": "IEEE Transactions\non Audio,\nSpeech,\nand Language\ntrajectory,”",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Battenberg, Joel Shor, Ying Xiao, Ye Jia, Fei Ren, and Rif A Saurous,"
        },
        {
          "7. REFERENCES": "Processing, vol. 15, no. 8, pp. 2222–2235, 2007.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "“Style tokens: Unsupervised style modeling, control and transfer\nin"
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "end-to-end speech synthesis,” in International Conference on Machine"
        },
        {
          "7. REFERENCES": "[6]\nElina Helander,\nTuomas Virtanen,\nJani Nurminen,\nand Moncef",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Learning, 2018, pp. 5180–5189."
        },
        {
          "7. REFERENCES": "Gabbouj,\n“Voice conversion using partial\nleast\nsquares\nregression,”",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "IEEE Transactions on Audio, Speech, and Language Processing, 2010.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[24] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy"
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Stanton,\nJoel\nShor, Ron Weiss, Rob Clark,\nand Rif A Saurous,"
        },
        {
          "7. REFERENCES": "[7] Berrak Sisman, Mingyang Zhang,\nand Haizhou Li,\n“Group sparse",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "“Towards end-to-end prosody transfer for expressive speech synthesis"
        },
        {
          "7. REFERENCES": "representation with wavenet\nvocoder\nadaptation\nfor\nspectrum and",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "with tacotron,”\nin International Conference on Machine Learning,"
        },
        {
          "7. REFERENCES": "IEEE/ACM Transactions on Audio, Speech, and\nprosody conversion,”",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "2018, pp. 4693–4702."
        },
        {
          "7. REFERENCES": "Language Processing, vol. 27, no. 6, pp. 1085–1097, 2019.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[25] Dagmar M Schuller and Bj¨orn W Schuller,\n“A review on ﬁve recent"
        },
        {
          "7. REFERENCES": "[8]\nLing-Hui Chen, Zhen-Hua Ling, Li-Juan Liu, and Li-Rong Dai, “Voice",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "and near-future developments in computational processing of emotion"
        },
        {
          "7. REFERENCES": "conversion\nusing\ndeep\nneural\nnetworks with\nlayer-wise\ngenerative",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "in the human voice,” Emotion Review, p. 1754073919898526, 2020."
        },
        {
          "7. REFERENCES": "IEEE/ACM Transactions on Audio, Speech, and Language\ntraining,”",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[26]\nSiddique Latif, Rajib Rana, Sara Khalifa, Raja Jurdak, Junaid Qadir,"
        },
        {
          "7. REFERENCES": "Processing, vol. 22, no. 12, pp. 1859–1872, 2014.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "and Bj¨orn W Schuller,\n“Deep\nrepresentation\nlearning\nin\nspeech"
        },
        {
          "7. REFERENCES": "[9]\nToru Nakashika, Tetsuya Takiguchi,\nand Yasuo Ariki,\n“High-order",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "arXiv\nprocessing: Challenges,\nrecent advances, and future trends,”"
        },
        {
          "7. REFERENCES": "sequence modeling using speaker-dependent\nrecurrent\ntemporal\nre-",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "preprint arXiv:2001.00378, 2020."
        },
        {
          "7. REFERENCES": "stricted boltzmann machines for voice conversion,” in Fifteenth annual",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[27]\nPaul Ekman, “An argument for basic emotions,” Cognition & emotion,"
        },
        {
          "7. REFERENCES": "conference\nof\nthe\ninternational\nspeech\ncommunication\nassociation,",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "1992."
        },
        {
          "7. REFERENCES": "2014.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Journal\nof\n[28]\nJames A Russell,\n“A circumplex model\nof\naffect.,”"
        },
        {
          "7. REFERENCES": "[10] Berrak Sisman, Mingyang Zhang, Minghui Dong, and Haizhou Li, “On",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "personality and social psychology, vol. 39, no. 6, pp. 1161, 1980."
        },
        {
          "7. REFERENCES": "the study of generative adversarial networks\nfor cross-lingual voice",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[29]\nEesung Kim and Jong Won Shin,\n“Dnn-based emotion recognition"
        },
        {
          "7. REFERENCES": "conversion,” IEEE ASRU, 2019.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "based on bottleneck acoustic features and lexical features,” in ICASSP"
        },
        {
          "7. REFERENCES": "[11]\nJianhua Tao, Yongguo Kang, and Aijun Li, “Prosody conversion from",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "2019-2019 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "7. REFERENCES": "IEEE Transactions on Audio,\nneutral\nspeech to emotional\nspeech,”",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Signal Processing (ICASSP). IEEE, 2019, pp. 6720–6724."
        },
        {
          "7. REFERENCES": "Speech, and Language Processing, 2006.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[30] Yang Gao, Weiyi Zheng,\nZhaojun Yang,\nThilo Kohler, Christian"
        },
        {
          "7. REFERENCES": "[12] Ryo Aihara,\nReina Ueda,\nTetsuya\nTakiguchi,\nand Yasuo Ariki,",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Fuegen, and Qing He,\n“Interactive text-to-speech via semi-supervised"
        },
        {
          "7. REFERENCES": "“Exemplar-based\nemotional\nvoice\nconversion\nusing\nnon-negative",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "style transfer learning,” arXiv preprint arXiv:2002.06758, 2020."
        },
        {
          "7. REFERENCES": "matrix factorization,” in APSIPA ASC. IEEE, 2014.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[31]\nSe-Yun Um, Sangshin Oh, Kyungguen Byun, Inseon Jang, ChungHyun"
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Ahn,\nand Hong-Goo Kang,\n“Emotional\nspeech synthesis with rich"
        },
        {
          "7. REFERENCES": "[13]\nZhaojie Luo, Tetsuya Takiguchi, and Yasuo Ariki,\n“Emotional voice",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "and granularized control,”\nin ICASSP 2020-2020 IEEE International"
        },
        {
          "7. REFERENCES": "conversion using deep neural networks with mcc and f0 features,”\nin",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Conference on Acoustics,\nSpeech and Signal Processing (ICASSP)."
        },
        {
          "7. REFERENCES": "2016 IEEE/ACIS 15th ICIS, 2016.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "IEEE, 2020, pp. 7254–7258."
        },
        {
          "7. REFERENCES": "[14] Huaiping Ming, Dongyan Huang, Lei Xie, Jie Wu, Minghui Dong, and",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[32] Rui Liu, Berrak Sisman, Guanglai Gao, and Haizhou Li,\n“Expressive"
        },
        {
          "7. REFERENCES": "Haizhou Li, “Deep bidirectional lstm modeling of timbre and prosody",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "arXiv preprint\ntts training with frame and style reconstruction loss,”"
        },
        {
          "7. REFERENCES": "for emotional voice conversion,”\nInterspeech 2016, pp. 2453–2457,",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "arXiv:2008.01490, 2020."
        },
        {
          "7. REFERENCES": "2016.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[33]\nLaurens van der Maaten and Geoffrey Hinton, “Visualizing data using"
        },
        {
          "7. REFERENCES": "[15] Carl Robinson, Nicolas Obin,\nand Axel Roebel,\n“Sequence-to-",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "t-sne,” Journal of machine learning research, vol. 9, no. Nov, pp. 2579–"
        },
        {
          "7. REFERENCES": "sequence modelling of f0 for speech emotion conversion,”\nin ICASSP",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "2605, 2008."
        },
        {
          "7. REFERENCES": "2019-2019 IEEE International Conference on Acoustics, Speech and",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[34] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe Kazemzadeh,"
        },
        {
          "7. REFERENCES": "Signal Processing (ICASSP). IEEE, 2019, pp. 6830–6834.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Emily Mower, Samuel Kim,\nJeannette N Chang, Sungbok Lee, and"
        },
        {
          "7. REFERENCES": "[16] Yawen Xue, Yasuhiro Hamada, and Masato Akagi,\n“Voice conversion",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "Shrikanth S Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic"
        },
        {
          "7. REFERENCES": "for emotional\nspeech: Rule-based synthesis with degree of emotion",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "motion capture database,”\nLanguage resources and evaluation, vol."
        },
        {
          "7. REFERENCES": "controllable in dimensional space,” Speech Communication, 2018.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "42, no. 4, pp. 335, 2008."
        },
        {
          "7. REFERENCES": "[17] Kun Zhou, Berrak Sisman, and Haizhou Li,\n“Transforming Spectrum",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[35] Mingyi Chen,\nXuanji He,\nJing Yang,\nand Han\nZhang,\n“3-d"
        },
        {
          "7. REFERENCES": "and\nProsody\nfor\nEmotional Voice Conversion with Non-Parallel",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "convolutional\nrecurrent\nneural\nnetworks with\nattention model\nfor"
        },
        {
          "7. REFERENCES": "Training Data,”\nin Proc. Odyssey 2020 The Speaker and Language",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "speech emotion recognition,” IEEE Signal Processing Letters, vol. 25,"
        },
        {
          "7. REFERENCES": "Recognition Workshop, 2020, pp. 230–237.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "no. 10, pp. 1440–1444, 2018."
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "[36] Masanori Morise,\nFumiya Yokomori,\nand Kenji Ozawa,\n“World:"
        },
        {
          "7. REFERENCES": "[18] Ravi Shankar, Jacob Sager, and Archana Venkataraman, “Non-parallel",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "a vocoder-based high-quality speech synthesis\nsystem for\nreal-time"
        },
        {
          "7. REFERENCES": "emotion conversion using a deep-generative hybrid network and an",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "applications,”\nIEICE TRANSACTIONS on Information and Systems,"
        },
        {
          "7. REFERENCES": "arXiv\npreprint\nadversarial\npair\ndiscriminator,”\narXiv:2007.12932,",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        },
        {
          "7. REFERENCES": "",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": "vol. 99, no. 7, pp. 1877–1884, 2016."
        },
        {
          "7. REFERENCES": "2020.",
          "[19]\nJian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan Olaleye,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "The handbook of pragmatics",
      "authors": [
        "Julia Hirschberg"
      ],
      "year": "2004",
      "venue": "The handbook of pragmatics"
    },
    {
      "citation_id": "3",
      "title": "Emotion and personality",
      "authors": [
        "Magda B Arnold"
      ],
      "year": "1960",
      "venue": "Emotion and personality"
    },
    {
      "citation_id": "4",
      "title": "Teacher-student training for robust tacotron-based tts",
      "authors": [
        "Rui Liu",
        "Berrak Sisman",
        "Jingdong Li",
        "Feilong Bao",
        "Guanglai Gao",
        "Haizhou Li"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "An overview of voice conversion and its challenges: From statistical modeling to deep learning",
      "authors": [
        "Berrak Sisman",
        "Junichi Yamagishi",
        "Simon King",
        "Haizhou Li"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Voice conversion based on maximum-likelihood estimation of spectral parameter trajectory",
      "authors": [
        "Tomoki Toda",
        "Alan Black",
        "Keiichi Tokuda"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Voice conversion using partial least squares regression",
      "authors": [
        "Elina Helander",
        "Tuomas Virtanen",
        "Jani Nurminen",
        "Moncef Gabbouj"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Group sparse representation with wavenet vocoder adaptation for spectrum and prosody conversion",
      "authors": [
        "Berrak Sisman",
        "Mingyang Zhang",
        "Haizhou Li"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Voice conversion using deep neural networks with layer-wise generative training",
      "authors": [
        "Ling-Hui Chen",
        "Zhen-Hua Ling",
        "Li-Juan Liu",
        "Li-Rong Dai"
      ],
      "year": "2014",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "High-order sequence modeling using speaker-dependent recurrent temporal restricted boltzmann machines for voice conversion",
      "authors": [
        "Toru Nakashika",
        "Tetsuya Takiguchi",
        "Yasuo Ariki"
      ],
      "year": "2014",
      "venue": "Fifteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "11",
      "title": "On the study of generative adversarial networks for cross-lingual voice conversion",
      "authors": [
        "Berrak Sisman",
        "Mingyang Zhang",
        "Minghui Dong",
        "Haizhou Li"
      ],
      "year": "2019",
      "venue": "IEEE ASRU"
    },
    {
      "citation_id": "12",
      "title": "Prosody conversion from neutral speech to emotional speech",
      "authors": [
        "Jianhua Tao",
        "Yongguo Kang",
        "Aijun Li"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Exemplar-based emotional voice conversion using non-negative matrix factorization",
      "authors": [
        "Ryo Aihara",
        "Reina Ueda",
        "Tetsuya Takiguchi",
        "Yasuo Ariki"
      ],
      "year": "2014",
      "venue": "APSIPA ASC"
    },
    {
      "citation_id": "14",
      "title": "Emotional voice conversion using deep neural networks with mcc and f0 features",
      "authors": [
        "Zhaojie Luo",
        "Tetsuya Takiguchi",
        "Yasuo Ariki"
      ],
      "year": "2016",
      "venue": "2016 IEEE/ACIS 15th ICIS"
    },
    {
      "citation_id": "15",
      "title": "Deep bidirectional lstm modeling of timbre and prosody for emotional voice conversion",
      "authors": [
        "Huaiping Ming",
        "Dongyan Huang",
        "Lei Xie",
        "Jie Wu",
        "Minghui Dong",
        "Haizhou Li"
      ],
      "year": "2016",
      "venue": "Deep bidirectional lstm modeling of timbre and prosody for emotional voice conversion"
    },
    {
      "citation_id": "16",
      "title": "Sequence-tosequence modelling of f0 for speech emotion conversion",
      "authors": [
        "Carl Robinson",
        "Nicolas Obin",
        "Axel Roebel"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Voice conversion for emotional speech: Rule-based synthesis with degree of emotion controllable in dimensional space",
      "authors": [
        "Yawen Xue",
        "Yasuhiro Hamada",
        "Masato Akagi"
      ],
      "year": "2018",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "18",
      "title": "Transforming Spectrum and Prosody for Emotional Voice Conversion with Non-Parallel Training Data",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Haizhou Li"
      ],
      "year": "2020",
      "venue": "Proc. Odyssey 2020 The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "19",
      "title": "Non-parallel emotion conversion using a deep-generative hybrid network and an adversarial pair discriminator",
      "authors": [
        "Ravi Shankar",
        "Jacob Sager",
        "Archana Venkataraman"
      ],
      "year": "2020",
      "venue": "Non-parallel emotion conversion using a deep-generative hybrid network and an adversarial pair discriminator",
      "arxiv": "arXiv:2007.12932"
    },
    {
      "citation_id": "20",
      "title": "Nonparallel emotional speech conversion",
      "authors": [
        "Jian Gao",
        "Deep Chakraborty",
        "Hamidou Tembine",
        "Olaitan Olaleye"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "21",
      "title": "Converting Anyone's Emotion: Towards Speaker-Independent Emotional Voice Conversion",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Mingyang Zhang",
        "Haizhou Li"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Voice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks",
      "authors": [
        "Chin-Cheng Hsu",
        "Hsin-Te Hwang",
        "Yi-Chiao Wu",
        "Yu Tsao",
        "Hsin-Min Wang"
      ],
      "year": "2017",
      "venue": "Voice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks",
      "arxiv": "arXiv:1704.00849"
    },
    {
      "citation_id": "23",
      "title": "Adapting and controlling dnn-based speech synthesis using input codes",
      "authors": [
        "Hieu-Thi Luong",
        "Shinji Takaki",
        "Gustav Henter",
        "Junichi Yamagishi"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
      "authors": [
        "Yuxuan Wang",
        "Daisy Stanton",
        "Yu Zhang",
        "Rj-Skerry Ryan",
        "Eric Battenberg",
        "Joel Shor",
        "Ying Xiao",
        "Ye Jia",
        "Fei Ren",
        "Rif Saurous"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "25",
      "title": "Towards end-to-end prosody transfer for expressive speech synthesis with tacotron",
      "authors": [
        "Eric Skerry-Ryan",
        "Ying Battenberg",
        "Yuxuan Xiao",
        "Daisy Wang",
        "Joel Stanton",
        "Ron Shor",
        "Rob Weiss",
        "Rif Clark",
        "Saurous"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "26",
      "title": "A review on five recent and near-future developments in computational processing of emotion in the human voice",
      "authors": [
        "M Dagmar",
        "Björn Schuller",
        "Schuller"
      ],
      "year": "2020",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "27",
      "title": "Deep representation learning in speech processing: Challenges, recent advances, and future trends",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Junaid Qadir",
        "Björn Schuller"
      ],
      "year": "2020",
      "venue": "Deep representation learning in speech processing: Challenges, recent advances, and future trends",
      "arxiv": "arXiv:2001.00378"
    },
    {
      "citation_id": "28",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "29",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "30",
      "title": "Dnn-based emotion recognition based on bottleneck acoustic features and lexical features",
      "authors": [
        "Eesung Kim",
        "Jong Shin"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Interactive text-to-speech via semi-supervised style transfer learning",
      "authors": [
        "Yang Gao",
        "Weiyi Zheng",
        "Zhaojun Yang",
        "Thilo Kohler",
        "Christian Fuegen",
        "Qing He"
      ],
      "year": "2020",
      "venue": "Interactive text-to-speech via semi-supervised style transfer learning",
      "arxiv": "arXiv:2002.06758"
    },
    {
      "citation_id": "32",
      "title": "Emotional speech synthesis with rich and granularized control",
      "authors": [
        "Se-Yun Um",
        "Sangshin Oh",
        "Kyungguen Byun",
        "Inseon Jang",
        "Chunghyun Ahn",
        "Hong-Goo Kang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "Expressive tts training with frame and style reconstruction loss",
      "authors": [
        "Rui Liu",
        "Berrak Sisman",
        "Guanglai Gao",
        "Haizhou Li"
      ],
      "year": "2020",
      "venue": "Expressive tts training with frame and style reconstruction loss",
      "arxiv": "arXiv:2008.01490"
    },
    {
      "citation_id": "34",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "35",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "36",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "Mingyi Chen",
        "Xuanji He",
        "Jing Yang",
        "Han Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "37",
      "title": "World: a vocoder-based high-quality speech synthesis system for real-time applications",
      "authors": [
        "Masanori Morise",
        "Fumiya Yokomori",
        "Kenji Ozawa"
      ],
      "year": "2016",
      "venue": "IEICE TRANSACTIONS on Information and Systems"
    }
  ]
}