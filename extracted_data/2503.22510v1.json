{
  "paper_id": "2503.22510v1",
  "title": "Automated Ux Insights From User Research Videos By Integrating Facial Emotion And Text Sentiment",
  "published": "2025-03-28T15:14:08Z",
  "authors": [
    "Simran Kaur Ghatoray",
    "Yongmin Li"
  ],
  "keywords": [
    "user experience",
    "facial emotion recognition",
    "speech-to-text",
    "text-based emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition technology has been studied from the past decade. With its growing importance and applications such as customer service, medical, education, etc., this research study aims to explore its potential and importance in the field of User experience evaluation. Recognizing and keeping track of user emotions in user research video is important to understand user needs and expectations from a service/product. Little research has been done that focuses on automating emotion extraction from a video where more than one modality has been incorporated in the field of UX. The study aims at implementing different modalities such as facial emotion recognition, speech-to-text and text-based emotion recognition for capturing emotional nuances from a user research video and extract meaningful actionable insights. For selection of facial emotion recognition model, 10 pre-trained models were evaluated on three benchmark datasets i.e. FER-2013, AffectNet and CK+, selecting the model with most generalization ability. To extract speech and convert to text, OpenAI's Whisper model was implemented and finally the emotions from text were recognized using a pre-trained model available at HuggingFace website having an evaluation accuracy more than 95%. The study also integrates the gathered data using temporal alignment and fusion for deeper and contextual insights. The study further demonstrates a way of automating data analysis through PandasAI Python library where OpenAI's GPT-4o model was implemented along with a discussion on other possible solutions. This study is an attempt to demonstrate a proof of concept where automated meaningful insights are extracted from a video based on user emotions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "In today's digital world, user emotions play a vital role in developing user-centric designs. The emotional state of a person can affect concen-tration, decision-making skills, and task-solving skills  [21] . The growing importance of recognizing emotions can be seen in different fields such as healthcare, medical, customer service, human-computer interactions, education, gaming, etc. Among its vast applications, in recent years, focus has been made towards User experience and its evaluation methods.\n\nUser experience (UX) is a growing field that includes disciplines such as User Interface Design. Better user design leads to greater user satisfaction following good revenue and retention  [8] . UX evaluation is carried out to ensure better user satisfaction and meet user requirements through various methods and techniques. Usability and UX are intertwined terms, but serve different purposes. UX methods focus on improving user satisfaction by achieving pragmatic and hedonic goals, while usability methods aim to improve human performance  [8] . With the advancement in the field of Artificial Intelligence, there always remains a question of how AI methods and techniques can improve UX evaluation?\n\nDifferent techniques, methods and modalities, now, make it possible to capture the whole picture of user emotions, allowing to change the traditional ways of UX evaluation. User emotions being a crucial part for the betterment of systems, can be detected and recognized with the help of machine learning and deep learning methods. Various modalities such as facial emotion recognition, speech emotion recognition, text based emotion recognition allow one to automate the process of analyzing user emotions be it in an image or video, revolutionizing the ways of user feedback analysis.\n\nResearchers have made efforts to implement AI-driven solutions to make UX evaluation easy and efficient  [8]   [3] . The literature on improving individual elements such as facial recognition in UX by  [2] , emotion detection using EEG signals  [15] , eye tracking and tracking of mouse clicks, etc. represented by  [8]   [33] [48] could be found. Some work in building a framework for UX evaluation by  [12] [7] can be seen.\n\nEmotions are complex, and it might not be possible to grasp the intentions and feelings of the user through a single modality of recognizing emotions. Integration of data gathered through the combination of different modalities such as face, speech, etc. provide deeper and meaningful insights. It also helps to capture any anomalies detected in the video. As demonstrated by  [7] , the integration of facial and speech emotion recognition methods can be a powerful tool to improve UX.\n\nAmong all the applications of emotion recognition technology, little research and solutions have been provided focusing on user emotions in an end-to-end workflow. The practice of manually analyzing the change and evolution of user emotions while reviewing a service/product in a recorded session still remains. UX designers re-watch the whole session number of times to keep track of emotion nuances, limiting the insights available to them. This study, focusing on capturing user emotion nuances in user research videos through combination of different modalities, demonstrates a concept, one way of solving the problem of collecting meaningful insights from a video, making the end-to-end process of UX evaluation automated.\n\nThe project aims to extract meaningful and actionable information from user research videos, focusing on the use of facial, speech-to-text, and text-based emotion recognition modalities for data gathering and in-tegration to capture nuances in user emotions expressed in the video. Following the effective emotion recognition and data integration process, the project aims to provide one of the possible ways to automate the data processing step, to extract and generate insights, through the implementation of a conversational analyst tool.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Literature Review",
      "text": "Emotion Recognition (ER) has been studied for more than a decade. Various methods have been introduced to recognize emotions. Some of which include recognizing emotions from face, speech, tone, text, physiological signals, body language, etc. In  [38] , extensive research is conducted on various techniques for recognizing emotions through artificial intelligence (AI) from the past decade. The paper covers four modalities for ER, namely face, text, audio and physiological signals, where the experiments have shown that majority of the work has been done in facial, followed by textual and audio emotion recognition. Knowing these technologies and their better use could be an efficient way to handle complex problems on healthcare, media, customer service, and especially Human-Computer Interaction (HCI).\n\nAmong all the applications, its importance in User experience (UX) research and usability testing needs to be emphasized. UX evaluation (User Testing) is a process to gain insight into user satisfaction with using / reviewing a product or service. It analyzes how well the product has met the expectations of the user. Their valuable response helps in identifying strengths, weakness, and areas of improvement. According to  [48] , the user experience combines the physical and technical aspects of the product with the cognitive processes of the user, focusing on the emotional impacts and satisfaction during the interaction with the product, while usability tests usually focus on the performance of tasks such as the execution time of a task, the number of clicks, etc.  [16] . Various methods have been discovered to evaluate the UX process. In  [48] , the author states that UX is usually measured quantitatively on objective data at its core or qualitatively, where usability testing is considered at its core. The author of  [16]  has reported nearly 96 methods for UX evaluation through comprehensive research, emphasizing the difference between usability testing and UX evaluation. According to the author, the relationship between UX and usability testing is intertwined, but objective usability testing is not a sufficient measure for subjective UX evaluation, as it focuses on how the user feels about a system/service.\n\nMany applications of AI technology in the field of UX have focused on usability testing where they keep track of eye, keyboard input, number of mouse clicks, etc. Work has been done in objective evaluation of UX with very little focus on subjective part. In  [8] , a framework is presented for UX evaluation focusing on various tracking techniques such as eye and mouse tracking, keyboard inputs, self-assessment questionnaire to categorize users in terms of performance profile.  [33]  proposed an eye tracking technology to objectively evaluate UX for smartphone APPs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Models And Theories",
      "text": "Understanding emotion models and theories helps contextualize implementation. For decades, psychologists and researchers have proposed different theories and models related to human emotion. In  [31] , model of a lay theory of emotions explains how the observer, called an agent by the lay theory, infers about the target of reasoning. The presence of emotional stimuli and the interaction of these stimuli with other mental states such as goals, generate emotions within the agent. External manifestations of these emotions include speech, body language, facial expressions, and future actions. In  [28] , the emotion theories are categorized into two different views as manifestation and structure.\n\nThe author states that emotional reactions can arise from either cognitive judgment or bodily responses when focusing on the manifestation point of view, whereas the structural point of view follows discrete and continuous approaches. The discrete approach as a consideration of universally recognized basic emotions (such as fear, anger, disgust, happiness, sadness, and surprise) is described in  [13] . The continuous approach considers two or more dimensions that describe different emotions, as can be seen in the circumplex model of affect proposed by  [36] . The model distributes the emotions in a two-dimensional circular space containing arousal and valence. The vertical/horizontal axes represent the arousal/valence, where the center of the circle represents the medium level of arousal and the neutral level of valence. In addition to this, six basic emotions on the border of Russell's proposed model are added in  [14] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Emotion Recognition Modalities",
      "text": "A facial recognition (FR) system automatically detects and identifies human faces from a digital image or video frames from a video source  [22, 40, 24, 25] . Researchers approach the problem of facial recognition system in different ways. In  [20] , the researcher presents state of the art of existing facial recognition techniques through three different approaches such as local -uses features with partially defined face such as Local Binary pattern (LBP), holistic -uses features which describes complete face as a model such as Principal Component Analysis (PCA), Eigenfaces, Support Vector Machine (SVM), Convolutional Neural Network (CNN) and hybrid -combination of local and holistic  [27, 23, 26] , whereas in  [39] , the researcher divides the solution to the FR problem into two categories of non-deep learning methods-Eigenface, Fisherface, SVM, LBP and deep learning (DL) methods -Multi-layer Perceptron (MLP) and CNN.\n\nSimilar to author's approach in  [39] , researchers in  [5] ,  [19] ,  [29]  also categorize facial emotion recognition (FER) methods into the conventional and DL approach. The general steps involved in developing a FER model includes data pre-processing, feature extraction, classification and finally results and validation. Three types of data pre-processing steps, ie, gray scale conversion, face detection, and dimensionality reduction, are presented by  [5] . Whether a conventional approach or DL is being used, it is usually decided by the methods used for feature extraction and classifi-cation. Researchers have different opinions about which approach works best. According to  [20] , local feature techniques are better in terms of complexity, rotation and accuracy, whereas in  [39]  experimentation concludes that DL methods are more promising for facial recognition reporting an accuracy of 94.67% of low-high complexity for DL methods and 90.6% of low complexity for non-DL methods.\n\nMoving forward to other emotion recognition technologies for speech and text, researchers have made an impressive contribution. A Speech Emotion Recognition (SER) system extracts and classifies the existing emotions of the target speaker from a pre-processed speech signal. A comprehensive survey on various SER methods is demonstrated in  [45] . SER systems also follow the same workflow of data pre-processing, feature extraction, classification, and evaluation. Different ways of speech processing such as framing, windowing, normalization, noise reduction, etc. Different classifiers such as Artificial Neural Networks (ANN), KNN, SVM, deep neural networks (DNN), recurrent neural networks (RNN), CNN, etc. are broadly discussed in  [45] . Another technology that could be used is speech-to-text (STT), which recognizes speech from an audio or video and converts it into text.\n\nDifferent methods for STT and text-to-speech (TTS) have been reviewed in  [30] . The basic process discussed in the paper for STT involves feature extraction, word matching using acoustic word models, sentence matching using syntax and semantics and finally language modelling to text.\n\nRecognizing emotions from text is often classed as Sentiment Analysis (SA), however, if the textual classification is done on more than just classifying it into positive, negative and neutral, could provide more depth and context to the text. In  [1] , three ways to approach text-based emotion detection as rule construction, ML and hybrid where rule-based approach uses the grammatical and logical rules to find emotions from a document, the ML approach uses ML algorithms to classify text into emotions and hybrid approach is the combination of both are presented.\n\nIn recent years, with advances in the DL methods, most of the research has been done using convolutional neural networks in a varied field of applications. A fine-tuned VGGNet architecture, in  [18] , based on CNN is used to achieve the state-of-the-art single network precision of 73. 28% on the FER-2013 dataset.  [6] 's researcher uses SVM, CNN and pre-trained VGG-16 models to recognize emotions from facial expression along with age and gender prediction. The author of  [32]  explores the potential of CNN-based emotion recognition in marketing research for advertising and gains insight into consumer behavior. In  [46] , CNN-based technology is incorporated into e-learning platforms by comparing five different algorithms (VGG16, VGG19, RESNET50V2, EfficientNETB0 and EfficientB7) on their accuracy to find the best-suited algorithm. However, among all these CNN applications, a recent trend of Vision Transformers (ViT) has emerged for computer vision tasks.\n\nThe potential of ViT-based models is explored in  [4]  for FER on three datasets; RAF-DB, FER2013 and a clean, augmented and balanced dataset using images from the FER dataset. They conducted a comprehensive evaluation of 13 ViT models on these datasets and concluded a promising success of these models for FER tasks. A ViT method introduced in  [17] , to recognize driver expressions by performing data augmentation based on parallel imaging framework with the StarGAN network on CK + and KMU-FED data sets, achieved a higher recognition rate than CNN and ResNet18.\n\nFollowing the trend and being inspired by  [17] , this paper explores a different ViT based model in the application of user testing to enhance the user experience. Text-based emotion detection would provide more contextual understanding by providing context to emotions. A combination of STT and Text-based emotion detection would provide deeper and nuanced emotional insights than directly using the SER which only focuses on the tone of the speaker to recognize emotions.",
      "page_start": 4,
      "page_end": 6
    },
    {
      "section_name": "Emotion Recognition In User Testing",
      "text": "While reviewing different applications of emotion recognition, it was observed that little has been done in the field of user testing. Emotion recognition would not only provide deeper meaningful insights by capturing verbal and non-verbal emotions but also help in improving design decisions leading to better product performance. Limited literature was found in which the problem of manually keeping track of user emotions, be it facial or speech, has been addressed. The author in  [9]  also agrees with the fact that little has been done in the evaluation of UX through automated emotion recognition. The author presents a systematic review on implementation of FER in the UX evaluation thus concluding the need to attend the lacking standardization and modernization of tools, procedures, and evaluation criteria for emotion recognition in UX.\n\nSome efforts have been made to decrease the gap. In  [35] , a hybrid multimodal emotion recognition framework was introduced for UX evaluation, achieving an average accuracy of 98. 19% on detecting four emotions; happiness, neutral, sadness, and anger. A UX evaluation model using an automated tool to calculate the user experience rating of a digital product / service in positive, negative, and neutral points called UXAPP has been implemented and validated in  [7] . The author used state-of-the-art emotion recognition modalities to carry out implementation and conducted experiments to generate a report through nine individuals who carried out the designated task and finally compared the results with the generated report of UXAPP.\n\nEven after implementing different modalities to automate the process of manually keeping track of user emotions, it would not make sense for UX designers to handle gathered data manually. In order to automate the data analysis/insight generation process and being inspired by  [7] , rather than generating a positive, negative and neutral point system, a way to make the user ask questions directly from the gathered data rather than manually analyzing was explored in this project.\n\nThe concept of a whole process from evaluating the video to gathering insights and analyzing the gathered data through automation has been demonstrated using this project.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conceptual Framework",
      "text": "The following architecture has been devised to address the stated problem (Figure  1 ). The architecture was divided into 3 stages: The data from both files are then integrated into a final data file. This is further fed to the AI chatbot through the conversational analyst tool for user interaction with the data.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Stage 3",
      "text": "The user can interact with the chatbot for any queries related to the video provided, reducing the manual labor of re-watching the video for emotion tracking. The integration of facial and speech expression data would also enhance the chances of finding discrepancies and anomalies in the video to get in-depth insights.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Facial Emotion Recognition",
      "text": "Facial emotion recognition, or FER, is used to detect and recognize faces in an image or video and classify them into basic emotions that give in depth insight into the person's feelings through facial expressions. The authors in  [39] , through experiments and comparisons, demonstrates the higher accuracy of DL methods. In this project, the DL methods were implemented to gather insights from user research video. MTCNN network was used to detect faces and the Vision Transformer (ViT) model was used for facial emotion recognition. The out-performance of the MTCNN architecture from other state-of-the-art CNN methods was demonstrated in  [47] , and according to  [11] , ViT is relatively inexpensive to pre-train and matches or exceeds the state-of-the-art on many image classification datasets. Thus, leading to the decision of using these method and model. The general workflow for facial emotion recognition can be seen in Figure  2 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Speech-To-Text And Text-Based Emotion Detection",
      "text": "A machine learning technology that allows computers to understand, analyze, manipulate, interpret, and comprehend human language is known as Natural Language Processing (NLP). In day-to-day life corporations and business gather a lot of textual data such as emails, surveys, and speech data such as voice calls which are needed to be processed and analyzed to understand the intent or sentiment in the message and respond in real time to human communication. NLP software makes it easy to handle this more conveniently and efficiently. The technologies used in this phase of the project fall under NLP tasks. Speech-to-Text is a technology used to recognize and transcribe spoken language in an audio, video, or from a dictation to text through computational linguistics. It is also known as Automatic Speech Recognition (ASR). Its applications range from home applications to industrial such as media, banking, and medical.\n\nText-based emotion detection involves identifying and analyzing the underlying emotions in a text. It can also be categorized as sentiment analysis where the main focus is unraveling any kind of emotions or sentiments expressed in a given text. Most of its applications are seen in the fields of healthcare, customer support, marketing, and HCI. In this project, both technologies are used to unravel the underlying emotions expressed by the speech of the participant while reviewing a product or web design during user testing. The overall framework is illustrated in Figure  3 . For ASR, the Whisper model would be implemented. This technology will help recognize the speech of the participant and transcribe it. In  [44] , the author compared the use of two prominent technologies for ASR at the moment, namely Wav2vec2.0 and Whisper, for forensic scenarios, and concluded that the latter was more accurate in most cases, especially in uncontrolled acoustic conditions. The researchers evaluated both models in corpora of seven Indo-European languages, additionally inventing their own in-domain corpus using different open source datasets from the community research with an aim of testing the models in more realistic and operative conditions. Considering the good results and dynamic capabilities of Whisper, it was chosen for ASR implementation.\n\nThe researchers in  [34]   As the transformer doesn't know about the order of the input sequence, a Sinusoidal Positional Encoding is added to the input before it is fed to the Encoder.\n\nThe BERT (Bidirectional Encoder Representations from Transformers) model was introduced by  [10]  to pre-train deep bidirectional representations from unlabeled text. As the name of the model suggests, its architecture is Transformer-based, specifically using only the Encoder part. The model has a bidirectional nature, meaning that rather than only reading the text from left-to-right or right-to-left, it reads the sequence of words at once considering the context from both directions. In  [10] , the model is developed based on the original implementation by  [43] . A detailed description of the model is provided in  [10] .",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Data Integration And Insight Generation",
      "text": "Data integration is a crucial step in this project. Analyzing integrated data from different modalities such as facial expressions and speech emotion detection would allow to have a comprehensive and insightful analysis of user research videos. A single model may not be sufficient to grasp the emotions of a person. As discussed in emotion theories and models, emotions can be expressed in different ways such as voice, body language, face, etc., and a combination of different modalities would help to capture the whole picture of emotions expressed in the video. Other benefit of joining the data is that it would help find any anomalies or patterns such as instances where the facial expressions is happy, but the speech suggests sarcasm or say otherwise, or instances where they align perfectly. This would provide more accurate interpretation and reliability of the gathered data.\n\nSometimes data generated in one modality is prone to errors, missing or misleading data, and the integration process would help to mitigate these errors by providing additional context and validation. For example, while detecting emotions from one of the user research videos (which was later not used in experiments), the FER model was not able to detect any face and reported 80% of the data as empty, that is, No face detected, while speech emotion detection went smoothly and classified the data into emotions. In such scenarios, even though little face data were available, the speech data helped in retaining the emotions expressed by the participant, which was better than nothing.\n\nIn addition, data integration would help analyze emotions in context, leading to more meaningful insights. For example, a combination of sad expression with a negative speech tone would suggest a deeper level of dissatisfaction from the participant.\n\nIn terms of ethics and responsibility, the combination of different data sources would respect the complexity of human emotions by reducing misinterpretations, and from a practical perspective, data integration would reduce the complexity of interpreting and handling multiple streams of data separately, allowing more streamlined data processing. In this project, data integration was implemented keeping in mind the benefits it offers.\n\nFor data integration, two concepts of Temporal alignment and Temporal fusion are used. Temporal alignment refers to synchronizing data from different data sources based on their time-related attributes, whereas Temporal Fusion refers to combining different time-aligned data from different sources into a single representation to obtain a more comprehensive view of the data.\n\nWith the help of AI, it is now possible to interact with the data directly without the need to learn SQL queries or hard coding, making the data analysis process automated. In this project, one of the possible solutions, a Python library called PandasAI, was used, which allows users to interact with the data in a natural language way, also known as a conversational data analyst tool. The link can be found at  [41] .\n\nThe tool allows to use different Large Language Models (LLMs) such as default BambooLLM, OpenAI models, Google PaLM, Google Vertexai, Azure OpenAI, HuggingFace via Text Generation, Amazon Bedrock models, IBM watsonx.ai models, and local models such as Ollama and LM Studio. Incorporation of such technologies in the application of User Testing would allow designers to evaluate the user research videos in less time and in more efficient way.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Case Study",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Facial Emotion Recognition",
      "text": "In this study, a pre-trained ViT model was used for facial emotion recognition. In order to choose a suitable model from 10 pre-trained working ViT models available on website called HuggingFace, the models were evaluated on the three benchmark datasets i.e. FER-2013, AffectNet and CK+48.\n\nThe FER-2013 dataset contains 28709 training and 5404 testing 48x48 pixels grayscale images of 7 emotions namely angry, disgust, fear, happy, sad, surprise and neutral. The AffectNet dataset contains high dimensional coloured 37553 training, 3200 testing and 800 validation images of 8 emotions namely angry, disgust, fear, sadness, happy, surprise, contempt and neutral. The CK+48 dataset contains 981 (48x48 pixels grayscale) images of 7 emotions namely angry, disgust, happy, sadness, fear, surprise and contempt.\n\nThe model selection was done by evaluating the available models on the three datasets on the basis of accuracy, F1-score, precision and recall. The performance of models on different datasets can be seen in Observing the performances on different datasets, Model 1 (trpakov/vitface-expression) was chosen for Facial Emotion Recognition process proving to have more generalization ability and therefore can be used in different scenarios. The specifications of the model can be found at  [42] .\n\nThe user testing videos for the demonstration of the concept were chosen from YouTube. The 2 chosen videos were labeled as Testing video1 and Testing video2 for anonymity. Usability testing videos are usually large and, due to hardware limitations, the frame rate of video file was decreased from 30 fps to 15 fps for Testing video1.mp4 along with some video quality compressions. The Testing video1 was also cut short to 9 mins and 32 seconds, removing the introduction and 1 task performed by the participant, from 14 mins and 39 seconds video.\n\nThe input video is fed as video frames to the MTCNN architecture. MTCNN group model detects the face and if a face is found, coordinates of the bounding box drawn on the face are recorded, and the face is cropped from the video frame (each frame can be seen as an image). This cropped face is preprocessed using an auto-feature extractor and fed to the pre-trained ViT model. The model recognizes the faces in each frame and classifies them as basic emotions as defined in the model configuration. To get the probabilities of each emotion in each frame, a softmax loss function is applied to the output logits. 'Id2label' attributes of the pre-trained ViT model are retrieved from the configuration and class labels (emotions defined in the model) are mapped to their respective probabilities. The output data generated is then saved in a CSV file for later use.\n\nA function was created taking face image and emotion probabilities as input to generate combined images consisting of two parts as face image and emotion probabilities bar plot, with the original face on the left and the bar plot on the right.\n\nThe total duration of the generated video was 9 min and 16 sec. All data generated by facial emotion recognition was stored in a CSV file called 'emotion probabilities.csv'.\n\nIn Figure  4 , it can be seen that the participant mainly felt 'sad' for a duration of 100 seconds (400-500 secs in the graph). There are some instances of 'happy' and 'angry' emotions but overall, the participant had a 'neutral' face while performing the assigned tasks. These little changes in the emotions while reviewing provide an important information to the UX designers as they can then check at what time the participant was happy or unhappy while performing tasks.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Speech-To-Text And Text-Based Emotion Detection",
      "text": "In this implementation phase, the video was loaded using the 'VideoFile-Clip()' function of the 'moviepy.editor' package. The audio was extracted and written in the '.wav' extension. After extracting the audio, the speech recognizer was initialized. The base version of the whisper model was loaded transcribing the recognized speech in the audio file. The balance between performance and computational efficiency is provided by the 'base' version of the model. Since the resultant text was timestamped word-by-word and segmented, it was concatenated to form the complete transcribed text and then tokenized into sentences. This step was necessary for later analysis in the Data Integration part.\n\nA pre trained classifier and tokenizer model called 'bhadresh-savani/bertbase-go-emotion' from HuggingFace was used for text classification into emotions. In  [37] , the authors evaluated the model attaining an accuracy of 96.14% with 0.116 evaluation loss and 0.12 as training loss. The model was trained on 169208 instances, using 3 epochs with 16 as batch size and 31728 optimization steps. A function was created that took the segments and the classifier as input and classified each segment of text within the list of segments using the given classifier. The results included a list of dictionaries in which each dictionary contained the original text, start and end time of the sentence, emotion class of the text, and its confidence score. Table  2  illustrates the resultant classification of recognized text from audio into emotions. The resultant data was saved in a CSV file named 'speech probabilities.csv'.\n\nFigure  5  illustrates that the participant's statement had different emotions over the course of the video. Between 369 and 391 seconds, the emotion of 'confusion' was quite dominant, indicating the participant's discomfort in completing the task during that period. Comparing the results from facial emotion recognition, between these time periods, the user also had sad expressions, thus providing deeper insights.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Text",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Data Integration And Insight Generation",
      "text": "Two types of data have been collected:\n\n1. Facial Emotion data",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Speech Emotion Data",
      "text": "Facial emotion data was collected from emotions detected by facial expression of the participant. The data was saved in a CSV file called 'emotion probabilities.csv'. Speech Emotion Data was gathered using the combination of STT and Text-based emotion detection. The 'speech probabilities.csv' file was created to save the data for speech emotion detection. There were no modifications done as the file was clean and ready to use. The file contains five columns, namely 'text' -containing the participant's statements, 'start'the start time (in seconds) of statement made by the participant, 'end'the end time (in seconds) of statement made by the participant, 'emotion' -the detected emotion of the text and 'confidence' -the confidence score of the detected emotion.\n\nData cleaning and preparation: There were some instances where the model could not detect any face and returned NULL values. To handle these, they were converted to numeric 0 and not deleted as they were considered as important information over the time period. To obtain the dominant facial emotion in each frame, the emotion with the highest probability score was used. For data without a record, the dominant emotion was converted to the value 'No face detected'. After all transformations, the data was saved in another CSV file called 'non zero dfFER.csv'.\n\nThe data file contains three columns, that is, Timestamp -time (in seconds) at which the emotion was recorded, 'Highest Score' -the confidence score of the detected emotion and 'Facial Emotion' -the dominant emotion at the time of detection.\n\nAfter data collection, cleaning, and preparations, the 'non zero dfFER.csv' and 'speech probabilities.csv' files were used for data integration. Implementation was done in two steps, i.e. Temporal alignment and Temporal Fusion.\n\nTemporal alignment was implemented in the implementation stages of collecting facial emotion data and speech emotion data. The 'Timestamp' variable was added to the facial emotion data and the 'start' and 'end' variable in the speech emotion data. To make sure that the data was aligned according to the time, the variables were converted to the H-M-S (hours-minutes-seconds) format from the using the datetime() function from the Pandas library before integration. This step was taken to accurately compare and combine the two different data sources.\n\nTemporal Fusion was done by aggregating or fusing the facial emotion data based on time intervals of the speech emotion data. Simply explaining, if the participant's statement initiated at 2 second of recorded time and ended at 4 seconds, the number of frames present in that time interval were aggregated to represent the most frequently detected emotion within that same interval of time.\n\nIn addition, the fusion was done considering two scenarios. For the number of frames in the specified time interval, if the emotions detected in all the frames are same, then the average confidence score for facial emotion data at that time would be 'mean' of all the data points representing the same emotion. Secondly, if the emotions detected in the facial emotion data are not same within the specified time interval, the most frequent emotion is extracted and the average of confidence score for only those particular frames would be considered as the final score for the detected emotion. For example, if within time interval of 2 seconds, 30 frames were captured, and for all 30 frames the emotion detected was 'neutral', then the Facial Emotion for that time would be 'neutral' with the confidence score as the average of Highest Score of all 30 frames. And if among 30 frames, 20 are detected as 'happy' and 10 as 'neutral', the Facial Emotion for that time would be 'happy' as it is the most frequent one with confidence score as the average of Highest Score of only those 20 frames.\n\nFurthermore, if the facial emotion data were not in alignment with the speech emotion data, logic would return the 'None' value.\n\nThe implementation was done in Python version 3.12.3 in VS Code. Temporal alignment was performed using the datetime() function. A 'for' loop was initiated by iterating through each row of the speech emotion data. After finding the facial emotion data that fall within the current time interval of the speech emotion data, temporal fusion was implemented. The resultant integrated data was saved in 'Test1.csv' file. A snippet of the final integrated data file is shown in Table  3 . The resultant data integrated \"Test1.csv\" file.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Insight Generation",
      "text": "Among all the available options, Open AI's GPT-4o was used to evaluate the data collected by asking queries from the 'Test1.csv' file. In the query session with the LLM, the following questions were asked.\n\n1. What was the most dominant facial emotion and how many times was it detected?\n\n2. At what time did the user feel the most happy? Can you provide the statement and the confidence score?\n\n3. How many times has the user felt confusion/frustration in the video, and can you list the corresponding data? 4. Was there a time where no face was detected in the video?\n\n5. Can you find anomalies in speech and facial emotions?\n\n6. If I were to map the speech emotions into facial emotions, were there any anomalies where speech emotions did not match facial emotions?\n\n7. Can you find where the user recommended any improvement or would like to have something different from what was presented?\n\n8. This data is based on user testing, as part of user experience study, can you tell me the key places where the experience can be improved?\n\nTo get an overview of the emotions expressed by the participant/user, there is always a question about the dominant emotion detected. When asked the same question, it responded as shown in Table  4  Q1. The 'neutral' emotion detected as the dominant facial emotion also aligns with previous findings that show a positive result of using the LLM as a conversational data analyst tool.\n\nKnowing when the user felt happy helps to capture the strengths of the product. In Testing video1, there were instances detected where the user was happy. Due to data integration, it is now possible to extract What was the most dominant facial emotion and how many times was it detected?\n\n'type': 'string', 'value': \"The most dominant facial emotion is 'neutral' and it was detected 63 times.\"\n\n2 At what time did the user feel the most happy? Can you provide the statement and the confidence score?\n\n'type': 'string', 'value': \"The user showed the most happy face at 330.08 seconds with the statement: ' Returning to the one I posted, it sounds pretty awesome.' and confidence score: 0.899553418.\" insights about the timing and part of the video when this happened. In Q2 in Table  4 , it can be seen that the user was happy at 330.08 seconds in the video with the statement and the confidence score of the text. Confusion or frustration are emotions that depicts when a person is not able to understand the task or felt annoying. Insights into these emotions help to find areas of improvement for UX designers. The response in Table  4  Q3 illustrates instances in which the user was unable to perform tasks conveniently.\n\nStatements made by the user such as \"I'm not really sure how to find.\", \"I clicked it and then I wasn't really sure exactly how to filter three results.\" are meaningful insights for a UX designer that would allow them to track down the time when the task was being performed and which task was being performed that confused the user. From the above figure, it can also be observed that the facial emotion is sad and the speech emotion is confusion, align with each other, making a strong impression of the statement and providing an example of benefits of the data integration step performed earlier.\n\nSometimes, it is possible that data might be lost or not recorded, the simple question asked about whether there were instances when no facial expressions were recorded was made to investigate the same problem. In Table  4  Q4, it can be seen that there are some instances where the face was not detected in the video, but since there is speech emotion recorded, it might not affect the insight gathering process to a greater extent, thus again demonstrating integration of data from different modalities as a wise method for insight gathering.\n\nOne of the objectives of the project was to find anomalies through the implementation of different modalities for deeper insight extractions. LLM was able to find instances where the facial and speech emotions did not align as shown in Table  4  Q5 response. A total of 100 anomalies were found.\n\nTo further refine the response, a question was asked to which the LLM provided more detailed answer by displaying a generated column called 'mapped fer emotion' as can be seen in Table  4  Q6. The LLM was able to manipulate the data itself and generate the desired response.\n\nDetecting user speech has a very important benefit for the UX designer. Any suggestions, feedback and advice help to improve the service. Designers had to ask for feedback or conduct surveys after user testing. Keeping this in mind, the next question about instances in which user recommendations were requested and the response can be seen in Table  4  Q7.\n\nHowever, the suggestion made by the user as seen in index 101 is displayed, but with other irrelevant information, shows limitations in handling complex questions.\n\nThrough the final question in Table  4  Q8, it was observed that the current LLM is capable of generating responses to questions in which any data in the file are not explicitly mentioned.\n\nAccording to the findings, the designer should focus on areas or duration when the user was confused or annoyed. A timeline visualization is also presented, illustrating the key highlights and insights found through the implementation of automating user emotion detection from Testing video1.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Conclusion",
      "text": "Recalling the purpose and goal of the project, the project aimed in getting emotional insights from a user research video. To achieve this aim, the project was done following SMART objectives. Firstly, the importance of emotions in a person's life and recognizing other people's emotions were discussed. Different emotion theories and models were explored to get a grasp of what emotions are and what are the mechanics behind it. The need to recognize emotions could be found in various applications such as healthcare, medical, customer services and HCI.\n\nWith the help available technology, it is now possible to automate traditional ways of working which were time-consuming and tedious. In the field of HCI, user testing or UX evaluation plays an important role in gathering insights for fulfilling the user's needs and expectations. A UX designer needs feedback and suggestions from its customers to improve the service and meet expectations. One way of doing this is through user testing in which a task is assigned to a participant/user, and the user talks about the strengths and weaknesses he/she experienced while reviewing the product in a recorded session. The role of UX designer is to gather insights from the user testing video and keep track of user's changing emotions, key areas where the experience need to be improved, user's recommendations made while reviewing, etc. Traditionally, to meet all these requirements, designers had to re-watch the whole video again and again to keep track and note everything. And after performing this time-consuming and tedious task, they had to perform data analysis for their key findings. With the help of AI, and the solution presented in this project , it was made possible to automate the evaluation process of extracting insights from a video.\n\nInsights were gathered from different emotion recognition modalities such as facial emotion recognition, speech-to-text and text-based emotion recognition. For FER process, a pre-trained ViT based model trained on FER-2013 dataset was implemented which was chosen on the basis of quantitative as well as qualitative analysis. Among 10 available pretrained ViT models, the selected model performed well on three facial recognition benchmark datasets i.e. FER-2013, CK+48 and AffectNet, displaying better generalization ability compared to other models. For speech-to-text conversion, Open AI's Whisper model was chosen and for text-based emotion recognition process a pre-trained BERT based model trained on GoEmotions dataset from Google with evaluation accuracy of 96.14% was implemented.\n\nThe FER model recognized emotions into 7 basic emotions of sad, happy, neutral, angry, surprise, disgust and fear whereas the text-based ED model recognized 27 different emotions. For better understanding, the data integration step was implemented. Not only this was necessary for gathering richer data for analysis, but it was also equally important for UX evaluation. Data integration made it possible to extract deeper meaningful insights by combining data from different modalities. This step also helped finding any anomalies or patterns depicted in user behaviour.\n\nData integration was done using Temporal alignment and Temporal Fusion methods in which the data from different modalities was aligned based on timestamps and this temporally aligned data was then fused based on time. Finally, the integrated data was fed to a conversational data analyst tool i.e. PandasAI, which allows incorporating different LLMs, through which the designer could interact with the data in a natural language way without the need of learning SQL queries. This would reduce the burden of manually processing the data and make the process of UX evaluation time-efficient and convenient. Among available options, Open AI's GPT-4o was used for data analysis process in which the designer could directly ask questions about the user testing video. In this process, the tool provided answers focused on the video data provided. User testing is a confidential process among organizations due to sensitive nature of emotional data. Directly uploading a video on any general purpose LLM would invade privacy and integrity of the organization/company, therefore, a more secure way of data processing would be beneficial.\n\nIn addition, general-purpose LLMs generate generic answers. In this project, the use of PandasAI enabled the incorporated LLM to focus only on the data provided and uncover specific trends and actionable insights that a general-purpose LLM might not provide. The integration of data enables correlation with facial expression and spoken content, which might be difficult to achieve with a general purpose LLM.\n\nBy integrating facial and speech data, the project contributes to the research of multimodal analysis, offering a comprehensive approach to understanding user emotions. Incorporating a conversational data analyst tool, the project showcases a novel application of AI in UX research and evaluation, contributing to methodological advancements in the field.\n\nThe gained insights on user emotions directly inform and improve UX design. Automating emotion detection and data processing reduces the need for manual review, making user testing more scalable and efficient for UX designers. In addition, to the researcher's knowledge, very limited applications were found where a user can extract any sort of insight from a video. This project is a step forward in its application in the field of UX.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Future Developments And Scope",
      "text": "More accurate and efficient emotion recognition models are required for better application. Among all the challenges and limitations, a successful application of emotion recognition and data processing technology was implemented in the field of UX. Future work would include better models where different speakers could be recognized and insights could be gathered accordingly. Real-time emotion detection and analysis could be implemented to get immediate feedback on UX improvements. In addition to face and speech emotion recognition, physiological sensors could provide more deeper insight. The proposed workflow could be implemented in other fields of applications such as education, healthcare, and marketing to extract insights from a video. An application could be implemented for a better demonstration of the project.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Ethical Considerations",
      "text": "To prevent unfair and inaccurate detection of user emotions in FER model, the models were evaluated on three different dataset representing different age groups and gender. The model with the most generalizability was chosen for implementation. Detecting user emotions from facial and speech expressions is sensitive information; therefore, only publicly available videos were used to demonstrate the concept to prevent any breach of privacy and consent.",
      "page_start": 23,
      "page_end": 23
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). The architecture was divided into 3 stages:",
      "page": 7
    },
    {
      "caption": "Figure 1: Project architecture.",
      "page": 7
    },
    {
      "caption": "Figure 2: Figure 2: FER model workflow",
      "page": 8
    },
    {
      "caption": "Figure 3: Figure 3: Overall framework of STT and Text-based Emotion detection.",
      "page": 9
    },
    {
      "caption": "Figure 4: , it can be seen that the participant mainly felt sad for",
      "page": 12
    },
    {
      "caption": "Figure 4: Normalized values of all emotions detected at each frame in",
      "page": 12
    },
    {
      "caption": "Figure 5: illustrates that the participants statement had different emo-",
      "page": 13
    },
    {
      "caption": "Figure 5: Emotions detected from participants speech over time from",
      "page": 15
    },
    {
      "caption": "Figure 6: Timeline visualization of Testing video1.",
      "page": 21
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Performance evaluation of models on different datasets. Model",
      "data": [
        {
          "Model No": "",
          "Performance Evaluation on Datasets": "FER-2013"
        },
        {
          "Model No": "",
          "Performance Evaluation on Datasets": "Accuracy"
        },
        {
          "Model No": "1",
          "Performance Evaluation on Datasets": "0.7115"
        },
        {
          "Model No": "2",
          "Performance Evaluation on Datasets": "0.3536"
        },
        {
          "Model No": "3",
          "Performance Evaluation on Datasets": "0.0906"
        },
        {
          "Model No": "4",
          "Performance Evaluation on Datasets": "0.1106"
        },
        {
          "Model No": "5",
          "Performance Evaluation on Datasets": "0.1209"
        },
        {
          "Model No": "6",
          "Performance Evaluation on Datasets": "0.1123"
        },
        {
          "Model No": "7",
          "Performance Evaluation on Datasets": "0.0741"
        },
        {
          "Model No": "8",
          "Performance Evaluation on Datasets": "0.0911"
        },
        {
          "Model No": "9",
          "Performance Evaluation on Datasets": "0.1180"
        },
        {
          "Model No": "10",
          "Performance Evaluation on Datasets": "0.0984"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 2: Text classification into emotions from recognized speech of",
      "data": [
        {
          "Text": "Well be going to be posting\nthat next part of the task.",
          "Start": "166.92",
          "End": "170.52",
          "Emotion": "neutral",
          "Confidence": "0.3332"
        },
        {
          "Text": "And Im just going to kind of\nget us back to the homepage\nso we can restart.",
          "Start": "171.8",
          "End": "176.72",
          "Emotion": "neutral",
          "Confidence": "0.2554"
        },
        {
          "Text": "Okay.",
          "Start": "179.0",
          "End": "179.22",
          "Emotion": "caring",
          "Confidence": "0.4461"
        },
        {
          "Text": "So next task.",
          "Start": "179.8",
          "End": "181.2",
          "Emotion": "neutral",
          "Confidence": "0.9552"
        },
        {
          "Text": "Youre\ngoing\ncamping\nthis\nweekend but you dont have\na tent.",
          "Start": "182.34",
          "End": "185.62",
          "Emotion": "neutral",
          "Confidence": "0.2950"
        },
        {
          "Text": "You want\nto find and rent a\ntwo-person tent to use.",
          "Start": "186.02",
          "End": "189.0",
          "Emotion": "neutral",
          "Confidence": "0.7209"
        },
        {
          "Text": "Use Surfboard Board to\nac-\ncomplish this.",
          "Start": "189.52",
          "End": "191.42",
          "Emotion": "neutral",
          "Confidence": "0.8308"
        },
        {
          "Text": "Okay.",
          "Start": "191.42",
          "End": "192.68",
          "Emotion": "caring",
          "Confidence": "0.4461"
        },
        {
          "Text": "So Im going to get here this\ntime.",
          "Start": "193.62",
          "End": "196.42",
          "Emotion": "neutral",
          "Confidence": "0.3660"
        },
        {
          "Text": "And Im looking for a tent.",
          "Start": "197.32",
          "End": "199.64",
          "Emotion": "neutral",
          "Confidence": "0.7322"
        },
        {
          "Text": "I want a two-person tent.",
          "Start": "201.74",
          "End": "205.16",
          "Emotion": "desire",
          "Confidence": "0.4894"
        },
        {
          "Text": "Im not really sure if I should\nclick the menu looking thing\nor take an icon.",
          "Start": "207.0",
          "End": "214.26",
          "Emotion": "confusion",
          "Confidence": "0.4445"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 3: The resultant data integrated Test1.csv file.",
      "data": [
        {
          "Text": "So Im going to print it out.",
          "Start": "2.92",
          "End": "5.96",
          "Emotion": "Neutral",
          "Confidence": "0.326197386",
          "Avg FER Score": "0.815838384",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "So Im going to click List Your Gear.",
          "Start": "6.44",
          "End": "9.24",
          "Emotion": "Neutral",
          "Confidence": "0.321020454",
          "Avg FER Score": "0.834265302",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "And that Own Attent.",
          "Start": "13.82",
          "End": "15.86",
          "Emotion": "Neutral",
          "Confidence": "0.661488712",
          "Avg FER Score": "0.736815128",
          "Dominant FER Emotion": "Angry"
        },
        {
          "Text": "So Im going to click Tent.",
          "Start": "16.3",
          "End": "17.94",
          "Emotion": "Neutral",
          "Confidence": "0.61031723",
          "Avg FER Score": "0.745623811",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "Give your listing a descriptive title.",
          "Start": "22",
          "End": "25",
          "Emotion": "Neutral",
          "Confidence": "0.385479093",
          "Avg FER Score": "0.807047625",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "And Im going to click the check box listed.",
          "Start": "25",
          "End": "32.22",
          "Emotion": "Neutral",
          "Confidence": "0.433583885",
          "Avg FER Score": "0.77720179",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "Yeah, this is the Tent.",
          "Start": "47",
          "End": "48.3",
          "Emotion": "Neutral",
          "Confidence": "0.461419493",
          "Avg FER Score": "0.994635472",
          "Dominant FER Emotion": "Happy"
        },
        {
          "Text": "Sorry.",
          "Start": "49",
          "End": "49.2",
          "Emotion": "Remorse",
          "Confidence": "0.399663627",
          "Avg FER Score": "0.995449468",
          "Dominant FER Emotion": "Happy"
        },
        {
          "Text": "Now Im going to write a description about my Tent.",
          "Start": "49.2",
          "End": "55.12",
          "Emotion": "Neutral",
          "Confidence": "0.69373101",
          "Avg FER Score": "0.955230098",
          "Dominant FER Emotion": "Happy"
        },
        {
          "Text": "So this here.",
          "Start": "56.5",
          "End": "59.78",
          "Emotion": "Neutral",
          "Confidence": "0.839199185",
          "Avg FER Score": "0.809589095",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "And this is about three people.",
          "Start": "62.52",
          "End": "68.36",
          "Emotion": "Neutral",
          "Confidence": "0.952671409",
          "Avg FER Score": "0.870423921",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "Two, and if you move a lot in your sleep,\nits pretty light.",
          "Start": "68.36",
          "End": "82.34",
          "Emotion": "Caring",
          "Confidence": "0.28432548",
          "Avg FER Score": "0.86390207",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "Okay, Im going to type that correctly.",
          "Start": "84.12",
          "End": "87.24",
          "Emotion": "Neutral",
          "Confidence": "0.349999487",
          "Avg FER Score": "0.783816478",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "Cool.",
          "Start": "88.86",
          "End": "89.98",
          "Emotion": "Admiration",
          "Confidence": "0.361270905",
          "Avg FER Score": "0.577821005",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "And Im going to set my price.",
          "Start": "90.32",
          "End": "92.08",
          "Emotion": "Neutral",
          "Confidence": "0.450031757",
          "Avg FER Score": "0.580378833",
          "Dominant FER Emotion": "Angry"
        },
        {
          "Text": "Lets see.",
          "Start": "94.56",
          "End": "95.42",
          "Emotion": "Neutral",
          "Confidence": "0.903746367",
          "Avg FER Score": "0.521529078",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "Probably just put it up for like $5 per day.",
          "Start": "97.06",
          "End": "101.04",
          "Emotion": "Neutral",
          "Confidence": "0.663080513",
          "Avg FER Score": "0.725880786",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "Its like $100 Tent.",
          "Start": "101.42",
          "End": "102.7",
          "Emotion": "Neutral",
          "Confidence": "0.824869871",
          "Avg FER Score": "0.676134035",
          "Dominant FER Emotion": "Sad"
        },
        {
          "Text": "That would be pretty cool.",
          "Start": "103.1",
          "End": "104",
          "Emotion": "Admiration",
          "Confidence": "0.665003479",
          "Avg FER Score": "0.732171007",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "Ill pick up Address.",
          "Start": "105.36",
          "End": "106.62",
          "Emotion": "Neutral",
          "Confidence": "0.692145646",
          "Avg FER Score": "0.714218024",
          "Dominant FER Emotion": "Sad"
        },
        {
          "Text": "Where will people pick it up from you?",
          "Start": "107.9",
          "End": "109.84",
          "Emotion": "Curiosity",
          "Confidence": "0.474241048",
          "Avg FER Score": "0.640520512",
          "Dominant FER Emotion": "Angry"
        },
        {
          "Text": "And you see a C.",
          "Start": "112.06",
          "End": "114.12",
          "Emotion": "Neutral",
          "Confidence": "0.956321597",
          "Avg FER Score": "0.821540532",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "And take a look at this.",
          "Start": "115.4",
          "End": "117.64",
          "Emotion": "Neutral",
          "Confidence": "0.684900999",
          "Avg FER Score": "0.881912405",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "Like 10.",
          "Start": "117.78",
          "End": "118.28",
          "Emotion": "Neutral",
          "Confidence": "0.887729764",
          "Avg FER Score": "0.955703162",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "Whats this?",
          "Start": "118.54",
          "End": "119",
          "Emotion": "Curiosity",
          "Confidence": "0.488924116",
          "Avg FER Score": "0.960007565",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "And then weve got my product details.",
          "Start": "121.98",
          "End": "124.76",
          "Emotion": "Neutral",
          "Confidence": "0.90899092",
          "Avg FER Score": "0.666946242",
          "Dominant FER Emotion": "Neutral"
        },
        {
          "Text": "Im going to click here.",
          "Start": "125.34",
          "End": "126.4",
          "Emotion": "Neutral",
          "Confidence": "0.49662587",
          "Avg FER Score": "0.53009551",
          "Dominant FER Emotion": "Angry"
        }
      ],
      "page": 17
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Text-based emotion detection: Advances, challenges, and opportunities",
      "authors": [
        "F Acheampong",
        "C Wenyu",
        "H Nunoo-Mensah"
      ],
      "year": "2020",
      "venue": "Text-based emotion detection: Advances, challenges, and opportunities"
    },
    {
      "citation_id": "2",
      "title": "Facial expression classification for user experience testing using k-nearest neighbor",
      "authors": [
        "Y Afriansyah",
        "R Nugrahaeni",
        "A Prasasti"
      ],
      "venue": "Proceedings -2021 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology, IAICT 2021"
    },
    {
      "citation_id": "3",
      "title": "Comparative study of user experience evaluation techniques based on mouse and gaze tracking",
      "authors": [
        "I Aviz",
        "K Souza",
        "E Ribeiro",
        "H De Mello",
        "M Junior",
        "R Da",
        "Seruffo"
      ],
      "venue": "Proceedings of the 25th Brazillian Symposium on Multimedia and the Web"
    },
    {
      "citation_id": "4",
      "title": "Comparative analysis of vision transformer models for facial emotion recognition using augmented balanced datasets",
      "authors": [
        "S Bobojanov",
        "B Kim",
        "M Arabboev",
        "S Begmatov"
      ],
      "venue": "Applied Sciences (Switzerland)"
    },
    {
      "citation_id": "5",
      "title": "A survey on facial emotion recognition techniques: A state-of-the-art literature review",
      "authors": [
        "F Canal",
        "T Mller",
        "J Matias",
        "G Scotton",
        "A De Sa Junior",
        "E Pozzebon",
        "A Sobieranski"
      ],
      "venue": "Information Sciences"
    },
    {
      "citation_id": "6",
      "title": "Smart facial emotion recognition with gender and age factor estimation",
      "authors": [
        "S Chavali",
        "C Kandavalli",
        "T Sugash",
        "R Subramani"
      ],
      "year": "2023",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "7",
      "title": "Uxapp: Evaluation of the user experience of digital products through emotion recognition",
      "authors": [
        "R Cordeiro",
        "G Sant",
        "A Erven"
      ],
      "year": "2024",
      "venue": "Uxapp: Evaluation of the user experience of digital products through emotion recognition"
    },
    {
      "citation_id": "8",
      "title": "An evaluation framework for user experience using eye tracking, mouse tracking, keyboard input, and artificial intelligence: A case study",
      "authors": [
        "K De Souza",
        "I De Aviz",
        "H De Mello",
        "K Figueiredo",
        "M Vellasco",
        "F Costa",
        "M Da Rocha Seruffo"
      ],
      "year": "2022",
      "venue": "International Journal of Human-Computer Interaction"
    },
    {
      "citation_id": "9",
      "title": "Facial Emotion Recognition in UX Evaluation: A Systematic Review",
      "authors": [
        "E De Souza",
        "J Veriscimo",
        "L Jnior",
        "Digiampietri"
      ],
      "year": "2021",
      "venue": "Facial Emotion Recognition in UX Evaluation: A Systematic Review"
    },
    {
      "citation_id": "10",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "11",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition in usability testing: A framework for improving web application ui design",
      "authors": [
        "D Drungilas",
        "I Ramaauskas",
        "M Kurmis"
      ],
      "year": "2024",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "13",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "14",
      "title": "Smart environment architecture for emotion detection and regulation",
      "authors": [
        "A Fernndez-Caballero",
        "A Martnez-Rodrigo",
        "J Pastor",
        "J Castillo",
        "E Lozano-Monasor",
        "M Lpez",
        "R Zangrniz",
        "J Latorre",
        "A Fernndez-Sotos"
      ],
      "year": "2016",
      "venue": "Journal of Biomedical Informatics"
    },
    {
      "citation_id": "15",
      "title": "Software usability testing using eeg-based emotion detection and deep learning",
      "authors": [
        "S Gannouni",
        "K Belwafi",
        "A Aledaily",
        "H Aboalsamh",
        "A Belghith"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "16",
      "title": "The 27th annual CHI conference on Human Factors in Computing Systems : CHI 2008. Association for Computing Machinery",
      "authors": [
        "S Greenberg"
      ],
      "year": "2009",
      "venue": "The 27th annual CHI conference on Human Factors in Computing Systems : CHI 2008. Association for Computing Machinery"
    },
    {
      "citation_id": "17",
      "title": "Driver facial expression recognition based on vit and stargan",
      "authors": [
        "Z Huang",
        "Y Yu",
        "C Gou"
      ],
      "venue": "2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI)"
    },
    {
      "citation_id": "18",
      "title": "Facial emotion recognition: State of the art performance on fer",
      "authors": [
        "Y Khaireddin",
        "Z Chen"
      ],
      "year": "2013",
      "venue": "Facial emotion recognition: State of the art performance on fer"
    },
    {
      "citation_id": "19",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "Sensors (Switzerland)"
    },
    {
      "citation_id": "20",
      "title": "Face recognition systems: A survey",
      "authors": [
        "Y Kortli",
        "M Jridi",
        "A Falou",
        "M Atri"
      ],
      "year": "2020",
      "venue": "Face recognition systems: A survey"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition and its applications",
      "authors": [
        "A Ko Lakowska",
        "A Landowska",
        "M Szwoch",
        "W Szwoch",
        "M Wrbel"
      ],
      "year": "2014",
      "venue": "Emotion recognition and its applications"
    },
    {
      "citation_id": "22",
      "title": "Dynamic face models: construction and applications",
      "authors": [
        "Y Li"
      ],
      "year": "2001",
      "venue": "Dynamic face models: construction and applications"
    },
    {
      "citation_id": "23",
      "title": "Constructing facial identity surfaces in a nonlinear discriminating space",
      "authors": [
        "Y Li",
        "S Gong",
        "H Liddell"
      ],
      "year": "2001",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Video-based online face recognition using identity surfaces",
      "authors": [
        "Y Li",
        "S Gong",
        "H Liddell"
      ],
      "year": "2001",
      "venue": "The Second International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems"
    },
    {
      "citation_id": "25",
      "title": "Constructing facial identity surfaces for recognition",
      "authors": [
        "Y Li",
        "S Gong",
        "H Liddell"
      ],
      "year": "2003",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "26",
      "title": "Recognising trajectories of facial identities using kernel discriminant analysis. Image and Vision Computing",
      "authors": [
        "Y Li",
        "S Gong",
        "H Liddell"
      ],
      "year": "2003",
      "venue": "Recognising trajectories of facial identities using kernel discriminant analysis. Image and Vision Computing"
    },
    {
      "citation_id": "27",
      "title": "Multi-view face detection using support vector machines and eigenspace modelling",
      "authors": [
        "Y Li",
        "S Gong",
        "J Sherrah",
        "H Liddell"
      ],
      "year": "2000",
      "venue": "KES'2000. Fourth International Conference on Knowledge-Based Intelligent Engineering Systems and Allied Technologies. Proceedings (Cat. No. 00TH8516)"
    },
    {
      "citation_id": "28",
      "title": "Theories, methods and current research on emotions in library and information science, information retrieval and human-computer interaction",
      "authors": [
        "I Lopatovska",
        "I Arapakis"
      ],
      "year": "2011",
      "venue": "Information Processing and Management"
    },
    {
      "citation_id": "29",
      "title": "A survey on: Facial emotion recognition and classification",
      "authors": [
        "M Moolchandani",
        "S Dwivedi",
        "S Nigam",
        "K Gupta"
      ],
      "venue": "Proceedings -5th International Conference on Computing Methodologies and Communication, ICCMC 2021"
    },
    {
      "citation_id": "30",
      "title": "A review on methods for speech-to-text and text-to-speech conversion",
      "authors": [
        "S Nagdewani",
        "A Jain"
      ],
      "year": "2020",
      "venue": "International Research Journal of Engineering and Technology"
    },
    {
      "citation_id": "31",
      "title": "Affective cognition: Exploring lay theories of emotion",
      "authors": [
        "D Ong",
        "J Zaki",
        "N Goodman"
      ],
      "year": "2015",
      "venue": "Cognition"
    },
    {
      "citation_id": "32",
      "title": "Development of an application for recognizing emotions using convolutional neural networks",
      "authors": [
        "V Pomazan",
        "I Tvoroshenko",
        "V Gorokhovatskyi"
      ],
      "year": "2023",
      "venue": "Development of an application for recognizing emotions using convolutional neural networks"
    },
    {
      "citation_id": "33",
      "title": "User Experience Design Based on Eye-Tracking Technology: A Case Study on Smartphone APPs",
      "authors": [
        "Q.-X Qu",
        "L Zhang",
        "W.-Y Chao",
        "V Duffy"
      ],
      "year": "2017",
      "venue": "User Experience Design Based on Eye-Tracking Technology: A Case Study on Smartphone APPs"
    },
    {
      "citation_id": "34",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision"
    },
    {
      "citation_id": "35",
      "title": "A hybrid multimodal emotion recognition framework for ux evaluation using generalized mixture functions",
      "authors": [
        "M Razzaq",
        "J Hussain",
        "J Bang",
        "C Hua",
        "F Satti",
        "U Rehman",
        "H Bilal",
        "S Kim",
        "S Lee"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "36",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "37",
      "title": "",
      "authors": [
        "B Savani"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "38",
      "title": "Emotion recognition and detection methods: A comprehensive survey",
      "authors": [
        "A Saxena",
        "A Khanna",
        "D Gupta"
      ],
      "year": "2020",
      "venue": "Journal of Artificial Intelligence and Systems"
    },
    {
      "citation_id": "39",
      "title": "A review of optimization method in face recognition: Comparison deep learning and non-deep learning methods",
      "authors": [
        "S Setiowati",
        "E Zulfanahri",
        "I Franita",
        "Ardiyanto"
      ],
      "year": "2017",
      "venue": "2017 9th International Conference on Information Technology and Electrical Engineering"
    },
    {
      "citation_id": "40",
      "title": "A review paper on facial recognition",
      "authors": [
        "A Sharma",
        "A Gupta"
      ],
      "year": "2013",
      "venue": "A review paper on facial recognition"
    },
    {
      "citation_id": "41",
      "title": "Github -sinaptik-ai/pandas-ai: Chat with your database (sql, csv, pandas, polars, mongodb, nosql, etc). pandasai makes data analysis conversational using llms",
      "year": "2024",
      "venue": "Github -sinaptik-ai/pandas-ai: Chat with your database (sql, csv, pandas, polars, mongodb, nosql, etc). pandasai makes data analysis conversational using llms"
    },
    {
      "citation_id": "42",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "43",
      "title": "Novel speech recognition systems applied to forensics within child exploitation: Wav2vec2.0 vs. whisper",
      "authors": [
        "J Vsquez-Correa",
        "A lvarez Muniain"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "44",
      "title": "Ambikairajah. A comprehensive review of speech emotion recognition systems",
      "authors": [
        "T Wani",
        "T Gunawan",
        "S Qadri",
        "M Kartiwi"
      ],
      "year": "2021",
      "venue": "Ambikairajah. A comprehensive review of speech emotion recognition systems"
    },
    {
      "citation_id": "45",
      "title": "Deep learning and emotion recognition for an e-learning platform",
      "authors": [
        "A Zbaida",
        "M Kodad",
        "Y Mohamed",
        "N Benmoussa",
        "Z Achraf"
      ],
      "year": "2023",
      "venue": "Deep learning and emotion recognition for an e-learning platform"
    },
    {
      "citation_id": "46",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "47",
      "title": "Eye tracking, usability, and user experience: A systematic review",
      "authors": [
        "J tpn Novk",
        "J Masner",
        "P Benda",
        "P imek",
        "V Merunka"
      ],
      "year": "2023",
      "venue": "International Journal of Human-Computer Interaction"
    }
  ]
}