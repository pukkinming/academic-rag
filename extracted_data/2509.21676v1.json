{
  "paper_id": "2509.21676v1",
  "title": "Hula: Prosody-Aware Anti-Spoofing With Multi-Task Learning For Expressive And Emotional Synthetic Speech",
  "published": "2025-09-25T22:49:58Z",
  "authors": [
    "Aurosweta Mahapatra",
    "Ismail Rasim Ulgen",
    "Berrak Sisman"
  ],
  "keywords": [
    "Anti-Spoofing",
    "Multi-task Learning",
    "Prosody"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Current anti-spoofing systems remain vulnerable to expressive and emotional synthetic speech, since they rarely leverage prosody as a discriminative cue. Prosody is central to human expressiveness and emotion, and humans instinctively use prosodic cues such as F0 patterns and voiced/unvoiced structure to distinguish natural from synthetic speech. In this paper, we propose HuLA, a two-stage prosody-aware multi-task learning framework for spoof detection. In Stage 1, a self-supervised learning (SSL) backbone is trained on real speech with auxiliary tasks of F0 prediction and voiced/unvoiced classification, enhancing its ability to capture natural prosodic variation similar to human perceptual learning. In Stage 2, the model is jointly optimized for spoof detection and prosody tasks on both real and synthetic data, leveraging prosodic awareness to detect mismatches between natural and expressive synthetic speech. Experiments show that HuLA consistently outperforms strong baselines on challenging out-of-domain dataset, including expressive, emotional, and crosslingual attacks. These results demonstrate that explicit prosodic supervision, combined with SSL embeddings, substantially improves robustness against advanced synthetic speech attacks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Anti-spoofing aims to detect audio generated through replay attacks, speech synthesis, and voice conversion (VC)  [1] . Recent progress in text-to-speech (TTS)  [2] -  [6]  and VC systems  [7] -  [11]  has amplified concerns about expressive synthetic speech, which can be misused to compromise biometric authentication or impersonate speakers for spreading misinformation  [12] ,  [13] . These risks highlight the urgent need for robust and generalizable anti-spoofing systems.\n\nOne of the goals of speech generation is to produce speech that is natural and indistinguishable from human speech. Expressiveness and emotion are the defining characteristics of human speech. TTS and VC systems often rely on prosody, particularly the fundamental frequency (F 0 ) to approximate these aspects  [14] -  [17] . However, accurately modeling F 0 remains challenging  [18] -  [20] , and current synthesis systems do not fully capture the subtleties of human expressiveness. While this limitation is a weakness for synthesis, it represents a valuable opportunity for anti-spoofing: imperfect expressive-A. Mahapatra is with the Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD 21218 USA (e-mail: amahapa2@jhu.edu).\n\nIsmail Rasim Ulgen is with the Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD 21218 USA (e-mail: iulgen1@jhu.edu).\n\nB. Sisman is with the Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD 21218 USA (e-mail: sisman@jhu.edu).\n\nManuscript received September 24, 2025.\n\nness can serve as a discriminative cue for detecting synthetic speech.\n\nAnti-spoofing research has been driven largely by community challenges such as ASVspoof  [21]  and the Audio Deep Synthesis Detection (ADD) Challenge  [22] ,  [23] , which introduced benchmark datasets including ASVspoof 2019  [24] , 2021  [25] , and 2024  [26] . These resources have significantly advanced the field and now serve as standard evaluation benchmarks.\n\nCurrent approaches span three main categories: (i) endto-end models that operate directly on raw waveforms (e.g., RawNet2  [27] , AASIST  [28] ); (ii) methods based on handcrafted spectral features combined with supervised classifiers (e.g., LCNN  [29] , ASSERT  [30] , ResNet34  [31] ); and (iii) pretrained self-supervised learning (SSL) models such as wav2vec 2.0, HuBERT, and WavLM  [32] -  [35] . SSL-based approaches are now widely preferred due to their pretraining on largescale data, ability to learn generalizable representations, and flexibility for fine-tuning in anti-spoofing tasks  [36] -  [38] . Despite these advances, current SSL-based models fail to address the imperfect replication of emotion and expressiveness in synthetic speech, an underexplored, discriminative cue. In this work, we focus on the prosodic dimension of expressiveness and explicitly incorporate prosodic information alongside SSL embeddings to enhance spoof detection.\n\nProsody refers to suprasegmental features of speech  [39] ,  [40]  that shape perception beyond linguistic content, playing a central role in conveying expressiveness and emotion. Humans are inherently sensitive to prosodic cues due to lifelong exposure to natural emotional and expressive speech, and can often distinguish real from synthetic audio based on these cues  [41] . Common prosodic attributes include fundamental frequency (F 0 ), jitter, shimmer, and speaking rate. Among these, F 0 is most widely used in synthetic speech systems to improve naturalness  [42] -  [44] , supported by reliable pitch estimation algorithms such as YAAPT  [45] , DIO  [46] , and Harvest  [47] .\n\nDespite its importance to human perception, prosody remains underexplored in anti-spoofing. Recent studies  [48] -  [50]  have examined prosodic features in isolation  [48] , in combination with speaker verification embeddings  [49] , or through emotion recognition features  [50] , achieving performance comparable to or exceeding baselines. These findings underscore the potential of prosody as a valuable yet underutilized cue for spoof detection, especially when explicitly integrated with SSL-based contextual embeddings.\n\nInspired by this, we aim to make anti-spoofing models listen like a human by leveraging prosodic awareness to better distinguish between real and spoofed speech. To this end, we propose HuLA (Human-Like Listener for Anti-spoofing), a two-stage prosody-aware multi-task learning framework. In Stage 1, a pretrained SSL model is fine-tuned on prosodyrelated tasks (e.g. F 0 prediction and voiced/unvoiced (V-UV) classification) using only real speech. This stage is designed to improve the model's understanding of natural prosodic variation. We extract F 0 values at the frame level using a pitch extraction algorithm and use them as reference labels during training. To support the F 0 prediction task, we incorporate V-UV frame classification. This helps the model learn voiced/unvoiced positioning within the F 0 sequence which is an important aspect of prosodic structure. By learning these patterns in real speech, the model builds a foundation for detecting prosodic deviations in synthetic speech. In Stage 2, the model is jointly optimized for spoof detection and the same prosody tasks on both real and synthetic audio, exposing it to general differences while explicitly leveraging prosodic cues. HuLA consistently outperforms baselines on challenging out-of-domain datasets, including expressive/emotional speech (ASVspoof  [25] ,  [26] , EmoFake  [51] , mixed emotions  [52] ) and cross-lingual data  [22] ,  [53] .\n\nOur key contributions are as follows:\n\n• We identify imperfect modeling of expressiveness as both a vulnerability of speech synthesis and a discriminative opportunity for anti-spoofing. • We highlight the overlooked role of prosody, particularly F 0 and voiced/unvoiced patterns, as a discriminative cue for detecting expressive synthetic speech, addressing a key gap in current anti-spoofing research. • We introduce HuLA, a two-stage multi-task framework that leverages prosody-awareness for improved spoof detection.\n\n• We demonstrate strong out-of-domain robustness, showing HuLA's effectiveness against expressive, emotional, and cross-lingual spoofing attacks.\n\nThe rest of this paper is organized as follows. Section II reviews related work. Section III examines how expressiveness in synthetic speech presents both a threat and an opportunity for anti-spoofing, motivating our approach. Section IV details the proposed method, while Section V describes the experimental setup. Section VI reports results and explanations, and Section VII provides a discussion. Finally, Section VIII concludes the paper and outlines future directions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Traditional Anti-Spoofing And Datasets",
      "text": "Anti-spoofing research has been actively driven by community challenges such as ASVspoof and ADD. These initiatives have produced large-scale benchmark datasets including ASVspoof 2019  [24] , ASVspoof 2021  [25] , ASVspoof 5  [26] , ADD 2022  [22] , and ADD 2023  [23] . The datasets cover a broad range of attack types such as replay, TTS, VC, and adversarial methods, and emphasize different aspects of spoofing. Some focus on realistic scenarios with channel variability, while others extend coverage to multiple languages.\n\nCollectively, these challenges and datasets have provided the foundation for progress in spoof detection research.\n\nBuilding on these resources, anti-spoofing methods can be grouped into three major categories: end-to-end models, handcrafted-feature and ssl-based approaches. End-to-end systems such as RawNet2  [27]  and AASIST  [28]  represent the state of the art, operating directly on raw waveforms and achieving strong benchmark performance. RawNet2 combines a SincNet layer, residual blocks, a GRU, and fully connected layers, while AASIST employs graph attention networks to capture both spectral and temporal artifacts. In contrast, handcrafted approaches extract spectral features such as MFCC  [54] , CQCC  [55] , Mel-spectrograms  [56] , or silence proportion  [57] ,  [58] , followed by supervised classifiers such as LCNN  [29] , ASSERT  [30] , or ResNet34  [31] . However, these models generally struggle with newer datasets that include attacks generated by advanced TTS and VC systems under more realistic conditions. This limitation has motivated a shift toward selfsupervised learning (SSL)-based models, which we review in the following subsection.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Ssl-Based Anti-Spoofing",
      "text": "Pretrained self-supervised learning (SSL) models have become increasingly popular in anti-spoofing research due to their strong generalization ability and competitive performance. Recent state-of-the-art (SOTA) SSL-based systems  [36] ,  [37] ,  [59]  leverage large-scale pretrained representations to achieve significant improvements over traditional approaches. Commonly used SSL architectures include Wav2Vec 2.0  [33] , XLS-R  [32] , HuBERT  [35] , Whisper  [60] , and WavLM  [34] , which are fine-tuned for spoof detection. Several integration strategies have been explored. For example,  [37]  combines XLS-R with a RawNet2 encoder and an AASIST backend, while  [36]  employs XLS-R with a Sensitive Layer Selection (SLS) module, reporting substantial gains on the ASVspoof 2021 DF evaluation track. Both approaches also benefit from data augmentation techniques such as RawBoost  [61] . Another recent work  [62]  fuses Wav2Vec 2.0 with a Mixture of Experts (MoE) mechanism and AASIST for classification.\n\nDespite their success, current SSL-based anti-spoofing models largely overlook prosody, a central cue in human perception of expressiveness and emotion. They do not exploit the imperfect reproduction of prosodic patterns in modern TTS and VC systems, an underutilized discriminative signal with strong potential for spoof detection and the central focus of this work.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. The Use Of Prosody Features In Anti-Spoofing",
      "text": "Although prosody is imperfectly reproduced in synthetic speech, its use as a feature for spoof detection has been relatively limited. Nonetheless, several studies highlight its potential. For example, ProsoSpeaker  [49]  combined prosodic embeddings with speaker verification features and achieved improvements over the baseline. Similarly,  [48]  showed that a detector based solely on classical prosodic features performed on par with established systems. Another study  [63]  Fig.  1 : Conceptual overview of our approach. Stage 1 captures the natural prosodic variation of real speech, while Stage 2 leverages this knowledge to distinguish real from synthetic expressive speech.\n\nproposed a cross-dataset framework that fused prosodic and pronunciation features with learned representations, improving generalization.\n\nCollectively, these works demonstrate that prosody can serve as a valuable cue for spoof detection. However, despite the recent success of self-supervised learning (SSL) models, explicit integration of prosodic modeling with contextual SSL embeddings remains largely unexplored. Addressing this gap forms the core motivation of our work.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Summary Of Research Gap",
      "text": "We identify four major gaps in current spoof detection research:\n\n• Existing models are not designed to handle attacks generated with emotional and expressive speech, which limits their robustness in realistic scenarios. • The imperfect reproduction of expressiveness and emotion in synthetic speech remains an underexploited cue that could be leveraged for detection.\n\n• Prosody, though central to human perception of expressiveness, is underutilized. Most prior work treats it in isolation rather than integrating it with SSL-based embeddings.\n\n• Training datasets are often skewed toward spoofed samples, giving models limited exposure to authentic speech and weakening their ability to learn natural acoustic properties. This work addresses these gaps by proposing HuLA 1 , a prosody-aware, two-stage multi-task learning framework that combines SSL representations with explicit prosodic supervision to improve the robustness of anti-spoofing systems.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Expressive Speech Synthesis: A Threat And An",
      "text": "OPPORTUNITY FOR ANTI-SPOOFING Recent advances in TTS and VC have enabled increasingly natural synthetic speech with human-like emotion and 1 Project page: https://aurosweta-jm18.github.io/HuLA/  expressiveness  [11] ,  [52] ,  [64] . These improvements pose a growing threat to anti-spoofing systems. For example,  [65]  investigated emotion-targeted attacks, where adversaries generated high-quality, expressive speech to bypass anti-spoofing models. The results revealed high equal error rates (EERs), with performance varying across both emotions and synthesis models, underscoring the vulnerability of current anti-spoofing systems to emotionally expressive attacks.\n\nTo illustrate this challenge, we evaluated state-of-the-art models trained on ASVspoof 2019  [24] , which does not contain emotional or expressive samples. For evaluation, we used EmoFake  [51] , an emotional anti-spoofing dataset, and ASVspoof 2024  [26] , the most recent edition of the ASVspoof challenge that includes advanced synthesis techniques capable of generating expressive and realistic speech. As shown in Table  I , both RawNet2 and AASIST perform poorly on emotional/expressive attacks. We then fine-tuned these models on the emotional synthetic speech dataset, EmoFake  [51] .  II  indicate that while performance improves on the EmoFake test partition, performance on ASVspoof 2024 degrades further. These findings suggest that fine-tuning on an emotional corpus (such as EmoFake) improves performance within the same dataset but reduces generalization to other expressive data. This trade-off reflects an architectural limitation: current models often learn dataset-specific artifacts, as also observed in  [66] , and fail to capture more generalizable cues of expressiveness.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results In Table",
      "text": "These findings raise a deeper question: Is synthetic expressiveness truly convincing to human listeners? Studies on human perception of deepfakes suggest otherwise  [41] . Human listeners often rely on prosodic cues such as intonation, rhythm, and emphasis to judge authenticity, and such cues remain difficult to reproduce in generated speech. What remains a challenge for machines is often a strength for humans. Current anti-spoofing models, however, do not explicitly incorporate prosody, focusing instead on acoustic features such as log-mel spectrograms or SSL embeddings. This gap presents an opportunity: by explicitly modeling prosody, detectors can learn to identify the same expressive mismatches that humans perceive. Supporting this view, recent work  [48]  shows that detectors based solely on prosodic features can perform on par with state-of-the-art systems, underscoring the potential of prosody-aware modeling as a complementary path to robustness.\n\nWe believe it is essential for anti-spoofing models to learn the natural prosodic variation of real speech in order to detect synthetic expressive speech produced by advanced synthesis frameworks. Motivated by this, we propose a two-stage training strategy, illustrated in Figure  1 . Stage 1 focuses on learning the prosodic patterns of real speech, while Stage 2 leverages this knowledge to capture prosodic deviations and other spoofing cues that distinguish synthetic from genuine speech. In this view, expressive synthetic speech is not only a threat but also an opportunity: its imperfect replication of prosody can serve as a powerful discriminative signal, if models are designed to \"listen like a human.\"\n\nThe proposed anti-spoofing model, HuLA, follows this strategy and is described in detail in the next section.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. The Proposed Method: Hula",
      "text": "Multi-task learning (MTL)  [67]  is a training paradigm in which a model learns multiple tasks simultaneously, often incorporating auxiliary objectives that support the main task. MTL is known to improve generalization by encouraging the model to learn shared representations, and it has been successfully applied in computer vision  [68]  and speech processing  [69] -  [71] . In anti-spoofing, prior work has shown that MTL can enhance both accuracy and robustness  [72] .\n\nIn this work, we introduce HuLA, a two-stage MTL framework that makes a self-supervised learning (SSL) backbone explicitly prosody-aware while jointly learning spoof detection. Our design incorporates auxiliary prosodyrelated tasks-fundamental frequency (F 0 ) prediction and voiced/unvoiced (V-UV) classification-alongside the main classification objective. This enables the model to exploit prosodic variation as a complementary cue to SSL representations, thereby improving the detection of expressive and emotional synthetic speech. Figure  2  provides an overview of the proposed method.\n\nHuLA follows a two-stage training strategy. In Stage 1, the SSL backbone is fine-tuned on real speech using only prosody-related tasks, allowing the model to internalize natural prosodic variation. In Stage 2, the model is trained on both real and spoofed speech, jointly optimizing spoof detection and prosody tasks. This design leverages the prosodic awareness acquired in Stage 1 to better capture mismatches between real and synthetic expressiveness.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Xls-R Backbone",
      "text": "We build HuLA on XLS-R  [32] , a large-scale SSL model trained on 128 languages that extends the wav2vec 2.0 architecture. A raw waveform x is passed through a convolutional feature encoder to produce latent representations z ∈ R T ×1024 , where T is the number of frames. A 24-layer transformer network then generates contextualized representations h.\n\nThe same XLS-R model is used in both training stages. In Stage 1, the output from the final Transformer layer (H 1 = h (24) ) is used as input to the prosody-related modules. In Stage 2, we compute a weighted sum of all the layers of the Transformer, following  [36] , to form an aggregate representation H 2 that serves as input for both spoof detection and prosody prediction.\n\nWe believe that the contextualized representations from XLS-R are rich and informative. Fine-tuning them for prosody-aware auxiliary tasks and spoof detection enhances performance and improves generalization across diverse antispoofing datasets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Reference Prosody Labels: F 0 And V-Uv",
      "text": "As we use F 0 prediction and V-UV classification as auxiliary prosody-related tasks on XLS-R, we require frame-level labels for both tasks during training. To obtain these, we use the DIO algorithm  [46] , a pitch extraction method, and treat its outputs as labels for training.\n\nFor each audio sample, the DIO algorithm outputs a sequence of pitch values, denoted as\n\nEach value f t in the sequence represents the estimated F 0 at frame t. The F 0 contour of the audio is defined by the full sequence F ref 0 . Voicing is determined based on the F 0 values:\n\nThese F ref 0 and V U V ref reference labels are used in both stages of MTL.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. First-Stage Multi-Task Learning",
      "text": "In the first stage of our MTL framework, we aim to guide the model in learning the prosodic characteristics of expressive speech, specifically focusing on F 0 prediction and V-UV classification. To achieve this, we fine-tune a pretrained XLS-R model using large-scale real speech data with speaker variability. a) Architecture: As illustrated in Figure  2a , we use the final hidden layer output of XLS-R as input features to the prosody-related task module, referred to as the Pro-MTL module. The Pro-MTL module consists of a linear layer, a GRU, an F 0 prediction head, and a V-UV classification head.\n\nFirst, the input H 1 is projected through a linear layer to reduce its dimensionality:\n\nThe projected features are then passed through a single-layer Gated Recurrent Unit (GRU)  [73]  with a hidden size of 256, which captures temporal dynamics relevant to prosody:\n\nWe choose the GRU due to its ability to efficiently model sequential dependencies. We believe that it's configuration provides a good balance between model capacity and generalization, allowing the network to effectively capture prosodyrelated patterns. The GRU output is fed into two parallel heads: one for predicting F 0 , and another for binary V-UV classification:\n\nb) Loss Function: We use a weighted multi-task loss that combines:\n\n• Mean Squared Error (MSE) loss for F 0 prediction, • Binary Cross Entropy (BCE) loss for V-UV classification.\n\nWe empirically set the weight λ = 0.3. c) Normalization: The reference F 0 and V-UV labels are generated using the DIO algorithm  [46] . To improve training stability, we normalize the F 0 values using speakerwise statistics computed on voiced frames only:\n\nHere, µ s and σ s denote the speaker-wise mean and standard deviation of the F ref 0 values, computed using only the voiced frames for speaker s.\n\nd) Length Adjustment: During training, we ensure that the model output and target sequences are adjusted by trimming both to the minimum length:\n\nThis process prevents frame mismatches during loss computation. By the end of the first-stage MTL, the model develops a strong understanding of prosodic patterns in real speech. In the second stage of our MTL framework, we finetune the XLS-R model using both real and spoofed speech. Since the model has already been trained to capture prosodic patterns from real speech in the first stage, this phase focuses on learning how prosody differs between real and synthetic speech. In parallel, the model continues to learn other relevant spoof detection cues through updated contextual embeddings obtained after the first-stage MTL. The architecture for the second stage is illustrated in Figure  2b .\n\nWe follow the architecture proposed in  [36] , performing a weighted sum of the outputs from all Transformer layers in XLS-R. This aggregated representation is then passed through the spoof classifier, similar to the setup in  [36] . In parallel, we retain the Pro-MTL module used in Stage 1 for auxiliary F 0 prediction and V-UV classification, as illustrated in Figure  2b . The same preprocessing pipeline is applied, including speakerwise normalization of F 0 and length adjustment between predicted and reference sequences.\n\nThe overall loss is a weighted combination of spoof classification loss and auxiliary prosody task losses:\n\nwhere L CLS is the weighted cross-entropy loss for spoof detection, and α = 0.4, β = 0.2 are empirically chosen weights for the auxiliary tasks. This joint optimization enables the model to learn from both spoof-relevant and prosody-relevant cues, enhancing robustness under diverse spoofing conditions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Inference Phase",
      "text": "During the inference phase, we use only the spoof classification component of the model as shown in Figure  3 . We load the XLS-R model fine-tuned in the second stage of multi-task learning and retain only the classification head for prediction. In this stage, we discard the auxiliary heads used for prosodyrelated tasks and retain only the spoof classification head.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Dataset",
      "text": "We evaluate HuLA across a diverse set of corpora, using two datasets for training and the rest only for evaluation. This design ensures that performance is assessed under realistic, out-of-domain conditions. The datasets are summarized below and detailed statistics are provided in Table  III . on Latin American Spanish accents (Argentinian, Colombian, Peruvian, Venezuelan, and Chilean). Fake speech is generated using six TTS and VC systems. We use this dataset to assess cross-lingual robustness.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Baseline",
      "text": "We compare HuLA against several state-of-the-art antispoofing models, covering both end-to-end architectures and SSL-based approaches: • RawNet2  [27] : An end-to-end model that operates directly on raw waveforms and has achieved robust performance across ASVspoof benchmarks. We evaluate the official pre-trained model released by the authors, trained on the ASVspoof 2019 dataset, and apply it directly to all evaluation sets in this study.\n\n• AASIST  [28] : An anti-spoofing model that employs integrated spectro-temporal graph attention networks to capture both spectral and temporal artifacts in speech.\n\nIt has consistently ranked among the top-performing systems in recent challenges. For comparison, we use the model trained on ASVspoof 2019, as released by the authors.\n\n• SSL-SLS  [36] : A self-supervised learning-based antispoofing system that integrates XLS-R representations with a Selective Layer Selection (SLS) module. The SLS mechanism computes a weighted combination of Transformer layer outputs, emphasizing layers most relevant for spoof detection. Following  [36] , we replicate their experimental setup: the XLS-R backbone is fine-tuned on the ASVspoof 2019 LA training set, and a validation set is used to monitor convergence and select the bestperforming checkpoint.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Hula Implementation Details",
      "text": "For consistency across experiments, all audio samples are either trimmed or zero-padded to approximately four seconds, ensuring uniform input length and frame structure. Frame-level F 0 labels are extracted using a frame length of 25 ms and a frame period of 20 ms, matching the configuration of XLS-R. We adopt the pretrained XLS-R 300M model  [32]  as the backbone for all experiments.\n\nTraining is carried out in two stages, each for 50 epochs with a batch size of 5 and layer-wise learning rates. In Stage 1, the XLS-R backbone is fine-tuned with a learning rate of 1 × 10 -6 , while the Pro-MTL module uses a slightly higher  This augmentation introduces stationary, signal-independent noise at the waveform level. Prior studies  [36]  have shown that RawBoost improves cross-condition generalization in antispoofing, making it a natural choice for our training pipeline.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vi. Results",
      "text": "The experiments evaluate the effectiveness of HuLA across multiple benchmarks. We analyze how each stage of our two-stage MTL framework contributes to spoof detection, and demonstrate that explicit prosody modeling improves robustness to expressive, emotional, and cross-lingual attacks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Results On Asvspoof Benchmarks",
      "text": "Table IV compares HuLA with strong baselines on three editions of the ASVspoof challenge. End-to-end models such as RawNet2  [27]  and AASIST  [28]  perform competitively on ASVspoof 2019, but their performance drops sharply on ASVspoof 2021 and ASVspoof 2024. This reflects the increased difficulty of the newer datasets, which introduce channel variability (ASVspoof 2021) and advanced spoofing systems (ASVspoof 2024). The SSL-SLS baseline  [36]  achieves clear gains by leveraging pretrained representations,",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Results On Expressive Synthetic Speech",
      "text": "ASVspoof 2024 includes a number of modern TTS and VC systems known for producing expressive, human-like speech (e.g.,  [75] -  [82] ). As shown in Table  IV , such attacks remain challenging for existing state-of-the-art baselines: RawNet2, AASIST, and SSL-SLS all experience notable degradation.\n\nHuLA achieves substantially better results (17.34% EER), demonstrating that explicitly modeling prosody enables the system to detect subtle mismatches between synthetic and natural expressiveness that other models overlook. In addition, the two-stage variant surpasses the one-stage version, confirming that learning prosodic variation from real speech in Stage 1 provides a stronger foundation than training only in mixed real and spoofed speech in Stage 2.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Results On Emotional Synthetic Speech",
      "text": "Emotional synthetic speech poses a particularly serious challenge for anti-spoofing  [65] . As shown in Table  V , baseline models such as RawNet2  [27]  and AASIST  [28]  perform poorly on the EmoFake dataset, which contains emotionally expressive synthetic speech. The SSL-based baseline achieves stronger results, but HuLA without pretraining further improves performance. The full two-stage HuLA model provides the best results overall, indicating that each stage of the framework contributes to increased prosody-awareness. These findings suggest that HuLA is especially effective for prosodyrich attacks such as emotional speech.\n\nWhile EmoFake focuses on single-emotion attacks, realworld speech often blends multiple emotions  [52] . To examine this case, we also evaluate on a mixed-emotion dataset containing combinations such as surprise+happy and happy+angry. As shown in Table V, RawNet2 and AASIST are highly vulnerable to such attacks, while SSL-SLS shows notable improvement. HuLA without pretraining underperforms SSL-SLS in this case, but the full two-stage HuLA achieves the best results. This experiment underscores the value of learning prosodic variation from real speech in Stage 1, which equips HuLA to handle complex emotional mixtures where multiple prosodic patterns interact.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Results On Cross-Lingual Synthetic Speech",
      "text": "Cross-lingual evaluation investigates whether models trained in one language can generalize to others. This is particularly challenging since the model is tested on a language it has never encountered during training. As an additional evaluation, we assess HuLA in this setting, using spoofing datasets in languages other than English while training all models exclusively on English data. Prior work has shown that anti-spoofing models often achieve limited cross-lingual generalization  [53] ,  [83] ,  [84] . Building on these insights, we test whether HuLA's prosody-aware design offers an advantage in this more difficult scenario.\n\nTable VI reports results on HABLA, a Latin American Spanish dataset. HuLA achieves the best performance, outperforming both end-to-end and SSL-based baselines. Although both HuLA and SSL-SLS rely on the multilingual XLS-R backbone, HuLA's superior results highlight the added value of explicit prosody modeling. Interestingly, HuLA without Stage 1 pretraining outperforms the full model, likely because pretraining on English-only speech limits generalization for Spanish.\n\nWe also evaluate on ADD 2022 Track 1, a Mandarin dataset with fully fake utterances. Here, RawNet2 and AASIST perform poorly, while SSL-SLS achieves stronger results. Both HuLA variants surpass SSL-SLS, with the full two-stage HuLA achieving the best performance. While HuLA was not specifically designed for cross-lingual attacks, its prosodyaware design proves beneficial in this setting. Performance remains strong on Spanish but is more limited on Mandarin, reflecting the difficulty of transfer to typologically distant languages. However, HuLA outperforms the baselines in both cases. These findings suggest that prosody-aware modeling is a promising path toward cross-lingual robustness and motivate further investigation in this direction.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vii. Discussion",
      "text": "Our experiments demonstrate the effectiveness of HuLA, a two-stage MTL framework that improves spoof detection through explicit prosody modeling. Although trained only on ASVspoof 2019, which lacks the diversity and realism of recent attacks, HuLA generalizes well across datasets that differ substantially from the training domain. Several of these sets include expressive and emotional synthetic speech, which typically fool state-of-the-art baselines. Prior work  [66]  shows that anti-spoofing models often overfit to dataset-specific artifacts, limiting cross-domain robustness. In contrast, HuLA benefits from prosody-aware training, which equips the model to detect mismatches in expressiveness that are not dataset-dependent. This aligns with our design principle of listening like a human: just as listeners use prosodic cues to judge naturalness, HuLA leverages prosodic variation in both real and spoofed speech to capture subtle differences in expressiveness.\n\nAblation results suggest that HuLA without pretraining (denoted as HuLA w/o PT) already provides substantial improvements over strong baselines, underscoring the value of prosody-aware supervision. Stage 1 pretraining on 100 hours of real speech then delivers further gains, leading to the best overall performance. These findings suggest that pretraining on large, diverse corpora of real speech can enhance detection ability. Performance on emotion attacks further demonstrates the importance of modeling prosodic variation: HuLA effectively handles challenging cases where different emotional states introduces additional variability in acoustic patterns.\n\nFinally, our cross-lingual evaluations reveal both opportunities and limitations. Although trained only on English, HuLA achieves strong performance on Spanish (HABLA) but shows more limited effectiveness on Mandarin (ADD 2022). In both cases, however, HuLA outperforms the baselines. These consistent gains across languages highlight the potential of prosody-aware anti-spoofing for multilingual settings. In future work, we will explore methods that exploit core prosodic patterns to achieve stronger generalization across diverse languages.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Viii. Conclusion",
      "text": "We presented HuLA, a prosody-aware, two-stage multitask learning framework for spoof detection. Our approach is motivated by the observation that emotions and expressiveness, while challenging for anti-spoofing, also provide discriminative cues that humans instinctively rely on. By explicitly modeling prosodic features such as F 0 and voiced/unvoiced classification alongside SSL embeddings, HuLA learns to \"listen like a human.\" In Stage 1, the model acquires prosodic variation from real speech, and in Stage 2 it jointly optimizes spoof detection and prosody tasks using both real and synthetic speech. Extensive experiments demonstrate that HuLA outperforms strong baselines on diverse datasets, including recent ASVspoof editions, emotional and expressive speech, and cross-lingual corpora. These results show that imperfect replication of prosody in synthetic speech can be exploited as a powerful signal for detection. Future work will extend HuLA by incorporating richer prosodic and paralinguistic features, exploring emotion-aware modeling across diverse affective states, and investigating strategies for stronger multilingual generalization.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Conceptual overview of our approach. Stage 1 captures the natural prosodic variation of real speech, while Stage 2",
      "page": 3
    },
    {
      "caption": "Figure 1: Stage 1 focuses on",
      "page": 4
    },
    {
      "caption": "Figure 2: provides an overview of",
      "page": 4
    },
    {
      "caption": "Figure 2: Training phase of HuLA, the proposed prosody-aware multi-task learning method for anti-spoofing. Blue blocks",
      "page": 5
    },
    {
      "caption": "Figure 2: a, we use the",
      "page": 5
    },
    {
      "caption": "Figure 3: Inference Phase of HuLA",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "Evaluation (EER%)": "EmoFake\nASVspoof 2024 Track 1"
        },
        {
          "Model": "RawNet2 [27]",
          "Evaluation (EER%)": "21.71\n40.67"
        },
        {
          "Model": "AASIST [28]",
          "Evaluation (EER%)": "13.64\n35.53"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "Evaluation (EER%)": "EmoFake\nASVspoof 2024 Track 1"
        },
        {
          "Model": "RawNet2 [27]",
          "Evaluation (EER%)": "11.38\n44.06"
        },
        {
          "Model": "AASIST [28]",
          "Evaluation (EER%)": "4.18\n36.39"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "Train-clean-100 (LibriSpeech)\nDev-clean (LibriSpeech)",
          "Real": "28,539\n2,703",
          "Spoof": "–\n–",
          "Total": "28,539\n2,703"
        },
        {
          "Dataset": "Train (ASVspoof 2019)\nDev (ASVspoof 2019)",
          "Real": "2,580\n2,548",
          "Spoof": "22,800\n22,296",
          "Total": "25,380\n24,844"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "ASVspoof": "2019 LA"
        },
        {
          "Model": "RawNet2 [27]",
          "ASVspoof": "4.60"
        },
        {
          "Model": "AASIST [28]",
          "ASVspoof": "0.83"
        },
        {
          "Model": "SSL-SLS [36]",
          "ASVspoof": "0.56"
        },
        {
          "Model": "HuLA w/o PT",
          "ASVspoof": "0.48"
        },
        {
          "Model": "HuLA (proposed)",
          "ASVspoof": "0.80"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "Emotional datasets": "EmoFake"
        },
        {
          "Model": "RawNet2 [27]",
          "Emotional datasets": "21.71"
        },
        {
          "Model": "AASIST [28]",
          "Emotional datasets": "13.64"
        },
        {
          "Model": "SSL-SLS [36]",
          "Emotional datasets": "8.84"
        },
        {
          "Model": "HuLA w/o PT",
          "Emotional datasets": "5.24"
        },
        {
          "Model": "HuLA (proposed)",
          "Emotional datasets": "3.01"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "Non-English datasets": "HABLA"
        },
        {
          "Model": "RawNet2 [27]",
          "Non-English datasets": "40.99"
        },
        {
          "Model": "AASIST [28]",
          "Non-English datasets": "39.65"
        },
        {
          "Model": "SSL-SLS [36]",
          "Non-English datasets": "11.58"
        },
        {
          "Model": "HuLA\nw/o PT",
          "Non-English datasets": "8.83"
        },
        {
          "Model": "HuLA\n(proposed)",
          "Non-English datasets": "13.51"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Battling voice spoofing: a review, comparative analysis, and generalizability evaluation of state-of-the-art voice spoofing counter measures",
      "authors": [
        "A Khan",
        "K Malik",
        "J Ryan",
        "M Saravanan"
      ],
      "year": "2023",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "2",
      "title": "Emoq-tts: Emotion intensity quantization for fine-grained controllable emotional text-tospeech",
      "authors": [
        "C.-B Im",
        "S.-H Lee",
        "S.-B Kim",
        "S.-W Lee"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Prompttts++: Controlling speaker identity in prompt-based text-to-speech using natural language descriptions",
      "authors": [
        "R Shimizu",
        "R Yamamoto",
        "M Kawamura",
        "Y Shirahata",
        "H Doi",
        "T Komatsu",
        "K Tachibana"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "F5-tts: A fairytaler that fakes fluent and faithful speech with flow matching",
      "authors": [
        "Y Chen",
        "Z Niu",
        "Z Ma",
        "K Deng",
        "C Wang",
        "J Zhao",
        "K Yu",
        "X Chen"
      ],
      "year": "2024",
      "venue": "F5-tts: A fairytaler that fakes fluent and faithful speech with flow matching",
      "arxiv": "arXiv:2410.06885"
    },
    {
      "citation_id": "5",
      "title": "Styletts 2: Towards human-level text-to-speech through style diffusion and adversarial training with large speech language models",
      "authors": [
        "Y Li",
        "C Han",
        "V Raghavan",
        "G Mischler",
        "N Mesgarani"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "6",
      "title": "Laugh now cry later: Controlling time-varying emotional states of flow-matching-based zero-shot text-to-speech",
      "authors": [
        "H Wu",
        "X Wang",
        "S Eskimez",
        "M Thakker",
        "D Tompkins",
        "C.-H Tsai",
        "C Li",
        "Z Xiao",
        "S Zhao",
        "J Li",
        "N Kanda"
      ],
      "year": "2024",
      "venue": "2024 IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "7",
      "title": "Expressive voice conversion: A joint framework for speaker identity and emotional style transfer",
      "authors": [
        "Z Du",
        "B Sisman",
        "K Zhou",
        "H Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "8",
      "title": "Expressive-vc: Highly expressive voice conversion with attention fusion of bottleneck and perturbation features",
      "authors": [
        "Z Ning",
        "Q Xie",
        "P Zhu",
        "Z Wang",
        "L Xue",
        "J Yao",
        "L Xie",
        "M Bi"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Pmvc: Data augmentation-based prosody modeling for expressive voice conversion",
      "authors": [
        "Y Deng",
        "H Tang",
        "X Zhang",
        "J Wang",
        "N Cheng",
        "J Xiao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Emotion intensity and its control for emotional voice conversion",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Rana",
        "B Schuller",
        "H Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Not my voice! a taxonomy of ethical and safety harms of speech generators",
      "authors": [
        "W Hutiri",
        "O Papakyriakopoulos",
        "A Xiang"
      ],
      "year": "2024",
      "venue": "The 2024 ACM Conference on Fairness, Accountability, and Transparency"
    },
    {
      "citation_id": "13",
      "title": "Deepfakes generation and detection: State-of-the-art, open challenges, countermeasures, and way forward",
      "authors": [
        "M Masood",
        "M Nawaz",
        "K Malik",
        "A Javed",
        "A Irtaza",
        "H Malik"
      ],
      "year": "2023",
      "venue": "Applied intelligence"
    },
    {
      "citation_id": "14",
      "title": "Gr0: Self-supervised global representation learning for zero-shot voice conversion",
      "authors": [
        "Y Wang",
        "J Su",
        "A Finkelstein",
        "Z Jin"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Diff-hiervc: Diffusion-based hierarchical voice conversion with robust pitch generation and masked prior for zero-shot speaker adaptation",
      "authors": [
        "H.-Y Choi",
        "S.-H Lee",
        "S.-W Lee"
      ],
      "year": "2023",
      "venue": "Diff-hiervc: Diffusion-based hierarchical voice conversion with robust pitch generation and masked prior for zero-shot speaker adaptation",
      "arxiv": "arXiv:2311.04693"
    },
    {
      "citation_id": "16",
      "title": "Language-independent prosodyenhanced speech representations for multilingual speech synthesis",
      "authors": [
        "C Liu",
        "Z.-H Ling",
        "Y.-J Hu"
      ],
      "year": "2024",
      "venue": "2024 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "17",
      "title": "Towards natural and controllable cross-lingual voice conversion based on neural tts model and phonetic posteriorgram",
      "authors": [
        "S Zhao",
        "H Wang",
        "T Nguyen",
        "B Ma"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Deep learning-based expressive speech synthesis: a systematic review of approaches, challenges, and resources",
      "authors": [
        "H Barakat",
        "O Turk",
        "C Demiroglu"
      ],
      "year": "2024",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "19",
      "title": "Reimagining speech: a scoping review of deep learning-based methods for non-parallel voice conversion",
      "authors": [
        "A Bargum",
        "S Serafin",
        "C Erkut"
      ],
      "year": "2024",
      "venue": "Frontiers in signal processing"
    },
    {
      "citation_id": "20",
      "title": "Generative adversarial network based voice conversion: Techniques, challenges, and recent advancements",
      "authors": [
        "S Dhar",
        "N Jana",
        "S Das"
      ],
      "year": "2025",
      "venue": "Generative adversarial network based voice conversion: Techniques, challenges, and recent advancements",
      "arxiv": "arXiv:2504.19197"
    },
    {
      "citation_id": "21",
      "title": "Asvspoof 2015: Automatic speaker verification spoofing and countermeasures challenge evaluation plan",
      "authors": [
        "Z Wu",
        "T Kinnunen",
        "N Evans",
        "J Yamagishi"
      ],
      "year": "2014",
      "venue": "Training"
    },
    {
      "citation_id": "22",
      "title": "Add 2022: the first audio deep synthesis detection challenge",
      "authors": [
        "J Yi",
        "R Fu",
        "J Tao",
        "S Nie",
        "H Ma",
        "C Wang",
        "T Wang",
        "Z Tian",
        "Y Bai",
        "C Fan",
        "S Liang",
        "S Wang",
        "S Zhang",
        "X Yan",
        "L Xu",
        "Z Wen",
        "H Li"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Add 2023: the second audio deepfake detection challenge",
      "authors": [
        "J Yi",
        "J Tao",
        "R Fu",
        "X Yan",
        "C Wang",
        "T Wang",
        "C Zhang",
        "X Zhang",
        "Y Zhao",
        "Y Ren"
      ],
      "year": "2023",
      "venue": "Add 2023: the second audio deepfake detection challenge",
      "arxiv": "arXiv:2305.13774"
    },
    {
      "citation_id": "24",
      "title": "Asvspoof 2019: A large-scale public database of synthesized, converted and replayed speech",
      "authors": [
        "X Wang",
        "J Yamagishi",
        "M Todisco",
        "H Delgado",
        "A Nautsch",
        "N Evans",
        "M Sahidullah",
        "V Vestman",
        "T Kinnunen",
        "K Lee",
        "L Juvela",
        "P Alku",
        "Y.-H Peng",
        "H.-T Hwang",
        "Y Tsao",
        "H.-M Wang",
        "S Maguer",
        "M Becker",
        "F Henderson",
        "R Clark",
        "Y Zhang",
        "Q Wang",
        "Y Jia",
        "K Onuma",
        "K Mushika",
        "T Kaneda",
        "Y Jiang",
        "L.-J Liu",
        "Y.-C Wu",
        "W.-C Huang",
        "T Toda",
        "K Tanaka",
        "H Kameoka",
        "I Steiner",
        "D Matrouf",
        "J.-F Bonastre",
        "A Govender",
        "S Ronanki",
        "J.-X Zhang",
        "Z.-H Ling"
      ],
      "year": "2020",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "25",
      "title": "Asvspoof 2021: Towards spoofed and deepfake speech detection in the wild",
      "authors": [
        "X Liu",
        "X Wang",
        "M Sahidullah",
        "J Patino",
        "H Delgado",
        "T Kinnunen",
        "M Todisco",
        "J Yamagishi",
        "N Evans",
        "A Nautsch",
        "K Lee"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "26",
      "title": "Asvspoof 5: Crowdsourced speech data, deepfakes, and adversarial attacks at scale",
      "authors": [
        "X Wang",
        "H Delgado",
        "H Tak",
        "J.-W Jung",
        "H -J. Shim",
        "M Todisco",
        "I Kukanov",
        "X Liu",
        "M Sahidullah",
        "T Kinnunen"
      ],
      "year": "2024",
      "venue": "Asvspoof 5: Crowdsourced speech data, deepfakes, and adversarial attacks at scale",
      "arxiv": "arXiv:2408.08739"
    },
    {
      "citation_id": "27",
      "title": "End-to-end anti-spoofing with rawnet2",
      "authors": [
        "H Tak",
        "J Patino",
        "M Todisco",
        "A Nautsch",
        "N Evans",
        "A Larcher"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Aasist: Audio anti-spoofing using integrated spectrotemporal graph attention networks",
      "authors": [
        "J.-W Jung",
        "H.-S Heo",
        "H Tak",
        "H -J. Shim",
        "J Chung",
        "B.-J Lee",
        "H.-J Yu",
        "N Evans"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Light convolutional neural network with feature genuinization for detection of synthetic speech attacks",
      "authors": [
        "Z Wu",
        "R Das",
        "J Yang",
        "H Li"
      ],
      "year": "2020",
      "venue": "Light convolutional neural network with feature genuinization for detection of synthetic speech attacks",
      "arxiv": "arXiv:2009.09637"
    },
    {
      "citation_id": "30",
      "title": "Assert: Antispoofing with squeeze-excitation and residual networks",
      "authors": [
        "C.-I Lai",
        "N Chen",
        "J Villalba",
        "N Dehak"
      ],
      "year": "2019",
      "venue": "Assert: Antispoofing with squeeze-excitation and residual networks",
      "arxiv": "arXiv:1904.01120"
    },
    {
      "citation_id": "31",
      "title": "Spoof detection using voice contribution on lfcc features and resnet-34",
      "authors": [
        "K Mon",
        "K Galajit",
        "C Mawalim",
        "J Karnjana",
        "T Isshiki",
        "P Aimmanee"
      ],
      "year": "2023",
      "venue": "2023 18th International Joint Symposium on Artificial Intelligence and Natural Language Processing"
    },
    {
      "citation_id": "32",
      "title": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal",
        "K Singh",
        "P Von Platen",
        "Y Saraf",
        "J Pino"
      ],
      "year": "2021",
      "venue": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "arxiv": "arXiv:2111.09296"
    },
    {
      "citation_id": "33",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "34",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "36",
      "title": "Audio deepfake detection with selfsupervised xls-r and sls classifier",
      "authors": [
        "Q Zhang",
        "S Wen",
        "T Hu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Automatic speaker verification spoofing and deepfake detection using wav2vec 2.0 and data augmentation",
      "authors": [
        "H Tak",
        "M Todisco",
        "X Wang",
        "J.-W Jung",
        "J Yamagishi",
        "N Evans"
      ],
      "year": "2022",
      "venue": "Automatic speaker verification spoofing and deepfake detection using wav2vec 2.0 and data augmentation",
      "arxiv": "arXiv:2202.12233"
    },
    {
      "citation_id": "38",
      "title": "Xlsr-mamba: A dual-column bidirectional state space model for spoofing attack detection",
      "authors": [
        "Y Xiao",
        "R Das"
      ],
      "year": "2025",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "39",
      "title": "Prosody in context: A review",
      "authors": [
        "J Cole"
      ],
      "year": "2015",
      "venue": "Cognition and Neuroscience"
    },
    {
      "citation_id": "40",
      "title": "Learning second language suprasegmentals: Effect of l2 experience on prosody and fluency characteristics of l2 speech",
      "authors": [
        "P Trofimovich",
        "W Baker"
      ],
      "year": "2006",
      "venue": "Studies in second language acquisition"
    },
    {
      "citation_id": "41",
      "title": "How do users perceive deepfake personas? investigating the deepfake user perception and its implications for human-computer interaction",
      "authors": [
        "I Kaate",
        "J Salminen",
        "S.-G Jung",
        "H Almerekhi",
        "B Jansen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter"
    },
    {
      "citation_id": "42",
      "title": "Instructtts: Modelling expressive tts in discrete latent space with natural language style prompt",
      "authors": [
        "D Yang",
        "S Liu",
        "R Huang",
        "C Weng",
        "H Meng"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "43",
      "title": "Textless speech emotion conversion using discrete and decomposed representations",
      "authors": [
        "F Kreuk",
        "A Polyak",
        "J Copet",
        "E Kharitonov",
        "T.-A Nguyen",
        "M Rivière",
        "W.-N Hsu",
        "A Mohamed",
        "E Dupoux",
        "Y Adi"
      ],
      "year": "2021",
      "venue": "Textless speech emotion conversion using discrete and decomposed representations",
      "arxiv": "arXiv:2111.07402"
    },
    {
      "citation_id": "44",
      "title": "Pe-wav2vec: A prosody-enhanced speech model for self-supervised prosody learning in tts",
      "authors": [
        "Z.-C Liu",
        "L Chen",
        "Y.-J Hu",
        "Z.-H Ling",
        "J Pan"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "45",
      "title": "Yet another algorithm for pitch tracking (yaapt)",
      "authors": [
        "K Kasi"
      ],
      "year": "2002",
      "venue": "Yet another algorithm for pitch tracking (yaapt)"
    },
    {
      "citation_id": "46",
      "title": "World: a vocoder-based high-quality speech synthesis system for real-time applications",
      "authors": [
        "M Morise",
        "F Yokomori",
        "K Ozawa"
      ],
      "year": "2016",
      "venue": "IEICE TRANSACTIONS on Information and Systems"
    },
    {
      "citation_id": "47",
      "title": "Harvest: A high-performance fundamental frequency estimator from speech signals",
      "authors": [
        "M Morise"
      ],
      "year": "2017",
      "venue": "Harvest: A high-performance fundamental frequency estimator from speech signals"
    },
    {
      "citation_id": "48",
      "title": "Pitch imperfect: Detecting audio deepfakes through acoustic prosodic analysis",
      "authors": [
        "K Warren",
        "D Olszewski",
        "S Layton",
        "K Butler",
        "C Gates",
        "P Traynor"
      ],
      "year": "2025",
      "venue": "Pitch imperfect: Detecting audio deepfakes through acoustic prosodic analysis",
      "arxiv": "arXiv:2502.14726"
    },
    {
      "citation_id": "49",
      "title": "Combining automatic speaker verification and prosody analysis for synthetic speech detection",
      "authors": [
        "L Attorresi",
        "D Salvi",
        "C Borrelli",
        "P Bestagini",
        "S Tubaro"
      ],
      "year": "2022",
      "venue": "International Conference on Pattern Recognition"
    },
    {
      "citation_id": "50",
      "title": "Deepfake speech detection through emotion recognition: a semantic approach",
      "authors": [
        "E Conti",
        "D Salvi",
        "C Borrelli",
        "B Hosler",
        "P Bestagini",
        "F Antonacci",
        "A Sarti",
        "M Stamm",
        "S Tubaro"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "51",
      "title": "Emofake: An initial dataset for emotion fake audio detection",
      "authors": [
        "Y Zhao",
        "J Yi",
        "J Tao",
        "C Wang",
        "Y Dong"
      ],
      "year": "2024",
      "venue": "China National Conference on Chinese Computational Linguistics"
    },
    {
      "citation_id": "52",
      "title": "Speech synthesis with mixed emotions",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Rana",
        "B Schuller",
        "H Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "53",
      "title": "Habla: A dataset of latin american spanish accents for voice anti-spoofing",
      "authors": [
        "P Flórez",
        "R Manrique",
        "B Nunes"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "54",
      "title": "Synthetic speech detection through short-term and long-term prediction traces",
      "authors": [
        "C Borrelli",
        "P Bestagini",
        "F Antonacci",
        "A Sarti",
        "S Tubaro"
      ],
      "year": "2021",
      "venue": "EURASIP Journal on Information Security"
    },
    {
      "citation_id": "55",
      "title": "An explainability study of the constant q cepstral coefficient spoofing countermeasure for automatic speaker verification",
      "authors": [
        "H Tak",
        "J Patino",
        "A Nautsch",
        "N Evans",
        "M Todisco"
      ],
      "year": "2020",
      "venue": "An explainability study of the constant q cepstral coefficient spoofing countermeasure for automatic speaker verification",
      "arxiv": "arXiv:2004.06422"
    },
    {
      "citation_id": "56",
      "title": "Feature genuinization based residual squeeze-andexcitation for audio anti-spoofing in sound ai",
      "authors": [
        "R Ray",
        "S Karthik",
        "V Mathur",
        "P Kumar",
        "S Tiwari",
        "R Shankarappa"
      ],
      "year": "2021",
      "venue": "2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT)"
    },
    {
      "citation_id": "57",
      "title": "The impact of silence on speech anti-spoofing",
      "authors": [
        "Y Zhang",
        "Z Li",
        "J Lu",
        "H Hua",
        "W Wang",
        "P Zhang"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "58",
      "title": "Speech is silver, silence is golden: What do asvspooftrained models really learn?",
      "authors": [
        "N Müller",
        "F Dieckmann",
        "P Czempin",
        "R Canals",
        "K Böttinger",
        "J Williams"
      ],
      "year": "2021",
      "venue": "Speech is silver, silence is golden: What do asvspooftrained models really learn?",
      "arxiv": "arXiv:2106.12914"
    },
    {
      "citation_id": "59",
      "title": "Xlsr-mamba: A dual-column bidirectional state space model for spoofing attack detection",
      "authors": [
        "Y Xiao",
        "R Das"
      ],
      "year": "2025",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "60",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "61",
      "title": "Rawboost: A raw data boosting and augmentation method applied to automatic speaker verification anti-spoofing",
      "authors": [
        "H Tak",
        "M Kamble",
        "J Patino",
        "M Todisco",
        "N Evans"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "62",
      "title": "Mixture of experts fusion for fake audio detection using frozen wav2vec 2.0",
      "authors": [
        "Z Wang",
        "R Fu",
        "Z Wen",
        "J Tao",
        "X Wang",
        "Y Xie",
        "X Qi",
        "S Shi",
        "Y Lu",
        "Y Liu"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "63",
      "title": "Detection of cross-dataset fake audio based on prosodic and pronunciation features",
      "authors": [
        "C Wang",
        "J Yi",
        "J Tao",
        "C Zhang",
        "S Zhang",
        "X Chen"
      ],
      "year": "2023",
      "venue": "Detection of cross-dataset fake audio based on prosodic and pronunciation features",
      "arxiv": "arXiv:2305.13700"
    },
    {
      "citation_id": "64",
      "title": "Reinforcement learning for emotional text-to-speech synthesis with improved emotion discriminability",
      "authors": [
        "R Liu",
        "B Sisman",
        "H Li"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "65",
      "title": "Can emotion fool anti-spoofing?",
      "authors": [
        "A Mahapatra",
        "I Ulgen",
        "A Naini",
        "C Busso",
        "B Sisman"
      ],
      "year": "2025",
      "venue": "Can emotion fool anti-spoofing?",
      "arxiv": "arXiv:2505.23962"
    },
    {
      "citation_id": "66",
      "title": "Harder or different? understanding generalization of audio deepfake detection",
      "authors": [
        "N Müller",
        "N Evans",
        "H Tak",
        "P Sperl",
        "K Böttinger"
      ],
      "year": "2024",
      "venue": "Harder or different? understanding generalization of audio deepfake detection",
      "arxiv": "arXiv:2406.03512"
    },
    {
      "citation_id": "67",
      "title": "Multitask learning",
      "authors": [
        "R Caruana"
      ],
      "year": "1997",
      "venue": "Machine learning"
    },
    {
      "citation_id": "68",
      "title": "End-to-end multi-task learning with attention",
      "authors": [
        "S Liu",
        "E Johns",
        "A Davison"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "69",
      "title": "Speech emotion recognition with multi-task learning",
      "authors": [
        "X Cai",
        "J Yuan",
        "R Zheng",
        "L Huang",
        "K Church"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "70",
      "title": "An overview of multi-task learning",
      "authors": [
        "Y Zhang",
        "Q Yang"
      ],
      "year": "2018",
      "venue": "National Science Review"
    },
    {
      "citation_id": "71",
      "title": "Joint learning using mixture-ofexpert-based representation for enhanced speech generation and robust emotion recognition",
      "authors": [
        "J.-T Tzeng",
        "C Busso",
        "C.-C Lee"
      ],
      "year": "2025",
      "venue": "Joint learning using mixture-ofexpert-based representation for enhanced speech generation and robust emotion recognition",
      "arxiv": "arXiv:2509.08470"
    },
    {
      "citation_id": "72",
      "title": "Multi-task learning improves synthetic speech detection",
      "authors": [
        "Y Mo",
        "S Wang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "73",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "J Chung",
        "C Gulcehre",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "arxiv": "arXiv:1412.3555"
    },
    {
      "citation_id": "74",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "75",
      "title": "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech",
      "authors": [
        "J Kim",
        "J Kong",
        "J Son"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "76",
      "title": "Xtts: a massively multilingual zero-shot text-to-speech model",
      "authors": [
        "E Casanova",
        "K Davis",
        "E Gölge",
        "G Göknar",
        "I Gulea",
        "L Hart",
        "A Aljafari",
        "J Meyer",
        "R Morais",
        "S Olayemi"
      ],
      "year": "2024",
      "venue": "Xtts: a massively multilingual zero-shot text-to-speech model",
      "arxiv": "arXiv:2406.04904"
    },
    {
      "citation_id": "77",
      "title": "Fastpitch: Parallel text-to-speech with pitch prediction",
      "authors": [
        "A Łańcucki"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "78",
      "title": "Glow-tts: A generative flow for text-to-speech via monotonic alignment search",
      "authors": [
        "J Kim",
        "S Kim",
        "J Kong",
        "S Yoon"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "79",
      "title": "Gradtts: A diffusion probabilistic model for text-to-speech",
      "authors": [
        "V Popov",
        "I Vovk",
        "V Gogoryan",
        "T Sadekova",
        "M Kudinov"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "80",
      "title": "Starganv2-vc: A diverse, unsupervised, non-parallel framework for natural-sounding voice conversion",
      "authors": [
        "Y Li",
        "A Zare",
        "N Mesgarani"
      ],
      "year": "2021",
      "venue": "Starganv2-vc: A diverse, unsupervised, non-parallel framework for natural-sounding voice conversion",
      "arxiv": "arXiv:2107.10394"
    },
    {
      "citation_id": "81",
      "title": "Diffusion-based voice conversion with fast maximum likelihood sampling scheme",
      "authors": [
        "V Popov",
        "I Vovk",
        "V Gogoryan",
        "T Sadekova",
        "M Kudinov",
        "J Wei"
      ],
      "year": "2021",
      "venue": "Diffusion-based voice conversion with fast maximum likelihood sampling scheme",
      "arxiv": "arXiv:2109.13821"
    },
    {
      "citation_id": "82",
      "title": "Voice conversion using speech-to-speech neuro-style transfer",
      "authors": [
        "E Albadawy",
        "S Lyu"
      ],
      "year": "2020",
      "venue": "Interspeech"
    },
    {
      "citation_id": "83",
      "title": "Are audio deepfake detection models polyglots?",
      "authors": [
        "B Marek",
        "P Kawa",
        "P Syga"
      ],
      "year": "2024",
      "venue": "Are audio deepfake detection models polyglots?",
      "arxiv": "arXiv:2412.17924"
    },
    {
      "citation_id": "84",
      "title": "Multilingual audio deepfakes dataset for robust and generalizable detection",
      "authors": [
        "C Mawalim",
        "Y Wang",
        "A Adila",
        "S Okada",
        "M Unoki"
      ],
      "year": "2025",
      "venue": "Multilingual audio deepfakes dataset for robust and generalizable detection"
    }
  ]
}