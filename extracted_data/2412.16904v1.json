{
  "paper_id": "2412.16904v1",
  "title": "Temporal-Frequency State Space Duality: An Efficient Paradigm For Speech Emotion Recognition",
  "published": "2024-12-22T07:37:13Z",
  "authors": [
    "Jiaqi Zhao",
    "Fei Wang",
    "Kun Li",
    "Yanyan Wei",
    "Shengeng Tang",
    "Shu Zhao",
    "Xiao Sun"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Multi-Domain Learning",
    "State Space Duality",
    "Mamba"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) plays a critical role in enhancing user experience within human-computer interaction. However, existing methods are overwhelmed by temporal domain analysis, overlooking the valuable envelope structures of the frequency domain that are equally important for robust emotion recognition. To overcome this limitation, we propose TF-Mamba, a novel multi-domain framework that captures emotional expressions in both temporal and frequency dimensions. Concretely, we propose a temporal-frequency mamba block to extract temporal-and frequency-aware emotional features, achieving an optimal balance between computational efficiency and model expressiveness. Besides, we design a Complex Metric-Distance Triplet (CMDT) loss to enable the model to capture representative emotional clues for SER. Extensive experiments on the IEMOCAP and MELD datasets show that TF-Mamba surpasses existing methods in terms of model size and latency, providing a more practical solution for future SER applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Understanding human emotions through speech  [1] -  [5]  has emerged as a critical frontier in social interactions  [6] -  [9] , with the potential to revolutionize applications ranging from mental health monitoring to adaptive user interfaces. Speech Emotion Recognition (SER), which aims to identify emotional states from vocal cues, presents a significant challenge due to the inherent variability in speech patterns and the subtlety of emotional expressions  [2] ,  [3] .\n\nPrevious works have primarily focused on fusing acoustic features across modalities to enrich emotional expression. For instance, Zou et al.  [10]  combined hand-created acoustic features with deep learning-based features for co-learning. However, this approach may limit the autonomous learning potential of high-dimensional complex features  [11] -  [13] , as it can introduce inter-feature conflicts or redundancies. Shen et al.  [1]  introduced Automatic Speech Recognition (ASR) to model more fine-grained emotional features. On the other hand, while improvements of the attention mechanism inside Transformer architectures  [2] -  [4] ,  [14] -  [19]   tend to focus only on intensity variations in the temporal domain. Besides, the stacking of Transformer blocks often prevents the balance between accuracy and computational efficiency, limiting their practicality.\n\nTo tackle the challenge, we focus on two key aspects: (a) Given the automatic learning capability of complex speech features  [11] , we mainly focus on their high-dimensional frequency distribution. Inspired by  [21] , Fig.  1  visualizes a real speech signal's intensity, pitch, and spectrum aligned along time. The frequency energy space provides more finely detailed frequency envelope structures, such as tone and timbre, to capture richer emotional information. (b) The State Space Duality (SSD)  [22] -  [24]  of the Mamba architecture is naturally suited for processing long temporal sequences. It excels at capturing the continuous evolution of acoustic features. Besides, Mamba offers significant computational complexity and efficiency advantages compared to traditional Transformer structures  [2] -  [4] ,  [25] -  [27] . Based on these considerations, our main contributions are as follows:  SER, centered on a Temporal-Frequency Mamba (TF-Mamba) designed to capture the combined temporal and frequency-based speech emotional expressions precisely.\n\n• TF-Mamba leverages temporal awareness and frequency filtering within an efficient Bi-Domain SSD, optimizing the balance between computational efficiency and model expressiveness for high-dimensional speech modeling.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "• We Introduce An Innovative Complex Metric-Distance",
      "text": "Triplet (CMDT) loss, which enhances the clustering of similar emotional samples and the separation of different emotions in fine-grained utterance-level SER, improving the model's emotional discrimination and robustness.\n\n• Extensive experiments show that we outperform existing methods with fewer parameters and faster latency.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Proposed Method A. Overview",
      "text": "We aim to design an efficient multi-domain learning paradigm to address the limitations of traditional acoustic temporal features in SER tasks. As shown in Fig.  2 , the proposed TF-Mamba first processes the speech signal through WavLM-Large to extract embeddings Λ 0 ∈ R L×D , refined through a multi-head self-attention mechanism to obtain shallow features Λ 1 ∈ R L×D , where L denotes the number of feature tokens and D is the feature dimension. Next, the core TF-Mamba block performs deep feature extraction via State Space Duality in both the temporal and frequency domains, generating complex and interconnected speech emotion features M ∈ R L×D . Finally, a multi-layer perceptual emotion classifier predicts emotions with precision. Notably, we introduce a CMDT loss to enhance the clustering of similar emotional samples and the separation of distinct ones in fine-grained utterance-level SER.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Temporal-Frequency Mamba Block",
      "text": "Effectively capturing subtle emotional expressions in complex speech features is crucial. Therefore, we leverage the advanced SSD mechanism in the Mamba architecture and propose a Bi-Domain SSD structure to construct the Temporal-Frequency Mamba block, enabling precise temporal and frequency-based comprehensive speech emotion representation. Specifically, the normalized input Λ 1 is fed into the temporal-domain mamba branch T (•) and the frequencydomain mamba branch F(•) to model both the temporaland frequency-aware features. The resulting outputs, T (Λ T 1 ) and F(Λ F 1 ), are then fused via concatenation Concat(•) and connected to the residual to obtain temporal and frequencybased speech emotion representation M. The complete process can be summarized as:\n\nNotably, T (•) and F(•) are designed based on the Mamba blocks of the Bi-Domain SSD structure, i.e., for the linear projections\n\nwhere D ′ and d denote the expansion and state dimension and G means the number of groups, the temporal and frequency-domain SSD structures leverage their parallel duality properties to model deep and complex information across different domains efficiently.\n\n1) Temporal-Aware Module: To understand how temporal information in speech signals is used for SSD modeling, we apply a 1D Conv layer W 1d (•) to the high-dimensional projection Φ T , sharing convolutional kernel weights along the time dimension to capture short-term dependencies in speech. Then, the SiLU activation function σ dynamically adjusts the weights of the inputs, allowing for more precise handling of subtle emotional variations across different time steps. Consequently, the state vector d) , and state transition matrix A T ∈ R L×D ′ modeled by the temporal-aware module as:\n\n(2)\n\n2) Frequency Filter Module: Inspired by  [28] -  [30] , the low-frequency components of speech signals contain a significant amount of fundamental frequency information, while emotional states are often primarily reflected in pitch and intonation changes as reflected by the low-frequency region. Therefore, as for\n\nis first transformed into the frequency domain along L via the 1D Fast Fourier Transform FFT(•) as:\n\nwhere L ′ is the transformed sequence length of the frequency domain representation Θ F . To effectively disentangle the fundamental frequency information, we designed an adaptive low-pass filter that can learn appropriate frequency thresholds ω from Θ F to optimize the capture of emotional features. Subsequently, the filtered parameter matrices\n\n} are reconstructed by inverse Fourier transform IFFT(•), generating features with enhanced emotional expressiveness.\n\nwhere Θ F 2 is the power spectrum, measuring the intensity of different frequencies within the speech features.\n\n3) Bi-Domain State Space Dualit (SSD): SSD, as an efficient enhancement to the basic State Space Model (SSM)  [22] , introduces a structured duality mask matrix to describe the evolution of the state space, allowing parallel processing across different time steps to efficiently derive the mapping relationship y ← {x, B, C, A}. Both SSD and SSM are derived from the system of equations in Eq. 5.\n\nwhere x t and y t are treated as scalars, denoting the input and output at time t, respectively. h t is the hidden state vector, and the three matrix parameters (A t , B t , C t ) can vary over t.\n\nThe SSD in Mamba2  [23]  simplifies the diagonal matrix A t by setting all diagonal elements to the same value and all off-diagonal elements to zero, which increases computing performance. Thanks to the large reduction in computing cost, the attention mechanism can be integrated with SSD while retaining the model's structural integrity. Specifically, SSD employs a single projection at the block's entry point to simultaneously handle the state vector X, state transition matrix A, input matrix B, and output matrix C, modeling the mapping from [X, A, B, C] to Y . This mechanism is analogous to how standard attention architectures generate Q, K, and V projections in a parallel manner. Therefore, the SSD mechanism is applied to both the temporal-aware and frequency filter, enabling precise extraction of complex emotional features with lower computational cost, thereby improving the overall SER performance.\n\nCombining Eqs.2 and 5, we obtain the outputs {Y T , Y F } as: C. Loss Function 1) Complex Metric-Distance Triplet Loss: Conventional Cross-Entropy Loss L CE  [2] ,  [31]  trains SER models by minimizing errors between predictions and true labels. Still, it struggles to effectively capture subtle differences in the emotional feature space, particularly when the emotion classes are similar. To address this, we propose adopting the existing InfoNCE triplet loss  [32] ,  [33]  to mitigate this shortcoming. Given that speech signals contain rich acoustic nuances in the frequency domain, we extend the metric space to the complex frequency domain by FFT and construct a method using Vector Cosine Similarity VecSim(•) to approximate the Euclidean distance, i.e., for any vectors ⃗ U and ⃗ V , denoted as:\n\nwhere Re(•) and | • | are the real part and modulus operations respectively, and ⃗ V is the complex conjugate of ⃗ V . Based on this, we build a CMDT loss L c (illustrated in Fig.  2 ), which enhances the model's ability to distinguish between similar emotional features. This is achieved by simultaneously minimizing the distance between the anchor (predicted emotion complex vector { ⃗ M i [k]}) and the positive anchor (true emotion complex vector { ⃗ P i [k]}), while maximizing its distance from negative anchors (incorrect emotion complex vectors { ⃗ G j [k]}). The loss is defined as:\n\nwhere N means the number of samples in a batch, and τ is the temperature coefficient for regularizing the score distribution.\n\n2) Optimization: The overall loss function L SER is the weighted sum of the following two loss terms:\n\nIII. EXPERIMENT A. Experiment Setup 1) Datasets & Evaluation Metrics: Following standard practice in SER  [2] , we conduct experiments on two benchmarks, i.e., IEMOCAP  [20]  and MELD  [34] . IEMOCAP  2) Implementation Details: We standardize the raw audio signals to a 16 kHz sampling rate and use WavLM embeddings  [11]  with a dimension of 1024, coupled with the selfattention encoder comprising 8 heads. To ensure fair comparisons, we follow the settings in  [2] ,  [3] ,  [15] , using five-fold cross-validation exclusively for the IEMOCAP dataset with a fixed batch size of 32. We employ the AdamW optimizer with an initial learning rate of 5×10 -4 . In addition, the weight coefficient λ in the loss function is set to 0.1.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Results And Comparison 1)",
      "text": "Comparison with SOTA Methods: Table  I  reports the comparison between TF-Mamba and existing SOTA methods. Under identical experimental settings, TF-Mamba significantly outperforms SOTA methods across both datasets.\n\n2) Computational Complexity Analysis: In Fig.  3 , we compare the model parameters (M) and computational latency (ms) across SOTA methods. The results demonstrate that the proposed TF-Mamba reduces the parameters by 1.81× and improves computational latency by 1.87× compared to the advanced ShiftFormer method  [15] . These improvements highlight that TF-Mamba significantly lowers computational costs efficiency while maintaining high performance, making it more effective and practical for real-world applications.\n\n3) Visualization of TF-Mamba: Fig.  4  visualizes the changes in speech features after processing through the temporal and frequency modules. In the temporal domain, emotionally salient features are emphasized while irrelevant features are down-weighted. In the frequency domain, high-frequency noise is reduced, minimizing interference in emotion detection.  C. Ablation Study 1) Effects of Pre-Trained Models: To investigate the impact of different pre-trained models (PTMs) on SER accuracy, we compare various speech feature extractors. In Table  II , WavLM-Large significantly outperforms the other PTMs in SER performance.\n\n2) Necessity of Key Components: Table  III  presents the impact of each key component on performance. The baseline model only input speech embeddings into an emotion classifier for SER. Notably, to verify the necessity of introducing the Frequency-Domain Mamba, we compare TF-Mamba with the Dual Temporal-Domain Mamba. The results show that WA, UA, and WF1 have 2.9%, 3.0%, and 3.0% gains on the IEMOCAP and MELD datasets, respectively, highlighting the effectiveness of the Frequency-Domain Mamba.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Conclusions",
      "text": "This paper proposes TF-Mamba, a novel multi-domain learning paradigm for fine-grained Speech Emotion Recognition (SER). By combining temporal awareness and frequency filtering, TF-Mamba provides comprehensive emotional representation with reduced computational costs. Additionally, the proposed CMDT loss enhances emotional discrimination. Experimental results show that TF-Mamba outperforms existing methods on multiple benchmarks, offering a more practical solution for future SER tasks.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Visualization of time-aligned speech intensity, pitch, and frequency",
      "page": 1
    },
    {
      "caption": "Figure 2: The proposed TF-Mamba framework introduces an innovative multi-domain learning paradigm designed to precisely capture speech emotion expressions",
      "page": 2
    },
    {
      "caption": "Figure 3: Our TF-Mamba achieves the SOTA performance on the SER task",
      "page": 3
    },
    {
      "caption": "Figure 4: visualizes the",
      "page": 4
    },
    {
      "caption": "Figure 4: The top displays the feature token intensity comparison before and",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "Venue": "",
          "Modality Type": "",
          "IEMOCAP": "WA(%) ↑",
          "MELD": "WF1(%) ↑"
        },
        {
          "Methods": "CA-MSER [10]\nSpeechFormer\n[14]\nSpeechFormer++ [4]\nShiftCNN [15]\nShiftLSTM [15]\nShiftFormer\n[15]\nDWFormer\n[2]\nDST [3]\nMSTR∗ [27]\nENT [1]\nTENT [1]",
          "Venue": "ICASSP’22\nInterSpeech’22\nTASLP’23\nICASSP’23\nICASSP’23\nICASSP’23\nICASSP’23\nICASSP’23\nInterSpeech’23\nICASSP’24\nICASSP’24",
          "Modality Type": "Spec+Logmel+Speech Feature\nSpeech Feature\nSpeech Feature\nSpeech Feature\nSpeech Feature\nSpeech Feature\nSpeech Feature\nSpeech Feature\nSpeech Feature\nSpeech Feature\nSpeech Feature",
          "IEMOCAP": "68.9\n62.9\n68.8\n71.9\n71.4\n72.1\n71.8\n70.9\n70.6\n71.6\n71.4",
          "MELD": "42.2\n41.9\n46.0\n43.1\n45.0\n45.6\n46.7\n45.8\n46.2\n46.3\n45.1"
        },
        {
          "Methods": "TF-Mamba (Ours)",
          "Venue": "-",
          "Modality Type": "Speech Feature",
          "IEMOCAP": "75.3 ↑3.2%",
          "MELD": "48.5 ↑1.8%"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PTMs": "",
          "Venue": "",
          "IEMOCAP": "WA(%) ↑\nUA(%) ↑",
          "MELD": "WF1(%) ↑"
        },
        {
          "PTMs": "Wav2vec2-base [35]\nWav2vec2-large [35]\nHuBERT-base [36]\nHuBERT-large [36]\nWavLM-base [11]\nWavLM-large [11]\nWhisper-base [37]\nWhisper-large [37]",
          "Venue": "NeurIPS’20\nNeurIPS’20\nTALSP’21\nTALSP’21\nJSTSP’21\nJSTSP’21\nICML’23\nICML’23",
          "IEMOCAP": "54.4\n55.4\n58.4\n59.4\n62.7\n62.9\n67.7\n68.7\n71.7\n70.3\n75.3\n75.7\n62.8\n61.0\n68.4\n66.7",
          "MELD": "31.9\n35.2\n37.7\n42.9\n42.5\n48.5\n34.3\n40.2"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Networks": "",
          "#Params": "",
          "IEMOCAP": "WA(%) ↑\nUA(%) ↑",
          "MELD": "WF1(%) ↑"
        },
        {
          "Networks": "Baseline",
          "#Params": "0.66 M",
          "IEMOCAP": "64.5\n65.3",
          "MELD": "40.3"
        },
        {
          "Networks": "+ Self-Attention Encoder\n+ Temporal-Domain Mamba\n+ Frequency-Domain Mamba\n+ CMDT Loss (Full model)",
          "#Params": "5.25 M\n14.39 M\n20.79 M\n20.79 M",
          "IEMOCAP": "68.4\n69.9\n71.7\n72.3\n74.6\n74.3\n75.3\n75.7",
          "MELD": "41.0\n44.3\n47.8\n48.5"
        },
        {
          "Networks": "Dual Temporal-Domain Mamba",
          "#Params": "20.77 M",
          "IEMOCAP": "72.4\n72.7",
          "MELD": "45.5"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion neural transducer for fine-grained speech emotion recognition",
      "authors": [
        "Siyuan Shen",
        "Yu Gao",
        "Feng Liu",
        "Hanyang Wang",
        "Aimin Zhou"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "2",
      "title": "Dwformer: Dynamic window transformer for speech emotion recognition",
      "authors": [
        "Shuaiqi Chen",
        "Xiaofen Xing",
        "Weibin Zhang",
        "Weidong Chen",
        "Xiangmin Xu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Dst: Deformable speech transformer for emotion recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Xiangmin Xu",
        "Jianxin Pang",
        "Lan Du"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Speechformer++: A hierarchical efficient framework for paralinguistic speech processing",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Xiangmin Xu",
        "Jianxin Pang",
        "Lan Du"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "5",
      "title": "Audio-visual segmentation with semantics",
      "authors": [
        "Jinxing Zhou",
        "Xuyang Shen",
        "Jianyuan Wang",
        "Jiayi Zhang",
        "Weixuan Sun",
        "Jing Zhang",
        "Stan Birchfield",
        "Dan Guo",
        "Lingpeng Kong",
        "Meng Wang"
      ],
      "year": "2024",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "6",
      "title": "Prototype learning for micro-gesture classification",
      "authors": [
        "Guoliang Chen",
        "Fei Wang",
        "Kun Li",
        "Zhiliang Wu",
        "Hehe Fan",
        "Yi Yang",
        "Meng Wang",
        "Dan Guo"
      ],
      "year": "2024",
      "venue": "Prototype learning for micro-gesture classification",
      "arxiv": "arXiv:2408.03097"
    },
    {
      "citation_id": "7",
      "title": "Data augmentation for human behavior analysis in multi-person conversations",
      "authors": [
        "Kun Li",
        "Dan Guo",
        "Guoliang Chen",
        "Feiyang Liu",
        "Meng Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "8",
      "title": "Patch-level sounding object tracking for audio-visual question answering",
      "authors": [
        "Zhangbin Li",
        "Jinxing Zhou",
        "Jing Zhang",
        "Shengeng Tang",
        "Kun Li",
        "Dan Guo"
      ],
      "year": "2024",
      "venue": "Patch-level sounding object tracking for audio-visual question answering",
      "arxiv": "arXiv:2412.10749"
    },
    {
      "citation_id": "9",
      "title": "Cluster-phys: Facial clues clustering towards efficient remote physiological measurement",
      "authors": [
        "Wei Qian",
        "Kun Li",
        "Dan Guo",
        "Bin Hu",
        "Meng Wang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition with co-attention based multilevel acoustic information",
      "authors": [
        "Heqing Zou",
        "Yuke Si",
        "Chen Chen",
        "Deepu Rajan",
        "Eng Siong"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Graphbased multimodal sequential embedding for sign language translation",
      "authors": [
        "Shengeng Tang",
        "Dan Guo",
        "Richang Hong",
        "Meng Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "Gloss-driven conditional diffusion models for sign language production",
      "authors": [
        "Shengeng Tang",
        "Feng Xue",
        "Jingjing Wu",
        "Shuo Wang",
        "Richang Hong"
      ],
      "year": "2024",
      "venue": "ACM Transactions on Multimedia Computing, Communications and Applications"
    },
    {
      "citation_id": "14",
      "title": "Speechformer: A hierarchical efficient framework incorporating the characteristics of speech",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Xiangmin Xu",
        "Jianxin Pang",
        "Lan Du"
      ],
      "year": "2022",
      "venue": "Speechformer: A hierarchical efficient framework incorporating the characteristics of speech",
      "arxiv": "arXiv:2203.03812"
    },
    {
      "citation_id": "15",
      "title": "Mingling or misalignment? temporal shift for speech emotion recognition with pre-trained representations",
      "authors": [
        "Siyuan Shen",
        "Feng Liu",
        "Aimin Zhou"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Adaptive dynamic filtering network for image denoising",
      "authors": [
        "Zhong-Qiu Hao Shen",
        "Wandi Zhao",
        "Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Mutual information-driven triple interaction network for efficient image dehazing",
      "authors": [
        "Zhong-Qiu Hao Shen",
        "Yulun Zhao",
        "Zhao Zhang",
        "Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Deraincyclegan: Rain attentive cyclegan for single image deraining and rainmaking",
      "authors": [
        "Yanyan Wei",
        "Zhao Zhang",
        "Yang Wang",
        "Mingliang Xu",
        "Yi Yang",
        "Shuicheng Yan",
        "Meng Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "19",
      "title": "Low-light wheat image enhancement using an explicit inter-channel sparse transformer",
      "authors": [
        "Yu Wang",
        "Fei Wang",
        "Kun Li",
        "Xuping Feng",
        "Wenhui Hou",
        "Lu Liu",
        "Liqing Chen",
        "Yong He",
        "Yuwei Wang"
      ],
      "year": "2024",
      "venue": "Computers and Electronics in Agriculture"
    },
    {
      "citation_id": "20",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "21",
      "title": "Spice: Self-supervised pitch estimation",
      "authors": [
        "Beat Gfeller",
        "Christian Frank",
        "Dominik Roblek",
        "Matt Sharifi",
        "Marco Tagliasacchi",
        "Mihajlo Velimirović"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "22",
      "title": "Mamba: Linear-time sequence modeling with selective state spaces",
      "authors": [
        "Albert Gu",
        "Tri Dao"
      ],
      "year": "2023",
      "venue": "Mamba: Linear-time sequence modeling with selective state spaces",
      "arxiv": "arXiv:2312.00752"
    },
    {
      "citation_id": "23",
      "title": "Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality",
      "authors": [
        "Tri Dao",
        "Albert Gu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 41st International Conference on Machine Learning"
    },
    {
      "citation_id": "24",
      "title": "Micro-gesture online recognition using learnable query points",
      "authors": [
        "Pengyu Liu",
        "Fei Wang",
        "Kun Li",
        "Guoliang Chen",
        "Yanyan Wei",
        "Shengeng Tang",
        "Zhiliang Wu",
        "Dan Guo"
      ],
      "year": "2024",
      "venue": "Micro-gesture online recognition using learnable query points",
      "arxiv": "arXiv:2407.04490"
    },
    {
      "citation_id": "25",
      "title": "Eulermormer: Robust eulerian motion magnification via dynamic filtering within transformer",
      "authors": [
        "Fei Wang",
        "Dan Guo",
        "Kun Li",
        "Meng Wang"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "26",
      "title": "Maskable retentive network for video moment retrieval",
      "authors": [
        "Jingjing Hu",
        "Dan Guo",
        "Kun Li",
        "Zhan Si",
        "Xun Yang",
        "Meng Wang"
      ],
      "year": "2024",
      "venue": "ACM Multimedia 2024"
    },
    {
      "citation_id": "27",
      "title": "Multi-scale temporal transformer for speech emotion recognition",
      "authors": [
        "Zhipeng Li",
        "Xiaofen Xing",
        "Yuanbo Fang",
        "Weibin Zhang",
        "Hengsheng Fan",
        "Xiangmin Xu"
      ],
      "venue": "Proc. Interspeech, 2023"
    },
    {
      "citation_id": "28",
      "title": "Global filter networks for image classification",
      "authors": [
        "Yongming Rao",
        "Wenliang Zhao",
        "Zheng Zhu",
        "Jiwen Lu",
        "Jie Zhou"
      ],
      "year": "2021",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "29",
      "title": "Non-linear frequency warping using constant-q transformation for speech emotion recognition",
      "authors": [
        "Premjeet Singh",
        "Goutam Saha",
        "Md Sahidullah"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Computer Communication and Informatics"
    },
    {
      "citation_id": "30",
      "title": "Speech emotion recognition based on optimized deep features of dual-channel complementary spectrogram",
      "authors": [
        "Juan Li",
        "Xueying Zhang",
        "Fenglian Li",
        "Lixia Huang"
      ],
      "year": "2023",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "31",
      "title": "Proposal-free video grounding with contextual pyramid network",
      "authors": [
        "Kun Li",
        "Dan Guo",
        "Meng Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Frequency decoupling for motion magnification via multi-level isomorphic architecture",
      "authors": [
        "Fei Wang",
        "Dan Guo",
        "Kun Li",
        "Zhun Zhong",
        "Meng Wang"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "33",
      "title": "Contrastive positive sample propagation along the audio-visual event line",
      "authors": [
        "Jinxing Zhou",
        "Dan Guo",
        "Meng Wang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "34",
      "title": "MELD: A multimodal multiparty dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "35",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "36",
      "title": "Hubert: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "37",
      "title": "Robust speech recognition via largescale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    }
  ]
}