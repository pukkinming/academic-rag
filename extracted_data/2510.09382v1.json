{
  "paper_id": "2510.09382v1",
  "title": "Chuckle -- When Humans Teach Ai To Learn Emotions The Easy Way",
  "published": "2025-10-10T13:38:06Z",
  "authors": [
    "Ankush Pratap Singh",
    "Houwei Cao",
    "Yong Liu"
  ],
  "keywords": [
    "emotion recognition",
    "curriculum learning",
    "intended label",
    "human perception",
    "computational efficiency",
    "deep neural networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Curriculum learning (CL) structures training from simple to complex samples, facilitating progressive learning. However, existing CL approaches for emotion recognition often rely on heuristic, data-driven, or model-based definitions of sample difficulty, neglecting the difficulty for human perception, a critical factor in subjective tasks like emotion recognition. We propose CHUCKLE (Crowdsourced Human Understanding Curriculum for Knowledge Led Emotion Recognition), a perception-driven CL framework that leverages annotator agreement and alignment in crowd-sourced datasets to define sample difficulty, under the assumption that clips challenging for humans are similarly hard for machine learning models. Empirical results suggest that CHUCKLE increases the relative mean accuracy by 6.56% for LSTMs and 1.61% for Transformers over non-curriculum baselines, while reducing the number of gradient updates, thereby enhancing both training efficiency and model robustness.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions shape human experience, influencing communication, decision-making, and social interaction. Automatic emotion recognition seeks to infer human affective states from multi-modal signals such as speech  [1, 2, 3] , text  [3] , facial expressions  [3, 4] , gestures  [5, 6] , and physiological signals  [7, 8] . Among these, speech emotion recognition (SER) remains central yet challenging, as it is hindered by speaker variability, contextual and cultural differences, noise, linguistic confounds, and the subjectivity of emotional perception. These factors make SER a high-variance, label-noisy problem where conventional training often fails to generalize.\n\nCurriculum learning (CL) offers a promising solution by structuring training from easy to hard samples, inspired by human learning  [9] . In SER, this is well-motivated: starting with easy, unambiguous samples helps models learn stable low-level features, while progressively ambiguous cases refine higher-level emotional representations.\n\nEarly speech-related CL studies relied on heuristics, e.g., SNR-based ranking for noise-robust recognition  [10]  or progressively ordering training data for speaker recognition  [11] . Later work expanded to dataset-level curricula, for instance, treating acted corpora as easy and large-scale voice data on the Internet as difficult  [12] , or hybrid designs combining utterance-and conversation-level difficulty  [13] . Recent approaches adopt model-based definitions of difficulty, such as mutual information  [14] . Difficulty for human perception can be naturally explored to develop curricula. Lotfian and Busso  [15]  defined perception difficulty in SER as inter-annotator disagreement. They used quantitative disagreement measures such as entropy and error rate to rank samples.\n\nIn this paper, we propose CHUCKLE, a novel human perception-centered CL framework for SER that integrates data-driven strategies (entropy, proportion of intendedemotion votes) (Section 3.1.1) with novel rule-based curricula (Section 3.1.2) that explicitly model the relationship between intended and perceived labels (Figure  1 ). Intuitively, assuming samples difficult for humans are likewise challenging for models, training progresses from unambiguous to ambiguous cases to improve both accuracy and efficiency. Specifically, in an acted emotion dataset, each sample has one intended emotion label and multiple perceived labels from annotators. In addition to agreement/disagreement among annotators, alignment/misalignment between intended and perceived emotions is also a strong indicator of perception difficulty. We develop curricula based on different ranking rules for agreement and alignment. Unlike prior work, CHUCKLE benchmarks multiple curricula, showing rule-based approaches consistently outperform data-driven ones. Further, we quantify the training efficiency, demonstrating fewer gradient updates for comparable performance. Our contributions are three-fold:\n\n1. We develop a novel perception-driven CL framework that integrates rule-based and data-driven curricula.\n\n2. Our rule-based curricula derived from human perception difficulty consistently outperform non-curriculum and data-driven curricula.\n\n3. We demonstrate that CL model training is more efficient, and converges faster to strong performance with fewer gradient updates.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Dataset And Features",
      "text": "We used the CREMA-D dataset  [16] , a standard audiovisual benchmark comprising 7,442 clips (≈12 hours) from 91 actors, who express six emotions across 12 sentences. Each clip has one intended label and multiple perceived labels (8-12 ratings) from 2,443 raters, yielding four types of labels per clip (three perceived, one per modality, and one intended). Table 1 reports agreement between perceived and intended emotions. Notably, audio agreement ranged from 95.7% (neutral) to 16.4% (sadness), underscoring the difficulty in recognizing certain emotions from speech alone. We used a 130-dimensional subset of the ComParE  [17]  feature set, capturing prosodic and voice quality (F0, voicing probability, jitter, shimmer, HNR), energy and temporal cues (RMS, ZCR, auditory spectrum lengths), spectral properties (RASTA bands, roll-off, flux, centroid, entropy, etc.), and cepstral features (14 MFCCs). Smoothed contours and their deltas were included, providing a compact and comprehensive representation for paralinguistic analysis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Curriculum Design",
      "text": "The design of curricula for SER must account for the subjective and often ambiguous nature of emotional labels. In acted datasets such as CREMA-D, the agreement between intended and perceived labels shows how consistently an expression is recognized, while disagreement indicates ambiguity or possible misinterpretation. From a learning perspective, clips with strong agreement serve as clear, low-ambiguity signals that help models establish reliable low-level features, while clips with disagreement or ambiguity are more challenging samples that push models to refine and better generalize. We propose two complementary curriculum strategies:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data-Driven Curricula",
      "text": "Data-driven curricula use continuous scores to quantify label ambiguity. The samples are equally divided into quartiles: Easy, Borderline Easy, Borderline Tough, and Tough based on the difficulty scores.\n\nIntended Emotion Score: This curriculum assigns each sample a score based on the proportion of annotators who agree with the actor's intended emotion, where higher agreement indicates clearer and more reliable samples.\n\nwhere n intended is the number of annotators selecting the intended emotion and N is the total number of annotators.\n\nEntropy Score: This curriculum assigns scores using Shannon entropy H  [18]  over annotator label distributions. Higher entropy indicates greater disagreement, marking the sample as more challenging.\n\nwhere p i denotes the fraction of annotations with emotion i.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Rule-Based Curricula",
      "text": "Rule-based curricula were based on the relationships between the majority of the perceived emotions and the intended emotion. They can be categorized as follows: (1) Clear Match: There is a clear majority among the perceived emotions, and the majority matches the intended emotion; (2) Clear Mismatch: The clear majority of the perceived emotions does not match the intended emotion; (3) Ambiguous Match: The annotations distribute among several emotions without a dominating majority. Multiple annotations are consistent with the intended emotion. (4) Ambiguous Mismatch: There is no majority perceived emotion, and the intended emotion does not receive multiple annotations. The numbers of data samples in all categories are reported in Table  2 . To develop a curriculum from easy to hard, one should order the relative difficulty of these four categories. All orderings start with Clear Match (1) as Easy, as these samples show strong agreement that matches the intended emotion.\n\nThey are unambiguous, reliable, and therefore the easiest to learn from. The differences across curricula stem from how they order the other categories (2, 3, and 4) and whether they prioritize agreement strength or intended-label alignment.\n\nIntended-Perceived Agreement 1 (1 → 2 → 3 → 4): This curriculum prioritizes agreement strength over alignment. After Clear Match (1), Clear Mismatch (2) comes as Borderline Easy because, although the perceived label differs from the intended one, annotators strongly agree on a single alternative. Ambiguous Match (3) is Borderline Tough because there are multiple agreements between annotators, suggesting partial ambiguity, but still one of the agreements matches the intended emotion. Ambiguous Mismatch (  4 ) is Tough, as there are multiple agreements between annotators, but none of them match the intended label. This combined ambiguity and misalignment make the sample highly confusing.\n\nIntended-Perceived Agreement 2 (1 → 3 → 4 → 2): This curriculum emphasizes alignment with the intended emotion as the key factor. Ambiguous Match (3) follows Clear Match (1), because even with multiple agreements, at least one aligns with the intended emotion. Ambiguous Mismatch (4) comes next, since none of the ambiguous agreements align with the intended label, introducing both conflict and misalignment. Clear Mismatch (2) is placed last, the strong agreement among annotators is not consistent with the intended emotion. Such \"confidently incorrect\" labels are considered most misleading, as they provide no alignment and may cause the model to learn the wrong mapping.\n\nIntended-Perceived Agreement 3 (1 → 3 → 2 → 4): This curriculum represents a compromise between alignment and agreement strength. Similar to the previous ordering, Ambiguous Match (3) follows Clear Match  (1) . Clear Mismatch (2) is then placed before Ambiguous Mismatch (4), because even though the majority perceived label disagrees with the intended one, the strong agreement provides a consistent (albeit incorrect) signal. This is easier to model than Ambiguous Mismatch (4), where both ambiguity and misalignment occur simultaneously.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Training Strategy And Computational Cost (Gradient Updates) Analysis",
      "text": "Training starts with samples in the Easy bin only. Samples in the other bins are sequentially added in the following stages, enabling gradual learning while minimizing the risk of catastrophic forgetting. This design reduces computation with fewer gradient updates to reach the target performance. Assuming one update per batch, total updates for noncurriculum training are:\n\nwhere N represents the total number of training samples, B is the batch size, and E total is the total number of epochs.\n\nIn K-stage curriculum learning, the dataset used in each stage increases S 1 ⊂ S 2 ⊂ • • • ⊂ S K = S full . The total updates are:\n\nwhere |S i | denotes the size of subset S i and E i the number of epochs allocated to stage i. LSTMs: All curricula and the random baseline outperformed the non-curriculum strategy (Table  4 ). Rule-based curricula, particularly Intended-Perceived Agreement 1 and 3, achieved the highest, statistically significant gains, indicating LSTMs strongly benefit from staged curriculum learning.    Transformer Models: The baseline (Table  4 ) achieved strong accuracy, outperforming LSTMs due to its superior ability to capture long-term dependencies. Unlike LSTMs, most curricula did not improve this baseline, in fact, random and entropy-based ones even decreased performance. However, the Intended-Perceived Agreement 1 curriculum yielded the best result with a statistically significant improvement, showing that while transformers are robust, perception-based curricula can still enhance them.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experimental Evaluation",
      "text": "Figure  2 (c) shows a consistent decrease in training loss across stages, with a noticeable spike in stage 2 due to the larger and more diverse Borderline Easy set (Table  2 ). Unlike LSTMs, which gradually adapt, Transformers are more sensitive to distribution shifts  [21] , likely due to residual connections and multi-head attention, making them temporarily unstable before re-converging. Accuracy trends (Figure  2(d) ) highlight steady improvements across stages, with the largest jump from Easy (Stage 1) to Borderline Easy (Stage 2).\n\nCost Analysis: Figures 3(a) and 3(b) show that both LSTMs and Transformers with curricula are more efficient than their non-curriculum counterparts. For LSTMs, curricula increase the relative mean accuracy by 4.2-6.5% above the baseline while requiring up to 40.3% fewer updates. Intended-Perceived Agreement 1 achieves the highest mean accuracy gain (a 6.5% relative increase) with 17.6% fewer updates, and Intended-Perceived Agreement 2 achieves the greatest efficiency gain (a 3.59% relative increase) with 40.3% fewer updates. For Transformers, Intended-Perceived Agreement 1, achieves the highest mean accuracy gain (a 1.6% relative increase over the baseline) while reducing updates by 17.6%. The most efficient curriculum, Intended-Perceived Agreement 2, decreases costs by 40.3% with a slight mean accuracy loss (a 1.4% relative decrease). In general, curriculum learning provides a significant cost-efficiency advantage by reducing the updates required to reach a specific performance level.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "This study highlights the effectiveness of curriculum learning for speech emotion recognition. Rule-based curricula derived from agreement and alignment of human perception consistently outperformed non-curriculum and data-driven curricula, improving both accuracy and efficiency. LSTMs achieved a 6.56% relative gain in mean macro accuracy, while Transformers saw a 1.61% relative boost with the best curriculum. Furthermore, curricula reduced training cost by achieving strong performance with fewer gradient updates. Future work will extend the proposed curricula to the other modalities and validate the findings across additional datasets.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed framework maps human annotations",
      "page": 1
    },
    {
      "caption": "Figure 1: ). Intuitively, assum-",
      "page": 1
    },
    {
      "caption": "Figure 2: Model Performances for Intended-Perceived Agreement 1 (Left to Right): (a) LSTM Training Loss (single trial), (b)",
      "page": 4
    },
    {
      "caption": "Figure 3: Training Cost Comparisons (Left to Right): Mean Macro Accuracy vs Gradient Updates (a) LSTMs, (b) Transformers.",
      "page": 4
    },
    {
      "caption": "Figure 2: (a) shows that the curriculum framework reduced",
      "page": 4
    },
    {
      "caption": "Figure 2: (c) shows a consistent decrease in training loss",
      "page": 4
    },
    {
      "caption": "Figure 2: (d)) highlight steady improvements across stages,",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Mean Macro Accuracy (± Std.) for LSTMs and",
      "data": [
        {
          "Curriculum": "Non Curriculum",
          "LSTM": "0.5244 (±0.0404)",
          "Transformer": "0.5946 (±0.0139)"
        },
        {
          "Curriculum": "Random Curriculum",
          "LSTM": "0.5462 (±0.0205)",
          "Transformer": "0.5212 (±0.0382)"
        },
        {
          "Curriculum": "Intended Emotion Score",
          "LSTM": "0.5466 (±0.0350)",
          "Transformer": "0.5672 (±0.0095)"
        },
        {
          "Curriculum": "Entropy Score",
          "LSTM": "0.5504 (±0.0154)",
          "Transformer": "0.5505 (±0.0269)"
        },
        {
          "Curriculum": "Intended-Perceived Agreement 1",
          "LSTM": "0.5587 (±0.0319)∗",
          "Transformer": "0.6042 (±0.0113)∗"
        },
        {
          "Curriculum": "Intended-Perceived Agreement 2",
          "LSTM": "0.5432 (±0.0330)",
          "Transformer": "0.5861 (±0.0091)"
        },
        {
          "Curriculum": "Intended-Perceived Agreement 3",
          "LSTM": "0.5567 (±0.0174)∗",
          "Transformer": "0.5872 (±0.0167)"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A review on speech emotion recognition: A survey, recent advances, challenges, and the influence of noise",
      "authors": [
        "S George",
        "P Ilyas"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition: two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "4",
      "title": "A survey of deep learning-based multimodal emotion recognition: Speech, text, and face",
      "authors": [
        "H Lian",
        "C Lu",
        "S Li",
        "Y Zhao",
        "C Tang",
        "Y Zong"
      ],
      "year": "2023",
      "venue": "Entropy"
    },
    {
      "citation_id": "5",
      "title": "A survey on facial emotion recognition techniques: A state-of-the-art literature review",
      "authors": [
        "F Canal",
        "T Müller",
        "J Matias",
        "G Scotton",
        "A De Sa Junior",
        "E Pozzebon",
        "A Sobieranski"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "6",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "C Corneanu",
        "D Kamińska",
        "T Sapiński",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Comprehensive survey on recognition of emotions from body gestures",
      "authors": [
        "R Gandi",
        "A Geetha",
        "B Reddy"
      ],
      "venue": "Journal of Informatics Education and Research"
    },
    {
      "citation_id": "8",
      "title": "Review of studies on emotion recognition and judgment based on physiological signals",
      "authors": [
        "W Lin",
        "C Li"
      ],
      "year": "2023",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "9",
      "title": "Research progress of eeg-based emotion recognition: A survey",
      "authors": [
        "Y Wang",
        "B Zhang",
        "L Di"
      ],
      "year": "2024",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "10",
      "title": "Curriculum learning",
      "authors": [
        "Y Bengio",
        "J Louradour",
        "R Collobert",
        "J Weston"
      ],
      "year": "2009",
      "venue": "Proceedings of the 26th Annual International Conference on Machine Learning"
    },
    {
      "citation_id": "11",
      "title": "A curriculum learning method for improved noise robustness in automatic speech recognition",
      "authors": [
        "S Braun",
        "D Neil",
        "S Liu"
      ],
      "year": "2017",
      "venue": "2017 25th European Signal Processing Conference"
    },
    {
      "citation_id": "12",
      "title": "Curriculum learning based approaches for noise robust speaker recognition",
      "authors": [
        "S Ranjan",
        "J Hansen"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc"
    },
    {
      "citation_id": "13",
      "title": "Inferring emotion from large-scale internet voice data: A semisupervised curriculum augmentation based deep learning approach",
      "authors": [
        "S Zhou",
        "J Jia",
        "Z Wu",
        "Z Yang",
        "Y Wang",
        "W Chen",
        "F Meng",
        "S Huang",
        "J Shen",
        "X Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "L Yang",
        "Y Shen",
        "Y Mao",
        "L Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "An interpretable deep mutual information curriculum metric for a robust and generalized speech emotion recognition system",
      "authors": [
        "W Lin",
        "K Sridhar",
        "C Busso"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc"
    },
    {
      "citation_id": "16",
      "title": "Curriculum learning for speech emotion recognition from crowdsourced labels",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "17",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "The interspeech 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "A Vinciarelli",
        "K Scherer",
        "F Ringeval",
        "M Chetouani",
        "F Weninger",
        "F Eyben",
        "E Marchi",
        "M Mortillaro",
        "H Salamin",
        "A Polychroniou",
        "F Valente",
        "S Kim"
      ],
      "year": "2013",
      "venue": "The interspeech 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism"
    },
    {
      "citation_id": "19",
      "title": "A mathematical theory of communication",
      "authors": [
        "C Shannon"
      ],
      "year": "1948",
      "venue": "The Bell System Technical Journal"
    },
    {
      "citation_id": "20",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "21",
      "title": "SGDR: stochastic gradient descent with warm restarts",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations"
    },
    {
      "citation_id": "22",
      "title": "Understanding the difficulty of training transformers",
      "authors": [
        "L Liu",
        "X Liu",
        "J Gao",
        "W Chen",
        "J Han"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    }
  ]
}