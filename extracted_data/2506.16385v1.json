{
  "paper_id": "2506.16385v1",
  "title": "Clip-Mg: Guiding Semantic Attention With Skeletal Pose Features And Rgb Data For Micro-Gesture Recognition On The Imigue Dataset",
  "published": "2025-06-19T15:16:06Z",
  "authors": [
    "Santosh Patapati",
    "Trisanth Srinivasan",
    "Amith Adiraju"
  ],
  "keywords": [
    "Micro-gesture recognition",
    "Vision-language models",
    "CLIP adaptation",
    "Pose-guided fusion",
    "Multi-modal deep learning",
    "Semantic query generation",
    "Human pose estimation",
    "Affective computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Micro-gesture recognition is a challenging task in affective computing due to the subtle, involuntary nature of the gestures and their low movement amplitude. In this paper, we introduce a Pose-Guided Semantics-Aware CLIP-based architecture, or CLIP for Micro-Gesture recognition (CLIP-MG), a modified CLIP model tailored for micro-gesture classification on the iMiGUE dataset. CLIP-MG integrates human pose (skeleton) information into the CLIP-based recognition pipeline through pose-guided semantic query generation and a gated multi-modal fusion mechanism. The proposed model achieves a Top-1 accuracy of 61.82%. These results demonstrate both the potential of our approach and the remaining difficulty in fully adapting vision-language models like CLIP for micro-gesture recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Micro-gestures (MGs) are subtle, spontaneous body movements that can reveal hidden emotional states  [1, 2] , often occurring when people attempt to suppress their true feelings. Unlike overt actions or expressive gestures, micro-gestures involve minute motions (e.g., slight fidgeting, brief facial or limb movements) that are short in duration and low in amplitude, making them hard to detect and classify  [3] . The analysis of micro-gestures has gained traction in affective computing and human behavior understanding because these involuntary cues provide valuable insight into a person's internal state. Automatic recognition of micro-gestures is therefore important for applications in psychology, human-computer interaction, and emotion analysis  [4, 5] .\n\nIn this paper, we present CLIP-MG, a novel multi-modal framework for micro-gesture classification on iMiGUE. Our approach builds upon previous work by incorporating pose (skeleton) data in a principled way. The main contributions are summarized as follows:\n\n1. We develop a system that uses human pose cues to help guide the semantic query extraction from video frames. The skeleton information helps focus the CLIP visual encoder on the regions of subtle motion. This creates a semantic query embedding that is rich with features relevant to pose. 2. We introduce a gated fusion mechanism to combine visual and skeleton representations effectively.\n\nOur gated fusion learns to weight and integrate the two modalities. This allows pose features to adaptively modulate the visual features before and during the cross-attention process. 3. We extend CLIP to a multi-modal setting. The pose-based query is fed into the CLIP transformer for cross-attention over semantically significant visual token features. This limits the model to attend to parts relevant to gesture. This results in a fused representation that has both semantic and motion-specific information for classification.\n\n4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous ablation studies to quantify how much each proposed component improves performance. The results of our ablation studies provide insights for future researchers and future research directions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Imigue Dataset",
      "text": "Recent progress in this area has been driven by the introduction of specialized datasets for micro-gesture understanding. In particular, iMiGUE is a large-scale video dataset introduced by Liu et al.  [6]  for identity-free micro-gesture understanding and emotion analysis. The iMiGUE dataset consists of video footage of tennis players during post-match interviews, with detailed frame-level annotations of various micro-gestures. The dataset contains 72 subjects (split into 37 for training and 35 for testing in a cross-subject protocol) and a total of 33 gesture classes.\n\nOne thing to note is that the class distribution is highly imbalanced. 28 of the 33 classes are tail classes with relatively few samples, meaning they collectively only make up less than 60% of the data. This long-tailed distribution, combined with the subtlety and high intra-class variability of micro-gestures, makes the recognition task extremely challenging  [7] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Signal Processing & Machine Learning Techniques For Micro-Gesture Recognition",
      "text": "To tackle these challenges, the research community has organized the Micro-Gesture Analysis (MiGA) challenges in recent years  [7, 8] . These competitions have spurred the development of novel multimodal approaches that leverage both video (RGB) and skeleton (pose) modalities for micro-gesture recognition. In the 2024 MiGA Challenge, for example, all top-performing methods integrated pose information alongside RGB frames. The winning entry by Chen et al. introduced a prototype-based learning approach with a two-stream 3D CNN (PoseConv3D) backbone for RGB and pose, cross-modal attention fusion, and a prototypical refinement component to calibrate ambiguous samples  [9] . This method achieved a Top-",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Clip-Based Video Understanding",
      "text": "Meanwhile, in the broader action recognition field, researchers have explored using pre-trained visionlanguage models like CLIP for video understanding. CLIP (Contrastive Language-Image Pre-training)  [12]  has shown powerful visual feature representations aligned with semantics via natural language supervision. However, straightforward fine-tuning of CLIP on video data can neglect smaller semantic information. To address this, Quan et al. proposed Semantic-Constrained CLIP (SC-CLIP)  [13] . SC-CLIP adapts CLIP to video by generating a compact semantic query from dense visual tokens and using cross-attention to refocus the model on those action-relevant semantics. This \"constrains\" CLIP's attention to discriminative features and yields stronger zero-shot and fine-grained recognition. SC-CLIP demonstrates that directing attention to semantically meaningful regions can improve fine-grained video understanding. Micro-gestures, despite being low in their extent of movement, still take place with subtle visual semantics. Skeleton key-points give precise spatial-temporal anchors (hands, face, shoulders) that show where and when these cues take place. Thus, we design a pose-guided semantic attention mechanism that uses skeletal cues to steer CLIP towards where the gesture is taking place. This creates a query that captures the subtle semantics of micro-gestures.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "Our model (illustrated in Figure  1 ) has several components working in sequence: (1) a visual encoder (based on CLIP's vision transformer) processes the RGB frames, (2) a skeleton encoder processes the pose sequence, (3) pose-based semantic query generation producse semantic queries from visual features guided by pose features, (4) a gated fusion and semantics-based cross-attention fuses the modalities and improves the representation, and (5) a classification head outputs the predicted gesture label. In the following, we detail each component and the overall pipeline.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Visual Encoder",
      "text": "We adopt the OpenAI CLIP ViT-B/16 image tower  [14]  with the standard 224224 input resolution and 1616 patching, which yields ğ‘ƒ = 196 patch tokens plus one [CLS] token per frame. The internal transformer width is 768 dimensions, while CLIP's projection head maps the final [CLS] embeddings to a 512-dimensional space. From each micro-gesture clip we uniformly sample ğ‘‡ â€² = 8 frames. Formally, for frame ğ‘¡ we obtain the token sequence:\n\nDuring training we freeze the first 10 of the 12 ViT blocks and fine-tune only the last two blocks together with our added components  [15] . Temporal information is aggregated by average-pooling the eight [CLS] embeddings to produce the per-clip visual feature.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Skeleton Encoder",
      "text": "We use the OpenPose format  [16]  to extract skeleton features. Given a clip, we first sample ğ‘‡ â€² = 32 pose frames:\n\nğ‘– âˆˆ R 18Ã—2 . To stay time-aligned with the eight RGB frames, the 32 pose frames are grouped into eight nonoverlapping four-frame windows centered on ğ‘¡ 1 , . . . , ğ‘¡ 8 . The heat-maps of each window are averagepooled along the temporal axis. This results in eight pose volumes that correspond one-to-one with the RGB inputs.\n\nEach joint is then rasterized into a 256 Ã— 256 canvas as a 2D Gaussian  [17] :\n\nwhere (ğ‘¥\n\nğ‘› ) is the ğ‘›-th joint of frame ğ‘¡. Stacking all joints and time-steps yields a 4D tensor\n\nUtilizing an implementation similar to Tessa  [18] , we then employ a three-stage 3x3x3 convolutional network (with channel depths of 64, 128, and 256) to encode subtle pose dynamics  [19] . We apply downsampling only in the spatial dimensions to preserve the motion details over time.\n\nGlobal average pooling over (ğ‘‡ â€² , ğ», ğ‘Š ) produces a 256-dimensional clip descriptor â„ (ğ‘) . A linear projection:\n\nmaps it to ğ‘‘ = 512 so the pose feature matches the CLIP visual dimension ğ· = 512:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Pose-Guided Semantic Query Generation",
      "text": "The proposed semantic-query approach extracts a representation of the video's most important cues with the support of the skeleton features. It does so (1) spatially, by concentrating on visual tokens that are near body parts exhibiting motion, and (2) temporally, by giving higher weight to frames where the pose dynamics show that a micro-gesture is taking place. Let ğ‘§ ğ‘¡,ğ‘ be the set of patch embeddings from all selected frames (excluding the global tokens). We first identify a subset of these visual tokens that are relevant to the micro-gesture. \"Pose guidance\" is applied by using the skeleton features to weight or select visual tokens:\n\nâ€¢ We compute an attention mask over image patches based on the distance of each patch to the nearest skeletal joint position. If a patch lies close to a joint that is moving significantly, it receives a higher weight. For example, if ğ‘— ğ‘¡,ğ‘˜ are the coordinates of joint ğ‘˜ in frame ğ‘¡, we can define a relevance score:\n\nwhere pos(ğ‘§ ğ‘¡,ğ‘ ) is the spatial location of patch ğ‘ and ğœ controls the spatial scale. This yields weights ğ‘¤ ğ‘¡,ğ‘ âˆˆ [0, 1] that highlight patches near active joints. â€¢ Additionally, we leverage the skeleton encoder's output â„ (ğ‘) as a global descriptor of the motion.\n\nWe project â„ (ğ‘) to the same dimension ğ· and use it to modulate the visual tokens via a simple gating: ğ‘§ Ëœğ‘¡,ğ‘ = ğ›¼ ğ‘§ ğ‘¡,ğ‘ ,\n\nwhere ğ›¼ = ğœ (ï¸€ âŸ¨ğ‘Š â„ (ğ‘) , ğ‘§ ğ‘¡,ğ‘ âŸ© )ï¸€ .\n\nHere ğ›¼ is the sigmoid of the dot-product between the projected pose feature ğ‘Š â„ (ğ‘) and the visual token ğ‘§ ğ‘¡,ğ‘ , so it down-weights any token not well aligned with the pose direction.\n\nAfter This pose-weighted query ğ‘ thus encapsulates the most relevant gesture semantics and is passed to the cross-attention component to guide the final feature fusion and classification.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Gated Multi-Modal Fusion",
      "text": "Before feeding the query into the CLIP transformer, we further integrate the pose information via a gated fusion mechanism inspired by Arevalo et al.  [20] . The goal here is to merge the skeleton representation with the visual representation in a way that the model can selectively attend to one or the other modality as needed. We implement gated fusion at two points in the pipeline:\n\nâ€¢ We fuse the skeleton encoder output â„ (ğ‘) with the semantic query ğ‘. First we compute a gating vector\n\nwhere ğ‘Š ğ‘” âˆˆ R ğ·Ã—ğ‘‘ is a learned projection and ğœ is the sigmoid. We then modulate the query by\n\nIn our implementation ğ‘” is element-wise, so each feature of ğ‘ is scaled into [0, 1], allowing pose-aligned dimensions to be amplified or suppressed. â€¢ Similarly, we fuse the pose descriptor â„ (ğ‘) into the CLIP encoder's intermediate token sequence.\n\nLet ğ¹ = { ğ‘“ 1 , ğ‘“ 2 , . . . , ğ‘“ ğ‘ } be the set of visual features from CLIP's penultimate layer (these serve as keys and values in cross-attention). We then compute a second gating vector\n\nand apply it element-wise:\n\nThis global gate highlights or suppresses certain channels based on pose.\n\nThese gating operations are learned end-to-end and ensure that the multi-modal information is blended before the cross-attention step. The gating is soft (continuous values between 0 and 1), so the model can learn to rely on pose heavily in some scenarios or ignore it in others. This adaptability is important because pose data can sometimes be noisy or incomplete (e.g., occluded joints), so a static fusion might hurt performance if pose is trusted blindly. Our gated fusion allows the network to fall back to visual cues when pose is uncertain, and vice versa.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Cross-Attention With Semantic Query",
      "text": "Next, we apply a cross-attention mechanism driven by our pose-guided query. We insert the query vector ğ‘ ğ‘“ as an extra token into the final transformer layer of the CLIP visual encoder, allowing it to attend over the gated visual token set ğ¹ . This focused attention refines the representation by pooling features most relevant to the detected micro-gesture.\n\nConsider the transformer architecture of CLIP's visual encoder. Let\n\nWe then insert our pose-guided query ğ‘ ğ‘“ into the final layer and compute cross-attention:\n\nThe cross-attention computes an output query embedding ğ‘ out that is a weighted sum of the values ğ‘‰ , with weights determined by the compatibility of ğ‘„ with keys ğ¾. Mathematically, if we denote ğ‘„ (1Ã—D), ğ¾ (NÃ—D), ğ‘‰ (NÃ—D), the attention is:\n\nwhere the softmax produces a 1 Ã— ğ‘ vector of attention weights. The resulting ğ‘ out (of dimension 1Ã—D) is effectively a semantic-aware video representation that has \"pooled\" information from the visual tokens, biased by the semantic content of ğ‘„ and thereby by the pose cues we injected. In other words, ğ‘ out should ideally encode the crucial features needed to distinguish the micro-gesture class.\n\nThis unique attention mechanism forces the model to concentrate on what is important for the gesture. It acts as a form of feature selection: among the many visual features of a scene (some possibly irrelevant background or person identity cues), it emphasizes those that correlate with the action semantics. In our case, because ğ‘„ was guided by pose, the attention is further narrowed to regions of actual motion or posture change.\n\nAfter cross-attention, we obtain ğ‘ out which we consider as the fused video representation for the whole clip.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Classification And Training Objective",
      "text": "The final stage is the classification of the micro-gesture. We feed the fused representation ğ‘ out (dimension ğ·) into a classifier head, implemented as a simple two-layer MLP followed by softmax. This yields a probability distribution ğ‘¦ ^âˆˆ R ğ¶ over the ğ¶ gesture classes (here ğ¶ = 33 for iMiGUE). We train the model using a supervised classification objective. The primary loss is the cross-entropy between the predicted distribution and the ground-truth label. Given a training sample ğ‘– with true class label ğ‘¦ ğ‘– (represented as a one-hot vector) and predicted probabilities ğ‘¦ ^ğ‘–, the loss is:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Reference Method",
      "text": "Top-1 (%)  [21]  GCN + Skeleton (ST-GCN) 46.38  [22]  Multi-scale GCN + Skeleton (MS-G3D) 52.00  [23]  Temporal Relational + RGB (TRN) 55.24  [24]  Temporal Shift + RGB (TSM) 58.77  [19]  3D CNN + Skeleton Heatmaps (PoseConv3D) 61.11  [25]  Vision Transformer + RGB (Video Swin-B) 61.73  [26]  Dense-Sparse Fusion + RGB+Skeleton (DSCNet) 62.50  [11]  CLIP Distillation + Skeleton 3DCNN 68.90  [10]  Multi-scale Ensemble + RGB+Skeleton (M2HEN) 70.19  [9]  Prototype-based GCN + Skeleton 70.25 -Pose-guided Semantic Attention + CLIP + Skeleton (CLIP-MG, ours) 61.82\n\nwhere ğ‘ is the number of training examples in a batch and ğ‘¦ ğ‘–,ğ‘ âˆˆ 0, 1, âˆ‘ï¸€ ğ‘ ğ‘¦ ğ‘–,ğ‘ = 1. We minimize this loss with respect to the parameters of the skeleton encoder, the fusion components, the classifier, and the parts of the CLIP encoder we allow to be fine-tuned.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dataset And Evaluation Protocol",
      "text": "We conduct experiments on the iMiGUE dataset, focusing on the micro-gesture classification task. As described earlier, iMiGUE contains 33 micro-gesture classes collected from interview videos of tennis players. These gestures include subtle body-language cues such as pressing lips or touching one's jaw. We set aside a portion of the training data (20%) to serve as a local validation set for our experiments. This local validation is used for model selection and ablation studies due to the unavailability of a separate testing environment at the time of experimentation. However, final results on the test set are referenced for comparison with other approaches  [8, 7] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results And Comparisons",
      "text": "As shown in Table  1 , CLIP-MG achieves a Top-1 accuracy of 61.82%, outperforming a range of singlemodality baselines  [21, 22, 23, 24] . Notably, CLIP-MG even performs on par with standalone architectures such as Video Swin-B  [25]  and PoseConv3D  [19]  despite using a largely frozen CLIP backbone and a compact pose encoder. This shows that steering CLIP's attention with pose-guided semantic queries yields more discriminative features for fine-grained micro-gestures. However, the proposed architecture does not set a new state-of-the-art in this area. It comes close in performance to the dense-sparse fusion network DSCNet  [26]  (62.50%), but falls behind architectures presented in previous editions of the MiGA challenge  [9, 10, 11] . These results motivate future work to explore richer query adaptation and improved temporal fusion to close the gap with top models.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "We performed comprehensive ablation experiments to validate the contribution of each component in CLIP-MG. Table  2  reports Top-1 accuracy on our validation split.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Without Pose Branch",
      "text": "Here, we completely eliminate the pose branch to see the benefit of adding pose at all. The model gave 45.30% (-16.52 pp) accuracy. Thus, adding the pose branch (with our fusion and guidance) yields a 16.52% gain, which demonstrates that skeleton data carries complementary information for the task.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Without Pose Guidance",
      "text": "In this variant, we remove the pose influence from the semantic query generation. The query is generated purely by clustering visual tokens without using skeleton data. The semantics-based cross-attention still operates, but only on the visual-based query. We found that the accuracy dropped to 51.23% (-10.59 pp). This confirms that pose guidance is essential and provides a significant boost. This makes sense because, in theory, without pose the model may attend to irrelevant semantics or background context. This misses subtle gesture cues.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Without Semantic Cross-Attention",
      "text": "Here, we skip SCCA. Instead, we simply concatenate the global visual [CLS] embedding with the pose feature and feed that to a classifier. This essentially tests a late-fusion approach without our semantic query mechanism. The accuracy was 53.17% (-8.65 pp). This indicates that the semantic query and cross-attention are effective at focusing on important features that a flat concatenation would miss.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Without Gated Fusion",
      "text": "In this ablation, we disable the gating in both the query generation and the visual token modulation. We still generate a query using pose (via simple concatenation of average visual token and pose feature) and perform cross-attention. The accuracy achieved we 60.08% (-1.74 pp), a modest drop. This shows that gating helps but is not as critical as the presence of pose info or semantics-based cross-attention. The gating mostly fine-tunes the balance between modalities.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "These ablations show that each component of CLIP-MG plays a supporting role, although certain components are more important than others. Dropping the entire pose branch drives accuracy down to 45.30% (-16.52 pp). This demonstrates how much discriminative information there is within the skeletal signal. Removing pose guidance lowers accuracy from 61.82% to 51.23% (-10.59 pp), showing that skeletal information is very important for localizing subtle joint motions, as they act as an attention prior  [27]  that focuses the visual stream towards the regions where micro-gestures occur. Eliminating cross-attention drops accuracy to 53.17% (-8.65 pp), which indicates that without a mechanism to selectively pool pose-weighted tokens, the model may struggle to tell apart very similar gestures. Finally, disabling gated fusion yields 60.08% (-1.74 pp), which indicates that adaptively balancing pose and visual information slightly improves the robustness of the architecture.\n\nTaken together, these results show how the different components effectively complement each other. Pose cues localize the gesture, cross-attention extracts the relevant semantics, and gating balances both streams. We find the highest performance when all the components are combined.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusions And Future Works",
      "text": "We introduced CLIP-MG, a pose-guided, multi-modal CLIP architecture for micro-gesture recognition on the iMiGUE benchmark. By guiding CLIP's visual attention with skeleton-based spatial priors, generating compact semantic queries, and fusing pose and appearance via a learnable gate, CLIP-MG extracts subtle, discriminative features that simple RGB or pose-only models cannot recognize. Our model achieves 61.82% Top-1 accuracy, outperforming most single-modality baselines and performing on par with strong 3D-CNN and vision-transformer approaches. Extensive ablation studies confirm that each component provides a measurable benefit. The experiments provide insights into how each component interacts with and complements others, which highlights important design patterns that can inform future model development in micro-gesture classification and similar fine-grained recognition tasks. Our findings demonstrate the value of integrating multimodal and semantic information to address challenging visual recognition problems.\n\nOur future work will explore richer temporal approaches and data strategies to close the gap between CLIP-MG and more recent state-of-the-art models  [9] . First, integrating sequence models (temporal transformers or recurrent layers over cross-attention outputs) should capture patterns that static sampling loses. Second, video motion magnification  [28]  could amplify imperceptible movements. This would help with pose tracking and visual encoding. Third, joint pre-training on related actiongesture datasets and weakly-or self-supervised learning could improve feature robustness  [29] . Finally, regarding accuracy, we plan to incorporate uncertainty-aware gating for noisy skeletons and classbalanced or prototype-based calibrations to address long-tail imbalance  [30] . To improve and better evaluate the explainability of the model, we will incorporate gradient-weighted class activation mapping (Grad-CAM)  [31]  and more recent attention-aware token-filtering approaches  [32] . Currently, the proposed architecture suffers heavily due to its relatively low speed on commodity hardware. To address this issue, we plan to experiment with several multimodal compression and optimization algorithms for more efficient computing  [33, 34, 35, 36] . We plan to train an improved version of our architecture for downstream tasks on the DAIC-WoZ dataset  [5, 37, 38]  for low-level mental health analysis. These different research directions are promising in pushing pose-guided CLIP models closer to (and beyond) human-level understanding of the subtlest gestures.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the proposed architecture for micro-gesture classification. The CLIP-MG pipeline",
      "page": 3
    },
    {
      "caption": "Figure 1: ) has several components working in sequence: (1) a visual encoder",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CLIP-MG: Guiding Semantic Attention with Skeletal Pose": "Features and RGB Data for Micro-Gesture Recognition on"
        },
        {
          "CLIP-MG: Guiding Semantic Attention with Skeletal Pose": "the iMiGUE Dataset"
        },
        {
          "CLIP-MG: Guiding Semantic Attention with Skeletal Pose": "Santosh Patapati1, Trisanth Srinivasan1 and Amith Adiraju1"
        },
        {
          "CLIP-MG: Guiding Semantic Attention with Skeletal Pose": "1Cyrion Labs, Texas, United States"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Santosh Patapati1, Trisanth Srinivasan1 and Amith Adiraju1": "1Cyrion Labs, Texas, United States"
        },
        {
          "Santosh Patapati1, Trisanth Srinivasan1 and Amith Adiraju1": "Abstract"
        },
        {
          "Santosh Patapati1, Trisanth Srinivasan1 and Amith Adiraju1": "Micro-gesture recognition is a challenging task in affective computing due to the subtle, involuntary nature of"
        },
        {
          "Santosh Patapati1, Trisanth Srinivasan1 and Amith Adiraju1": "the gestures and their low movement amplitude. In this paper, we introduce a Pose-Guided Semantics-Aware"
        },
        {
          "Santosh Patapati1, Trisanth Srinivasan1 and Amith Adiraju1": "CLIP-based architecture, or CLIP for Micro-Gesture recognition (CLIP-MG), a modified CLIP model tailored for"
        },
        {
          "Santosh Patapati1, Trisanth Srinivasan1 and Amith Adiraju1": "micro-gesture classification on the iMiGUE dataset. CLIP-MG integrates human pose (skeleton) information into"
        },
        {
          "Santosh Patapati1, Trisanth Srinivasan1 and Amith Adiraju1": "the CLIP-based recognition pipeline through pose-guided semantic query generation and a gated multi-modal"
        },
        {
          "Santosh Patapati1, Trisanth Srinivasan1 and Amith Adiraju1": "fusion mechanism. The proposed model achieves a Top-1 accuracy of 61.82%. These results demonstrate both the"
        },
        {
          "Santosh Patapati1, Trisanth Srinivasan1 and Amith Adiraju1": "potential of our approach and the remaining difficulty in fully adapting vision-language models like CLIP for"
        },
        {
          "Santosh Patapati1, Trisanth Srinivasan1 and Amith Adiraju1": "micro-gesture recognition."
        },
        {
          "Santosh Patapati1, Trisanth Srinivasan1 and Amith Adiraju1": "Keywords"
        },
        {
          "Santosh Patapati1, Trisanth Srinivasan1 and Amith Adiraju1": "Micro-gesture recognition, Vision-language models, CLIP adaptation, Pose-guided fusion, Multi-modal deep"
        },
        {
          "Santosh Patapati1, Trisanth Srinivasan1 and Amith Adiraju1": "learning, Semantic query generation, Human pose estimation, Affective computing"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "In this paper, we present CLIP-MG, a novel multi-modal framework for micro-gesture classification on": "iMiGUE. Our approach builds upon previous work by incorporating pose (skeleton) data in a principled"
        },
        {
          "In this paper, we present CLIP-MG, a novel multi-modal framework for micro-gesture classification on": "way. The main contributions are summarized as follows:"
        },
        {
          "In this paper, we present CLIP-MG, a novel multi-modal framework for micro-gesture classification on": "1. We develop a system that uses human pose cues to help guide the semantic query extraction from"
        },
        {
          "In this paper, we present CLIP-MG, a novel multi-modal framework for micro-gesture classification on": "video frames. The skeleton information helps focus the CLIP visual encoder on the regions of"
        },
        {
          "In this paper, we present CLIP-MG, a novel multi-modal framework for micro-gesture classification on": "subtle motion. This creates a semantic query embedding that is rich with features relevant to"
        },
        {
          "In this paper, we present CLIP-MG, a novel multi-modal framework for micro-gesture classification on": "pose."
        },
        {
          "In this paper, we present CLIP-MG, a novel multi-modal framework for micro-gesture classification on": "2. We introduce a gated fusion mechanism to combine visual and skeleton representations effectively."
        },
        {
          "In this paper, we present CLIP-MG, a novel multi-modal framework for micro-gesture classification on": "Our gated fusion learns to weight and integrate the two modalities. This allows pose features to"
        },
        {
          "In this paper, we present CLIP-MG, a novel multi-modal framework for micro-gesture classification on": "adaptively modulate the visual features before and during the cross-attention process."
        },
        {
          "In this paper, we present CLIP-MG, a novel multi-modal framework for micro-gesture classification on": "3. We extend CLIP to a multi-modal setting. The pose-based query is fed into the CLIP transformer"
        },
        {
          "In this paper, we present CLIP-MG, a novel multi-modal framework for micro-gesture classification on": "for cross-attention over semantically significant visual token features. This limits the model to"
        },
        {
          "In this paper, we present CLIP-MG, a novel multi-modal framework for micro-gesture classification on": "attend to parts relevant to gesture. This results in a fused representation that has both semantic"
        },
        {
          "In this paper, we present CLIP-MG, a novel multi-modal framework for micro-gesture classification on": "and motion-specific information for classification."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "ablation studies to quantify how much each proposed component improves performance. The"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "results of our ablation studies provide insights for future researchers and future research directions."
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "2. Related Works"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "2.1. The iMiGUE Dataset"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "Recent progress in this area has been driven by the introduction of specialized datasets for micro-gesture"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "understanding.\nIn particular,\niMiGUE is a large-scale video dataset introduced by Liu et al.\n[6] for"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "identity-free micro-gesture understanding and emotion analysis. The iMiGUE dataset consists of video"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "footage of tennis players during post-match interviews, with detailed frame-level annotations of various"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "micro-gestures. The dataset contains 72 subjects (split\ninto 37 for training and 35 for testing in a"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "cross-subject protocol) and a total of 33 gesture classes."
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "One thing to note is that the class distribution is highly imbalanced. 28 of the 33 classes are tail classes"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "with relatively few samples, meaning they collectively only make up less than 60% of the data. This"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "long-tailed distribution, combined with the subtlety and high intra-class variability of micro-gestures,"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "makes the recognition task extremely challenging [7]."
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "2.2. Signal Processing & Machine Learning Techniques for Micro-Gesture"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "Recognition"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "To tackle these challenges, the research community has organized the Micro-Gesture Analysis (MiGA)"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "challenges in recent years [7, 8]. These competitions have spurred the development of novel multi-"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "modal approaches that leverage both video (RGB) and skeleton (pose) modalities for micro-gesture"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "recognition.\nIn the 2024 MiGA Challenge, for example, all top-performing methods integrated pose"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "information alongside RGB frames. The winning entry by Chen et al.\nintroduced a prototype-based"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "learning approach with a two-stream 3D CNN (PoseConv3D) backbone for RGB and pose, cross-modal"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "attention fusion, and a prototypical refinement component to calibrate ambiguous samples [9]. This"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "method achieved a Top-1 accuracy of 70.25% on the iMiGUE test set, substantially outperforming earlier"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "approaches. The second-place method by Huang et al. proposed a multi-scale heterogeneous ensemble"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "network (M2HEN) combining a 3D convolutional model and a Transformer for feature diversity, reaching"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "70.19% accuracy [10]. Another notable approach by Wang et al.\nleveraged the vision-language model"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "CLIP: they used a frozen CLIP as a teacher network for RGB frames and injected CLIP-derived text"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "embeddings into a pose-based model, achieving 68.9% accuracy with an ensemble of RGB, joint, and"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "limb pose streams. These efforts demonstrate that multi-modal fusion and semantic knowledge transfer"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "are highly important in improving micro-gesture recognition [11]."
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "2.3. CLIP-Based Video Understanding"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "Meanwhile, in the broader action recognition field, researchers have explored using pre-trained vision-"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "language models like CLIP for video understanding. CLIP (Contrastive Language-Image Pre-training)"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "[12] has shown powerful visual feature representations aligned with semantics via natural language"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "supervision. However, straightforward fine-tuning of CLIP on video data can neglect smaller semantic"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "information. To address this, Quan et al. proposed Semantic-Constrained CLIP (SC-CLIP) [13]. SC-CLIP"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "adapts CLIP to video by generating a compact semantic query from dense visual tokens and using"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "cross-attention to refocus the model on those action-relevant semantics. This \"constrains\" CLIPâ€™s"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "attention to discriminative features and yields stronger zero-shot and fine-grained recognition."
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "SC-CLIP demonstrates that directing attention to semantically meaningful regions can improve"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "fine-grained video understanding. Micro-gestures, despite being low in their extent of movement, still"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "take place with subtle visual semantics. Skeleton key-points give precise spatial-temporal anchors"
        },
        {
          "4. We evaluate CLIP-MG on the iMiGUE micro-gesture dataset. We additionally perform numerous": "(hands, face, shoulders) that show where and when these cues take place. Thus, we design a pose-guided"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Overview of the proposed architecture for micro-gesture classification. The CLIP-MG pipeline": "integrates a frozen CLIP ViT-B/16 visual encoder and an OpenPose-based skeleton encoder to generate"
        },
        {
          "Figure 1: Overview of the proposed architecture for micro-gesture classification. The CLIP-MG pipeline": "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated"
        },
        {
          "Figure 1: Overview of the proposed architecture for micro-gesture classification. The CLIP-MG pipeline": "multimodal fusion and cross-attention mechanism then blend the visual and pose features before a"
        },
        {
          "Figure 1: Overview of the proposed architecture for micro-gesture classification. The CLIP-MG pipeline": "simple classification head predicts the micro-gesture label."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated": "multimodal fusion and cross-attention mechanism then blend the visual and pose features before a"
        },
        {
          "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated": ""
        },
        {
          "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated": ""
        },
        {
          "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated": ""
        },
        {
          "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated": ""
        },
        {
          "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated": "Our model (illustrated in Figure 1) has several components working in sequence: (1) a visual encoder"
        },
        {
          "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated": ""
        },
        {
          "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated": ""
        },
        {
          "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated": ""
        },
        {
          "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated": "In the"
        },
        {
          "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated": ""
        },
        {
          "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated": ""
        },
        {
          "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated": ""
        },
        {
          "pose-guided semantic queries. This focuses attention on image patches relevant to gestures. A gated": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "a 512-dimensional space. From each micro-gesture clip we uniformly sample ğ‘‡ â€² = 8 frames. Formally,"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ""
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ". . . , ğ‘§ğ‘¡,196 },\nğ‘ğ‘¡ = { ğ‘§ğ‘¡,CLS, ğ‘§ğ‘¡,1,\nğ‘§ğ‘¡,CLS âˆˆ R768 ."
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "During training we freeze the first 10 of the 12 ViT blocks and fine-tune only the last two blocks"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "together with our added components [15]. Temporal information is aggregated by average-pooling the"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ""
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ""
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ""
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ""
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "ğ‘‹ (ğ‘) = {ï¸€ğ‘¥(ğ‘)\n,\n. . . , ğ‘¥(ğ‘)\n}ï¸€,\nğ‘¥(ğ‘)\nâˆˆ R18Ã—2 ."
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "1\n32\nğ‘–"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "To stay time-aligned with the eight RGB frames, the 32 pose frames are grouped into eight non-"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "overlapping four-frame windows centered on ğ‘¡1, . . . , ğ‘¡8. The heat-maps of each window are average-"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "pooled along the temporal axis. This results in eight pose volumes that correspond one-to-one with the"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ""
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ""
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "(ï¸ƒ\n)ï¸ƒ"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "(ğ‘– âˆ’ ğ‘¥(ğ‘¡)\n)2\nğ‘› )2 + (ğ‘— âˆ’ ğ‘¦(ğ‘¡)"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "ğ» (ğ‘¡)\nğœ = 2.5 px,"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ",\nğ‘›,ğ‘–,ğ‘— = exp"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "2 ğœ2"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ") is the ğ‘›-th joint of frame ğ‘¡. Stacking all joints and time-steps yields a 4D tensor"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "â„‹ âˆˆ R8Ã—18Ã—256Ã—256 ."
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "Utilizing an implementation similar to Tessa [18], we then employ a three-stage 3x3x3 convolutional"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "network (with channel depths of 64, 128, and 256) to encode subtle pose dynamics [19]. We apply"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ""
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "Global average pooling over (ğ‘‡ â€², ğ», ğ‘Š ) produces a 256-dimensional clip descriptor â„(ğ‘). A linear"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ""
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "ğ‘Šğ‘ âˆˆ R512Ã—256"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ""
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "Ëœ(ğ‘)"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "â„\n= ğ‘Šğ‘ â„(ğ‘) âˆˆ R512 ."
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ""
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ""
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "It does so (1) spatially, by concentrating on visual tokens that"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "are near body parts exhibiting motion, and (2) temporally, by giving higher weight to frames where the"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ""
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "Let ğ‘§ğ‘¡,ğ‘ be the set of patch embeddings from all selected frames (excluding the global tokens). We"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "first identify a subset of these visual tokens that are relevant to the micro-gesture. \"Pose guidance\" is"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": ""
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "â€¢ We compute an attention mask over image patches based on the distance of each patch to the"
        },
        {
          "transformer width is 768 dimensions, while CLIPâ€™s projection head maps the final [CLS] embeddings to": "nearest skeletal joint position. If a patch lies close to a joint that is moving significantly, it receives"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ëœ": "ğ‘– = ğ‘“ğ‘– âŠ™ ğ‘¢,"
        },
        {
          "Ëœ": "This global gate highlights or suppresses certain channels based on pose."
        },
        {
          "Ëœ": "These gating operations are learned end-to-end and ensure that the multi-modal"
        },
        {
          "Ëœ": "blended before the cross-attention step. The gating is soft (continuous values between 0 and 1), so the"
        },
        {
          "Ëœ": "model can learn to rely on pose heavily in some scenarios or ignore it in others. This adaptability is"
        },
        {
          "Ëœ": "important because pose data can sometimes be noisy or incomplete (e.g., occluded joints), so a static"
        },
        {
          "Ëœ": "fusion might hurt performance if pose is trusted blindly. Our gated fusion allows the network to fall"
        },
        {
          "Ëœ": "back to visual cues when pose is uncertain, and vice versa."
        },
        {
          "Ëœ": "3.5. Cross-Attention with Semantic Query"
        },
        {
          "Ëœ": ""
        },
        {
          "Ëœ": "vector ğ‘ğ‘“ as an extra token into the final transformer layer of the CLIP visual encoder, allowing it to"
        },
        {
          "Ëœ": "attend over the gated visual token set ğ¹ . This focused attention refines the representation by pooling"
        },
        {
          "Ëœ": "features most relevant to the detected micro-gesture."
        },
        {
          "Ëœ": "Consider the transformer architecture of CLIPâ€™s visual encoder. Let"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1": "Comparison of prior micro-gesture classifiers."
        },
        {
          "Table 1": "Reference"
        },
        {
          "Table 1": "[21]"
        },
        {
          "Table 1": "[22]"
        },
        {
          "Table 1": "[23]"
        },
        {
          "Table 1": "[24]"
        },
        {
          "Table 1": "[19]"
        },
        {
          "Table 1": "[25]"
        },
        {
          "Table 1": "[26]"
        },
        {
          "Table 1": "[11]"
        },
        {
          "Table 1": "[10]"
        },
        {
          "Table 1": "[9]"
        },
        {
          "Table 1": "â€”"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2": "Ablation study on CLIP-MG components"
        },
        {
          "Table 2": "Varian"
        },
        {
          "Table 2": "w/o Pose branch"
        },
        {
          "Table 2": "w/o Pose guidance"
        },
        {
          "Table 2": "w/o Cross-attention"
        },
        {
          "Table 2": "w/o Gated fusion"
        },
        {
          "Table 2": "Full model"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Taken together, these results show how the different components effectively complement each other.": "Pose cues localize the gesture, cross-attention extracts the relevant semantics, and gating balances both"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "streams. We find the highest performance when all the components are combined."
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "5. Conclusions and Future Works"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "We introduced CLIP-MG, a pose-guided, multi-modal CLIP architecture for micro-gesture recognition"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "on the iMiGUE benchmark. By guiding CLIPâ€™s visual attention with skeleton-based spatial priors,"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "generating compact semantic queries, and fusing pose and appearance via a learnable gate, CLIP-MG"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "extracts subtle, discriminative features that simple RGB or pose-only models cannot recognize. Our"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "model achieves 61.82% Top-1 accuracy, outperforming most single-modality baselines and performing"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "on par with strong 3D-CNN and vision-transformer approaches. Extensive ablation studies confirm"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "that each component provides a measurable benefit. The experiments provide insights into how each"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "component interacts with and complements others, which highlights important design patterns that can"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "inform future model development in micro-gesture classification and similar fine-grained recognition"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "tasks. Our findings demonstrate the value of\nintegrating multimodal and semantic information to"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "address challenging visual recognition problems."
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "Our future work will explore richer temporal approaches and data strategies to close the gap between"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "CLIP-MG and more recent state-of-the-art models [9]. First, integrating sequence models (temporal"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "transformers or recurrent\nlayers over cross-attention outputs) should capture patterns that static"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "sampling loses.\nSecond, video motion magnification [28] could amplify imperceptible movements."
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "This would help with pose tracking and visual encoding. Third, joint pre-training on related action-"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "gesture datasets and weakly- or self-supervised learning could improve feature robustness [29]. Finally,"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "regarding accuracy, we plan to incorporate uncertainty-aware gating for noisy skeletons and class-"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "balanced or prototype-based calibrations to address long-tail imbalance [30]. To improve and better"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "evaluate the explainability of the model, we will incorporate gradient-weighted class activation mapping"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "(Grad-CAM) [31] and more recent attention-aware token-filtering approaches [32]. Currently,\nthe"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "proposed architecture suffers heavily due to its relatively low speed on commodity hardware. To"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "address this issue, we plan to experiment with several multimodal compression and optimization"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "algorithms for more efficient computing [33, 34, 35, 36]. We plan to train an improved version of our"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "architecture for downstream tasks on the DAIC-WoZ dataset [5, 37, 38] for low-level mental health"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "analysis. These different research directions are promising in pushing pose-guided CLIP models closer"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "to (and beyond) human-level understanding of the subtlest gestures."
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "Declaration on Generative AI"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "During the preparation of this work, the author(s) used GPT-4o to edit the paper, checking for grammar"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "and spelling mistakes. GPT-4o was also utilized to revise the draft for brevity and improved flow. After"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "using this tool, the author(s) reviewed and edited the content as needed and take(s) full responsibility"
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "for the publicationâ€™s content."
        },
        {
          "Taken together, these results show how the different components effectively complement each other.": "References"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "for the publicationâ€™s content.": "References"
        },
        {
          "for the publicationâ€™s content.": "[1]\nJ. F. Cohn, P. Ekman, Observing and coding facial expression of emotion, Handbook of Emotion"
        },
        {
          "for the publicationâ€™s content.": "(2000)."
        },
        {
          "for the publicationâ€™s content.": "[2] M. Funes, et al., Micro-expression recognition: A survey,\nin: FG, 2019."
        },
        {
          "for the publicationâ€™s content.": "[3] M. Pantic, Affective multimedia databases: Affective video databases, Handbook of Affective"
        },
        {
          "for the publicationâ€™s content.": "Computing (2009)."
        },
        {
          "for the publicationâ€™s content.": "[4] A. Kapoor, R. Picard, Automatic prediction of human behavior in social settings,\nin: IUI, 2007."
        },
        {
          "for the publicationâ€™s content.": "[5]\nS. V. Patapati,\nIntegrating large language models into a tri-modal architecture for automated"
        },
        {
          "for the publicationâ€™s content.": "depression classification, 2024. arXiv:2407.19340v5, preprint."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "gesture understanding and emotion analysis,\nin: CVPR, 2021."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[7] C. Haoyu, et al., The 2nd challenge on micro-gesture analysis for hidden emotion understanding"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "(miga) 2024: Dataset and results,\nin: MiGA 2024: Proceedings of IJCAI 2024 Workshop & Challenge"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA 2024) co-located with 33rd"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "International Joint Conference on Artificial Intelligence (IJCAI 2024), 2024."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[8] G. Zhao, et al., The workshop & challenge on micro-gesture analysis for hidden emotion understand-"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "ing (miga),\nin: MiGA 2023: Proceedings of IJCAI 2023 Workshop & Challenge on Micro-gesture"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "Analysis for Hidden Emotion Understanding (MiGA 2023) co-located with 32nd International Joint"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "Conference on Artificial Intelligence (IJCAI 2023), 2023."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[9] G. Chen, et al., Prototype learning for micro-gesture classification,\nin: MiGA 2024: Proceedings of"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "IJCAI 2024 Workshop & Challenge on Micro-gesture Analysis for Hidden Emotion Understanding"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "(MiGA 2024) co-located with 33rd International Joint Conference on Artificial Intelligence (IJCAI"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "2024), 2024."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[10] H. Huang, et al., Multi-modal micro-gesture classification via multi-scale heterogeneous ensemble"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "network,\nin: MiGA 2024: Proceedings of IJCAI 2024 Workshop & Challenge on Micro-gesture"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "Analysis for Hidden Emotion Understanding (MiGA 2024) co-located with 33rd International Joint"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "Conference on Artificial Intelligence (IJCAI 2024), 2024."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[11] Y. Wang, et al., A multimodal micro-gesture classification model based on clip,\nin: MiGA 2024:"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "Proceedings of IJCAI 2024 Workshop & Challenge on Micro-gesture Analysis for Hidden Emotion"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "Understanding (MiGA 2024) co-located with 33rd International Joint Conference on Artificial"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "Intelligence (IJCAI 2024), 2024."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[12] A. Radford, et al., Learning transferable visual models from natural language supervision,\nICML"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "(2021)."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[13] Z. Quan, et al., Semantic matters: A constrained approach for zero-shot video action recognition,"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "in: Pattern Recognition, 2025."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[14] A. Dosovitskiy, et al., An image is worth 16x16 words: Transformers for image recognition at"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "scale,\nin: ICLR, 2021."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[15] X. H., et al., Videoclip: Learning video representations from text and clips, arXiv (2021)."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[16] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, Y. Sheikh, Realtime multi-person 2d pose estimation using"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "part affinity fields,\nin: CVPR, 2017."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[17] X. Zhou, et al., On heatmap representation for 6d pose estimation,\nin: ICCV, 2019."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[18]\nS. V. Patapati, T. Srinivasan, H. Musku, A. Adiraju, A framework for eca-based psychotherapy,"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "2025."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[19]\nJ. Zhang, Z. Huang, Y. Chen,\nPoseconv3d: Revisiting skeleton-based action recognition,\nin:"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "European Conference on Computer Vision (ECCV), 2020."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[20]\nJ. Arevalo, et al., Gated multimodal units for information fusion,\nin: ICLR, 2020."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[21]\nS. Yan, Y. Xiong, D. Lin, Spatial temporal graph convolutional networks for skeleton-based action"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "recognition,\nin: AAAI Conference on Artificial Intelligence, 2018."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[22]\nS. Liu, et al., MS-G3D: Multi-scale graph convolution for skeleton-based action recognition,\nin:"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[23] B. Zhou, A. Andonian, A. Oliva, A. Torralba, Temporal relational reasoning in videos,\nin: European"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "Conference on Computer Vision (ECCV), 2018."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[24]\nJ. Lin, C. Gan, S. Han, TSM: Temporal shift module for efficient video understanding,\nin: IEEE/CVF"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "International Conference on Computer Vision (ICCV), 2019."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "in:\n[25] Z. Liu, et al., Video swin transformer: Hierarchical vision transformer for video recognition,"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "IEEE/CVF International Conference on Computer Vision (ICCV), 2022."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[26] Q. Cheng, et al., DSCNet: Dense-sparse complementary network for human action recognition,"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "Expert Systems with Applications (2024)."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[27] H. Zhang, et al., Look closer to see better: Recurrent attention convolutional neural network for"
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "fine-grained image recognition, CVPR (2019)."
        },
        {
          "[6] X. Liu, H. Shi, H. Chen, Z. Yu, X. Li, G. Zhao,\nimigue: An identity-free video dataset for micro-": "[28] N. Wadhwa, et al., Eulerian video magnification for revealing subtle changes in the world,\nin:"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SIGGRAPH, 2013.": "[29] T. Han, et al., Self-supervised video representation learning with neighborhood context aggregation,"
        },
        {
          "SIGGRAPH, 2013.": "ECCV (2020)."
        },
        {
          "SIGGRAPH, 2013.": "[30] B. Kang, et al., Decoupling representation and classifier for long-tail recognition,\nin: ICLR, 2020."
        },
        {
          "SIGGRAPH, 2013.": "[31] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra, Grad-cam: Visual explana-"
        },
        {
          "SIGGRAPH, 2013.": "tions from deep networks via gradient-based localization,\nin: 2017 IEEE International Conference"
        },
        {
          "SIGGRAPH, 2013.": "on Computer Vision (ICCV), IEEE, 2017, pp. 618â€“626."
        },
        {
          "SIGGRAPH, 2013.": "[32] T. Naruko, H. Akutsu, Speed-up of vision transformer models by attention-aware token filtering,"
        },
        {
          "SIGGRAPH, 2013.": "2025. arXiv:2506.01519v1, preprint."
        },
        {
          "SIGGRAPH, 2013.": "[33] Y. Omri, P. Shroff, T. Tambe, Token sequence compression for efficient multimodal computing,"
        },
        {
          "SIGGRAPH, 2013.": "2025. arXiv:2504.17892v1."
        },
        {
          "SIGGRAPH, 2013.": "[34] L. Lei,\nJ. Gu, X. Ma, C. Tang,\nJ. Chen, T. Xu, Generic token compression in multimodal\nlarge"
        },
        {
          "SIGGRAPH, 2013.": "language models from an explainability perspective, 2025. arXiv:2506.01097."
        },
        {
          "SIGGRAPH, 2013.": "[35] X. Tan, P. Ye, C. Tu, J. Cao, Y. Yang, L. Zhang, D. Zhou, T. Chen, Tokencarve: Information-preserving"
        },
        {
          "SIGGRAPH, 2013.": "visual token compression in multimodal large language models, 2025. arXiv:2503.10501."
        },
        {
          "SIGGRAPH, 2013.": "[36]\nJ. Cao, P. Ye, S. Li, C. Yu, Y. Tang, J. Lu, T. Chen, Madtp: Multimodal alignment-guided dynamic"
        },
        {
          "SIGGRAPH, 2013.": "token pruning for accelerating vision-language transformer,\nin: Proceedings of the IEEE/CVF"
        },
        {
          "SIGGRAPH, 2013.": "Conference on Computer Vision and Pattern Recognition (CVPR), 2024, p. â€“. doi:10.1109/CVPR."
        },
        {
          "SIGGRAPH, 2013.": "2024.00XX."
        },
        {
          "SIGGRAPH, 2013.": "[37]\nJ. Gratch, R. Artstein, G. Lucas, G. Stratou, S. Scherer, A. Nazarian, R. Wood, J. Boberg, D. DeVault,"
        },
        {
          "SIGGRAPH, 2013.": "S. Marsella, D. Traum, S. Rizzo, L.-P. Morency, The distress analysis interview corpus of human"
        },
        {
          "SIGGRAPH, 2013.": "and computer interviews,\nin: Proceedings of the Ninth International Conference on Language"
        },
        {
          "SIGGRAPH, 2013.": "Resources and Evaluation (LREC), European Language Resources Association (ELRA), 2014, pp."
        },
        {
          "SIGGRAPH, 2013.": "3123â€“3128."
        },
        {
          "SIGGRAPH, 2013.": "[38]\nF. Ringeval, B. Schuller, M. Valstar, N. Cummins, R. Cowie, L. Tavabi, M. Schmitt, S. Alisamir,"
        },
        {
          "SIGGRAPH, 2013.": "S. Amiriparian, E.-M. Messner, S. Song, S. Liu, Z. Zhao, A. Mallol-Ragolta, Z. Ren, M. Soleymani,"
        },
        {
          "SIGGRAPH, 2013.": "M. Pantic, Avec 2019 workshop and challenge: State-of-mind, detecting depression with ai, and"
        },
        {
          "SIGGRAPH, 2013.": "cross-cultural affect recognition,\nin: Proceedings of the 9th International on Audio/Visual Emotion"
        },
        {
          "SIGGRAPH, 2013.": "Challenge and Workshop, AVEC â€™19, Association for Computing Machinery, New York, NY, USA,"
        },
        {
          "SIGGRAPH, 2013.": "2019, p. 3â€“12. URL: https://doi.org/10.1145/3347320.3357688. doi:10.1145/3347320.3357688."
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Observing and coding facial expression of emotion, Handbook of Emotion",
      "authors": [
        "J Cohn",
        "P Ekman"
      ],
      "year": "2000",
      "venue": "Observing and coding facial expression of emotion, Handbook of Emotion"
    },
    {
      "citation_id": "2",
      "title": "Micro-expression recognition: A survey",
      "authors": [
        "M Funes"
      ],
      "year": "2019",
      "venue": "Micro-expression recognition: A survey"
    },
    {
      "citation_id": "3",
      "title": "Affective multimedia databases: Affective video databases, Handbook of Affective Computing",
      "authors": [
        "M Pantic"
      ],
      "year": "2009",
      "venue": "Affective multimedia databases: Affective video databases, Handbook of Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Automatic prediction of human behavior in social settings",
      "authors": [
        "A Kapoor",
        "R Picard"
      ],
      "year": "2007",
      "venue": "Automatic prediction of human behavior in social settings"
    },
    {
      "citation_id": "5",
      "title": "Integrating large language models into a tri-modal architecture for automated depression classification",
      "authors": [
        "S Patapati"
      ],
      "year": "2024",
      "venue": "Integrating large language models into a tri-modal architecture for automated depression classification",
      "arxiv": "arXiv:2407.19340v5"
    },
    {
      "citation_id": "6",
      "title": "imigue: An identity-free video dataset for microgesture understanding and emotion analysis",
      "authors": [
        "X Liu",
        "H Shi",
        "H Chen",
        "Z Yu",
        "X Li",
        "G Zhao"
      ],
      "year": "2021",
      "venue": "imigue: An identity-free video dataset for microgesture understanding and emotion analysis"
    },
    {
      "citation_id": "7",
      "title": "The 2nd challenge on micro-gesture analysis for hidden emotion understanding (miga) 2024: Dataset and results",
      "authors": [
        "C Haoyu"
      ],
      "venue": "MiGA 2024: Proceedings of IJCAI 2024 Workshop & Challenge on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA 2024) co-located with 33rd International Joint Conference on Artificial Intelligence (IJCAI 2024)"
    },
    {
      "citation_id": "8",
      "title": "The workshop & challenge on micro-gesture analysis for hidden emotion understanding (miga)",
      "authors": [
        "G Zhao"
      ],
      "venue": "MiGA 2023: Proceedings of IJCAI 2023 Workshop & Challenge on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA 2023) co-located with 32nd International Joint Conference on Artificial Intelligence (IJCAI 2023)"
    },
    {
      "citation_id": "9",
      "title": "Prototype learning for micro-gesture classification",
      "authors": [
        "G Chen"
      ],
      "venue": "MiGA 2024: Proceedings of IJCAI 2024 Workshop & Challenge on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA 2024) co-located with 33rd International Joint Conference on Artificial Intelligence (IJCAI 2024)"
    },
    {
      "citation_id": "10",
      "title": "Multi-modal micro-gesture classification via multi-scale heterogeneous ensemble network",
      "authors": [
        "H Huang"
      ],
      "venue": "MiGA 2024: Proceedings of IJCAI 2024 Workshop & Challenge on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA 2024) co-located with 33rd International Joint Conference on Artificial Intelligence (IJCAI 2024)"
    },
    {
      "citation_id": "11",
      "title": "A multimodal micro-gesture classification model based on clip",
      "authors": [
        "Y Wang"
      ],
      "venue": "MiGA 2024: Proceedings of IJCAI 2024 Workshop & Challenge on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA 2024) co-located with 33rd International Joint Conference on Artificial Intelligence (IJCAI 2024)"
    },
    {
      "citation_id": "12",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "13",
      "title": "Semantic matters: A constrained approach for zero-shot video action recognition",
      "authors": [
        "Z Quan"
      ],
      "year": "2025",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy"
      ],
      "year": "2021",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale"
    },
    {
      "citation_id": "15",
      "title": "Videoclip: Learning video representations from text and clips",
      "year": "2021",
      "venue": "Videoclip: Learning video representations from text and clips"
    },
    {
      "citation_id": "16",
      "title": "Realtime multi-person 2d pose estimation using part affinity fields",
      "authors": [
        "Z Cao",
        "G Hidalgo",
        "T Simon",
        "S.-E Wei",
        "Y Sheikh"
      ],
      "year": "2017",
      "venue": "Realtime multi-person 2d pose estimation using part affinity fields"
    },
    {
      "citation_id": "17",
      "title": "On heatmap representation for 6d pose estimation",
      "authors": [
        "X Zhou"
      ],
      "year": "2019",
      "venue": "On heatmap representation for 6d pose estimation"
    },
    {
      "citation_id": "18",
      "title": "A framework for eca-based psychotherapy",
      "authors": [
        "S Patapati",
        "T Srinivasan",
        "H Musku",
        "A Adiraju"
      ],
      "year": "2025",
      "venue": "A framework for eca-based psychotherapy"
    },
    {
      "citation_id": "19",
      "title": "Revisiting skeleton-based action recognition",
      "authors": [
        "J Zhang",
        "Z Huang",
        "Y Chen"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "20",
      "title": "Gated multimodal units for information fusion",
      "authors": [
        "J Arevalo"
      ],
      "year": "2020",
      "venue": "Gated multimodal units for information fusion"
    },
    {
      "citation_id": "21",
      "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "S Yan",
        "Y Xiong",
        "D Lin"
      ],
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "22",
      "title": "MS-G3D: Multi-scale graph convolution for skeleton-based action recognition",
      "authors": [
        "S Liu"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "23",
      "title": "European Conference on Computer Vision (ECCV)",
      "authors": [
        "B Zhou",
        "A Andonian",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "24",
      "title": "TSM: Temporal shift module for efficient video understanding",
      "authors": [
        "J Lin",
        "C Gan",
        "S Han"
      ],
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "25",
      "title": "Video swin transformer: Hierarchical vision transformer for video recognition",
      "authors": [
        "Z Liu"
      ],
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "26",
      "title": "DSCNet: Dense-sparse complementary network for human action recognition",
      "authors": [
        "Q Cheng"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "27",
      "title": "Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition",
      "authors": [
        "H Zhang"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "28",
      "title": "Eulerian video magnification for revealing subtle changes in the world",
      "authors": [
        "N Wadhwa"
      ],
      "year": "2013",
      "venue": "Eulerian video magnification for revealing subtle changes in the world"
    },
    {
      "citation_id": "29",
      "title": "Self-supervised video representation learning with neighborhood context aggregation",
      "authors": [
        "T Han"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "30",
      "title": "Decoupling representation and classifier for long-tail recognition",
      "authors": [
        "B Kang"
      ],
      "year": "2020",
      "venue": "Decoupling representation and classifier for long-tail recognition"
    },
    {
      "citation_id": "31",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "32",
      "title": "Speed-up of vision transformer models by attention-aware token filtering",
      "authors": [
        "T Naruko",
        "H Akutsu"
      ],
      "year": "2025",
      "venue": "Speed-up of vision transformer models by attention-aware token filtering",
      "arxiv": "arXiv:2506.01519v1"
    },
    {
      "citation_id": "33",
      "title": "Token sequence compression for efficient multimodal computing",
      "authors": [
        "Y Omri",
        "P Shroff",
        "T Tambe"
      ],
      "year": "2025",
      "venue": "Token sequence compression for efficient multimodal computing",
      "arxiv": "arXiv:2504.17892v1"
    },
    {
      "citation_id": "34",
      "title": "Generic token compression in multimodal large language models from an explainability perspective",
      "authors": [
        "L Lei",
        "J Gu",
        "X Ma",
        "C Tang",
        "J Chen",
        "T Xu"
      ],
      "year": "2025",
      "venue": "Generic token compression in multimodal large language models from an explainability perspective",
      "arxiv": "arXiv:2506.01097"
    },
    {
      "citation_id": "35",
      "title": "Tokencarve: Information-preserving visual token compression in multimodal large language models",
      "authors": [
        "X Tan",
        "P Ye",
        "C Tu",
        "J Cao",
        "Y Yang",
        "L Zhang",
        "D Zhou",
        "T Chen"
      ],
      "year": "2025",
      "venue": "Tokencarve: Information-preserving visual token compression in multimodal large language models",
      "arxiv": "arXiv:2503.10501"
    },
    {
      "citation_id": "36",
      "title": "Multimodal alignment-guided dynamic token pruning for accelerating vision-language transformer",
      "authors": [
        "J Cao",
        "P Ye",
        "S Li",
        "C Yu",
        "Y Tang",
        "J Lu",
        "T Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2024.00XX"
    },
    {
      "citation_id": "37",
      "title": "The distress analysis interview corpus of human and computer interviews",
      "authors": [
        "J Gratch",
        "R Artstein",
        "G Lucas",
        "G Stratou",
        "S Scherer",
        "A Nazarian",
        "R Wood",
        "J Boberg",
        "D Devault",
        "S Marsella",
        "D Traum",
        "S Rizzo",
        "L.-P Morency"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC)"
    },
    {
      "citation_id": "38",
      "title": "Avec 2019 workshop and challenge: State-of-mind, detecting depression with ai, and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "N Cummins",
        "R Cowie",
        "L Tavabi",
        "M Schmitt",
        "S Alisamir",
        "S Amiriparian",
        "E.-M Messner",
        "S Song",
        "S Liu",
        "Z Zhao",
        "A Mallol-Ragolta",
        "Z Ren",
        "M Soleymani",
        "M Pantic"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop, AVEC '19",
      "doi": "10.1145/3347320.3357688"
    }
  ]
}