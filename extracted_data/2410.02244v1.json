{
  "paper_id": "2410.02244v1",
  "title": "Visual Prompting In Llms For Enhancing Emotion Recognition",
  "published": "2024-10-03T06:33:43Z",
  "authors": [
    "Qixuan Zhang",
    "Zhifeng Wang",
    "Dylan Zhang",
    "Wenjia Niu",
    "Sabrina Caldwell",
    "Tom Gedeon",
    "Yang Liu",
    "Zhenyue Qin"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Vision Large Language Models (VLLMs) are transforming the intersection of computer vision and natural language processing. Nonetheless, the potential of using visual prompts for emotion recognition in these models remains largely unexplored and untapped. Traditional methods in VLLMs struggle with spatial localization and often discard valuable global context. To address this problem, we propose a Set-of-Vision prompting (SoV) approach that enhances zero-shot emotion recognition by using spatial information, such as bounding boxes and facial landmarks, to mark targets precisely. SoV improves accuracy in face count and emotion categorization while preserving the enriched image context. Through a battery of experimentation and analysis of recent commercial or open-source VLLMs, we evaluate the SoV model's ability to comprehend facial expressions in natural environments. Our findings demonstrate the effectiveness of integrating spatial visual prompts into VLLMs for improving emotion recognition performance. * Equal contribution † Corresponding authors Identify and locate faces in the image. (1) Box 2 Enhance grounding capabilities by box and number (2) Box + Number 1 2 3 (3) Box + Number + Facial Landmarks Analyze facial expression by spatial relationships 3",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As the integration of computer vision and natural language processing progresses, VLLMs  (Dai et al., 2024)  are revolutionizing the way machines interpret visual and textual data. Emotion recognition is gaining considerable interest across multiple disciplines and presents distinct challenges  (Yang et al., 2023a) . It requires the decoding of emotions from nuanced indicators like facial expressions, body language, and contextual details.\n\nPrevious methods, such as those in  (Xenos et al., 2024)  and  (Zhang et al., 2023b) , enhance in-context emotion classification by training transformerbased models or CLIP to generate descriptions of emotions in visual contexts. However, these methods overlook the spatial relationships between different people and facial features within a single face. These spatial relationships can be labeled with numbers and bounding boxes in our SoV prompts to guide VLLMs. A person nearby may have similar facial expressions, while a person farther away may show different facial expressions. Additionally, the relationships between the eyes, mouth, and nose features can be highlighted by facial landmarks in SoV prompts to guide VLLMs.\n\nRecent studies  (Yang et al., 2024; Zou et al., 2024; Yang et al., 2023b; Zou et al., 2024)  have explored Visual Prompting, a technique employed in image-language tasks to guide LLMs by incorporating markers such as colorful boxes or circles to emphasize specific targets within an image. ReCLIP  (Subramanian et al., 2022)  adds colorful boxes directly onto an image to highlight specific targets and blurs other irrelevant areas to reduce the performance gap with supervised models on both real and synthetic datasets. Additionally, RedCircle  (Shtedritski et al., 2023)  employs visual prompt engineering, specifically drawing a red circle around an object in an image, to direct a Vision-Language",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Sov",
      "text": "Figure  2 : Comparative analysis of emotion recognition methods in a group setting: assessing the precision of facial emotion categorization and face detection using plain text prompts versus Set-of-Vision (SoV) prompts incorporating facial landmarks, bounding boxes, and face enumeration. Top: Results using plain text prompts. Bottom: Results using Set-of-Vision (SoV) prompts. The use of SoV prompts, such as numbering each face, placing bounding boxes, and identifying facial landmarks, allows for a more precise analysis.\n\nModel's attention to that region and enhance its performance in tasks like zero-shot keypoint localization. However, both of these approaches focus on local objects and ignore spatial context information. Yang et al.  (Yang et al., 2024)  propose using fine-grained visual prompts, such as segmentation masks, and enhancing focus on relevant areas with a 'Blur Reverse Mask' that blurs regions outside the target mask to minimize distractions and maintain spatial context. Although visual prompting techniques have garnered interest, their full potential remains unexplored for emotion recognition tasks. Current approaches rely solely on coarse markers like colorful boxes, circles, or masks, which can introduce ambiguity, blur the face images, and pose challenges for accurate recognition tasks. This paper addresses these issues by systematically organizing and investigating various forms of visual prompting. Furthermore, we propose a new prompting approach called Set-of-Vision prompting (SoV) in Fig.  1 , which utilizes spatial information such as numbers, bounding boxes, and facial landmarks to precisely mark each target while maintaining background context, thereby enhancing the zero-shot performance of facial expression recognition. The top of Fig.  2 , shows an approach where specific vi-sion prompts are not used. As a result, the analysis inaccurately counts 22 visible faces and misclassifies persons' emotions into incorrect categories, with a number of faces labeled under 'Neutral Emotion' and fewer under 'Mildly Positive Emotion' and 'Happy'. This misclassification and miscount demonstrate the limitations when detailed visual cues are not utilized in the analysis. In the bottom of Fig.  2 , the use of SoV prompts, such as numbering each face, placing bounding boxes, and identifying facial landmarks, allows for a more precise analysis. The correct number of faces is identified (18), and the emotions are accurately categorized into more nuanced groups: 'Neutral Emotion', 'Mildly Positive Emotion', and 'Smiling or Happy'. This method provides a clearer and more detailed breakdown of each individual's emotional state based on visible facial expressions. This comparison highlights the importance and effectiveness of integrating visual prompts in VLLMs analysis for more accurate and detailed recognition and categorization of human emotions in images.\n\nTo summarize, our main contributions are: (1) The paper introduces a novel visual prompting method (SoV) that highlights facial regions directly within the entire image. This preserves background context, enhancing the ability of VLLMs",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Input",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Sov Prompting",
      "text": "Step 1: To identify and locate faces in the image.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Face Detection",
      "text": "Step 2: To number each face.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Number Faces",
      "text": "Step 3: To detect and extract facial landmarks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Landmarks Detection",
      "text": "Step 4: To analyze facial expression by using position of nose, eye and mouth. to perform accurate emotion recognition without the need for cropping faces, thus maintaining the holistic view of the image. (2) The proposed face overlap handling algorithm effectively addresses conflicts arising from overlapping face detections, especially in images with dense face clusters. By prioritizing larger faces and iteratively checking for overlaps, the algorithm ensures that non-occluded faces are retained for subsequent emotion analysis. (3) Our results show that incorporating spatial visual prompts (SoVs) into VLLMs can enhance their performance in recognizing emotions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Spatial Relationship",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Human Detection",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vision Large Language Models",
      "text": "LLMs such as LLaMA  (Touvron et al., 2023) ,  ChatGPT-3 (Brown et al., 2020) ,  ChatGPT-4 (Achiam et al., 2023) , and PaLM  (Chowdhery et al., 2023)  have demonstrated remarkable zeroshot transfer capabilities in natural language processing. Recently, VLLMs, which leverage imagetext data pairs from the web, have gained prominence in the computer vision domain.  MiniGPT-4 (Zhu et al., 2023) , a model that combines a visual encoder with an advanced language model, can enable multi-modal capabilities such as generating detailed image descriptions and designing websites from sketches. Video-LLaVA  (Zhang et al., 2023a)  is a multi-modal framework that enhances Large Language Models with the ability to understand and generate responses based on both visual and auditory content in videos. LLaVA  (Liu et al., 2023)  is a newly developed, end-to-end trained, large multimodal model that combines a vision encoder with a language model, demonstrating promising abilities in multimodal chat. Although VLLMs exhibit remarkable capabilities in vision-based tasks such as image segmentation and object detection, they typically require fine-tuning of the vision and text encoders using existing open vocabulary methods when applied to specific tasks. In contrast, this paper proposes a zero-shot architecture for emotion recognition, overcoming the need for task-specific fine-tuning.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Prompting Methods",
      "text": "Prompt engineering is a widely employed technique in the field of NLP  (Strobelt et al., 2022; Zhou et al., 2024) . AdbGPT  (Feng and Chen, 2024 ) is a novel, lightweight approach that leverages fewshot learning and chain-of-thought reasoning in Large Language Models to automatically reproduce bugs from bug reports, mimicking a developer's problem-solving process without the need for training or hard-coding. Although prompts for large language models have been extensively explored, prompts for vision tasks have received less attention and investigation. Yang et al.  (Yang et al., 2024)  propose using fine-grained visual prompts like segmentation masks and a Blur Reverse Mask strategy to focus on relevant areas. The Imageof-Thought (IoT) prompting method  (Zhou et al., 2024)  enhances Multimodal Large Language Models by guiding them to extract and refine visual rationales step-by-step from images, combining visual and textual insights to improve zero-shot performance on complex visual reasoning tasks. Although these methods have shown promise in tasks like semantic segmentation and object grounding, their performance in emotion recognition has been less effective. This is largely because they tend to analyze individual objects in isolation, overlooking global information and specific facial features, which are crucial for accurately interpreting emotions. To address these issues, the proposed approach directly focuses on the fine-grained facial features present in the entire image, preserving spatial information by utilizing bounding boxes, numbers, and facial landmarks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Problem Definition",
      "text": "The task of matching images to emotions for each visible face in a given image involves several sophisticated steps, combining face detection and emotion recognition. Typically, the VLLMs, denoted as Φ, will take an image I ∈ R H×W ×3 and a text question of length l i , Q i = [q i 1 , q i 2 , ..., q i l i ], as input. The output is a sequence of answers with length l o , containing emotions,\n\nwhich can be formulated as (Eq. 1):\n\nIn our task, we aim to find the best matching image-emotion pairs (I, A o ) for each visible face. Traditionally, this involves cropping the face from the image using face detectors. However, with the introduction of visual prompting, faces can be directly marked on the entire image, highlighting the facial region while preserving the background context and avoiding the obscuration of faces. With this in mind, we have developed Set-of-Vision prompts (SoV), a simple method of overlaying a number of visual prompts on the facial regions in an image.  if\n\n// Replace the smaller face 19:\n\nThis operation augments the input image I to a new image I new = SoV (I), while keeping the text prompts to VLLMs unchanged as shown in Fig.  2 . It can be formulated as (Eq. 2):\n\n3.2 Set of Vision Prompts",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Box Detection",
      "text": "Once the image is obtained, we need to generate visual prompts for the image that will be utilized by VLLMs for emotion recognition. We employ the RetinaFace  (Deng et al., 2020)\n\n, where I is the input image; θ i represents the hyperparameters for the RetinaFace model D; and b i corresponds to the i-th face bounding box.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Box Overlap Handling Algorithm",
      "text": "However, this face detection algorithm inevitably introduces some overlaps or conflicts that confuse VLLMs, especially in images with densely populated faces, such as when two faces overlap in one area or one face is obscured by another. This is illustrated in   4 )):\n\n, where Area(b 1 ) ≥ Area(b 2 ) ≥ . . . ≥ Area(b n ), ensuring that larger faces are prioritized.\n\nBy iterating through the sorted faces, the algorithm checks for overlaps and compares the areas of overlapping faces. For each face\n\nIf the overlap is significant, compare their areas and discard the smaller face by (Eq. (  6 )):\n\nAdd non-occluded faces to F f inal by (Eq. (  7 )):\n\nIt ensures that only the faces that are close to the camera and not obstructed remain in the final list.\n\nOnce we determine the location of boxes B = {b 1 , b 2 , . . . , b n } for each face, We need to assign a unique ID to each face. The unique ID N = {1, 2, . . . , n} will be used to locate each face in the image, where n is the number of detected faces. Thus, the set of vision prompts becomes distinguishable and can be effectively interpreted by VLLMs.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Facial Landmarks Detection And Analysis",
      "text": "After identifying and handling overlapping faces, we proceed with the detection and extraction of facial landmarks for each face. For each face b i ∈ F f inal :\n\n, where Θ is landmarks extraction model, L i is the facial landmarks, (x m , y m ) is coordinates of facial landmarks. For each set of landmarks L i , analyze spatial relationships for facial expression within RGB image:\n\n, where f is a function mapping spatial relationships R(L i ) to facial expressions E(L i ). The entire process is illustrated in Fig.  3 .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Text And Vision Prompts",
      "text": "We have a collection of n pairs of location-vision prompts, represented as (l 1 , v 1 ), . . . , (l n , v n ).\n\nWhen introducing additional text prompts for a new image I new , we can choose to use either plain text prompts or a combination of text and vision prompts.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Plain Text Prompts",
      "text": "This method is exemplified on the left side in Fig.  5 . It involves asking a general question about the emotional state of a group of people without referencing specific individuals. For example, the question \"What is the emotion for this group of people?\" yields an answer that considers the overall mood and setting of the group. This approach is useful for understanding group dynamics or the general atmosphere of a scene.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Combined Text-Vision Prompts",
      "text": "Shown on the right side in Fig.  5 , this method involves more detailed prompts that focus on individual persons within the group. This allows for a more nuanced analysis of specific people's emotions and actions. For instance, questions such as 'What is Person 1's emotion?' or 'What food is Person 3 eating and drinking?' prompt answers that delve into specific details regarding individuals' facial expressions, body language, and interactions with objects, like food and drinks.\n\nWhat is the emotion for this group of people?\n\nIn the image, we see a group of four people enjoying a picnic outdoors, which suggests a relaxed and pleasant setting. What food is Person 3 eating and drinking?\n\nIn the image, Person 3 appears to be holding a glass, likely containing a lemonade or similar light-colored drink. There is no clear indication of her eating any specific food in the scene;\n\nhowever, there are various foods visible on the picnic blanket, including fruits and possibly some sandwiches or snacks, which any of the picnic participants, including her, might be enjoying.\n\nWhat food is Person 4 eating and drinking?\n\nIn the image, Person 4 is visibly holding a sandwich. He appears to be either about to take a bite or is currently eating it. As for his drink, he has a glass in front of him, similar to the other glasses on the picnic blanket, which likely contains lemonade or a similar beverage. This setup suggests a typical picnic meal consisting of simple, easy-to-handle foods and refreshing drinks.   Acc (%) R@1 Acc (%) R@1 Acc (%) R@1 Acc (%) R@1 MiniGPT-4 ( We do not need to train any models for our method.\n\nWe evaluate the model's performance in a zero-shot manner using VLLMs. We include both commercial models such as GPT-4V-turbo  (Achiam et al., 2023)  1 and GPT-4o-2024-05-13  (Achiam et al., 2024)  as well as open-sourced models including MiniGPT-4-Vicuna  (Zhu et al., 2023)  2 , LLaVA-1.5-7B  (Liu et al., 2023)",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset Details",
      "text": "We collect original images from ABC News website 5  . Following the collection, we undertake meticulous preprocessing, initially removing any identical and blurry images through deduplication. To minimize human effort and cost in data annotation, we employ DeepFace  (Serengil and Özpınar, 2024)  for emotion annotation. Subsequently, two human annotators revise and refine the image labels. Finally, to finalize the labels, we involved a third annotator who has a professional background in psychology to verify correctness of facial expressions with their domain knowledge. This procedure guarantees the quality of the annotated data used to construct benchmarks.  The models compared include GPT-4V+Ours, GPT-4V  (Achiam et al., 2023) , LLaVA  (Liu et al., 2023) , Video-LLaVA  (Zhang et al., 2023a) , and MiniGPT-4  (Zhu et al., 2023) . These results are distributed across seven emotions.\n\nmodel on the task of zero-shot emotion recognition, structured across three different levels of difficulty: Easy, Medium, and Hard. This structured dataset aids in understanding the robustness and adaptability of the model in varying conditions of visual complexity.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Quantitative Results",
      "text": "Table  1  provides a detailed analysis of different zero-shot emotion recognition methods. MiniGPT-4  (Zhu et al., 2023)  exhibits low performance, with accuracy ranging from 15.78% to 30.45% and Recall@1 from 12.85% to 16.17%. LLaVA  (Liu et al., 2023)  and Video-LLaVA  (Zhang et al., 2023a)  perform better in simpler categories but struggle significantly in the Hard category, where accuracy plummets to 3.50% and Recall@1 to 1.58%. In contrast, GPT-4V  (Achiam et al., 2023)  demonstrates robust performance across all levels, markedly improved by the SoV prompts. Specifically, GPT-4V+SoV achieves an impressive 60.91% accuracy and 41.96% Recall@1 in the Easy category, maintaining 50.00% accuracy and 18.97% Recall@1 even in the Hard category. These results underline SoV's effectiveness in boosting the model's ability to accurately interpret emotions across different complexities.\n\nIn Fig.  6 , the chart highlights the varied efficacy of different VLLMs in emotion recognition tasks. GPT-4V+Ours consistently outperforms other models across nearly all emotions, particularly excelling in neutral and angry emotions. This highlights the specialized capabilities of GPT-4V+Ours in capturing more nuanced and varied emotional states. Meanwhile, other models show selective strengths and general weaknesses, partic-ularly in recognizing negative emotions like fear and disgust.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Visual Prompting",
      "text": "Table . 2 presents a comparative analysis of stateof-the-art methods for zero-shot emotion recognition across datasets categorized by varying levels of difficulty: Easy, Medium, and Hard. It details performance metrics such as Accuracy and Recall, comparing the effectiveness of different visual prompting strategies utilized by GPT-4V. The baseline method (GPT-4V), which uses plain text prompts, demonstrates moderate effectiveness, with an overall accuracy of 44.44% and a Recall of 22.11%. Methods such as ReCLIP, RedCircle, and SoV employ more complex combinations of visual prompts. SoV (Ours), incorporating Numbers, Boxes, and Facial Landmarks, achieves the highest overall accuracy and recall scores of 55.33% and 28.69%, respectively. This suggests that the integration of multiple visual cues, particularly those that enhance the recognition of facial features, significantly improves performance across all difficulty levels, especially in more challenging datasets. Fig.  7  visually represents how each visual prompting approach modifies the image to focus on emotion-relevant features. ReCLIP and RedCircle blur out non-facial areas and highlight faces with rectangles and circles, respectively. SoV applies a combination of visual prompts to emphasize facial areas while maintaining background context, which is critical for emotion recognition.\n\nFig.  8  reveals a comparative analysis of the performance across different emotional categories using three different visual prompts for GPT-4V: SoV (Ours), RedCircle, and ReCLIP. In this assessment,",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Sota Methods",
      "text": "Visual Prompt Easy Medium Hard Total Acc (%) R@1 Acc (%) R@1 Acc (%) R@1 Acc (%) R@1 Baseline (",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ablation Study",
      "text": "We investigate the effects of different vision prompts on GPT-4V. The data from  Acc (%) R@1 Acc (%) R@1 Acc (%) R@1 Acc (%) R@1 Baseline (",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "What Is The Emotion For Each Face?",
      "text": "In the image you provided: 1. The woman labeled \"1\" has a cheerful expression, looking relaxed and happy. 2. The figure labeled \"2\" has an obscured face so I can't determine their expression. 3. The woman labeled \"3\" seems to be smiling mildly, giving off a serene and content vibe.\n\nWhat is the emotion for each face?\n\nFigure  10 : The impacts of segmentation masks for emotion recognition. Top: SoV provides a clearer view for emotion recognition. Bottom: the segmentation masks obscure parts of their faces, making it more challenging to accurately discern these emotions, especially for Person 2. In addition, the added segmentation masks also result in a lack of precise context.\n\nRecall@1 figures, confirming its effectiveness in accurately interpreting emotions even in the most difficult settings. Fig.  9  showcases an ablation study that compares the effectiveness of different vision prompts on GPT-4V. The GPT-4V+SoV configuration consistently outperforms the other methods in nearly all emotional categories, particularly excelling in 'Neutral' and 'Angry'. While the Box + Number prompts demonstrates moderate success in 'Happy' and 'Surprise', it still falls short compared to SoV prompts in other emotion categories. This result highlights that after adding extra facial landmarks, the VLLMs can capture more nuanced and varied emotional states.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Qualitative Observations",
      "text": "The image in Fig.  10  shows three people at a picnic setting, each displaying happy and relaxed expression, which suggests they are enjoying the outing. However, due to the segmentation masks applied, the masks obscure parts of their faces, making it more challenging to accurately discern these emotions, especially for person 2. In addition, the added segmentation masks also result in a lack of precise context.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Limitations",
      "text": "We suggest the implementation of Set-of-Vision prompting to bridge visual and textual prompts. However, a challenge arises as it is difficult to precisely describe visual prompts, such as the shape, location, or color of a bounding box and facial landmarks, in language. This issue might require encoding the visual prompts and fine-tuning the entire model for better accuracy. Moreover, this method is computationally intensive, potentially limiting its scalability and practicality in real-time applications, especially when handling large datasets or streaming videos such as tracking one person's emotion in different video frames.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "In conclusion, our Set-of-Vision prompting (SoV) approach significantly advances the field of emotion recognition within VLLMs by addressing critical challenges in spatial localization and global context preservation. By leveraging spatial information such as bounding boxes and facial landmarks, SoV enhances zero-shot emotion recognition accuracy, ensuring precise face count and emotion categorization. Our face overlap handling algorithm and combined text-vision prompting strategy further refine the recognition process, highlighting the efficacy of integrating visual prompts in VLLMs for more accurate and detailed emotion analysis. This approach not only preserves the enriched image context but also offers a solution for detailed and nuanced emotion recognition, underscoring its potential impact on various applications within computer vision and natural language processing.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A Appendix",
      "text": "",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A.1 Dataset Details",
      "text": "In the dataset Table  4 , the \"easy\" dataset includes pictures with three or fewer faces. The \"medium\" dataset includes pictures with 3 to 7 faces. The \"hard\" dataset includes pictures with more than 7 faces. Easy dataset contains 76 images with a total of 174 faces. Medium dataset consists of 34 images featuring 171 faces. Hard dataset is the smallest set, comprising 9 images but still containing a significant number of faces (114). The table categorizes the datasets based on the complexity and density of faces in the images, which likely affects the challenge level for the model's emotion recognition capabilities. The usage of SoV prompts across all categories suggests a consistent testing approach, aiming to evaluate how well the model can interpret and predict emotions without prior specific training on these images (zero-shot learning). The metrics, Accuracy and Recall, are chosen to assess the model's precision in correctly identifying emotions and its ability to retrieve relevant instances across the datasets, respectively.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "A.2 Scene Based Emotion Recognition",
      "text": "The image in Fig.  11  captures a moment from a sports event, specifically a match between teams from Australia and the Philippines, with Australia leading based on the scoreboard (\"PHI 0 -1 AUS\").\n\nThe scene includes various spectators and players, each showing distinct emotions which can aid in understanding the context and overall sentiment related to the ongoing game. Including scene context can significantly enhance the performance of language and image models (LLMs) in recognizing emotions. By understanding not just the facial expressions but also the situational context (such as the score in a sports game), models can make more accurate inferences about the probable emotions being displayed. In addition, environmental cues like scoreboards, team colors, and body language provide additional data points that help in accurately deducing the emotional state of individuals in group settings. Our SoV approach can bridge the gap between purely facial expressionbased recognition and a more situation-aware understanding, leading to more nuanced and accurate emotion recognition capabilities in VLLMs. In Fig.  12 , the SoV method, by focusing on specific individuals within the image and retaining the clarity of the background context, offers a comprehensive approach to emotion recognition. This allows for a detailed analysis of not just the visible facial expressions, but also the situational context and interactions among individuals, providing a more nuanced understanding of the emotions being conveyed. For example, in the case of Person 2, despite a partially visible face, the SoV approach leverages additional contextual cues to deduce the emotion more accurately. In contrast, the RedCircle approach primarily relies on clear facial expressions by blurring background details, potentially overlooking subtler emotional cues embedded in the environment and body language.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "A.3 Chain Of Thought For Emotion Recognition",
      "text": "The \"Chain of Thought\" approach, as illustrated in Fig.  13 , offers advantages by breaking down the thought process into sequential, logical steps. This method enhances accuracy in interpreting emotions by considering various contextual clues such as activities, and body language, which are then systematically analyzed. For example, the analysis first identifies the activity (Halloween trick-or-treating) and setting, which sets the emotional backdrop. Subsequently, it classifies each individual based on visible attributes and costumes, leading to more nuanced emotion recognition. This step-by-step reasoning mirrors human cognitive processes, allowing for more refined and contextually appropriate interpretations of emotions, such as distinguishing neutral expressions from smiles, even in a complex social setting like Halloween, where expressions might otherwise be ambiguous. Based on their expressions in the image: 1. The girl labeled \"1\" seems focused and somewhat intense, likely reflecting her determination in the game given that her team is currently losing. 2. The girl labeled \"2\" appears joyful and is smiling, which suggests she is happy, likely because her team is in the lead. 3. The person labeled \"3\" is also smiling and appears to be enjoying the moment, which is consistent with a positive emotion like happiness or contentment. 4. The person labelled \"4\" (in the inset) has a neutral expression, which might suggest he is calm or composed, possibly observing the game with interest or concentration. 1.The man in the blue shirt is smiling widely, indicating happiness or joy.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Scene Based Emotion Recognition",
      "text": "2.The girl to his left also seems happy, with a broad smile.\n\n3.The woman in the foreground on the right is smiling as well, suggesting she is also feeling happy.\n\n4.The boy in the foreground on the left appears to be laughing or speaking enthusiastically, which generally reflects excitement or happiness.\n\nOverall, the group appears to be enjoying a pleasant and happy moment together. Overall, the scene conveys a warm and cheerful family mealtime, where everyone seems to be enjoying both the food and each other's company.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Compare Redcircle And Sov Prompts",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Gpt-4V + Sov:",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Redcircle Sov",
      "text": "Figure  12 : Compare RedCircle and SoV prompts for emotion recognition.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "User:",
      "text": "Step 1: What are they planning to do?\n\nStep 2: How many people are there in the image?\n\nStep 3: Can you give me the related labels for these person?\n\nStep 4: What are their emotions?",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Gpt-4V + Sov:",
      "text": "Question 1:\n\nThe group in the image seems to be participating in a Halloween trick-or-treating event. They are dressed in costumes and carrying buckets for collecting candy, which is a traditional activity where participants visit houses to ask for treats. The setting, with residential cars and houses in the background, further supports that they are likely walking through a neighborhood to gather sweets from different homes.\n\nQuestion 2: There are four people in the image. Question 3:\n\nIn the image:\n\n1. Person 1 (on the far right) -labeled as \"1\", wearing a black shirt with white skeleton bones printed on it and carrying an orange pumpkin bucket. Chain of thought for emotion recognition If Sally mentioned seeing a very sad news video that deeply affected her, it's likely she referred to the image depicting people in a scene of destruction or after a disaster. This would be the first image of the three presented in your query, where individuals are seen amidst rubble and damage. Such scenes typically involve serious issues like natural disasters, conflicts, or other tragic events that can have a profound emotional impact on viewers. This type of content aligns with a description of a \"very sad news video.\"\n\nThe other two images show more casual or celebratory scenarios that are less likely to be described as deeply affecting in a sad context.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "User:",
      "text": "My friend Sally mentioned seeing a very sad news video that deeply affected her. Which of the images could she have seen?",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "User:",
      "text": "What are the face emotion of the people in the first image?",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Gpt-4V + Sov:",
      "text": "In the image where individuals are amidst rubble, the emotions of the people can be described as follows:\n\n1.Person 1: Appears to be in distress or discomfort, potentially indicative of shock or sadness.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "2.Person 2:",
      "text": "Looks serious and concerned, likely reflecting the gravity of the situation.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "3.Person 3:",
      "text": "His expression is hard to discern fully due to the angle, but he seems to be focused and possibly worried. 4.Person 4: Shows a pensive or somber expression, which might suggest contemplation or distress.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "5.Person 5:",
      "text": "Seems to have a neutral or tired expression, which might reflect fatigue or resignation in the face of the situation. Overall, the expressions suggest a mixture of concern, sadness, and seriousness, aligning with a response to a serious or tragic event.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Grounding Ability",
      "text": "The person labeled \"2\" in the image appears to be smiling and engaging positively with the interviewer, which suggests that he is feeling happy or content during the interview. His overall demeanor and facial expression convey a sense of enjoyment or satisfaction, likely related to the context of the sports event.\n\nIn the image you provided from a sporting event, the person labeled \"2\" would be the most appropriate individual for a reporter to ask questions about the game. This person is likely a coach or player involved in the game, given that he is in uniform and holding a child who is also in a similar uniform. This suggests he is directly related to the team and would have insights about the game's specifics.\n\nUser: If you are a reporter and want to ask a question about the game, which labeled person is the best person for you to talk to?",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Gpt-4V + Sov:",
      "text": "Answer:\n\nUser: what is the emotion of the person labeled \"2\"",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Gpt-4V + Sov:",
      "text": "Answer:",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed Set-of-Vision (SoV) prompting",
      "page": 1
    },
    {
      "caption": "Figure 2: Comparative analysis of emotion recognition methods in a group setting: assessing the precision",
      "page": 2
    },
    {
      "caption": "Figure 1: , which utilizes spatial information such as",
      "page": 2
    },
    {
      "caption": "Figure 2: , shows an approach where specific vi-",
      "page": 2
    },
    {
      "caption": "Figure 2: , the use of SoV prompts, such as",
      "page": 2
    },
    {
      "caption": "Figure 3: Workflow diagram for enhanced face recognition and emotion analysis using the Set-of-Vision",
      "page": 3
    },
    {
      "caption": "Figure 4: Face detection inevitably introduces some",
      "page": 4
    },
    {
      "caption": "Figure 2: It can be formulated as (Eq. 2):",
      "page": 4
    },
    {
      "caption": "Figure 4: To mitigate this problem,",
      "page": 5
    },
    {
      "caption": "Figure 5: It involves asking a general question about the",
      "page": 5
    },
    {
      "caption": "Figure 5: , this method in-",
      "page": 5
    },
    {
      "caption": "Figure 5: We use two types of prompt methods. Left: plain text prompts, which can be used for group emotion",
      "page": 6
    },
    {
      "caption": "Figure 6: The bar chart shows the performance of various VLLMs in recognizing different emotions from images.",
      "page": 7
    },
    {
      "caption": "Figure 6: , the chart highlights the varied effi-",
      "page": 7
    },
    {
      "caption": "Figure 7: visually represents how each visual",
      "page": 7
    },
    {
      "caption": "Figure 8: reveals a comparative analysis of the per-",
      "page": 7
    },
    {
      "caption": "Figure 7: Visualization of the SOTA visual prompting approaches such as ReCLIP (Subramanian et al., 2022),",
      "page": 8
    },
    {
      "caption": "Figure 8: The bar chart illustrates the performance of",
      "page": 8
    },
    {
      "caption": "Figure 9: The bar chart displayed in the image illustrates",
      "page": 8
    },
    {
      "caption": "Figure 10: The impacts of segmentation masks for emo-",
      "page": 9
    },
    {
      "caption": "Figure 9: showcases an ablation study that com-",
      "page": 9
    },
    {
      "caption": "Figure 10: shows three people at a picnic",
      "page": 9
    },
    {
      "caption": "Figure 11: captures a moment from a",
      "page": 11
    },
    {
      "caption": "Figure 12: , the SoV method, by focusing on",
      "page": 11
    },
    {
      "caption": "Figure 13: , offers advantages by breaking down the",
      "page": 11
    },
    {
      "caption": "Figure 11: Scene based Emotion Recognition",
      "page": 12
    },
    {
      "caption": "Figure 12: Compare RedCircle and SoV prompts for emotion recognition.",
      "page": 13
    },
    {
      "caption": "Figure 13: Chain of thought for emotion recognition",
      "page": 14
    },
    {
      "caption": "Figure 14: Detect negative emotions for different people",
      "page": 15
    },
    {
      "caption": "Figure 15: Grounding ability",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Scene based Emotion Recognition": ""
        },
        {
          "Scene based Emotion Recognition": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SoV\nRedCircle\nUser: What’s emotion for these person?": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GPT-4V + SoV: \nQuestion  1: \nThe group in the image seems to be participating in a Halloween trick-or-treating \nevent. They are dressed in costumes and carrying buckets for collecting candy, \nwhich is a traditional activity where participants visit houses to ask for treats. The \nsetting, with residential cars and houses in the background, further supports that \nthey are likely walking through a neighborhood to gather sweets from different \nhomes.\nQuestion  2: \nThere are four people in the image.\nQuestion  3: \nIn the image:\n1. Person 1 (on the far right) -\nlabeled as \"1\", wearing a black shirt with white \nskeleton bones printed on it and carrying an orange pumpkin bucket.\n2. Person 2 (second from the right) - labeled as \"2\", dressed in casual clothes, \nholding documents and an orange bucket.\n3. Person 3 (second from the left) -\nlabeled as \"3\", dressed in a black tutu and \ncarrying an orange bucket.\n4. Person 4 (on the far left) - labeled as \"4\", also dressed in a black tutu, but with a \nblue bucket instead.\nEach label corresponds to the numbered box drawn over their faces in the image.\nQuestion  4: \nFrom the visible expressions in the image:\n1. Person 1 (on the far right): Appears to have a neutral expression.\n2. Person 2 (second from the right): Their face is not visible in the image.\n3.  Person 3 (second from the left): Seems to be smiling, indicating happiness or \ncheerfulness.\n4. Person 4 (on the far left): Appears to have a neutral expression.\nThese are the inferred emotions based on the limited facial expressions visible in the \nimage.": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Grounding ability\nGPT-4V + SoV: \nAnswer: \nIn the image you provided from a sporting event, the person labeled \"2\" would be \nthe most appropriate individual for a reporter to ask questions about the game. \nThis person is likely a coach or player involved in the game, given that he is in \nuniform and holding a child who is also in a similar uniform. This suggests he is \ndirectly related to the team and would have insights about the game's specifics.\nUser: what is the emotion of the person labeled \"2\"": ""
        },
        {
          "Grounding ability\nGPT-4V + SoV: \nAnswer: \nIn the image you provided from a sporting event, the person labeled \"2\" would be \nthe most appropriate individual for a reporter to ask questions about the game. \nThis person is likely a coach or player involved in the game, given that he is in \nuniform and holding a child who is also in a similar uniform. This suggests he is \ndirectly related to the team and would have insights about the game's specifics.\nUser: what is the emotion of the person labeled \"2\"": ""
        }
      ],
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Shyamal Anadkat, et al. 2023. GPT-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "venue": "Shyamal Anadkat, et al. 2023. GPT-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "Palm: Scaling language modeling with pathways",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Chung",
        "Charles Sutton",
        "Sebastian Gehrmann"
      ],
      "year": "2023",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "5",
      "title": "Instructblip: Towards general-purpose visionlanguage models with instruction tuning",
      "authors": [
        "Wenliang Dai",
        "Junnan Li",
        "Dongxu Li",
        "Anthony Meng",
        "Huat Tiong",
        "Junqi Zhao",
        "Weisheng Wang",
        "Boyang Li",
        "Pascale Fung",
        "Steven Hoi"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "6",
      "title": "Retinaface: Single-shot multi-level face localisation in the wild",
      "authors": [
        "Jiankang Deng",
        "Jia Guo",
        "Evangelos Ververas",
        "Irene Kotsia",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Prompting is all you need: Automated android bug replay with large language models",
      "authors": [
        "Sidong Feng",
        "Chunyang Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/ACM International Conference on Software Engineering"
    },
    {
      "citation_id": "8",
      "title": "",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "9",
      "title": "A benchmark of facial recognition pipelines and co-usability performances of modules",
      "authors": [
        "Sefik Serengil",
        "Alper Özpınar"
      ],
      "year": "2024",
      "venue": "Bilişim Teknolojileri Dergisi"
    },
    {
      "citation_id": "10",
      "title": "What does clip know about a red circle? visual prompt engineering for vlms",
      "authors": [
        "Aleksandar Shtedritski",
        "Christian Rupprecht",
        "Andrea Vedaldi"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "11",
      "title": "Interactive and visual prompt engineering for ad-hoc task adaptation with large language models",
      "authors": [
        "Hendrik Strobelt",
        "Albert Webson",
        "Victor Sanh",
        "Benjamin Hoover",
        "Johanna Beyer",
        "Hanspeter Pfister",
        "Alexander Rush"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Visualization and Computer Graphics"
    },
    {
      "citation_id": "12",
      "title": "Reclip: A strong zero-shot baseline for referring expression comprehension",
      "authors": [
        "Sanjay Subramanian",
        "William Merrill",
        "Trevor Darrell",
        "Matt Gardner",
        "Sameer Singh",
        "Anna Rohrbach"
      ],
      "year": "2022",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "13",
      "title": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro"
      ],
      "venue": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "14",
      "title": "Vllms provide better context for emotion understanding through common sense reasoning",
      "authors": [
        "Alexandros Xenos",
        "Niki Foteinopoulou"
      ],
      "year": "2024",
      "venue": "Ioanna Ntinou, Ioannis Patras, and Georgios Tzimiropoulos",
      "arxiv": "arXiv:2404.07078"
    },
    {
      "citation_id": "15",
      "title": "2023a. Context de-confounded emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Zhaoyu Chen",
        "Yuzheng Wang",
        "Shunli Wang",
        "Mingcheng Li",
        "Siao Liu",
        "Xiao Zhao",
        "Shuai Huang",
        "Zhiyan Dong",
        "Peng Zhai"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v",
      "authors": [
        "Jianwei Yang",
        "Hao Zhang",
        "Feng Li",
        "Xueyan Zou",
        "Chunyuan Li",
        "Jianfeng Gao"
      ],
      "year": "2023",
      "venue": "Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v",
      "arxiv": "arXiv:2310.11441"
    },
    {
      "citation_id": "17",
      "title": "Fine-grained visual prompting",
      "authors": [
        "Lingfeng Yang",
        "Yueze Wang",
        "Xiang Li",
        "Xinlong Wang",
        "Jian Yang"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems, 36. Hang Zhang, Xin Li, and Lidong Bing. 2023a. Videollama: An instruction-tuned audio-visual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    },
    {
      "citation_id": "18",
      "title": "Learning emotion representations from verbal and nonverbal communication",
      "authors": [
        "Sitao Zhang",
        "Yimu Pan",
        "James Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Image-of-thought prompting for visual reasoning refinement in multimodal large language models",
      "authors": [
        "Qiji Zhou",
        "Ruochen Zhou",
        "Zike Hu",
        "Panzhong Lu",
        "Siyang Gao",
        "Yue Zhang"
      ],
      "year": "2024",
      "venue": "Image-of-thought prompting for visual reasoning refinement in multimodal large language models",
      "arxiv": "arXiv:2405.13872"
    },
    {
      "citation_id": "20",
      "title": "MiniGPT-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "MiniGPT-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    },
    {
      "citation_id": "21",
      "title": "Segment everything everywhere all at once",
      "authors": [
        "Xueyan Zou",
        "Jianwei Yang",
        "Hao Zhang",
        "Feng Li",
        "Linjie Li",
        "Jianfeng Wang",
        "Lijuan Wang",
        "Jianfeng Gao",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "Proceedings of the Conference on Neural Information Processing Systems"
    }
  ]
}