{
  "paper_id": "2312.05735v1",
  "title": "A Comprehensive Survey On Multi-Modal Conversational Emotion Recognition With Deep Learning",
  "published": "2023-12-10T03:07:23Z",
  "authors": [
    "Yuntao Shou",
    "Tao Meng",
    "Wei Ai",
    "Nan Yin",
    "Keqin Li"
  ],
  "keywords": [
    "Multi-modal conversational emotion recognition",
    "Deep Learning",
    "Multimodal datasets",
    "Multi-modal feature fusion",
    "Multimodal feature extraction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multi-modal conversation emotion recognition (MCER) aims to recognize and track the speaker's emotional state using text, speech, and visual information in the conversation scene. Analyzing and studying MCER issues is significant to affective computing, intelligent recommendations, and human-computer interaction fields. Unlike the traditional single-utterance multi-modal emotion recognition or single-modal conversation emotion recognition, MCER is a more challenging problem that needs to deal with more complex emotional interaction relationships. The critical issue is learning consistency and complementary semantics for multi-modal feature fusion based on emotional interaction relationships. To solve this problem, people have conducted extensive research on MCER based on deep learning technology, but there is still a lack of systematic review of the modeling methods. Therefore, a timely and comprehensive overview of MCER's recent advances in deep learning is of great significance to academia and industry. In this survey, we provide a comprehensive overview of MCER modeling methods and roughly divide MCER methods into four categories, i.e., context-free modeling, sequential context modeling, speaker-differentiated modeling, and speaker-relationship modeling. In addition, we further discuss MCER's publicly available popular datasets, multi-modal feature extraction methods, application areas, existing challenges, and future development directions. We hope that our review can help MCER researchers understand the current research status in emotion recognition, provide some inspiration, and develop more efficient models. CCS Concepts: â€¢ General and reference â†’ Surveys and overviews; â€¢ Human-centered computing â†’ Natural language interfaces.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "With the development of the mobile Internet, social media has become the main platform for people to communicate with each other  [74] . Users can fully express their emotions through multimodal data such as text, voice, image, and video. Building an multi-modal conversational emotion recognition model through multi-modal data is of vital practical significance for understanding users' true emotional intentions  [21] . Therefore, researchers have been trying to give machines the ability to understand emotions in recent years  [51] ,  [132] .\n\nBefore the rise of multi-modal conversation emotion recognition research, early conversational emotion recognition methods  [41, 62, 104, 131]  mainly focused on single-modal text or speech, and they only considered the context dependencies between texts (or speech) and the semantic information of words (or audio) themselves to analyze and recognize emotions  [71] ,  [118] ,  [94] . However, only extracting the emotional information contained in the text data may not be enough for the model to understand the emotion expressed by the speaker, because the speaker may be more reserved when expressing his opinion  [20, 133] . For example, a speaker may be veiled in expressing his anger, which may result in a more neutral utterance. In response to the above problems, multi-modal conversational emotion recognition (MCER) technology was proposed to solve the problem of insufficient expression of text semantic information  [92] ,  [91] ,  [118] . As shown in Fig.  1 , MCER aims to extract semantic information complementary within and between modalities and identify the emotions expressed by speakers in text, audio, and video. The advantage of MCER is that when the emotional polarity of text information is insufficiently expressed  [55] , the model can use visual information (such as facial expressions, etc.) and audio information (such as tone, etc.) to enhance the emotional understanding of text information  [114] ,  [115] ,  [117] ,  [116] . However, unlike traditional single-utterance multi-modal emotion recognition or single-modal conversation emotion recognition, MCER is a more challenging issue that requires consideration of factors such as multi-modal context, dialogue scenarios, speaker's own emotional inertia, and interlocutor stimulation  [9, 127] . Powerful deep learning technology  [93]  enables MCER to recognize emotion by fusing semantic features with complex emotional interactions. Feature fusion in MCER mainly considers intra-modal contextual semantic information fusion and inter-modal complementary semantic information fusion  [36] . On the one hand, intra-modal contextual semantic information fusion refers to extracting the temporal and spatial dependencies of speaker feature representations in each modality. On the other hand, complementary semantic information fusion between modalities refers to using the interactive information between different modalities to enhance the emotional understanding ability of the model. MCER synergistically improves the effect of emotion recognition by fusing the characteristics of various modal data, which has important theoretical significance for processing and understanding multi-modal data  [29, 113, 130] .\n\nDespite more and more people being devoted to researching new models and methods for multimodal conversation emotion recognition  [9, 18, 59] , there is still a lack of understanding of the theoretical and methodological classification of multi-modal conversational emotion recognition, especially multi-modal conversational emotion recognition based on deep learning. To the best of our knowledge, this survey is the first comprehensive survey focusing on deep learning in multi-modal conversation emotion recognition. Existing surveys mainly focus on single-utterance multi-modal emotion recognition  [133]  or single-modal conversation emotion recognition  [11] . Therefore, with the rapid penetration of deep learning in various fields, multi-modal conversation emotion recognition methods based on deep learning have become a research hotspot and require timely and comprehensive investigation.\n\nBased on the above analysis, our survey summarizes the research work on multi-modal conversational emotion recognition. We first list some publicly available and popular datasets in the field of MCER, and list the commonly used feature extraction methods for each modality. Secondly, as shown in Fig.  2 , we roughly divide the MCER methods into four categories, i.e., context-free modeling, sequential context modeling, distinguishing-speaker modeling, speaker-relationship modeling. Third, we provide evaluation indicators for MCER experiments. Fourth, we give the applications and problems of MCER. Finally, we illustrate future research directions.\n\nThe contributions made in this paper are summarized as follows:\n\nâ€¢ New Taxonomy: We provide a new taxonomy for multi-modal conversational emotion recognition. Specifically, we classify existing MCER methods into four groups: contextfree modeling, sequential context modeling, distinguishing-speaker modeling, and speakerrelationship modeling. â€¢ Comprehensive Review: This paper provides the most comprehensive review of deep learning and machine learning algorithms for MCER. For each modeling approach, we provide representative models and make corresponding comparisons. â€¢ Abundant Resources: We collect relevant resour-ces about MCER, including state-of-the-art models and publicly available datasets. This paper can serve as a practical guide for learning and developing different emotion recognition algorithms. â€¢ Future Directions: We analyzed the limitations of existing MCER methods and proposed possible future research directions in many aspects, such as the collaborative generation of multi-modal data, the deep fusion of multi-modal features, and the unbiased learning of multi-modal emotions.\n\nThe paper is organized as follows: Section 2 summarizes the publicly available and popular datasets in the field of MCER. Section 3 illustrates the background, definitions, and commonly used feature extraction techniques for MCER. Section 4 broadly divides MCER methods into four categories and analyzes their advantages and disadvantages. Section 5 summarizes some commonly used evaluation metrics for MCER tasks. Section 6 gives the performance of different algorithms on the IEMOCAP and MELD data sets. Section 7 discusses the real-life applications of MCER. Section multi-modal dataset for MCER. In the IEMOCAP dataset, ten theater actors express specific emotion categories (i.e., sad, neutral, frustrated, anger, happy, excited) through binary dialogue. To ensure the consistency and accuracy of annotation, each sentence is annotated by multiple experts.",
      "page_start": 1,
      "page_end": 5
    },
    {
      "section_name": "Meld",
      "text": "The MELD dataset  [80]  is from the classic TV series Friends, which contains text, video and audio data. The MELD dataset contains a total of 13,709 video clips, and each sentence is labeled as a specific emotion (i.e., anger, neutral, fear, disgust, surprise, joy, disgust). In addition, the MELD dataset is also annotated by neutral, negative and positive three-category emotion. To ensure the consistency and accuracy of annotation, each sentence is annotated by multiple experts.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dailydialog",
      "text": "The DailyDialog dataset  [52]  is a multi-turn dialogue dataset about daily chat scenarios, which only contains text modalities. The DailyDialog dataset contain 13, 000 dialogues and labels each sentence with intention (i.e., inform, commissive, directives, questions) and emotion (surprise, sadness, fear, happiness, disgust, anger). Each sentence is annotated jointly by three experts.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Emorynlp",
      "text": "EmoryNLP  [125]",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotionnlines",
      "text": "The EmotionLines dataset  [30]  comes from binary conversations between Friends and Facebook, and it only contains text data. The EmotionLines dataset contains 1,000 dialogues with a total of 29,245 sentences. Seven categories of emotions are marked: neutral, fear, surprise, sadness, anger, happiness, disgust. The EmotionLines dataset is rarely used in conversational emotion recognition.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Emocontext",
      "text": "The EmoContext data set  [7]  only contains text data. It has a total of 38,421 dialogues and a total of 115,263 sentences. Three types of emotions are marked: happiness, sadness, and anger. Although the EmoContext data is large, it is rarely used in conversational emotion recognition because it only contains text data.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Background, Definition, And Feature Extraction",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Background",
      "text": "As shown in Fig.  3 , we counted multi-modal conversational emotion recognition algorithms from 2000 to 2023. As can be seen from the figure, before 2018, traditional machine learning algorithms were mainly used, and then deep learning algorithms gradually became the main ones. Next, we briefly return to the main development history of the MCER algorithm.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Brief History Of Conversational Emotion",
      "text": "Recognition. The emotion recognition method based on the dictionary is the earliest used for emotion recognition  [24] , which motivated early work on naive bayes method  [15] . With the widespread application of machine learning algorithms in classification tasks, the representative machine learning classification algorithm (SVM  [85] ,  [35] , and binary decision tree  [10] ,  [47] ,  [61] , etc) has also begun to shine in the field of emotion recognition. The above-mentioned method determines the category of emotion by learning the polarity and occurrence frequency of emotional words in the text, which is difficult to extract the semantic information and context information.\n\nEncouraged by the success of CNNs in the field of computer vision (CV), CNNs began to be migrated to text classification tasks and received extensive research attention  [39] ,  [44] ,  [42] . In 2017, Poria et al.  [78]  used LSTM for the first time to resolve dependencies between contexts. Since then, improvements, extensions and applications of LSTMs and GRUs have increased  [26] ,  [69] ,  [25] ,  [82] . Until recently, many GNN-based methods (e.g.,  [20] ,  [37] ,  [89] ,  [50] ,  [124] ) emerged. Apart from CNNs, RNNs, and GNNs, many alternative Transformer-based methods (e.g.,  [131] ,  [49] ,  [132] ) have been developed in the past decades. We detail the categories to which these algorithms belong in Section 4.\n\n3.1.2 Multi-modal Conversational Emotion Recognition Versus Traditional Machine Learning. MCER methods based on traditional machine learning  [85] ,  [10] ,  [47] ,  [61] ,  [57] ,  [35]  are closely related to hand-extracted features, which have attracted increasing attention from the data mining and emotion recognition communities. These methods aim to learn the feature embeddings of raw data for subsequent downstream tasks such as classification and clustering. The classic conversational emotion recognition method based on machine learning is to use support vector machine to map emotional features to a hyperplane and classify them  [85] ,  [3] . However, these methods require a large amount of high-quality labeled data.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conversational Emotion Recognition",
      "text": "Versus Convolutional Neural Network. The CNN-based emotion recognition methods  [63] ,  [44] ,  [42]  are the first deep learning method to solve the emotion classification problem historically  [41] . These CNN-based methods employ convolutional filters to extract semantic features of text so that the model can use supervised learning to understand the meaning of text. Similar to machine learning algorithms, CNN can also map emotional features into vector space through mapping functions. The difference is that this mapping function is learned in an end-to-end manner. Since the convolution kernel extracts local receptive field information, CNN cannot contain contextual semantic information.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Multi-Modal Conversational Emotion Recognition",
      "text": "Versus Recurrent Neural Network. The RNNbased emotion recognition methods  [64] ,  [100] ,  [69] ,  [42]  are developed on the basis of CNN, but they believe that contexts should be mutually influential and interdependent  [64] ,  [100] . These RNNbased methods usually use LSTM or GRU (to avoid gradient disappearance or gradient explosion) to extract semantic features including context. Similar to CNN, RNN can also map emotional features into vector space through mapping functions in an end-to-end manner.\n\n3.1.5 Multi-modal Conversational Emotion Recognition Versus Transformer. Similar to the RNNbased emotion recognition method, the Transformer-based emotion recognition methods  [36] ,  [56] ,  [102] ,  [81]  also extract semantic information including context, and completes subsequent emotion classification based on this  [56] . However, unlike RNN, Transformer's sequential context modeling ability is better than RNN. Therefore, the accuracy of the Transformer-based emotion recognition methods are significantly better than RNNs.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Multi-Modal Conversational Emotion Recognition",
      "text": "Versus Graph Neural Network. The GNNbased emotion recognition methods  [58] ,  [130] ,  [50] ,  [37] ,  [89]  inherit the idea of the RNN method, i.e., the contexts should interact and depend on each other  [20] . On the basis of RNN, GNNs believe that there is also a relationship of mutual influence between speakers. Therefore, GNNs model the dialogue relationship between speakers through the inherent properties of the graph structure.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Definitions And Preliminaries",
      "text": "The symbols used in this paper are listed in Table  3 . Now, we define the sets needed to understand this paper. In particular, we use uppercase letters for matrices and lowercase letters for vectors.\n\nDefinition 1 (Utterances context) The multi-modal conversational emotion recognition task aims to recognize the emotional changes (e.g., happiness, and sadness, etc) of speakers {ğ‘† 1 , ğ‘† 2 , . . . , ğ‘† ğ‘€ }",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "ğ¾",
      "text": "The context window size.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "ğ‘€",
      "text": "The number of the speakers.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "ğ¿",
      "text": "The number of utterances in a dialogue.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "ğ‘ˆ",
      "text": "The set of contextual utterence.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "R",
      "text": "The type of edge. W Learnable parameters.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A",
      "text": "The adjacency matrix of a graph.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "ğ‘š",
      "text": "The node properties of the graph.\n\nğ‘¥ ğ‘¡ âˆˆ R ğ‘‘ ğ‘‘-dimensional text feature vectors.\n\nğ‘¥ ğ‘ âˆˆ R ğ‘˜ ğ‘˜-dimensional audio feature vectors.\n\nğ‘¥ ğ‘£ âˆˆ R â„ â„-dimensional video feature vectors.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "ğ‘¥",
      "text": "Concatenated video, audio and text feature vectors.\n\nat the current moment ğ‘¡ in a dialogue. ğ¿ represents the number of utterances in a dialogue, ğ‘ˆ represent a set of contextual utterances, and ğ‘ˆ = {ğ‘¢ 1 , ğ‘¢ 2 , . . . , ğ‘¢ ğ¿ }.\n\nThe MCER task aims to correctly classify each utterance by incorporating contextual information. At the current moment ğ‘¡, the model needs to infer the speaker's emotion based on the context information {ğ‘¢ 1 , ğ‘¢ 2 , . . . , ğ‘¢ ğ‘¡ -1 }. We assume that the context window size is set to ğ¾. The set of contextual utterances is defined as follows:\n\nWhen the context window size is 6, the speaker's contextual utterances and predicted utterances are shown in Table  4 .\n\nDefinition 2 (Dialogue graph) A dialogue graph is represented as G = {V, E, R, W}, where V is a set of nodes in the graph, E is a set of edges, ğ‘£ ğ‘– âˆˆ V represents the ğ‘–-th node, ğ‘’ ğ‘– ğ‘— = (ğ‘£ ğ‘– , ğ‘£ ğ‘— ) âˆˆ E represents a directed edge from ğ‘£ ğ‘– to ğ‘£ ğ‘— , the relationship ğ‘Ÿ ğ‘– ğ‘— âˆˆ E represents that there is a dialog relationship between nodes ğ‘£ ğ‘– and ğ‘£ ğ‘— . The neighbor nodes of node ğ‘£ are represented as ğ‘ (ğ‘£) = {ğ‘¢ âˆˆ V |(ğ‘£, ğ‘¢) âˆˆ E}. A âˆˆ R ğ‘›Ã—ğ‘› means the adjacency matrix with A ğ‘– ğ‘— = 1 if ğ‘’ ğ‘– ğ‘— âˆˆ E, otherwise A ğ‘– ğ‘— = 0. X âˆˆ R ğ‘šÃ—ğ‘š represents the node properties of the graph. For the MCER task based on GCN, the speaker's utterance information is regarded as the node of the graph, and the dialogue relationship information between speakers is regarded as the edge of the graph.  Definition 3 (Problem definition) For a given multi-modal utterance sequence ğ‘ˆ , the MCER task requires using the utterance context information to determine a deep neural network ğ¹ (ğ‘¢ ğ‘– ) so that the output emotion label Å·ğ‘– is as close as possible to the real emotion label ğ‘¦ ğ‘– , ğ‘– âˆˆ {1, ..., ğ¿}. Deep neural networks can solve the optimal parameters by minimizing loss, and its loss is defined as follows:\n\nwhere ğ¿ represents the number of utterances in the dialogue, L is an indicator function.\n\nFrom the development history and related preliminary definitions of MCER, it can be seen that the process of multi-modal conversation emotion recognition mainly includes three aspects: multimodal feature extraction, multi-modal feature fusion representation, and emotion classification. The overall process is shown in Fig.  4 , and we will provide a comprehensive overview of these three aspects below.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Multi-Modal Feature Extraction",
      "text": "Multi-modal feature extraction (e.g., text, video and audio, etc) is one of the important techniques for emotion analysis. In this section, we introduce the process of using feature extraction methods to perform data preprocessing on text, video, and audio, and list some commonly used feature extraction methods. As shown in Table  5 , we count the multi-modal feature extraction techniques used by many deep learning methods.  distance to measure the similarity between words. Unlike traditional one-hot encoding methods, word embedding technology can map high-dimensional sparse feature vectors to low-dimensional dense feature vectors, thus saving a lot of computing resources and solving the problem that one-hot encoding cannot distinguish the semantic gap between words. A commonly used word embedding method is Word2Vec  [8] , which contains two different forms: CBOW  [21]  and Skip-gram  [13] . CBOW predicts the central word based on the surrounding words, and Skip-gram predicts the surrounding words based on the central word. Although the above methods can capture the semantic similarity between words, they require large datasets for training. Some recent studies use simple TextCNN  [41]  and GLOVE  [16]  to extract text features. In addition, large-scale predictive pre-training models such as BERT  [65]  and RoBERTa  [40]  are often used, which capture contextual information through attention mechanisms.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Video Feature Extraction.",
      "text": "Visual feature extraction is mainly to extract information such as facial expressions and gestures that contain the speaker's emotions from the video. In recent years, deep neural networks have been able to extract deep features from images in an end-to-end learning manner, avoiding the tedious manual feature extraction. For example, Tran et al.  [101]  proposed an effective and efficient 3D-CNN to process video frames containing spatio-temporal features.\n\nNowadays, most of the visual features are extracted by some advanced neural networks (e.g., CNN  [38, 107] , and Transformer  [22, 23] , etc) or some open source toolkits (e.g., OKAO Vision  [133] , OpenFace [1], CERT  [4]  and Facet  [2] , etc). OKAO Vision extracts the speaker's facial expression in each frame of the video, and finally gets the speaker's smile intensity (0-100) and eye gaze direction. CERT can adaptively extract visual features such as face and head pose. OpenFace 2.0 is capable of detecting facial landmarks, recognizing facial movements, and estimating eye gaze direction. Facet extracts visual features, including facial movements, head poses, HOG features and etc.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Audio Feature Extraction.",
      "text": "Deep learning has also begun to get more and more attention in research in audio feature extraction research. For example, LSTM  [111]  has been widely used to automatically extract acoustic characteristics. Poria et al.  [78]  uses CNN to extract features from the audio, and then enter the extracted audio features into the classifier for emotional classification.\n\nRecently, more and more emotion analysis models  [69, 104, 131]  have started to use open source toolkits such as CONVAREP  [14] , openSMILE  [43] , LibROSA  [95] , and OpenEAR  [87]  to extract audio features. Specifically, OpenEAR adaptively extracts a set of acoustic features (e.g., prosody, spectrum and sepstral, etc.) and uses Z-normalization to normalize the audio features. The features extracted by openSMILE consist of MFCC, pitch and sound intensity. The LibROSA Speech Toolkit extracts 33 frame-level acoustic features (i.e., 20-dimensional MFCC and CQT) including speaker intonation variations. Similar to other audio feature extraction methods, COVAREP can also be used to extract features such as 12-dimensional MFCC, maximum dispersion scale (MDQ) and Liljencrants-Fant (LF).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Taxonomy Of Multi-Modal Conversational Emotion Recognition Algorithms",
      "text": "In this section, we present a taxonomy of MCER modeling approaches. We categorize existing work into context-free modeling, sequential context modeling, distingguishing speaker modeling, and speaker relation modeling. We briefly introduce each method in the following.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Context-Free Modeling",
      "text": "These are mostly pioneering works on conversational emotion recognition. Context-free modeling methods aim to learn a feature representation for each sentence, which does not exploit the contextual information of the sentence  [127] ,  [88] ,  [62] . For example, some traditional machine methods (e.g., SVMs  [57, 85] , and decision trees  [10, 47] , etc) is used to extracts the feature representation of each sentence, and utilize the extracted sentence features to complete emotion classification. The above process assumes that each sentence is independent and does not influence each other. We introduce several common context-free modeling methods based on feature fusion below.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "4.",
      "text": "1.1 Add. The early fusion method based on addition operation obtains the final emotional feature representation by weigh-ted summation of different modality features  [11] . This fusion method is simple to operate and requires only a small amount of calculation. However, its shortcomings are also obvious. It cannot model the context information in a fine-grained manner, and the information that can be utilized is limited. The formula for implementing the context-free modeling method using the additive approach is defined as follows:\n\nwhere â„ ğ‘’ represents the fused emotional vectors, ğ‘¥ ğ‘¡ , ğ‘¥ ğ‘ , ğ‘¥ ğ‘£ represent the text, audio, and video vectors, respectively.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Concatenation.",
      "text": "The early fusion method based on concatenation operation obtains the final emotion feature representation by concatenating and merging different modal features  [6] .\n\nAlthough this fusion method does not introduce additional calculations, it leads to very high dimensionality of the data, which makes calculations difficult. Furthermore, it also fails to capture intra-modal and inter-modal semantic information that is complementary.\n\nwhere ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (â€¢) represents concatenation operation.\n\nwhere ğ‘ƒ ( Åµğ‘ |ğœ† ğ‘ ğ‘– , ğœ† ğ‘£ ğ‘— , ğœ† ğ‘¡ ğ‘˜ , ğ‘¥ ğ‘ , ğ‘¥ ğ‘£ , ğ‘¥ ğ‘¡ ) represents the probability of predicted label. 4.1.4 SVM. SVM is a machine learning algorithm for classification and regression whose optimization goal is to find a hyperplane (a straight line in two-dimensional space, and a hyperplane in high-dimensional space) that separates samples of different classes. Based on the above research, Perez-Rosas et al.  [75]  concatenate multi-modal features as input vectors and use SVM to classify utterances for emotion. SVM works better for binary classification problems, but is less effective in multi-classification problems, and is only suitable for training small-scale data sets. The formula of SVM is defined as follows:\n\nwhere\n\nrepresents the kernel function, ğ‘ is the number of the samples. 4.1.5 Multiple Kernel Learning. After preprocessing the features of three different modalities, Poria et al.  [77]  constructed two different feature selectors to achieve feature dimensionality reduction. One of the feature selectors is based on circular correlated feature subset selection (CFS), and the other is based on principal component analysis (PCA). The above two feature selectors can not only eliminate redundant information and noise information, but also improve the running speed of the model. After feature selection and dimensionality reduction, the researchers spliced and merged the processed feature vectors and trained a classifier using a multi-kernel learning (MKL) algorithm  [77] . Based on the previous research work, the authors further propose the Convolutional Recurrent Multi-kernel Learning (CRMKL)  [79]  model. CRNKL uses a convolutional recurrent neural network for emotion detection, which can extract contextual information. The formula of MKL is defined as follows:\n\nwhere ğ‘¦ ğ‘– is the true label, ğ›¼, ğ›½ are the learnable parameters, ğ‘€ is the feature dimension.\n\n4.1.6 Select-Additive Learning CNN. CNN is a classic neural network in visual tasks and cannot be directly used for emotion recognition. In order to solve this problem, Kim et al.  [41]  proposed the TextCNN model, and its overall process is shown in Fig.  5 . In order to perform multi-modal emotion recognition, Wang et al.  [105]  proposed the SAL-CNN model, which first uses multi-modal data to fully train the CNN, and then uses Select-Additive Learning (SAL) to improve its versatility and prevent the model from overfitting during training. The SAL method consists of two phases (i.e., selection and addition). In the selection phase, SAL preserves important features and removes noisy information from the latent feature representations learned from neurons. In the addition phase, SAL improves the model's noise immunity by adding Gaussian noise to the feature representation. The SAL method improves the generalization performance of deep fusion models.\n\nThe formula for extracting text features by CNN is defined as follows:\n\nwhere âŠ• represents concatenation operator, ğœ” represents convolution filter, ğ‘ ğ‘– represents the feature representation within a window, ğ‘“ (â€¢) represents activation function. Convolutional filters are used to extract features from all sentences to generate feature maps:\n\nThe max pooling operation is used to capture the most critical semantic information in the sentence.\n\nIt can be seen from the processing flow of the convolutional neural network that using CNN to extract text features does not contain contextual information, i.e., it is assumed that each sentence is independent of each other.\n\n4.1.7 Tensor Fusion Network. The tensor-based feature fusion method mainly calculates the tensor product of different modal feature representations through Cartesian product to obtain the fused tensor representation  [73] . Therefore, the above methods need to first map the input multi-modal feature representation into a high-dimensional space, and then map it back to a low-dimensional tensor space for emotion representation. Tensor-based methods are able to capture important high-order interaction information across time, space, and modality. However, the computational complexity of tensor methods is very high and grows exponentially, and there is no fine-grained semantic information interaction between modalities. Zadeh et al.  [121]  proposed the multi-modal tensor fusion network TFN. TFN adopts the method of tensor fusion, which can simulate the interaction process between the three modalities of text, audio and video, and effectively fuse multi-modal features. Although TFN can effectively model information interaction within and between modalities, the model complexity of the TFN method is related to the dimensionality of multi-modal features and grows exponentially. The formula of TFN is defined as follows:\n\nwhere the extra dimension with 1 is used to perform modal interaction. The Cartesian product is then used to fuse the three modal features as follows:\n\nwhere âŠ— represents the outer product, ğ‘¥ ğ‘š represents fused vectors.\n\n4.1.8 Low-rank Tensor Fusion Network. On the basis of TFN, in order to more efficiently fuse multi-modal data, Liu et al.  [60]  proposed a low-rank tensor method LFM to achieve dimensionality reduction of multi-modal features, so as to improve the fusion efficiency of multi-modal features as shown in Fig.  6 . LFM has achieved high performance on many different tasks.\n\nwhere w ğ‘ , w ğ‘£ , w ğ‘¡ represents the decomposed low-rank learnable tensor.\n\n4.1.9 Data Augmentation with Generative Adversarial Networks. Multimodal emotion recognition based on adversarial learning is an advanced direction in this field, which combines the principles of adversarial learning to improve the accuracy and robustness of emotion recognition  [84] ,  [120] .\n\nNext, we introduce the existing overall process of data augmentation based on adversarial generative networks. 1) Conditional GANs Conditional Generative Adversarial Network (cGAN)  [98]  is a variant of GAN that introduces conditional information to more precisely control the output of the generator. The core idea of cGAN is to pass additional condition information to the generator and discriminator during the generation process, thereby generating specific types of data based on given conditions. The main advantage of cGAN is its ability to precisely control the generation process in order to generate data that meets the conditional information. The optimization goal of cGAN is defined as follows:\n\nmin\n\nwhere ğ‘¥ represents real data, and ğ‘¦ represents extra information.\n\n2) Adversarial Autoencoders Adversarial Autoencoder (AAE)  [46]  combines the ideas of Autoencoder and GAN. The main goal of AAE is to make this encoding space more continuous and have better data generation capabilities while learning a compressed representation of data. The training objective function of AAE usually includes two parts: one is the reconstruction error of the autoencoder, which is used to ensure the quality of the encoding, and the other is the GAN loss, which is used to make the encoding distribution more continuous and closer to the real distribution. The formula is defined as follows:\n\nwhere ğ‘ ğ‘§ (ğ‘§) represents the prior distribution.\n\n3) Adversarial Data Augmentation Network Adversarial Data Augmentation Network (ADAN)  [106]  includes the following components: autoencoder ğ‘…(ğ¸ (ğ‘¥)), auxiliary classifier ğ¶ (ğ¸ (ğ‘¥)), generator ğº (ğ‘§, ğ‘¦) and discriminator ğ· (â„). First, ADAN aims to learn a latent representation of the input data x in order to preserve the emotional information in it. Second, it attempts to ensure that the generated latent representation is consistent with the emotional information of the input data by matching the posterior distribution ğ‘ (â„|ğ‘§, ğ‘¦) with ğ‘ (â„|ğ‘¥). Third, ADAN simultaneously strives to minimize the reconstruction error between the input data ğ‘¥ and its reconstructed version x to ensure high-quality data reconstruction. The generator ğº (ğ‘§, ğ‘¦) accepts a sample ğ‘§ drawn from an M-dimensional Gaussian distribution and a one-hot encoding of the emotion label ğ‘¦ as input, and the goal is to generate samples in the latent space such that they are indistinguishable from real samples. The discriminator ğ· (â„) is optimized to distinguish whether the latent vector â„ comes from real data or from the generator.\n\nwhere ğ›¼ determines the contribution of classification error to model optimization.",
      "page_start": 11,
      "page_end": 16
    },
    {
      "section_name": "Sequential Context Modeling",
      "text": "Context-free modeling is conceptually important and has inspired later research on sequential context modeling  [103] . In particular, sequential context modeling methods consider that contextual sentences are mutually influential. Sequential context modeling approaches  [64, 100, 111]  consider each sentence influenced by its surrounding utterances. The main idea is to generate a feature representation with rich contextual semantic information by combining its own utterance representation ğ‘¥ ğ‘– with the surrounding contextual sentence representation\n\nwhere ğ‘˜ represents the context window size. Different from the context-free modeling method, the sequential context modeling method obtains a better feature representation by setting a memory network to preserve the context information of the sentence. Taking Fig.  7  as an example, a LSTM or Transformer is used to extract contextual information for three modalities of video, audio and text. The sequential context modeling approach plays an important role for many other MCER modeling approaches. LSTM is a variant of RNN that can remember contextual information. Specifically, LSTM models long-distance dependent context through cellular units and can solve the vanishing gradient problem. Each LSTM consists of input gate ğ‘— ğ‘¡ , output gate ğ‘‚ ğ‘¡ , cell state ğ¶ ğ‘¡ , and forget gate ğ‘“ ğ‘¡ .\n\nwhere ğœ represents activation function.\n\nAfter LSTM was used in multi-modal conversational emotion recognition, many other works were proposed to extract contextual emotional information. Lu et al.  [63]  proposed a multi-scale LSTM multi-modal emotion recognition model, which uses LSTM to extract low-level and highlevel local emotional features in multi-modal features. This method can capture subtle changes in complex expressions in a more fine-grained manner and implement an information feedback mechanism. However, it cannot capture the status information of the utterance and the status information of the speaker. Existing models ignore modal alignment and directly fuse information on different modal features. Modal alignment can eliminate the heterogeneity of single-modal features and obtain accurate emotional representations of different modal features. Based on this current situation, Hou et al.  [29]  proposed a semantic alignment network based on multi-space learning, which uses LSTM to extract emotional features of different modalities and obtains high-level emotional representations as supervisory signals for modal alignment. This method can capture the global correlation between different modalities and achieve feature fusion between modalities.\n\nTransformers are another way of modeling sequential context  [36, 49, 132 ]. Transformer's longdistance modeling capabilities are far superior to recurrent neural networks like Bi-LSTM, and Transformer can achieve parallel computing. Therefore, existing research on multi-modal emotion recognition based on sequential context modeling often regards Transformer as an important technology. The implementation details of Transformer are as follows.\n\nFirstly, video, audio and text features (i..e., ğ‘¥ ğ‘¡ , ğ‘¥ ğ‘ , ğ‘¥ ğ‘£ ) are concatenated into a fusion vector. The formula is defined as follows:\n\nwhere ğ‘„, ğ¾, ğ‘‰ represent the query vector, key vector and value vector of multi-modal features, respectively. Secondly, we use a feedforward neural network to perform multiple linear transformations on ğ‘„, ğ¾, and ğ‘‰ . The formula is defined as follows:\n\nwhere ğ‘š represents the number of linear transformations.\n\nWe then perform multi-head attention in parallel to obtain emotion feature representation: where ğ» â„ğ‘’ğ‘ğ‘‘ represents the emotion feature vectors. Finally, we use sine and cosine position encoding to obtain the position information of the emotion sequence:",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Multi-Modal Features",
      "text": "where pos is the index of the ğ‘–-th sentence, position encoding information is fused into ğ‘„, ğ¾, and ğ‘‰ .\n\nAfter Transformer was proposed, many Transformer-based multi-modal conversational emotion recognition research methods were proposed to model long-distance context dependencies  [17, 27] . In view of the inability of previous works to model long-distance dependencies between different modal features, Yang et al.  [113]  proposed a multi-modal speech emotion recognition method using context Transformer, which improves the emotional representation of the current utterance by embedding contextual information. This method can adaptively learn feature fusion between modalities.\n\nIn view of the fact that existing methods cannot dynamically identify subtle changes in emotion in multi-modal features and multi-scale features in different modal features, Liu et al.  [59]  proposed a multi-scale self-attention fusion emotion recognition method, which uses the self-attention mechanism to extract context-related dependencies in multi-modal features. Therefore, there is potential to use Transformers to model long-distance context dependencies. This method combines bc-LSTM and a multi-head attention mechanism to achieve fine-grained emotional information mining, and uses feature-level fusion and decision-level fusion methods to experiment with crossmodal feature fusion.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Distinguishing Speaker Modeling",
      "text": "The distinguishing speakers modeling method considers that the speaker's emotion is not only related to the global context, but also related to the speaker's own emotional state. Take Fig.  8  as an example, there are three GRU states (i.e., a global GRU, an emotional GRU and a speaker GRU). The global GRU is utilized to extract global multi-modal information and speaker's emotional state information. The speaker GRU is used to fuse the semantic information with context captured by the attention mechanism and the speaker's emotional state information. The emotion GRU combines the speaker's emotional state information and global context information to complete the final emotion classification.\n\nGlobal GRU captures the contextual semantic information of an utterance by modeling the utterance and speaker states. Each speaker state is used to memorize a speaker-specific representation of an utterance. By distinguishing the subordination relationship between speakers and utterances, it is beneficial to model the dependency relationship between speakers and utterances, thereby enhancing the semantic representation ability of context. The formula of Global GRU is defined as follows:\n\nğ‘” ğ‘¡ = ğºğ‘…ğ‘ˆ G (ğ‘¥ ğ‘¡ -1 , (ğ‘¥ ğ‘¡ âŠ• ğ‘ ğ‘  (ğ‘¥ ğ‘¡ ),ğ‘¡ -1 )) (23) where ğ‘” ğ‘¡ represents the latent feature representation of the global state, ğ‘ ğ‘  (ğ‘¥ ğ‘¡ ) represents the speaker state of the current utterance ğ‘¥ ğ‘¡ .\n\nSpeakers typically reply to conversations based on contextual information from other speakers. Therefore, speaker GRU extracts the context ğ‘ ğ‘¡ related to the utterance ğ‘¥ ğ‘¡ . The formula is defined as follows:\n\nwhere ğ‘Š ğ›½ is the learnable parameters. We first calculate the attention score of the global state in the previous ğ‘¡ -1 time. The attention score give higher weight to utterances related to utterance ğ‘¥ ğ‘¡ . The final context vector ğ‘ ğ‘¡ is obtained by the dot product of the attention score ğ›½ and the global state ğ‘” ğ‘¡ .\n\nğ‘ ğ‘  (ğ‘¢ ğ‘¡ ),ğ‘¡ = ğºğ‘…ğ‘ˆ P (ğ‘ ğ‘  (ğ‘¢ ğ‘¡ ),ğ‘¡ -1 , (ğ‘¢ ğ‘¡ âŠ• ğ‘ ğ‘¡ ))\n\n(25) The emotional representation et of the utterance ut is obtained by combining the speaker's state ğ‘ ğ‘  (ğ‘¢ğ‘¡ ),ğ‘¡ and the utterance ğ‘’ ğ‘¡ -1 at time ğ‘¡ -1. One underlying intuition is that context has a greater impact on utterance ğ‘¢ ğ‘¡ , and ğ‘’ğ‘¡ -1 integrates emotional contextual information from other parties' states into the emotional representation et. Therefore, we use the Emotion GRU unit to model ğ‘’ ğ‘¡ 1 , and the formula is defined as follows:\n\nThe emotion representation ğ‘’ ğ‘¡ that combines context information and speaker status information is used for the final emotion classification.\n\nIn modeling methods based on distinguishing between speakers, Ghosal et al.  [19]  proposed COSMIC, which clarifies the relationship between the speaker and the utterance, and introduces common sense knowledge to enhance the emotional understanding of the model. COSMIC can learn a variety of different prior knowledge (e.g., event relationships and causal relationships, etc.), and can distinguish speaker information and dynamically detect the speaker's emotional changes.\n\nIn view of the fact that existing methods cannot pay attention to the correlation between utterances and speakers and the lack of interaction between speakers, Zhang et al.  [126]  proposed a conversational interaction model, which extracts contextual semantic information and state interaction information of utterances through stacked global interaction modules. In addition, this method also implements adversarial feature representation of the model by introducing noise information. Experimental results prove that adversarial learning can improve the performance of emotion recognition.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Speaker Relationship Modeling",
      "text": "4.4.1 GNN for speaker relationship modeling. The speaker relationship modeling method innovatively introduces graph neural network to capture the speaker's dialogue relationship information while extracting sequential context information. Taking Fig.  9  as an example, it extracts dialogue relationships between speakers and inter-speaker dependencies by constructing a speaker relationship graph.\n\nGCN extends convolution operations into graph-structured data to extract structural information. GCN performs first-order neighbor information aggregation and spectral domain estimation. The formula of GCN is defined as follows:\n\nwhere ğ‘¾ (ğ‘™ ) is the learnable parameters, Ãƒ = ğ´ + ğ¼ ğ‘› , ğ¼ ğ‘› is the identity matrix, Dğ‘–ğ‘– = ğ‘— Ã£ğ‘– ğ‘— . ğ‘¯ (ğ‘™+1) represents the latent feature representations of layer ğ‘™ + 1.\n\nThe steps to apply GCN to the field of multi-modal emotion recognition are as follows. First, each utterance is represented as a node in the graph, and edge relationships are constructed based on the context between utterances (i.e., if there is a dialogue between utterances, an edge is constructed). We then apply GCN to the constructed dialogue graph for speaker-level information extraction. Through the above process, the model can dynamically learn the correlation between sentences. According to the definition of Equation  27 , our formula for aggregating surrounding contextual utterence information is deformed as follows:\n\nwhere ğ‘Š ğœƒ 1 and ğ‘Š ğœƒ 2 are the learnable parameters, N ğ‘Ÿ ğ‘– represents the neighbor node under the relationship ğ‘Ÿ âˆˆ R.\n\nGAT is a variant of GCN that aggregates surrounding neighbor node features through learnable weights with an attention mechanism. GAT captures the more important node features in the graph by calculating the degree of similarity between nodes. The formula for GAT is defined as follows:\n\nwhere ğ›¼ ğ‘– ğ‘— is the edge weight between node ğ‘– and node ğ‘—.\n\nSimilarly, the formula for using GAT to extract conversational relationships between speakers is defined as follows:\n\nThe multi-modal method based on GNN is the current mainstream research, which can consider context information and speaker relationship information simultaneously  [20] . In order to jointly learn the three tasks of sequential context information, multi-modal information interaction and multi-task representation, Zhang et al.  [130]  designed a multi-modal, multi-task interactive graph attention network (M3GAT) to simultaneously model context dependencies, multi-modal emotional interactions, and speaker dependencies. M3GAT can achieve cross-modal feature interaction, capture of sequential contextual semantic information, and correlation between tasks.\n\nIn view of the fact that existing graph fusion methods will cause the model to lose important semantic information and fail to eliminate redundant information in the model, Li et al.  [50]  proposed a graph network based on cross-modal feature complementarity, which effectively extracts the speaker's context and interaction information through multiple hypothesis spaces of the graph. This method eliminates the heterogeneity between modalities and fuses modal information by performing different message aggregation on different nodes and edge relationships in the graph, thereby extracting contextual information and speaker relationship information.\n\nAlthough existing MCER methods use GCN to model conversational relationships between speakers. In particular, the most competitive methods model the dependence of conversational relations between speakers and the importance between conversational relations by using relational graph attention networks. However, existing GCN-based multimodal conversational emotion recognition methods do not consider conversational relationships and sequential information in contextual relationships. Based on the above problems, Ishiwatari et al.  [37]  introduced relational position coding in RGAT to provide sequence information. The specific flow chart of RGAT is shown in Fig.  10 .\n\nThe position encoding formula used by RGAT is defined as follows:\n\nwhere ğ‘ƒğ¸ ğ‘– ğ‘—ğ‘Ÿ represents the relative position distance between node ğ‘– under relationship type ğ‘Ÿ and its surrounding neighbor nodes ğ‘—. The maximum relative position distance between nodes is clipped to ğ‘ or 4 , which represents the context window size. Nğ‘Ÿ (ğ‘–) represents the neighborhood of node ğ‘– under relationship type ğ‘Ÿ . In order to make the position encoding information learnable, FFN is used to obtain position embeddings.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Emotion Classification",
      "text": "After obtaining the multi-modal emotion feature representation, the MCER task uses a multilayer perceptron and a softmax layer to achieve the final emotion classification. The probability distribution of emotion categories is as follows:\n\nwhere ğ‘Š ğ‘™ ,ğ‘Š , ğ‘ ğ‘™ , ğ‘ are the learnable parameters, P ğ‘¡ is the probability distribution of emotion categories, Å·ğ‘¡ is the predicted labels.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "For MCER tasks, there are four commonly used evaluation indicators, i.e., accuracy rate, weighted average accuracy rate (WA), F1 value, and weighted average F1 value (WF1). These four indicators are defined as follows:\n\nWe assume that ğ‘ is the number of emotion labels in the dialogue emotion dataset, ğ¸ ğ‘— represents the total number of samples of emotion labels in the ğ‘—-th, ğ‘— âˆˆ [1, ğ‘ ].\n\n1) Accuracy represents the emotion recognition accuracy of the model, and the formula is defined as follows:\n\nwhere ğœ— 1 is the number of labels on a certain category of emotion. ğœ— 2 is the number that the model predicts on a certain category of emotion. ğ¸ ğ‘– ğ‘— means that the ğ‘–-th sample in the ğ‘—-th emotionally predicted correctly. ğ¸ ğ‘– ğ‘— âˆˆ [0, 1]. ğ‘† ğ‘š ğ‘— represents the ğ‘š-th sample of the ğ‘—-th emotion. The larger the value of ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ ğ‘— , the better the recognition effect of the model on the ğ‘—-th type of emotion.\n\n2) The F1 value is the F1-score of each emotion, and the formula is defined as follows:\n\nwhere ğ¸ ğ‘— ğ‘‡ ğ‘ƒ is the number of samples that the model predicts correctly on the ğ‘—-th category of emotion, ğ¸ ğ‘— ğ¹ ğ‘ƒ is the number of samples that the model predicts incorrectly on the ğ‘—-th category of emotion, and ğ¸ ğ‘— ğ¹ ğ‘ƒ is the number of emotions from other categories that the model predicts as the ğ‘—-th category of emotion. ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›(ğ¸ ğ‘— ğ‘‡ ğ‘ƒ , ğ¸ ğ‘— ğ¹ ğ‘ ) is the model's precision on the ğ‘—-th category of emotion, and ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ (ğ¸ ğ‘— ğ‘‡ ğ‘ƒ , ğ¸ ğ‘— ğ¹ ğ‘ƒ ) is the recall of the model on the ğ‘—-th emotion. f1 value combines the effects of both precision and recall metrics. Usually, the larger the value of f1, the better the prediction of the model.\n\n3) Weight accuracy (WA) is the weighted average of the classification accuracy of all emotion categories. The more samples of the ğ‘—-th emotion, the smaller the weight of the sample. The formula is defined as follows:  (36)  WA is the classification accuracy of the model combining all emotions. The larger the WA, the better the model performs on average across all classes.\n\n4) Weight F1 (WF1) is the weighted F1 value of all emotion categories. The more samples of the ğ‘—-th emotion, the smaller the weight of the sample. The formula is defined as follows:\n\nWF1 is the F1 value where the model integrates all emotions. WF1 is another effective index to evaluate the model effect. In general, the larger the WF1, the better the average performance of the model across all classes.",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Experimental Results",
      "text": "As Shown in Table  6 , we present the emotion recognition effects of different algorithms on multiple data sets (i.e., IEMOCAP, MELD, EmoryNLP and DailyDialogue). In particular, each algorithm uses multi-modal data, and we distinguish different MCER algorithms according to our classification method. Experimental results show that context-free based algorithms have the worst performance because they contain the least semantic information and cannot obtain good emotional feature representation. The multi-modal conversational emotion recognition algorithm based on sequential context has significant performance improvement compared to the context-free algorithm. The performance improvement may be attributed to the sequential context algorithm's ability to model the dependencies between contexts and its ability to utilize context information to improve the feature representation of emotions. The emotion recognition effects of modeling methods based on distinguishing speaker relationships and sequential context modeling methods are similar, and both are better than context-independent modeling methods. The performance improvement may be attributed to the ability of the distinguishing speakers modeling method to dynamically capture the speaker status information of the utterance and integrate it into the emotion representation information. The modeling method based on speaker relationship has the best performance and is currently the most popular modeling method. The modeling method based on speaker relationship mainly constructs the dialogue relationship between speakers through the inherent properties of the graph structure, and extracts the dialogue relationship representation between speakers through GCN. In addition, the speaker relationship modeling method can also consider the dependency information of the sequential context simultaneously.\n\nIn addition, we also counted the emotion recognition effects of different MCER algorithms on different emotion categories. As shown in Table  7 , on the IEMOCAP data set, the performance effects of each algorithm on various emotions are consistent with the overall results introduced previously. The method based on context-free modeling has the worst effect, with the recognition effect on the \"happy\" emotion being less than 50%. In comparison, most of the other three types of algorithms have exceeded 60%, and some categories of emotions have exceeded 80%. The performance of each algorithm on the MELD data set is shown in Table  8 . The recognition effects of each algorithm on most categories of emotions are similar to those on the IEMOCAP data set. It is important to note that we found that all emotion recognition methods have poor performance in identifying \"fear\" and \"disgust\" emotions, and the accuracy of some algorithms is even 0%. When we observe the distribution of the data set, we can find that the MELD data set has a serious data imbalance problem. This results in the model's very poor emotion recognition performance on minority classes.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Applications Of Multi-Modal Conversational Emotion Analysis",
      "text": "Emotion recognition is a method of applying natural language processing, machine learning, and deep learning techniques to multi-modal data such as text, video, and audio to identify and analyze the emotional state expressed in multi-modal data  [11] . Therefore, analyzing and studying the problem of emotion recognition has broad application value in many practical application scenarios.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Social Media Analysis",
      "text": "Multi-modal conversational emotion recognition has many broad applications in social media analysis  [128] . The most typical application is product improvement and innovation, that is, by analyzing user comments and feedback on social media, companies can understand users' preferences and dissatisfaction with products. This helps companies tweak product designs, improve functionality, and develop products that better meet user needs. Therefore, businesses can employ emotion analysis techniques to improve their products. In addition, emotion analysis can also help advertisers understand users' emotional attitudes towards advertisements, thereby optimizing advertisement content and strategies, and improving advertisement effectiveness.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Public Opinion Analysis",
      "text": "Multi-modal conversational emotion analysis also has a wide application value in the field of opinion mining, which can help mine and analyze people's opinions and emotions expressed in text, video and audio  [99] . For example, in market research, researchers can use emotion analysis techniques to analyze users' opinions on different products, as well as users' purchase intentions. This has great potential value for developing marketing strategies.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Recommendations Systems",
      "text": "Multi-modal conversational emotion analysis in recommender systems can help personalize recommendations more in line with users' emotions and preferences  [12] . For example, the recommendation system can recommend products that users are more interested in according to the emotional changes of consumers, and can perform emotion analysis on multi-modal data of user evaluations to realize real-time early warning and disposal of negative product evaluations.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Medical Care",
      "text": "Multi-modal conversational emotion analysis plays an important role in many aspects in the field of health care  [86] . It can help medical institutions and doctors better understand the current emotional state of patients, so as to give better treatment plans. For example, doctors can use emotion analysis technology to analyze patients' medical records and symptom descriptions, so as to better understand the patient's emotional state and help make more accurate diagnosis and treatment plans. Furthermore, in diagnosis and treatment decisions, understanding a patient's emotional state is important for developing an appropriate medical regimen. Emotion analysis can help doctors make more targeted decisions.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Financial Field Analysis",
      "text": "In the field of financial analysis, emotion analysis can help financial practitioners and investors better understand the emotional state of the market and predict market trends, thereby helping investors make correct investment decisions  [17] . For example, some financial institutions and financial practitioners use emotion analysis technology to calculate market emotion index to measure market emotion, which can provide investors with an objective reference value.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Social Robot",
      "text": "Multimodal conversational emotion recognition has many potential applications on social robots, which can enhance the capabilities of social robots and make them more intelligent and humane  [48] . Social robots can use multimodal emotion recognition to sense the emotional state of the users they interact with. This includes identifying users' facial expressions, voice emotions, text emotions, and other modal emotional signals  [45] . The robot can then adjust its interaction to better meet the user's emotional needs, providing support, comfort or entertainment. In addition, social robots can use MCER to better understand users' needs and emotional states to provide personalized suggestions and assistance. For example, in the field of mental health, robots can provide targeted psychological support suggestions based on the user's emotional state.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Research Challenges",
      "text": "Although deep learning technology has promoted the prosperity of MCER tasks, many scholars have proposed many state-of-the-art algorithms. However, building an accurate MCER model still faces challenges.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Scarcity Of Training Data",
      "text": "Multi-modal conversation emotion recognition models require sufficient and comprehensive emotional samples as a basis to achieve accurate prediction or classification of emotions. The existing multi-modal benchmark data sets IEMOCAP, MELD, and SEMAINE have only 11098, 5810, and 394 utterances, respectively. Unfortunately, although we can easily collect large amounts of multimodal conversation data from channels such as social media, the emotion labeling process is often expensive and time-consuming. In addition, the collected multi-modal data inevitably has problems such as ambiguous labels or multiple labels, which makes it a great challenge to obtain sufficient multi-modal labeled data, which in turn leads to the scarcity of multi-modal training data. Therefore, the scarcity of training data limits the effectiveness of current multi-modal conversational emotion recognition models.",
      "page_start": 27,
      "page_end": 28
    },
    {
      "section_name": "Data Is Heterogeneous And Noisy",
      "text": "Multi-modal conversation emotion recognition models need to fully eliminate heterogeneity and noise information between modalities to achieve accurate prediction or classification of emotions. Multi-modal data is naturally heterogeneous, and features of different modalities have huge differences in processing methods and representation forms. In addition, multi-modal conversation data contains a large amount of redundant or noisy information, and its emotion is usually determined by only a small amount of consistent key information, such as certain words in a sentence, a specific frequency band in speech, or a particular expression in a video. Even in some extreme cases, part of the modal information is basically unavailable under noise interference, such as ambiguous sentence expressions, noise in the speech, blocked expressions, etc. Therefore, the heterogeneity and noise of data limit the effectiveness of current multi-modal conversational emotion recognition models.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Unbalanced Data Distribution",
      "text": "Multi-modal dialogue data samples have serious imbalance problems, and the unbiased learning of the model is seriously interfered with. The multi-modal conversation emotion recognition model is based on cross-modal feature fusion, driven by emotion category sample data, and is easily affected by the number of emotion category samples. However, multi-modal conversation emotion data naturally suffers from the problem of category sample imbalance. A few emotion category samples account for a larger proportion, while most emotion category samples account for a small proportion. For example, in the MELD data set, the \"fear\" emotion only accounts for 1.91% of the total samples, and the \"disgust\" emotion only accounts for 2.61% of the total samples. A similar sample distribution also exists on the benchmark data set SEMAINE. Small samples are difficult to drive unbiased learning of the model, which seriously affects the model's prediction accuracy for small sample emotional categories. Therefore, the unbalanced sample distribution limits the effectiveness of current multi-modal conversational emotion recognition models.",
      "page_start": 27,
      "page_end": 28
    },
    {
      "section_name": "Consistent Semantic Association",
      "text": "Multi-modal conversation emotion recognition requires the model to learn the consistent semantics between modalities to filter noise information and eliminate heterogeneity between modalities, which is the basis for building an accurate multimodal dialogue emotion recognition model. However, the consistent semantic association of multi-modal conversation is more complicated and is not only related to the multi-modal context but also to factors such as the conversation scene, the speaker's own emotional inertia, and the speaker's stimulation. In addition, multi-modal data are heterogeneous, each modality has differentiated representation and distribution characteristics in space, and some consistent semantic associations are hidden in the feature distribution space between modalities. Therefore, efficiently performing consistent semantic association is the primary issue that needs to be considered at the model level.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Complementary Semantic Capture",
      "text": "Multi-modal conversation emotion recognition models need to establish accurate and consistent semantic associations and capture complementary semantic features between modalities, which can expand the emotional representation capabilities of a single modality. However, unlike consistency semantics, complementary semantics represent differences between modalities, and this difference may contain noise components. Therefore, consistency semantics and complementarity semantics are a pair of game entities, and how to balance the relationship between them is another issue that needs to be considered at the model level.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Multi-Model Collaboration",
      "text": "Multi-model collaboration is the third challenge faced at the model level in building accurate multimodal conversation emotion recognition models. Multi-modal conversation emotion recognition often requires the collaboration of multiple models to complete tasks, such as feature extraction models and feature fusion models. However, existing methods often perform task collaboration from the data level and ignore the collaborative relationship between models. Therefore, in order to achieve ideal synergistic results, not only the respective characteristics of the modes and their interrelationships need to be considered, but also the synergistic relationships between models need to be considered.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Future Work",
      "text": "",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Multi-Modal Conversation Data Generation",
      "text": "Multi-modal conversational emotion recognition models require sufficient and comprehensive emotional samples as a basis. When sample data is scarce, training multi-modal conversation emotion recognition models without causing overfitting or underfitting problems is extremely challenging. However, the sample size of existing benchmark data sets is relatively small, and there is a common problem of data scarcity. Multimodal dialogue data generation can effectively alleviate this problem. However, the distribution of multi-modal conversation data is more complex, and traditional single-modal data generation or cross-modal data generation models cannot meet the requirements. Therefore, there is an urgent need to solve the problem of collaborative generation of multi-modal conversation data.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Multi-Modal Feature Deep Fusion",
      "text": "Multi-modal feature fusion is crucial to the MCER task. The fused feature vector can represent the consistent semantics and complementary information between modalities. However, many different information interactions exist between multi-modalities, and many consistent or complementary features are hidden in multiple time series or local spatial correlations. Since multi-modal conversation data is heterogeneous and contains noise, there are significant differences in the temporal period and spatial distribution of different modal features, and the spatiotemporal importance between modalities is dynamic. Currently, few works consider this difference, and more efforts are still needed for deep fusion of multi-modal features.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Unbiased Emotional Learning",
      "text": "Many benchmark datasets in the field of multi-modal conversational emotion recognition suffer from serious sample category imbalance, that is, the minority emotion category contains a large amount of data, while the majority category emotion only contains a small amount of data. In the case of unbalanced data, the existing models tend to be biased towards fitting the minority emotion with a large amount of data, and the learning is insufficient on the majority emotion with a small amount of data, which leads to the model being in a small sample emotion category, resulting in the recognition accuracy is poor. Thus, the small-sample problem in multi-modal dialogue emotion recognition urgently requires further research.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Incomplete Multi-Modal Conversation Emotion Recognition",
      "text": "Each modality is not always available in real-world scenarios, which can lead to modal incompleteness problems. For example, the voice contains much noise, the expression is blocked, the light is dim, etc. At this moment, some modal information becomes unavailable due to noise interference. Modal integrity requirements reduce the applicability of multi-modal conversation emotion recognition methods. Therefore, cross-modal content recovery methods based on deep learning should continue to be developed to achieve multi-modal conversation emotion recognition in missing modalities.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Zero-Shot Multi-Modal Conversation Emotion Recognition",
      "text": "Affected by factors such as the complexity of emotions and the high cost of labeling, it is difficult to fully label some emotional samples. Furthermore, with the rapidly growing personal emotion annotation space, real-world emotion recognition systems may frequently encounter unseen emotion labels. Therefore, improving the generalization performance of emotion recognition models is an issue that needs to be considered. Deep methods utilizing zero-shot learning are expected to achieve better multi-modal dialogue emotion recognition.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Multi-Modal Conversation Multi-Label Emotion Recognition",
      "text": "In multi-modal conversation scenarios, existing emotion recognition models usually use a singlelabel supervised learning. Due to the ambiguity of emotions, emotion recognition in real life is often a multi-label task. The single-label requirement greatly limits the application scenarios of multi-modal conversation emotion recognition. Therefore, the multi-label emotion recognition problem in multi-modal conversation scenarios should be considered in future work.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Conclusion",
      "text": "This paper reviews the latest research results in the field of multi-modal conversational emotion recognition. To allow readers to implement emotion recognition tasks better, we have collected popular data sets in this field and given relevant download links. Since text, video, and audio are unstructured data that cannot be directly input into a computer for computation, we summarize some publicly available feature extraction methods. We divide emotion recognition methods into four categories, i.e., context-free modeling, sequential context modeling, distinguishing speaker modeling, and speaker relationship modeling. This paper further discusses the challenges faced by existing methods and future research directions. According to the review of existing work, it is found that multi-modal emotion recognition mainly improves the effect of emotion recognition by modeling intra-modal and inter-modal complementary semantic information. We hope this review can shed some light on developments in this field.",
      "page_start": 30,
      "page_end": 30
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of a multimodal conversation emotion recognition dataset which contains three modal",
      "page": 2
    },
    {
      "caption": "Figure 1: , MCER aims to extract semantic information complementary within and between",
      "page": 2
    },
    {
      "caption": "Figure 2: , we roughly divide the MCER methods into four categories, i.e., context-free",
      "page": 3
    },
    {
      "caption": "Figure 2: A taxonomy of modeling approaches for multi-modal conversational emotion recognition in conver-",
      "page": 4
    },
    {
      "caption": "Figure 3: Timeline of multimodal conversational emotion recognition algorithms.",
      "page": 6
    },
    {
      "caption": "Figure 3: , we counted multi-modal conversational emotion recognition algorithms from",
      "page": 6
    },
    {
      "caption": "Figure 4: The proposed MCER methods mainly include multi-modal feature extraction, multi-modal emotion",
      "page": 9
    },
    {
      "caption": "Figure 4: , and we will provide a comprehensive overview of these",
      "page": 9
    },
    {
      "caption": "Figure 5: The flowchart of the proposed TextCNN method. Specifically, given text features, the TextCNN",
      "page": 13
    },
    {
      "caption": "Figure 5: In order to perform multi-modal emotion",
      "page": 13
    },
    {
      "caption": "Figure 6: LFM has achieved high performance on many different tasks.",
      "page": 14
    },
    {
      "caption": "Figure 6: The overall flow chart of LFM. LFM mainly performs low-rank decomposition of the learnable",
      "page": 15
    },
    {
      "caption": "Figure 7: as an example, a LSTM",
      "page": 16
    },
    {
      "caption": "Figure 7: The flowchart of the proposed contextual modeling approach. Sequential context modeling methods",
      "page": 17
    },
    {
      "caption": "Figure 8: The flowchart of the proposed Distingguishing speakers modeling approach. The distinguishing",
      "page": 18
    },
    {
      "caption": "Figure 9: The flowchart of the proposed speaker relationship modeling approach. The speaker modeling method",
      "page": 20
    },
    {
      "caption": "Figure 9: as an example, it extracts dialogue re-",
      "page": 20
    },
    {
      "caption": "Figure 10: The flow chart of RGAT. RGAT mainly includes dialogue relationship dependency graph, speaker",
      "page": 21
    },
    {
      "caption": "Figure 10: The position encoding formula used by RGAT is defined as follows:",
      "page": 22
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "MELD": "Neutral\nSurprise\nFear\nSadness\nJoy\nDisgust\nAnger"
        },
        {
          "Methods": "",
          "MELD": "Acc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1"
        },
        {
          "Methods": "TextCNN [41]\nbc-LSTM [78]\nbc-LSTM+Att [78]\nA-DMN [112]\nDialogueRNN [69]\nCT-Net [56]\nDialogueCRN [33]\nSumAggGIN [90]\nDisGCN [96]\nMM-DFN [32]\nM2FNet [9]\nEmoCaps [53]\nLR-GCN [83]",
          "MELD": "76.23 74.91\n43.35 45.51\n4.63 3.71\n18.25 21.17\n46.14 49.47\n8.91 8.36\n35.33 34.51\n78.45 73.84\n46.82 47.71\n3.84 5.46\n22.47 25.19\n51.61 51.34\n4.31 5.23\n36.71 38.44\n70.45 75.55\n46.43 46.35\n0.00 0.00\n21.77 16.27\n49.30 50.72\n0.00 0.00\n41.77 40.71\n76.54 78.92\n56.24 55.35\n8.22 8.61\n22.14 24.94\n59.81 57.45\n1.23 3.45\n41.31 40.96\n72.12 73.54\n54.42 49.47\n1.61 1.23\n23.97 23.83\n52.01 50.74\n1.52 1.73\n41.01 41.54\n75.61 77.45\n51.32 52.76\n5.14 10.09\n30.91 32.56\n54.31 56.08\n11.62 11.27\n42.51 44.65\n70.91 75.73\n47.32 47.18\n0.00 0.00\n34.06 13.29\n41.95 49.72\n0.00 0.00\n41.66 35.69\n78.19 77.82\n52.27 54.11\n2.17 2.31\n35.79 36.43\n54.15 55.07\n4.05 2.12\n48.31 47.22\n70.84 76.67\n42.71 46.13\n1.17 1.55\n32.08 16.97\n50.03 50.17\n2.35 1.99\n38.25 39.97\n78.17 77.76\n52.15 50.69\n0.00 0.00\n25.77 22.93\n56.19 54.78\n0.00 0.00\n48.31 47.82\n50.09 47.03\n17.69 25.24\n72.88 67.98\n72.76 58.66\n5.57 3.45\n68.49 65.50\n57.33 55.25\n58.79 57.54\n75.24 77.12\n63.57 63.19\n3.45 3.03\n43.78 42.52\n58.34 57.05\n7.01 7.69\n81.51 80.83\n55.42 57.11\n0.00 0.00\n36.36 36.96\n62.21 65.84\n7.32 11.07\n52.63 54.74"
        }
      ],
      "page": 25
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A Comprehensive Survey on Multi-modal Conversational Emotion Recognition with Deep Learning 111:25 Table 7. On the IEMOCAP dataset, we counted the emotion recognition effects of different MCER algorithms on different emotion categories",
      "venue": "A Comprehensive Survey on Multi-modal Conversational Emotion Recognition with Deep Learning 111:25 Table 7. On the IEMOCAP dataset, we counted the emotion recognition effects of different MCER algorithms on different emotion categories"
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "M2fnet"
      ],
      "venue": ""
    },
    {
      "citation_id": "4",
      "title": "",
      "authors": [
        "Ct-Net"
      ],
      "venue": ""
    },
    {
      "citation_id": "5",
      "title": "On the MELD dataset, we counted the emotion recognition effects of different MCER algorithms on different emotion categories. The best result in each column is in bold",
      "venue": "Methods MELD Neutral Surprise Fear Sadness Joy Disgust Anger Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 TextCNN"
    },
    {
      "citation_id": "7",
      "title": "",
      "authors": [
        "Ct-Net"
      ],
      "venue": ""
    },
    {
      "citation_id": "8",
      "title": "",
      "authors": [
        "M2fnet"
      ],
      "venue": ""
    },
    {
      "citation_id": "9",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrusaitis",
        "Amir Zadeh",
        "Chong Lim",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "10",
      "title": "What makes an engaged employee? A facet-level approach to trait emotional intelligence as a predictor of employee engagement",
      "authors": [
        "Carmen Amador",
        "Luke Treglown"
      ],
      "year": "2020",
      "venue": "Personality and Individual Differences"
    },
    {
      "citation_id": "11",
      "title": "Bagged support vector machines for emotion recognition from speech",
      "authors": [
        "Anjali Bhavan",
        "Pankaj Chauhan",
        "Rajiv Ratn Shah"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "12",
      "title": "Cnn-cert: An efficient framework for certifying robustness of convolutional neural networks",
      "authors": [
        "Akhilan Boopathy",
        "Tsui-Wei Weng",
        "Pin-Yu Chen",
        "Sijia Liu",
        "Luca Daniel"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "14",
      "title": "Benchmarking multimodal sentiment analysis",
      "authors": [
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Hussain",
        "Subramanyam"
      ],
      "year": "2017",
      "venue": "Computational Linguistics and Intelligent Text Processing: 18th International Conference"
    },
    {
      "citation_id": "15",
      "title": "Understanding emotions in text using deep learning and big data",
      "authors": [
        "Ankush Chatterjee",
        "Umang Gupta",
        "Manoj Kumar Chinnakotla",
        "Radhakrishnan Srikanth",
        "Michel Galley",
        "Puneet Agrawal"
      ],
      "year": "2019",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "16",
      "title": "Audio word2vec: Sequence-to-sequence autoencoding for unsupervised learning of audio segmentation and representation",
      "authors": [
        "Yi-Chen Chen",
        "Sung-Feng Huang",
        "Hung-Yi Lee",
        "Yu-Hsuan Wang",
        "Chia-Hao Shen"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "17",
      "title": "Pankaj Wasnik, and Naoyuki Onoe. 2022. M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Shah"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition in speech signal using emotion-extracting binary decision trees",
      "authors": [
        "JarosÅ‚aw Cichosz",
        "Krzysztof Slot"
      ],
      "year": "2007",
      "venue": "Proceedings of Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "19",
      "title": "A survey of textual emotion recognition and its challenges",
      "authors": [
        "Jiawen Deng",
        "Fuji Ren"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Tsception: Capturing temporal dynamics and spatial asymmetry from EEG for emotion recognition",
      "authors": [
        "Yi Ding",
        "Neethu Robinson",
        "Su Zhang",
        "Qiuhao Zeng",
        "Cuntai Guan"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Cross-network skip-gram embedding for joint network alignment and link prediction",
      "authors": [
        "Xingbo Du",
        "Junchi Yan",
        "Rui Zhang",
        "Hongyuan Zha"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "22",
      "title": "Manifestation of depression in speech overlaps with characteristics used to represent and recognize speaker identity",
      "authors": [
        "Sri Harsha Dumpala",
        "Katerina Dikaios",
        "Sebastian Rodriguez",
        "Ross Langley",
        "Sheri Rempel",
        "Rudolf Uher",
        "Sageev Oore"
      ],
      "year": "2023",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "23",
      "title": "Naive Bayes for regression",
      "authors": [
        "Eibe Frank",
        "Leonard Trigg",
        "Geoffrey Holmes",
        "Ian Witten"
      ],
      "year": "2000",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "24",
      "title": "Semglove: Semantic co-occurrences for glove from bert",
      "authors": [
        "Leilei Gan",
        "Zhiyang Teng",
        "Yue Zhang",
        "Linchao Zhu",
        "Fei Wu",
        "Yi Yang"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Emonet: A transfer learning framework for multi-corpus speech emotion recognition",
      "authors": [
        "Maurice Gerczuk",
        "Shahin Amiriparian",
        "Sandra Ottl",
        "Bjorn Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Contextual inter-modal attention for multi-modal sentiment analysis",
      "authors": [
        "Deepanway Ghosal",
        "Shad Md",
        "Dushyant Akhtar",
        "Chauhan"
      ],
      "year": "2018",
      "venue": "proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "27",
      "title": "COSMIC: COmmonSense knowledge for eMotion Identification in Conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "28",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Depression intensity estimation via social media: a deep learning approach",
      "authors": [
        "Shreya Ghosh",
        "Tarique Anwar"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "30",
      "title": "A survey on vision transformer",
      "authors": [
        "Kai Han",
        "Yunhe Wang",
        "Hanting Chen",
        "Xinghao Chen",
        "Jianyuan Guo",
        "Zhenhua Liu",
        "Yehui Tang",
        "An Xiao",
        "Chunjing Xu",
        "Yixing Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Transformer in transformer",
      "authors": [
        "Kai Han",
        "An Xiao",
        "Enhua Wu",
        "Jianyuan Guo",
        "Chunjing Xu",
        "Yunhe Wang"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "Dictionary based approach to sentiment analysis-a review",
      "authors": [
        "Tanvi Hardeniya",
        "A Dilipkumar",
        "Borikar"
      ],
      "year": "2016",
      "venue": "International Journal of Advanced Engineering, Management and Science"
    },
    {
      "citation_id": "33",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "34",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "35",
      "title": "Conversational transfer learning for emotion recognition",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Roger Zimmermann",
        "Rada Mihalcea"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "36",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "37",
      "title": "Semantic Alignment Network for Multi-Modal Emotion Recognition",
      "authors": [
        "Mixiao Hou",
        "Zheng Zhang",
        "Chang Liu",
        "Guangming Lu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "38",
      "title": "EmotionLines: An Emotion Corpus of Multi-Party Conversations",
      "authors": [
        "Chao-Chun",
        "Sheng-Yeh Hsu",
        "Chuan-Chun Chen",
        "Ting-Hao Kuo",
        "Lun-Wei Huang",
        "Ku"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "39",
      "title": "Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations",
      "authors": [
        "Dou Hu",
        "Yinan Bao",
        "Lingwei Wei",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "year": "2023",
      "venue": "Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations",
      "arxiv": "arXiv:2306.01505"
    },
    {
      "citation_id": "40",
      "title": "MM-DFN: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lianxin Jiang",
        "Yang Mo"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "41",
      "title": "DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "42",
      "title": "UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition",
      "authors": [
        "Guimin Hu",
        "Ting-En Lin",
        "Yi Zhao",
        "Guangming Lu",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "43",
      "title": "GMM supervector based SVM with spectral features for speech emotion recognition",
      "authors": [
        "Hao Hu",
        "Ming-Xing Xu",
        "Wei Wu"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07"
    },
    {
      "citation_id": "44",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "Jian Huang",
        "Jianhua Tao",
        "Bin Liu",
        "Zheng Lian",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "45",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "46",
      "title": "Review on Convolutional Neural Networks (CNN) in vegetation remote sensing",
      "authors": [
        "Teja Kattenborn",
        "Jens Leitloff",
        "Felix Schiefer",
        "Stefan Hinz"
      ],
      "year": "2021",
      "venue": "ISPRS journal of photogrammetry and remote sensing"
    },
    {
      "citation_id": "47",
      "title": "Time-frequency representation and convolutional neural network-based emotion recognition",
      "authors": [
        "K Smith",
        "Varun Khare",
        "Bajaj"
      ],
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "48",
      "title": "Randomly wired network based on RoBERTa and dialog history attention for response selection",
      "authors": [
        "Byoungjae Kim",
        "Jungyun Seo",
        "Myoung-Wan Koo"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "49",
      "title": "Convolutional Neural Networks for Sentence Classification",
      "authors": [
        "Yoon Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "50",
      "title": "Exploiting multi-cnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "51",
      "title": "Fake Speech Detection Using OpenSMILE Features",
      "authors": [
        "Devesh Kumar",
        "Pavan Kumar V Patil",
        "Ayush Agarwal",
        "Sr Mahadeva",
        "Prasanna"
      ],
      "year": "2022",
      "venue": "International Conference on Speech and Computer"
    },
    {
      "citation_id": "52",
      "title": "MLT-DNet: Speech emotion recognition using 1D dilated CNN based on multi-learning trick approach",
      "authors": [
        "Soonil Kwon"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "53",
      "title": "Informal caregivers disclose increasingly more to a social robot over time",
      "authors": [
        "Guy Laban",
        "Val Morrison",
        "Arvid Kappas",
        "Emily Cross"
      ],
      "year": "2022",
      "venue": "Chi Conference on Human Factors in Computing Systems Extended Abstracts"
    },
    {
      "citation_id": "54",
      "title": "Multi-task semisupervised adversarial autoencoding for speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Julien Epps",
        "BjÃ¶rn Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective computing"
    },
    {
      "citation_id": "55",
      "title": "Emotion recognition using a hierarchical binary decision tree approach",
      "authors": [
        "Chi-Chun Lee",
        "Emily Mower",
        "Carlos Busso",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "56",
      "title": "The unboxing experience: Exploration and design of initial interactions between children and social robots",
      "authors": [
        "Christine P Lee",
        "Bengisu Cagiltay",
        "Bilge Mutlu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "57",
      "title": "Hitrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "Jingye Li",
        "Donghong Ji",
        "Fei Li",
        "Meishan Zhang",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "58",
      "title": "Graphcfc: A directed graph based cross-modal feature complementation approach for multimodal conversational emotion recognition",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "59",
      "title": "BiERU: Bidirectional emotional recurrent unit for conversational sentiment analysis",
      "authors": [
        "Wei Li",
        "Wei Shao",
        "Shaoxiong Ji",
        "Erik Cambria"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "60",
      "title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "61",
      "title": "EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "62",
      "title": "AMOA: Global acoustic feature enhanced modal-order-aware network for multimodal sentiment analysis",
      "authors": [
        "Ziming Li",
        "Yan Zhou",
        "Weibo Zhang",
        "Yaxin Liu",
        "Chuanpeng Yang",
        "Zheng Lian",
        "Songlin Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "63",
      "title": "GCNet: graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Zheng Lian",
        "Lan Chen",
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "64",
      "title": "CTNet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "65",
      "title": "Speech emotion recognition based on HMM and SVM",
      "authors": [
        "Yi-Lin Lin",
        "Gang Wei"
      ],
      "year": "2005",
      "venue": "2005 International Conference on Machine Learning and Cybernetics"
    },
    {
      "citation_id": "66",
      "title": "Modeling intra-and inter-modal relations: Hierarchical graph contrastive learning for multimodal sentiment analysis",
      "authors": [
        "Zijie Lin",
        "Bin Liang",
        "Yunfei Long",
        "Yixue Dang",
        "Min Yang",
        "Min Zhang",
        "Ruifeng Xu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "67",
      "title": "Multi-modal speech emotion recognition using self-attention mechanism and multi-scale fusion framework",
      "authors": [
        "Yang Liu",
        "Haoqin Sun",
        "Wenbo Guan",
        "Yuqi Xia",
        "Zhen Zhao"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "68",
      "title": "Efficient Low-rank Multimodal Fusion With Modality-Specific Factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "69",
      "title": "Speech emotion recognition based on feature selection and extreme learning machine decision tree",
      "authors": [
        "Zhen-Tao Liu",
        "Min Wu",
        "Wei-Hua Cao",
        "Jun-Wei Mao",
        "Jian-Ping Xu",
        "Guan-Zheng Tan"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "70",
      "title": "Curriculum learning for speech emotion recognition from crowdsourced labels",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "71",
      "title": "Exploring multimodal data analysis for emotion recognition in teachers' teaching behavior based on LSTM and MSCNN",
      "authors": [
        "Yuanyuan Lu",
        "Zengzhao Chen",
        "Qiuyu Zheng",
        "Yanhui Zhu",
        "Mengke Wang"
      ],
      "year": "2023",
      "venue": "Soft Computing"
    },
    {
      "citation_id": "72",
      "title": "Emotion recognition using multimodal residual LSTM network",
      "authors": [
        "Jiaxin Ma",
        "Hao Tang",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM international conference on multimedia"
    },
    {
      "citation_id": "73",
      "title": "T-bertsum: Topic-aware text summarization based on bert",
      "authors": [
        "Tinghuai Ma",
        "Qian Pan",
        "Huan Rong",
        "Yurong Qian",
        "Yuan Tian",
        "Najla Al-Nabhan"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "74",
      "title": "Divide, conquer and combine: Hierarchical feature fusion network with local and global perspectives for multimodal affective computing",
      "authors": [
        "Sijie Mai",
        "Haifeng Hu",
        "Songlong Xing"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "75",
      "title": "Locally confined modality fusion network with a global perspective for multimodal human affective computing",
      "authors": [
        "Sijie Mai",
        "Songlong Xing",
        "Haifeng Hu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "76",
      "title": "Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis",
      "authors": [
        "Sijie Mai",
        "Ying Zeng",
        "Shuangjia Zheng",
        "Haifeng Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "77",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "78",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "Gary Mckeown",
        "Michel Valstar",
        "Roddy Cowie",
        "Maja Pantic",
        "Marc Schroder"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "79",
      "title": "A Multi-Message Passing Framework Based on Heterogeneous Graphs in Conversational Emotion Recognition",
      "authors": [
        "Tao Meng",
        "Yuntao Shou",
        "Wei Ai",
        "Jiayi Du",
        "Haiyan Liu",
        "Keqin Li"
      ],
      "year": "2021",
      "venue": "A Multi-Message Passing Framework Based on Heterogeneous Graphs in Conversational Emotion Recognition"
    },
    {
      "citation_id": "80",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "Louis-Philippe Morency",
        "Rada Mihalcea",
        "Payal Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on multimodal interfaces"
    },
    {
      "citation_id": "81",
      "title": "Attention gated tensor neural network architectures for speech emotion recognition",
      "authors": [
        "Sandeep Kumar Pandey",
        "Hanumant Singh Shekhawat",
        "Prasanna"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "82",
      "title": "Multimodal analysis and prediction of persuasiveness in online social multimedia",
      "authors": [
        "Sunghyun Park",
        "Suk Han",
        "Moitreya Shim",
        "Kenji Chatterjee",
        "Louis-Philippe Sagae",
        "Morency"
      ],
      "year": "2016",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)"
    },
    {
      "citation_id": "83",
      "title": "Utterance-level multimodal sentiment analysis",
      "authors": [
        "VerÃ³nica PÃ©rez-Rosas",
        "Rada Mihalcea",
        "Louis-Philippe Morency"
      ],
      "year": "2013",
      "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "84",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "BarnabÃ¡s PÃ³czos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "85",
      "title": "Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Alexander Gelbukh"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "86",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "87",
      "title": "Convolutional MKL based multimodal emotion recognition and sentiment analysis",
      "authors": [
        "Soujanya Poria",
        "Iti Chaturvedi",
        "Erik Cambria",
        "Amir Hussain"
      ],
      "year": "2016",
      "venue": "2016 IEEE 16th international conference on data mining (ICDM)"
    },
    {
      "citation_id": "88",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "89",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "Wasifur Rahman",
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Amir Zadeh",
        "Chengfeng Mao",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "90",
      "title": "A novel attention-based gated recurrent unit and its efficacy in speech emotion recognition",
      "authors": [
        "Srividya Tirunellai Rajamani",
        "Adria Kumar T Rajamani",
        "Shuo Mallol-Ragolta",
        "BjÃ¶rn Liu",
        "Schuller"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "91",
      "title": "Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "Minjie Ren",
        "Xiangdong Huang",
        "Wenhui Li",
        "Dan Song",
        "Weizhi Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "92",
      "title": "MALN: Multimodal Adversarial Learning Network for Conversational Emotion Recognition",
      "authors": [
        "Minjie Ren",
        "Xiangdong Huang",
        "Jing Liu",
        "Ming Liu",
        "Xuanya Li",
        "An-An Liu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "93",
      "title": "Ensemble of svm trees for multimodal emotion recognition",
      "authors": [
        "Sankaranarayanan Viktor RozgiÄ‡",
        "Shirin Ananthakrishnan",
        "Rohit Saleem",
        "Rohit Kumar",
        "Prasad"
      ],
      "year": "2012",
      "venue": "Proceedings of the 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "94",
      "title": "Emotion recognition for everyday life using physiological signals from wearables: A systematic literature review",
      "authors": [
        "Stanislaw Saganowski",
        "Bartosz Perz",
        "Adam Polak",
        "Przemyslaw Kazienko"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "95",
      "title": "Acoustic transparency in hearables-Perceptual sound quality evaluations",
      "authors": [
        "Henning Schepker",
        "Florian Denk",
        "Birger Kollmeier",
        "Simon Doclo"
      ],
      "year": "2020",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "96",
      "title": "A combined rule-based & machine learning audio-visual emotion recognition approach",
      "authors": [
        "Kah Phooi",
        "Li-Minn Ang",
        "Chien Shing Ooi"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "97",
      "title": "Directed Acyclic Graph Network for Conversational Emotion Recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "98",
      "title": "Summarize before aggregate: a global-to-local heterogeneous graph inference network for conversational emotion recognition",
      "authors": [
        "Dongming Sheng",
        "Dong Wang",
        "Ying Shen",
        "Haitao Zheng",
        "Haozhuang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "99",
      "title": "Graph Information Bottleneck for Remote Sensing Segmentation",
      "authors": [
        "Yuntao Shou",
        "Wei Ai",
        "Tao Meng"
      ],
      "year": "2023",
      "venue": "Graph Information Bottleneck for Remote Sensing Segmentation",
      "arxiv": "arXiv:2312.02545"
    },
    {
      "citation_id": "100",
      "title": "CZL-CIAE: CLIP-driven Zero-shot Learning for Correcting Inverse Age Estimation",
      "authors": [
        "Yuntao Shou",
        "Wei Ai",
        "Tao Meng",
        "Keqin Li"
      ],
      "year": "2023",
      "venue": "CZL-CIAE: CLIP-driven Zero-shot Learning for Correcting Inverse Age Estimation",
      "arxiv": "arXiv:2312.01758"
    },
    {
      "citation_id": "101",
      "title": "Object Detection in Medical Images Based on Hierarchical Transformer and Mask Mechanism",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Canhao Xie",
        "Haiyan Liu",
        "Yina Wang"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "102",
      "title": "Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Sihan Yang",
        "Keqin Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "103",
      "title": "Visualization of audio files using librosa",
      "authors": [
        "Shubham Suman",
        "Kshira Sagar Sahoo",
        "Chandramouli Das",
        "Ambik Jhanjhi",
        "Mitra"
      ],
      "year": "2022",
      "venue": "Proceedings of 2nd International Conference on Mathematical Modeling and Computational Science: ICMMCS 2021"
    },
    {
      "citation_id": "104",
      "title": "A discourse-aware graph neural network for emotion recognition in multi-party conversation",
      "authors": [
        "Yang Sun",
        "Nan Yu",
        "Guohong Fu"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "105",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Zhongkai Sun",
        "Prathusha Sarma",
        "William Sethares",
        "Yingyu Liang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "106",
      "title": "A discriminatively deep fusion approach with improved conditional GAN (im-cGAN) for facial expression recognition",
      "authors": [
        "Zhe Sun",
        "Hehao Zhang",
        "Jiatong Bai",
        "Mingyang Liu",
        "Zhengping Hu"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "107",
      "title": "Speech emotion recognition enhanced traffic efficiency solution for autonomous vehicles in a 5G-enabled space-air-ground integrated intelligent transportation system",
      "authors": [
        "Liang Tan",
        "Keping Yu",
        "Long Lin",
        "Xiaofan Cheng",
        "Gautam Srivastava",
        "Jerry Chun-Wei Lin",
        "Wei Wei"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "108",
      "title": "Advanced LSTM: A study about better time dependency modeling in emotion recognition",
      "authors": [
        "Fei Tao",
        "Gang Liu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "109",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "Du Tran",
        "Lubomir Bourdev",
        "Rob Fergus",
        "Lorenzo Torresani",
        "Manohar Paluri"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "110",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "111",
      "title": "",
      "authors": [
        "J Acm"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "112",
      "title": "A Comprehensive Survey on Multi-modal Conversational Emotion Recognition with Deep Learning",
      "venue": "A Comprehensive Survey on Multi-modal Conversational Emotion Recognition with Deep Learning"
    },
    {
      "citation_id": "113",
      "title": "Context-and sentiment-aware networks for emotion recognition in conversation",
      "authors": [
        "Geng Tu",
        "Jintao Wen",
        "Cheng Liu",
        "Dazhi Jiang",
        "Erik Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "114",
      "title": "Integrating Recurrence Dynamics for Speech Emotion Recognition",
      "authors": [
        "Efthymios Tzinis",
        "Georgios Paraskevopoulos"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "115",
      "title": "Select-additive learning: Improving generalization in multimodal sentiment analysis",
      "authors": [
        "Haohan Wang",
        "Aaksha Meghawat",
        "Louis-Philippe Morency",
        "Eric Xing"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "116",
      "title": "M2R2: Missing-Modality Robust emotion Recognition framework with iterative data augmentation",
      "authors": [
        "Ning Wang",
        "Hui Cao",
        "Jun Zhao",
        "Ruilin Chen",
        "Dapeng Yan",
        "Jie Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "117",
      "title": "CNN-generated images are surprisingly easy to spot... for now",
      "authors": [
        "Sheng-Yu Wang",
        "Oliver Wang",
        "Richard Zhang",
        "Andrew Owens",
        "Alexei Efros"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "118",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "119",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Yan Wang",
        "Jiayu Zhang",
        "Jun Ma",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "120",
      "title": "Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors",
      "authors": [
        "Yang Wu",
        "Yanyan Zhao",
        "Hao Yang",
        "Song Chen",
        "Bing Qin",
        "Xiaohuan Cao",
        "Wenting Zhao"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "121",
      "title": "Speech emotion classification using attention-based LSTM",
      "authors": [
        "Yue Xie",
        "Ruiyu Liang",
        "Zhenlin Liang",
        "Chengwei Huang",
        "Cairong Zou",
        "BjÃ¶rn Schuller"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "122",
      "title": "Adapted dynamic memory network for emotion recognition in conversation",
      "authors": [
        "Songlong Xing",
        "Sijie Mai",
        "Haifeng Hu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "123",
      "title": "Contextual and Cross-Modal Interaction for Multi-Modal Speech Emotion Recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Yang Liu",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "124",
      "title": "DEAL: An Unsupervised Domain Adaptive Framework for Graph-Level Classification",
      "authors": [
        "Nan Yin",
        "Li Shen",
        "Baopu Li",
        "Mengzhu Wang",
        "Xiao Luo",
        "Chong Chen",
        "Zhigang Luo",
        "Xian-Sheng Hua"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia (MM '22)",
      "doi": "10.1145/3503161.3548012"
    },
    {
      "citation_id": "125",
      "title": "CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification",
      "authors": [
        "Nan Yin",
        "Li Shen",
        "Mengzhu Wang",
        "Long Lan",
        "Zeyu Ma",
        "Chong Chen",
        "Xian-Sheng Hua",
        "Xiao Luo"
      ],
      "year": "2023",
      "venue": "CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification",
      "arxiv": "arXiv:2306.04979"
    },
    {
      "citation_id": "126",
      "title": "OMG: Towards Effective Graph Classification Against Label Noise",
      "authors": [
        "Nan Yin",
        "Li Shen",
        "Mengzhu Wang",
        "Xiao Luo",
        "Zhigang Luo",
        "Dacheng Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "doi": "10.1109/TKDE.2023.3271677"
    },
    {
      "citation_id": "127",
      "title": "Messages are Never Propagated Alone: Collaborative Hypergraph Neural Network for Time-Series Forecasting",
      "authors": [
        "N Yin",
        "L Shen",
        "H Xiong",
        "B Gu",
        "C Chen",
        "X Hua",
        "S Liu",
        "X Luo"
      ],
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2023.3331389"
    },
    {
      "citation_id": "128",
      "title": "Prediction Model of Dow Jones Index Based on LSTM-Adaboost",
      "authors": [
        "Runkai Ying",
        "Yuntao Shou",
        "Chang Liu"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Communications, Information System and Computer Engineering (CISCE)"
    },
    {
      "citation_id": "129",
      "title": "EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling for Emotion Recognition in Conversation",
      "authors": [
        "Li Liu Yingjian",
        "Wang Jiang",
        "Zeng Xiaoping",
        "Zhigang"
      ],
      "year": "2023",
      "venue": "EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling for Emotion Recognition in Conversation",
      "arxiv": "arXiv:2303.11117"
    },
    {
      "citation_id": "130",
      "title": "Noise Imitation Based Adversarial Training for Robust Multimodal Sentiment Analysis",
      "authors": [
        "Ziqi Yuan",
        "Yihe Liu",
        "Hua Xu",
        "Kai Gao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "131",
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "132",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "133",
      "title": "Multiattention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Prateek Vij",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "134",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "135",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho D Zahiri",
        "Choi"
      ],
      "year": "2017",
      "venue": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "arxiv": "arXiv:1708.04299"
    },
    {
      "citation_id": "136",
      "title": "Coin: Conversational interactive networks for emotion recognition in conversation",
      "authors": [
        "Haidong Zhang",
        "Yekun Chai"
      ],
      "year": "2021",
      "venue": "Proceedings of the Third Workshop on Multimodal Artificial Intelligence"
    },
    {
      "citation_id": "137",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "Jianhua Zhang",
        "Zhong Yin",
        "Peng Chen",
        "Stefano Nichele"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "138",
      "title": "Erik Cambria, and Xuelong Li. 2021. Real-time video emotion recognition based on reinforcement learning and domain knowledge",
      "authors": [
        "Ke Zhang",
        "Yuanqing Li",
        "Jingyu Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "139",
      "title": "ICDN: integrating consistency and difference networks by transformer for multimodal sentiment analysis",
      "authors": [
        "Qiongan Zhang",
        "Lei Shi",
        "Peiyu Liu",
        "Zhenfang Zhu",
        "Liancheng Xu"
      ],
      "year": "2023",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "140",
      "title": "M3GAT: A Multi-Modal Multi-Task Interactive Graph Attention Network for Conversational Sentiment Analysis and Emotion Recognition",
      "authors": [
        "Yazhou Zhang",
        "Ao Jia",
        "Bo Wang",
        "Peng Zhang",
        "Dongming Zhao",
        "Pu Li",
        "Yuexian Hou",
        "Xiaojia Jin",
        "Dawei Song",
        "Jing Qin"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Information Systems"
    },
    {
      "citation_id": "141",
      "title": "Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "142",
      "title": "Topic-driven and knowledge-aware transformer for dialogue emotion detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "143",
      "title": "Multimodal sentiment analysis based on fusion methods: A survey",
      "authors": [
        "Linan Zhu",
        "Zhechao Zhu",
        "Chenwei Zhang",
        "Yifei Xu",
        "Xiangjie Kong"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    }
  ]
}