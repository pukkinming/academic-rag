{
  "paper_id": "2406.08809v3",
  "title": "Are We There Yet? A Brief Survey Of Music Emotion Prediction Datasets, Models And Outstanding Challenges",
  "published": "2024-06-13T05:00:27Z",
  "authors": [
    "Jaeyong Kang",
    "Dorien Herremans"
  ],
  "keywords": [
    "Deep learning",
    "artificial intelligence",
    "music emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deep learning models for music have advanced drastically in recent years, but how good are machine learning models at capturing emotion, and what challenges are researchers facing? In this paper, we provide a comprehensive overview of the available music-emotion datasets and discuss evaluation standards as well as competitions in the field. We also offer a brief overview of various types of music emotion prediction models that have been built over the years, providing insights into the diverse approaches within the field. Through this examination, we highlight the challenges that persist in accurately capturing emotion in music, including issues related to dataset quality, annotation consistency, and model generalization. Additionally, we explore the impact of different modalities, such as audio, MIDI, and physiological signals, on the effectiveness of emotion prediction models. Through this examination, we identify persistent challenges in music emotion recognition (MER), including issues related to dataset quality, the ambiguity in emotion labels, and the difficulties of cross-dataset generalization. We argue that future advancements in MER require standardized benchmarks, larger and more diverse datasets, and improved model interpretability. Recognizing the dynamic nature of this field, we have complemented our findings with an accompanying GitHub repository 1 . This repository contains a comprehensive list of music emotion datasets and recent predictive models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Music has long been revered for its profound ability to evoke and convey emotions, transcending cultural, linguistic, and geographical barriers. Researchers from various fields have been captivated by the intricate interplay between music and human emotions for decades. Classic works such as Meyer's paper  [1]  and pioneering studies conducted by scholars such as Seashore  [2]  and Hevner  [3]  in the early 20th century laid the groundwork for understanding the emotional impact of music. However, understanding and quantifying this poses a significant challenge due to its multifaceted nature  [4] . In this paper, we provide an overview of the current state-ofthe-art along with challenges and directions for future work.\n\nWith the advent of technology and data-driven methodologies, new avenues have opened up for exploring the complex relationship between music and emotions, such as deep learning models. Despite these advancements, accurate predictions of emotions from music remain elusive. This is due to a number of reasons, including the subjective, personal nature of emotional perception, as well as bias and limitations in the current datasets, and challenges in benchmarking. We discuss these and other challenges at length in Section V.\n\nDespite these challenges, the potential applications of Music Emotion Recognition (MER) systems are vast and varied. In the healthcare domain, we may find personalized music recommendation systems that guide a listener to different emotional states  [5] , or we may even see MER systems used to build large-scale datasets on which we can train generative music AI systems that can be controlled to generate music with specific emotions  [6] -  [8] . In the same line of thought,  [9]  developed a Brain-Computer Interface system that can provide musical feedback about the listener's current emotional state and subsequently influence this state. Additionally, MER systems may help facilitate emotional analysis during music composition, interactive experiences in media and entertainment, as well as inform market research  [10] . The implications of understanding music and emotions extend across diverse domains. In general, following the idea of positive psychology  [11] , understanding music emotions can be used to enhance the user experience when designing various systems.\n\nIn this paper, we do not aim to provide a comprehensive overview of MER models, instead, we focus on discussing datasets, evaluation approaches, and identifying key challenges as well as future directions. We only briefly touch upon some of the more recent MER models. For a more comprehensive overview of MER machine learning models, the reader is referred to  [10] ,  [12] -  [15] .\n\nEarlier surveys on Music Emotion Recognition (MER) include the review by  Panda et al. (2020)    [16] , which focuses on the taxonomy of audio features and their relationship to emotion perception. More recently, a broad overview of deep learning techniques for MER was provided in  [17] , with extended discussion on emotion representations. However, these surveys primarily focus on either specific feature engineering strategies or model architectures, and they often lack comprehensive coverage of datasets, evaluation protocols, and the challenges of integrating multimodal sources or predicting induced emotion. In contrast, our goal is to provide a broader and more integrative perspective on the MER landscape. We examine not only the evolution of model architectures, but also the diversity of datasets, annotation strategies, emotion models, and evaluation practices. This comprehensive perspective allows us to highlight emerging trends, identify gaps in",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset",
      "text": "Year # of instances Length Type Categorical Dimensional S/D/B P/I MoodsMIREX  [18]  2007 269 30s MP3 5 labels -S P CAL500  [19]  2007 500 full MP3 174 labels -S P Yang-Dim  [20]  2008 195 25s WAV -Russell S P MoodSwings  [21]  2008 240 15s MP3 -Russell D P NTWICM  [22]  2010 2,648 full MP3 -Russell S P Soundtrack  [23]  2011 470 15s-1m MP3 6 labels 3 dimensions S P MoodSwings Turk  [24]  2011 240 15s MP3 -Russell D P Last.fm subset of MSD  [25]  2011 505,216 full Metadata only listener tags -S P DEAP  [26]  2012 120 60s YouTube id -Russell S I Panda et al.'s dataset  [27]  2013 903 30s MP3, MIDI 21 labels -S P Solymani et al.'s dataset  [28]  2013 1,000 45s MP3 -Russell B P CAL500exp  [29]  2014 3,223 3s-16s MP3 67 labels -S P AMG1608  [30]  2015 1,608 30s WAV -Russell S P Emotify  [31]  2016 400 60s MP3 GEMS -S I Moodo  [32]  2016 200 15s WAV -Russell S P Malheiro et al.'s dataset  [33]  2016 200 30s Audio, Lyrics Quadrants -S P CH818  [34]  2017 818 30s MP3 -Russell S P MoodyLyrics  [35]  2017 2,595 full Lyrics 4 labels -S P 4Q-emotion  [36]  2018 900 30s MP3 Quadrants -S P DEAM  [37]  2018 2,058 45s MP3 -Russell B P PMEmo  [38]  2018 794 full MP3 -Russell B I RAVDESS  [39]  2018 1,012 full MP3, MP4 5 labels -S P DMDD  [40]  2018 18,644 full Audio, Lyrics -Russell S P MTG-Jamendo  [41]  2019 18,486 full MP3 56 labels -S P VGMIDI  [42]  2019 200 full MIDI -Russell D P Turkish Music Emotion  [43]  2019 400 30s MP3 4 labels -S P EMOPIA  [44]  2021 1,087 30s-40s Audio, MIDI Quadrants -S P MER500  [45]  2020 494 10s WAV 5 labels -S P Music4all  [46]  2020 109,269 30s WAV -3 dimensions S P CCMED-WCMED  [47]  2020 800 8-20s WAV -Russell S P MuSe  [48]  2021 90,001 full Audio -Russell (V-A-D) S P HKU956  [49]  2022 956 full MP3 -Russell S I MERP  [50]  2022 54 full WAV -Russell B P MuVi  [51]  2022 81 full YouTube id GEMS Russell B P YM2413-MDB  [52]  2022 699 full WAV, MIDI 19 labels -S P MusAV  [53]  2022 2,092 full WAV -Russell S P EmoMV  [54]  2023 5,986 30s WAV 6 labels -S P Indonesian Song  [55]  2023 476 full WAV 3 labels -S P TROMPA-MER  [56]  2023 1,161 30s WAV 11 labels -S P Music-Mouv  [57]  2023 188 full Spotify id GEMS -S I ENSA  [58]  2023 60 full MP3 -Russell D P EMMA  [59]  2024 364 30s-60s WAV GEMS -S I SiTunes  [60]  2024 300 full WAV -Russell S I MERGE  [61]  2024 In the next section, we provide an extensive overview of the available emotion-annotated music datasets. This is followed by a discussion of the evaluation practices in the field (Section III). After that, we describe a selected list of recent models and approaches in Section IV. Finally, Section V dives into the remaining challenges and future direction for the field of music emotion recognition (MER), followed by a general conclusion.  I  presents an extensive overview of emotion-annotated music datasets. These datasets vary in size, annotation granularity, and focus on either perceived or induced emotions. We have attempted to provide an exhaustive overview of emotion datasets. This was achieved by using Google Scholar with search terms such as 'music emotion dataset', 'affective music dataset', as well as following references within these articles. Before delving deeper into the datasets, we first describe key dimensions that help structure our comparison. These include:",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Datasets Table",
      "text": "(1) the emotion representation model (e.g., categorical vs. dimensional), (2) the annotation method (static or dynamic), (3) the type of emotion captured (perceived vs. induced), (4) the data modalities involved (e.g., audio, video, lyrics), and (5) the dataset's size and diversity in genre or listener demographics. Below, we elaborate on each of these aspects and illustrate them with representative examples.\n\nEmotion representations One of the earliest emotion representation models in music is Hevner's affective ring  [65] , developed in 1936. Based on extensive experimental studies, Hevner's model categorizes music emotions into eight fundamental categories: dignified, sad, dreamy, serene, graceful, happy, exciting, and vigorous. In current-day MER research, we see that Russell's Circumplex Model of Affect  [66]  is widely used. This model characterizes emotions along two dimensions: valence and arousal. Valence represents the degree of positive or negative emotion, while arousal reflects the intensity of emotion, ranging from passive to activated states. Russell's original model contained a third dimension: dominance  [66] . This dimension is typically omitted as it can be hard to annotate, although some researchers have argued to re-include it  [67] . It is worth noting, however, that the dominance dimension was more central to Russell's earlier PAD (Pleasure-Arousal-Dominance) framework, designed for environmental and contextual emotion analysis. Its omission from the Circumplex Model was a simplification intended to facilitate self-reported emotion studies.\n\nRussell's model is a dimensional model, as it consists of continuous values along multiple dimensions (valence/arousal). Hevner's model, on the other hand, is categorical as it consists of discrete emotion labels. Thayer's twodimensional model  [68]  focuses on energetic arousal and tense arousal as the primary dimensions of emotion. Thayer suggests that valence can be inferred from the combination of energetic and tense arousal levels. Other categorical models include the Geneva Emotional Music Scales (GEMS)  [31] . This model was specifically designed for music-induced emotions and consists of 45 emotion tags grouped into nine categories, including amazement, solemnity, tenderness, nostalgia, calmness, power, joyful activation, tension, and sadness.\n\nRecently, alternative emotion frameworks grounded in psychological theory have also been explored. For instance, Affolter et al.  [63]  introduced a dataset that maps listenergenerated tags and playlist names to emotion labels derived from Plutchik's psychoevolutionary model of emotions. Their method employs natural language processing to associate each track with an 8-dimensional vector corresponding to Plutchik's basic emotions, offering a novel bridge between tag-based annotations and theoretical models of affect.\n\nThe datasets listed in Table  I  use a variety of emotion representation models, with Russell's model being the most popular dimensional representation. Some datasets do not use a specific emotion model, for instance, the MTG-Jamendo dataset  [41]  consists of freely assigned tags by the listeners, resulting in a diverse and comprehensive set of 56 tags, spanning from melancholic' to upbeat'. A subset of this dataset is used for the Emotion and Theme Recognition in Music' Task of MediaEval  [69] , which serves as a benchmark for evaluating MER systems. In Table  I , the number of tags used to represent the emotions are listed in the column \"Categorical\".\n\nStatic versus dynamic Regardless of which emotion representation model is being used, we notice two fundamentally different approaches: static versus dynamic annotations. In a static setting, the listeners indicate the emotion for the entire song or fragment. In a dynamic annotation setting, the listener continuously indicates the emotion throughout the song or fragment. For instance, the MTG-Jamendo dataset  [41]  offers categorical tags for each full-length song. The annotation is static, but multiple tags are allowed per song. The MoodSwings dataset  [21] , on the other hand, offers dynamic annotations of valence/arousal for every second of the musical fragments. Finally, some datasets offer both, for instance, MERP  [50]  provides both static GEM labels for the entire song, as well as dynamic valence/arousal ratings for every 1s of a song.\n\nInduced versus perceived Emotion labels in the datasets can represent either perceived or induced emotions. Perceived emotions are emotions that listeners consciously recognize within the music itself. Induced emotions, on the other hand, are emotions that listeners experience as a result of listening to the music, which involve an actual emotional experience provoked by the stimulus. This emotional experience can be influenced by context, memories, and personal experiences, and may differ from the perceived emotion  [70] .\n\nMost of the datasets in Table I use perceived emotion labels, which are easier to annotate. For induced emotion labels, researchers have used psycho-physiological measurements ranging from electromyogram (EMG), volume pulse (BVP), electrocardiograms (ECG), skin conductance, respiration rate, heart rate, to electroencephalograms (EEG). There have been many studies in psychology that use such biosensors to explore how music can influence our emotions  [71] -  [75] . Since these studies include medical data, the datasets are not often public. However, there are a few datasets with music and its induced emotions. Firstly the DEAP dataset  [26]  includes EEG, facial video recordings, as well as peripheral physiological signals that were recorded while they watched music videos. In addition, they collected perceived emotion ratings in terms of arousal, valence, like/dislike, dominance and familiarity. Second, the HKU956 dataset  [49]  records five kinds of physiological signals (i.e., heart rate, electrodermal activity, blood volume pulse, inter-beat interval, and skin temperature) of participants as they listen to music, along with reported emotions in the arousal and valence dimensions. Third, the Music-Mouv' dataset  [57]  investigates the impact of emotional context induced by music on gait initiation, focusing on anticipatory postural adjustments crucial for the elderly and individuals with Parkinson's disease. It includes subjective emotional responses, physiological data from wristbands, and biomechanical data from shoe insoles equipped with sensors collected during and after music listening. Finally, SiTunes  [60]  includes physiological signals (i.e., heart rate, activity intensity, activity step, and activity type) measured both before and after music listening, alongside environmental data (i.e., time of day, weather information, and location) recorded during users' daily lives. The MER models that can predict induced emotions have various medical applications, such as curating playlists to guide patients to different emotional states  [5] . An interesting study by  [76]  concludes that music tends to induce the emotion that is perceived, enabling researchers to use perceived emotion datasets for developing emotioninducing models.\n\nModalities Music comes in many formats, the most common one being 'audio'. Audio files can either be raw waveforms or compressed .mp3 files. However, we should not neglect the MIDI format, a popular format still often used by music producers, composers and performers. Whereas most datasets contain audio files, only a handful focus on MIDI: 1) the VGMIDI dataset  [42] , which contains continuous valence/arousal ratings for MIDI files of piano arrangements for video game soundtracks; 2) the Panda et al.'s dataset  [27] , which provides a diverse collection of audio clips, lyrics, and aligned MIDI files, contains 28 emotion labels grouped into five emotion clusters derived from a cluster analysis of online tags by  [18] ; and 3) the EMOPIA dataset  [44] , which contains paired piano music audio with MIDI that has emotion annotations of the four high/low valence/arousal quadrants.\n\nMany sources can elicit emotion. For instance, video, or lyrics may also affect our perceived or induced emotions. Some datasets offer alternative multimedia streams such as emotion-rated music videos from a variety of genres, including pop, rock, classical, and jazz as in the DEAP dataset  [26] ; or text of lyrics with the annotated music in the DMDD dataset  [40] . The MuVi dataset  [51]  even offers isolated modality ratings for music videos. In this study, the raters were presented with either the music video, the music alone, or the muted video. The final dataset contains ratings for each of these modalities separately as well as together. This allows  [51]  to build a model on pure isolated modalities, which proved to be more accurate than a traditional model.\n\nThe RAVDESS dataset  [39]  is a multimodal dataset containing both speech and music with emotional expressions: calm, happy, sad, angry, fearful, surprise, and disgust for speech; and calm, happy, sad, angry, and fearful for music. It provides a rich combination of modalities, including Audio-only, Audio-Video, and Video-only formats.\n\nAdditionally, the EmoMV dataset  [54]  focuses on affective music-video correspondence learning by providing labeled pairs of music and video segments that either match or mismatch in terms of emotional content. This dataset enables models to learn the affective alignment between audio and visual modalities, making it particularly valuable for emotionbased matching and retrieval tasks.\n\nRecently, the Popular Hooks dataset  [62]  was introduced as a multimodal dataset that contains 38,694 popular musical hooks (i.e., memorable sections of songs) with synchronized MIDI, music video, audio, and lyrics. It also offers detailed (predicted) labels for high-level musical attributes such as tonality, structure, genre, emotion, and region. Leveraging a pre-trained multimodal music emotion recognition framework, the dataset provides predicted emotion labels, which were evaluated through a user study.\n\nLastly, datasets like MERP  [50]  and EMOPIA  [44]  provide metadata on song features and listener demographics, which can be useful for detailed analysis and personalized emotion prediction.\n\nDataset size Compared to affective datasets available in other domains (e.g., the Sentiment140 dataset  [77]  in NLP, which contains 1.6 million tweets annotated as positive or negative), the size of the available datasets is still very limited, with the largest dataset containing 109,269 instances. In addition to the number of instances in datasets, we also notice a difference in the length of the instances. A number of datasets (e.g. MERP  [50] , VGMIDI  [42] , and HKU956  [49] ) offer ratings for full-length songs. In datasets with dynamic ratings throughout the song, this may provide a means for researchers to analyse how our emotions evolve throughout a song. Other datasets focus on short fragments, often ranging from 30 seconds to 1-minute fragments (e.g. DEAM  [37] , EMOPIA  [44] , and EMMA  [59] ), but even as short as 10s as is the case for the MER500 dataset which consists of Indian Hindi film music  [45] .\n\nVariety of music The genres covered in these datasets vary widely, reflecting the diverse nature of music. For instance, the DEAP dataset  [26]  includes classical as well as jazz pieces, which are known for their rich emotional and structural complexities. The VGMIDI dataset on the other hand,  [42]  focuses exclusively on video game soundtracks, a genre that often aims to evoke specific emotions to enhance the gaming experience. Rock and electronic music can primarily be found in the DEAM dataset  [37] , both genres that are prevalent in contemporary music culture. This genre diversity allows researchers to select datasets that align with their specific research goals and to explore how different genres affect emotional perception and annotation. Additionally, studying a variety of genres can help in developing more robust and generalized MER models that can perform well across different types of music.\n\nAnnotation Process Annotation processes and label distributions vary significantly across datasets, impacting the effectiveness and reliability of machine learning models in MER. Some datasets, such as MTG-Jamendo  [41] , rely on freely assigned tags by multiple annotators, allowing a broad range of emotional descriptors. In contrast, DEAP  [26]  and HKU956  [49]  use a predetermined list of emotions to ensure consistency across annotations.\n\nA major distinction between annotation strategies relates to if they use absolute or relative annotation methods, this is particularly important to note for dimensional models of emotion. In absolute annotation, annotators assign a direct score or category to each sample independently. This approach is common in datasets such as DEAP and HKU956, where participants provide valence and arousal ratings individually for each stimulus. Recenlty, several datasets propose relative annotation strategies to simplify the annotation process and improve consistency across annotators. Instead of rating individual tracks absolutely, annotators are asked to compare pairs of samples and judge their relative emotional positioning. Examples include the MusAV dataset  [53] , which gathers comparative annotations of arousal and valence for pairs of music tracks, and Emo-Soundscapes  [78] , where participants rank soundscape recordings based on perceived emotion. Earlier work by Yang et al.  [79]  introduced rankingbased emotion recognition methods for music organization and retrieval, showing that relative judgments can lower cognitive load and lead to more reliable ground truth data. Similarly, in the EMusic dataset  [80] , a ranking-based annotation approach was used for experimental music, demonstrating improvements over traditional absolute labeling methods.\n\nIn addition, the CCMED-WCMED dataset  [47]  compared Western and Chinese classical music based on emotion dimen- sions collected through relative annotations, highlighting the importance of cultural factors in emotion perception. These relative approaches can mitigate some of the variability and subjectivity inherent in absolute annotations, particularly in continuous emotion models like valence-arousal space.\n\nThe number of annotators also varies: some datasets involve large-scale crowdsourcing efforts, providing a wide range of perspectives (e.g., MusAV, Emo-Soundscapes), while others use smaller, more controlled groups to maintain annotation consistency (e.g., DEAP, HKU956). Additionally, the logic behind selecting music segments differs, with some datasets focusing on full-length songs and others on shorter clips to better capture transient emotional responses.\n\nOverall, the choice between absolute and relative annotation methods, the number of annotators, and the segment selection strategies are all critical factors influencing dataset reliability and the resulting model performance in MER research.\n\nLabel distribution and splits The distribution of labels across datasets can be imbalanced, affecting the performance of machine learning models. For instance, the MTG-Jamendo dataset  [41]  contains a diverse set of 56 tags, freely assigned by users, but their frequency is highly imbalanced. This skewed distribution, with some tags like \"happy\" and \"film\" appearing far more often than others like \"sexy\" or \"fast\", is visualized in Figure  2 , which shows the top 10 most and least frequent tags. In contrast, the DEAP dataset  [26]  provides a more balanced set of labels across arousal and valence dimensions. Finally, some datasets, such as DEAM  [37]  and MTG-Jamendo  [41] , propose fixed splits for training and testing. This facilitates the replication of experiments and ensures consistent evaluation metrics which are easily used as a benchmark.\n\nRecommendations and Future Directions While the landscape of emotion-annotated music datasets is expanding, several challenges and opportunities remain. For general-purpose MER models, datasets like MTG-Jamendo  [41]  and MERP  [50]  offer breadth in genre and label diversity, while EMOPIA  [44]  and VGMIDI  [42]  are more suitable for symbolicdomain or piano-focused emotion modeling. For multimodal applications, Popular Hooks  [62]  and EmoMV  [54]  are relevant. However, there is a notable lack of large-scale, publicly available datasets focusing on induced emotions with multimodal or biosensor data, limiting medical and affective computing applications. The community would benefit from more datasets capturing diverse demographics, cross-cultural emotional interpretations, and consistent emotion annotation schemas across modalities. We encourage future dataset creators to include dynamic annotations, listener metadata, and align modalities when possible.\n\nIn sum, there is a variety of emotion-annotated datasets available as shown in Table  I . They differ in terms of emotion representation models, annotations labels, as well as music format and perceived versus induced emotion labels. Understanding these variations is crucial for developing robust and accurate MER models that can generalize across different datasets and musical genres. The challenges and future research directions related to datasets are discussed in detail in Section V.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Evaluation Protocols",
      "text": "Evaluation metrics play a crucial role in assessing the performance of MER systems. Depending on which type of emotion ratings are being predicted (dimensional versus categorical), the evaluation metrics change as we are dealing with a regression or a classification task respectively. Commonly used evaluation metrics for categorical MER systems include accuracy, precision, area under the ROC curve (AUC), and confusion matrices. In the case of regression MER models, metrics such as mean squared error (MSE), R 2 , and Pearson correlation coefficient  [81]  are used. These metrics provide insights into the effectiveness of MER models when it comes to emotions represented by dimensional models.\n\nThe evaluation approach also depends on whether a dataset provides static or dynamic annotations. Static annotations assign a single label (or vector) to an entire track or segment, making evaluation straightforward with standard classification or regression metrics (e.g., accuracy, F1-score, ROC-AUC, or R 2 ) computed at the song level  [20] ,  [36] ,  [41] . In contrast, dynamic annotations provide time-continuous emotion ratings-often sampled at a rate of 1 Hz-reflecting how emotions evolve throughout a track  [26] ,  [37] ,  [38] . In such cases, evaluation typically involves comparing predicted and ground-truth time series using frame-level metrics such as Pearson Correlation Coefficient (PCC), Mean Squared Error (MSE), or Concordance Correlation Coefficient (CCC)  [37] ,  [38] ,  [82] . Some studies apply smoothing or window-based aggregation (e.g., moving average or median filters) to reduce annotation noise and capture broader emotional trends  [37] ,  [82] . For example, the DEAM dataset  [37]  provides both static and dynamic valence-arousal annotations, with dynamic evaluation conducted on a per-frame basis across time.\n\nWhen evaluating MER models, it is also important to distinguish between perceived and induced (invoked) emotion. Evaluating perceived emotion predictions typically involves comparing predicted labels with human-annotated ground truth, which reflects how listeners interpret the emotional content of the music. In contrast, evaluating induced emotion is more complex, as it involves the actual emotional response elicited in the listener. This often requires physiological measurements (e.g., heart rate, EEG) or self-reports in a controlled experimental setup. For this reason, evaluation of induced emotion prediction models may rely on additional modalities (e.g., biosensors), and often focuses on subjective measures such as emotion regulation effectiveness or user satisfaction. These differences highlight the need for tailored evaluation protocols depending on the target emotion type.\n\nEvaluating MER systems goes beyond just defining a common metric. Due to the inherent differences between datasets, a comparison across datasets is often not possible. For an in-depth discussion on this, the reader is referred to Section V. Within one dataset, however, it is possible to establish benchmarks and compare the performance of different models, provided that the train/test split is shared. There are some initiatives to facilitate the comparison between models, such as competitions, as well as individual papers that offer clear data splits and metrics  [82] -  [86] .\n\nCompetitions and challenges provide valuable platforms for evaluating and comparing the performance of MER systems. These initiatives often involve standardized datasets, evaluation protocols, and metrics, enabling researchers to benchmark their algorithms against state-of-the-art methods. Existing benchmarking initiatives and competitions in MER include the 'Audio K-POP Mood Classification' task in MIREX (Music Information Retrieval Evaluation eXchange) (last organized in 2019)  2  , the 'Emotion in Music' task in MediaEval (last organized in 2015)  3  , and the 'Emotion and Theme Recognition in Music Using Jamendo' in MediaEval (last organized in 2021)  4  .\n\nGiven the variability and potential noisiness of emotion annotations in music datasets  [50] , evaluation protocols should be designed to assess not only performance against possibly imperfect ground truth but also the perceptual validity of model outputs. One way to address this is by incorporating listening tests, where human subjects rate the emotions conveyed by model-predicted tracks. This does not directly mitigate the noise in the training data, but rather provides IV. MODELS AND APPROACHES In this section, we briefly touch upon some of the more recent MER models, highlighting the state-of-the-art approaches over the last few years. This is not an exhaustive overview, and for a more comprehensive review, readers are referred to other survey papers  [10] ,  [12] -  [15] . The aim of this section is to point out the current state-of-the-art and various approaches over the last five years. The models were selected through Google Scholar searches using the search terms 'music emotion prediction model', 'affective music prediction model', and 'music emotion recognition model' starting from the year 2020, as well as by following links in the articles.\n\nTable  II  presents an overview of selected MER models released since 2020, summarizing their modalities, approaches, emotion models, and datasets. This table highlights the diversity in methodologies and datasets used in recent research. Some of the earliest attempts at music emotion prediction involved rule-based approaches and hierarchical frameworks. For instance, Feng et al.  [125]  used Computational Media Aesthetics (CMA) to analyze tempo and articulation, mapping them into four mood categories: happiness, anger, sadness, and fear. They achieved a total precision of 67% and a total recall of 66%. Lu et al.  [126]  developed a hierarchical framework for automatically extracting music emotion from acoustic data. They employed music intensity to represent the energy dimension of Thayer's model while using timbre and rhythm to capture the stress dimension. They achieved an average accuracy of mood detection of up to 86.3%. These early models laid the groundwork for subsequent advancements in music emotion recognition, paving the way for the adoption of more sophisticated techniques, including deep learning approaches.\n\nIn recent years, various deep-learning MER models have been developed, employing techniques such as Convolutional Neural Networks (CNNs)  [55] ,  [83] ,  [88] ,  [90] ,  [92] -  [94] ,  [96] -  [98] ,  [103] ,  [109] ,  [112] ,  [116] ,  [119] ,  [121] ,  [122] , Recursive Neural Networks (RNNs) such as Gated Recurrent Units (GRUs)  [112]  and Long-Short Term Memory networks (LSTMs)  [51] ,  [55] ,  [88] -  [91] ,  [96] ,  [97] ,  [99] ,  [109] ,  [118] , and more recently, Transformer architectures  [55] ,  [100] ,  [114] ,  [117] .\n\nAdditionally, generative models such as Generative Adversarial Networks (GANs) have been applied. For instance, Huang et al.  [102]  developed a GAN-based model for emotion recognition using audio inputs under IoT environments.\n\nOn the MTG-Jamendo dataset  [41] , several models have demonstrated noteworthy performance. For example, Mayerl    Other models have employed more traditional machine learning techniques alongside deep learning approaches. Medina et al.  [110]  focused on classifying emotions from audio using SVM, Random Forest, and Multi-Layer Perceptron (MLP) models, achieving an F-score of 0.73 and 0.69 for predicting valence and arousal values, respectively, on the DEAM dataset  [37] . Sharma et al.  [87]  combined machine learning algorithms such as Support Vector Machines (SVM) and Naive Bayes (NB) to predict emotions using both audio and lyric features. They achieved an accuracy of 63% on the PMEmo dataset  [38] . Similarly, Griffiths et al.  [104]  developed a multi-genre MER model using linear regressors. They achieved a R 2 score of 0.776 and 0.85 for predicting valence and arousal values, respectively, on the self-built dataset. Meanwhile, Xia et al.  [105]  utilized clustering algorithms alongside machine learning techniques like SVM and K-Nearest Neighbors (KNN) for emotion recognition, and their hybrid model which combined all the machine learning techniques achieved an accuracy of 85% on the Solymani et al.'s dataset  [28] .\n\nA distinction between the various models can be made based on the input modality. Some models are exclusively based on MIDI such as the multi-task architecture proposed by  [106] , and as such use a token-based representation. Given the limited size of MIDI datasets, the accuracy of such models is limited, e.g., the model by  [106]  reaches 67.56% accuracy when predicting between four emotion classes. Most of the existing MER models are based on audio, and hence they take as input raw audio. This is often converted into Melspectrograms which are then processed through convolutional neural networks (CNNs)  [94] , or the audio could be directly processed through a WaveNet architecture, which is a type of temporal CNN  [127] .\n\nIn recent years, audio embeddings pretrained on largescale datasets have become increasingly common, enabling transfer learning for Music Emotion Recognition (MER). Beyond traditional embeddings, recent research has explored both supervised and unsupervised pretraining strategies for audio representation learning, where MER is treated as a downstream task. For instance, MERT (Music undERstanding model with self-supervised Training)  [128]  is specifically tailored to music, addressing challenges like pitch and tonality through a novel training scheme. MERT incorporates pseudo labels from two teacher models-an acoustic teacher based on Residual Vector Quantization Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT)-within a masked language modeling framework. When used as a feature extractor for MER, MERT-95M achieves a PR-AUC of 0.1340 and ROC-AUC of 0.7640 on the MTG-Jamendo dataset  [41] , while MERT-330M reaches a PR-AUC of 0.1400 and ROC-AUC of 0.7650. Building upon MERT embeddings, Kang and Herremans  [124]  introduce a unified multitask learning framework that leverages both categorical and dimensional emotion labels. Their model achieved a PR-AUC of 0.1543 and ROC-AUC of 0.7810 on the MTG-Jamendo dataset  [41] . For dimensional emotion regression, it attained R 2 scores of 0.5473 for valence and 0.7940 for arousal on PMEmo  [38] , 0.5184 for valence and 0.6228 for arousal on DEAM  [37] , and 0.6512 for valence and 0.7616 for arousal on the dataset by Solymani et al.  [28] .\n\nMcCallum et al.  [129]  present a comparative analysis of audio representation learning strategies, showing that supervised pretraining on expert-annotated music datasets leads to state-of-the-art performance in emotion tagging. Their models achieve 78.6 ROC-AUC and 16.1 PR-AUC on the MTG-Jamendo Mood/Theme dataset-outperforming previously reported benchmarks. Alonso-Jim√©nez et al.  [130]  propose MAEST, a transformer-based architecture trained with patchout and pre-initialized with ImageNet or AudioSet weights. Although its performance is slightly lower (78.1 ROC-AUC and 15.4 PR-AUC), MAEST demonstrates the potential of efficient, convolution-free architectures in music emotion tagging, particularly when speed and scalability are important.\n\nAlternatives to these approaches include directly extracting spectral features (e.g., MFCCs, spectral centroids) with libraries such as OpenSmile, which has a configuration file specifically for the emotion recognition task  [131] .  [82]  uses this approach and achieves R 2 scores of 0.378 and 0.638 for predicting dynamic valence and arousal values, respectively, on the Solymani et al.'s dataset  [28] . Recognizing the importance of induced emotions, recent research has focused on models that leverage physiological data to predict musicinduced emotions. For instance,  [49]  constructed the HKU956 dataset  [49]  with aligned peripheral physiological signals (i.e., heart rate, skin conductance, blood volume pulse, skin temperature) and self-reported emotion from 30 participants. The study revealed that physiological features significantly contribute to valence classification and that multimodal classifiers outperform single-modality ones.  [101]  uses PMEmo  [38]  as induced emotion recognition dataset. This study merges au-dioLIME, a source-separation-based explainable model, with mid-level perceptual features to form an intuitive connection between input audio and emotion predictions, providing insights into model predictions.  [115]  also uses PMEmo as induced emotion recognition dataset and introduces a novel method named Modularized Composite Attention Network (MCAN). This method enhances feature extraction and employs attention mechanisms to improve the stability and accuracy of emotion prediction models.\n\nFinally, musically meaningful features may be included, such as Rhythmic features (e.g., tempo, beat histogram), or note features (e.g., pitch), as implemented by Shi et al.  [132]  and Panda et al.  [36] , respectively. Shi et al. achieved a precision of 92.8% for 4 emotion categories (calm, sad, pleasant, and excited), while Panda et al. achieved an F1-score of 76.0% for 4 emotion categories (Quadrants).\n\nOther input modalities may include text (lyrics), or video. In the case of the former, some models extract the sentiment from the lyrics using Natural Language Processing (NLP) tools  [108] ,  [111] , or they use the entire lyrics with an embedding model  [113] ,  [117] . These features are then combined with audio-based features or analyzed independently to predict the emotional content of music. Including the lyrics does not always improve the sentiment prediction, particularly in predicting arousal  [111] ; however,  [120]  did manage to increase the performance of a MER model by using embeddings, such as word2vec  [133]  or stacked ensemble models that integrate both audio and lyrics. Finally, models that include video modalities typically use pretrained networks to capture image and video features, and thus improve model performance. For instance,  [54]  uses ResNet-50  [134]  and FlowNetS  [135] , respectively.\n\nIdentifying the current state-of-the-art Music Emotion Recognition (MER) model is challenging due to factors such as differences in datasets and performance metrics, as highlighted in the previous section. A commonly used benchmark is the Emotion and Theme Recognition in Music competition based on the MTG-Jamendo dataset, where the top-performing model by Knox et al.  [95]  achieved a PR-AUC-macro of 0.161 and a ROC-AUC-macro of 0.781 using an ensemble of CNNbased models trained with focal loss and receptive field tuning. While benchmark performances of MER models have steadily improved (e.g., some achieving ROC-AUC scores over 0.78 on datasets like MTG-Jamendo), their translation to real-world applications remains limited.\n\nOne area where we do see emotion models integrated is emotion-conditioned music generation, where preliminary MER models are incorporated to guide the emotional content of generated music (e.g., Makris et al.  [6] ). Furthermore, emotion detection systems have been piloted in adaptive gaming environments or personalized music therapy applications  [5] , but large-scale real-world deployment of deep MER models remains rare due to variability across listeners, subjective annotation challenges, and differences between training and inference conditions.\n\nFigure  2  illustrates the most frequently used datasets by the listed Music Emotion Recognition (MER) models since 2020, as summarized in Table  II . This count includes studies where MER is the primary task as well as those where it serves as a downstream application, such as in music representation learning  [128] . Self-built datasets, created by authors for specific research purposes, remain the most commonly used, appearing in 11 instances. These datasets often contain licensed music, restricting their accessibility to the broader research community. Other frequently utilized datasets include DEAM (8 occurrences) and MTG-Jamendo (7 occurrences), highlighting their importance in MER studies. Some datasets, such as PMEmo  [38]  and Solymani et al.'s dataset  [28] , appear less frequently but still contribute valuable insights. For instance, PMEmo  [38]  includes dynamic emotion labels and physiological signals (EDA), enabling multimodal affective analysis, while Solymani et al.'s dataset  [28]  offers continuous valence-arousal annotations and standard deviations, supporting studies on annotation reliability and temporal emotion dynamics.\n\nWe have only provided a glimpse into the existing MER models in this section. From the performance of the various models, we see, however, that much improvement can still be made. We discuss some of the remaining challenges in the next section.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Challenges And Future Directions",
      "text": "Whereas the first publications on music and emotion surfaced in the 1930s  [65] , current MER models still struggle to match human performance in emotion recognition. The field of MER still faces several challenges and various opportunities for future exploration remain, ranging from overcoming data limitations to the integration of emerging technologies. Understanding and addressing these challenges is crucial for advancing the field.\n\nDataset limitations One of the primary challenges in MER is the scarcity of large, diverse, copyright-cleared, emotionannotated datasets. Limited datasets hinder the development and evaluation of robust MER models, leading to potential biases and generalization issues, and also prevent the establishment of reliable performance benchmarks across studies.\n\nWe have recently seen advances in this area with the release of larger datasets such as MTG-Jamendo  [41] , Music4all  [46] , MuSe  [48] , which contain 18k, 109k, and 90k instances respectively. Whereas MTG-Jamendo and MuSe are available under a Creative Commons licence, however, Music4all contains copyrighted tracks. In addition, when exploring datasets, we notice that they are often skewed towards one particular genre. For instance, the DEAM dataset  [37]  consists mostly of rock and electronic music genres, while the VGMIDI dataset is focused solely on video game soundtracks.\n\nTo deal with the current dataset size limitations, techniques such as unsupervised pretrained may be helpful. With this technique, latent representations are first learned using unlabeled datasets. This evolution has led to some available largescale audio encoders such as the Variational Auto Encoder used in  [136] , and the AST Audio Spectrogram Transformer presented in  [137] , as well as various recently developed neural audio encoders (e.g. Descript Audio Codec  [138] ). These novel pretrained representations may help deal with the limiting size of emotion-annotated datasets.\n\nSubjective labels The subjective and variable nature of emotion perception also poses a significant challenge. Emotions are inherently complex and subjective, varying across individuals, cultures, and contexts  [50] . For instance, researchers have observed significant dependence between the number of years of musical training  [50] ,  [139] , gender  [51] , familiarity with songs  [51] , culture  [50] ,  [140] , genre preference  [50] , and even age  [50] . The latter study makes an argument for including profile information in the emotion prediction model to personalize the predictions. However, not many datasets include this type of information other than MERP.\n\nIn addition, many datasets have a cultural bias, meaning that they are annotated by people from the same culture, often simply because of the locality of the experiment or language constraints. However, it has been established that people from different countries or cultural backgrounds have different perceptions of emotion for the same music fragment  [141] . For instance,  [50]  have raters from both the US as well as India, and see a significant difference in their annotations. HKU956  [49]  go even further and include the rater's responses to a personality test: 'Ten Item Personality Measure'. In future work, one could develop datasets annotated by annotators from different cultural backgrounds, that include this information about the raters. The personalized nature of emotion ratings can cause a low inter-rater reliability, a metric of agreement between raters often calculated using Cronbach's Alpha  [142] .\n\nNoisy labels When creating datasets, an additional challenge arises: it can be hard to identify the emotion perceived from music, especially when working with valence and arousal models. In fact, Russell's model  [66]  included a third dimension: dominance. This dimension is typically omitted because of the ambiguity in annotation. In general, categorical models, although less precise, are often easier to annotate  [76] . This personal variability and ambiguity in emotion labels introduces noise in dataset labels, causing low inter-rater reliability, and making it challenging to train accurate and reliable MER models. In addition, some emotion representations are prone to more noise. For instance, the MTG-Jamendo dataset  [41]  offers a large number of freely assigned tags, making it suitable for exploring a wide range of emotions, but the lack of a controlled annotation process can lead to noisy data, as there may be synonyms of emotion terms.\n\nMachine learning models may also offer useful techniques to deal with noisy labels, as discussed in the survey by  [143] . These could include Noisy Graph Cleaning (NGC)  [144] , Joint Training with Co-Regularization (JoCoR)  [145] , and Robust Curriculum Learning (RoCL)  [146] .\n\nAnnotation interfaces When creating a static dataset with static annotations, some standard tools can be used, including PsyToolkit  [147] . However, to create any larger-scale dataset, the annotations are often done through crowdsourcing services such as Amazon Mechanical Turk 5  (e.g. for MERP  [50] , DEAM  [37] , and Solyman's dataset  [28] ). These services offer access to a large 'army' of annotators, which may come at the cost of accuracy. There are, however, a number of techniques that can be used to filter out some of this annotation noise. This includes limiting the annotations to Master raters (raters with a known track record that often work at a premium price)  [50] , by including qualification tasks to assess participants' understanding of the dimensional model  [28] , or by using multiple ground truth questions that are shown to all raters  [50] . Finally, inter-rater reliability can be used to filter out low-quality annotations  [50] . When doing this, it is important to keep in mind personal characteristics, which may cause different people to rate music differently. Hence, inter-rater reliability should ideally be calculated by taking into account the rater's profile features.\n\nAn additional difficulty is that the amount of available interfaces for music emotion annotation is very limited. Especially when it comes to time-continuous annotations of valence and arousal.  [50]  released their dynamic annotation interface  6  that hooks into Amazon mTurk. Kim et al.  [21]  also introduced, an annotation interface called MoodSwings, designed to record dynamic emotion labels.\n\nBenchmarking The ImageNet competition has been instrumental in establishing a clear performance benchmark among computer vision models  [148] . While there have been similar competitions for music emotion prediction (see Section III), none of these competitions ran in the last three years, indicating the lack of a current benchmark for MER systems. To establish benchmarks on individual datasets, we have to revert to individual model papers, such as  [28] ,  [37] . It is unfortunately not always clear which train/test split these systems use, making direct comparisons hard. One way to facilitate an easy overview would be to leverage Leaderboard features on popular websites such as Papers With Code 7  and HuggingFace  8  .\n\nWhile there have been some efforts toward cross-dataset comparisons, such practices are still relatively rare. A notable example is the MusAV dataset  [53] , which provides a benchmark for evaluating arousal-valence (AV) regression models trained on different datasets. MusAV uses relative pairwise comparisons as ground truth and enables comparative validation across models trained on diverse AV datasets. Such initiatives are promising steps toward better generalization and standardized evaluation, but they are currently exceptions rather than the rule. In general, many datasets still use different emotion representations, and cross-dataset evaluation remains challenging. Recent efforts have aimed to address this limitation: Kang et al.  [124]  propose a unified multitask framework to combine categorical and dimensional labels, while Liu et al.  [123]  use LLM-based label embeddings to align emotion annotations across datasets and enable zero-shot generalization.\n\nBridging between datasets that use continuous representations and categorical models is not straightforward. Several studies, such as Paltoglou and Thelwall  [149] , have proposed mappings between arousal/valence and categorical labels. This approach has been further utilized, for example, by Makris et al.  [6]  for emotion-controlled lead sheet generation. In addition, large-scale affective norm resources such as Warriner et al.  [150] , which provides valence, arousal, and dominance (VAD) ratings for 13,915 English lemmas, and Mohammad  [151] , who developed the NRC VAD Lexicon with ratings for over 20,000 words, offer valuable foundations for linking continuous and categorical representations. Such mappings could support merging continuous and categorical datasets into larger-scale resources for music emotion research.\n\nMIDI The forgotten format in music emotion recognition. Emotion originates from many different aspects of the music, including the tonal tension  [8] , instrumentation and timbre  [152] , production quality  [153] , expressiveness of the performance  [154] , harmony  [155] . The symbolic MIDI format only captures parts of these  [13] , which may explain the lack of models for predicting emotion from MIDI. Another reason may simply be the lack of emotion-annotated datasets (three in total). Even though MIDI is an abstraction of music, it is still a widely used format by music producers and performers and warrants its own models for emotion prediction. Even more, such datasets may enable generative music systems (which are typically trained on MIDI) to be controlled by emotion  [6] ,  [156] .\n\nMultimodal predictions Our sensory input is multimodal. As such, some of the emotion-annotated datasets enable us to look at multiple modalities. For instance, DEAP  [26]  offers biofeedback data, i.e. EEG recordings, and frontal face videos from participants. Similarly, HKU956  [49]  offers physiological signals including heart rate, electrodermal activity, blood volume pulse, inter-beat interval, and skin temperature. SiTunes  [60] , on the other hand, provides physiological and environmental situation recordings collected via smart wristband devices.\n\nThese biological data can serve as the induced emotion labels, e.g. EEG signals can be translated into human emotions  [157] . Increased datasets with different types of physiological signals enable researchers to focus on creating models for induced versus perceived emotion detection. This offers new avenues to use biofeedback in music emotion mediation applications through smart devices.\n\nSometimes other emotion-inducing modalities are present, such as video, or lyrics. In this case, it is important to consider the influence of each of these modalities. In the case of video and music, Chua et al.  [51]  have studied the influence of exposing participants not only to the music but also the muted video, as well as the music videos. They found that the music modality explains most of the variance in arousal values, and both music and video modalities explain the variance in valence values. Phuong et al.  [158]  explore the influence of using only audio features and only video features to predict emotions from movie fragments. They found that the prediction is most accurate when both modalities are used. However, when predicting from a single modality, the audio model is most accurate.\n\nReal-time Many of the currently existing models are not implemented as an easy-to-use library, nor are they quick to run. They often require a GPU and may take several minutes to run. There are use cases, however, for real-time emotion recognition systems, as they would be able to integrate into therapeutic emotion detection systems  [5] , mood guidance playlist systems, as well as more commercial systems such as advertisement targeting systems.\n\nToward reliable and comparable MER research Despite significant advancements, the field still lacks standardized, high-quality datasets that serve as universal benchmarks. As discussed above, most existing datasets differ widely in genre coverage, emotion representation models, annotation procedures, and data quality. This heterogeneity, combined with inconsistent use of evaluation metrics and train/test splits, often makes it difficult to compare results across studies-even when using the same dataset. For example, the MTG-Jamendo dataset has been used in numerous studies (see Table  II ), yet reported results vary substantially due to differing preprocessing strategies, loss functions, or model inputs. This wide variance raises questions about reproducibility and comparability in the field. While recent initiatives like MusAV  [53]  and unified multitask frameworks  [124]  represent important progress, the lack of standardized benchmarks still hinders progress. We argue that no single dataset can serve all research needs, especially given the subjective and culture-specific nature of emotion. Rather than searching for a one-size-fits-all dataset, we encourage the development of dataset-agnostic benchmarking tools, clearly defined splits, and model evaluation protocols. The community would benefit from centralized efforts (e.g., leaderboards, open splits, and documentation hubs) that promote transparency and make it easier to assess which models and results should be taken more seriously. Without such measures, the field risks being undermined by non-comparable results and hard-to-replicate studies.\n\nIn sum, the challenges mentioned above provide direct opportunities and future directions to further advance the exciting field of music emotion recognition.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "Music emotion recognition is a promising field with various practical applications. With the rise of large-language models, we have seen impressive performance in various tasks. The field of music emotion recognition, however, still seems to be lagging. Given the importance of large training datasets to facilitate the training of LLMs, we provide a comprehensive overview and discussion of the existing datasets for music emotion recognition.\n\nWe also explore current state-of-the-art models and dive into evaluation methods such as metrics as well as competitions, leaderboards, and benchmarks within the MER field. With this knowledge, we discuss the current challenges of the MER field at length and provide concrete future directions and emerging trends such as real-time systems and multimodal prediction systems.\n\nIn closing, this survey serves as a valuable resource for the MER community, by offering insights into the current stateof-the-art, as well as a discussion of challenges and inspiration for future directions.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Top 10 most and least frequent tags in the MTG-Jamendo dataset.",
      "page": 5
    },
    {
      "caption": "Figure 2: , which shows the top 10 most and least frequent tags.",
      "page": 5
    },
    {
      "caption": "Figure 2: Number of Music Emotion Recognition Models since 2020 that use the listed datasets.",
      "page": 8
    },
    {
      "caption": "Figure 2: illustrates the most frequently used datasets by",
      "page": 10
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion and meaning in music",
      "authors": [
        "Meyer Leonard"
      ],
      "year": "1956",
      "venue": "Emotion and meaning in music"
    },
    {
      "citation_id": "2",
      "title": "The psychology of music",
      "authors": [
        "Carl E Seashore"
      ],
      "year": "1937",
      "venue": "Music Educators Journal"
    },
    {
      "citation_id": "3",
      "title": "The affective character of the major and minor modes in music",
      "authors": [
        "Kate Hevner"
      ],
      "year": "1935",
      "venue": "The American Journal of Psychology"
    },
    {
      "citation_id": "4",
      "title": "Emotional responses to music: The need to consider underlying mechanisms",
      "authors": [
        "N Patrik",
        "Daniel Juslin",
        "V√§stfj√§ll"
      ],
      "year": "2008",
      "venue": "Behavioral and brain sciences"
    },
    {
      "citation_id": "5",
      "title": "Music, computing, and health: a roadmap for the current and future roles of music technology for health care and well-being",
      "authors": [
        "Rebecca Kat R Agres",
        "Anja Schaefer",
        "Susan Volk",
        "Andre Van Hooren",
        "Simone Holzapfel",
        "Meinard Dalla Bella",
        "Martina M√ºller",
        "Dorien Witte",
        "Rafael Herremans",
        "Melendez"
      ],
      "year": "2021",
      "venue": "Music & Science"
    },
    {
      "citation_id": "6",
      "title": "Generating lead sheets with affect: A novel conditional seq2seq framework",
      "authors": [
        "Dimos Makris",
        "Kat Agres",
        "Dorien Herremans"
      ],
      "year": "2021",
      "venue": "2021 Int. Joint Conf. on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "7",
      "title": "Ai-based affective music generation systems: A review of methods and challenges",
      "authors": [
        "Adyasha Dash",
        "Kathleen Agres"
      ],
      "year": "2024",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "8",
      "title": "Morpheus: generating structured music with constrained patterns and tension",
      "authors": [
        "Dorien Herremans",
        "Elaine Chew"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Affectmachineclassical: a novel system for generating affective classical music",
      "authors": [
        "Adyasha Kat R Agres",
        "Phoebe Dash",
        "Chua"
      ],
      "year": "2023",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "10",
      "title": "Machine recognition of music emotion: A review",
      "authors": [
        "Yi-Hsuan Yang",
        "Homer Chen"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)"
    },
    {
      "citation_id": "11",
      "title": "Positive psychology: An introduction",
      "authors": [
        "E Martin",
        "Mihaly Seligman",
        "Csikszentmihalyi"
      ],
      "year": "2000",
      "venue": "Positive psychology: An introduction"
    },
    {
      "citation_id": "12",
      "title": "A survey of music emotion recognition",
      "authors": [
        "Donghong Han",
        "Yanru Kong",
        "Jiayi Han",
        "Guoren Wang"
      ],
      "year": "2022",
      "venue": "Frontiers of Computer Science"
    },
    {
      "citation_id": "13",
      "title": "Music emotion recognition: A state of the art review",
      "authors": [
        "Erik Youngmoo E Kim",
        "Raymond Schmidt",
        "Migneco",
        "Patrick Brandon G Morton",
        "Jeffrey Richardson",
        "Jacquelin Scott",
        "Douglas Speck",
        "Turnbull"
      ],
      "year": "2010",
      "venue": "Proc. ismir"
    },
    {
      "citation_id": "14",
      "title": "Review of data features-based music emotion recognition methods",
      "authors": [
        "Xinyu Yang",
        "Yizhuo Dong",
        "Juan Li"
      ],
      "year": "2018",
      "venue": "Multimedia systems"
    },
    {
      "citation_id": "15",
      "title": "Music emotion recognition: From content-to context-based models",
      "authors": [
        "Mathieu Barthet",
        "Gy√∂rgy Fazekas",
        "Mark Sandler"
      ],
      "year": "2012",
      "venue": "From Sounds to Music and Emotions: 9th Int. Symposium, CMMR 2012"
    },
    {
      "citation_id": "16",
      "title": "Audio features for music emotion recognition: a survey",
      "authors": [
        "Renato Panda",
        "Ricardo Malheiro",
        "Rui Pedro"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Music emotion recognition based on deep learning: A review",
      "authors": [
        "Xingguo Jiang",
        "Yuchao Zhang",
        "Guojun Lin",
        "Ling Yu"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "18",
      "title": "Exploring mood metadata: Relationships with genre, artist and usage metadata",
      "authors": [
        "Xiao Hu",
        "J Stephen Downie"
      ],
      "year": "2007",
      "venue": "ISMIR"
    },
    {
      "citation_id": "19",
      "title": "Towards musical query-by-semantic-description using the cal500 data set",
      "authors": [
        "Douglas Turnbull",
        "Luke Barrington",
        "David Torres",
        "Gert Lanckriet"
      ],
      "year": "2007",
      "venue": "Proc. of the 30th annual Int. ACM SIGIR Conf. on Research and development in information retrieval"
    },
    {
      "citation_id": "20",
      "title": "A regression approach to music emotion recognition",
      "authors": [
        "Yi-Hsuan Yang",
        "Yu-Ching Lin",
        "Ya-Fan Su",
        "Homer H Chen"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "21",
      "title": "Moodswings: A collaborative game for music mood label collection",
      "authors": [
        "Erik Youngmoo E Kim",
        "Lloyd Schmidt",
        "Emelle"
      ],
      "year": "2008",
      "venue": "Ismir"
    },
    {
      "citation_id": "22",
      "title": "Determination of nonprototypical valence and arousal in popular music: features and performances",
      "authors": [
        "Bj√∂rn Schuller",
        "Johannes Dorfner",
        "Gerhard Rigoll"
      ],
      "year": "2010",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "23",
      "title": "A comparison of the discrete and dimensional models of emotion in music",
      "authors": [
        "Tuomas Eerola",
        "Jonna Vuoskoski"
      ],
      "year": "2011",
      "venue": "Psychology of Music"
    },
    {
      "citation_id": "24",
      "title": "A comparative study of collaborative vs. traditional musical mood annotation",
      "authors": [
        "Jacquelin Speck",
        "Erik Schmidt",
        "Brandon Morton",
        "Youngmoo Kim"
      ],
      "year": "2011",
      "venue": "ISMIR"
    },
    {
      "citation_id": "25",
      "title": "The million song dataset",
      "authors": [
        "Thierry Bertin-Mahieux",
        "P Daniel",
        "Brian Ellis",
        "Paul Whitman",
        "Lamere"
      ],
      "year": "2011",
      "venue": "The million song dataset"
    },
    {
      "citation_id": "26",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "27",
      "title": "Ant√≥nio Pedro Oliveira, and Rui Pedro Paiva. Multi-modal music emotion recognition: A new dataset, methodology and comparative analysis",
      "authors": [
        "Renato Eduardo",
        "Silva Panda",
        "Ricardo Malheiro",
        "Bruno Rocha"
      ],
      "year": "2013",
      "venue": "10th Int. symposium on computer music multidisciplinary research (CMMR 2013)"
    },
    {
      "citation_id": "28",
      "title": "1000 songs for emotional analysis of music",
      "authors": [
        "Mohammad Soleymani",
        "Micheal Caro",
        "Erik Schmidt",
        "Cheng-Ya Sha",
        "Yi-Hsuan Yang"
      ],
      "year": "2013",
      "venue": "Proc. of the 2nd ACM Int. workshop on Crowdsourcing for multimedia"
    },
    {
      "citation_id": "29",
      "title": "Towards time-varying music auto-tagging based on cal500 expansion",
      "authors": [
        "Shuo-Yang Wang",
        "Ju-Chiang Wang",
        "Yi-Hsuan Yang",
        "Hsin-Min Wang"
      ],
      "year": "2014",
      "venue": "2014 IEEE Int. Conf. on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "30",
      "title": "The amg1608 dataset for music emotion recognition",
      "authors": [
        "Yu-An Chen",
        "Yi-Hsuan Yang",
        "Ju-Chiang Wang",
        "Homer Chen"
      ],
      "year": "2015",
      "venue": "2015 IEEE Int. Conf. on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "31",
      "title": "Emotions evoked by the sound of music: characterization, classification, and measurement",
      "authors": [
        "Marcel Zentner",
        "Didier Grandjean",
        "Klaus Scherer"
      ],
      "year": "2008",
      "venue": "Emotion"
    },
    {
      "citation_id": "32",
      "title": "The moodo dataset: Integrating user context with emotional and color perception of music for affective music information retrieval",
      "authors": [
        "Matev≈æ Pesek",
        "Gregor Strle",
        "Alenka Kavƒçiƒç",
        "Matija Marolt"
      ],
      "year": "2017",
      "venue": "Journal of New Music Research"
    },
    {
      "citation_id": "33",
      "title": "Bi-modal music emotion recognition: Novel lyrical features and dataset",
      "authors": [
        "Ricardo Malheiro",
        "Renato Panda",
        "Paulo Gomes",
        "Rui Pedro"
      ],
      "year": "2016",
      "venue": "th International Workshop on Music and Machine Learning-MML 2016-in conjunction with the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases-ECML/PKDD 2016"
    },
    {
      "citation_id": "34",
      "title": "The mood of chinese pop music: Representation and recognition",
      "authors": [
        "Xiao Hu",
        "Yi-Hsuan Yang"
      ],
      "year": "2017",
      "venue": "Journal of the Association for Information Science and Technology"
    },
    {
      "citation_id": "35",
      "title": "Moodylyrics: A sentiment annotated lyrics dataset",
      "authors": [
        "Erion √áano",
        "Maurizio Morisio"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 international conference on intelligent systems, metaheuristics & swarm intelligence"
    },
    {
      "citation_id": "36",
      "title": "Musical texture and expressivity features for music emotion recognition",
      "authors": [
        "Renato Panda",
        "Ricardo Malheiro",
        "Rui Pedro"
      ],
      "year": "2018",
      "venue": "19th Int. Society for Music Information Retrieval Conf. (ISMIR 2018)"
    },
    {
      "citation_id": "37",
      "title": "Developing a benchmark for emotional analysis of music",
      "authors": [
        "Anna Aljanaki",
        "Yi-Hsuan Yang",
        "Mohammad Soleymani"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "38",
      "title": "The pmemo dataset for music emotion recognition",
      "authors": [
        "Kejun Zhang",
        "Hui Zhang",
        "Simeng Li",
        "Changyuan Yang",
        "Lingyun Sun"
      ],
      "year": "2018",
      "venue": "Proc. of the 2018 acm on Int. Conf. on multimedia retrieval"
    },
    {
      "citation_id": "39",
      "title": "The ryerson audiovisual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "40",
      "title": "Music mood detection based on audio and lyrics with deep neural net",
      "authors": [
        "R√©mi Delbouys",
        "Romain Hennequin",
        "Francesco Piccoli",
        "Jimena Royo-Letelier",
        "Manuel Moussallam"
      ],
      "year": "2018",
      "venue": "Music mood detection based on audio and lyrics with deep neural net",
      "arxiv": "arXiv:1809.07276"
    },
    {
      "citation_id": "41",
      "title": "The mtg-jamendo dataset for automatic music tagging",
      "authors": [
        "Dmitry Bogdanov",
        "Minz Won",
        "Philip Tovstogan",
        "Alastair Porter",
        "Xavier Serra"
      ],
      "year": "2019",
      "venue": "ICML"
    },
    {
      "citation_id": "42",
      "title": "Learning to generate music with sentiment",
      "authors": [
        "N Lucas",
        "Jim Ferreira",
        "Whitehead"
      ],
      "year": "2021",
      "venue": "Learning to generate music with sentiment",
      "arxiv": "arXiv:2103.06125"
    },
    {
      "citation_id": "43",
      "title": "Music emotion recognition by using chroma spectrogram and deep visual features",
      "authors": [
        "Mehmet Bilal",
        "Ibrahim Berkan"
      ],
      "year": "2019",
      "venue": "Int. Journal of Computational Intelligence Systems"
    },
    {
      "citation_id": "44",
      "title": "Emopia: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation",
      "authors": [
        "Hsiao-Tzu Hung",
        "Joann Ching",
        "Seungheon Doh",
        "Nabin Kim",
        "Juhan Nam",
        "Yi-Hsuan Yang"
      ],
      "year": "2021",
      "venue": "Emopia: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation",
      "arxiv": "arXiv:2108.01374"
    },
    {
      "citation_id": "45",
      "title": "Mer500 -music emotion recognition",
      "authors": [
        "M Velankar"
      ],
      "year": "2020",
      "venue": "Mer500 -music emotion recognition"
    },
    {
      "citation_id": "46",
      "title": "Rafael Biazus Mangolin, Val√©ria Delisandra Feltrim, Marcos Aur√©lio",
      "authors": [
        "Igor Andr√©",
        "Pegoraro Santana",
        "Fabio Pinhelli",
        "Juliano Donini",
        "Leonardo Catharin ; Domingues"
      ],
      "year": "2020",
      "venue": "2020 Int. Conf. on Systems, Signals and Image Processing"
    },
    {
      "citation_id": "47",
      "title": "A comparative study of western and chinese classical music based on soundscape models",
      "authors": [
        "Jianyu Fan",
        "Yi-Hsuan Yang",
        "Kui Dong",
        "Philippe Pasquier"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "48",
      "title": "Muse: The musical sentiment dataset",
      "authors": [
        "Christopher Akiki",
        "Manuel Burghardt"
      ],
      "venue": "Journal of Open Humanities Data"
    },
    {
      "citation_id": "49",
      "title": "Detecting music-induced emotion based on acoustic analysis and physiological sensing: A multimodal approach",
      "authors": [
        "Xiao Hu",
        "Fanjie Li",
        "Ruilun Liu"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "50",
      "title": "Merp: a music dataset with emotion ratings and raters' profile information",
      "authors": [
        "En Yan Koh",
        "Kin Cheuk",
        "Kwan Yee Heung",
        "Kat Agres",
        "Dorien Herremans"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "51",
      "title": "Predicting emotion from music videos: exploring the relative contribution of visual and auditory information to affective responses",
      "authors": [
        "Phoebe Chua",
        "Dimos Makris",
        "Dorien Herremans",
        "Gemma Roig",
        "Kat Agres"
      ],
      "year": "2022",
      "venue": "Predicting emotion from music videos: exploring the relative contribution of visual and auditory information to affective responses",
      "arxiv": "arXiv:2202.10453"
    },
    {
      "citation_id": "52",
      "title": "Ym2413-mdb: A multi-instrumental fm video game music dataset with emotion annotations",
      "authors": [
        "Eunjin Choi",
        "Yoonjin Chung",
        "Seolhee Lee",
        "Jongik Jeon",
        "Taegyun Kwon",
        "Juhan Nam"
      ],
      "year": "2022",
      "venue": "Ym2413-mdb: A multi-instrumental fm video game music dataset with emotion annotations",
      "arxiv": "arXiv:2211.07131"
    },
    {
      "citation_id": "53",
      "title": "Musav: A dataset of relative arousal-valence annotations for validation of audio models",
      "authors": [
        "Dmitry Bogdanov",
        "Xavier Seijas",
        "Pablo Alonso-Jim√©nez",
        "Xavier Serra"
      ],
      "year": "2022",
      "venue": "Musav: A dataset of relative arousal-valence annotations for validation of audio models"
    },
    {
      "citation_id": "54",
      "title": "Emomv: Affective music-video correspondence learning datasets for classification and retrieval",
      "authors": [
        "Ha Thi",
        "Phuong Thao",
        "Gemma Roig",
        "Dorien Herremans"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "55",
      "title": "Multimodal music emotion recognition in indonesian songs based on cnn-lstm, xlnet transformers",
      "authors": [
        "Steven Sams",
        "Amalia Zahra"
      ],
      "year": "2023",
      "venue": "Bulletin of Electrical Engineering and Informatics"
    },
    {
      "citation_id": "56",
      "title": "Trompa-mer: an open dataset for personalized music emotion recognition",
      "authors": [
        "Juan Sebasti√°n G√≥mez-Ca√±√≥n",
        "Nicol√°s Guti√©rrez-P√°ez",
        "Lorenzo Porcaro",
        "Alastair Porter",
        "Estefan√≠a Cano",
        "Perfecto Herrera-Boyer",
        "Aggelos Gkiokas",
        "Patricia Santos",
        "Davinia Hern√°ndez-Leo",
        "Casper Karreman"
      ],
      "year": "2023",
      "venue": "Journal of Intelligent Information Systems"
    },
    {
      "citation_id": "57",
      "title": "Characterizing the emotional context induced by music listening and its effects on gait initiation: Exploiting physiological and biomechanical data",
      "authors": [
        "M√©hania Doumbia",
        "Maxime Renard",
        "Laure Coudrat",
        "Geoffray Bonnin"
      ],
      "year": "2023",
      "venue": "Adjunct Proc. of the 31st ACM Conf. on User Modeling, Adaptation and Personalization"
    },
    {
      "citation_id": "58",
      "title": "Ensa dataset: a dataset of songs by non-superstar artists tested with an emotional analysis based on time-series",
      "authors": [
        "Yesid Ospitia-Medina",
        "Jos√© Ram√≥n Beltr√°n",
        "Sandra Baldassarri"
      ],
      "year": "2023",
      "venue": "Ensa dataset: a dataset of songs by non-superstar artists tested with an emotional analysis based on time-series"
    },
    {
      "citation_id": "59",
      "title": "The emotion-to-music mapping atlas (emma): A systematically organized online database of emotionally evocative music excerpts",
      "authors": [
        "Hannah Strauss",
        "Julia Vigl",
        "Peer-Ole Jacobsen",
        "Martin Bayer",
        "Francesca Talamini",
        "Wolfgang Vigl",
        "Eva Zangerle",
        "Marcel Zentner"
      ],
      "year": "2024",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "60",
      "title": "Situnes: A situational music recommendation dataset with physiological and psychological signals",
      "authors": [
        "Vadim Grigorev",
        "Jiayu Li",
        "Weizhi Ma",
        "Zhiyu He",
        "Min Zhang",
        "Yiqun Liu",
        "Ming Yan",
        "Ji Zhang"
      ],
      "year": "2024",
      "venue": "Proc. of the 2024 ACM SIGIR Conf. on Human Information Interaction and Retrieval"
    },
    {
      "citation_id": "61",
      "title": "Merge-a bimodal dataset for static music emotion recognition",
      "authors": [
        "Pedro Lima Louro",
        "Hugo Redinho",
        "Ricardo Santos",
        "Ricardo Malheiro",
        "Renato Panda",
        "Rui Pedro"
      ],
      "year": "2024",
      "venue": "Merge-a bimodal dataset for static music emotion recognition",
      "arxiv": "arXiv:2407.06060"
    },
    {
      "citation_id": "62",
      "title": "Popular hooks: A multimodal dataset of musical hooks for music understanding and generation",
      "authors": [
        "Xinda Wu",
        "Jiaming Wang",
        "Jiaxing Yu",
        "Tieyao Zhang",
        "Kejun Zhang"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)"
    },
    {
      "citation_id": "63",
      "title": "Utilizing listener-provided tags for music emotion recognition: A data-driven approach",
      "authors": [
        "Joanne Affolter",
        "Martin Rohrmeier"
      ],
      "year": "2024",
      "venue": "25th Int. Society for Music Information Retrieval Conf. (ISMIR 2024)"
    },
    {
      "citation_id": "64",
      "title": "Xmusic: Towards a generalized and controllable symbolic music generation framework",
      "authors": [
        "Can Sida Tian",
        "Wei Zhang",
        "Wei Yuan",
        "Wenjie Tan",
        "Zhu"
      ],
      "year": "2025",
      "venue": "Xmusic: Towards a generalized and controllable symbolic music generation framework",
      "arxiv": "arXiv:2501.08809"
    },
    {
      "citation_id": "65",
      "title": "Experimental studies of the elements of expression in music",
      "authors": [
        "Kate Hevner"
      ],
      "year": "1936",
      "venue": "The American journal of psychology"
    },
    {
      "citation_id": "66",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "67",
      "title": "Pleasure, arousal, dominance: Mehrabian and russell revisited",
      "authors": [
        "Iris Bakker",
        "Theo Van Der",
        "Peter Voordt",
        "Jan Vink",
        "Boon"
      ],
      "year": "2014",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "68",
      "title": "The biopsychology of mood and arousal",
      "authors": [
        "Robert Thayer"
      ],
      "year": "1990",
      "venue": "The biopsychology of mood and arousal"
    },
    {
      "citation_id": "69",
      "title": "Mediaeval 2019: Emotion and theme recognition in music using jamendo",
      "authors": [
        "Dmitry Bogdanov",
        "Alastair Porter",
        "Philip Tovstogan",
        "Minz Won ; Larson",
        "M Hicks",
        "S Constantin",
        "M Bischke",
        "B Porter",
        "A Zhao",
        "P Lux",
        "M Cabrera Quiros",
        "L Calandre",
        "J Jones"
      ],
      "year": "2019",
      "venue": "MediaEval'19, Multimedia Benchmark Workshop"
    },
    {
      "citation_id": "70",
      "title": "Expression, perception, and induction of musical emotions: A review and a questionnaire study of everyday listening",
      "authors": [
        "N Patrik",
        "Petri Juslin",
        "Laukka"
      ],
      "year": "2004",
      "venue": "Journal of new music research"
    },
    {
      "citation_id": "71",
      "title": "Psychophysiological measures of emotional response to romantic orchestral music and their musical and acoustic correlates",
      "authors": [
        "Konstantinos Trochidis",
        "David Sears",
        "Di√™u-Ly Tr√¢n",
        "Stephen Mcadams"
      ],
      "year": "2012",
      "venue": "From Sounds to Music and Emotions: 9th Int. Symposium, CMMR 2012"
    },
    {
      "citation_id": "72",
      "title": "Anatomically distinct dopamine release during anticipation and experience of peak emotion to music",
      "authors": [
        "Mitchel Valorie N Salimpoor",
        "Kevin Benovoy",
        "Alain Larcher",
        "Robert Dagher",
        "Zatorre"
      ],
      "year": "2011",
      "venue": "Nature neuroscience"
    },
    {
      "citation_id": "73",
      "title": "Emotion in motion: A study of music and affective response",
      "authors": [
        "Javier Jaimovich",
        "Niall Coghlan",
        "Benjamin Knapp"
      ],
      "year": "2012",
      "venue": "From Sounds to Music and Emotions: 9th Int. Symposium, CMMR 2012"
    },
    {
      "citation_id": "74",
      "title": "Relationship of skin temperature changes to the emotions accompanying music",
      "authors": [
        "Richard Mcfarland"
      ],
      "year": "1985",
      "venue": "Biofeedback and Self-regulation"
    },
    {
      "citation_id": "75",
      "title": "Emotion recognition based on physiological changes in music listening",
      "authors": [
        "Jonghwa Kim",
        "Elisabeth Andr√©"
      ],
      "year": "2008",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "76",
      "title": "Perceived and induced emotion responses to popular music: Categorical and dimensional models",
      "authors": [
        "Yading Song",
        "Simon Dixon",
        "Marcus Pearce",
        "Andrea Halpern"
      ],
      "year": "2016",
      "venue": "Music Perception: An Interdisciplinary Journal"
    },
    {
      "citation_id": "77",
      "title": "Twitter sentiment classification using distant supervision",
      "authors": [
        "Alec Go",
        "Richa Bhayani",
        "Lei Huang"
      ],
      "year": "2009",
      "venue": "CS224N project report"
    },
    {
      "citation_id": "78",
      "title": "Emosoundscapes: A dataset for soundscape emotion recognition",
      "authors": [
        "Jianyu Fan",
        "Miles Thorogood",
        "Philippe Pasquier"
      ],
      "year": "2017",
      "venue": "2017 Seventh international conference on affective computing and intelligent interaction (ACII)"
    },
    {
      "citation_id": "79",
      "title": "Ranking-based emotion recognition for music organization and retrieval",
      "authors": [
        "Yi-Hsuan Yang",
        "Homer Chen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "80",
      "title": "Ranking-based emotion recognition for experimental music",
      "authors": [
        "Jianyu Fan",
        "Kivan√ß Tatar",
        "Miles Thorogood",
        "Philippe Pasquier"
      ],
      "year": "2017",
      "venue": "ISMIR"
    },
    {
      "citation_id": "81",
      "title": "Pearson correlation coefficient. Noise reduction in speech processing",
      "authors": [
        "Israel Cohen",
        "Yiteng Huang",
        "Jingdong Chen",
        "Jacob Benesty",
        "Jacob Benesty",
        "Jingdong Chen",
        "Yiteng Huang",
        "Israel Cohen"
      ],
      "year": "2009",
      "venue": "Pearson correlation coefficient. Noise reduction in speech processing"
    },
    {
      "citation_id": "82",
      "title": "Regression-based music emotion prediction using triplet neural networks",
      "authors": [
        "Kin Cheuk",
        "Yin-Jyun Luo",
        "Gemma Balamurali",
        "Dorien Roig",
        "Herremans"
      ],
      "year": "2020",
      "venue": "2020 Int. joint Conf. on neural networks (ijcnn)"
    },
    {
      "citation_id": "83",
      "title": "Emotion and theme recognition in music using attention-based methods",
      "authors": [
        "Srividya Tirunellai Rajamani",
        "Kumar Rajamani",
        "Bj√∂rn Schuller"
      ],
      "year": "2020",
      "venue": "Emotion and theme recognition in music using attention-based methods"
    },
    {
      "citation_id": "84",
      "title": "Recognizing song mood and theme: Clusteringbased ensembles",
      "authors": [
        "Maximilian Mayerl",
        "Michael V√∂tter",
        "Andreas Peintner",
        "G√ºnther Specht",
        "Eva Zangerle"
      ],
      "year": "2021",
      "venue": "MediaEval"
    },
    {
      "citation_id": "85",
      "title": "Semi-supervised music emotion recognition using noisy student training and harmonic pitch class profiles",
      "authors": [
        "Hao Hao"
      ],
      "year": "2021",
      "venue": "Semi-supervised music emotion recognition using noisy student training and harmonic pitch class profiles",
      "arxiv": "arXiv:2112.00702"
    },
    {
      "citation_id": "86",
      "title": "Frequency dependent convolutions for music tagging",
      "authors": [
        "Vincent Bour"
      ],
      "year": "2021",
      "venue": "MediaEval"
    },
    {
      "citation_id": "87",
      "title": "A new model for emotion prediction in music",
      "authors": [
        "Hardik Sharma",
        "Shelly Gupta",
        "Yukti Sharma",
        "Archana Purwar"
      ],
      "year": "2020",
      "venue": "2020 6th International Conference on Signal Processing and Communication (ICSC)"
    },
    {
      "citation_id": "88",
      "title": "Multi-view neural networks for raw audio-based music emotion recognition",
      "authors": [
        "Na He",
        "Sam Ferguson"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Symposium on Multimedia (ISM)"
    },
    {
      "citation_id": "89",
      "title": "Musical instrument emotion recognition using deep recurrent neural network",
      "authors": [
        "Sangeetha Rajesh",
        "Nalini"
      ],
      "year": "2020",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "90",
      "title": "A multimodal music emotion classification method based on multifeature combined network classifier",
      "authors": [
        "Changfeng Chen",
        "Qiang Li"
      ],
      "year": "2020",
      "venue": "Mathematical Problems in Engineering"
    },
    {
      "citation_id": "91",
      "title": "Attentive rnns for continuous-time emotion prediction in music clips",
      "authors": [
        "Sanga Chaki",
        "Pranjal Doshi",
        "Priyadarshi Patnaik",
        "Sourangshu Bhattacharya"
      ],
      "year": "2020",
      "venue": "AffCon@ AAAI"
    },
    {
      "citation_id": "92",
      "title": "The multiple voices of musical emotions: source separation for improving music emotion recognition models and their interpretability",
      "authors": [
        "Jacopo De Berardinis",
        "Angelo Cangelosi",
        "Eduardo Coutinho"
      ],
      "year": "2020",
      "venue": "ISMIR"
    },
    {
      "citation_id": "93",
      "title": "Cochleogram-based approach for detecting perceived emotions in music",
      "authors": [
        "Mladen Russo",
        "Luka Kraljeviƒá",
        "Maja Stella",
        "Marjan Sikora"
      ],
      "year": "2020",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "94",
      "title": "Recognition of emotion in music based on deep convolutional neural network",
      "authors": [
        "Rajib Sarkar",
        "Sombuddha Choudhury",
        "Saikat Dutta",
        "Aneek Roy",
        "Sanjoy Saha"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "95",
      "title": "Mediaeval 2020 emotion and theme recognition in music task: Loss function approaches for multilabel music tagging",
      "authors": [
        "Dillon Knox",
        "Timothy Greer",
        "Benjamin Ma",
        "Emily Kuo",
        "Krishna Somandepalli",
        "Shrikanth Narayanan"
      ],
      "year": "2020",
      "venue": "MediaEval"
    },
    {
      "citation_id": "96",
      "title": "Research on music emotion classification based on cnnlstm network",
      "authors": [
        "Yin Yu"
      ],
      "year": "2021",
      "venue": "2021 5th Asian Conference on Artificial Intelligence Technology (ACAIT)"
    },
    {
      "citation_id": "97",
      "title": "Music emotion recognition using convolutional long short term memory deep neural networks",
      "authors": [
        "Serhat Hizlisoy",
        "Serdar Yildirim",
        "Zekeriya Tufekci"
      ],
      "year": "2021",
      "venue": "Engineering Science and Technology, an International Journal"
    },
    {
      "citation_id": "98",
      "title": "Selab-hcmus at mediaeval 2021: Music theme and emotion classification with co-teaching training strategy",
      "authors": [
        "Phu-Thinh Pham",
        "Minh-Hieu Huynh",
        "Hai-Dang Nguyen",
        "Minh-Triet Tran"
      ],
      "year": "2021",
      "venue": "MediaEval"
    },
    {
      "citation_id": "99",
      "title": "Music emotion recognition using recurrent neural networks and pretrained models",
      "authors": [
        "Jacek Grekow"
      ],
      "year": "2021",
      "venue": "Journal of Intelligent Information Systems"
    },
    {
      "citation_id": "100",
      "title": "Transformer-based approach towards music emotion recognition from lyrics",
      "authors": [
        "Yudhik Agrawal",
        "Ramaguru Guru",
        "Ravi Shanker",
        "Vinoo Alluri"
      ],
      "year": "2021",
      "venue": "European Conf. on information retrieval"
    },
    {
      "citation_id": "101",
      "title": "Tracing back music emotion predictions to sound sources and intuitive perceptual qualities",
      "authors": [
        "Shreyan Chowdhury",
        "Verena Praher",
        "Gerhard Widmer"
      ],
      "year": "2021",
      "venue": "Tracing back music emotion predictions to sound sources and intuitive perceptual qualities",
      "arxiv": "arXiv:2106.07787"
    },
    {
      "citation_id": "102",
      "title": "Asif Ali Laghari, and Rahul Yadav. A generative adversarial network model based on intelligent data analytics for music emotion recognition under iot",
      "authors": [
        "I-Sheng Huang",
        "Yu-Hsuan Lu",
        "Muhammad Shafiq"
      ],
      "year": "2021",
      "venue": "Mobile Information Systems"
    },
    {
      "citation_id": "103",
      "title": "Deep learning-based late fusion of multimodal information for emotion classification of music video",
      "authors": [
        "Raj Yagya",
        "Joonwhoan Pandeya",
        "Lee"
      ],
      "year": "2021",
      "venue": "Deep learning-based late fusion of multimodal information for emotion classification of music video"
    },
    {
      "citation_id": "104",
      "title": "A multi-genre model for music emotion recognition using linear regressors",
      "authors": [
        "Darryl Griffiths",
        "Stuart Cunningham",
        "Jonathan Weinel",
        "Richard Picking"
      ],
      "year": "2021",
      "venue": "Journal of New Music Research"
    },
    {
      "citation_id": "105",
      "title": "Study on music emotion recognition based on the machine learning model clustering algorithm",
      "authors": [
        "Yu Xia",
        "Fumei Xu"
      ],
      "year": "2022",
      "venue": "Mathematical Problems in Engineering"
    },
    {
      "citation_id": "106",
      "title": "A novel multi-task learning method for symbolic music emotion recognition",
      "authors": [
        "Jibao Qiu",
        "Tong Chen",
        "Zhang"
      ],
      "year": "2022",
      "venue": "A novel multi-task learning method for symbolic music emotion recognition",
      "arxiv": "arXiv:2201.05782"
    },
    {
      "citation_id": "107",
      "title": "Feature selection approaches for optimising music emotion recognition methods",
      "authors": [
        "Le Cai",
        "Sam Ferguson",
        "Haiyan Lu",
        "Gengfa Fang"
      ],
      "year": "2022",
      "venue": "Feature selection approaches for optimising music emotion recognition methods",
      "arxiv": "arXiv:2212.13369"
    },
    {
      "citation_id": "108",
      "title": "Merge lyrics: Music emotion recognition next generation-lyrics classification with deep learning",
      "authors": [
        "Rafael Alexandre",
        "Portugal Matos"
      ],
      "year": "2022",
      "venue": "Merge lyrics: Music emotion recognition next generation-lyrics classification with deep learning"
    },
    {
      "citation_id": "109",
      "title": "Music emotion recognition based on segment-level two-stage learning",
      "authors": [
        "Na He",
        "Sam Ferguson"
      ],
      "year": "2022",
      "venue": "International Journal of Multimedia Information Retrieval"
    },
    {
      "citation_id": "110",
      "title": "Emotional classification of music using neural networks with the mediaeval dataset",
      "authors": [
        "Yesid Ospitia Medina",
        "Jos√© Ram√≥n Beltr√°n",
        "Sandra Baldassarri"
      ],
      "year": "2022",
      "venue": "Personal and Ubiquitous Computing"
    },
    {
      "citation_id": "111",
      "title": "Multi-modality in music: Predicting emotion in music from high-level audio features and lyrics",
      "authors": [
        "Tibor Krols",
        "Yana Nikolova",
        "Ninell Oldenburg"
      ],
      "year": "2023",
      "venue": "Multi-modality in music: Predicting emotion in music from high-level audio features and lyrics",
      "arxiv": "arXiv:2302.13321"
    },
    {
      "citation_id": "112",
      "title": "Music emotion recognition based on a neural network with an inception-gru residual structure",
      "authors": [
        "Xiao Han",
        "Fuyang Chen",
        "Junrong Ban"
      ],
      "year": "2023",
      "venue": "Electronics"
    },
    {
      "citation_id": "113",
      "title": "Modeling emotion dynamics in song lyrics with state space models",
      "authors": [
        "Yingjin Song",
        "Daniel Beck"
      ],
      "year": "2023",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "114",
      "title": "Tollywood emotions: Annotation of valence-arousal in telugu song lyrics",
      "authors": [
        ", B Manikanta Shanker",
        "B Gupta",
        "Vinoo Koushik",
        "Alluri"
      ],
      "year": "2023",
      "venue": "Tollywood emotions: Annotation of valence-arousal in telugu song lyrics",
      "arxiv": "arXiv:2303.09364"
    },
    {
      "citation_id": "115",
      "title": "Modularized composite attention network for continuous music emotion recognition",
      "authors": [
        "Meixian Zhang",
        "Yonghua Zhu",
        "Wenjun Zhang",
        "Yunwen Zhu",
        "Tianyu Feng"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "116",
      "title": "Automatic music emotion classification model for movie soundtrack subtitling based on neuroscientific premises",
      "authors": [
        "Maria Jose",
        "Pablo Revuelta-Sanz",
        "Belen Ruiz-Mezcua",
        "Israel Gonzalez-Carrasco"
      ],
      "year": "2023",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "117",
      "title": "Transformerbased automatic music mood classification using multi-modal framework",
      "authors": [
        "Sujeesha Ajithakumari",
        "Suresh Kumar",
        "Rajeev Rajan"
      ],
      "year": "2023",
      "venue": "Journal of Computer Science & Technology"
    },
    {
      "citation_id": "118",
      "title": "Music emotion prediction using recurrent neural networks",
      "authors": [
        "Xinyu Chang",
        "Xiangyu Zhang",
        "Haoruo Zhang",
        "Yulu Ran"
      ],
      "year": "2024",
      "venue": "Music emotion prediction using recurrent neural networks",
      "arxiv": "arXiv:2405.06747"
    },
    {
      "citation_id": "119",
      "title": "Mmd-mii model: a multilayered analysis and multimodal integration interaction approach revolutionizing music emotion classification",
      "authors": [
        "Jingyi Wang",
        "Alireza Sharifi",
        "Thippa Reddy Gadekallu",
        "Achyut Shankar"
      ],
      "year": "2024",
      "venue": "International Journal of Computational Intelligence Systems"
    },
    {
      "citation_id": "120",
      "title": "Verse1-chorus-verse2 structure: A stacked ensemble approach for enhanced music emotion recognition",
      "authors": [
        "Love Jhoye",
        "Moreno Raboy",
        "Attaphongse Taparugssanagorn"
      ],
      "year": "2024",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "121",
      "title": "A gai-based multiscale convolution and attention mechanism model for music emotion recognition and recommendation from physiological data",
      "authors": [
        "Xiao Han",
        "Fuyang Chen",
        "Junrong Ban"
      ],
      "year": "2024",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "122",
      "title": "Improved differential evolution algorithm based convolutional neural network for emotional analysis of music data",
      "authors": [
        "Jiajia Li",
        "Samaneh Soradi-Zeid",
        "Amin Yousefpour",
        "Daohua Pan"
      ],
      "year": "2024",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "123",
      "title": "Leveraging llm embeddings for cross dataset label alignment and zero shot music emotion prediction",
      "authors": [
        "Renhang Liu",
        "Abhinaba Roy",
        "Dorien Herremans"
      ],
      "year": "2024",
      "venue": "Leveraging llm embeddings for cross dataset label alignment and zero shot music emotion prediction",
      "arxiv": "arXiv:2410.11522"
    },
    {
      "citation_id": "124",
      "title": "Towards unified music emotion recognition across dimensional and categorical models",
      "authors": [
        "Jaeyong Kang",
        "Dorien Herremans"
      ],
      "year": "2025",
      "venue": "Towards unified music emotion recognition across dimensional and categorical models",
      "arxiv": "arXiv:2502.03979"
    },
    {
      "citation_id": "125",
      "title": "Music information retrieval by detecting mood via computational media aesthetics",
      "authors": [
        "Yazhong Feng",
        "Yueting Zhuang",
        "Yunhe Pan"
      ],
      "year": "2003",
      "venue": "Proc. IEEE/WIC Int. Conf. on web intelligence (WI 2003)"
    },
    {
      "citation_id": "126",
      "title": "Automatic mood detection and tracking of music audio signals",
      "authors": [
        "Lie Lu",
        "Dan Liu",
        "Hong-Jiang Zhang"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "127",
      "title": "Hcmus at mediaeval 2020: Emotion classification using wavenet feature with specaugment and efficientnet",
      "authors": [
        "Tri-Nhan Do",
        "Minh-Tri Nguyen",
        "Hai-Dang Nguyen",
        "Minh-Triet Tran",
        "Xuan-Nam Cao"
      ],
      "year": "2020",
      "venue": "Hcmus at mediaeval 2020: Emotion classification using wavenet feature with specaugment and efficientnet"
    },
    {
      "citation_id": "128",
      "title": "Emmanouil Benetos, et al. Mert: Acoustic music understanding model with large-scale self-supervised training",
      "authors": [
        "Yizhi Li",
        "Ruibin Yuan",
        "Ge Zhang",
        "Yinghao Ma",
        "Xingran Chen",
        "Hanzhi Yin",
        "Chenghao Xiao",
        "Chenghua Lin",
        "Anton Ragni"
      ],
      "year": "2023",
      "venue": "Emmanouil Benetos, et al. Mert: Acoustic music understanding model with large-scale self-supervised training",
      "arxiv": "arXiv:2306.00107"
    },
    {
      "citation_id": "129",
      "title": "Supervised and unsupervised learning of audio representations for music understanding",
      "authors": [
        "Filip Matthew C Mccallum",
        "Sergio Korzeniowski",
        "Fabien Oramas",
        "Andreas Gouyon",
        "Ehmann"
      ],
      "year": "2022",
      "venue": "Supervised and unsupervised learning of audio representations for music understanding",
      "arxiv": "arXiv:2210.03799"
    },
    {
      "citation_id": "130",
      "title": "Efficient supervised training of audio transformers for music representation learning",
      "authors": [
        "Pablo Alonso-Jim√©nez",
        "Xavier Serra",
        "Dmitry Bogdanov"
      ],
      "year": "2023",
      "venue": "Efficient supervised training of audio transformers for music representation learning",
      "arxiv": "arXiv:2309.16418"
    },
    {
      "citation_id": "131",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin W√∂llmer",
        "Bj√∂rn Schuller"
      ],
      "year": "2010",
      "venue": "Proc. of the 18th ACM Int. Conf. on Multimedia"
    },
    {
      "citation_id": "132",
      "title": "A tempo feature via modulation spectrum analysis and its application to music emotion classification",
      "authors": [
        "Yuan-Yuan Shi",
        "Xuan Zhu",
        "Hyoung-Gook Kim",
        "Ki-Wan Eom"
      ],
      "year": "2006",
      "venue": "2006 IEEE Int. Conf. on Multimedia and Expo"
    },
    {
      "citation_id": "133",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "Tomas Mikolov",
        "Kai Chen",
        "Greg Corrado",
        "Jeffrey Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "134",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proc. of the IEEE Conf. on computer vision and pattern recognition"
    },
    {
      "citation_id": "135",
      "title": "Flownet: Learning optical flow with convolutional networks",
      "authors": [
        "Alexey Dosovitskiy",
        "Philipp Fischer",
        "Eddy Ilg",
        "Philip Hausser",
        "Caner Hazirbas",
        "Vladimir Golkov",
        "Patrick Van Der",
        "Daniel Smagt",
        "Thomas Cremers",
        "Brox"
      ],
      "year": "2015",
      "venue": "Proc. of the IEEE Int. Conf. on computer vision"
    },
    {
      "citation_id": "136",
      "title": "Toward controllable text-to-music generation",
      "authors": [
        "Jan Melechovsky",
        "Zixun Guo",
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Dorien Herremans",
        "Soujanya Poria",
        "Mustango"
      ],
      "year": "2024",
      "venue": "Proc. of NAACL"
    },
    {
      "citation_id": "137",
      "title": "Ast: Audio spectrogram transformer",
      "authors": [
        "Yuan Gong",
        "Yu-An Chung",
        "James Glass"
      ],
      "year": "2021",
      "venue": "Ast: Audio spectrogram transformer",
      "arxiv": "arXiv:2104.01778"
    },
    {
      "citation_id": "138",
      "title": "High-fidelity audio compression with improved rvqgan",
      "authors": [
        "Rithesh Kumar",
        "Prem Seetharaman",
        "Alejandro Luebs",
        "Ishaan Kumar",
        "Kundan Kumar"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "139",
      "title": "Emotion recognition in music changes across the adult life span",
      "authors": [
        "F C√©sar",
        "S√£o Lu√≠s Lima",
        "Castro"
      ],
      "year": "2011",
      "venue": "Cogn. Emot"
    },
    {
      "citation_id": "140",
      "title": "A crosscultural analysis of the influence of timbre on affect perception in western classical music and chinese music traditions",
      "authors": [
        "Xin Wang",
        "Yujia Wei",
        "Lena Heng",
        "Stephen Mcadams"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "141",
      "title": "Cross-cultural mood perception in pop songs and its alignment with mood detection algorithms",
      "authors": [
        "Harin Lee",
        "Frank Hoeger",
        "Marc Schoenwiesner",
        "Minsu Park",
        "Nori Jacoby"
      ],
      "year": "2021",
      "venue": "Cross-cultural mood perception in pop songs and its alignment with mood detection algorithms",
      "arxiv": "arXiv:2108.00768"
    },
    {
      "citation_id": "142",
      "title": "Statistics notes: Cronbach's alpha",
      "authors": [
        "Martin Bland",
        "Douglas Altman"
      ],
      "year": "1997",
      "venue": "Statistics notes: Cronbach's alpha"
    },
    {
      "citation_id": "143",
      "title": "Learning from noisy labels with deep neural networks: A survey",
      "authors": [
        "Hwanjun Song",
        "Minseok Kim",
        "Dongmin Park",
        "Yooju Shin",
        "Jae-Gil Lee"
      ],
      "year": "2022",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "144",
      "title": "Ngc: A unified framework for learning with openworld noisy data",
      "authors": [
        "Zhi-Fan Wu",
        "Tong Wei",
        "Jianwen Jiang",
        "Chaojie Mao",
        "Mingqian Tang",
        "Yu-Feng Li"
      ],
      "year": "2021",
      "venue": "Proc. of the IEEE/CVF Int. Conf. on Computer Vision"
    },
    {
      "citation_id": "145",
      "title": "Combating noisy labels by agreement: A joint training method with co-regularization",
      "authors": [
        "Hongxin Wei",
        "Lei Feng",
        "Xiangyu Chen",
        "Bo An"
      ],
      "year": "2020",
      "venue": "Proc. of the IEEE/CVF Conf. on computer vision and pattern recognition"
    },
    {
      "citation_id": "146",
      "title": "Robust curriculum learning: from clean label detection to noisy label self-correction",
      "authors": [
        "Tianyi Zhou",
        "Shengjie Wang",
        "Jeff Bilmes"
      ],
      "year": "2020",
      "venue": "Int. Conf. on Learning Representations"
    },
    {
      "citation_id": "147",
      "title": "Psytoolkit: A novel web-based method for running online questionnaires and reaction-time experiments",
      "authors": [
        "Gijsbert Stoet"
      ],
      "year": "2017",
      "venue": "Teaching of Psychology"
    },
    {
      "citation_id": "148",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE Conf. on computer vision and pattern recognition"
    },
    {
      "citation_id": "149",
      "title": "Seeing stars of valence and arousal in blog posts",
      "authors": [
        "Georgios Paltoglou",
        "Michael Thelwall"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "150",
      "title": "Norms of valence, arousal, and dominance for 13,915 english lemmas",
      "authors": [
        "Amy Beth Warriner",
        "Victor Kuperman",
        "Marc Brysbaert"
      ],
      "year": "2013",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "151",
      "title": "Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 english words",
      "authors": [
        "Saif Mohammad"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "152",
      "title": "The perception of musical timbre",
      "authors": [
        "Stephen Mcadams",
        "Bruno Giordano"
      ],
      "year": "2014",
      "venue": "The perception of musical timbre"
    },
    {
      "citation_id": "153",
      "title": "An empirical approach to the relationship between emotion and music production quality",
      "authors": [
        "David Ronan",
        "Joshua Reiss",
        "Hatice Gunes"
      ],
      "year": "2018",
      "venue": "An empirical approach to the relationship between emotion and music production quality",
      "arxiv": "arXiv:1803.11154"
    },
    {
      "citation_id": "154",
      "title": "Expression and communication of emotion in music performance. Handbook of music and emotion: Theory, research, applications",
      "authors": [
        "N Patrik",
        "Renee Juslin",
        "Timmers"
      ],
      "year": "2010",
      "venue": "Expression and communication of emotion in music performance. Handbook of music and emotion: Theory, research, applications"
    },
    {
      "citation_id": "155",
      "title": "A quantitative, parametric model of musical tension",
      "authors": [
        "Mary Morwaread",
        "Farbood"
      ],
      "year": "2006",
      "venue": "A quantitative, parametric model of musical tension"
    },
    {
      "citation_id": "156",
      "title": "Music fadernets: Controllable music generation based on high-level features via low-level feature modelling",
      "authors": [
        "Hao Hao",
        "Dorien Herremans"
      ],
      "year": "2020",
      "venue": "Proc. of ISMIR"
    },
    {
      "citation_id": "157",
      "title": "Recognition of human emotions using eeg signals: A review",
      "authors": [
        "Ajay Md Mustafizur Rahman",
        "Md Sarkar",
        "Md Amzad Hossain",
        "Md Rabiul Selim Hossain",
        "Md Islam",
        "Julian Mw Biplob Hossain",
        "Mohammad Quinn",
        "Moni Ali"
      ],
      "year": "2021",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "158",
      "title": "Attendaffectnet: Self-attention based networks for predicting affective responses from movies",
      "authors": [
        "Ha Thi",
        "Phuong Thao",
        "B Balamurali",
        "Dorien Herremans",
        "Gemma Roig"
      ],
      "year": "2021",
      "venue": "2020 25th Int. Conf. on Pattern Recognition (ICPR)"
    }
  ]
}