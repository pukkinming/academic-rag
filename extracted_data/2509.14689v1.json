{
  "paper_id": "2509.14689v1",
  "title": "Harness: Lightweight Distilled Arabic Speech Foundation Models",
  "published": "2025-09-18T07:30:37Z",
  "authors": [
    "Vrunda N. sukhadia",
    "Shammur Absar Chowdhury"
  ],
  "keywords": [
    "Self-supervised model",
    "Distillation",
    "Benchmark resources",
    "Arabic downstream tasks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Large pre-trained speech models excel in downstream tasks but their deployment is impractical for resource-limited environments. In this paper, we introduce HArnESS, the first Arabic-centric self-supervised speech model family, designed to capture Arabic speech nuances. Using iterative selfdistillation, we train large bilingual HArnESS (HL) SSL models and then distill knowledge into compressed student models (HS, HST), preserving Arabic-specific representations. We use low-rank approximation to further compact the teacher's discrete supervision into shallow, thin models. We evaluate HAr-nESS on Arabic ASR, Speaker Emotion Recognition (SER), and Dialect Identification (DID), demonstrating effectiveness against HuBERT and XLS-R. With minimal fine-tuning, HAr-nESS achieves SOTA or comparable performance, making it a lightweight yet powerful alternative for real-world use. We release our distilled models and findings to support responsible research and deployment in low-resource settings.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Self-supervised learning (SSL) revolutionizes speech processing by extracting and learning transferable representations from large amounts of unlabeled speech. These large SSL models capture rich speech representations, making them highly effective across a wide range of speech-related tasks  [1, 2, 3, 4, 5, 6] . These models can be leveraged either as feature extractors or fine-tuned with a small amount of labeled data, significantly improving performance in low-resource settings.\n\nHowever, the generalization capability of SSL models is highly dependent on the diversity and volume of training data. Multilingual SSL speech models such as XLS-R  [7]  have demonstrated superior performance in low-resource languages compared to monolingual models, particularly those trained in high-resource languages like English  [8] . Yet, recent findings by  [9]  indicate that models like XLS-R tend to prioritize languages with abundant training data, potentially leading to suboptimal performance for underrepresented languages. This suggests that relying solely on multilingual models may not ensure optimal performance across all languages, and languagespecific SSL models could be essential to address this gap effectively.\n\nThe Arabic language poses a unique challenge in speech processing due to its vast linguistic diversity. Spoken across 22 countries, Arabic includes more than 20 mutually incomprehensible dialects, with significant influences from other languages such as English and French  [10] . Given this linguistic complexity, choosing an appropriate SSL model for Arabic speech tasks requires a model that can effectively capture the nuances of spoken dialects while preserving cultural and phonetic diversity. Multilingual models, though useful, may not fully grasp the intricacies of Arabic speech, making the case for a dedicated Arabic-English SSL model. However, training and deploying language-specific speech SSL models requires significant computational resources, large-scale unlabeled data, and prolonged training times, making them costly and inaccessible for many researchers. These high resource demands hinder efficient finetuning and deployment in resource-constrained settings.\n\nKnowledge distillation has emerged as a key technique for compressing large models while preserving their performance. Distillation enables a smaller, more efficient model (student) to learn from a larger, computationally expensive teacher model, reducing memory usage and inference time without significant performance degradation. Early works such as DistillHuBERT  [11] , FitHuBERT  [12] , DPHuBERT  [13] , SKILL  [14]  among others  [15, 16]  have applied task-agnostic knowledge distillation to HuBERT  [2] .\n\nIn this study, we introduce HuBERT-based Arabic and English Self-Supervised Speech (HArnESS) models, the first family of Arabic self-supervised speech models, jointly trained on large-scale Arabic and English speech data. By training specifically on Arabic and English, HArnESS ensures better adaptation to dialectal variations, and phonetic intricacies, making it a suitable choice for Arabic speech applications.\n\nThe HArnESS models are trained using iterative selfdistillation, following the HuBERT architecture. This approach enhances model performance by using its own predecessor's predictions as supervisory signals across multiple training iterations. We leverage this learning paradigm, and train HArnESS-L with 24 encoder layers in the first few iterations, preserving its deep architecture. In the following iteration, we distill its knowledge into lightweight student models, resulting in HArnESS-S (shallow) and HArnESS-ST (shallow and thin) architectures. Furthermore, we apply low-rank approximation to compact the supervisory signal with rich and informative representation and ensure efficient knowledge transfer.\n\nWe evaluated the HArnESS-L, HArnESS-S and HArnESS-ST models on downstream tasks including Automatic Speech Recognition (ASR), Speaker emotion recognition (SER), and Dialect identification (DID). We also compared the performance of these models with HuBERT-large (English) and XLS-R (multilingual)  [7]  models. Therefore, our key contributions are: 1. Introducing HArnESS, the first Arabic-centric selfsupervised speech model family with HArnESS-L (large), HArnESS-S (shallow), and HArnESS-ST (shallow and thin) architectures. 2. Explore the iterative self-distillation paradigm for Arabic  Model Architecture The HArnESS model consists of a convolutional (CNN) feature extractor and Transformer encoders. Similar to HuBERT, WavLM  [1]  and Wav2Vec2.0  [17] , the CNN feature extractor includes 7 temporal convolutions layers. The transformer encoder layers l, has embedding dimension enc d , and are composed of a multi-head self-attention (MHA) with attn heads and a position-wise feed-forward network (FFN). Training Objectives We use the weighted sum of two crossentropy losses, applied separately to the masked and unmasked frames as training objectives. Weight Initialization We explored different weight initialization strategies to improve convergence and stability. We experimented with random weight initialization, where model weights were initialized using a uniform random distribution to introduce diversity in parameter values at the start of training. Additionally, we also employed a blocked averaging initialization approach, where weights were initialized by averaging blocks of layers from the preceding model. Pseudo-labels generation We employ K-means clustering to 1 Links removed for anonymity. assign discrete labels to speech frames, leveraging last-layer embeddings, which capture the most refined high-level representations from the model.  2  To enhance clustering efficiency for distillation, we apply Principal Component Analysis (PCA) to filter out redundant information and retain only the most meaningful features for representation. For the initial iteration, i = 0, we initialize training by extracting MFCC features from the raw speech input x and use them to generate the initial pseudolabels. We utilized publicly available Arabic and English speech corpora including QASR  [18] , MGB3  [19] , LibriSpeech  [20] , Common Voice (English and Arabic)  [21] , GigaSpeech  [22]  among others. To make the model culturally sensitive we incorporate spoken content from 15 Arabic-speaking countries (crawled from YouTube data), covering diverse dialects. Finally, we augment the selected datasets with approaches including speed perturbation, SpecAugment, adding reverberation and noise among others to regularize and enhance robustness of the model. We then pretrain the model on these 23K hours of speech, ensuring an almost balanced distribution between Arabic and English language. We strictly exclude any official test and development sets of the datasets, from pretraining to prevent data leakage and to ensure reliable evaluation. For training the k-means model, we used 300 hours subset of the 23K hours data. Iteration 3 Data: Our primary goal is to develop a lightweight Arabic-centric model. In this phase, we selected ≈1,100 hours of Arabic data from the QASR training dataset for knowledge distillation. To train the k-means model for generating pseudolabels, we randomly sampled 30% of the Iteration3 data (≈300 hours), ensuring efficient clustering while maintaining some linguistic diversity.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Harness Models",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experimental Setups",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Downstream Tasks And Data",
      "text": "Benchmarking speech SSL models for English has been extensively studied, with SUPERB  [6]  serving as a key standard for evaluating SSL efficacy across tasks like content recognition, speaker information, and paralinguistics. However, no such standardized benchmark exists for Arabic speech. To address this gap, we introduce a benchmarking effort for Arabic SSL models, evaluating performance across key tasks: ASR for content recognition, dialect identification (DID) for speaker information, and speaker emotion recognition (SER) for paralinguistic analysis. For ASR, we fine-tune Harness models on a small subset of QASR data (300 hrs) and test on MGB2. We investigate the generalization capability of these small models on unseen data by evaluating the model with MGB3 testset. For SER, we use KSUEmotion  [23] , a dataset collected from 23 speakers with 6 emotion classes. The dataset is split into train (3.30 h), dev (50 min) and test (1 h). 3 For DID, we use the ADI5 dataset covering 5 region-based dialect classes. Details in Table  1 . For downstream task evaluation, we opt for widely used measures -word error rate (WER) for ASR and accuracy (Acc) for DID and SER. Iteration 1 & 2: The HArnESS model is trained for three iterations using fairseq codebase  [24] . The architecture for HArnESS-L, HArnESS-S and HArnESS-ST are shown in Table  3 . For the first two iterations, we used the architecture HArnESS-L, which consists of a 24-layers. We train the HArnESS-L model on 23k hours of Arabic-English audio on 24 H100 GPUs, with a batch size of at most 62.5 seconds of audio per GPU. The first iteration is trained for 500k steps, while the second iteration is trained for 700k steps. In the first iteration, we use 39-dimensional MFCC features and apply k-means clustering with 1000 clusters to generate labels. For the second iteration, to get the better targets, we extract latent representations from the 9th layer of the first iteration model and cluster them using k-means with 1000 clusters to generate new labels. Iteration 3: In the iteration i = 3, we used the HArnESS-S and HArnESS-ST architectures, which have a shallower 4-layer transformer with reduced embedding dim 1024 and 512 respectively. We train HArnESS-S and HArnESS-ST for the third iteration on 1100 hours of arabic audio on 8 H100 GPUs, for 300k steps with a batch size of 75 seconds of audio per GPU. We extract features from the last transformer layer of the second iteration HArnESS-L for clustering (clusters=1000) and use those 3 We will release the data split for reproducibility. labels for training these two models. Hence, these two models can be seen as the third iteration models.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Pre-Training Training Parameters",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Downstream Training",
      "text": "For the downstream task, we utilized the SSL models as a feature extractor, and study its effectiveness for capturing rich representation. We averaged extracted embeddings from all the SSL layers and then passed them as a feature to the downstream network.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Did And Ser Architecture",
      "text": "For classification tasks, we opt for a simple CNN with the selfattenion-based network. The network processes the input SSL features with three consecutive temporal CNN layers, with a kernel size of 5, and ReLU activations with a dropout of 0.4 for the generalization. Following a self-attention pooling is applied, and passed through a FF layer before passing it to the output layer. The hidden dimensions for all are set to 80. Each model is trained with a batch size of 4 and ran for a total of 10K steps.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Automatic Speech Recognition",
      "text": "For ASR task, we trained an encoder-decoder architecture while optimizing joint CTC and attention loss. 4 The encoder com-",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "Comparision with Upper Bound: SOTA Model As an upper bound, we present the state-of-the-art (SOTA) performance for Arabic ASR, DID  [25] , and SER models. The results in Table  2  show that both compressed HArnESS models and HArnESS-L outperform SOTA models in SER and DID, despite having a simpler architecture and shorter training time. This highlights the model's ability to capture rich speaker and paralinguistic information effectively. For ASR, when compared to Fanar ASR  [26] , which is trained on 10K+ hours of dialectal data, HArnESS models, with only 300 hours of MSA finetuning, are just 5 (HArnESS-L) and 10 (HArnESS-S) points behind.\n\nHArnESS-L vs.\n\nExiting SSLs for Arabic The HAr-nESS models outperform both HuBERT and XLS-R across all the Arabic tasks, demonstrating the importance of having language-specific models. Despite the similar scale of the model, HArnESS-L clearly benefits from the language-specific knowledge while outperforming a multilingual model itself. HArnESS-S and HArnESS-ST clearly outperforms the XLS-R indicating that the compressed HArnESS also captured abstract representation from the large model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Effects Of Different Structural Compression And Design Choices",
      "text": "For iteration i = 3, we explored the impact of weight initialization in the student model and observed no significant change in downstream task performance (Figure  2 ), suggesting that initialization plays a minimal role at this stage of training.\n\nWith layer reduction, HArnESS-S achieves 79.4% structural compression compared to HArnESS-L while maintaining strong performance across tasks. It outperforms multilingual and English models, demonstrating its efficiency in Arabic speech tasks despite compression. However, compared to HArnESS-L, we observe a WER increase of 4.7, a SER accuracy drop of 3.51, and a DID accuracy decline of 14.4, indicating that dialectal nuances become less distinguishable in shallower networks, making DID the most impacted task.\n\nFurther reducing attention heads from H-S (attn = 16) to H-S * (attn = 4) results in an additional 26.15% structural compression (from 65M to 48M parameters). While DID performance suffers the most, the effect on SER and WER remains minimal (Figure  2 ).\n\nWith embedding dimension reduction (Table  4 ), we observe a sharp drop in performance at ∆S = 96.52% compression relative to HArnESS-L, highlighting the limitations of excessive dimensionality reduction, which significantly degrades model performance.\n\nHow Does Compressing the Supervision Signal Affect Performance? To examine the impact of compressing the supervision signal, we compare model loss during knowledge distillation in iteration i = 3 with and without PCA applied to the teacher's supervision signal before clustering. Figure  2  shows that PCA-based supervision converges faster than its counterparts, suggesting that reducing redundancy in the supervision signal improves training efficiency while maintaining effective knowledge transfer.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we introduced HArnESS, the first self-supervised Arabic-centric speech model family, designed to capture nuances of Arabic dialects. Through an iterative self-distillation approach, we transferred knowledge from large bilingual models to a shallow (and thin) student models while preserving Arabic-specific speech representations. Our experiments on Arabic ASR, SER, and DID tasks demonstrate that HAr-nESS achieves state-of-the-art or comparable results against existing multilingual models like HuBERT and XLS-R. The lightweight of HArnESS also makes it a highly efficient yet performance-compromised alternative. We will make available the lightweight models, and the benchmarking data for research purpose.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview: Iterative self-distillation training to build",
      "page": 2
    },
    {
      "caption": "Figure 1: gives an overview of the HArnESS training pipeline,",
      "page": 2
    },
    {
      "caption": "Figure 2: (a) Performance of different downstream tasks based",
      "page": 3
    },
    {
      "caption": "Figure 2: ), suggesting",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: show that both compressed HArnESS models and",
      "data": [
        {
          "22.6 51.2\n22.60 51.80": "15.50 41.60",
          "91.92%\n73.32%": "94.66%"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "2",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio"
    },
    {
      "citation_id": "3",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "A Baevski",
        "W.-N Hsu",
        "Q Xu",
        "A Babu",
        "J Gu",
        "M Auli"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "4",
      "title": "Self-supervised speech representation learning: A review",
      "authors": [
        "A Mohamed",
        "H -Y. Lee",
        "L Borgholt",
        "J Havtorn",
        "J Edin",
        "C Igel",
        "K Kirchhoff",
        "S.-W Li",
        "K Livescu",
        "L Maaløe"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training",
      "authors": [
        "Y.-A Chung",
        "Y Zhang",
        "W Han",
        "C.-C Chiu",
        "J Qin",
        "R Pang",
        "Y Wu"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "6",
      "title": "",
      "authors": [
        "S Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin",
        "T.-H Huang",
        "W.-C Tseng",
        "K Lee",
        "D.-R Liu",
        "Z Huang",
        "S Dong"
      ],
      "venue": ""
    },
    {
      "citation_id": "7",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S Li",
        "A Watanabe",
        "H Mohamed",
        "Yi Lee"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark"
    },
    {
      "citation_id": "8",
      "title": "Xls-r: Selfsupervised cross-lingual speech representation learning at scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal",
        "K Singh",
        "P Von Platen",
        "Y Saraf",
        "J Pino"
      ],
      "year": "2021",
      "venue": "Proceedings of Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Ml-superb: Multilingual speech universal performance benchmark",
      "authors": [
        "J Shi",
        "D Berrebbi",
        "W Chen",
        "H.-L Chung",
        "E.-P Hu",
        "W Huang",
        "X Chang",
        "S.-W Li",
        "A Mohamed",
        "H Yi Lee",
        "S Watanabe"
      ],
      "year": "2023",
      "venue": "Ml-superb: Multilingual speech universal performance benchmark"
    },
    {
      "citation_id": "10",
      "title": "Language bias in self-supervised learning for automatic speech recognition",
      "authors": [
        "E Storey",
        "N Harte",
        "P Bell"
      ],
      "year": "2024",
      "venue": "2024 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "11",
      "title": "Connecting Arabs: bridging the gap in dialectal speech recognition",
      "authors": [
        "A Ali",
        "S Chowdhury",
        "M Afify",
        "W El-Hajj",
        "H Hajj",
        "M Abbas",
        "A Hussein",
        "N Ghneim",
        "M Abushariah",
        "A Alqudah"
      ],
      "year": "2021",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "12",
      "title": "Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert",
      "authors": [
        "H.-J Chang",
        "S.-W Yang",
        "H.-Y Lee"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Fithubert: Going thinner and deeper for knowledge distillation of speech self-supervised learning",
      "authors": [
        "Y Lee",
        "K Jang",
        "J Goo",
        "Y Jung",
        "H.-R Kim"
      ],
      "year": "2022",
      "venue": "23rd Annual Conference of the International Speech Communication Association, INTERSPEECH 2022. ISCA"
    },
    {
      "citation_id": "14",
      "title": "Dphubert: Joint distillation and pruning of self-supervised speech models",
      "authors": [
        "Y Peng",
        "Y Sudo",
        "S Muhammad",
        "S Watanabe"
      ],
      "year": "2023",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH"
    },
    {
      "citation_id": "15",
      "title": "Skill: Similarity-aware knowledge distillation for speech selfsupervised learning",
      "authors": [
        "L Zampierin",
        "G Hacene",
        "B Nguyen",
        "M Ravanelli"
      ],
      "year": "2024",
      "venue": "Skill: Similarity-aware knowledge distillation for speech selfsupervised learning",
      "arxiv": "arXiv:2402.16830"
    },
    {
      "citation_id": "16",
      "title": "Deep versus wide: An analysis of student architectures for task-agnostic knowledge distillation of self-supervised speech models",
      "authors": [
        "T Ashihara",
        "T Moriya",
        "K Matsuura",
        "T Tanaka"
      ],
      "year": "2022",
      "venue": "Deep versus wide: An analysis of student architectures for task-agnostic knowledge distillation of self-supervised speech models"
    },
    {
      "citation_id": "17",
      "title": "Lighthubert: Lightweight and configurable speech representation learning with once-for-all hidden-unit bert",
      "authors": [
        "R Wang",
        "Q Bai",
        "J Ao",
        "L Zhou",
        "Z Xiong",
        "Z Wei",
        "Y Zhang",
        "T Ko",
        "H Li"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "18",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "19",
      "title": "QASR: QCRI Aljazeera Speech Resource. A Large Scale Annotated Arabic Speech Corpus",
      "authors": [
        "H Mubarak",
        "A Hussein",
        "S Chowdhury",
        "A Ali"
      ],
      "year": "2021",
      "venue": "Proc. of the 59th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "20",
      "title": "Speech recognition challenge in the wild: Arabic MGB-3",
      "authors": [
        "A Ali",
        "S Vogel",
        "S Renals"
      ],
      "year": "2017",
      "venue": "2017 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "21",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "22",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "R Ardila",
        "M Branson",
        "K Davis",
        "M Henretty",
        "M Kohler",
        "J Meyer",
        "R Morais",
        "L Saunders",
        "F Tyers",
        "G Weber"
      ],
      "year": "2020",
      "venue": "Common voice: A massively-multilingual speech corpus"
    },
    {
      "citation_id": "23",
      "title": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio",
      "authors": [
        "G Chen",
        "S Chai",
        "G Wang",
        "J Du",
        "W.-Q Zhang",
        "C Weng",
        "D Su",
        "D Povey",
        "J Trmal",
        "J Zhang",
        "M Jin",
        "S Khudanpur",
        "S Watanabe",
        "S Zhao",
        "W Zou",
        "X Li",
        "X Yao",
        "Y Wang",
        "Y Wang",
        "Z You",
        "Z Yan"
      ],
      "year": "2021",
      "venue": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio"
    },
    {
      "citation_id": "24",
      "title": "King saud university emotions corpus: Construction, analysis, evaluation, and comparison",
      "authors": [
        "A Meftah",
        "M Qamhan",
        "Y Seddiq",
        "Y Alotaibi",
        "S Selouani"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "25",
      "title": "fairseq: A fast, extensible toolkit for sequence modeling",
      "authors": [
        "M Ott",
        "S Edunov",
        "A Baevski",
        "A Fan",
        "S Gross",
        "N Ng",
        "D Grangier",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT 2019: Demonstrations"
    },
    {
      "citation_id": "26",
      "title": "Yet another model for Arabic dialect identification",
      "authors": [
        "A Kulkarni",
        "H Aldarmaki",
        "H Sawaf",
        "S El-Beltagy",
        "W Zaghouani",
        "W Magdy",
        "A Abdelali",
        "N Tomeh",
        "I Farha",
        "N Habash",
        "S Khalifa",
        "A Keleg",
        "H Haddad",
        "I Zitouni",
        "K Mrini",
        "R Almatham"
      ],
      "year": "2023",
      "venue": "Proceedings of ArabicNLP 2023"
    },
    {
      "citation_id": "27",
      "title": "Fanar: An Arabic-Centric Multimodal Generative AI Platform",
      "authors": [
        "U Fanar",
        "M Abbas",
        "F Ahmad",
        "E Alam",
        "E Altinisik",
        "Y Asgari",
        "S Boshmaf",
        "S Boughorbel",
        "S Chawla",
        "F Chowdhury",
        "K Dalvi",
        "N Darwish",
        "M Durrani",
        "A Elfeky",
        "M Elmagarmid",
        "M Eltabakh",
        "A Fatehkia",
        "M Fragkopoulos",
        "M Hasanain",
        "M Hawasly",
        "S.-G Husaini",
        "J Jung",
        "W Lucas",
        "S Magdy",
        "A Messaoud",
        "T Mohamed",
        "B Mohiuddin",
        "H Mousi",
        "A Mubarak",
        "Z Musleh",
        "M Naeem",
        "D Ouzzani",
        "A Popovic",
        "H Sadeghi",
        "M Sencar",
        "O Shinoy",
        "Y Sinan",
        "A Zhang",
        "Y Ali",
        "X Kheir",
        "C Ma",
        "Ruan"
      ],
      "year": "2025",
      "venue": "Fanar: An Arabic-Centric Multimodal Generative AI Platform"
    }
  ]
}