{
  "paper_id": "2409.04447v1",
  "title": "Leveraging Contrastive Learning And Self-Training For Multimodal Emotion Recognition With Limited Labeled Samples",
  "published": "2024-08-23T11:33:54Z",
  "authors": [
    "Qi Fan",
    "Yutong Li",
    "Yi Xin",
    "Xinyu Cheng",
    "Guanglai Gao",
    "Miao Ma"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The Multimodal Emotion Recognition challenge MER2024 focuses on recognizing emotions using audio, language, and visual signals. In this paper, we present our submission solutions for the Semi-Supervised Learning Sub-Challenge (MER2024-SEMI), which tackles the issue of limited annotated data in emotion recognition. Firstly, to address the class imbalance, we adopt an oversampling strategy. Secondly, we propose a modality representation combinatorial contrastive learning (MR-CCL) framework on the trimodal input data to establish robust initial models. Thirdly, we explore a self-training approach to expand the training set. Finally, we enhance prediction robustness through a multiclassifier weighted soft voting strategy. Our proposed method is validated to be effective on the MER2024-SEMI Challenge, achieving a weighted average F-score of 88.25% and ranking 6th on the leaderboard. Our project is available at https://github.com/WooyoohL/MER2024-SEMI.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal Emotion Recognition (MER) has increasingly garnered interest due to its potential in various applications including human-computer interaction (HCI)  (Moin et al. 2023; Li et al. 2024; Song et al. 2023) , mental health monitoring  (Xu, Wu, and Liu 2022; Shen et al. 2024) , and social media analysis  (Chandrasekaran, Nguyen, and Hemanth D 2021; Yu et al. 2024) . Research in MER primarily concentrates on three modalities: text, audio, and visual. There has been substantial progress in this field with the adoption of advanced techniques such as deep learning and multimodal fusion. Recent studies, for instance, Li et al.  (Li et al. 2021)  have introduced architectures like Deep Convolutional Neural Networks (DCNN) and Deep Separable Convolutional Neural Networks (DSCNN) for speech and face recognition. MFN  (Wu et al. 2019 ) explores how attention mechanisms can be successfully applied to model emotion recognition from rich narrative videos. DEFR  (Zhao and Liu 2021)  incorporates a novel transformer approach that combines convolutional spatial transformers with temporal transformers for enhanced extraction of facial features over time and space. Despite these advancements, challenges remain, particularly in the time-consuming and costly process of sentiment data annotation. The reliance on extensively annotated datasets severely restricts the practical applicability of most existing multimodal sentiment recognition methods.\n\nIn response to the above problems, the ACM MM unveiled the MER2024-SEMI challenge. This initiative requires participants to predict discrete emotions with a limited set of labeled data, complemented by a significant volume of unlabeled data for training. To enhance model generalization, the challenge encourages the exploration of largescale unlabeled data utilization. One effective strategy is semi-supervised learning (SSL), particularly self-training methods that have shown promise in various domains  (Liu et al. 2024a; Xin et al. 2023; Wang et al. 2023b ). These methods generate pseudo-labels from unlabeled data, integrating these with labeled data during the training process. Another promising strategy involves self-supervision  (Chen et al. 2020a,b; Liu et al. 2024b) , which entails pretraining models on large amounts of unlabeled data and then finetuning them for specific tasks  (Xin et al. 2024a,b) . In the context of the MER2024-SEMI challenge, participants must contend with a discrete set of six emotions: neutral, anger, happiness, sadness, worry, and surprise. Importantly, the distribution of data across these classes varies (as shown in Figure  1 ), posing additional challenges related to class imbalance that participants must address.\n\nTo tackle the MER2024-SEMI challenge, we propose a straightforward yet robust semi-supervised multimodal emotion recognition method. To address the class imbalance, we implement a crucial oversampling strategy, focusing on expanding classes with limited data. Next, we propose a modality representation combinatorial contrastive learning framework for effectively utilizing the provided unlabeled data in the trimodal input. Additionally, we explore semi-supervised techniques, particularly finding the selftraining strategy valuable. This approach involves generating pseudo-labels for unlabeled data and integrating them into the training process. Furthermore, to enhance the robustness of the prediction results, we employ ensemble learning via aggregating the confidence scores of multiclassifiers. Our approach achieves an 88.25% weighted average F-score on MER2024-SEMI.\n\ni=1 and and a large amount of unlabeled data Du = {(x i )} Nu i=1 , where y i is the discrete label of the sample x i and y i ∈ {1, 2, ..., C}. Here, N l , N u , and C respectively denote the number of labeled samples, unlabeled samples, and emotion classes. For each x i (or xi ), we extract visual features F v i , acoustic features F a i and text features F t i from the visual, audio, and text modalities, where\n\n, and {d m } m∈{v,a,t} represent the feature dimensions for each modality. For simplicity, we omit the subscript i in the following when there is no ambiguity.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Extraction",
      "text": "For the whole dataset, we extract the utterance level feature according to the MER2024 baseline  (Lian et al. 2023 (Lian et al. , 2024)) .\n\nVisual feature: Since the initial samples in MER2024 are in video format, we first crop and align the facial regions of each frame using the OpenFace toolkit  (Baltrušaitis, Robinson, and Morency 2016) . Subsequently, we leverage the pretrained CLIP-Large model  (Radford et al. 2021)  to extract frame-level features for each face image. These frame-level visual features are then aggregated using average pooling to generate video-level embeddings.\n\nAcoustic feature: First, we utilize the FFmpeg toolkit to separate the audio from the video with a sampling rate of 16kHz. Following this, we employ the Chinese-HuBert-Large acoustic encoder  (Hsu et al. 2021)  to extract features, which performs well on Chinese sentiment corpus. The averaged hidden representations from the last 4 layers of the model are used as the final acoustic representations due to their high sensitivity to the semantic information.\n\nText feature: We first convert audio files into transcripts by employing WeNet  (Yao et al. 2021) , an open-source automatic speech recognition toolkit. Then, we choose the Baichuan2-13B-Base model  (Baichuan 2023 ) that has been pre-trained on a large-scale corpus as a feature extractor and further input transcripts into the language model to obtain a feature vector as the representation of the text modality.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Preprocessing Imbalanced Data Processing",
      "text": "The distribution of labeled data provided by MER2024-Train&Val is shown in Figure  1 . Among them, the labels \"neutral\", \"angry\", and \"happy\" are dominant, \"worried\" and \"sad\" are secondary, while \"surprise\" accounts for only 3.8%. This dataset exhibits imbalance, where minority class samples risk being overwhelmed by majority samples during training, potentially impacting model performance. To address this issue, we rebalance the training data distribution by employing random oversampling. Through this process, each emotion category achieves a predetermined sample count, resulting in a more balanced dataset.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Noise Embedding Enhancement Strategy",
      "text": "Various data augmentation techniques can be applied to the raw data across different modalities to enhance the generalization of the model. For instance, image flipping and random cropping can be utilized for visual data, while spectrogram transformation can augment audio data. These augmentation strategies help the model generalize better and perform more reliably in diverse scenarios.\n\nHowever, integrating these diverse data enhancement methods with existing models can be challenging. To address this, inspired by the work of Wang et al.  (Wang et al. 2023a; Hazarika et al. 2022; Fan et al. 2023; Ho, Jain, and Abbeel 2020) , we introduce a noise embedding enhancement (NEE) strategy to intervene in the representation of each modality. It constructs noise embeddings {N m } m∈{v,a,t} for each extracted feature {F m } m∈{v,a,t} using Gaussian noise. The noise schedule is built across time steps, and the process is described by the following formula:\n\nwhere F m 0 denotes the embedding at time step 0, and ϵ is a noise vector randomly sampled from a Gaussian distribution, ϵ ∼ N (0, 1). Timestep parameter T is used to control the degree of noise injected into the data and simulate the process of gradual destruction of the data by random noise. The parameter β t is the t-th value of the schedule parameter sequence\n\nHere, the total number of steps T affects the gradual transition of the original embedding to the noise-dominated state and is set to 100. In our experiments, β 1 and β T are set to 0.001 and 0.1, respectively. By embedding random noise at the feature vector level through the aforementioned NEE strategy, the model is encouraged to adapt to variations in sample data to enhance its generalization ability.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Model Architecture Modality Representation Combinatorial Contrastive Learning",
      "text": "We propose Modality Representation Combinatorial Contrastive Learning (MR-CCL) framework to leverage the",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Noise Embedding Enhancement",
      "text": "Self-Training:\n\nStep  given unlabeled data more effectively. The overall architecture is illustrated in Figure  2 .\n\nIn the realm of multimodal emotion recognition tasks, a proven strategy involves extracting features specific to each modality as well as invariant features that capture relationships between different modalities, which facilitates the fusion and interaction of diverse modal inputs  (Zuo et al. 2023; Liu et al. 2024b ). To implement our framework, we begin by pre-training three specificity encoders and one invariant encoder using all available unlabeled data. This process is designed to enable the model to discern and encode the intrinsic characteristics and structural nuances of multimodal data. Each specificity encoder is composed of multiple Transformer layers, while the invariant encoder is structured with linear layers.\n\nThe input embedding {F m } m∈{v,a,t} for each modality along with their respective noise embeddings {N m } m∈{v,a,t} are fed into the specificity encoders to obtaining modality-specific embeddings { F m } m∈{v,a,t} and { N m } m∈{v,a,t} . Subsequently, the invariant encoder transforms the { F m } m∈{v,a,t} embeddings into a unified vector space, extracting modality-invariant emotion features {H m } m∈{v,a,t} , a technique validated for its effectiveness in multimodal emotion recognition  (Liu et al. 2024b) . Following this, we proceed the combinatorial contrastive learning method based on the features obtained from the encoders above.\n\n(1) Intra-modality Contrastive Learning with Noisy Embedding (IMCL-NE). We employ contrastive learning techniques to learn a consistent representation between the modality-specific representation { F m } m∈{v,a,t} and the noisy modality-specific representation { N m } m∈{v,a,t} to enhanced feature robustness and model generalization performance. Noise contrastive estimation  (Oord, Li, and Vinyals 2018 ) is utilized to calculate intra-modality contrastive loss of each modality. Taking a pair of visual embeddings F v and N v as an example, it can be implemented through the following equation:\n\nwhere B is the batch size, τ v is the temperature, (•) T represents the vector transpose operation, and L a f n and L t f n are calculated similarly. Thus, the total intra-modality contrastive loss L f n intra is given by:\n\n(2) Inter-Modality Contrastive Learning with Aligned Embeddings (IMCL-AE). Inspired by Shvetsova et al.  (Shvetsova et al. 2022) , we aim to calculate the intermodality contrastive loss between all possible modality combinations, mapping the inputs from single or multiple modalities into a joint embedding space where semantically similar inputs are closely positioned. First, the contrastive losses between single modalities are calculated using three pairs of embeddings: (H v , H a ), (H a , H t ), and (H v , H t ), from which the losses L av , L at , and L vt are respectively calculated according to Equation (3). Subsequently, we learn the close representation between one modality and the other two modalities. For instance, considering the embedding H v and concatenated vector H at = Concat(H a , H t ), this relationship can be quantified through the following equation:\n\nwhere τ at is the temperature, B is batch size, L a vt and L t av are calculated similarly. The total intermodality contrastive loss L imc is given by: L imc = (L av + L vt + L at + L v at + L a vt + L t av )/6.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Self-Training Strategy",
      "text": "Pseudo-labeling plays a significant role in utilizing unlabeled data. According to previous works  (Li, Gao, and Li 2023) , different models have different preferences for clas-sification. Similarly, we observe that the baseline model performs better on the \"happy\", \"worried\", and \"surprise\" categories, while our model performs better on the other categories. So we take the intersection between the inference results of these two models, generate and filter pseudo labels according to a certain confidence threshold.\n\nFirstly, we train an initial model and a baseline model using existing labeled data within our contrastive learning framework described in Section 2.4. These models are subsequently employed to predict unlabeled data and generate pseudo-labels. Next, we apply a confidence threshold to select predictions with high confidence. These highconfidence pseudo-labeled samples are incorporated into the training set as additional labeled data. Finally, the model is retrained using the augmented training set. Note that all the pseudo-labeled samples will not be divided into the validating set.\n\nHowever, the pseudo-labeling strategy also has potential drawbacks, particularly when incorrect pseudo-labels introduce erroneous information into the model. To mitigate these risks, we separate the training process into two steps. In the first step, we refrain from incorporating any pseudolabeled samples, which aims to establish a stable and relatively accurate model from the outset. In the second step, we introduce the pseudo-labeled samples into the training set while reducing the learning rate to gradually improve the model and decrease the influence of wrong labels.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Multi-Classifier Voting",
      "text": "We utilize a weighted soft voting strategy for classification, which combines outputs from four classifiers: one modality fusion classifier (Classifier F) and three unimodal classifiers (Classifiers A, V, and T). The unimodal classifiers receive modality-specific features and independently predict emotion categories. The modality fusion classifier processes a joint representation of multimodal inputs and performs emotion classification tasks.\n\nThe process begins with aggregating confidence scores from the four classifiers and assigning specific weights to each result. Subsequently, these weighted confidence scores are compared, and the classification with the highest confidence is selected. According to prior findings from the MER2024 Baseline  (Lian et al. 2024) , the audio modality classifier (Classifier A) has shown the highest individual accuracy. Therefore, Classifier A is assigned the highest weight. Following this, Classifiers V and T receive weights in descending order. The fusion classifier (Classifier F) is assigned a weight equal to Classifier A's weight to capitalize on its multimodal information.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets And Evaluation Metric",
      "text": "MER2024  (Lian et al. 2023 ) is a Chinese emotion dataset designed for emotion recognition challenge, comprising four subsets: Train&Val, MER-SEMI, MER-NOISE, and MER-OV. The Train&Val subset consists of 5030 labeled single-speaker video segments used for training and validation purposes in the MER2024 challenge. The MER-SEMI subset contains a total of 115,595 unlabeled data, with 1169 of these serving as the true test set for evaluating performance in the Track1 'semi-supervised learning challenge'. Participants in this track are tasked with predicting discrete emotions through a large number of unlabeled samples. They are encouraged to employ semi-supervised learning techniques to improve emotion recognition performance. Here, discrete emotion labels include 6 classes, i.e., worried, happy, neutral, angry, surprised, and sad. Similar to the baseline  (Lian et al. 2024) , we adopt weighted average Fscore (WAF)  (Lian, Liu, and Tao 2021)  to evaluate the overall recognition performance due to the inherent class imbalance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "We conduct all experiments on an NVIDIA A100 80GB GPU. For feature extraction, the embedding dimensions d v , d a , and d t for video, audio, and text are 768, 1024, and 5120, respectively.\n\nIn the process of pseudo label selection, we apply a threshold of 0.99 for the categories \"happy\", \"neutral\", \"angry\", and \"sad\" to determine pseudo labels. Given the inherent imbalance in the categories \"worried\" and \"surprise\", we use a lower threshold of 0.85. Table  2  presents the specific counts of pseudo labels assigned to each class. The weight of classifiers A, V, T, and F are set to  [0.7, 0.5, 0.4, 0.7] . During training, we employ the Adam optimizer for MR-CCL, setting the initial learning rate to 1e-4 and utilizing a batch size of 512. We train the model with a maximum of 40 epochs and employ an early stopping strategy with a patience of 5 epochs.\n\nIn the final training process, we incorporate both pseudolabel and oversample strategies. In the first step, we use the oversampling strategy to increase the three minority classes to 850 samples; in the second step, we add the data with false labels to the training set, increase the number of oversampling to 1000, and reduce the learning rate from 1e-4 to 5e-5. Each step contains 20 epochs.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Main Results And Analysis",
      "text": "Based on the MER-SEMI track theme, we employ two main ways to utilize unlabeled data: 1) The contrastive learning method based on unlabeled data, and 2) Expanding the training set by labeling unlabeled data with pseudo labels. Table  1  summarizes the results of our model in comparison to the baseline, as well as the outcomes from ablation experiments. The results indicate that using unlabeled data significantly enhances multimodal emotion recognition performance.\n\nWe present the results of expanding the training set with only the pseudo-labeling strategy or the oversample strategy in Table  2  and Table 3 . We can observe that utilizing the pseudo-label strategy gets a higher WAF on the validating set (and is more closer to the testing set) than using the original training set and the oversample strategy. This phenomenon proves that the pseudo-labeled data provides more diverse training data, improves the generalization ability of the model, and may make the data distribution of the training set closer to the data distribution of the test set, thus reducing the impact of distribution differences on the model performance. In addition, with the addition of pseudo-labeled data, the overfitting phenomenon of the model has been alleviated, and the generalization ability and performance of the model can be improved. We can also observe that the result of the pseudo-label strategy on the test set is similar to the oversample strategy. This may be because the distribution of the testing set still has differences from the validating and training sets, or because the model is overconfident in its predictions and generates false pseudo labels. Therefore, there is still room for improvement in the pseudo-labeling strategy. Table  4  also shows that the decrease in learning rate helps stabilize the performance of the model, which provides support for the above analysis.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "This paper describes our proposed semi-supervised multimodal emotion recognition method for the MER2024-SEMI challenge. Our method comprises three main steps: modality representation combinatorial contrastive learning, selftraining, and multi-classifier voting. Additionally, we perform oversampling to address the class imbalance problem. The effectiveness of the proposed method is validated, achieving a weighted average F-score of 88.25% on the test set of the MER2024-SEMI challenge. We intend to consider the stronger semi-supervised training strategy and fusion module in future work.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Among them, the labels",
      "page": 2
    },
    {
      "caption": "Figure 1: The distribution of MER2024-Train&Val.",
      "page": 2
    },
    {
      "caption": "Figure 2: Illustration of our MR-CCL framework.",
      "page": 3
    },
    {
      "caption": "Figure 2: In the realm of multimodal emotion recognition tasks, a",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 2: presents the specific",
      "page": 4
    },
    {
      "caption": "Table 1: summarizes the results of our model in compari-",
      "page": 4
    },
    {
      "caption": "Table 2: and Table 3. We can observe that utilizing",
      "page": 4
    },
    {
      "caption": "Table 1: Main results on the train&val and test set.",
      "page": 5
    },
    {
      "caption": "Table 2: The data distribution and WAF accuracy with",
      "page": 5
    },
    {
      "caption": "Table 4: also shows that the decrease in learning rate",
      "page": 5
    },
    {
      "caption": "Table 3: The impact of oversampling several minority",
      "page": 5
    },
    {
      "caption": "Table 4: The impact of learning rate adjustment strategy.",
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Baichuan 2: Open Large-scale Language Models",
      "authors": [
        "Baichuan"
      ],
      "year": "2023",
      "venue": "Baichuan 2: Open Large-scale Language Models",
      "arxiv": "arXiv:2309.10305"
    },
    {
      "citation_id": "2",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "T Baltrušaitis",
        "P Robinson",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "3",
      "title": "Multimodal sentimental analysis for social media applications: A comprehensive review",
      "authors": [
        "G Chandrasekaran",
        "T Nguyen",
        "D Hemanth"
      ],
      "year": "2021",
      "venue": "Data Mining and Knowledge Discovery"
    },
    {
      "citation_id": "4",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "5",
      "title": "Improved baselines with momentum contrastive learning",
      "authors": [
        "X Chen",
        "H Fan",
        "R Girshick",
        "K He"
      ],
      "year": "2020",
      "venue": "Improved baselines with momentum contrastive learning",
      "arxiv": "arXiv:2003.04297"
    },
    {
      "citation_id": "6",
      "title": "Learning noise-robust joint representation for multimodal emotion recognition under realistic incomplete data scenarios",
      "authors": [
        "Q Fan",
        "H Zuo",
        "R Liu",
        "Z Lian",
        "G Gao"
      ],
      "year": "2023",
      "venue": "Learning noise-robust joint representation for multimodal emotion recognition under realistic incomplete data scenarios",
      "arxiv": "arXiv:2311.16114"
    },
    {
      "citation_id": "7",
      "title": "Analyzing modality robustness in multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "Y Li",
        "B Cheng",
        "S Zhao",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2022",
      "venue": "Analyzing modality robustness in multimodal sentiment analysis",
      "arxiv": "arXiv:2205.15465"
    },
    {
      "citation_id": "8",
      "title": "Denoising diffusion probabilistic models",
      "authors": [
        "J Ho",
        "A Jain",
        "P Abbeel"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "9",
      "title": "Hubert: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Multimodal Emotion Recognition Model Based on a Deep Neural Network with Multiobjective Optimization",
      "authors": [
        "M Li",
        "X Qiu",
        "S Peng",
        "L Tang",
        "Q Li",
        "W Yang",
        "Y Ma"
      ],
      "year": "2021",
      "venue": "Multimodal Emotion Recognition Model Based on a Deep Neural Network with Multiobjective Optimization"
    },
    {
      "citation_id": "11",
      "title": "Mining High-quality Samples from Raw Data and Majority Voting Method for Multimodal Emotion Recognition",
      "authors": [
        "Q Li",
        "Y Gao",
        "Y Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM International Conference on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "MIE-Net: Motion Information Enhancement Network for Fine-Grained Action Recognition Using RGB Sensors",
      "authors": [
        "Y Li",
        "M Ma",
        "J Wu",
        "K Yang",
        "Z Pei",
        "J Ren"
      ],
      "year": "2024",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "13",
      "title": "CTNet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Mer 2023: Multilabel learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Z Lian",
        "H Sun",
        "L Sun",
        "K Chen",
        "M Xu",
        "K Wang",
        "K Xu",
        "Y He",
        "Y Li",
        "J Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "15",
      "title": "Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition",
      "authors": [
        "Z Lian",
        "H Sun",
        "L Sun",
        "Z Wen",
        "S Zhang",
        "S Chen",
        "H Gu",
        "J Zhao"
      ],
      "year": "2024",
      "venue": "Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition",
      "arxiv": "arXiv:2404.17113"
    },
    {
      "citation_id": "16",
      "title": "2024a. Enhanced detection classification via clustering svm for various robot collaboration task",
      "authors": [
        "R Liu",
        "X Xu",
        "Y Shen",
        "A Zhu",
        "C Yu",
        "T Chen",
        "Y Zhang"
      ],
      "venue": "2024a. Enhanced detection classification via clustering svm for various robot collaboration task",
      "arxiv": "arXiv:2405.03026"
    },
    {
      "citation_id": "17",
      "title": "Contrastive Learning based Modality-Invariant Feature Acquisition for Robust Multimodal Emotion Recognition with Missing Modalities",
      "authors": [
        "R Liu",
        "H Zuo",
        "Z Lian",
        "B Schuller",
        "H Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition framework using multiple modalities for an effective human-computer interaction",
      "authors": [
        "A Moin",
        "F Aadil",
        "Z Ali",
        "D Kang"
      ],
      "year": "2023",
      "venue": "The Journal of Supercomputing"
    },
    {
      "citation_id": "19",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "A Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "20",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "21",
      "title": "Harnessing XGBoost for Robust Biomarker Selection of Obsessive-Compulsive Disorder (OCD) from Adolescent Brain Cognitive Development (ABCD) data",
      "authors": [
        "X Shen",
        "Q Zhang",
        "H Zheng",
        "W Qi"
      ],
      "year": "2024",
      "venue": "Harnessing XGBoost for Robust Biomarker Selection of Obsessive-Compulsive Disorder (OCD) from Adolescent Brain Cognitive Development (ABCD) data"
    },
    {
      "citation_id": "22",
      "title": "Everything at once-multi-modal fusion transformer for video retrieval",
      "authors": [
        "N Shvetsova",
        "B Chen",
        "A Rouditchenko",
        "S Thomas",
        "B Kingsbury",
        "R Feris",
        "D Harwath",
        "J Glass",
        "H Kuehne"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs",
      "authors": [
        "X Song",
        "D Wu",
        "B Zhang",
        "Z Peng",
        "B Dang",
        "F Pan",
        "Z Wu"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "24",
      "title": "2023a. FEENN: The Feature Enhancement Embedded Neural Network for Robust Multimodal Emotion Recognition",
      "authors": [
        "C Wang",
        "J Dong",
        "Y Sui",
        "P Xu",
        "Y Wu",
        "Y Xu"
      ],
      "venue": "Proceedings of the International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "EMP: Emotion-guided Multi-modal Fusion and Contrastive Learning for Personality Traits Recognition",
      "authors": [
        "Y Wang",
        "D Li",
        "K Funakoshi",
        "M Okumura"
      ],
      "year": "2023",
      "venue": "Proceedings of the International Conference on Multimedia Retrieval"
    },
    {
      "citation_id": "26",
      "title": "Attending to Emotional Narratives",
      "authors": [
        "Z Wu",
        "X Zhang",
        "T Zhi-Xuan",
        "J Zaki",
        "D Ong"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "27",
      "title": "2024a. Vmtadapter: Parameter-efficient transfer learning for multi-task dense scene understanding",
      "authors": [
        "Y Xin",
        "J Du",
        "Q Wang",
        "Z Lin",
        "K Yan"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "28",
      "title": "Mmap: Multi-modal alignment prompt for cross-domain multi-task learning",
      "authors": [
        "Y Xin",
        "J Du",
        "Q Wang",
        "K Yan",
        "S Ding"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "29",
      "title": "Self-Training with Label-Feature-Consistency for Domain Adaptation",
      "authors": [
        "Y Xin",
        "S Luo",
        "P Jin",
        "Y Du",
        "C Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the International Conference on Database Systems for Advanced Applications"
    },
    {
      "citation_id": "30",
      "title": "A measurement method for mental health based on dynamic multimodal feature recognition",
      "authors": [
        "H Xu",
        "X Wu",
        "X Liu"
      ],
      "year": "2022",
      "venue": "Frontiers in Public Health"
    },
    {
      "citation_id": "31",
      "title": "Wenet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit",
      "authors": [
        "Z Yao",
        "D Wu",
        "X Wang",
        "B Zhang",
        "F Yu",
        "C Yang",
        "Z Peng",
        "X Chen",
        "L Xie",
        "X Lei"
      ],
      "year": "2021",
      "venue": "Wenet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit",
      "arxiv": "arXiv:2102.01547"
    },
    {
      "citation_id": "32",
      "title": "Credit card fraud detection using advanced transformer model",
      "authors": [
        "C Yu",
        "Y Xu",
        "J Cao",
        "Y Zhang",
        "Y Jin",
        "M Zhu"
      ],
      "year": "2024",
      "venue": "Credit card fraud detection using advanced transformer model",
      "arxiv": "arXiv:2406.03733"
    },
    {
      "citation_id": "33",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Z Zhao",
        "Q Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM International Conference on Multimedia"
    },
    {
      "citation_id": "34",
      "title": "Exploiting modality-invariant feature for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "H Zuo",
        "R Liu",
        "J Zhao",
        "G Gao",
        "H Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}