{
  "paper_id": "2211.16363v1",
  "title": "Analysis Of Constant-Q Filterbank Based Representations For Speech Emotion Recognition",
  "published": "2022-11-29T16:45:47Z",
  "authors": [
    "Premjeet Singh",
    "Shefali Waldekar",
    "Md Sahidullah",
    "Goutam Saha"
  ],
  "keywords": [
    "Constant-Q filterbank",
    "Constant-Q transform (CQT)",
    "Continuous wavelet transform (CWT)",
    "Time invariance",
    "Speech emotion recognition (SER)"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This work analyzes the constant-Q filterbank-based time-frequency representations for speech emotion recognition (SER). Constant-Q filterbank provides non-linear spectrotemporal representation with higher frequency resolution at low frequencies. Our investigation reveals how the increased low-frequency resolution benefits SER. The time-domain comparative analysis between short-term mel-frequency spectral coefficients (MFSCs) and constant-Q filterbank-based features, namely constant-Q transform (CQT) and continuous wavelet transform (CWT), reveals that constant-Q representations provide higher time-invariance at low-frequencies. This provides increased robustness against emotion irrelevant temporal variations in pitch, especially for low-arousal emotions. The corresponding frequency-domain analysis over different emotion classes shows better resolution of pitch harmonics in constant-Q-based time-frequency representations than MFSC. These advantages of constant-Q representations are further consolidated by SER performance in the extensive evaluation of features over four publicly available databases with six advanced deep neural network architectures as the back-end classifiers. Our inferences in this study hint toward the suitability and potentiality of constant-Q features for SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is a machine's ability to recognize emotions in a speech sample. With the evolution of smart devices, emotion recognition has become an essential facet of artificial intelligence. To master emotion recognition is a long-sought aim of researchers. The ability of machines to automatically gather human sentiment will lead to efficient man-machine interaction. Various applications of SER include assessing a driver's behavior in autonomous driving vehicles, patient monitoring in health-care services, consumer satisfaction in call centers, product analysis  [1] [2] [3] , etc. Although a considerable amount of progress has been made so far, emotion recognition continues to be one of the most challenging domains in speech signal processing  [4, 5] . This is mainly due to the differences in emotion expression by different individuals. Although few studies suggest that there are no significant differences between how emotions such as Happy, Angry, Fearful and Sad are expressed by individuals, there are some variations in the intensity of expression among people from different cultures and background  [6] [7] [8] .\n\nA general SER system contains an emotion-relevant information (feature) extraction module followed by an emotion-class classifier. In terms of emotion information extraction, the unavailability of a standard feature that promises decent emotion information extraction adds to the challenge around SER. Two types of speech features are prominently used in SER, namely prosodic and spectral features  [9, 10] . Prosodic features mainly include pitch, pitch harmonics, intonation, energy, and speaking rate. Spectral features  [9] [10] [11]  include vocal tract resonant frequencies (formants), spectral flux, spectral roll-off, mel-frequency based analysis, etc. Various works which propose specific features for SER generally incorporate techniques to obtain detailed spectral, prosodic, or both spectral-prosodic information  [12] . The aim here is to extract distinguishing emotional cues. This information is then fed to a classifier that classifies the speech utterances among different emotion classes.\n\nAnother popular approach in SER is combining information obtained from both spectral and prosodic speech characteristics  [9] . These methods include extraction of an exhaustive feature set and its statistics over speech segments and whole utterances. They are also termed brute-force methods  [13] . Although these show promising results, curse of dimensionality becomes an important issue while handling large number of features. Due to the impressive ability of deep learning methods to find the minima of loss functions, many studies employ deep neural networks (DNNs) to self-learn optimum features to either develop an end-to-end SER system  [14, 15]  or to use the learned features for SER after some post-processing  [16, 17] .\n\nDeep learning networks provide mathematical models that self-learn an end-to-end solution to a particular pattern recognition problem. This makes deep networks very appealing. However, due to the high complexity, deep networks are onerous to interpret  [18] , hence making it difficult to obtain an insight into the emotionally relevant characteristics of input speech. Further, due to random initialization of parameters, DNNs converge to different local minima even with identical network architecture and optimization criteria. This raises the question of whether the conclusion about interpretability is consistent for all possible solutions.\n\nDeep learning networks contain many parameters and for their proper training massive amount of data is required  [19] . As most of the available SER databases are small, transfer learning is frequently used as a substitute approach. However, since the pretrained models are trained on a completely different database, transfer learning limits the exploitation of the full potential of deep networks. Studies also show that deep networks fail to generalize well in out-of-domain SER scenarios  [20] . Despite all such disadvantages, DNNs perform better in SER by automatically learning features from speech representations as compared to the traditional handcrafted features applied to other machine learning techniques, such as support vector machine (SVM), linear discriminant analysis (LDA), k -nearest neighbors (k -NN)  [16, 17] , etc. Therefore, we employ a combination of handcrafted features and deep networks for improved emotion information extraction and enhanced SER performance.\n\nIn this work, we use constant-Q filterbank based time-frequency representations with various deep neural network architectures for SER. The time-frequency representation provides the handcrafted descriptor over which the deep neural network further extracts the emotion-relevant information. We analyze the relevance of constant-Q representations from emotion perspective in both time and frequency domains. We then compare constant-Q and mel-scale representations to investigate the effect of different nonlinearities on emotion prediction. We also examine and compare the stability of the two features toward time deformations. Additionally, our study shows a striking similarity between the constant-Q feature representations used in this work and first-layer scattering transform coefficients.\n\nIn the next section, we review the relevant literature and present our contributions. Section 3 elaborates the features used in our experiments. In Section 4, we analyze the differences between time-frequency representation of features in both time and frequency domains. Section 5.1 describes different classifiers employed in this work. The SER databases used in this study are detailed in Section 5.2. Section 5.3 outlines the experimentation methods. The results and corresponding discussion are given in Section 6, followed by conclusive statements in Section 7.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works And Motivation",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Literature Survey",
      "text": "Before the introduction of deep learning into signal processing, handcrafted features were the default choice of the SER researchers. One of the first seminal works on SER used 17 prosodic features, including speaking rate, voiced region duration, and statistics of pitch  [21] . The work in  [22]  used 32 features post feature selection on various statistics of prosodic features, e.g., mean, standard deviation, skewness, kurtosis, etc. Similarly, for discrimination between stressed and normal speech,  [23]  proposed using spectral features with their autocorrelation-based variants and showed significant improvement. Again, considering only spectral features,  [24]  used log frequency power coefficients for SER, and they were shown better than mel-frequency cepstral coefficients (MFCCs). In  [25] , statistics of MFCC on three different phoneme classes of speech signal reportedly improved the performance with increased speech segment length. Experiments performed in  [26]  showed that modulation spectral features, obtained by applying a separate modulation filterbank on the response of the auditory filterbank, are better in characterising different speech emotions than both MFCC and perceptual linear prediction. In  [27] , a discrete Fourier-parameters based model was made for SER. Authors observed that frequency harmonics extracted using Fourier analysis, and their first and second-order derivatives, contain adequate information to discriminate among different emotion classes.\n\nRecently, automatic feature extraction using deep neural networks has gained huge interest because of their ability to learn emotion relevant information from speech signals. Those learned features yield competitive SER performance compared to the traditional handcrafted features. End-to-end SER models were proposed with raw emotional speech and convolutional neural networks (CNNs) with long-short term memory (LSTM) networks in  [14, 28, 29] . Whereas  [17]  attempted to learn detailed emotionrelated information by providing log mel-spectrogram to the input of CNN and then applying discriminant temporal pyramid matching. Similarly,  [30]  used spectrogram of raw speech and glottal waveform as input to stacked denoising autoencoder with bidirectional LSTM (BLSTM) as the classifier. Regarding the suitability of deep learning network architectures for SER,  [31]  studied 2D CNN, LSTM, and fully-connected (FC) network architectures and reported that 2D CNN network fairs better for SER on IEMO-CAP database  [32] . CNN can be considered a static classifier that jointly processes many speech frames taken at a time. This led to the inference that SER is more dependent on static or utterance-level speech characteristics than dynamic or frame-level information, which is better processed by LSTM networks  [31] . Authors in  [20]  evaluated the generalization capability of various deep learning architectures in a cross-corpus SER scenario. The study concluded that convolution-based architectures are better for 'in the wild' test conditions.\n\nAmong different types of handcrafted features used in SER, spectral features are considered to contain a substantial amount of emotionally-rich information. According to  [25] , spectral features \"convey information on both what is being said, and how it is being said\". A time-frequency representation provides spectral features at different time intervals. In SER, spectrogram, mel-frequency spectral coefficients (MFSCs) 1  , and MFCC have been the common choice of time-frequency representation. Spectrogram and its mel-filter variants represent energy distribution across both time and frequency dimensions  [33] . Such representations provide details about the temporal spread of emotional patterns in the speech signal. However, these time-frequency representations were less effective without post-processing using phoneme class-wise spectral feature extraction in  [25] .\n\nRegarding the frequency domain localization of emotion information in time-frequency representation, several studies have shown that the low-frequency regions of the speech spectrum contain important emotion-related details compared to the high-frequency region  [34] [35] [36] [37] . These studies reveal the relevance of prosody features for SER and report that the emotions with higher arousal, such as Angry, Happy, and Fear, have higher average pitch frequency. In contrast, emotions with lower arousal, for example, Sad, and Neutral have lower pitch values and consistent pitch contours.\n\nAuthors in  [23]  reported that Neutral has better recognition rate around the first formant frequency F1 (200-1000 Hz) while around the second formant frequency F2 (1250-1270 Hz), the recognition accuracy of Anger is higher. The authors in  [38]  found the center frequencies of formants F2 and F3 to reduce in depressed individuals. In  [39] , it was shown that emotions with higher arousal have higher values of mean F1 and lower values of F2, whereas positive valence emotions have higher mean F2. Some discrimination between idle and negative emotions was shown using the temporal patterns of first two formant frequencies in  [40] . Authors in  [41]  demonstrated that non-linear frequency scales, such as logarithmic, mel, and equidistant rectangular bandwidth (ERB), positively impact SER performance when compared with linear scale. Hence, the studies performed in SER literature hint at the potential emotion salience of low-frequency speech regions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Motivation And Contributions",
      "text": "The existing SER literature draws attention to the importance of low frequencies of speech signals for emotion recognition. To exploit this characteristic, a non-linear timefrequency representation is required that can emphasize the low-frequency regions. The mel-scale warping in well-known mel-frequency based analysis introduces a logarithmbased non-linearity which provides some emphasis on lower frequencies. To further improve the resolution, we propose the use of constant-Q filterbank based non-linear scale for SER. The constant-Q transform (CQT) applies constant-Q filters to offer higher frequency resolution in low-frequency regions and higher time resolution in high-frequency regions. We hypothesize that, since the pitch harmonics and lower formants, which play a major role in emotion discrimination, reside in the speech spectrum's low-frequency regions, having a higher resolution in this region would capture emotion-related information more efficiently. Authors in  [42]  also argue that in CQT, the pitch information is visible and harmonics are well separated. Figure  1  compares CQT and MFSC timefrequency representations for two different emotions from EmoDB database. CQT was originally proposed for music processing  [43] , after which it was successfully applied to other audio processing applications, like anti-spoofing  [44, 45] , speaker verification,  [46]  and acoustic scene classification  [47, 48] . In  [29] , CQT was also studied for SER, but no significant improvement was observed. The reason could be the end-to-end model's inability to exploit CQT completely or the inappropriate choice of CQT computation parameters in the experiments.\n\nAnother transform that provides constant-Q filter based structure is the continuous wavelet transform (CWT)  [49] . CWT also gives varying frequency resolution, similar to CQT, by utilising different scale values of the wavelet basis function. Hence, we also use the CWT time-frequency representation and compare it with MFSC representation for SER. The difference in CQT and CWT then lies only in how time-frequency representation is computed. This difference also helps us in analysing the importance of timeinvariance in feature representation for SER. Although CQT is very similar to CWT, the former provides non-redundant features and is, therefore, better suited for a varying resolution time-frequency representation. Also, the CWT has been found superior than mel-filter based techniques for SER in different works, e.g.,  [50] [51] [52] , and  [53] .\n\nIn this work, we extend our preliminary study performed in  [54]  with an in-depth analysis focused on the advantages of constant-Q representations for SER and its comparison with the mel-scale features. We also extensively evaluate features on different databases, using six different DNN architectures. The following are the main contributions of this paper:\n\n• Detailed time-domain analysis of CQT time-frequency representation for SER and comparison with MFSC.\n\n• Time-domain analysis of CWT for SER and its comparison with CQT and MFSC.\n\n• Frequency-domain investigation of differences between mel and constant-Q filterbank representation.\n\n• Comparison of the performances obtained with different deep neural network architectures for SER.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Description Of Time-Frequency Representations",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Constant-Q Transform",
      "text": "The constant-Q transform of a time-domain signal x[n] is given as  [55] ,\n\nwhere, k denotes the CQT frequency index, ⌊.⌋ denotes the rounding-off to nearest integer towards negative infinity and a * k (n) is the complex conjugate of the CQT basis function  for k th CQT bin. The CQT basis, or the time-frequency atom, is a complex time-domain waveform given as,\n\nwhere, f k is the center frequency of a k , f s is the sampling frequency, and w(n) is the window function. In this work, Hann window is used for CQT calculation. The nonlinear placement of f k in CQT is given by the relation\n\nwith B being the number of frequency bins used per octave of frequency and f min is the frequency of the lowest bin. This bin placement is inspired by the equal-temperament scale used in music analysis  [44] . The constraints of equal-temperament scale and constant-Q factor of filters result in window length varying over frequency index k, given by,\n\nEquation 1 describes the CQT computation as convolution of the atom with every input signal sample. However, to reduce redundancy in feature and computational complexity,  [55]  proposed a CQT implementation which uses hop-length as a parameter to shift the constant-Q atom (or constant-Q basis) by a specific number of samples and hence provides time-frames in the time-frequency representation. Further improvements in CQT computation mentioned in  [55]  are,\n\n• Computation of frequency-domain inner product (correlation) between the signal 7 frame and time-frequency atom (spectral kernel) instead of time-domain computation.\n\n• Use of only non-zero values of sparse spectral kernels for computation.\n\n• Octave-wise transformation, starting from the highest octave followed by downsampling and low-pass filtering of the signal to obtain lower octaves.\n\nThese computation steps differentiate CQT from CWT and provide a non-linear timefrequency representation with reduced redundancy and computational complexity. In our experiments, we used the CQT implementation provided in the LibROSA 2  toolbox which uses the above mentioned computational improvements. When discrete cosine transform (DCT) is applied on CQT values after uniform resampling, the obtained coefficients are called constant-Q cepstral coefficients (CQCC)  [44] . However, when DCT is applied without resampling of CQT coefficients, we obtain constant-Q coefficients (CQC)  [56] . To get an estimate of class-separability of timefrequency representations, we performed the F-ratio analysis (as given in  [57] ) on frequency bins obtained from CQT and MFSC. Figure  2  and 5  show the F-ratio statistic of CQT and MFSC obtained at different frequency bins for different emotions with respect to Neutral emotion on EmoDB database. The higher F-ratio values at low-frequency bins of CQT and MFSC show the presence of more emotion discriminative information at these bins. The figures also indicate that CQT-spectrogram has higher percentage  of discriminative coefficients on an average due to higher resolution in low-frequency regions. Similarly, Fig.  3  shows the F-ratio plot for STFT (short-time Fourier transform) based spectrogram. Although some emotion classes in spectrogram has higher F-ratio values, the percentage of such discriminative coefficients is low in total. However, most of the higher F-ratio coefficients are again observed at low frequencies. CQT was first introduced for music analysis owing to its ability to model an equaltempered frequency scale followed in Western music  [44] . Studies performed in psychology also explain some relationship between music training and emotion perception. Individuals with music expertise can better perceive emotions from speech  [58, 59] . As CQT is a well-matched representation for music, this could also explain its suitability for SER. Although the CQT configuration found best for SER in our previous contribution  ([54] ) differs slightly from that used in music analysis, the division of complete frequency range in octaves with equal number of bins supports the similarity between music perception and SER.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Continuous Wavelet Transform",
      "text": "CWT is another transform that provides time-frequency representation of the input signal with varying frequency resolution  [60] . CWT of a signal x(t) is given as,\n\nwhere, ψ(t) is the wavelet basis function while varying attributes, whereas smaller scale values can capture minute details of the signal x(t). The collection of wavelet basis with selected scale values also provides a constant-Q factor filterbank representation  [60] . Moreover, the scale values in CWT can also be selected to obtain an equal-temperament representation similar to CQT. Thus, CWT also provides high frequency resolution at low frequencies and high time resolution at high frequencies. As described by Eq. 4, CWT coefficients are computed for every integer value of dilation u leading to a highly redundant and computationally extensive coefficient representation for different scale values. This makes the use of CWT as time-frequency representation less efficient. As the CNN requires the input in image (2-dimension) form, we performed sliding window fashioned summation of CWT coefficients to reduce the size of image (time-frequency representation) in time axis, making it more suitable to be used with CNN. However, the issue of redundancy and computation time remains unaffected.\n\nOur coefficient summation approach is also based on windowing performed in MFSC to obtain frames in the corresponding time-frequency representation.\n\nIn our CWT implementation, we used complex Morlet wavelet pertaining to its relatedness with human perception of vision and sound  [53] . We used dyadic scale values (2 a , where a ∈ R) in CWT for fair comparison with CQT time-frequency representation. The discrete wavelet transform (DWT) provides orthogonal wavelet bases which provide uncorrelated time-frequency features at their disposal. Also, since CNNs are efficient in extracting correlation across time and frequency, redundancy due to correlation among features available in CWT should assist in better capturing of correlation across CWT coefficients for SER. Hence, we exclude DWT from our experiments. Figure  4  shows the F-ratio plot of frequency bins of CWT for different emotion classes in EmoDB database. The plot shows that CWT also has frequency bins which are discriminative (have greater F-ratio values). However, the magnitude of F-ratio values are less for every emotion indicating lesser discriminative characteristics of CWT coefficients.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Mel-Frequency Spectral Coefficients",
      "text": "Mel-frequency based features, specifically MFSC and MFCC, are widely used in speech signal processing. These features are computed by framing and windowing the speech into short segments followed by computation of power spectrum of every frame  [61] . This provides a time-frequency representation with number of time frames on one dimension and number of frequency bins on the other. A mel-frequency based filterbank is then applied on the frequency response of every frame to obtain an output coefficient from each filter and hence introduce perception based non-linearity on the time-frequency representation.\n\nAlthough the non-linearity in mel-scale is based on human sound perception, it was mainly designed to recognize phonemes in speech. Also, mel filterbank provides poor low-frequency resolution as compared to constant-Q filterbank. As a result, the timefrequency representation generated using mel-scale provides less emphasis on the emotion salient frequency regions of speech signal. The lower F-ratio values of MFSC in Fig.  5  throughout the complete frequency range further indicates the inferiority of MFSC to discriminate among different emotions classes.\n\nWe also performed the F-ratio analysis of CQT, CWT, MFSC, and spectrogram feature on IEMOCAP and eNTERFACE. We found higher emotion discriminability of low-frequencies with these databases as well. However, here we report F-ratio analysis only over EmoDB to prevent text redundancy and to have F-ratio plots made with audio files containing the same contextual information (i.e., transcript).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Comparison Between Time-Frequency Representations",
      "text": "To understand the effect of filters applied on speech with non-linearly spaced center frequencies from SER perspective, we performed both time and frequency domain comparison between mel-scale and constant-Q based time-frequency representations. In time-domain analysis, we analyzed the time-warp stability and time-shift invariance of MFSC by reproducing the analysis performed in  [62]  and then extended it for analysis on constant-Q filterbank based features.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Time-Domain Representation Of Mfsc",
      "text": "Consider a signal x(t), its time-shifted version is given by x(t -c). The relation between Fourier transform of the original and the shifted version of the signal is,\n\n(\n\nApplying modulus on both sides, we obtain,\n\nEquation 6 shows that the modulus of the Fourier transform remains stable to time shift c under the condition that c << T , where T is the duration of the window over which the Fourier transform is computed. Hence, Fourier transform is inherently stable but only for duration T . Consider a time-warped (deformed) version of the signal given by x τ (t) = x(t -ǫt), with 0 < ǫ ≪ 1. The modulus of Fourier transform of the time warped signal is given as,\n\nThe factor 1 1-ǫ in the frequency term leads to a shift of the frequency components at ω ′ by a factor ǫ|ω ′ |. This effect gets more prominent at higher values of ω. Hence, the Fourier transform is stable to time-shifts but not to time-warping. Such effect would cause the same emotion utterance of different speakers (e.g., same utterance of a male and a female speaker) to have different frequency attributes making the classification process more difficult. Also, in STFT, the utterance is divided into frames of smaller duration (for example, 20 ms) and then the modulus of Fourier transform is computed for every frame. The STFT, therefore, remains invariant to time-shifts existing only under 20 ms duration. However, since speech segments with length greater than 250 ms contain the information required for emotion prediction  [17] , the STFT fails to capture it efficiently. Therefore, the STFT (spectrogram) time-frequency representation is not an effectual representation from SER perspective.\n\nIn mel-scale based representations, a mel-filterbank is applied on the computed STFT to obtain MFSC/MFCC. The mel-filterbank contains filters with logarithmically spaced center frequencies. Let the signal frame centered at time t be given as x t (u) = x(u)φ(ut), where u is the time index and φ is the framing window. In MFSC, the Fourier transform of the signal frame (X t (ω)) is multiplied with Fourier transform of every melfilter (ψ λ (ω)) and then summed to obtain a single coefficient at the output of every filter. Mathematically, this is given as,\n\nwhere, λ is the support or center frequency of the mel-filter, t is the center of the time frame and M x(t, λ) is the corresponding MFSC . Equation 8 can also be considered as averaging in frequency domain. Converting the multiplication in frequency-domain in Eq. 8 into convolution in time-domain, we get,\n\nSince the filter support λ of mel-filters in time is generally smaller than the support of φ, we have, represent mel and constant-Q filters with frequency support λ, φ(t) is the averaging window, p ∈ Z + , N k and f k are the time-span and frequency of the k th constant-Q basis, and * denotes convolution operation. We show the MFSC feature extraction process in continuous time-domain representation (t) for similarity with analysis performed in  [62] .\n\nEquation 9 shows that averaging performed in frequency-domain (Eq. 8) is equivalent to time-domain averaging by filter φ. Also, the averaging in frequency domain in Eq. 8 provides one single coefficient representation of a band of frequencies (defined by bandwidth of mel-filters). This averaging makes the MFSC representation less susceptible to frequency shifts and hence stable to time-warps  [62] . Thus the MFSC representation is invariant to both deformation and time-shift but only for a specific frame duration (here, 20 ms). Therefore, although stable, MFSC is still incapable of capturing long time scale information, which is essential for SER  [17] . The left most column of Fig.  6  shows a pictorial representation of different steps involved in MFSC computation.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Time Domain Representation Of Cqt",
      "text": "We now show similar analysis on CQT to investigate the time and deformation invariance of CQT. From Eq. 1, the time-shifted version of CQT is given as,\n\nwhere, k and n are the frequency and time bins respectively, N k is the length of k th basis function, c is the time shift, X CQT is the corresponding CQT coefficient, and,\n\nTherefore, the relation between CQT of time shifted and original version of signal x is given as,\n\nAgain applying modulus on both sides,\n\nWe observe that the modulus of CQT exhibits the same time-shift invariance as observed in STFT. However, in CQT, the time window over which the transform is computed is not fixed. The time window duration varies with the relation\n\nTherefore, the time-invariance of CQT also varies with the frequency bin k. Since the value of N k is higher for small values of k and vice-versa, the time-invariance varies from high to low towards high frequencies in the CQT representation. The same is also evident from the smearing of frequency components observed in Fig.  7 . This is in contrast with the fixed time-invariance in MFSC, defined by the duration of window function φ(t). This property of CQT should help provide more time-invariance to the emotion relevant characteristic of speech, i.e., the pitch frequency  [34, 35] . With higher invariance the effect of emotion irrelevant variations in pitch, e.g., variations due to different speaking styles, contexts, etc., can be reduced, hence improving the complete emotion representation. At the same time, the quick variations in time that takes place at high frequency can also be simultaneously captured due to higher time-resolution. This property is more useful for emotions with comparatively more energy at high frequencies, for example, Angry and Happy. The Eq. 9 equivalent of CQT can be given as,\n\nHence, the time-frequency representation of CQT is the modulus of convolution of signal x with different filters (ψ λ ) in CQT filterbank. The modulus term provides timeinvariance to filterbank convolution output. The middle column of Fig.  6  shows different steps in CQT computation. Regarding stability to time-warping deformations, CQT also employs varying bandwidth filters with same effect as averaging across different frequency bands. This introduces similar deformation stability as done by mel-filter bank in MFSC. Figure  8  explains the effects of deformations on STFT and CQT and the corresponding stability in CQT. The stability removes the variations appearing due to different speaking styles, e.g., slower or faster speaking rate of different individuals.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Comparison Between Cqt And Cwt",
      "text": "CWT also provides a time-frequency representation generated from a constant-Q filterbank  [60] . However, the CWT representation is redundant as it does not include downsampling of band-pass filter responses. Also, due to its larger size, it is difficult to consider raw CWT as input to CNN. To alleviate this, our employed implementation includes CWT computation followed by framing and summation of the coefficients. The Eq. 9 equivalent of CWT computation is given as, where, ψ λ is band-pass wavelet basis with support at λ. The framing and averaging performed on computed CWT can also be given as low-pass filtering of Eq. 15. Hence,\n\nwhere, φ is the framing window and X CWT is the corresponding CWT coefficient. The modulus is applied to again remove the phase component and obtain time-invariant response. The right most column of Fig.  6  describes the computation of CWT. As the filters used in CWT closely follow the structure of CQT filters, Eq. 15 also provides varying time-invariance and higher low-frequency resolution. However, the averaging in Eq. 16 fixes the extent of time-invariance, similar to MFSC. In our experiments, CWT's frame duration is also kept equal to 20 ms following MFSC. Therefore, the time invariance in X CWT varies for low-frequency bins, i.e., for filters with time span greater than 20 ms, and remain fixed to 20 ms for filters with span less than 20 ms. Eq. 16 closely follows the time-domain MFSC (Eq. 9) and the first layer scattering domain coefficients explained in  [62]  for 1D signals. The comparison of CQT with the CWT time-frequency representation used in this work would help to analyze the importance of varying timeinvariance and improved low-frequency resolution, as compared to standard MFSC. This is further consolidated with the experiments that follow in later sections.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Frequency Domain Comparison Of Mel And Constant-Q Filterbanks",
      "text": "The major difference between the MFSC and CQT time-frequency representation in time domain appears in the form of fixed and varying time invariance across different frequency bins. In frequency domain, the major difference exists in the center frequencies of filters in the filterbank. Mel-scale follows a decadic logarithm scale, whereas CQT follows a binary logarithm scale. This leads to higher low-frequency resolution in CQT as compared to MFSC. Another property of the log-frequency is that the distance between  pitch harmonics is invariant to pitch frequency. This is contrast with in linear frequency representation (e.g., spectrogram) where the harmonic distance reduces with reduced pitch frequency. Figure  9  describes this phenomenon. This also helps CQT (or constant-Q response, in general) in better resolution of pitch and its harmonics as compared to STFT or MFSC. For further analysis, we compute the mean and standard deviation of pitch and first pitch harmonic, averaged across every utterance for different emotions of male and female speakers in EmoDB database. We then generate tones corresponding to average pitch and its first harmonic frequency and compute the CQT and MFSC representation of the tones. Figure  10  shows the obtained representations. We observe that CQT better resolves the pitch and its first harmonic as compared to MFSC for both male and female speakers. MFSC shows overlapping between pitch and its harmonic for emotions with low mean pitch frequency (Boredom, Neutral and Sadness). However, CQT clearly differentiates pitch and its harmonics for every emotion in EmoDB. Also, for Disgust, Neutral and Sad emotions, the separation in CQT representation of females is higher than that in males. This is because of the naturally higher pitch of female speakers as compared to that of the males. The error bars show the standard deviation of pitch for various emotions in EmoDB. Same value of standard deviation show different dynamic range over frequency bins in CQT and MFSC. This emphasizes the difference in low-frequency resolution in MFSC and CQT and also the superior pitch resolution in CQT.\n\nThe studies in  [63]  and  [64]  suggest that a filterbank structure with high resolution (dense filters) on mid and high frequency regions are beneficial for speaker recognition. In contrast to this, the dense filter arrangement at low-frequency in CQT and CWT should provide speaker complimentary information. Therefore, constant-Q representation should also remain invariant to speaker information compared to MFSC (because of more low-frequency resolution) facilitating SER.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Neural Network Architectures",
      "text": "In this subsection, we briefly review the different deep neural network architectures that were employed to evaluate SER performance of features. Our choice of these architectures was inspired by the success of techniques such as 1D and 2D convolutions, LSTM, attention mechanism, squeeze and excitation module, Res2Net module, etc. in different speech processing domains  [15, 29, [65] [66] [67] [68] .",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Tdnn Architecture",
      "text": "As a 1D convolutional network, a TDNN architecture, shown in Table  1 , was used for SER with the time-frequency representations as inputs. The TDNN structure was inspired by the x-vector systems developed for speaker verification  [69] . A TDNN structure processes frame-level information in the convolutional layer by applying different dilation values at different layers to efficiently capture the temporal spread of information. This is followed by a statistics pooling layer which aggregates the processed information in the temporal dimension to generate a segment level representation. The segment-level features are then fed to FC layers and a softmax layer for final classification. The TDNN structure used in this work employed an end-to-end classification system, in contrast with the structure used in  [69] , which extracts segment-level speaker embeddings from the statistics pooling layers for classification with a different classifier.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Conv2D Architecture",
      "text": "Table  1  also shows the architecture of a 2-dimensional CNN or Conv2D network used for SER classification. Conv2D network involves convolution of the input feature matrix with a kernel which allows movement in both time and frequency dimensions of input time-frequency representation. This helps in capturing the feature correlation across both time and frequency dimensions. As the emotion information is known to remain temporally spread throughout the utterance, 2D convolution windows with different kernel sizes helps in information extraction across different time scales. Similar to the TDNN architecture, our Conv2D architecture also applied temporal mean and standard deviation pooling on the output of the final convolution layer. This operation again aggregates the features extracted by convolutional layers across time dimension to obtain a vector representation of the input segment. The temporal pooling is followed by FC and Softmax layers for classification. We used the Conv2D network for end-to-end classification, with speech features at the input and probable emotion class at the output. We kept the parameters, i.e., no. of filters, kernel sizes, and number of layers in Conv2D same as used in TDNN architecture for comparison. We also placed a max-pooling layer after the first convolutional layer to reduce parameters in Conv2D architecture and prevent overfitting.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Merc2020 Baseline Architecture",
      "text": "To compare SER performance across different architectures, we used the speech emotion recognition model proposed in multimodal emotion recognition competition 2020 (MERC 2020). The model contains three Conv2D layers, one LSTM layer followed by attention pooling and a dense layer. We used the implementation provided by the organizers of the MERC2020 3  .",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Attention-Based Lstm",
      "text": "We also evaluated the performance of different features with attention-based LSTM model proposed in  [65] . The model includes a modified LSTM, in which the forget gate is replaced with an attention mechanism called attention gate. The attention gate provides increased focus on the most emotion-relevant parts of the input time-frequency representation. Moreover, this modification decreases the number of trainable parameters in every attention-based LSTM block, causing a reduction in the model training time  [65] . The output of attention-based LSTM is fed to two separate attention layers, one focusing on time and the other on the frequency dimension. Hence, the complete architecture consists of two layers of modified LSTM units (attention-based LSTM blocks) followed by parallelly placed time and frequency attention layers. The activations from attention layers are concatenated and fed to two FC layers and softmax for final classification. We chose the same parameter values for different layers as used in the original paper.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Transformer Encoder Model",
      "text": "We employed a transformer encoder model proposed in  [66]  to compare with the state-of-the-art deep learning architectures. In  [66] , information from various modalities (speech, speaker, and text) was extracted using encoder blocks, and the output was concatenated for SER. Since our approach includes only speech modality, we chose the speech signal's transformer encoder block from  [66]  in our experiments. The model consists of a Conv1D layer followed by a combination of multi-head attention and linear layers, constituting the encoder block. The output of the encoder block is then fed to a frame-level (or time-level) attention pooling layer followed by an FC and softmax. The parameter values of the neural network layers were the same as used in the original paper.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Ecapa-Tdnn",
      "text": "The emphasized channel attention, propagation, and aggregation in time-delay neural network (ECAPA-TDNN) proposed in  [68]  introduces multiple enhancements over the standard x-vector TDNN architecture  [69] . These include using multiple 1D Res2Net modules, squeeze and excitation blocks, and channel-dependent time-frame attention. The ECAPA-TDNN architecture is found useful for speaker recognition and speaker diarization tasks  [67] . In this work, we used the implementation of ECAPA-TDNN provided in SpeechBrain 4  Python toolkit without any change in parameter configuration.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Conv2D-Lstm",
      "text": "In  [70] , an attention-based convolutional recurrent neural network (ACRNN) was proposed for SER. Inspired by this, we designed an architecture consisting of only CNN and LSTM layers. We removed the attention layer from this architecture to analyze the effect of the temporal memory-based recurrent layer on CNN learned features and compare it with plain Conv2D architecture. Also, the MERC2020 model already included a combination of LSTM and attention with convolution layers. Table  1  describes our employed Conv2D-LSTM architecture. An LSTM contains a memory element that can accumulate the required information across several time frames. Using LSTM on CNN activations helps extract emotion-relevant temporal information from translation invariant and downsampled feature representations provided by CNN. Since the last time-frame output of the LSTM contains the most abundant emotion information  [65] , we fed only this output to the final FC and softmax layers and discarded the remaining time frames. This architecture also helped to compare the temporal aggregation of emotion information of LSTM with that of attention layers in Attention-based LSTM, Transformer encoder, and ECAPA-TDNN architectures.\n\nFor the above mentioned deep networks, we used Keras 5  deep learning library for Conv2D, TDNN, and Conv2D-LSTM architecture and PyTorch 6  deep learning library for the remaining selected state-of-the-art architectures.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Databases",
      "text": "We used four different databases for analysis and evaluation of features. They are freely available and widely used in SER. Performance of selected features on these corpora also facilitates comparison with similar SER methods. Table  2  summarizes the different SER databases used in our experiments.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Berlin Emotion Database (Emodb)",
      "text": "Berlin emotion database (EmoDB)  [71]  is one of the most widely used database in SER. It includes acted spoken utterances of 10 professional artists (5 female and 5 male). Ten sentences, emotionally neutral and phonetically rich, are spoken by the actors in German language. The database contains speech recordings of seven different emotions: Angry, Happy, Fear, Sad, Boredom, Disgust and Neutral. The authenticity of the recorded emotions was evaluated by listening test performed on 20 subjects. In total, the actors recorded 800 utterances but only 535, having more than 80% recognition rate and 60% naturalness, were finally selected. The mean recognition accuracy of emotions in the listening test was 84.3% on the selected 535 recordings. The diligent recording setup and free availability has led this database to be used in various important works  [10, 16, 17, 25-27, 37, 51]  and hence is used here as well.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Ryerson Audio-Visual Database Of Emotional Speech And Song (Ravdess)",
      "text": "The RAVDESS database  [72]  contains emotion speech and song samples recorded from 12 male and 12 female artists speaking English language. The database contains a total of 7536 clips with data recorded in three modalities: audio-only, video-only, and audio-video. The audio-only modality contains 1440 speech utterances from all speakers spoken with eight different emotions (Happy, Angry, Sad, Neutral, Disgust, Calm, Surprised and Fear ) and two intensity levels, strong and normal. Evaluation of recorded clips were performed by 319 subjects, out of which 247 evaluated the validity and 72 provided test-retest reliability of recorded emotions. An average of 60% accuracy was obtained in validity test on recordings of all emotions. Recent design and inclusion of an extensive emotion set with varying intensities make this an important database for SER.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Interactive Emotional Dyadic Motion Capture Database (Iemocap)",
      "text": "IEMOCAP is an audio-visual database recorded on 10 professionally trained English speakers (5 male and 5 female) with two recording methods, scripted and improvised  [32] . This makes the IEMOCAP recordings more natural compared to the two databases mentioned previously. Eight emotions (Happy, Angry, Sad, Neutral, Fear, Disgust, Excitement and Surprise) were captured over a total of 10039 recorded samples (5255 scripted and 4784 spontaneous) with an average utterance length of 4 seconds. Samples were annotated into both discrete and continuous emotion labels by six evaluators. In our work, we used discrete emotion labels from only four classes (Happy, Angry, Sad and Neutral ) of both scripted and improvised recordings because of comparatively sparse speech samples for the remaining emotions and also for better comparison with existing SER literature  [20, 30, 31, 74, 75] .",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Enterface '05",
      "text": "eNTERFACE is also an audio-visual database containing recording of six different emotions: Happy, Sad, Surprise, Anger and Fear, recorded from 44 different subjects  [73] . Although the subjects were from different nationalities, English was the common language for recording the data. This introduced accent variability on recorded samples, representing the real-world scenario in a better way. To induce emotions, subjects were made to read a short story before recording their reactions to the story on a fixed set Table  3 : Optimized parameter settings for different features (Q = Q-factor of the filter, fs = sampling frequency, f k = k th frequency bin). The parameters values are taken from experiments performed in  [54] .",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Mel-Frequency Features (Mfsc)",
      "text": "Library of answers. This introduced genuineness into the subject's reactions. We used only the audio modality having a total of 1293 utterances across all the subjects.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Training/Testing Evaluation",
      "text": "Leave-one-speaker-out (LOSO) cross-validation strategy was employed for evaluation of features with DNN classifiers. The databases were divided into training, validation, and test partitions such that the training and validation groups contained sets of disjoint speakers with one left-out speaker kept for testing. The validation set contained speech utterances of only two speakers. Hence, the number of training-validation-testing sets were same as the total number of speakers for every database. According to the literature, speaker-dependent SER generally fairs better than speaker-independent SER  [76] . However, using speaker-independent sets from the database eliminates the chances of the trained classifier being biased towards a set of speakers, and also simulates the practical/real-world scenario. We used LOSO even for RAVDESS and eNTERFACE databases which have higher number of speakers (> 10), unlike the leave-one-speakergroup-out (LOSGO) method used in other works, e.g.,  [17, 77] .\n\nAlthough LOSO cross-validation is computationally extensive, we can safely ignore the increase in complexity due to the small sizes of available SER databases. Also, using only one speaker for testing provides the advantage of more data samples in training and validation, which is essential for small databases.\n\nWe used an energy-based speech activity detector to remove silence parts of speech utterances before feature extraction. We also employed cepstral mean variance normalization on features before providing them to the classifiers. To increase the size of available training data, we employed five-fold data augmentation using additive and re- verberation noises following x-vector extraction recipe  [69]  used in Kaldi 7  toolkit. All available databases were downsampled to 16 kHz before feature extraction.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "Feature Evaluation",
      "text": "Table  3  shows the values of the arguments of the built-in feature extraction functions. LibROSA 8  Python library was used for CQT and MFSC while PyWavelets 9  was employed for CWT feature extraction in this work. The parameter values were inspired by our preliminary study, where we performed the optimization of bins per octave and hop length over 8 octaves on EmoDB database  [54] . For a fair comparison, similar optimization on MFSC and CWT is employed in this work. We used 24 mel-filter bank with 64 samples hop across different time frames. This corresponds to the optimized CQT parameters values which provided the best results (3 bins per octave with a total 8 octaves, refer  Fig 11) . In CWT, we used scale values obtained from the expression 2\n\nwhere k varies from 3 to 26, to obtain the same number of frequency bins as in CQT. The 3 in 2 k 3 corresponds to the voices per octave with number of octaves again fixed to 8. We chose default values for the remaining input parameters of CQT and MFSC functions in LibROSA and CWT in PyWavelets.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Classifier Evaluation",
      "text": "For training of DNN architectures, we used non-overlapping segments of input timefrequency representation of length 100 frames. However, during testing, we used the utterances' complete duration. These settings improved the final recognition accuracy compared to the scenario where the test data was truncated. We used a learning rate value of 0.001 with a batch size of 64. The networks were trained for 50 epochs, and the model with the highest validation unweighted average recall (UAR) was used during inference. In TDNN, Conv2D, and Conv2D-LSTM architectures, a drop-out value of 0.3 was used in fully connected layers. ReLU activation was used in every layer except for the final softmax layer.\n\nFor SER evaluation, we employed commonly used accuracy and UAR as performance metrics. Accuracy is calculated by finding the ratio between the number of correctly classified utterances to the total number of utterances in test set. The UAR metric is given as  [78] :\n\nwhere, A refers to the contingency matrix, A ij corresponds to the number of samples in class i classified into class j, and K is the total number of classes. As accuracy is considered unintuitive for databases with unequal samples across different classes, we optimized the feature extraction parameters based on the UAR metric.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Embedding Visualization",
      "text": "To understand the discriminability among the constant-Q and mel-scaled representations, we present the tSNE plots of the speech embeddings of CQT and MFSC extracted from the statistics pooling layer the TDNN architecture with EmoDB database in Fig.  12 . We kept the Angry embeddings on top and considered the positions of other emotion embeddings across the x-and y-axis as separation across valence and arousal axes on the arousal-valence plane  [3] . Notice better clustering by CQT embeddings at the emotion level. Nevertheless, both the embeddings show discrimination mainly across the arousal (vertical) axis. No major separation across the valence (horizontal) scale is observed across the embeddings. This indicates that the time-frequency representation can distinguish emotions across the arousal scale more than the valence scale.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Comparison Of Ser Performances",
      "text": "Table  4  shows the results obtained for different features, while Fig.  13  provides the visual representation of the numerical values in the table. Both constant-Q filterbank representations (CQT and CWT) perform better than MFSC in predicting emotion classes across different databases. This demonstrates the inappropriateness of mel-scale for emotion prediction. The considerable improvement in constant-Q representations across different architectures justifies the salience of low-frequency regions of speech for SER. Between the constant-Q features, the performance varies across databases. CWT is either equivalent to or better than CQT over different databases. For EmoDB, CWT mostly shows higher UAR than CQT. For RAVDESS, CQT is better than CWT, whereas, in IEMOCAP and eNTERFACE, CWT performs better than CQT. Over RAVDESS database, an anomaly is the performance of CWT which is equivalent to MFSC with TDNN and Conv2D architectures.\n\nAmong the TDNN, Conv2D, and Conv2D-LSTM architectures, Conv2D-LSTM fairs better over every database. Between TDNN and Conv2D, the latter performs better in    4 .\n\ndifferent layers of the model, which generates a better representation of emotion at the output. The LSTM layer further extracts the required temporal information from the activation of Conv2D layers, thereby improving performance.\n\nFor every architecture, constant-Q features outperform MFSC over every database. Also, the selected state-of-the-art deep learning architectures, i.e., Attention-based LSTM, Transformer encoder model, and ECAPA-TDNN, show inferior performance compared to plain Conv2D, TDNN, and Conv2D-LSTM. The Transformer encoder model has the poorest results among state-of-the-art networks across different databases because of the presence of only non-linear transformation-based attention layers and the lack of filterbased convolution operation to extract both time and frequency-based features. However, Attention-based LSTM architecture provides comparable performance (at least in UAR) to plain Conv2D and TDNN architectures. The final time and frequency-based attention layers in Attention-based LSTM help focus more on the emotion-relevant time and frequency cues. Similarly, ECAPA-TDNN outperforms both plain Conv2D and TDNN on eNTERFACE (in UAR) and provides comparable results on IEMOCAP. As ECAPA-TDNN contains many network parameters, its higher performance on databases with more data samples is justified. The Conv2D-LSTM model combines convolutional and temporal information extraction (due to the recurrent layer) to provide better performance on most databases. ing and windowing, the emotions less sensitive to high-frequency content and benefited from high-frequency averaging are emphasized by CWT. Figure  14  shows that Anger, which is known to have greater high frequency relevance  [34, 35] , gains slightly from time-invariance (or averaging) applied at high frequency. The same is observed for Neutral. In contrast, Disgust, which is also sensitive to high-frequency variations (  [35] ), is represented better with CQT rather than with averaged high-frequency representation in CWT.\n\nThe improvement with constant-Q features is more than that for MFSC, especially on high-arousal emotions (Fear, Anger, Happy) because of the higher pitch resolution in the former. Contours of pitch harmonics in high-arousal emotions contain sudden rises and drops. These intonations are better captured in constant-Q features with higher resolution at low frequencies. For low-arousal emotions like Sad and Boredom, pitch contours usually follow straight-line patterns causing similar performance of MFSC and constant-Q features. For every feature, Sad emotion class yields the highest classification accuracy. MFSC provided the lowest accuracy score for Fear whereas the constant-Q representations yield the lowest accuracy score for Boredom. The difference between recognition accuracies of Sad and Boredom appears mainly because of similar arousal and valence characteristics.\n\nTable  5  lists a few recent and relevant SER works. The absence of consensus on a standard experimental setup in SER literature is evident in the table. High SER performance can be achieved with less stringent evaluation frameworks, e.g., speakerdependent train/test split, fewer cross-validation folds, fewer emotion classes from the database, etc. Thus, the performance comparison of multiple systems is difficult and mostly non-conclusive. Other factors like the absence of standard performance metric and lack of reproducible research in SER adds to inaccuracy in comparison. In our attempt to compare the employed system with other SER works, we include both methodology and experimental setup in the table so that the differences among works are understood and, to some extent, the comparison is valid.\n\nComparison with the referred works shows that our method outperforms most of them on eNTERFACE and IEMOCAP databases but not EmoDB and RAVDESS. In EmoDB, the less stringent evaluation protocol in  [20]  and  [80] , in terms of train/test split, provides better classification results. The same argument applies to the works on the RAVDESS database. Although higher results can be achieved with lenient evaluation strategies, they do not imitate the real-world SER testing scenario. As discussed in Section 5.3, the LOSO cross-validation extensively evaluates the generalizability of a system with input from different unseen speakers without a large increase in computation complexity with small databases, making it a better evaluation strategy, especially for SER.\n\nLet us now consider the works that use large feature sets  [79, 80] . The famous eGeMAPS  [10]  and ComParE  [87]  feature sets contain many handcrafted features, including a combination of spectral and prosody features with statistics of a different order, unlike our method, which used spectral information only. Although considered appropriate as baseline, eGeMAPS and ComParE feature sets were designed after reviewing and selecting various handcrafted features found successful in previous SER studies  [10] . Therefore, using these feature sets can also be considered human intervention in the train/test phases of the machine learning system contrary to using a specific handcrafted time-frequency representation for supervised learning and hence, also explains the performance difference from our method.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Time-Invariance Analysis Over Features",
      "text": "The increased low-frequency resolution in constant-Q filterbank representation brings out better resolution of pitch information (Section 4.4: Fig.  10 ) thereby enhancing emotion recognition performance (Section 6.2: Table  5 , Fig.  13 , and Fig.  14 ). Regarding the difference in time-invariant property between MFSC and constant-Q-based responses, we compared the temporal lengths of optimized CQT basis and mel-filters in Fig.  15 . The low-frequency CQT basis functions (bases with center frequency ≤ 262 Hz) have a temporal spread greater than that of the 20 ms window used in STFT. Also, the modulus operation on computed CQT coefficients removes the phase information, leading to time-invariance defined by the length of the basis functions. Human pitch frequencies exist at around 250 Hz  [88] . Therefore, the time-invariance in CQT is greater than that of the standard STFT around pitch frequencies, making the former more robust against emotion-irrelevant pitch variations. Notice that the temporal spread of melfilters remains below 20 ms over the complete frequency range. Hence, in MFSC, the temporal invariance is defined by the window length (φ) fixed to 20 ms. In CWT, we applied a similar averaging of computed CWT coefficients by a 20 ms window causing a varying time-invariance when the CWT basis is greater than the window length and fixed time-invariance otherwise. The results show that the combination of varying and fixed time-invariance leads to slight improvement across most emotion-database pairs. Additionally, increased time-resolution at high frequencies in constant-Q filterbank response contributes less to SER. Rather, capturing long temporal information is a better approach. The long time-scale information alongside the translation invariant feature learning capability of deep networks (obtained from convolutional layers, statistics pooling layer, etc.) improved SER performance in our experiments.\n\nAlthough our CWT implementation is very similar to the first layer scattering coefficients  [62]  (Section 4), unlike the former, the scattering coefficients provide fixed  time-invariance over the complete frequency range. In a separate study, we found that our CWT implementation also performs better than the first layer scattering coefficients for SER  [89] .",
      "page_start": 31,
      "page_end": 32
    },
    {
      "section_name": "Complexity Analysis",
      "text": "To compare the complexity of different features, we calculated the floating-point operations (FLOPs) and the time required to compute the features. Table  6  reports features' FLOPs and computation time averaged across 100 runs of feature extraction of a randomly chosen one-second speech sample. The FLOPs count estimates the number of mathematical operations (additions, subtractions, multiplications, and divisions) required to compute the feature. We calculated FLOPs during feature extraction using only one CPU core (Intel Xeon E5-2670 2.6 GHz) and Linux perf 10  command. The",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "Conclusion",
      "text": "This paper presented the role of constant-Q filterbank based time-frequency representations, namely CQT and CWT, for SER and compared them with the traditional mel-scaled representation. Our analysis expounded the emotion-relevant advantages provided by the former representations in both time and frequency domains. The comparison with the latter also showed that the greater emphasis provided by the constant-Q non-linearity over low-frequency regions accounts for better suitability of constant-Q representations for SER. The superiority remained consistent across different neural network architectures. From the experiments and analysis, we conclude the following:\n\n1. Constant-Q filterbank representation provides higher time-invariance and increased frequency resolution at low speech-frequencies, causing an improved SER performance compared to mel-scale based representation. 2. In the time-domain, CQT provides time-invariance increasing towards low frequencies leading to robustness against emotion irrelevant temporal variations and eventually better emotion prediction. 3. The CWT implementation bears a combination of varying and fixed time-invariance over different frequency bins and offers an advantage similar to CQT in SER performance. The difference in performance between the two is attributed to the fixed time-invariance in CWT at mid and high frequencies. 4. Like mel-scale representation, constant-Q representations also provide stability to time-warp deformations begetting a robust descriptor in terms of variations due to different speaking styles. 5. In the frequency domain, constant-Q filterbank representations are more efficient in resolving pitch harmonics than mel-scaled representations contributing to a better SER performance because of the higher relevance of pitch in emotion prediction. 6. Constant-Q representations outperform mel-scale representations over multiple neural network architectures.\n\n7. Studies in psychology show better emotion perception abilities of musicians than non-musicians  [58, 59] . As CQT is a more appropriate representation for music analysis  [43] , better SER capabilities of CQT over MFSC suggest some linkage between human emotion and music perception.\n\nAlthough there was a noticeable improvement with constant-Q filterbank representations, there is a need for further effort to develop a deployable real-world SER system. Table  6  shows approximately 60% and 10% more computation time for CQT and CWT features, respectively, compared with MFSC. This increase is the cost for better performance calling for further exploration of time-frequency representations for SER tasks. Future directions of this work include cross-corpora evaluation to study the generalization ability of constant-Q representation with out-of-domain training data. The SER datasets are small-scale datasets that limit the investigations of large-scale deep architectures for this task. The constant-Q representation can be examined in a transfer learning framework which involves learning a pretext task first on a large dataset followed by training downstream emotion recognition task on the limited size dataset.",
      "page_start": 34,
      "page_end": 34
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: MFSC and CQT plots of Angry and Sad utterances of Sentence a05 by Speaker 09 from EmoDB",
      "page": 5
    },
    {
      "caption": "Figure 1: compares CQT and MFSC time-",
      "page": 6
    },
    {
      "caption": "Figure 2: CQT F-ratio for diﬀerent frequency bins over EmoDB database.",
      "page": 7
    },
    {
      "caption": "Figure 3: Spectrogram F-ratio for diﬀerent frequency bins over EmoDB database.",
      "page": 8
    },
    {
      "caption": "Figure 2: and 5 show the F-ratio statistic of",
      "page": 8
    },
    {
      "caption": "Figure 4: CWT F-ratio for diﬀerent frequency bins over EmoDB database.",
      "page": 9
    },
    {
      "caption": "Figure 3: shows the F-ratio plot for STFT (short-time Fourier transform)",
      "page": 9
    },
    {
      "caption": "Figure 5: MFSC F-ratio for diﬀerent frequency bins over EmoDB database.",
      "page": 10
    },
    {
      "caption": "Figure 4: shows the F-ratio plot of frequency bins of CWT for diﬀerent emotion classes in EmoDB",
      "page": 10
    },
    {
      "caption": "Figure 5: throughout the complete frequency range further indicates the inferiority of MFSC to",
      "page": 11
    },
    {
      "caption": "Figure 6: Block diagrams showing time-domain formulation of diﬀerent features used in this work. Here,",
      "page": 13
    },
    {
      "caption": "Figure 7: This is in contrast with the ﬁxed",
      "page": 14
    },
    {
      "caption": "Figure 6: shows diﬀerent",
      "page": 14
    },
    {
      "caption": "Figure 8: explains the eﬀects of deformations on STFT and CQT and the corresponding",
      "page": 14
    },
    {
      "caption": "Figure 7: The frequency varying time invariance in CQT. Due to higher value of Nk, smearing in time",
      "page": 15
    },
    {
      "caption": "Figure 6: describes the computation of CWT. As the",
      "page": 15
    },
    {
      "caption": "Figure 8: Comparison between deformation stability of STFT, CQT, and CQT with linear frequency scale.",
      "page": 16
    },
    {
      "caption": "Figure 9: describes this phenomenon. This also helps CQT (or constant-",
      "page": 16
    },
    {
      "caption": "Figure 10: shows the obtained representations. We observe",
      "page": 16
    },
    {
      "caption": "Figure 9: Time-frequency representation of signal consisting of 100 Hz, 250 Hz, 500 Hz, and 700 Hz",
      "page": 17
    },
    {
      "caption": "Figure 10: Pitch and ﬁrst pitch harmonic of Female and Male speakers for various emotions in EmoDB.",
      "page": 18
    },
    {
      "caption": "Figure 11: CQT parameter comparison on EmoDB database. The left subplot shows the change in accuracy",
      "page": 24
    },
    {
      "caption": "Figure 11: ). In CWT, we used scale values obtained from the expression 2",
      "page": 24
    },
    {
      "caption": "Figure 12: We kept the Angry embeddings on top and considered the positions of other",
      "page": 25
    },
    {
      "caption": "Figure 13: provides the",
      "page": 25
    },
    {
      "caption": "Figure 12: t-SNE plots of CQT and MFSC embeddings for diﬀerent speakers of EmoDB. The embeddings",
      "page": 26
    },
    {
      "caption": "Figure 13: provides a visual representation of the values in this table.",
      "page": 27
    },
    {
      "caption": "Figure 13: Bar graph showing accuracy and UAR obtained with diﬀerent features, neural network archi-",
      "page": 28
    },
    {
      "caption": "Figure 14: shows the comparison of emotion-wise accuracy obtained with diﬀerent fea-",
      "page": 28
    },
    {
      "caption": "Figure 14: Emotion-wise performance comparison among CQT, CWT, and MFSC on EmoDB database.",
      "page": 29
    },
    {
      "caption": "Figure 14: shows that Anger,",
      "page": 29
    },
    {
      "caption": "Figure 10: ) thereby enhancing emo-",
      "page": 31
    },
    {
      "caption": "Figure 13: , and Fig. 14). Regarding the",
      "page": 31
    },
    {
      "caption": "Figure 15: The low-frequency CQT basis functions (bases with center frequency ≤262 Hz) have a",
      "page": 31
    },
    {
      "caption": "Figure 15: Time invariance provided by CQT, CWT, and MFSC with changing frequency. y-axis: the",
      "page": 32
    }
  ],
  "tables": [
    {
      "caption": "Table 3: shows the values of the arguments of the built-in feature extraction func-",
      "data": [
        {
          "Hop len:64": "Hop len:128"
        },
        {
          "Hop len:64": "Hop len:192"
        }
      ],
      "page": 24
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akçay",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: A review",
      "authors": [
        "S Krothapalli",
        "S Koolagudi"
      ],
      "year": "2013",
      "venue": "Emotion Recognition using Speech Features"
    },
    {
      "citation_id": "3",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Affective computing: challenges",
      "authors": [
        "R Picard"
      ],
      "year": "2003",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "6",
      "title": "Gender and culture differences in emotion",
      "authors": [
        "A Fischer",
        "P Rodriguez Mosquera",
        "A Van Vianen",
        "A Manstead"
      ],
      "year": "2004",
      "venue": "Emotion"
    },
    {
      "citation_id": "7",
      "title": "Vocal emotion recognition across disparate cultures",
      "authors": [
        "G Bryant",
        "H Barrett"
      ],
      "year": "2008",
      "venue": "Journal of Cognition and Culture"
    },
    {
      "citation_id": "8",
      "title": "Cultural differences in emotion: Differences in emotional arousal level between the East and the West",
      "authors": [
        "N Lim"
      ],
      "year": "2016",
      "venue": "Integrative Medicine Research"
    },
    {
      "citation_id": "9",
      "title": "Towards a standard set of acoustic features for the processing of emotion in speech",
      "authors": [
        "F Eyben",
        "A Batliner",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proc. Meetings on Acoustics"
    },
    {
      "citation_id": "10",
      "title": "The geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan",
        "K Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition: Features and classification models",
      "authors": [
        "L Chen",
        "X Mao",
        "Y Xue",
        "L Cheng"
      ],
      "year": "2012",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition using both spectral and prosodic features",
      "authors": [
        "Y Zhou",
        "Y Sun",
        "J Zhang",
        "Y Yan"
      ],
      "year": "2009",
      "venue": "Proc. International Conference on Information Engineering and Computer Science"
    },
    {
      "citation_id": "13",
      "title": "Whodunnit-searching for the most important feature types signalling emotion-related user states in speech",
      "authors": [
        "A Batliner",
        "S Steidl",
        "B Schuller",
        "D Seppi",
        "T Vogt",
        "J Wagner",
        "L Devillers",
        "L Vidrascu",
        "V Aharonson",
        "L Kessous"
      ],
      "year": "2011",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "14",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Characterizing types of convolution in deep convolutional recurrent neural networks for robust speech emotion recognition",
      "authors": [
        "C.-W Huang",
        "S Narayanan"
      ],
      "year": "2017",
      "venue": "Characterizing types of convolution in deep convolutional recurrent neural networks for robust speech emotion recognition",
      "arxiv": "arXiv:1706.02901"
    },
    {
      "citation_id": "16",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "The mythos of model interpretability",
      "authors": [
        "Z Lipton"
      ],
      "year": "2018",
      "venue": "The mythos of model interpretability"
    },
    {
      "citation_id": "19",
      "title": "Deep learning is robust to massive label noise",
      "authors": [
        "D Rolnick",
        "A Veit",
        "S Belongie",
        "N Shavit"
      ],
      "year": "2017",
      "venue": "Deep learning is robust to massive label noise",
      "arxiv": "arXiv:1705.10694"
    },
    {
      "citation_id": "20",
      "title": "Analysis of deep learning architectures for cross-corpus speech emotion recognition",
      "authors": [
        "J Parry",
        "D Palaz",
        "G Clarke",
        "P Lecomte",
        "R Mead",
        "M Berger",
        "G Hofer"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "21",
      "title": "Recognizing emotion in speech",
      "authors": [
        "F Dellaert",
        "T Polzin",
        "A Waibel"
      ],
      "year": "1996",
      "venue": "Proc. ICSLP"
    },
    {
      "citation_id": "22",
      "title": "Approaching automatic recognition of emotion from voice: A rough benchmark",
      "authors": [
        "S Mcgilloway",
        "R Cowie",
        "E Douglas-Cowie",
        "S Gielen",
        "M Westerdijk",
        "S Stroeve"
      ],
      "year": "2000",
      "venue": "Proc. ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion"
    },
    {
      "citation_id": "23",
      "title": "A comparative study of traditional and newly proposed features for recognition of speech under stress",
      "authors": [
        "S Bou-Ghazale",
        "J Hansen"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Speech and Audio Processing"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion recognition using hidden Markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "25",
      "title": "Class-level spectral features for emotion recognition",
      "authors": [
        "D Bitouk",
        "R Verma",
        "A Nenkova"
      ],
      "year": "2010",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "26",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "T Falk",
        "W.-Y Chan"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition using Fourier parameters",
      "authors": [
        "K Wang",
        "N An",
        "B Li",
        "Y Zhang",
        "L Li"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "29",
      "title": "An end-to-end deep learning framework for speech emotion recognition of atypical individuals",
      "authors": [
        "D Tang",
        "J Zeng",
        "M Li"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "30",
      "title": "Representation learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "E Laksana",
        "L.-P Morency",
        "S Scherer"
      ],
      "year": "2016",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "31",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "32",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "33",
      "title": "Advanced time-frequency representation in voice signal analysis, Advances in Science and",
      "authors": [
        "D Mika",
        "J Józwik"
      ],
      "year": "2018",
      "venue": "Technology Research Journal"
    },
    {
      "citation_id": "34",
      "title": "Emotions and speech: Some acoustical correlates",
      "authors": [
        "C Williams",
        "K Stevens"
      ],
      "year": "1972",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "35",
      "title": "Acoustic profiles in vocal emotion expression",
      "authors": [
        "R Banse",
        "K Scherer"
      ],
      "year": "1996",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "36",
      "title": "Automatic statistical analysis of the signal and prosodic signs of emotion in speech",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie"
      ],
      "year": "1996",
      "venue": "Proc. ICSLP"
    },
    {
      "citation_id": "37",
      "title": "Multiscale amplitude feature and significance of enhanced vocal tract information for emotion classification",
      "authors": [
        "S Deb",
        "S Dandapat"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "38",
      "title": "Acoustical properties of speech as indicators of depression and suicidal risk",
      "authors": [
        "D France",
        "R Shiavi",
        "S Silverman",
        "M Silverman",
        "M Wilkes"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "39",
      "title": "Emotion dimensions and formant position",
      "authors": [
        "M Goudbeek",
        "J Goldman",
        "K Scherer"
      ],
      "year": "2009",
      "venue": "Proc. INTERPSEECH"
    },
    {
      "citation_id": "40",
      "title": "Formant position based weighted spectral features for emotion recognition",
      "authors": [
        "E Bozkurt",
        "E Erzin",
        "C Erdem",
        "A Erdem"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "41",
      "title": "Amplitude-frequency analysis of emotional speech using transfer learning and classification of spectrogram images",
      "authors": [
        "M Lech",
        "M Stolar",
        "R Bolia",
        "M Skinner"
      ],
      "year": "2018",
      "venue": "Technology and Engineering Systems Journal"
    },
    {
      "citation_id": "42",
      "title": "Investigation of different time-frequency representations for intelligibility assessment of dysarthric speech",
      "authors": [
        "H Chandrashekar",
        "V Karjigi",
        "N Sreedevi"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "43",
      "title": "Calculation of a constant Q spectral transform",
      "authors": [
        "J Brown"
      ],
      "year": "1991",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "44",
      "title": "Constant Q cepstral coefficients: A spoofing countermeasure for automatic speaker verification",
      "authors": [
        "M Todisco",
        "H Delgado",
        "N Evans"
      ],
      "year": "2017",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "45",
      "title": "Synthetic speech detection using fundamental frequency variation and spectral features",
      "authors": [
        "M Pal",
        "D Paul",
        "G Saha"
      ],
      "year": "2018",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "46",
      "title": "Further optimisations of constant Q cepstral processing for integrated utterance and text-dependent speaker verification",
      "authors": [
        "H Delgado"
      ],
      "year": "2016",
      "venue": "Proc. IEEE SLT"
    },
    {
      "citation_id": "47",
      "title": "CQT-based convolutional neural networks for audio scene classification",
      "authors": [
        "T Lidy",
        "A Schindler"
      ],
      "year": "2016",
      "venue": "Proc. Detection and Classification of Acoustic Scenes and Events 2016 Workshop"
    },
    {
      "citation_id": "48",
      "title": "Classification of audio scenes with novel features in a fused system framework",
      "authors": [
        "S Waldekar",
        "G Saha"
      ],
      "year": "2018",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "49",
      "title": "Wavelets and signal processing",
      "authors": [
        "O Rioul",
        "M Vetterli"
      ],
      "year": "1991",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "50",
      "title": "Extraction of adaptive wavelet packet filter-bank-based acoustic feature for speech emotion recognition",
      "authors": [
        "Y Huang",
        "A Wu",
        "G Zhang",
        "Y Li"
      ],
      "year": "2015",
      "venue": "IET Signal Processing"
    },
    {
      "citation_id": "51",
      "title": "Modeling the temporal evolution of acoustic parameters for speech emotion recognition",
      "authors": [
        "S Ntalampiras",
        "N Fakotakis"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "52",
      "title": "Time-frequency feature representation using multi-resolution texture analysis and acoustic activity detector for real-life speech emotion recognition",
      "authors": [
        "K.-C Wang"
      ],
      "year": "2015",
      "venue": "Sensors"
    },
    {
      "citation_id": "53",
      "title": "Continuous wavelet transform based speech emotion recognition",
      "authors": [
        "P Shegokar",
        "P Sircar"
      ],
      "year": "2016",
      "venue": "Proc. ICSPCS"
    },
    {
      "citation_id": "54",
      "title": "Non-linear frequency warping using constant-Q transformation for speech emotion recognition",
      "authors": [
        "P Singh",
        "G Saha",
        "M Sahidullah"
      ],
      "venue": "Proc. International Conference on Computer Communication and Informatics"
    },
    {
      "citation_id": "55",
      "title": "Constant-Q transform toolbox for music processing",
      "authors": [
        "C Schörkhuber",
        "A Klapuri"
      ],
      "year": "2010",
      "venue": "Proc. 7th Sound and Music Computing Conference"
    },
    {
      "citation_id": "56",
      "title": "Improving anti-spoofing with octave spectrum and short-term spectral statistics information",
      "authors": [
        "J Yang",
        "R Das"
      ],
      "year": "2020",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "57",
      "title": "Evaluating feature set performance using the F-ratio and Jmeasures",
      "authors": [
        "S Nicholson",
        "B Milner",
        "S Cox"
      ],
      "year": "1997",
      "venue": "Proc. EUROSPEECH"
    },
    {
      "citation_id": "58",
      "title": "Speaking to the trained ear: Musical expertise enhances the recognition of emotions in speech prosody",
      "authors": [
        "C Lima",
        "S Castro"
      ],
      "year": "2011",
      "venue": "Emotion"
    },
    {
      "citation_id": "59",
      "title": "Benefits of music training for perception of emotional speech prosody in deaf children with cochlear implants",
      "authors": [
        "A Good",
        "K Gordon",
        "B Papsin",
        "G Nespoli",
        "T Hopyan",
        "I Peretz",
        "F Russo"
      ],
      "year": "2017",
      "venue": "Ear Hear"
    },
    {
      "citation_id": "60",
      "title": "Wavelets and signal processing",
      "authors": [
        "O Rioul",
        "M Vetterli"
      ],
      "year": "1991",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "61",
      "title": "Design, analysis and experimental evaluation of block based transformation in MFCC computation for speaker recognition",
      "authors": [
        "M Sahidullah",
        "G Saha"
      ],
      "year": "2012",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "62",
      "title": "Deep scattering spectrum",
      "authors": [
        "J Andén",
        "S Mallat"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "63",
      "title": "An investigation of dependencies between frequency components and speaker characteristics for text-independent speaker identification",
      "authors": [
        "X Lu",
        "J Dang"
      ],
      "year": "2008",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "64",
      "title": "Optimization of data-driven filterbank for automatic speaker verification",
      "authors": [
        "S Sarangi",
        "M Sahidullah",
        "G Saha"
      ],
      "year": "2020",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "65",
      "title": "Speech emotion classification using attention-based LSTM",
      "authors": [
        "Y Xie",
        "R Liang",
        "Z Liang",
        "C Huang",
        "C Zou",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "66",
      "title": "CTNet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "67",
      "title": "ECAPA-TDNN embeddings for speaker diarization",
      "authors": [
        "N Dawalatabad",
        "M Ravanelli",
        "F Grondin",
        "J Thienpondt",
        "B Desplanques",
        "H Na"
      ],
      "venue": "Proc. INTERSPEECH, 2021"
    },
    {
      "citation_id": "68",
      "title": "ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in TDNN based speaker verification",
      "authors": [
        "B Desplanques",
        "J Thienpondt",
        "K Demuynck"
      ],
      "year": "2020",
      "venue": "ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in TDNN based speaker verification"
    },
    {
      "citation_id": "69",
      "title": "X-vectors: Robust DNN embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "70",
      "title": "3-D convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "71",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "72",
      "title": "The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS One"
    },
    {
      "citation_id": "73",
      "title": "The eNTERFACE'05 audio-visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "Proc. International Conference on Data Engineering Workshops"
    },
    {
      "citation_id": "74",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "75",
      "title": "Investigation on joint representation learning for robust feature extraction in speech emotion recognition",
      "authors": [
        "D Luo",
        "Y Zou",
        "D Huang"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "76",
      "title": "Speaker independent speech emotion recognition by ensemble classification",
      "authors": [
        "B Schuller",
        "S Reiter",
        "R Muller",
        "M Al-Hames",
        "M Lang",
        "G Rigoll"
      ],
      "year": "2005",
      "venue": "Proc. International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "77",
      "title": "Acoustic emotion recognition: A benchmark comparison of performances",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "G Rigoll",
        "A Wendemuth"
      ],
      "year": "2009",
      "venue": "Proc. Workshop on Automatic Speech Recognition Understanding"
    },
    {
      "citation_id": "78",
      "title": "Classifying skewed data: Importance weighting to optimize average recall",
      "authors": [
        "A Rosenberg"
      ],
      "year": "2012",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "79",
      "title": "Towards robust speech emotion recognition using deep residual networks for speech enhancement",
      "authors": [
        "A Triantafyllopoulos",
        "G Keren",
        "J Wagner",
        "I Steiner",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "80",
      "title": "Emotion recognition in low-resource settings: An evaluation of automatic feature selection methods",
      "authors": [
        "F Haider",
        "S Pollak",
        "P Albert",
        "S Luz"
      ],
      "year": "2021",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "81",
      "title": "Multi-time-scale convolution for emotion recognition from speech audio signals",
      "authors": [
        "E Guizzo",
        "T Weyde",
        "J Leveson"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "82",
      "title": "Speech emotion recognition 'in the wild' using an autoencoder",
      "authors": [
        "V Dissanayake",
        "H Zhang",
        "M Billinghurst",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "83",
      "title": "Multi-modal sequence fusion via recursive attention for emotion recognition",
      "authors": [
        "R Beard",
        "R Das",
        "R Ng",
        "P Gopalakrishnan",
        "L Eerens",
        "P Swietojanski",
        "O Miksik"
      ],
      "year": "2018",
      "venue": "Proc. Conference on Computational Natural Language Learning"
    },
    {
      "citation_id": "84",
      "title": "Attention gated tensor neural network architectures for speech emotion recognition",
      "authors": [
        "S Pandey",
        "H Shekhawat",
        "S Prasanna"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "85",
      "title": "Applying TDNN architectures for analyzing duration dependencies on speech emotion recognition",
      "authors": [
        "P Kumawat",
        "A Routray"
      ],
      "year": "2021",
      "venue": "Proc. INTERSPEECH 2021"
    },
    {
      "citation_id": "86",
      "title": "What do classifiers actually learn? a case study on emotion recognition datasets",
      "authors": [
        "P Meyer",
        "E Buschermöhle",
        "T Fingscheidt"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "87",
      "title": "The INTERSPEECH 2016 computational paralinguistics challenge: Deception, sincerity & native language",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "J Hirschberg",
        "J Burgoon",
        "A Baird",
        "A Elkins",
        "Y Zhang",
        "E Coutinho",
        "K Evanini"
      ],
      "year": "2016",
      "venue": "Proc. INTERPSEECH"
    },
    {
      "citation_id": "88",
      "title": "The role of F0 and formant frequencies in distinguishing the voices of men and women, Attention, Perception",
      "authors": [
        "J Hillenbrand",
        "M Clark"
      ],
      "year": "2009",
      "venue": "& Psychophysics"
    },
    {
      "citation_id": "89",
      "title": "Deep scattering network for speech emotion recognition",
      "authors": [
        "P Singh",
        "G Saha",
        "M Sahidullah"
      ],
      "venue": "Proc. EUSIPCO, 2021"
    }
  ]
}