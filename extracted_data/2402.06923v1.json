{
  "paper_id": "2402.06923v1",
  "title": "Cochceps-Augment: A Novel Self-Supervised Contrastive Learning Using Cochlear Cepstrum-Based Masking For Speech Emotion Recognition",
  "published": "2024-02-10T11:13:13Z",
  "authors": [
    "Ioannis Ziogas",
    "Hessa Alfalahi",
    "Ahsan H. Khandoker",
    "Leontios J. Hadjileontiadis"
  ],
  "keywords": [
    "CochCeps-Augment",
    "Self-Supervised Learning",
    "Contrastive Learning",
    "SimCLR",
    "Cochlear Cepstrum",
    "Cepstral Augmentation",
    "Bio-inspired SSL",
    "Speech Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Self-supervised learning (SSL) for automated speech recognition in terms of its emotional content, can be heavily degraded by the presence noise, affecting the efficiency of modeling the intricate temporal and spectral informative structures of speech. Recently, SSL on large speech datasets, as well as new audio-specific SSL proxy tasks, such as, temporal and frequency masking, have emerged, yielding superior performance compared to classic approaches drawn from the image augmentation domain. Our proposed contribution builds upon this successful paradigm by introducing CochCeps-Augment, a novel bio-inspired masking augmentation task for self-supervised contrastive learning of speech representations. Specifically, we utilize the newly introduced bio-inspired cochlear cepstrogram (CCGRAM) to derive noise robust representations of input speech, that are then further refined through a self-supervised learning scheme. The latter employs SimCLR to generate contrastive views of a CCGRAM through masking of its angle and quefrency dimensions. Our experimental approach and validations on the emotion recognition K-EmoCon benchmark dataset, for the first time via a speaker-independent approach, features unsupervised pre-training, linear probing and fine-tuning. Our results potentiate CochCeps-Augment to serve as a standard tool in speech emotion recognition analysis, showing the added value of incorporating bio-inspired masking as an informative augmentation task for self-supervision. Our code for implementing CochCeps-Augment will be made available at: https://github.com/GiannisZgs/CochCepsAugment.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Retrieving information implicitly from spoken language and natural sounds has been believed to constitute one of the core learning mechanisms during development in humans, who essentially form a perception of their acoustic surroundings based on salient acoustic representations  [1] . Similarly, advances in neural networks have formed the yet nascent, though promising, Self-Supervised representation Learning (SSL) field, in an effort to mimic human acquisition of knowledge that does not rely on explicitly taught or labeled paradigms  [1] . Contrastive learning, a sub-category of SSL, leverages simple augmentations to generate multiple views of a sample, in order to foster invariance, and then encourage similarity between their feature representations  [1] .\n\nIn the area of speech and audio processing, SSL provides a compelling paradigm that can mitigate the lack of sufficiently large and well-annotated datasets  [2]  , hence facilitating the learning of intelligible speech features that enhance the performance on a multitude of downstream tasks. SSL concepts from computer vision and natural language processing such as masking  [3] ,  [4]  have been extended to audio processing, and have successfully exploited vast unlabeled speech datasets to create large, pre-trainedon-speech models such as, wav2vec2.0  [5]  and HuBERT  [6] . However, SSL for audio has limitations that relate to the temporal structure of time series, as well as the heavy noise contamination usually present in audio  [7] , factors that limit the application of classic contrastive image augmentations to audio or audio-derived representations, such as, spectrograms and MFCCs. To that end, audio-tailored augmentation methods that are based on masking have been designed, such as, SpecAugment  [8]  and Mask Spec  [2]  and have been successfully applied for contrastive learning in audio data  [9] .\n\nAmong the recently evolving tasks are speech emotion recognition (SER); which currently plays a huge role in Human-Computer Interaction (HCI)  [10] , as well as various healthcare  [11]  and elearning paradigms in education applications  [12] . SER constitutes a sub-field of automatic speech recognition that benefits from feature representations of the raw audio such as spectrograms or MFCCs  [13] ,  [14] . SSL masking-based approaches in particular, have proven to be very effective in the SER paradigm, yet large pretrained models on generic speech datasets are preferred, as these representations have been proven to be beneficial for the task of SER  [15] .\n\nWe believe that bio-inspired signal representation methods could guide contrastive learning methods to learn noise-robust intelligible features, thereby enhancing the generalization ability and efficacy of the model for the problem at hand (i.e., SER). To the best of our knowledge, this is the first work that attempts to enhance synergies between machine intelligence and human perception by adopting a bio-inspired cochlear cepstral representation of speech   [16]  and GammaTone filterbank Cepstral Coefficients (GTCCs)  [17] , we adopt the Cochlear Filterbank Cepstral Coefficients (CFCCs), also referred to as cochlear cepstrogram (CCGRAM), that mimic both the function and the structure of the human cochlea  [18] . The human cochlea is characterized by a spiral structure that encodes the frequency-position map; which is the essence of the superb frequency resolution of the human ear. In particular, the cochlear spiral is geometrically composed of a bit more than two and a half turns spanning θ = 0 • at the base (high frequency hearing, up to 20 kHz) of the cochlea to θ = 990 • at the apex (low frequency hearing, up to 10 Hz) of the cochlea  [19] .\n\nIn light of the aforementioned, the contribution of our work is a novel augmentation method which we call CochCeps-Augment. Our method draws inspiration from SpecAugment  [8] , however, in contrast to SpecAugment, our method operates on the image representation of the CCGRAM of the input audio by applying masking along the angle and quefrency axis, therefore encouraging self-supervision to attend to meaningful tonotopically-organized audio properties that are intelligible to humans. CochCeps-Augment is simple and cost-effective to be applied during self-supervised pre-training and due to the bio-inspired nature of the CFCCs  [18] , exhibits enhanced noise robustness which is highly desirable in audio analysis. The results of applying CochCeps-Augment on the K-EmoCon dataset  [20]  for the first time in a speaker-independent manner, via a self-supervised contrastive learning scheme, showcase the potentiality of our bio-inspired masking augmentation task. We believe that CochCeps-Augment will not only enhance SER tasks, but will also likely unlock new avenues for various applications in speech and acoustic signal processing; especially by blending human auditory mechanisms into SSL. An overview of our method is presented in Figure  1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Cochlear Cepstrum Background",
      "text": "The human ear shows remarkable ability in recognizing speech content under variable conditions of background noise, thereby inspiring a range of signal processing and representation methods  [21] ,  [22] . We employed a recently proposed bio-inspired, noiserobust feature space, called Cochlear Filterbank Cepstral Coefficients (CFCCs)  [18] . Such CFCCs replicate both the structure and the function of the human spiral cochlea and are based on the concept of the cochlear transform (CT)  [22] . The latter is briefly a novel and general signal processing framework that mimics the active and non-linear multiscale analysis of the cochlea for acoustic Hence, for the computation of the CFCCs of a signal p(t), we start by computing the orthogonal cochlear modes (in the frequency domain), denoted F CT that result from the CT, as follows:\n\nwhere θ is the angle along the spiral cochlear space; namely θ ∈\n\nIt should be noted that the tonotopic place-pitch map is defined as follows:\n\nwhere f is the frequency and θ is the angular position across the spiral cochlea. It should be noted that θ = 0 • corresponds to the base (High frequency region) and θ = 990 • corresponds to the apex of the cochlea (Low frequency region). Afterwards, we compute the log magnitude spectrum and the DCT to extract the tonotopically (spatially) organized cochlear cepstral coefficients. By definition, the real CFCCs at a specific angular position θ, is:\n\nwhere CF CC(m, θ) is the mth cochlear cepstral coefficient at angle θ and m is the cochlear quefrency index, 1 ≤ m ≤ M , K is the number of cochlear modes and X k is the energy of the kth cochlear mode (i.e., F CTp(θ)).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Method Iii-A. Cepstral Masking Augmentation Policy",
      "text": "To enhance the expressivity of the CCGRAM features in modelling speech in a SSL setting, we design our masking augmentations to operate along both the angle θ and quefrency axis m. In doing so, we encourage self-supervision to infer the masked frequency tones arising from particular angular positions along the cochlea and masked speech segments along the time domain, both individually and in a combined fashion. Therefore, in a given sample with two cepstral-augmented views, the model should be able to predict the missing tones given the presence of other, contextually and acoustically relevant tones. Hence, it becomes evident that the proposed CochCeps-Augment proxy task, is relevant both for generic speech and audio processing, as well as in the context of SER. This leads us to the following three augmentation transforms:\n\n1) Angle masking, applied along the angle θ axis, so that Φ distinct masks are applied. Each mask spans ϕ consecutive angle bands, i.e. [θ0, θ0 + ϕ), where ϕ is sampled randomly from a uniform distribution from 0 to Φ, and θ0 is a random angle sampled from [0, 990 -ϕ). 2) Quefrency masking, applied along the quefrency indexes m, so that Q distinct masks are applied. Each mask spans Q consecutive quefrency indexes, i.e. [m0, m0 + q), where q is sampled randomly from a uniform distibution from 0 to Q, and m0 is chosen from the [0, M -m). 3) Cepstral masking, defined as the simultaneous masking of both angle bands and quefrency indexes according to the above parameters. Figure  2  shows an example of the application of our proposed augmentations to an input CCGRAM.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii-B. Simclr -Self-Supervised Contrastive Learning Of Representations",
      "text": "To take advantage of the masking benefits that CochCeps-Augment adds to the learning process of a SSL system, we adopt SimCLR as our pre-training framework  [23] . In SimCLR, two augmentation transforms (t ∼ T and t ′ ∼ T ) are sampled from the family of the proposed CochCeps-Augment transforms T . SimCLR promotes similarity between two augmented views of an input sample, designed to maintain invariance on information axes that are deemed redundant. Specifically, meaningful representations can be learned by maximizing the similarity between positive pairs (views of the same sample) and minimizing the similarity between negative pairs (views belonging to different samples) in a latent space. To that end, every sample CCGRAM x is masked twice in order to produce two distinct augmented views, xi and xj. By forwardpropagating the two views through a shared encoder f (.), two representations hi and hj are obtained, which are further projected through a projector g(.) to obtain the final representations zi and zj.\n\nThe self-supervision without labels in SimCLR, is facilitated by the NT-Xent (normalized temperature-scaled cross-entropy) loss function, which is given by:\n\nwhere zi and zj are the representations of augmented views i and j, sim(zi, zj) = z i •z j ∥z i ∥∥z j ∥ is the cosine similarity, τ is a temperature parameter, I[•] is the indicator function and N are the number of samples in a batch.\n\nIn the pre-training phase, the encoder f (.) and the projector g(.) are trained end-to-end and evaluated through the NT-Xent loss. In Table  I . Quadrant Annotations: Arousal (a) /Valence (v) values are binned to one of the four quadrants of the A/V space.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Arousal Valence",
      "text": "downstream evaluations, the projector is discarded and the encoder f (.) is used as a feature extractor that yields the features hi and hj.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experiments Iv-A. K-Emocon Database",
      "text": "The K-EmoCon database contains multi-modal recordings from 32 participants divided in 16 groups  [20] . Participants engaged in a dyadic debate in English ( 10 minutes) during which multimodal data acquisition took place. For emotional labeling, we adopt the self-rated arousal/valence (A/V) space quadrant scheme (see Table  I ), where the provided integer ratings in the range  [1, 5] , are binned to a specific quadrant based on their combined arousal and valence, yielding four classes: LowA/LowV (LALV), LowA/HighV (LAHV), HighA/LowV (HALV), HighA/HighV(HAHV).\n\nData collection in K-EmoCon was modelled as a natural conversational setting, hence various sources of noise are present. Moreover, the problem of source separation needs to be addressed in speech segments where participants and the referee's voices overlap, as well as when heavy degradation occurs due to loud, nonconversation-related sounds. To that end, we design a combined pre-processing strategy which is mainly focused on source separation to obtain speaker-independent speech segments. The raw audio is first downsampled from the initial sampling rate 22.5 kHz to 16kHz, followed by max scaling. Silent segments are then removed with a Short-Time Fourier Transform energy thresholding  [24] . We manually annotate speech segments identified as sound and based on their content, we apply one of two source separation techniques. For stationary noise, we apply the WTST-NST filter  [25]  to isolate the non-stationary speech. For speaker separation, we utilize a pretrained on the Librimix dataset Sepformer model that outputs two speaker signals  [26] . Finally, resulting speech segments are scaled again, and segmented into 3 second windows. To ensure uniform input to our deep learning models, we zero-pad segments that are shorter than 3s, and discard segments that are shorter than 1s.\n\nWe compute the CCGRAM of an input 3-second segment according to  [18] , using Hamming windows of 25ms duration with 50% overlap. We set the angle spacing between cochlear modes at θ = 45 • , resulting in CCGRAMs of size 20x239.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv-B. Self-Supervised Pre-Training",
      "text": "Prior to any processing, we split the data into 4 distinct folds: pre-training (26 speakers), validation (2 speakers), fine-tuning (2 speakers) and test (2 speakers). For SSL pre-training we use the pre-training and validation folds, while for evaluation we use the pre-training, validation and test folds in the linear probing, and the fine-tuning, validation and test folds in the fine-tuning. We follow a speaker-independent approach, that relies on a 5-fold cross-validation scheme of K-EmoCon. Specifically, we design the folds in order to ensure that all speakers are present once at each fold, and that all folds contain only whole speakers.\n\nFor our augmentation pipeline, we first z-normalize all images in a fold with the mean value and standard deviation of that fold. Then, we generate two views of each input CCGRAM by randomly selecting one of the three masking transforms in the family of CochCeps-Augment for each view. The parameters Φ and Q for the angle and quefrency masking are evaluated as 2 and 5, respectively. After masking, we resize the CCGRAM to a size of 239x239 with nearest-neighbors interpolation. We pre-train from scratch a ResNet18 to be used as the encoder f (.)  [27] , where we slightly modify the input convolutional layer to accept singlechannel intensity images. The projector g(.) is implemented as a two-layered linear head with an output dimension of 256. For the NT-XEnt loss, we choose a temperature parameter of 0.07. For our implementation via PyTorch, we trained on a machine with four NVIDIA RTX6000 ADA GPUs, with a batch size of 64 for 1000 epochs on each pre-train fold. The LARS optimizer was used  [28] , with a starting learning rate of 0.1 and a weight decay parameter of 1e-6. A warm-up period of 10 epochs is followed by training with the cosine decay scheduler  [29] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv-C. Evaluations",
      "text": "We evaluate the performance of our SSL approach through two schemes: linear probing and fine-tuning. It should be noted that in the evaluations, we do not apply CochCeps-Augment but only the z-normalization and resizing operations. Linear probing refers to the evaluation of the frozen encoder f (.) through a simple linear head, that is used to perform the downstream task. Linear probing constitutes a simple evaluation method and allows us to understand the pure quality of the features that the encoder learned during the pre-training phase  [1] . Moreover, to test the ability of the contrastive self-supervision to recover the CCGRAM-encoded speech information, we conduct a sanity check where the flattened CCGRAM is fed directly to the linear probe. We expect that this evaluation will reveal the information gain that our proposed pretraining approach contributes to the end result. From a technical perspective, this is attributed to the fact that flattening disrupts the spatio-temporal cochlear cepstral features. Fine-tuning on the other hand, utilizes a non-linear head for downstream classification, and refers to the classic process of fine-tuning the whole network, including the encoder. We train with linear probing on the same data that was used for the pre-training, and evaluate on the leftout test fold. For fine-tuning, we tune on the fine-tuning fold and evaluate on the test fold. On both settings, we train with a batch size of 16, for 50 epochs to avoid overfitting. The Adam optimizer is used  [30]  with a starting learning rate of 1e-4 for linear probing and 5e-6 for fine-tuning, with weight decay of 1e-6 and a cosine decay schedule. For performance evaluation, we use the weighted accuracy and weighted F1-score.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "The results of our proposed CochCeps-Augment-driven SSL approach for the SER task can be seen in Table  II . We employed two main evaluation schemes for the proposed approach; namely linear probing and fine-tuning. As illustrated in Table  II , for the linear probing task, the weighted accuracy/F1 score are 0.42/0.45 for the flattening sanity check, and 0.61/0.50 for the ResNet18 pre-trained encoder; respectively. However, the performance is significantly enhanced upon the fine-tuning stage; yielding an accuracy and F1 score of 0.69 and 0.57 respectively. This indicates that selfsupervision through CochCeps-Augment and further refinement of the learned features in the downstream speaker-independent SER task, can indeed uncover meaningful non-redundant representations.\n\nAs K-EmoCon is a small-scale emotion recognition corpus, we believe that pre-training on large-scale speech corpora and validating on more emotional speech corpora, would benefit the power of our conclusions. Moreover, longer training times have been proven beneficial for SSL feature extractors  [9]  and thus could enhance our results. It is still not clearly understood how each distinct masking scheme of CochCeps-Augment affects the learned representations; angle masking for instance, steers the learning towards perceived tonotopical relationships in an input utterance, whereas quefrency masking promotes learning of contextual links between speech segments. However, simultaneous or excess masking of angle and quefrency content may irreversibly degrade the CCGRAM and hence, the capacity of self-supervision to recover information from the perturbed CCGRAM. Finally, CochCeps-Augment is a task-agnostic SSL proxy task for speech; therefore, we hypothesize that representations learned through CochCeps-Augment could improve speech recognition systems in a variety of tasks, i.e. speaker identification, automatic speech recognition, audio events classification. Investigation towards these directions is already underways.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this work, we have presented our bio-inspired masking augmentation method, namely CochCeps-Augment, for learning selfsupervised speech representations through contrastive learning in the context of SER. We demonstrated, for the first time, how CochCeps-Augment can be seamlessly and cost-effectively integrated in a resource-demanding contrastive SSL setting through SimCLR, in the context of SER, with promising results. This novel approach showcases how human perception of sounds, encapsulated in the signal processing framework of the cochlea, can be placed in the epicentre of a speech self-supervision model, which by design tries to implicitly mimic the way humans perceive auditory events.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our proposed bio-inspired CochCeps-Augment SSL framework: (a) Cochlear Cepstrum, (b) SimCLR Contrastive pre-training",
      "page": 2
    },
    {
      "caption": "Figure 1: II. COCHLEAR CEPSTRUM BACKGROUND",
      "page": 2
    },
    {
      "caption": "Figure 2: Family of the proposed CCGRAM augmentations applied on",
      "page": 3
    },
    {
      "caption": "Figure 2: shows an example of the application of our proposed",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "LALV",
          "Arousal": "a<3",
          "Valence": "v<3"
        },
        {
          "Column_1": "LAHV",
          "Arousal": "a<3",
          "Valence": "v≥3"
        },
        {
          "Column_1": "HALV",
          "Arousal": "a≥3",
          "Valence": "v<3"
        },
        {
          "Column_1": "HAHV",
          "Arousal": "a≥3",
          "Valence": "v≥3"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Linear Probing – Flattening",
          "Weighted Accuracy": "0.42",
          "Weighted F1": "0.45"
        },
        {
          "Column_1": "Linear Probing – ResNet18",
          "Weighted Accuracy": "0.61",
          "Weighted F1": "0.50"
        },
        {
          "Column_1": "Fine-Tuning – ResNet18",
          "Weighted Accuracy": "0.69",
          "Weighted F1": "0.57"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A Cookbook of Self-Supervised Learning",
      "authors": [
        "Randall Balestriero",
        "Mark Ibrahim",
        "Vlad Sobal",
        "Ari Morcos",
        "Shashank Shekhar",
        "Tom Goldstein",
        "Florian Bordes",
        "Adrien Bardes",
        "Gregoire Mialon",
        "Yuandong Tian",
        "Avi Schwarzschild",
        "Andrew Gordon Wilson",
        "Jonas Geiping",
        "Quentin Garrido",
        "Pierre Fernandez",
        "Amir Bar",
        "Hamed Pirsiavash",
        "Yann Lecun",
        "Micah Goldblum"
      ],
      "venue": "A Cookbook of Self-Supervised Learning"
    },
    {
      "citation_id": "2",
      "title": "Masked spectrogram prediction for self-supervised audio pretraining",
      "authors": [
        "Dading Chong",
        "Helin Wang",
        "Peilin Zhou",
        "Qingcheng Zeng"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr Dollar",
        "Ross Girshick"
      ],
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "NAACL HLT 2019 -2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference"
    },
    {
      "citation_id": "5",
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "6",
      "title": "Hu-BERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": [
        "Wei Hsu",
        "Benjamin Bolte",
        "Hung Yao",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Unsupervised Pre-Training of Bidirectional Speech Encoders via Masked Reconstruction",
      "authors": [
        "Weiran Wang",
        "Qingming Tang",
        "Karen Livescu"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics"
    },
    {
      "citation_id": "8",
      "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
      "authors": [
        "Daniel Park",
        "William Chan",
        "Yu Zhang",
        "Chung-Cheng Chiu",
        "Barret Zoph",
        "Ekin Cubuk",
        "Quoc Le"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH"
    },
    {
      "citation_id": "9",
      "title": "Contrastive learning of heart and lung sounds for label-efficient diagnosis",
      "authors": [
        "Siyu Pratham N Soni",
        "Shi",
        "R Pranav",
        "Andrew Sriram",
        "Pranav Ng",
        "Rajpurkar"
      ],
      "year": "2022",
      "venue": "Patterns"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie",
        "Nicolas Tsapatsoulis",
        "George Votsis",
        "Stefanos Kollias",
        "Winfried Fellenz",
        "John Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition for healthcare surveillance systems using neural networks: A survey",
      "authors": [
        "Marwan Dhuheir",
        "Abdullatif Albaseer",
        "Emna Baccour",
        "Aiman Erbad",
        "Mohamed Abdallah",
        "Mounir Hamdi"
      ],
      "year": "2021",
      "venue": "2021 International Wireless Communications and Mobile Computing (IWCMC)"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition in e-learning system based on affective computing",
      "authors": [
        "Wu Li",
        "Yanhui Zhang",
        "Yingzi Fu"
      ],
      "year": "2007",
      "venue": "Third international conference on natural computation (ICNC 2007"
    },
    {
      "citation_id": "13",
      "title": "Speech Emotion Recognition with Co-Attention based Multi-level Acoustic Information",
      "authors": [
        "Heqing Zou",
        "Yuke Si",
        "Chen Chen",
        "Deepu Rajan",
        "Eng Siong"
      ],
      "year": "2022",
      "venue": "ICASSP, IEEE International Conference on Acoustics"
    },
    {
      "citation_id": "14",
      "title": "Multiple Acoustic Features Speech Emotion Recognition Using Cross-Attention Transformer",
      "authors": [
        "Yurun He",
        "Nobuaki Minematsu",
        "Daisuke Saito"
      ],
      "venue": "Multiple Acoustic Features Speech Emotion Recognition Using Cross-Attention Transformer"
    },
    {
      "citation_id": "15",
      "title": "Speech-Based Emotion Recognition with Self-Supervised Models Using Attentive Channel-Wise Correlations and Label Smoothing",
      "authors": [
        "Sofoklis Kakouros",
        "Themos Stafylakis",
        "Ladislav Mošner",
        "Lukáš Burget"
      ],
      "venue": "Speech-Based Emotion Recognition with Self-Supervised Models Using Attentive Channel-Wise Correlations and Label Smoothing"
    },
    {
      "citation_id": "16",
      "title": "Mfcc and prosodic feature extraction techniques: a comparative study",
      "authors": [
        "Nilu Singh",
        "Raj Khan",
        "Shree"
      ],
      "year": "2012",
      "venue": "International Journal of Computer Applications"
    },
    {
      "citation_id": "17",
      "title": "Gammatone cepstral coefficients: Biologically inspired features for non-speech audio classification",
      "authors": [
        "Xavier Valero",
        "Francesc Alias"
      ],
      "year": "2012",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "18",
      "title": "Spiral shape matters: Novel bio-inspired cochlear cepstrum",
      "authors": [
        "Hessa Alfalahi",
        "Ahsan Khandoker",
        "Leontios Hadjileontiadis"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Human cochlea: anatomical characteristics and their relevance for cochlear implantation",
      "authors": [
        "Helge Rask-Andersen",
        "Wei Liu",
        "Elsa Erixon",
        "Anders Kinnefors",
        "Kristian Pfaller",
        "Annelies Schrott-Fischer",
        "Rudolf Glueckert"
      ],
      "year": "2012",
      "venue": "The Anatomical Record: Advances in Integrative Anatomy and Evolutionary Biology"
    },
    {
      "citation_id": "20",
      "title": "K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations",
      "authors": [
        "Young Cheul",
        "Narae Park",
        "Soowon Cha",
        "Auk Kang",
        "Ahsan Kim",
        "Leontios Habib Khandoker",
        "Alice Hadjileontiadis",
        "Yong Oh",
        "Uichin Jeong",
        "Lee"
      ],
      "year": "2020",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "21",
      "title": "Cochlear decomposition: A novel bio-inspired multiscale analysis framework",
      "authors": [
        "Hessa Alfalahi",
        "Ahsan Khandoker",
        "Ghada Alhussein",
        "Leontios Hadjileontiadis"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Cochlear transform",
      "authors": [
        "Hessa Alfalahi",
        "Ahsan Khandoker",
        "Georgios Apostolidis",
        "Leontios Hadjileonitiadis"
      ],
      "year": "2023",
      "venue": "Authorea Preprints"
    },
    {
      "citation_id": "23",
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "venue": "A Simple Framework for Contrastive Learning of Visual Representations"
    },
    {
      "citation_id": "24",
      "title": "A method for silence removal and segmentation of speech signals, implemented in matlab",
      "authors": [
        "Theodoros Giannakopoulos"
      ],
      "venue": "A method for silence removal and segmentation of speech signals, implemented in matlab"
    },
    {
      "citation_id": "25",
      "title": "Enhancement of bowel sounds by wavelet-based filtering",
      "authors": [
        "J Leontios",
        "M Hadjileontiadis",
        "Theodore Rokkas",
        "Stavros Panas"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "26",
      "title": "SpeechBrain: A General-Purpose Speech Toolkit",
      "authors": [
        "Mirco Ravanelli",
        "Titouan Parcollet",
        "Peter Plantinga",
        "Aku Rouhe",
        "Samuele Cornell",
        "Loren Lugosch",
        "Cem Subakan",
        "Nauman Dawalatabad",
        "Abdelwahab Heba",
        "Jianyuan Zhong",
        "Ju-Chieh Chou",
        "Sung-Lin Yeh",
        "Szu-Wei Fu",
        "Chien-Feng Liao",
        "Elena Rastorgueva",
        "William Franc ¸ois Grondin",
        "Hwidong Aris",
        "Yan Na",
        "Renato Gao",
        "Yoshua Mori",
        "Bengio"
      ],
      "venue": "SpeechBrain: A General-Purpose Speech Toolkit"
    },
    {
      "citation_id": "27",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Large Batch Training of Convolutional Networks",
      "authors": [
        "Yang You",
        "Igor Gitman",
        "Boris Ginsburg"
      ],
      "venue": "Large Batch Training of Convolutional Networks"
    },
    {
      "citation_id": "29",
      "title": "SGDR: Stochastic Gradient Descent with Warm Restarts",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2016",
      "venue": "5th International Conference on Learning Representations"
    },
    {
      "citation_id": "30",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings"
    }
  ]
}