{
  "paper_id": "2402.11954v1",
  "title": "Multimodal Emotion Recognition From Raw Audio With Sinc-Convolution",
  "published": "2024-02-19T08:49:09Z",
  "authors": [
    "Xiaohui Zhang",
    "Wenjie Fu",
    "Mangui Liang"
  ],
  "keywords": [
    "multimodal emotion recognition",
    "raw audio signal",
    "Sinc-convolution layer",
    "feature-level fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) is still a complex task for computers with average recall rates usually about 70% on the most realistic datasets. Most SER systems use handcrafted features extracted from audio signal such as energy, zero crossing rate, spectral information, prosodic, mel frequency cepstral coefficient (MFCC), and so on. More recently, using raw waveform for training neural network is becoming an emerging trend. This approach is advantageous as it eliminates the feature extraction pipeline. Learning from time-domain signal has shown good results for tasks such as speech recognition, speaker verification etc. In this paper, we utilize Sinc-convolution layer, which is an efficient architecture for preprocessing raw speech waveform for emotion recognition, to extract acoustic features from raw audio signals followed by a long short-term memory (LSTM). We also incorporate linguistic features and append a dialogical emotion decoding (DED) strategy. Our approach achieves a weighted accuracy of 85.1% in four class emotion on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is a major research area. Most of the acoustic features used in the fusing system are hand-crafted features  [1] , such as MFCC, pitch and voice quality features extracted from the raw audio or features learned automatically from spectrograms  [2] ,  [3] . However, the hand-crafted features contain more limited information than the raw audio signal, which makes more and more researchers pay attention to extracting features from raw audio more flexibly. For example, Guizzo et el  [4]  present a multitime-scale (MTS) convolution layer which does not increase the number of parameters but increases the temporal flexi-bility compared to standard CNNs. Xu et el  [5]  propose a hierarchical grained and feature model (HGFM) which includes a frame-level representation module with before and after information, a utterance-level representation module with context information, and a different level acoustic feature fusing module. In this paper, we append the sentences of raw audio as acoustic features to our model. Due to the great success of CNN in the field of speech emotion recognition, most systems use the convolution layer to process acoustic features. However, the standard CNN convolution layer has many filter parameters, which makes the convolution layer only capable of learning the low dimensional and compacted features  [6, 7] . To help the input layer discover more meaningful filters, Ravanelli el et  [8]  proposes SincNet, a neural architecture of directly processing waveform audio. The Sinc-convolution layer of Sinc-Net is not only faster in convergence speed than a standard CNN but also more computationally efficient due to the exploitation of filter symmetry. For this reason, we utilize the Sinc-convolution layer as the input layer to extract acoustic information from raw audio signals, and we utilize LSTM followed by the Sinc-convolution layer to extract information between sentences. Compared with conventional convolution layers, Sinc-convolution layers can learn the parameters of filters from raw audio signals, have fast convergence and higher interpretability with a smaller number of parameters. Humans express emotions through multi-modal ways, which suggests that we can process multi-modal features by fusing model to improve the accuracy of predictions  [9] . At present, the feature-level fusion of acoustic features and linguistic features is more commonly used. Pepino et el  [2]  present different fusing models for SER by combining acoustic and linguistic features. The result shows that multi-modal system leads to significant improvements of approximately 16% on Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [10] . In this paper, we study a multimodal emotion recognition model with combined acoustic and linguistic information and utilize dialogical emotion decoder (DED)  [11]  to process the pre-diction result of pre-trained classifier. DED decodes each utterance into one of the four emotion categories at inference stage. Experiments are performed on IEMOCAP dataset. Our approach achieves a weighted accuracy of 85.1% in four class emotion on IEMOCAP.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Acoustic Model",
      "text": "To learn more useful information from raw signals, we utilize Sinc-convolution filter layers  [8]  to learn custom filter banks tuned for emotion recognition from speech audio. Neural architecture for processing raw audio samples as shown in The first layer of a standard CNN performs a set of timedomain convolutions between the input waveform and some Finite Impulse Response (FIR) filters. Each convolution is defined as follows:\n\nwhere x[n] is a chunk of speech signal. h[n] is the filter of length L, and y[n] is the filtered output. In conventional convolution layer, all the L elements of each filter are learned from data. Conversely, the proposed Sinc-convolution layer performs the convolution with a pre-defined function g that depends on fewer learnable parameters θ only  [8]  y\n\nEach defined filter-bank is composed of rectangular bandpass filters. In the frequency domain, the magnitude of a generic band-pass filter can be represented by two low-pass filters.\n\nwhere f 1 , f 2 refers to low and high cutoff frequencies and rect function is the rectangular function in the magnitude fre-quency domain. The time-domain representation of the function g can be derived as follows\n\nwhere the sinc function is defined as sinc(x) = sin(x)/x. We compare Sinc-convolution layers followed by Deep Neural Network (DNN) and LSTM. We also compare conventional convolution layers of CNN fed by spectrogram computed from raw audio and Sinc-convolution layers of Sinc-DNN and Sinc-LSTM fed by 250ms chunk selected from raw audio randomly. Though CNN is based on the same architecture as Sinc-DNN, it replaces the sinc-based convolution with a standard one. Both Sinc-convolution layers and convolution layers of CNN use Batch Normalization. The performance is evaluated with Sentence Error Rate (SER), which represents the average of error rate of each sentence.\n\nThe learning curves of Sinc-DNN and CNN can be observed from Figure  2 . Both convolution layer of Sinc-DNN and standard CNN followed by a Deep Neural Network (DNN), which contains several fully connected layers and Batch Normalization. Figure  2  shows that both Sinc-DNN and Sinc-LSTM have faster convergence than CNN beacuse they reduces the number of parameters in the first convolutional layer and the function g is symmetric that means we only need to consider one side of the filter and inherit the result for the other half. However, the CNN converges to a better performance leading to a Sentence Error Rate (SER) of 46% against a SER of 51% achieved with the Sinc-DNN. One reason for this result is that Sinc-DNN does not pay attention to the information between sentences, which is helpful in the emotion recognition. We learn this neglected information by replacing DNN with LSTM. Another reason is that some of the extracted features may be meaningless noise due to our random chunk extractions from raw audio. To reduce the noise interference caused by randomly extracting features, we extract the features after pre-processing the signals in the database, such as extracting the parts with higher power to effectively avoid the noise interference. As observed in the Figure  2 , the SER of Sinc-LSTM dropped to 43%, which is 8% lower than Sinc-DNN and 3% lower than CNN.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Linguistic Model",
      "text": "Recurrent Neural Network (RNN) is widely used in natural language processing tasks. However, RNNs cannot remember longer sentences and sequences due to the vanishing/exploding gradient problem. It can only remember the parts which it has just seen. The emergence of LSTM has solved this problem well  [12] . Although an LSTM is supposed to capture the long-range dependency better than the RNN, it is difficult to give large weights to the important words for estimation. To this end, we deploy a self-attention layer on top of the LSTM. The self-attention mechanism can focus on the important parts in the sentence  [13] . The more relevant the semantic relation between words and emotion is, the greater the weight of the connection between them, which can assist the model to determine more accurate classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Fusion",
      "text": "Feature-level fusion is used to fuse linguistic and acoustic features obtained from individual networks. A 2048-D feature vector from the acoustic network and a 4800-D feature vector from the linguistic network are concatenated. We use attention before fusion where attention is applied on individual feature vectors. Finally, the utterance emotion is classified with the use of a \"Softmax\" activation and a dialogical emotion decoder (DED) which is a post-processing over the final dense layer of the fusing network.\n\nIn this paper, we study two fusing models without DED. One is the acoustic feature of Sinc-DNN fusing with the linguistic feature of LSTM (M1), and the other is the acoustic feature of Sinc-LSTM fusing with the linguistic feature of LSTM (M2). We also utilize DED to classify the prediction of M2, which makes the performance better than M2.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dialogical Emotion Decoder",
      "text": "The decoding process of Dialogical Emotion Decoder (DED) is built on top of a SER model which is an approximate inference algorithm which decodes each utterance into one of the four emotion categories at inference stage. This decoder is built on three core ideas: the emotion which occurs more frequently in dialog history is more likely to show up again; while not all utterances have consensus labels; and the posterior distributions capturing affective information would enable us to decode utterances in sequence. DED can model the emotion flows in a dialog consecutively by combining emotion classification, emotion shift and emotion assignment process together. Yeh et el  [11]  achieves 70.1% unweighted accuracy on four emotion class in the IEMOCAP.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment Setup",
      "text": "In this work, we used the IEMOCAP, a benchmark dataset containing about 12 hours of audio and video data, as well as text transcriptions. The dataset contains five sessions, each of which involves two distinct professional actors conversing with one another in both scripted and improvised manners. In this work, we utilize data from both scripted and improvised conversations, as well as mere audio data to stay consistent with the vast majority of prior work. We also train and evaluate our model on four emotions: happy, neutral, angry, and sad, resulting in a total of 5531 utterances (happy: 29.5%, neutral: 30.8%, angry: 19.9%, sad: 19.5%).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Result And Analysis",
      "text": "We compare the performance of three models: 1) a standard CNN; 2) a Sinc-DNN; and 3) a Sinc-LSTM. It can be seen   1  that Sinc-LSTM outperforms CNN with a 2.8% improvement that is also obtained with faster convergence (25 vs 40 epochs). Sinc-convolution layer is specifically designed to implement rectangular bandpass filters, leading to more meaningful CNN filters, and LSTM can learn more contextual information than DNN does. This makes Sinc-LSTM perform better. The performance of multi-modal systems is indicated in Table  2 . CNN+LSTM is the our baseline with acoustic model CNN and linguistic model LSTM. MDRE  [14]  using dual RNNs encode acoustic and linguistic information and predict using a feed-forward neural model. MHA-2  [15]  propose a multi-hop attention model which is designed to process the contextual information from transcripts and audio. Table  2  shows that MHA-2 has outperformed CNN+LSTM by 5.7% in the terms of WA when the model is applied to the IEMO-CAP dataset and MHA-2 has outperformed MDRE by 4.7% that demonstrates the attention framework can increase performance.\n\nAs observed from Table  2 , Both M1 and M2 perform better than CNN+LSTM, which indicates that the multi-modal model using Sinc-convolution layer not only has faster convergence than that using CNN, but also has better verification accuracy. The unweighted accuracy of Sinc-DNN is lower than CNN but M1 has outperformed CNN+LSTM by 1.3%. The main reason is that the information between sentences can learned from textual features by LSTM. The M2 system using LSTM performs better than M1. Specifically, M2  achieves 75% accuracy, with relative 3% improvement over M1, which verifies that LSTM is more suitable for processing contextual information on IEMOCAP than DNN. Figure  3  shows the architecture of M2+DED. Based on the M2 system, we use DED to classify the prediction of M2, and finally achieves an accuracy rate of 85%, which improves the accuracy rate by 8.8% compared with the MHA-2. However, as shown in Table  2 , the effectiveness of IAAN+DED is not as significant as M2+DED. First, DED relies on a wellperforming classifier  [11] . Due to the poorer performance of IAAN in the IEMOCAP, the effect of DED is more limited. Furthermore, as the IAAN+DED only extract acoustic lowlevel descriptors (LLD), including features such as MFCCs, the ability of DED in properly decoding through long sequence of dialogs is not as obvious as for the raw audio. From Figure  4 , We also find that the performance of DED is more significant when accuracy of the pre-classifier is higher than 30% because DED classifies the current emotion based on the previous and the current prediction of the pre-classifier.\n\nIf the correct account for a large proportion of all predictions reaches, for example, 75% of M2, then even if the current is wrong, it can still be corrected by the previous correct predictions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we utilize Sinc-convolution layer to extract acoustic features from raw audio followed by an LSTM and a multi-modal emotion recognition system based on Sinc-LSTM and LSTM combining acoustic and linguistic data (Sinc-LSTM+LSTM). The performance of Sinc-LSTM+LSTM is better than CNN+LSTM, which shows that not only acoustic model but also fusing model using Sincconvolution layer has higher verification accuracy. We do not use the hand-crafted features that have been used in the standard CNN but use the raw audio append to the Sincconvolution layer, which makes Sinc-LSTM+LSTM learn more information than the baseline using CNN and LSTM does. Finally, the prediction of Sinc-LSTM+LSTM is appended to dialogical emotion decoding (DED) strategy, a post-processing that classifies the current result based on the previous and current prediction of pre-classifier. The performance of DED has achieved a weighted accuracy of 85.29% on the IEMOCAP database, which outperforms the Sinc-LSTM+LSTM by 9.96%.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of Sinc-conv layer.",
      "page": 2
    },
    {
      "caption": "Figure 2: Both convolution layer of Sinc-DNN",
      "page": 2
    },
    {
      "caption": "Figure 2: shows that both Sinc-DNN",
      "page": 2
    },
    {
      "caption": "Figure 2: Sentence Error Rate of CNN, Sinc-DNN and Sinc-",
      "page": 2
    },
    {
      "caption": "Figure 3: The Architecture of fusing model combining Sinc-",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the architecture of M2+DED. Based on the M2 sys-",
      "page": 4
    },
    {
      "caption": "Figure 4: , We also find that the performance of DED is more",
      "page": 4
    },
    {
      "caption": "Figure 4: The performance of DED with different pre-classifiers",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Beijing 100044, China": "",
          "Huazhong University of Science and Technology": "Wuhan 430074, China"
        },
        {
          "Beijing 100044, China": "ABSTRACT",
          "Huazhong University of Science and Technology": "bility compared to standard CNNs. Xu et el\n[5] propose a"
        },
        {
          "Beijing 100044, China": "",
          "Huazhong University of Science and Technology": "hierarchical grained and feature model\n(HGFM) which in-"
        },
        {
          "Beijing 100044, China": "Speech Emotion Recognition (SER)\nis still a complex task",
          "Huazhong University of Science and Technology": ""
        },
        {
          "Beijing 100044, China": "",
          "Huazhong University of Science and Technology": "cludes a frame-level\nrepresentation module with before and"
        },
        {
          "Beijing 100044, China": "for computers with average recall\nrates usually about 70%",
          "Huazhong University of Science and Technology": ""
        },
        {
          "Beijing 100044, China": "",
          "Huazhong University of Science and Technology": "after\ninformation,\na\nutterance-level\nrepresentation module"
        },
        {
          "Beijing 100044, China": "on the most realistic datasets. Most SER systems use hand-",
          "Huazhong University of Science and Technology": ""
        },
        {
          "Beijing 100044, China": "",
          "Huazhong University of Science and Technology": "with context\ninformation, and a different\nlevel acoustic fea-"
        },
        {
          "Beijing 100044, China": "crafted features extracted from audio signal such as energy,",
          "Huazhong University of Science and Technology": ""
        },
        {
          "Beijing 100044, China": "",
          "Huazhong University of Science and Technology": "ture fusing module. In this paper, we append the sentences of"
        },
        {
          "Beijing 100044, China": "zero crossing rate,\nspectral\ninformation, prosodic, mel\nfre-",
          "Huazhong University of Science and Technology": ""
        },
        {
          "Beijing 100044, China": "",
          "Huazhong University of Science and Technology": "raw audio as acoustic features to our model."
        },
        {
          "Beijing 100044, China": "quency cepstral coefficient\n(MFCC), and so on. More re-",
          "Huazhong University of Science and Technology": ""
        },
        {
          "Beijing 100044, China": "cently, using raw waveform for training neural network is be-",
          "Huazhong University of Science and Technology": "Due to the great success of CNN in the field of speech"
        },
        {
          "Beijing 100044, China": "coming an emerging trend. This approach is advantageous as",
          "Huazhong University of Science and Technology": "emotion recognition, most systems use the convolution layer"
        },
        {
          "Beijing 100044, China": "it eliminates the feature extraction pipeline.\nLearning from",
          "Huazhong University of Science and Technology": "to process acoustic features.\nHowever,\nthe standard CNN"
        },
        {
          "Beijing 100044, China": "time-domain signal has\nshown good results\nfor\ntasks\nsuch",
          "Huazhong University of Science and Technology": "convolution layer has many filter parameters, which makes"
        },
        {
          "Beijing 100044, China": "as\nspeech recognition,\nspeaker verification etc.\nIn this pa-",
          "Huazhong University of Science and Technology": "the convolution layer only capable of\nlearning the low di-"
        },
        {
          "Beijing 100044, China": "per, we utilize Sinc-convolution layer, which is an efficient",
          "Huazhong University of Science and Technology": "mensional and compacted features\n[6, 7].\nTo help the in-"
        },
        {
          "Beijing 100044, China": "architecture for preprocessing raw speech waveform for emo-",
          "Huazhong University of Science and Technology": "put\nlayer discover more meaningful filters, Ravanelli el et"
        },
        {
          "Beijing 100044, China": "tion recognition,\nto extract acoustic features from raw audio",
          "Huazhong University of Science and Technology": "[8] proposes SincNet, a neural architecture of directly pro-"
        },
        {
          "Beijing 100044, China": "signals followed by a long short-term memory (LSTM). We",
          "Huazhong University of Science and Technology": "cessing waveform audio. The Sinc-convolution layer of Sinc-"
        },
        {
          "Beijing 100044, China": "also incorporate linguistic features and append a dialogical",
          "Huazhong University of Science and Technology": "Net\nis not only faster\nin convergence speed than a standard"
        },
        {
          "Beijing 100044, China": "emotion decoding (DED)\nstrategy.\nOur approach achieves",
          "Huazhong University of Science and Technology": "CNN but also more computationally efficient due to the ex-"
        },
        {
          "Beijing 100044, China": "a weighted accuracy of 85.1% in four class emotion on the",
          "Huazhong University of Science and Technology": "ploitation of filter symmetry. For this reason, we utilize the"
        },
        {
          "Beijing 100044, China": "Interactive Emotional Dyadic Motion Capture (IEMOCAP)",
          "Huazhong University of Science and Technology": "Sinc-convolution layer as the input\nlayer\nto extract acoustic"
        },
        {
          "Beijing 100044, China": "dataset.",
          "Huazhong University of Science and Technology": "information from raw audio signals,\nand we utilize LSTM"
        },
        {
          "Beijing 100044, China": "",
          "Huazhong University of Science and Technology": "followed by the Sinc-convolution layer to extract information"
        },
        {
          "Beijing 100044, China": "Index Terms— multimodal emotion recognition, raw au-",
          "Huazhong University of Science and Technology": ""
        },
        {
          "Beijing 100044, China": "",
          "Huazhong University of Science and Technology": "between sentences. Compared with conventional convolution"
        },
        {
          "Beijing 100044, China": "dio signal, Sinc-convolution layer, feature-level fusion",
          "Huazhong University of Science and Technology": ""
        },
        {
          "Beijing 100044, China": "",
          "Huazhong University of Science and Technology": "layers, Sinc-convolution layers can learn the parameters of fil-"
        },
        {
          "Beijing 100044, China": "",
          "Huazhong University of Science and Technology": "ters from raw audio signals, have fast convergence and higher"
        },
        {
          "Beijing 100044, China": "1.\nINTRODUCTION",
          "Huazhong University of Science and Technology": "interpretability with a smaller number of parameters. Humans"
        },
        {
          "Beijing 100044, China": "",
          "Huazhong University of Science and Technology": "express emotions through multi-modal ways, which suggests"
        },
        {
          "Beijing 100044, China": "Speech emotion recognition (SER) is a major research area.",
          "Huazhong University of Science and Technology": "that we can process multi-modal\nfeatures by fusing model"
        },
        {
          "Beijing 100044, China": "Most of\nthe acoustic features used in the fusing system are",
          "Huazhong University of Science and Technology": "to improve the accuracy of predictions [9]. At present,\nthe"
        },
        {
          "Beijing 100044, China": "hand-crafted features\n[1],\nsuch as MFCC, pitch and voice",
          "Huazhong University of Science and Technology": "feature-level fusion of acoustic features and linguistic features"
        },
        {
          "Beijing 100044, China": "quality\nfeatures\nextracted\nfrom the\nraw audio\nor\nfeatures",
          "Huazhong University of Science and Technology": "is more commonly used.\nPepino et el\n[2] present different"
        },
        {
          "Beijing 100044, China": "learned automatically from spectrograms [2],\n[3]. However,",
          "Huazhong University of Science and Technology": "fusing models for SER by combining acoustic and linguistic"
        },
        {
          "Beijing 100044, China": "the hand-crafted features contain more limited information",
          "Huazhong University of Science and Technology": "features. The result shows that multi-modal system leads to"
        },
        {
          "Beijing 100044, China": "than the raw audio signal, which makes more and more re-",
          "Huazhong University of Science and Technology": "significant\nimprovements of approximately 16% on Interac-"
        },
        {
          "Beijing 100044, China": "searchers pay attention to extracting features from raw audio",
          "Huazhong University of Science and Technology": "tive Emotional Dyadic Motion Capture (IEMOCAP) [10].\nIn"
        },
        {
          "Beijing 100044, China": "more flexibly. For example, Guizzo et el [4] present a multi-",
          "Huazhong University of Science and Technology": "this paper, we study a multimodal emotion recognition model"
        },
        {
          "Beijing 100044, China": "time-scale (MTS) convolution layer which does not\nincrease",
          "Huazhong University of Science and Technology": "with combined acoustic and linguistic information and utilize"
        },
        {
          "Beijing 100044, China": "the number of parameters but\nincreases\nthe temporal flexi-",
          "Huazhong University of Science and Technology": "dialogical emotion decoder\n(DED)\n[11]\nto process the pre-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "terance into one of\nthe four emotion categories at\ninference",
          "quency domain. The time-domain representation of the func-": "tion g can be derived as follows"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "stage. Experiments are performed on IEMOCAP dataset. Our",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "(4)\ng[n, f1, f2] = 2f2sinc(2πf2n) − 2f1sinc(2πf1n)"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "approach achieves a weighted accuracy of 85.1% in four class",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "emotion on IEMOCAP.",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "where the sinc function is defined as sinc(x) = sin(x)/x."
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "We compare Sinc-convolution layers followed by Deep Neu-"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "2. METHODOLOGY",
          "quency domain. The time-domain representation of the func-": "ral Network (DNN) and LSTM. We also compare conven-"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "tional convolution layers of CNN fed by spectrogram com-"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "2.1. Acoustic Model",
          "quency domain. The time-domain representation of the func-": "puted from raw audio and Sinc-convolution layers of Sinc-"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "DNN and Sinc-LSTM fed by 250ms chunk selected from raw"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "To learn more useful information from raw signals, we utilize",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "audio randomly. Though CNN is based on the same architec-"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "Sinc-convolution filter layers [8] to learn custom filter banks",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "ture as Sinc-DNN, it replaces the sinc-based convolution with"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "tuned for emotion recognition from speech audio. Neural ar-",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "a standard one. Both Sinc-convolution layers and convolution"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "chitecture for processing raw audio samples as shown in Fig-",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "layers of CNN use Batch Normalization. The performance is"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "ure 1.",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "evaluated with Sentence Error Rate (SER), which represents"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "the average of error rate of each sentence."
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "Convlution_1D\nConvlution_1D",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "Batch \nMax pooling 1D,",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "N_filters=80\nN_filters=60",
          "quency domain. The time-domain representation of the func-": "The learning curves of Sinc-DNN and CNN can be ob-"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "normalization\nlength=3",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "Filter_length=215\nFilters_length=5",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "served from Figure 2. Both convolution layer of Sinc-DNN"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "and\nstandard CNN followed\nby\na Deep Neural Network"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "(DNN), which contains\nseveral\nfully connected layers and"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "Batch normalization",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "Batch Normalization.\nFigure 2 shows that both Sinc-DNN"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "and Sinc-LSTM have faster convergence than CNN beacuse"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "Convlution_1D",
          "quency domain. The time-domain representation of the func-": "they reduces the number of parameters in the first convolu-"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "Max pooling 1D, \nBatch \nMax pooling 1D,",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "N_filters=60",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "length=3\nnormalization\nlength=3",
          "quency domain. The time-domain representation of the func-": "tional\nlayer and the function g is symmetric that means we"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "Filters_length=5",
          "quency domain. The time-domain representation of the func-": ""
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "only need to consider one side of\nthe filter and inherit\nthe"
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "",
          "quency domain. The time-domain representation of the func-": "result for the other half."
        },
        {
          "diction result of pre-trained classifier. DED decodes each ut-": "Fig. 1. Architecture of Sinc-conv layer.",
          "quency domain. The time-domain representation of the func-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "N_filters=60": "length=3\nnormalization\nlength=3"
        },
        {
          "N_filters=60": "Filters_length=5"
        },
        {
          "N_filters=60": ""
        },
        {
          "N_filters=60": ""
        },
        {
          "N_filters=60": "Fig. 1. Architecture of Sinc-conv layer."
        },
        {
          "N_filters=60": ""
        },
        {
          "N_filters=60": "The first layer of a standard CNN performs a set of time-"
        },
        {
          "N_filters=60": ""
        },
        {
          "N_filters=60": "domain convolutions between the input waveform and some"
        },
        {
          "N_filters=60": "Finite Impulse Response (FIR) filters.\nEach convolution is"
        },
        {
          "N_filters=60": ""
        },
        {
          "N_filters=60": "defined as follows:"
        },
        {
          "N_filters=60": ""
        },
        {
          "N_filters=60": "L−1"
        },
        {
          "N_filters=60": ""
        },
        {
          "N_filters=60": "(cid:88) l\ny[n] = x[n] ∗ h[n] =\nx[l] · h[n − l]\n(1)"
        },
        {
          "N_filters=60": ""
        },
        {
          "N_filters=60": "=0"
        },
        {
          "N_filters=60": ""
        },
        {
          "N_filters=60": "where x[n]\nis a chunk of speech signal. h[n]\nis the filter of"
        },
        {
          "N_filters=60": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: CNN+LSTM is the our baseline with acoustic",
      "data": [
        {
          "such as extracting the parts with higher power to effectively": "avoid the noise interference. As observed in the Figure 2, the",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "SER of Sinc-LSTM dropped to 43%, which is 8% lower than",
          "3. EXPERIMENTS": "3.1. Experiment Setup"
        },
        {
          "such as extracting the parts with higher power to effectively": "Sinc-DNN and 3% lower than CNN.",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "In this work, we used the IEMOCAP, a benchmark dataset"
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "containing about 12 hours of audio and video data, as well as"
        },
        {
          "such as extracting the parts with higher power to effectively": "2.2. Linguistic Model",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "text\ntranscriptions.\nThe dataset contains five sessions, each"
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "of which involves two distinct professional actors conversing"
        },
        {
          "such as extracting the parts with higher power to effectively": "Recurrent Neural Network (RNN)\nis widely used in natu-",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "with one another in both scripted and improvised manners. In"
        },
        {
          "such as extracting the parts with higher power to effectively": "ral\nlanguage processing tasks.\nHowever, RNNs cannot\nre-",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "this work, we utilize data from both scripted and improvised"
        },
        {
          "such as extracting the parts with higher power to effectively": "member\nlonger sentences and sequences due to the vanish-",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "conversations, as well as mere audio data to stay consistent"
        },
        {
          "such as extracting the parts with higher power to effectively": "ing/exploding gradient problem.\nIt can only remember\nthe",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "with the vast majority of prior work. We also train and eval-"
        },
        {
          "such as extracting the parts with higher power to effectively": "parts which it has just seen.\nThe emergence of LSTM has",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "uate our model on four emotions: happy, neutral, angry, and"
        },
        {
          "such as extracting the parts with higher power to effectively": "solved this problem well\n[12]. Although an LSTM is sup-",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "sad,\nresulting in a total of 5531 utterances (happy:\n29.5%,"
        },
        {
          "such as extracting the parts with higher power to effectively": "posed to capture the long-range dependency better\nthan the",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "neutral: 30.8%, angry: 19.9%, sad: 19.5%)."
        },
        {
          "such as extracting the parts with higher power to effectively": "RNN,\nit\nis difficult\nto give large weights\nto the important",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "words for estimation. To this end, we deploy a self-attention",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "3.2. Result and Analysis"
        },
        {
          "such as extracting the parts with higher power to effectively": "layer on top of the LSTM. The self-attention mechanism can",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "focus on the important parts in the sentence [13]. The more",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "We compare the performance of three models: 1) a standard"
        },
        {
          "such as extracting the parts with higher power to effectively": "relevant the semantic relation between words and emotion is,",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "CNN; 2) a Sinc-DNN; and 3) a Sinc-LSTM.\nIt can be seen"
        },
        {
          "such as extracting the parts with higher power to effectively": "the greater the weight of the connection between them, which",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "can assist the model to determine more accurate classification.",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "Table 1. Performance of CNN, Sinc-DNN and Sinc-LSTM."
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "Model\nTraining time(epoch) WA(%) UA(%)"
        },
        {
          "such as extracting the parts with higher power to effectively": "2.3. Feature Fusion",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "40\n54.3\n53.2\nCNN"
        },
        {
          "such as extracting the parts with higher power to effectively": "Feature-level fusion is used to fuse linguistic and acoustic fea-",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "25\n49.5\n47.8\nSinc-DNN"
        },
        {
          "such as extracting the parts with higher power to effectively": "tures obtained from individual networks. A 2048-D feature",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "25\n57.1\n54.5\nSinc-LSTM"
        },
        {
          "such as extracting the parts with higher power to effectively": "vector from the acoustic network and a 4800-D feature vec-",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "tor from the linguistic network are concatenated. We use at-",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "from Table 1 that Sinc-LSTM outperforms CNN with a 2.8%"
        },
        {
          "such as extracting the parts with higher power to effectively": "tention before fusion where attention is applied on individual",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "improvement that is also obtained with faster convergence (25"
        },
        {
          "such as extracting the parts with higher power to effectively": "feature vectors.\nFinally,\nthe utterance emotion is classified",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "vs 40 epochs). Sinc-convolution layer is specifically designed"
        },
        {
          "such as extracting the parts with higher power to effectively": "with the use of a “Softmax” activation and a dialogical emo-",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "to implement\nrectangular bandpass filters,\nleading to more"
        },
        {
          "such as extracting the parts with higher power to effectively": "tion decoder (DED) which is a post-processing over the final",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "meaningful CNN filters, and LSTM can learn more contextual"
        },
        {
          "such as extracting the parts with higher power to effectively": "dense layer of the fusing network.",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "information than DNN does. This makes Sinc-LSTM perform"
        },
        {
          "such as extracting the parts with higher power to effectively": "In this paper, we study two fusing models without DED.",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "better. The performance of multi-modal systems is indicated"
        },
        {
          "such as extracting the parts with higher power to effectively": "One is the acoustic feature of Sinc-DNN fusing with the lin-",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "in Table 2.\nCNN+LSTM is\nthe our baseline with acoustic"
        },
        {
          "such as extracting the parts with higher power to effectively": "guistic feature of LSTM (M1), and the other\nis\nthe acous-",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "model CNN and linguistic model LSTM. MDRE [14] using"
        },
        {
          "such as extracting the parts with higher power to effectively": "tic feature of Sinc-LSTM fusing with the linguistic feature of",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "dual RNNs encode acoustic and linguistic information and"
        },
        {
          "such as extracting the parts with higher power to effectively": "LSTM (M2). We also utilize DED to classify the prediction",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "predict using a feed-forward neural model. MHA-2 [15] pro-"
        },
        {
          "such as extracting the parts with higher power to effectively": "of M2, which makes the performance better than M2.",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "pose a multi-hop attention model which is designed to process"
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "the contextual\ninformation from transcripts and audio. Table"
        },
        {
          "such as extracting the parts with higher power to effectively": "2.4. Dialogical Emotion Decoder",
          "3. EXPERIMENTS": ""
        },
        {
          "such as extracting the parts with higher power to effectively": "",
          "3. EXPERIMENTS": "2 shows that MHA-2 has outperformed CNN+LSTM by 5.7%"
        },
        {
          "such as extracting the parts with higher power to effectively": "The decoding process of Dialogical Emotion Decoder (DED)",
          "3. EXPERIMENTS": "in the terms of WA when the model\nis applied to the IEMO-"
        },
        {
          "such as extracting the parts with higher power to effectively": "is built on top of a SER model which is an approximate in-",
          "3. EXPERIMENTS": "CAP dataset and MHA-2 has outperformed MDRE by 4.7%"
        },
        {
          "such as extracting the parts with higher power to effectively": "ference algorithm which decodes each utterance into one of",
          "3. EXPERIMENTS": "that demonstrates the attention framework can increase per-"
        },
        {
          "such as extracting the parts with higher power to effectively": "the four emotion categories at\ninference stage. This decoder",
          "3. EXPERIMENTS": "formance."
        },
        {
          "such as extracting the parts with higher power to effectively": "is built on three core ideas:\nthe emotion which occurs more",
          "3. EXPERIMENTS": "As observed from Table 2, Both M1 and M2 perform bet-"
        },
        {
          "such as extracting the parts with higher power to effectively": "frequently in dialog history is more likely to show up again;",
          "3. EXPERIMENTS": "ter\nthan CNN+LSTM, which indicates that\nthe multi-modal"
        },
        {
          "such as extracting the parts with higher power to effectively": "while not all utterances have consensus labels; and the pos-",
          "3. EXPERIMENTS": "model using Sinc-convolution layer not only has faster con-"
        },
        {
          "such as extracting the parts with higher power to effectively": "terior distributions capturing affective information would en-",
          "3. EXPERIMENTS": "vergence than that using CNN, but also has better verification"
        },
        {
          "such as extracting the parts with higher power to effectively": "able us to decode utterances in sequence. DED can model the",
          "3. EXPERIMENTS": "accuracy.\nThe unweighted accuracy of Sinc-DNN is lower"
        },
        {
          "such as extracting the parts with higher power to effectively": "emotion flows in a dialog consecutively by combining emo-",
          "3. EXPERIMENTS": "than CNN but M1 has outperformed CNN+LSTM by 1.3%."
        },
        {
          "such as extracting the parts with higher power to effectively": "tion classification, emotion shift and emotion assignment pro-",
          "3. EXPERIMENTS": "The main reason is that\nthe information between sentences"
        },
        {
          "such as extracting the parts with higher power to effectively": "cess together. Yeh et el [11] achieves 70.1% unweighted ac-",
          "3. EXPERIMENTS": "can learned from textual\nfeatures by LSTM. The M2 sys-"
        },
        {
          "such as extracting the parts with higher power to effectively": "curacy on four emotion class in the IEMOCAP.",
          "3. EXPERIMENTS": "tem using LSTM performs better than M1. Specifically, M2"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: , the effectiveness of IAAN+DED",
      "data": [
        {
          "the previous and the current prediction of\nthe pre-classifier.": ""
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "If the correct account for a large proportion of all predictions"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": ""
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "reaches, for example, 75% of M2,\nthen even if the current\nis"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "wrong, it can still be corrected by the previous correct predic-"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "tions."
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": ""
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "29.51% 0.00% 67.76% 2.73%\n70.97% 11.06% 7.37% 10.60%\n78.05% 3.48% 14.98% 3.48%"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": ""
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "42.96% 5.19% 40.74% 10.37%\n26.44% 60.34% 0.57% 12.64%\n23.59% 66.64% 6.90%\n2.87%"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "Labels\nAnger     sadness   Happiness   Neutral\nLabels\nAnger     sadness   Happiness   Neutral\nLabels\nAnger     sadness   Happiness   Neutral"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "12.50% 0.00% 87.50% 0.00%\n15.45% 6.50% 72.36% 5.69%\n20.44% 0.55% 77.35% 1.66%"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": ""
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "32.95% 10.23% 30.68% 26.14%\n7.58%\n3.03%\n3.79% 85.60%\n14.36% 1.66%\n2.76% 81.22%"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "Neutral    Happiness   Sadness   Anger\nNeutral    Happiness   Sadness   Anger\nNeutral    Happiness   Sadness   Anger"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "Predictions\nPredictions\nPredictions"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": ""
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "25.68% 0.00% 71.58% 2.73%\n83.49% 5.85%\n1.87%\n8.78%\n86.68% 5.03%\n7.85%\n6.44%"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": ""
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "Anger     sadness   Happiness   Neutral\nAnger     sadness   Happiness   Neutral\nAnger     sadness   Happiness   Neutral\n45.19% 2.22% 42.22% 10.37%\n12.39% 81.84% 0.38%\n5.38%\n11.82% 86.26% 1.63%\n0.29%"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "Labels\nLabels\nLabels\n12.50% 0.00% 87.50% 0.00%\n33.03% 1.11% 60.52% 5.35%\n4.61%\n0.55% 92.71% 2.12%"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": ""
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "43.59% 5.13% 25.64% 25.64%\n6.98%\n0.63%\n0.63% 91.75%\n12.87% 0.45%\n3.99% 82.68%"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": ""
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "Neutral    Happiness   Sadness   Anger\nNeutral    Happiness   Sadness   Anger\nNeutral    Happiness   Sadness   Anger"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "Predictions\nPredictions\nPredictions"
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": ""
        },
        {
          "the previous and the current prediction of\nthe pre-classifier.": "71.40%\n75.90%\n32.59%"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: , the effectiveness of IAAN+DED",
      "data": [
        {
          "Sinc-": "convolution",
          "Word2vector": ""
        },
        {
          "Sinc-": "layer",
          "Word2vector": ""
        },
        {
          "Sinc-": "",
          "Word2vector": ""
        },
        {
          "Sinc-": "",
          "Word2vector": ""
        },
        {
          "Sinc-": "",
          "Word2vector": ""
        },
        {
          "Sinc-": "",
          "Word2vector": ""
        },
        {
          "Sinc-": "",
          "Word2vector": ""
        },
        {
          "Sinc-": "",
          "Word2vector": ""
        },
        {
          "Sinc-": "",
          "Word2vector": ""
        },
        {
          "Sinc-": "",
          "Word2vector": ""
        },
        {
          "Sinc-": "",
          "Word2vector": ""
        },
        {
          "Sinc-": "8000\n10000\n12000",
          "Word2vector": ""
        },
        {
          "Sinc-": "",
          "Word2vector": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: , the effectiveness of IAAN+DED",
      "data": [
        {
          "-0.2": "-0.4"
        },
        {
          "-0.2": "-0.6\n-0.8"
        },
        {
          "-0.2": "-1\n0\n2000\n4000\n6000\n8000\n10000\n12000"
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": "Fig. 3.\nThe Architecture of\nfusing model combining Sinc-"
        },
        {
          "-0.2": "LSTM and LSTM with DED post-processing"
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": "achieves 75% accuracy, with relative 3% improvement over"
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": "M1, which verifies that LSTM is more suitable for process-"
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": "ing contextual\ninformation on IEMOCAP than DNN. Figure"
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": "3 shows the architecture of M2+DED. Based on the M2 sys-"
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": "tem, we use DED to classify the prediction of M2, and fi-"
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": "nally achieves an accuracy rate of 85%, which improves the"
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": "accuracy rate by 8.8% compared with the MHA-2.\nHow-"
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": "ever, as shown in Table 2,\nthe effectiveness of\nIAAN+DED"
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": "is not as significant as M2+DED. First, DED relies on a well-"
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": "performing classifier [11]. Due to the poorer performance of"
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": "IAAN in the IEMOCAP,\nthe effect of DED is more limited."
        },
        {
          "-0.2": ""
        },
        {
          "-0.2": "Furthermore, as the IAAN+DED only extract acoustic low-"
        },
        {
          "-0.2": "level descriptors (LLD),\nincluding features such as MFCCs,"
        },
        {
          "-0.2": "the ability of DED in properly decoding through long se-"
        },
        {
          "-0.2": "quence of dialogs is not as obvious as for the raw audio. From"
        },
        {
          "-0.2": "Figure 4, We also find that\nthe performance of DED is more"
        },
        {
          "-0.2": "significant when accuracy of the pre-classifier is higher than"
        },
        {
          "-0.2": "30% because DED classifies\nthe current emotion based on"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "Mower, Samuel Kim, Jeannette N.Chang, Sungbok Lee,"
        },
        {
          "5. REFERENCES": "[1] Christos-Nikolaos Anagnostopoulos, Theodoros\nlliou,",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "and Shrikanth S.Narayanan, “Iemocap: Interactive emo-"
        },
        {
          "5. REFERENCES": "and loannis Giannoukos,\n“Features and classifiers for",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "Language re-\ntional dyadic motion capture database,”"
        },
        {
          "5. REFERENCES": "emotion recognition from speech: a survey from 2000 to",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "sources and evaluation, vol. 42, no. 4, pp. 335, 2008."
        },
        {
          "5. REFERENCES": "2011,” Artificial Intelligence Review, vol. 43, pp. 155–",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "[11] Sung-Lin Yeh, Yun-Shao Lin, and Chi-Chun Lee,\n“A"
        },
        {
          "5. REFERENCES": "177, 2015.",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "dialogical emotion decoder for speech emotion recogni-"
        },
        {
          "5. REFERENCES": "[2] L. Pepino, P. Riera, L. Ferrer, and A. Gravano,\n“Fu-",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "tion in spoken dialog,”\nin ICASSP 2020 - 2020 IEEE"
        },
        {
          "5. REFERENCES": "sion approaches\nfor emotion recognition from speech",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "International Conference on Acoustics, Speech and Sig-"
        },
        {
          "5. REFERENCES": "using acoustic and text-based features,” in ICASSP 2020",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "nal Processing (ICASSP). IEEE, 2020, pp. 6479–6483."
        },
        {
          "5. REFERENCES": "- 2020 IEEE International Conference on Acoustics,",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "[12] Q. Huang, R. Chen, X. Zheng, and Z. Dong, “Deep sen-"
        },
        {
          "5. REFERENCES": "Speech and Signal Processing (ICASSP).\nIEEE, 2020,",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "timent representation based on cnn and lstm,”\nin 2017"
        },
        {
          "5. REFERENCES": "pp. 6484–6488.",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "International Conference on Green Informatics (ICGI),"
        },
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "2017, pp. 30–33."
        },
        {
          "5. REFERENCES": "[3] Aharon Satt, Shai Rozenberg, and Ron Hoory, “Efficient",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "emotion recognition from speech using deep learning on",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "[13]\nJ. Xie, B. Chen, X. Gu, F. Liang, and X. Xu,\n“Self-"
        },
        {
          "5. REFERENCES": "spectrograms,” in Proceedings of the IEEE. IEEE, 2017,",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "attention-based bilstm model for short\ntext fine-grained"
        },
        {
          "5. REFERENCES": "pp. 1089–1093.",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "sentiment\nclassification,”\nIEEE Access,\nvol.\n7,\npp."
        },
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "180558–180570, 2019."
        },
        {
          "5. REFERENCES": "[4] E. Guizzo, T. Weyde, and J. B. Leveson,\n“Multi-time-",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "scale convolution for emotion recognition from speech",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "[14] S. Yoon, S. Byun, S. Dey, and K. Jung,\n“Multimodal"
        },
        {
          "5. REFERENCES": "audio signals,”\nin ICASSP 2020 - 2020 IEEE Interna-",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "speech emotion recognition using audio and text,” 2018"
        },
        {
          "5. REFERENCES": "tional Conference on Acoustics, Speech and Signal Pro-",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "IEEE Spoken Language Technology Workshop (SLT),"
        },
        {
          "5. REFERENCES": "cessing (ICASSP). IEEE, 2020, pp. 6489–6493.",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "pp. 112–118, 2018."
        },
        {
          "5. REFERENCES": "[5] Y. Xu, H. Xu,\nand J. Zou,\n“Hgfm : A hierarchical",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "[15] S. Yoon, S. Byun, S. Dey, and K. Jung,\n“Speech emo-"
        },
        {
          "5. REFERENCES": "grained and feature model for acoustic emotion recog-",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "tion recognition using multi-hop attention mechanism,”"
        },
        {
          "5. REFERENCES": "nition,”\nin ICASSP 2020 - 2020 IEEE International",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "ICASSP 2019-2019 IEEE International Conference on"
        },
        {
          "5. REFERENCES": "Conference on Acoustics, Speech and Signal Processing",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "Acoustics, Speech and Signal Processing (ICASSP), pp."
        },
        {
          "5. REFERENCES": "(ICASSP), 2020, pp. 6499–6503.",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "2822–2826, 2019."
        },
        {
          "5. REFERENCES": "[6] Xiaohui\nZhang,\nJiangyan\nYi,\nChenglong Wang,",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "[16] Sung-Lin Yeh, Yun-Shao Lin, and Chi-Chun Lee,\n“An"
        },
        {
          "5. REFERENCES": "Chuyuan Zhang, Siding Zeng, and Jianhua Tao,\n“What",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "interaction-aware attention network for speech emotion"
        },
        {
          "5. REFERENCES": "to remember: Self-adaptive continual learning for audio",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "ICASSP 2019-2019\nrecognition in spoken dialogs,”"
        },
        {
          "5. REFERENCES": "deepfake detection,”\narXiv preprint arXiv:2312.09651,",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "IEEE International Conference on Acoustics,\nSpeech"
        },
        {
          "5. REFERENCES": "2023.",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": "and Signal Processing (ICASSP), pp. 6685–6689, 2019."
        },
        {
          "5. REFERENCES": "[7] Xiaohui Zhang,\nJiangyan Yi,\nJianhua Tao, Chenglong",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "Wang, and Chu Yuan Zhang, “Do you remember? Over-",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "coming catastrophic forgetting for fake audio detection,”",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "the 40th International Conference on\nin Proceedings of",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "Machine Learning, Andreas Krause, Emma Brunskill,",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "Jonathan Scarlett, Eds. 23–29 Jul 2023, vol. 202 of Pro-",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "ceedings of Machine Learning Research, pp. 41819–",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "41831, PMLR.",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "[8] M. Ravanelli and Y. Bengio, “Speaker recognition from",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "raw waveform with sincnet,”\nin 2018 IEEE Spoken",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "Language Technology Workshop (SLT). IEEE, 2018, pp.",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "1021–1028.",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "[9] Xiaohui Zhang,\nJaehong Yoon, Mohit Bansal,\nand",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "Huaxiu Yao,\n“Multimodal\nrepresentation learning by",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        },
        {
          "5. REFERENCES": "alternating unimodal adaptation,” 2023.",
          "[10] C. Busso, M. Bulut, C-C. Lee, Abe Kazemzadeh, Emily": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "Christos-Nikolaos Anagnostopoulos"
      ],
      "year": "2015",
      "venue": "Theodoros lliou, and loannis Giannoukos"
    },
    {
      "citation_id": "3",
      "title": "Fusion approaches for emotion recognition from speech using acoustic and text-based features",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer",
        "A Gravano"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "Aharon Satt",
        "Shai Rozenberg",
        "Ron Hoory"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "5",
      "title": "Multi-timescale convolution for emotion recognition from speech audio signals",
      "authors": [
        "E Guizzo",
        "T Weyde",
        "J Leveson"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "6",
      "title": "Hgfm : A hierarchical grained and feature model for acoustic emotion recognition",
      "authors": [
        "Y Xu",
        "H Xu",
        "J Zou"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "What to remember: Self-adaptive continual learning for audio deepfake detection",
      "authors": [
        "Xiaohui Zhang",
        "Jiangyan Yi",
        "Chenglong Wang",
        "Chuyuan Zhang",
        "Siding Zeng",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "What to remember: Self-adaptive continual learning for audio deepfake detection",
      "arxiv": "arXiv:2312.09651"
    },
    {
      "citation_id": "8",
      "title": "Do you remember? Overcoming catastrophic forgetting for fake audio detection",
      "authors": [
        "Xiaohui Zhang",
        "Jiangyan Yi",
        "Jianhua Tao",
        "Chenglong Wang",
        "Chu Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning"
    },
    {
      "citation_id": "9",
      "title": "Speaker recognition from raw waveform with sincnet",
      "authors": [
        "M Ravanelli",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "10",
      "title": "Multimodal representation learning by alternating unimodal adaptation",
      "authors": [
        "Xiaohui Zhang",
        "Jaehong Yoon",
        "Mohit Bansal",
        "Huaxiu Yao"
      ],
      "year": "2023",
      "venue": "Multimodal representation learning by alternating unimodal adaptation"
    },
    {
      "citation_id": "11",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C-C Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database"
    },
    {
      "citation_id": "12",
      "title": "A dialogical emotion decoder for speech emotion recognition in spoken dialog",
      "authors": [
        "Sung-Lin Yeh",
        "Yun-Shao Lin",
        "Chi-Chun Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Deep sentiment representation based on cnn and lstm",
      "authors": [
        "Q Huang",
        "R Chen",
        "X Zheng",
        "Z Dong"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Green Informatics (ICGI)"
    },
    {
      "citation_id": "14",
      "title": "Selfattention-based bilstm model for short text fine-grained sentiment classification",
      "authors": [
        "J Xie",
        "B Chen",
        "X Gu",
        "F Liang",
        "X Xu"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "15",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "S Dey",
        "K Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "S Yoon",
        "S Byun",
        "S Dey",
        "K Jung"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "An interaction-aware attention network for speech emotion recognition in spoken dialogs",
      "authors": [
        "Sung-Lin Yeh",
        "Yun-Shao Lin",
        "Chi-Chun Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}