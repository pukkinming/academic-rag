{
  "paper_id": "2204.12624v1",
  "title": "Evaluation Of Self-Taught Learning-Based Representations For Facial Emotion Recognition",
  "published": "2022-04-26T22:48:15Z",
  "authors": [
    "Bruna Delazeri",
    "Leonardo L. Veras",
    "Alceu de S. Britto Jr.",
    "Jean Paul Barddal",
    "Alessandro L. Koerich"
  ],
  "keywords": [
    "Self-taught Learging",
    "Autoencoder",
    "Unsupervised Representation Learning",
    "Dynamic Ensemble Selection"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This work describes different strategies to generate unsupervised representations obtained through the concept of self-taught learning for facial emotion recognition (FER). The idea is to create complementary representations promoting diversity by varying the autoencoders' initialization, architecture, and training data. SVM, Bagging, Random Forest, and a dynamic ensemble selection method are evaluated as final classification methods. Experimental results on Jaffe and Cohn-Kanade datasets using a leave-one-subject-out protocol show that FER methods based on the proposed diverse representations compare favorably against state-of-the-art approaches that also explore unsupervised feature learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "To build a model that generalizes properly, supervised learning methods require large amounts of labeled training data that, in many real-world applications, are scarce, difficult, and expensive to obtain. Consequently, this usually became a restriction in both computer vision and machine learning tasks. As a result, knowledge transfer between tasks emerged as a new learning framework to deal with the lack of labeled data. Self-taught learning (STL) is a particular transfer learning method that exploits data with a different distribution than the target problem  [2] . In opposition to transfer learning, STL does not require labeled data from an auxiliary domain, as it learns representations without labeling and considering different data distributions.\n\nThe use of unlabeled data has been the main advantage highlighted by authors in STL studies  [2] . The rationale behind STL has foundations borrowed from natural human learning. It is believed to rely on unlabeled data, which helps provide a solid foundation for high-level learning, thus generating more significant discriminative power  [2] . In computational terms, STL is relevant when: (i) there is little labeled data for training  [10] ; or (ii) despite having sufficiently enough labeled data, using examples from outside the classes of interest improves the learning process due to a greater generalization power  [11] .\n\nThis paper is mainly concerned with STL and its application in facial emotion recognition (FER). Facial expression is a natural and very significant way for human beings to convey their emotional state in the communication process. Researchers have investigated FER systems due to the various applications that benefit from their use, such as human interaction  [12] , disease diagnosis  [13] , virtual reality  [14] , augmented reality  [15] , and driver fatigue monitoring  [16] . FER is a task in which STL is feasible since there are several publicly available datasets, but the amount of labeled data for training robust models is scarce. Besides, the data variability is challenging due to the presence of people of different ages, skin colors, cultures, and genders  [9] . Consequently, we believe STL is an appropriate approach for developing a robust FER system since it bypasses the necessity of having large datasets with diverse and labeled data.\n\nIn such a context, this paper proposes strategies to generate unsupervised representations using the concept of STL with a focus on diversity. For such an aim, convolutional autoencoders (CAE) are trained considering strategies that promote diversity, such as different model initializations, architectures, and training data. The rationale is to investigate how complementary learned representations can contribute to the performance of a FER solution created using a small dataset.\n\nWe have two main research questions (RQs). The first one (RQ1) is related to the idea of exploring diverse unsupervised representations: \"Could the use of a pool of unsupervised representations contribute to the performance of a supervised classification model? The second question (RQ2) concerns the strategy used to promote diversity: Which could be the strategy used to promote diversity when creating a pool of unsupervised representations?\n\nTo answer these RQs, we investigate four strategies to generate a diverse pool of representations. First, the complementary of their members are evaluated, considering the performance of ensembles (Bagging and Random Forest) and a dynamic ensemble selection method (KnoraU)  [22]  produced based on them. Experiments on two FER databases using the leaveone-subject-out (LOSO) protocol have shown that promising results can be achieved by combining distinct representations. Moreover, the best strategies are based on varying the architecture of CAE.\n\nThis paper is organized as follows. Section II brings forwards works related to our research. Section III describes the proposed method for automatic representation generation using STL. Section IV presents our experimental results and a comparison with the state-of-the-art. Finally, Section V concludes this work and states envisioned future work.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Works",
      "text": "Proposed by  [2] , STL, also described as unsupervised transfer or transfer of learning of unlabeled data, is a machine learning framework that requires little human supervision. As a result, several authors have used STL in different classification applications such as audio  [1] , text  [3] ,  [6] , image  [4] , and sensor data  [5] .\n\nInitially, such a learning technique used out-of-distribution examples as a source of unlabeled data, showing positive effects in scenarios where labeled data was limited. The results presented in the seminal work of  [2] , who used a sparse surface coding for representation learning, showed that the relative gain of STL decreases as the number of labeled examples increases. However, studies using deep architectures have shown that such a positive effect is achieved even in a scenario with a large number of labeled examples  [17] . Another relevant characteristic of STL is that the deep layers of neural networks have hierarchically distributed characteristics that can be shared between tasks and data distributions.\n\nConcerning the application of STL for FER, two very interesting works can be highlighted. The first  [7]  describes a video FER system. An unsupervised feature learning method based on independent component analysis (ICA) learns spatiotemporal filters from natural videos, which are used to represent the images of facial expressions. They reported the area under the ROC curve for each of six classes of emotions present in the Cohn-Kanade database, ranging from 0.69 to 0.89.\n\nThe second one is the work presented in  [8]  that compares representations based on STL and transfer learning for FER. They trained base models on the CIFAR10 image dataset and applied them to the JAFFE expression recognition dataset. Their STL approach using a sparse autoencoder for feature extraction and a final CNN that receives the weights learned in the unsupervised step achieved an accuracy of 56.45%.\n\nWithout considering STL but exploring diversity to create ensembles of CNNs, the authors in  [18]  investigate different strategies for inducing diversity in an ensemble of CNNs applied to FER. The results on the FER2013 dataset showed that seed variation yielded the best recognition results while variations on the pre-training process of their CNNs achieved the best run-time performance.\n\nInspired by the works of  [8]  and  [18] , our idea is to investigate whether a diverse pool of representations learned using STL can bring some improvement in the accuracy of FER created using small FER datasets. We expect that unsupervised representations can generate ensembles composed of diverse members.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "This section describes the proposed method for generating pools of unsupervised representations using the concept of STL, detailing the different strategies evaluated to promote diversity. The rationale behind it is to investigate the impact of combining different but complementary unsupervised learned representations for the problem of FER. Fig.  1  presents a general overview of the proposed method, which is organized according to the three main steps of STL  [19] :\n\n• Representation Learning (Step 1): High-level representation is learned through unlabeled data, which does not necessarily present the same distribution as the labeled data of the target domain.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Representation Learning (Step 1)",
      "text": "This step learns in an unsupervised way high-level representations. To learn them from unlabeled data, given an input x u i ∈ R d the unsupervised representation learning method tries to find patterns (base vectors)\n\ni can be represented using a combination of a few basic functions as denoted in Eq. (1).\n\nwhere a u i,k is the combination of the coefficients of x u i , called activations and b u k are the base functions, i.e. high-level features. To allow that only some base vectors are used for an input x u i , only some activation values will be nonzero. Taking into account a set of unlabeled data, Eq. (  1 ) can be rewritten as:\n\nwhere\n\n, each a u i represents the coefficient vector of the given vector x u i . This equation decomposes the data matrix X u into two matrices A u , known as the activation matrix and B u , known as a dictionary array.\n\nWe can find in the literature robust evidence that greater diversity is highly correlated with the increase in supervised CNN-based ensembles accuracy  [18] ,  [21] . Thus, we decide to explore diversity to generate our ensemble of unsupervised representations as illustrated in Fig.  2 . The proposed algorithm uses a convolutional autoencoder (CAE) and different strategies to vary specific parameters, as follows:\n\n• Random Seed: here, different representations are generated by varying the distribution of weights during the CAE initialization process. The architecture is the same from one CAE to another, but the seed of the initialization process used differs. The input is the number of representations (R) to be generated with different seeds.\n\nThe algorithm randomly selects a seed from the range [0, 1000] at each iteration.\n\n• Training Dataset: in this case, the same CAE is trained on different unlabeled datasets. The promotion of diversity is clear but is also an opportunity to evaluate the use of datasets far from the target domain. In this direction, we have considered four datasets of different fields during our experiments. • Network Architecture: here, we explore the generation of representations using different CAE architectures. We must define the network depth (number of layers, D), the filters used in each one, and the size (I) of the intermediate (latent) layer. The generator will create the first architecture with a depth equal to the number of defined layers (D), then the second will have (D -1) up to the last architecture with depth D = 1, which has one input, one intermediate, and one output layer (basic structure of a CAE). The representations generated here differ on the CAE architectures' depth but use the same number of neurons (I) in the latent layer. • Latent Vector: in this strategy, we use the same CAE architecture, i.e., the encoder and decoder are the same for all generated representations. The diversity is obtained by varying the number of neurons in the middle layer of the network, named latent vector. The input is a list of L different sizes for the intermediate layer. Thus, L representations will be created with the sizes defined in that list.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Feature Building (Step 2)",
      "text": "According to the STL approach, after having learned the dictionary B u , from the input data X u , it is used to obtain the activations of the data labeled X l . Thus, the vectors labeled x l i can be represented as a combination of some base functions:\n\nwhere A l = {a l 1 , a l 2 , ...a lu M } ∈ R KxM is the activation matrix corresponding to the labeled data. This can be considered a new way of representing X l , where it is possible to assign to the original class y i each activation vector a l i , and then obtain a new representation for the target labeled data, which can be used to build some classifier in a supervised way.\n\nEach representation learned in the previous step (Diverse Representation on 1) is used to extract feature vectors from the target dataset. Thus, the unsupervised learned representations generate new feature sets using the different techniques described.\n\nIn this step, before extracting the features from the FER images, we pre-process them, cutting only the region of the area of interest (face) and selecting the reference points. Figure  4  illustrates the pre-processing applied to a sample image of Jaffe and CK datasets. To detect and cut out the face area of the image, we used the Viola-Jones face detection method. The reference points located in the cropped image are used to align the images, leaving them in the same position.\n\nThe pre-processed image is the input of each learned CAE, which use their encoder weights w 1 and bias b, to extract features a = h w1,b (x l ) from the labeled data (X l , Y ). Every learned representation is finally transformed using Principal Component Analysis (PCA)  [23]  to reduce its dimensionality, while decorrelating the learned features. We use this new representation a, with the label vector Y , to perform the third step of the STL approach, the classification.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Training A Classifier (Step 3)",
      "text": "At this point, we have the target datasets represented by R different strategies to generate supervised models. Thus, we evaluated a monolithic approach (SVM), two ensemble learning algorithms (Bagging and Random Forest -RF), and a dynamic ensemble selection method (Knora Union -KnoraU). Table  I  presents the parameters used for each technique. The motivation for using a single classifier, ensembles, and a dynamic selection method is to investigate the impact of the generated diversity.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experimental Results",
      "text": "The target datasets are the Japanese Female Facial Expression (JAFFE)  [20]  and the Cohn-Kanade (CK). JAFFE is a laboratory-controlled image database that has 213 images of 10 subjects (Japanese female models), with six basic facial expressions (happiness, anger, disgust, fear, sadness, and surprise) plus a neutral one. On JAFFE, all ten subjects have one or more images for each class. Another motivation for using JAFFE is that its images are grayscale, making them remarkably different from those used to learn our representations. CK is a laboratory-controlled database widely used to evaluate FER systems. The base contains 523 sequences from 123 subjects, 327 images of 118 subjects are labeled with  seven basic facial expression (anger, contempt, disgust, fear, joy, sad and surprise). On this database not all subjects have images in all classes.\n\nThe following datasets were used to generate the unsupervised representations (auxiliary datasets): a domain (FER) far from that dataset.\n\n• LFW: is a publicly available database of face photographs used for face verification, also known as peer matching.\n\nThe dataset has 13,233 images of faces collected on the web from 5,749 people. Unlike Kyoto, LFW was selected for our protocol because its images belong to a domain related to the FER problem. • LFW-Face: Similar to the LFW dataset, LFW-Face is close to our target domain. However, it uses only the bounding box of the face instead of the full image with the background.\n\n• STL-10: It is an image recognition database frequently used to develop unsupervised feature learning and deep models. The database has color images with 96x96 pixels categorized into ten classes. Here the labels were ignored. In our experimental protocol, we use the leave-one-subjectout (LOSO) cross validation. This protocol divides the database so that a subject in the test dataset cannot be in training. Suppose we have a dataset with N subjects, therefore, for each fold, one subject will be used as a test and the others for training. In such a protocol, when using the KNORAU dynamic ensemble selection, a subset of data (validation set) is separated from the training set. Such a validation set is necessary since KNORAU needs to compute the competence of each classifier.It is also essential to notice that all available images were used when training the CAEs with a specific auxiliary dataset. Moreover, in all experiments, except in Experiment 2 (varying the datasets), Kyoto was used as the auxiliary dataset.\n\nA set of five experiments were performed to answer our research questions. One to evaluate each of four strategies used to generate the unsupervised representations and a last one to combine all the results. Table I reports the parameter settings of the algorithms used to generate the supervised models in our experiments. The CAE parameters used in each experiment are shown in Table  II . The inducers were executed using the implementations from the scikit-learn Python library 1  , The CAEs were implemented using Keras 2  with TensorFlow, while the dynamic ensemble selection method KnoraU was implemented using Deslib 3  . A. Experiment 1 -Varying the Random Seeds\n\nTables III and IV show the results obtained with our first strategy considering R = 10 representations using different seeds to initialize the CAEs that were applied to represent each target dataset, JAFFE and CK, respectively. The CAEs were trained here using the Kyoto dataset. We can see that the fusion of all representations in the last three lines of these tables show a significant improvement no matter the classification approach used (SVM, Ensembles, or KnoraU). The best result for JAFFE was 56.60%, observed when combining the output of the RFs trained on each representation using the product rule, while the best outcome for CK was 84.39% observed when combining the output of the SVMs trained on each representation using the stacking fusion strategy.  respectively. Concerning the proximity of the auxiliary dataset to the target domain, we observed that the datasets far from the target domain presented the best results in 6 over ten experiments. Similar to the first set of experiments, we can see that the fusion of all representations in the last three lines of these tables show a significant improvement no matter the classification approach used. The best result for JAFFE was 61.69%, observed when combining the output of the SVMs trained on each representation using the stacking strategy, while the best outcome for CK was 86.92% observed using the same classification approach. In this experiment we have evaluated the use of different CAEs architectures. Tables VII and VIII show the results for JAFFE and CK datasets, respectively. We also can observe a significant contribution by combining the created representations. The best result for JAFFE was 59.67%, observed when combining the output of the SVMs trained on each representation using the stacking strategy, while the best outcome for CK was 87.21% observed using the same base classifier but with product rule. For the size of the architecture (D), we observed that the best results were achieved with architectures containing D >= 3 layers (6 over 10 experiments).  The results of our last strategy are shown in Tables IX and X. The same network architecture is used with different intermediate layer sizes (I). The best result for JAFFE was 62.26%, observed when combining the output of the SVMs trained on each representation using the stacking strategy, while the best outcome for CK was 86.99% observed using the same base classifier but with product rule. For the size of the latent vector (I), we observed that the best results were achieved with I >= 1500 (7 over 10 experiments).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Experiment 5 -Combining All The Representations",
      "text": "Finally, we have combined all the 29 representations based on different seeds  (10) , CAE architectures (5), latent vector sizes  (10) , and auxiliary datasets (4). The results for JAFFE and CK datasets are shown in Tables XI and XII. Concerning the JAFFE dataset, the best result (64.59% of accuracy) was observed when using KnoraU combining the output of RFs. For the CK dataset, the best accuracy was 89.22% observed when combining the output of the SVMs using the stacking strategy. It is well-known in the literature that the success of an ensemble or a dynamic selection method depends on a diverse pool of accurate classifiers. Thus, it may corroborate that the proposed strategies may generate diverse representations.\n\nTables XIII and XIV show that most of the proposed strategies compare favorably against related works representing the state-of-the-art when applying similar unsupervised techniques for JAFFE and CK datasets. In the case of JAFFE, our best result (64.59%) improved the state-of-art in 8.14 percentage points (from 56.45% to 64.59%). For CK, the area under the ROC curve (AUC) was computed to compare with the results reported in  [7] . We can observe in Table XIV that our results are better for most classes, losing only for sad. Moreover, we considered the seven classes available in the CK dataset in our experiments.  At this point, we can answer our two research questions. Answering RQ1, we can say that using a pool of unsupervised representations can contribute to the performance of the final classification process. Concerning RQ2, we have observed that the best approaches directly relate to the CAE architecture. Experiments have shown that varying the latent vector size (Jaffe experiments) and the number of CAE layers (CK experiments) were the most promising strategies.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "V. Conclusions And Future Work",
      "text": "This paper introduced different strategies to generate unsupervised representations obtained with STL for FER. These strategies explore diversity by varying the autoencoders' initialization, architecture, and training data. In addition, support vector machines, bagging, random forest, and a dynamic ensemble selection method (KNORA) were evaluated as classifiers that learn from the unsupervised STL representations provided.\n\nExperimental results on two well-known FER datasets shows that the proposed strategies compare favorably when compared with related works representing the state-of-the-art when applying similar unsupervised representation learning strategies. By answering the proposed research questions we can say that using a pool of unsupervised representations can contribute to the performance of a supervised model. Moreover, a promising strategy to generate diversity is by using CAEs with different architectures, i.e., varying the latent vector size and the number of CAE layers.\n\nIn future works, we intend to learn weights to combine the generated unsupervised representations. The idea is to optimize their fusion. In addition, we plan to apply the proposed strategies to other problems.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed method based on the three STL steps: (1) unsupervised",
      "page": 2
    },
    {
      "caption": "Figure 1: presents a",
      "page": 2
    },
    {
      "caption": "Figure 2: The proposed algorithm",
      "page": 3
    },
    {
      "caption": "Figure 4: illustrates the pre-processing applied to a sample image of",
      "page": 3
    },
    {
      "caption": "Figure 2: Automatic generator of representations based on different strategies.",
      "page": 4
    },
    {
      "caption": "Figure 3: Image samples. In the ﬁrst line images of the Kyoto database. In",
      "page": 4
    },
    {
      "caption": "Figure 4: Example of an original image, face detection, and landmark extraction",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CAE -\nSeeds\nUnlabeled\nDatasets": "CAE -\nNetwork\nArchitecture",
          "Input Length\nActivation\nLattent Vector\nEpochs\nLoss\nOptimizer\nFilter Convolution\nFilters\nNetwork Depth D": "Network Depth",
          "96X96X1\nReLU/Softmax\nI = 2500\n20\nmse\nSGD\n3x3\n16, 32, 64, 128\n5 encoder\n1 Lattent Vector\n5 decoder": "D = 5\nN = {D, D − 1...1}"
        },
        {
          "CAE -\nSeeds\nUnlabeled\nDatasets": "CAE -\nLattent Vector",
          "Input Length\nActivation\nLattent Vector\nEpochs\nLoss\nOptimizer\nFilter Convolution\nFilters\nNetwork Depth D": "Lattent Vector",
          "96X96X1\nReLU/Softmax\nI = 2500\n20\nmse\nSGD\n3x3\n16, 32, 64, 128\n5 encoder\n1 Lattent Vector\n5 decoder": "I = [150, 200, 250, 300, 400,\n500, 1000, 1500, 2000, 2500]"
        },
        {
          "CAE -\nSeeds\nUnlabeled\nDatasets": "PCA",
          "Input Length\nActivation\nLattent Vector\nEpochs\nLoss\nOptimizer\nFilter Convolution\nFilters\nNetwork Depth D": "# of components",
          "96X96X1\nReLU/Softmax\nI = 2500\n20\nmse\nSGD\n3x3\n16, 32, 64, 128\n5 encoder\n1 Lattent Vector\n5 decoder": "150"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Repr.": "Seed 1\nSeed 2\nSeed 3\nSeed 4\nSeed 5\nSeed 6\nSeed 7\nSeed 8\nSeed 9\nSeed 10",
          "SVM": "50.66\n29.83\n44.30\n34.27\n49.75\n52.90\n42.34\n48.34\n28.03\n35.33",
          "Ensembles\nBG\nRF": "37.75\n36.28\n38.84\n37.82\n33.06\n33.87\n33.22\n31.82\n39.40\n36.50\n40.67\n38.54\n36.58\n32.00\n43.64\n39.61\n32.72\n33.80\n34.01\n37.78",
          "KnoraU\nDT\nRF": "38.16\n35.42\n40.11\n35.58\n37.63\n33.47\n34.53\n34.86\n38.38\n34.60\n39.67\n38.91\n35.90\n34.77\n40.25\n40.50\n34.15\n34.79\n37.30\n35.85"
        },
        {
          "Repr.": "Sum\nProduct\nStacking",
          "SVM": "52.15\n51.74\n48.93",
          "Ensembles\nBG\nRF": "52.88\n56.53\n54.35\n56.60\n52.54\n54.67",
          "KnoraU\nDT\nRF": "56.20\n50.12\n56.20\n50.12\n55.68\n54.79"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Algorithm": "SVM",
          "Parameters": "Kernel\nPenalty Parameter\n(C)\nClass Weight\nProbability"
        },
        {
          "Algorithm": "BG with DT",
          "Parameters": "Max Depth\nTree Max Features\nNumber of Base Estimators\n% of Training Samples"
        },
        {
          "Algorithm": "RF",
          "Parameters": "Max Depth\nNumber of Trees\nOob score"
        },
        {
          "Algorithm": "KNORA",
          "Parameters": "Number of neighbors\npool\nclassiﬁers"
        },
        {
          "Algorithm": "Stacking",
          "Parameters": "Meta classiﬁer\nSolver\nPenalty parameter C"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Repr.": "Seed 1\nSeed 2\nSeed 3\nSeed 4\nSeed 5\nSeed 6\nSeed 7\nSeed 8\nSeed 9\nSeed 10",
          "SVM": "84.22\n82.10\n80.40\n84.06\n81.56\n78.09\n80.08\n82.71\n81.10\n80.93",
          "Ensembles\nBG\nRF": "61.01\n60.15\n57.90\n56.35\n63.98\n59.95\n66.51\n63.95\n63.41\n60.15\n63.03\n62.34\n66,86\n64.46\n64.06\n59.80\n64.60\n62.8\n60.50\n56.56",
          "KnoraU\nDT\nRF": "65.56\n60.90\n58.23\n58.96\n65.42\n60.12\n66.93\n65.53\n63.20\n61.07\n64.97\n62.16\n66.41\n67.00\n64.84\n59.92\n65.12\n62.86\n59.61\n58.26"
        },
        {
          "Repr.": "Sum\nProduct\nStacking",
          "SVM": "83.38\n83.03\n84.39",
          "Ensembles\nBG\nRF": "71.31\n70.83\n71.63\n69.77\n77.48\n72.64",
          "KnoraU\nDT\nRF": "70.83\n66.04\n69.77\n66.68\n72.64\n70.98"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Repr.": "Arch-1\nArch-2\nArch-3\nArch-4\nArch-5",
          "SVM": "53.41\n52.81\n56.39\n47.53\n50.15",
          "Ensembles\nBG\nRF": "40.15\n40.11\n41.09\n36.17\n37.39\n29.88\n30.82\n36.69\n41.69\n37.48",
          "KnoraU\nDT\nRF": "40.15\n37.95\n44.36\n35.45\n34.18\n36.46\n31.79\n38.08\n34.59\n40.25"
        },
        {
          "Repr.": "Sum\nProduct\nStacking",
          "SVM": "59.26\n58.76\n59.67",
          "Ensembles\nBG\nRF": "51.82\n49.95\n51.84\n50.53\n50.96\n50.41",
          "KnoraU\nDT\nRF": "50.00\n50.36\n50.45\n49.91\n51.34\n53.79"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Repr.": "Kyoto\nLFW-Face\nLFW\nSTL-10",
          "SVM": "60.35\n59.44\n58.94\n55.21",
          "Ensembles\nBG\nRF": "41.08\n35.55\n44.22\n38.39\n40.06\n38.71\n34.94\n35.04",
          "KnoraU\nDT\nRF": "43.40\n37.57\n41.48\n35.70\n37.71\n40.12\n35.51\n34.80"
        },
        {
          "Repr.": "Sum\nProduct\nStacking",
          "SVM": "59.40\n59.85\n61.69",
          "Ensembles\nBG\nRF": "50.27\n45.47\n50.33\n46.87\n51.72\n47.87",
          "KnoraU\nDT\nRF": "51.31\n46.92\n47.42\n51.85\n53.60\n47.08"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Repr.": "Arch-1\nArch-2\nArch-3\nArch-4\nArch-5",
          "SVM": "84.94\n86.56\n85.93\n86.75\n87.55",
          "Ensembles\nBG\nRF": "69.11\n67.07\n69.68\n61.48\n70.14\n65.42\n70.40\n66.63\n65.53\n66.83",
          "KnoraU\nDT\nRF": "69.85\n67.24\n69.54\n65.22\n69.43\n67.11\n70.48\n65.83\n65.53\n68.12"
        },
        {
          "Repr.": "Sum\nProduct\nStacking",
          "SVM": "87.20\n87.21\n86.85",
          "Ensembles\nBG\nRF": "74.12\n70.76\n76.94\n69.43\n78.20\n73.54",
          "KnoraU\nDT\nRF": "73.48\n70.31\n74.40\n70.31\n74.40\n72.83"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Repr.": "Kyoto\nLFW-Face\nLFW\nSTL-10",
          "SVM": "84.19\n85.08\n79.94\n83.07",
          "Ensembles\nBG\nRF": "64.90\n59.95\n63.74\n58.99\n65.11\n59.01\n65.84\n64.80",
          "KnoraU\nDT\nRF": "64.45\n61.85\n63.16\n60.19\n64.66\n59.77\n66.69\n65.72"
        },
        {
          "Repr.": "Sum\nProduct\nStacking",
          "SVM": "86.22\n86.01\n86.92",
          "Ensembles\nBG\nRF": "69.87\n64.26\n69.02\n64.83\n70.31\n68.89",
          "KnoraU\nDT\nRF": "70.01\n65.22\n70.81\n65.01\n69.08\n69.73"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Repr.": "150\n200\n250\n300\n400\n500\n1000\n1500\n2000\n2500",
          "SVM": "58.92\n59.85\n34.90\n59.92\n46.57\n59.96\n41.46\n50.49\n46.09\n51.33",
          "Ensembles\nBG\nRF": "34.29\n39.87\n41.68\n36.59\n34.11\n36.03\n33.53\n33.25\n32.74\n31.79\n42.77\n41.64\n37.28\n32.21\n43.54\n38.77\n37.81\n33.27\n41.66\n44.51",
          "KnoraU\nDT\nRF": "32.33\n37.91\n39.24\n37.28\n36.48\n34.41\n38.82\n34.24\n33.60\n37.08\n44.37\n41.73\n32.86\n30.32\n44.84\n41.59\n41.14\n34.71\n42.14\n39.99"
        },
        {
          "Repr.": "Sum\nProduct\nStacking",
          "SVM": "61.23\n60.23\n62.26",
          "Ensembles\nBG\nRF": "59.52\n56.25\n59.46\n57.66\n60.53\n58.09",
          "KnoraU\nDT\nRF": "59.83\n57.64\n59.33\n57.64\n58.86\n59.33"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Repr.": "150\n200\n250\n300\n400\n500\n1000\n1500\n2000\n2500",
          "SVM": "77.68\n84.13\n82.28\n83.95\n84.95\n87.06\n86.46\n86.54\n86.96\n85.97",
          "Ensembles\nBG\nRF": "63.51\n58.47\n63.10\n63.54\n64.73\n61.37\n63.02\n58.88\n64.78\n61.08\n71.07\n63.37\n68.82\n66.34\n67.76\n64.15\n71.96\n65.28\n68.33\n63.10",
          "KnoraU\nDT\nRF": "64.61\n59.42\n63.31\n64.19\n66.49\n64.12\n64.60\n60.19\n64.75\n60.70\n71.52\n65.50\n67.55\n66.76\n68.94\n65.14\n72.59\n67.21\n67.06\n64.98"
        },
        {
          "Repr.": "Sum\nProduct\nStacking",
          "SVM": "86.96\n86.99\n86.29",
          "Ensembles\nBG\nRF": "75.46\n66.97\n76.31\n67.25\n78.79\n74.30",
          "KnoraU\nDT\nRF": "75.29\n67.79\n75.50\n67.79\n76.49\n72.37"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Music genre classification using self-taught learning via sparse coding",
      "authors": [
        "K Markov",
        "T Matsui"
      ],
      "year": "2012",
      "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "2",
      "title": "Self-taught learning: transfer learning from unlabeled data",
      "authors": [
        "R Raina",
        "A Battle",
        "H Lee",
        "B Packer",
        "A Ng"
      ],
      "year": "2007",
      "venue": "Proceedings of the 24th international conference on Machine learning"
    },
    {
      "citation_id": "3",
      "title": "An insight analysis and detection of drug-abuse risk behavior on Twitter with self-taught deep learning",
      "authors": [
        "H Hu",
        "N Phan",
        "S Chun",
        "J Geller",
        "H Vo",
        "X Ye",
        ". Dou"
      ],
      "year": "2019",
      "venue": "Computational Social Networks"
    },
    {
      "citation_id": "4",
      "title": "Self-taught low-rank coding for visual learning",
      "authors": [
        "Sheng Li",
        "Kang Li",
        "Yun Fu"
      ],
      "year": "2017",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "5",
      "title": "Self-taught learning based on sparse autoencoder for e-nose in wound infection detection",
      "authors": [
        "P He",
        "P Jia",
        "S Qiao",
        "S Duan"
      ],
      "year": "2017",
      "venue": "Sensors"
    },
    {
      "citation_id": "6",
      "title": "Intrusion detection using deep sparse auto-encoder and self-taught learning",
      "authors": [
        "A Qureshi",
        "A Khan",
        "N Shamim",
        "M Durad"
      ],
      "year": "2020",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "7",
      "title": "Learning spatiotemporal features by using independent component analysis withapplication to facial expression recognition",
      "authors": [
        "F Long",
        "T Wu",
        "J Movellan",
        "M Bartlett",
        "G Littlewort"
      ],
      "year": "2012",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "8",
      "title": "Analysis for self-taught and transfer learning based approaches for emotion recognition",
      "authors": [
        "P Bhandari",
        "R Bijarniya",
        "S Chatterjee",
        "M Kolekar"
      ],
      "year": "2018",
      "venue": "2018 5th International Conference on Signal Processing and Integrated Networks (SPIN)"
    },
    {
      "citation_id": "9",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan",
        "Q Yang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on knowledge and data engineering"
    },
    {
      "citation_id": "10",
      "title": "Deep self-taught learning for remote sensing image classification",
      "authors": [
        "A Bettge",
        "R Roscher",
        "S Wenzel"
      ],
      "year": "2017",
      "venue": "Deep self-taught learning for remote sensing image classification",
      "arxiv": "arXiv:1710.07096"
    },
    {
      "citation_id": "11",
      "title": "Deep self-taught learning for handwritten character recognition",
      "authors": [
        "F Bastien",
        "Y Bengio",
        "A Bergeron",
        "N Boulanger-Lewandowski",
        "T Breuel",
        "Y Chherawala",
        ". Sicard"
      ],
      "year": "2010",
      "venue": "Deep self-taught learning for handwritten character recognition",
      "arxiv": "arXiv:1009.3589"
    },
    {
      "citation_id": "12",
      "title": "Learning social relation traits from face images",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "13",
      "title": "Visually interpretable representation learning for depression recognition from facial images",
      "authors": [
        "X Zhou",
        "K Jin",
        "Y Shang",
        "G Guo"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Eyemotion: Classifying facial expressions in VR using eye-tracking cameras",
      "authors": [
        "S Hickson",
        "N Dufour",
        "A Sud",
        "V Kwatra",
        "I Essa"
      ],
      "year": "2019",
      "venue": "2019 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "15",
      "title": "Augmented reality-based selffacial modeling to promote the emotional expression and social skills of adolescents with autism spectrum disorders",
      "authors": [
        "C Chen",
        "I Lee",
        "L Lin"
      ],
      "year": "2015",
      "venue": "Research in developmental disabilities"
    },
    {
      "citation_id": "16",
      "title": "A smartphone-based driver fatigue detection using fusion of multiple real-time facial features",
      "authors": [
        "Y Qiao",
        "K Zeng",
        "L Xu",
        "X Yin"
      ],
      "year": "2016",
      "venue": "2016 13th IEEE Annual Consumer Communications & Networking Conference (CCNC)"
    },
    {
      "citation_id": "17",
      "title": "Why does unsupervised pre-training help deep learning",
      "authors": [
        "D Erhan",
        "A Courville",
        "Y Bengio",
        "P Vincent"
      ],
      "year": "2010",
      "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics"
    },
    {
      "citation_id": "18",
      "title": "Comparing ensemble strategies for deep learning: An application to facial expression recognition",
      "authors": [
        "A Renda",
        "M Barsacchi",
        "A Bechini",
        "F Marcelloni"
      ],
      "year": "2019",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "19",
      "title": "Supervised selftaught learning: Actively transferring knowledge from unlabeled data",
      "authors": [
        "K Huang",
        "Z Xu",
        "I King",
        "M Lyu",
        "C Campbell"
      ],
      "year": "2009",
      "venue": "2009 International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "20",
      "title": "J Coding facial expressions with gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE international conference on automatic face and gesture recognition"
    },
    {
      "citation_id": "21",
      "title": "Deep Neural Network Ensembles Against Deception: Ensemble Diversity, Accuracy and Robustness",
      "authors": [
        "L Liu"
      ],
      "year": "2019",
      "venue": "Deep Neural Network Ensembles Against Deception: Ensemble Diversity, Accuracy and Robustness"
    },
    {
      "citation_id": "22",
      "title": "From dynamic classifier selection to dynamic ensemble selection",
      "authors": [
        "A Ko",
        "R Sabourin",
        "A Britto"
      ],
      "year": "2008",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "23",
      "title": "Discarding variables in a principal component analysis. II: Real data",
      "authors": [
        "I Jolliffe"
      ],
      "year": "1973",
      "venue": "Journal of the Royal Statistical Society: Series C (Applied Statistics)"
    }
  ]
}