{
  "paper_id": "2009.01822v2",
  "title": "Fine-Grained Early Frequency Attention For Deep Speaker Representation Learning",
  "published": "2020-09-03T17:40:27Z",
  "authors": [
    "Amirhossein Hajavi",
    "Ali Etemad"
  ],
  "keywords": [
    "Deep Learning",
    "Speaker Representation Learning",
    "Frequency Attention",
    "Early Attention",
    "Fine-grained Attention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deep learning techniques have considerably improved speech processing in recent years. Speaker representations extracted by deep learning models are being used in a wide range of tasks such as speaker recognition and speech emotion recognition. Attention mechanisms have started to play an important role in improving deep learning models in the field of speech processing. Nonetheless, despite the fact that important speakerrelated information can be embedded in individual frequencybins of the input spectral representations, current attention models are unable to attend to fine-grained information items in spectral representations. In this paper we propose Fine-grained Early Frequency Attention (FEFA) for speaker representation learning. Our model is a simple and lightweight model that can be integrated into various CNN pipelines and is capable of focusing on information items as small as frequency-bins. We evaluate the proposed model on three tasks of speaker recognition, speech emotion recognition, and spoken digit recognition. We use Three widely used public datasets, namely VoxCeleb, IEMOCAP, and Free Spoken Digit Dataset for our experiments. We attach FEFA to several prominent deep learning models and evaluate its impact on the final performance. We also compare our work with other related works in the area. Our experiments show that by adding FEFA to different CNN architectures, performance is consistently improved by substantial margins, and the models equipped with FEFA outperform all the other attentive models. We also test our model against different levels of added noise showing improvements in robustness and less sensitivity compared to the backbone networks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Audio recordings of human speech often contain specific features from which a number of characteristics can be determined regarding the speaker. Speaker identity and emotional state are two of such characteristics that have been the subject of a number studies in recent years  [1] ,  [2] . Analyzing speaker-related information for applications such as Speaker Recognition (SR) and Speech Emotion Recognition (SER) often requires the use of spectral representations of audio signals. These frequency features undergo different processes inside automated systems to generate a representation of the utterance, namely the speaker representation. The speaker representations ideally contain all the information required for\n\nThe authors would like to thank IMRSV Data Labs for their support of this work. The authors would also like to acknowledge the Natural Sciences and Engineering Research Council of Canada (NSERC) for supporting this research (grant number: CRDPJ 533919-18).\n\nperforming SR or SER. In classical solutions, i-Vector systems  [3]  generated these speaker representations under the name of identity vectors.\n\nMore recently, with advancements in Deep Neural Networks (DNN), speaker representation learning through DNNs has gained considerable attention. As a result, a significant number of DNNs have been explored for SR  [4] ,  [5]  and SER  [6] ,  [7] , respectively. The quality of speaker representations generated by DNNs far surpasses the ones generated by i-Vector systems  [8] ,  [9] , making DNNs the predominant solutions for speakerrelated tasks.\n\nAttention mechanisms have been an integral part of the recent advancements in deep learning  [10] ,  [11] . To describe the general process of Attention mechanisms, they take information items as input and assign learnable weights to each item  [10] . This process helps deep learning models to select the most valuable items to use further down in the pipeline, for instance for classification or regression tasks. Some examples of information items commonly used in attention mechanisms in SR and SER are the embeddings obtained from Convolutional Neural Networks (CNN)  [12] ,  [13] , Recurrent Neural Network (RNN)  [7] ,  [14] , or Time-Delay Neural Network (TDNN)  [15] ,  [16] .\n\nAn attention mechanism calculates the aforementioned weights during the training of the model using backpropagation by receiving gradients from the layer that follows it in the DNN  [10] . In the context of generating speaker representations from audio signals, the information items used by the majority of attention mechanisms  [12] ,  [17] ,  [18] ,  [19] ,  [20] ,  [21] ,  [22] ,  [7] ,  [14] ,  [16]  are the embeddings generated by encoding the input via a CNN module while the gradients are usually derived from the final layer of the model. Using the CNN embeddings of the utterances as information items, makes it so that every information item corresponds to a specific region of the input.\n\nThe most commonly used input for speaker representation learning is the spectral representations of an utterance, in which each individual input value is a frequency-bin. However, despite the fact that specific speaker-related information may be contained in individual frequency-bins, existing attention mechanisms have not yet treated these individual bins as information items. Instead, these mechanisms have so far used particular regions  [12] ,  [13] ,  [7] ,  [14] ,  [15] ,  [16] ,  [23]  of the input spectral representations as information items, thus suffering from low granularity.\n\nImproving the granularity of CNN embeddings used as information items results in a drastic increase in the dimensionality of the embeddings themselves and in turn leads to very large attention models. Such large attention models are generally hard to train  [10] ,  [11] . Hence, despite the abundance of studies investigating various attention models for speaker representation learning  [13] ,  [12] ,  [17] ,  [18] ,  [19] ,  [20] ,  [21] ,  [22] , very limited number of works have aimed to use more fine-grained attention models for SR and SER.\n\nIn this paper, we address the challenge of improving granularity of attention mechanisms for speaker representation learning by introducing the Fine-grained Early Frequency Attention (FEFA). This mechanism enables deep learning models to focus on individual frequency-bins of spectrogram representations without the drawbacks of having very complex attention models that typically involve many parameters. The aim of this model is to attend to each frequency-bin in the spectrogram representation in order to boost the contribution of salient bins. This mechanism also helps reduce the importance of bins with no useful information which in turn leads to more accurate representations, and thus leading to more robustness to the presence of noise. We study the impact of the proposed attention mechanism on the performance of different backbone models for two widely used datasets, VoxCeleb  [5]  and IEMOCAP  [24] , for SR and SER respectively. Additionally we experiment with the Free Spoken Digit Dataset (FSDD)  [25]  dataset as a simple additional dataset in order to fully analyze the different aspects of our work. The experimental results show that deploying FEFA in different models improves the performance of all the benchmark networks substantially while being less impacted by added noise.\n\nOur contributions in this paper are as follows:\n\n• We introduce a novel attention mechanism, FEFA, for speaker representation learning. Our model is a simple, yet effective module that can be easily integrated into existing DNN pipelines used for audio representation learning to boost their performance without requiring any modification to the DNN architecture. • We evaluate our method on two different speaker-related problem domains, namely SR and SER, using two large and widely used datasets, demonstrating considerable performance gains for both tasks. Our analysis shows that FEFA improves the quality of the obtained embeddings and boosts performance with little added complexity.\n\n• By testing our model against different levels of synthetic noise, we show an improvement in robustness compared to other models.\n\nThis paper is an extension of our work on 'Early Frequency Attention', presented at IJCNN 2022. The added contributions of this paper compared to the conference version can be summarized as follows:  (1)  We enhance the mechanism of the attention module by adding fully connected layers in addition to locally connected layers.  (2)  We add additional analysis and experiments on 2 other tasks of spoken digit recognition and speech emotion recognition in order to show the functionality and generalizability of our model.  (3)  We analyze the complexity of the proposed model and compare it to other forms of the attention mechanisms and show that despite using fewer number of parameters, our approach outperforms other solutions.\n\nThe rest of this paper is organized as follows. First, we discuss the related work in the area of speech representation learning with a focus on speaker-related tasks and explore particular approaches that have used attention mechanisms for this purpose. Next, we present our proposed method. Following, we discuss the experiments along with implementation details. Next, we provide the results and analysis of our experiments. And finally, we summarize and conclude the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Speaker Representation Learning",
      "text": "Extracting speaker representations or utterance embeddings, with the aim of acquiring speaker related information, has been extensively studied over the years. Classical techniques such as Gaussian Mixture Models  [26] , Hidden Markov Models  [27] , and Universal Background Models  [28] , have been used to obtain effective utterance representations that contain important information regarding the speaker of utterances. Comprehensive reviews of prior works that have used such conventional methods, particularly for SR and SER, can be found in  [1] ,  [29] .\n\nAlong with the classical techniques, some attempts were made to utilize artificial neural networks (ANN) for SR and SER. In some of the earlier works in this area, utterance embeddings extracted from audio signals using ANNs were fed to conventional classifiers to perform SR  [30]  and SER  [31] . The ANN-based solutions were able to achieve better performances compared to previous classical techniques. However, with the introduction of methods such as i-Vectors  [3]  and the computational requirements of ANNs, further studies did not consider ANNs as a viable option.\n\nRecent advancements in DNNs has led to a renewed interest in the use of such methods for learning effective representations of utterances for SR and SER. Most recent works on extracting deep utterance embeddings for these tasks have explored the impacts of different deep learning architectures on the quality of these representations  [13] ,  [12] ,  [17] ,  [18] ,  [19] ,  [20] ,  [21] ,  [22] . Most prominent works use CNN architectures such as ResNets for embedding speech representations towards performing SR  [32] ,  [12] ,  [4]  and using a combination of CNNs and RNNs such as long short-term memory (LSTM) for SER  [33] ,  [34] ,  [7] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Attention Mechanisms For Speaker Representation Learning",
      "text": "Attention mechanisms have been used to improve the performance of deep learning models  [44] ,  [45] ,  [46] . Subsequently, a large number of studies using attention mechanisms for SR and SER have shown substantial improvements compared to baseline models. These improvements have been achieved by simply utilizing such mechanisms to focus on features extracted from utterances by deep learning models with various architectures including CNN  [12] ,  [13] ,  [17] ,  [18] ,  [38] ,  [22] , RNN  [14] ,  [7] ,  [35] ,  [40] ,  [41] , and time-delay neural networks (TDNN)  [15] ,  [16] ,  [23] . Through the following paragraphs we briefly describe the attention mechanisms used in these studies.\n\nThe model proposed in  [13]  utilized a self-attention mechanism to focus on features obtained from a CNN model inspired by VGGNet  [47]  to generate utterance embeddings with more useful information for performing SR. The study done in  [12]  used CNN layers inside a self-attention mechanism to attend to features extracted from the utterance using a deep learning model based on the ResNet architecture  [48] . The work done in  [17]  proposed a gated attention mechanism that focused on features extracted from the utterance by a modified version of CNN, namely gated-CNN. In all of these studies, the use of attention mechanisms has enabled the models to focus on particular features extracted by the DNN, resulting in enhanced embeddings in comparison to the non-attentive solutions for SR.\n\nAlternatively, the proposed models in  [14] ,  [7] ,  [35] , utilized attention mechanisms to focus on differences between two sets of extracted utterance embeddings. These embeddings were extracted from the reference and test utterances using RNNs. In the common approach taken in these studies, attention models were added to the deep learning pipelines in the backend of the speaker verification system where the final similarity score is calculated. These studies showed that utilizing attention models in this way helps improve the accuracy of baseline models for in-the-wild datasets.\n\nA different approach was taken in  [15]  and  [23] . The attention models used in these studies replaced the statistical pooling layer of an X-Vector model. The proposed models utilized TDNN to extract features from short frames of the utterances. Attention models were then used to aggregate the features into an utterance-level embedding. The model proposed in  [23]  was evaluated against the NistSRE16 evaluation set  [49]  and the proposed model in  [15]  was evaluated against the VoxCeleb test set  [8] . Both models showed substantial improvements compared to their baseline models.\n\nIn Table  II -A, we summarize the prominent studies that use attentive models for SR and SER. These studies are grouped based on the task (SR/SER), and then by the type of attention mechanisms used. The table also presents the type of networks used. We also review the evaluation datasets, whether they are in-the-wild, and the type of features used as input to each system, in order to obtain a better picture of the type of data used with existing attention mechanisms for SR and SER. For completeness, we also include the main loss functions used in each study. Lastly we state whether each of the prior works are 'early' mechanisms and 'fine-grained' or not. We define early attention as those which are applied to the input spectrograms prior to processing by the DNN, while fine-grained is defined as the ability of the attention mechanism to consider and focus on each frequency-bin individually.  II -A, we make the following observations. First, it can be seen that despite attention mechanisms being widely used in SR, their use for SER has been limited. Second, we observe that the majority of attention mechanisms proposed in the literature use the features obtained from DNNs as the information items. Moreover, the queries of the attention models have been originated from a latent layer of the model from which the utterance-level embeddings are retrieved. Generally, DNNs learn to extract a low-dimensional latent representation from the input data without necessarily preserving localization with respect to the input information items. Thus, while the use of the latent layer of a DNN for extracting the query of an attention mechanism can be advantageous due to its reduced number of parameters, high levels of granularity and a localized relationship with respect to the input may not be achieved. As a result, most prior attention-based methods do not possess the ability to attend to highly granular individual input frequency features. Compared to the methods proposed in previous studies, our fine-grained attention model proposed in this paper does not rely on embeddings obtained from DNNs, and instead operates on spectrograms extracted from raw audio signals. Hence, the granularity of the attention model can be improved to attend to frequency-level features.",
      "page_start": 2,
      "page_end": 4
    },
    {
      "section_name": "Based On Table",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Method A. Overview",
      "text": "In this section we first describe the individual speech-based information items which our model intends to use for the proposed attention mechanism. Next we present our proposed method, FEFA, which is a special form of general attention specifically designed for being integrated into speech processing pipelines and using spectral information for enhancing the learned audio representations. We then define the kernel used in our model and propose two different kernel types, which we experiment with later in the next Section. Lastly we describe the different approaches for integrating FEFA into existing CNN speech-related pipelines.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Proposed Mechanism",
      "text": "Information Items. The spectrogram representation of speech signals is a frequently used input among deep learning models that exploit CNN architectures. While the number of frequency-bins may vary in different studies, the overall approach in calculating and using spectrogram representations are quite similar. The spectrogram representation of an utterance is obtained using Short-time Fourier Transform (STFT) as follows\n\nwhere x(t) serves as the signal amplitude at a given time t. M (t-τ ) is a mask function applied over the signal to enforce the time window of the STFT as well as to extract the phase information of the signal. ω represents the frequency band around which the STFT is performed. The calculated STFT values for different frequency bands ω, are then squared over a time window of τ , where the default value is set to 25 ms, as follows:\n\nThe value of 25 ms is selected for τ through a series of experiments. Values lower than 25 ms are observed to degrade the performance of DNN models. This may be due to the limited range of frequencies that can be extracted from such small window sizes. For τ values larger than 25 ms, no noticeable improvement is observed in the performance of DNN models, however, the computational load rises exponentially.\n\nFinally the spectrogram representation of the speech signal is obtained by repeating this process for a select number of frequency bands. The frequency bands are generally selected as a hyper-parameter in the form of a set of filters called filterbank. Each value acquired by function S (x(t), ω i ) represents the frequency information of the signal with regards to the frequency band ω i at a given time in a 25 ms time-window. By having the frequency-bins as the construction blocks of the spectrogram representations, every individual bin can be considered the smallest item carrying information, i.e., information item. FEFA. The fundamental paradigm of a general attention mechanism is the memory-query system. In general attention mechanisms used for audio signals, the memory typically consists of a set of information items, namely DNN embeddings of utterances, while the query is acquired from the layer immediately after the attention module in the DNN. The memory set M is saved in the form of key-value tuples (k i , x i ). The first element of the tuple k i helps with the calculation of the probability factor p i , which indicates the impact of the item in the output of the attention mechanism. This probability factor is calculated by\n\nwhere W is a set of trainable weights learned by the attention mechanism. The final output of the attention mechanism, O, with respect to the query q, is the expected value of items with regards to x i as follows\n\nWith this formulation and by taking into consideration that the source of query is usually the final layer of the DNN, using the frequency-bins as x i will result in a very large set of weights W , which in turn increases the complexity of the attention module. However, by localizing q to individual frequency-bins through changing the source of the query to the first layer of the DNN (illustrated in Figure  1 ), W can be divided into multiple smaller groups denoted by W i each corresponding to a frequency-bin. Thus, the probability-factor p i can be calculated using the following\n\nwhere the weight group W i can be as small as a single scalar. Here I is the index of the frequency bin in the input vector which acts as the positional encoding of the frequency bin in the attention mechanism.We use one-hot encoding for positional coding of the frequency-bins. Subsequently the output of the attention module, namely the attention map A, can be calculated as follows where S is the frequency-bin used as information items calculated in Eq. 2 and |M | is number of bins. The attention map acquired from FEFA is then multiplied by the original spectrogram representation of the utterance to enhance its representation for subsequent use by the DNN. The memory and computational complexities of FEFA are respectively linear (Θ(n)) and quadratic (Θ(n 2 )) with regards to the number of frequency-bins |M | in the spectrogram representation. Hence adding multiple layers of FEFA throughout the DNN pipeline does not drastically increase the computational complexity of the model. We perform empirical experiments on the complexity of our proposed method later in Section V-E. As another advantage of our approach, FEFA can be integrated into various deep learning pipelines without any need to change their architecture as long as they use spectrogram representations of utterances as input. This plug-andplay characteristic of our method makes it desirable, easy to use, and compatible with any DNN pipeline, as demonstrated later in Section IV. Kernel Type. We use an MLP as the main kernel for FEFA as illustrated in Figure  1 . Accordingly we define two different configurations for this kernel:\n\nUsing a locally connected layer (see Figure  2 (a) ) simply assigns a specific weight to each individual frequency-bin. This implies that the weight of the corresponding frequencybin in the attention map is only determined based on the value and the index of the frequency-bin itself and the query, while any information from other frequency-bins is ignored. In our experiments, we denote this mechanism by FEFA(LC). On the other hand, by using a fully connected layer (see Figure  2 (b) ), the model considers all the frequency-bins to determine the weight assigned to each frequency-bin in the final attention map. This is denoted in our experiments by FEFA(FC). In Section IV we perform extensive experiments to study both kernel types in different settings. Kernel Integration. As discussed earlier, FEFA (with either kernel type) can be easily integrated into different CNN architectures that learn and process speech representations. It is noteworthy to add that while we believe FEFA would benefit from integration into the first layer of CNNs so that individual salient frequency-bins can be attended to, it is highly feasible to integrate the module prior to every convolution layer of the pipeline. Accordingly, we define two integration methods for FEFA:\n\n• Single integration in the first layer;\n\n• Multiple integrations, one prior to every layer. The definition of the second category is inspired by  [43]  where an attention mechanism was integrated prior to every layer of CNN pipelines, and it was discussed that employing this strategy benefits the learned representations. We skip the integration of FEFA at the last layer of the pipeline as it is similar to the conventional attention models which have been studied in prior work  [15] ,  [12] ,  [50] . In Section V-B we perform extensive experiments on both strategies (Single vs. Multi) and demonstrate that FEFA is best suited for a single-integration into different CNNs. This further adds to the ease-of-use of our model and the need for very little added computations for learning enhanced audio representations.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments",
      "text": "In order to extensively evaluate FEFA along with its different kernel types and integration strategies, we select two tasks on speaker representation learning, namely SR and SER. Additionally, we include spoken digit recognition as the third and a more general, yet simple task, on which various aspects of our method can be easily studied. As mentioned earlier, FEFA can be integrated into different CNN backbones that take spectrogram representations of utterances as input. Hence we use a select number of prominent CNN architectures that are commonly used for these tasks as our benchmarks. In the following subsections, we introduce the datasets used in our experiments, implementation details regarding FEFA, as well as the details of the backbone networks used in which we integrate FEFA.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Datasets",
      "text": "Here we describe the three datasets used for our experiments. For the task of speaker recognition, we experiment on the widely used VoxCeleb dataset  [5] , while we use the IEMOCAP dataset  [24]  for the task of speaker emotion recognition. Lastly we use the Free Spoken Digit Dataset (FSDD)  [25]  for the spoken digit recognition task. Following we provide further details on each of these datasets. VoxCeleb. For the SR task we perform our evaluations using the large and widely used in-the-wild VoxCeleb dataset  [5] . This dataset includes voices from more than 6,000 individuals. The utterances are captured in uncontrolled conditions such as interviews published in open-source media. The VoxCeleb dataset is available in two versions, VoxCeleb1 which is used more commonly for evaluation, and VexCeleb2 which is used solely for training purposes. In this experiment we follow the common practice and use the VoxCeleb2 dataset with nearly 1.2 million utterances for training our model and VoxCeleb1 test set for evaluation. Similar to prior work such as  [12] ,  [15] ,  [19] ,  [21] ,  [20] ,  [18] ,  [39] ,  [42] , and  [43] , we use the entirety of the VoxCeleb2 dataset (not VoxCeleb1) for training. We then test the solution on the 38,000 trial pairs (4,874 unique utterances selected by the authors of VoxCeleb, from VoxCeleb1 dataset) spoken by 40 individuals. The trial pairs are labeled by \"1\" and \"0\", indicating whether both utterances in the pair are spoken by the same person or not. The number of pairs labeled as 0 or 1 is completely balanced. The details of the distributions of the instances between different splits are mentioned in Table  II . IEMOCAP. For the task of SER, we use the IEMOCAP dataset  [24] , which is widely used in the field. This dataset is a multi-modal emotion recognition dataset including speech recordings, videos, and motion capture. The dataset contains 12 hours of prompted and improvised dialogue performed by 10 actors. The audio recordings of the dataset are divided into short utterances each containing one sentence. Each utterance is then scored by several people to determine the category of emotion conveyed by the utterance. Four emotion categories, namely Sadness, Happiness, Angry, and Neutral, for a total of 6 thousand utterances, are used. We use k-fold cross validation with k = 5 for training and evaluating the backbone networks with and without FEFA. In order to maintain the balance between the classes of dataset we select equal number of utterances for each class both in the training split and test split during each run of cross validation process. This approach has been used in literature such as  [35] ,  [36] ,  [37] , and  [51] . Table  II  shows the number of instances used for training and testing in each fold as well as the number of instances for each class in the splits. FSDD. The free spoken digit dataset  [25]  is an open source dataset, consisting of recordings from 6 individuals, each uttering a single digit 40 times resulting in a total number of utterances equal to 2400 utterances. Most of the utterances have a duration of approximately 1 second with the longest utterance being 1.5 seconds. We have zero-padded the utterances to a fixed length of 1.5 seconds to maintain consistency throughout the dataset. These utterances are labeled from zero to nine corresponding to the digit spoken in the utterance. Unlike the other datasets used in this paper, there are no standard procedures for splitting FSDD into training and test sets. Therefore we select the standard method of splitting the dataset into 3 sets of training, validation, and test sets with the respective ratios of 70%, 10%, and 20% while maintaining a complete balance between the classes of utterances. The number of instances used for training,validation, and testing splits of the FSDD dataset are also presented in Table  II .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Data Preparation",
      "text": "For data preparation, we extract spectrogram representations of the utterances using 512 filter bands which are generated using \"hann\" function. The spectrograms are extracted using STFT on overlapping time windows with size of 25 ms with a stride of 10 ms. This results in spectral images of size 257×T where T is equal to the number of strides required for the time window τ to cover the entire utterance and in turn is equal to the dimensionality of the spectrograms in the time axis. We use 257 frequency-bins as it is standard practice in  [32] ,  [5] ,  [8] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Fefa Modules",
      "text": "For our proposed module, we use the average pooling layer with the input size of 257 × T , and output size of 257. In the LC configuration of FEFA the output of the average pooling layer is given to an LC layer with the output shape of 257 × 1.\n\nIn the FC configuration of FEFA the output of the average pooling layer is given to two FC layers each with 257 hidden units which results in an output with a shape of 257 × 1. The output of either LC or FC layers is then passed through a sigmoid activation function that generates a 257 × 1 attention map.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Backbone Models",
      "text": "FEFA can be attached to any backbone network that uses spectrograms as input. This makes FEFA a simple standalone module that is independent of the DNN architectures used for performing speaker representation learning. We select three popular CNN architectures as the backbones in which to integrate FEFA and its different configurations. The choice of these backbones is based on a survey of the literature to identify the common architectures used for audio-representation learning  [4] ,  [32] ,  [5] ,  [8] ,  [52] ,  [12] ,  [19] ,  [21] ,  [33] ,  [53] ,  [43] ,  [54] ,  [55] ,  [56] ,  [57] ,  [58] ,  [59] . The models proposed in these studies use architectures based on the selected backbone models and they are often compared to these backbone networks in their experiments.\n\nThe first backbone CNN used in our experiments is the VGG-based model proposed in  [8] , which consists of 5 convolution layers accompanied by 3 maxpooling layers. The utterance-level aggregation is done using a global-averagepooling and the final embedding is acquired using an FC layer with ReLU activation.\n\nThe second network that we use in this experiment is the ResNet-based model proposed in  [32] . This model consists of 35 convolution layers used in the form of residual blocks. The shortcuts integrated in residual blocks help the model convey the learning gradients throughout the pipeline of the model more easily, which in turn aids the model to learn faster and more effectively. This also enables the model to provide better queries for FEFA. The complete details about the hyperparameters of this backbone can be found in  [32] .\n\nThe final network used in our experiments is SEResNet. Similar to the ResNet model, the SEResNet consists of residual blocks. The formation of blocks and number of parameters used in the SEResNet is similar to a ResNet with an addition of a Squeeze-and-Excitation (SE) module  [60] . The SE module uses a global pooling layer to extract channel information inside the residual blocks. The channel information is then projected onto a latent space using 2 FC layers, a ReLU activation function, and a Sigmoid activation function. The resulting representation is then multiplied across channels of the ID block.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "E. Training",
      "text": "We use the cyclical learning rate proposed in  [61]  for training the backbone networks with the added FEFA module. This technique helps the model to achieve a better convergence by changing the learning rate periodically and preventing the model from getting trapped in local minima. An initial learning rate of 10 -4 is used. Throughout the entire training process, the models are trained using a single Nvidia Titan RTX (24 GB vRAM) GPU. We use a batch-size of 64 and Adam optimizer  [62]  for training. The values selected for the hyperparameters are obtained empirically. The training time of the backbone models do not experience any noticeable change after attaching the FEFA modules. This may be due to the fact that the number of parameters of the modules are considerably low, especially in comparison to the total number parameters in the backbone models. This shows that our proposed solution can be added to",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Results",
      "text": "In this section we provide the results of our experiments on the aforementioned datasets. First we illustrate the impact of integrating FEFA into different backbones on the training process. We then present the results of different backbones with different FEFA architectures in comparison to a large number of benchmarks. Through the following section, we experiment with added synthetic noise in order to test the robustness of FEFA in comparison to other models. Lastly, drawing on the results obtained in this section, we provide a  discussion on the differences between FEFA and other forms of attention.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Impact On Training",
      "text": "In order to fully understand the effect of FEFA on training the backbone networks, we compare the training curves of the backbone networks with and without integration of FEFA with different configurations. This comparison is presented in Figure  3  for FSDD. Here, we observe that the backbone networks equipped with FEFA not only don't experience any negative impact from the added modules, but in some cases even have smoother training curves and converge faster compared to the backbone networks without the added FEFA.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Performance",
      "text": "We use classification accuracy to measure the performance of backbone models equipped with different integrations and configurations of FEFA on the task of spoken digit recognition. The results, as presented in Table  III , show that when the FEFA modules are integrated into the DNN pipeline, the performance of the model is improved. The highest performance gain is achieved by adding the fully connected configuration of FEFA into the VGG pipeline, where a relative 14% boost is obtained compared to the baseline. This is while improvements of 4% and 5% are achieved by adding the same configurations to the ResNet and SEResNet backbones.\n\nFor evaluating the networks in the SR domain, we use the commonly used metric of Equal Error Rate (EER). This metric is the error threshold of the model at which the number of false positive errors is equal to the number of false negative errors. Table  IV  presents the results as well as the performance gain achieved by adding different configurations of FEFA to different backbones. The results show that when FEFA is integrated into simple CNN backbones, the model's performance is boosted, outperforming the respective baseline. In particular, we observe that by integrating either configurations of FEFA (LC and FC) into a Thin-ResNet followed by GhostVLAD aggregation, we outperform other works in the area. We should note that the original work in  [43]  reports the EER values with ArcSoftmax loss. However, in order to maintain consistency and provide a fair comparison, we retrained their model with Softmax loss. Table IV-E presents further experiments on FEFA for SR where we evaluate the performance of different both integration strategies discussed in Section III-B. It is observed that while both approaches improve the performance by considerable margins, the single  integration strategy provides better results. We hypothesize that this could be due to the mixing effect experienced by the individual frequency-bins as they are processed through the network. As a result of this mixing of bins, the individual values attended by FEFA are in fact a mixture of several frequency-bins, and thus the initial goal of providing finegrained attention on individual frequency-bins is less achieved.\n\nAs discussed, we also perform experiments on SER. In these experiments, we employ the commonly used unweighted accuracy (UA) and F1-Macro score as evaluation metrics. The F1-Macro score is used for this dataset for future comparisons should the unbalanced version of the dataset be used. On the other hand, for SR and spoken digit recognition, the datasets only have balanced versions and their evaluations are not affected by unbalanced data. To comply with the common practice  [51] ,  [36] ,  [37] ,  [35]  in using the IEMOCAP speech emotion dataset, we perform a k-fold cross validation for evaluating our solution. Given that in many recent works for emotion recognition from speech, VGG, ResNet, and SEResNet architectures have frequently been used for speech representation learning  [63] ,  [64] , we utilize the same back-bone networks for evaluating the impact of FEFA. This also enables us to compare our results to SR task performed earlier and provide a more consistent analysis of the results. Table  VI  shows the results of evaluating the FEFA model for the task of SER. It is evident by the results that the performance of different backbones is boosted by adding FEFA, with ResNet + FEFA with the FC integration outperforming all other works in the area. Moreover, in Table VII we experiment with different integration strategies for FEFA. We observe that much like the case for SR, while both approaches result in a performance boost, better results are achieved when a single integration of FEFA is performed.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Robustness To Noise",
      "text": "Given that FEFA has been designed to focus on the most salient frequency-bins at the input layer of CNN backbones, we anticipate that by integration of FEFA, robustness to different intensities and types of noise would be enhanced. In order to test this hypothesis, we evaluate the performance of FEFA against different types and levels of noise. To do so, a controlled level of synthetic noise is added to the test utterances used for SR. The model with the best performance from Table  IV  Thin-ResNet + GhostVlad, is selected and tested with the noisy utterances with and without FEFA. It is important to note that the model has not been retrained, and noise is only added to the test samples. The added noise is selected from both Gaussian and uniform distributions.\n\nFigure  4  presents the effect of FEFA on a sample spectral representation of a noisy utterance before being fed to a backbone network. The noisy utterance is depicted in the figure with areas most affected by noise highlighted by a red dotted rectangle. This area compared to the clean part of the utterance (highlighted by the green dotted rectangle) contains artifact noise which will negatively impact the performance. The rightmost spectrogram represents the final spectrogram enhanced by FEFA with the attended areas by FEFA shown with the grey mask. This spectrogram has been generated after applying the attention map obtained from FEFA onto the noisy input spectrogram during the experiments. The areas greyed out in the spectral representation show the frequency-bins with the least contribution according to FEFA. Hence by focusing on the other frequency-bins, FEFA decreases the effect of   the FEFA mechanism stays considerably more stable.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "D. Generated Attention Maps",
      "text": "Figure  5  shows the final attention weights learned for the tasks of SR and SER. Here we observe that as initially hypothesized, different frequency-bins carry different amounts of importance toward the final task. Specifically, it is demonstrated that the majority of the information for these two tasks lies in the range of frequency-bins 10 to 90, which correspond to 288 Hz to 2.8 KHz. The second important range can be seen as bins 106 to 162, corresponding to 3.3 KHz to 5.2 KHz. Interestingly, it is shown that the other bins have very little to no importance for SR and SER, and thus can be discarded to obtain smaller models and faster inference times.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "E. Number Of Parameters",
      "text": "Figure  6  portraits a comparison between performance of the two configurations of FEFA and that of the other forms of attention used in SR with respect to the number of parameters used. In the figure, the two configurations of FEFA are depicted by stars and the other attention mechanisms are shown by circles. We modify the metric for measuring the performance of the models and use the new metric EER -1 so that we can show the better performing models higher in the vertical axis. As shown in the figure, both FEFA models, although using fewer number of parameters, outperform other forms of attention. Naturally, the number of parameters in the FC configuration of FEFA is slightly higher than the LC configuration. However, even in the case of the FC configuration, our module has considerably fewer parameters compared to other forms of attention.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "F. Discussion And Comparison To Other Forms Of Attention",
      "text": "As shown in Table  IV , CNN models plus general forms of attention such as self-attention and soft-attention are outperformed by FEFA integrated into similar backbone networks. Our approach shows a clear performance enhancement over the classical attention mechanisms as such attention models only attend to parts of the latent representation that correspond to large areas in the input utterance spectrogram. Hence they fail to focus on very small yet salient frequency-level features that are often crucial in speech-related tasks.\n\nWhile a number of attempts have been made to achieve different levels of granularity with attention mechanisms, existing attention models do not achieve a fine-grained solution. The model proposed in  [65]  achieves varying degrees of granularity by creating different combinations of neighboring information items. However, the information items used in the combinations are embeddings already extracted by the DNN, limiting the level of granularity based on the resolution of the latent representation achieved by the network. Another attempt for a frequency-based attention model was proposed in  [43] . The attention model was adopted from image recognition and could be utilized in any hidden layer of a deep network as the source of query. The attention mechanism uses the latent representations obtained from different layers of the CNN as the information items, thus ruling out the possibility of a localized attention map with respect to the input and individual frequency-bins. Their model also uses a CNN layer to calculate the attention weight. In this approach, and others that similarly employ CNN layers in such way, the information items go through non-linear operations preventing the model from maintaining a one-to-one relation between the attention map and the information items. Therefore as this approach may be successful for some applications, it fails in others where the contribution of each separate information item, in this case individual frequency-bins, is important. Lastly, the notion of using input information items with attention has been used in the area of natural language processing in  [66] . It should be noted, however, that while the attention mechanism in their work is capable of focusing on individual words, the gradients used for training the attention weights are obtained from the last layers of the model. Adopting a similar mechanism on high dimensional inputs such as spectrograms could result in very complex models which would be hard to train.\n\nGenerally, the intuition behind many attention models (in speech-related tasks or otherwise) is to focus on different parts of some latent representation of the input to inform better classification. In these models, the representations are generally learned irrespective of important known information items in the input. Speech depends on frequency content to convey information. In fact, humans have evolved to understand different facts about the source of speech (e.g. identity, intent, emotions, etc.) based on factors such as tone, pitch, and others  [1] . The intuition behind our model, which separates it from others, is that by learning to exploit specific frequency-bins in the input that may contain effective taskrelated information, DNNs can learn to pay more attention to those particular bins to achieve better performance.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, a novel attention mechanism is proposed that allows deep learning models to focus on fine-grained information items, namely frequency-bins, without increasing the complexity of the model. Our proposed model, FEFA, uses the spectrogram representations as input and provides a better representation of the spectrogram by attending to each frequency-bin individually. We evaluate our attention mechanism on two tasks of speaker recognition and speech emotion recognition, along with spoken digit recognition. The comparison between models enhanced by FEFA and the original backbone baselines shows consistent improvement in the performance of deep learning models in these tasks. Our analysis shows that the fully connected configuration of FEFA generally outperforms the locally connected configuration. Moreover, we observe that multiple integrations of FEFA does not provide any advantages, and in fact slightly reduces performance, compared to the single integration. We also study the impact of FEFA on training of DNNs and show that not only it has no negative effects on the process, but in some cases it also helps the models to converge faster and stay more stable while doing so. Lastly we test the effect of adding FEFA into the backbone on the robustness of the models. Our experiments show that the models integrated with FEFA exhibit more robustness to different levels and types of synthetic noise.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Vii. Limitations And Future Work",
      "text": "In the end, to discuss the limitations of our model, we can mention the average pooling step deployed for aggregating frequency information across time. By using such a simple pooling technique, our approach may not be able to gain information such as changes in the values of frequency-bins throughout the duration of the utterance. As a future step we intend to address this weakness by using more complex mechanisms to replace the average pooling for more sophisticated and dynamic aggregation across time  [15] ,  [32] . Other future steps of this work may include further study of FEFA under more challenging scenarios such as severely imbalanced datasets. Another possible future step of this work would be to exploit FEFA in domain adaptation of DNN models without the need for retraining them. We also intend to study the use of FEFA in creating smaller DNNs by only using the frequency-bins that have the highest weights in the attention map generated by FEFA and removing the other ones. Another interesting possibility is to study the explainability of the attention maps generated by FEFA to better understand its role in performing different audio-related recognition tasks.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overview of our proposed model. The model uses the spectrogram representation of the utterance as the information items and the feature set",
      "page": 5
    },
    {
      "caption": "Figure 2: Tow conﬁgurations of the FEFA, (a) Using a single locally connected",
      "page": 5
    },
    {
      "caption": "Figure 1: Accordingly we deﬁne two different",
      "page": 5
    },
    {
      "caption": "Figure 2: (a)) simply",
      "page": 5
    },
    {
      "caption": "Figure 3: The training curves of the backbones with and without integration",
      "page": 7
    },
    {
      "caption": "Figure 3: for FSDD. Here, we observe that the backbone",
      "page": 8
    },
    {
      "caption": "Figure 4: presents the effect of FEFA on a sample spectral",
      "page": 9
    },
    {
      "caption": "Figure 4: Overview of the robustness test against adding synthetic noise to input utterances.",
      "page": 10
    },
    {
      "caption": "Figure 5: An illustration of generated attention maps for SR and SER.",
      "page": 10
    },
    {
      "caption": "Figure 6: Performance vs. number of parameters for different attention-based",
      "page": 10
    },
    {
      "caption": "Figure 5: shows the ﬁnal attention weights learned for the",
      "page": 10
    },
    {
      "caption": "Figure 6: portraits a comparison between performance of",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VoxCeleb": "IEMOCAP",
          "1,092,009": "4800\n(1200 per class)",
          "–": "–",
          "4,874": "1200\n(300 per class)"
        },
        {
          "VoxCeleb": "FSDD",
          "1,092,009": "1680\n(168 per class)",
          "–": "240\n(24 per class)",
          "4,874": "480\n(48 per class)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "w/o FEFA": "FEFA(LC)",
          "VGG [?]\nResNet\nSEResNet": "VGG\nVGG\nResNet\nResNet\nSEResNet\nSEResNet",
          "–\n–\n–": "+8.87\n+5.66\n+2.95\n+1.07\n+1.18\n+0.79",
          "65.4\n74.5\n75.8": "71.2\n69.1\n76.7\n75.3\n76.7\n76.4"
        },
        {
          "w/o FEFA": "FEFA(FC)",
          "VGG [?]\nResNet\nSEResNet": "VGG\nVGG\nResNet\nResNet\nSEResNet\nSEResNet",
          "–\n–\n–": "+14.67\n+12.32\n+4.56\n+3.08\n+5.40\n+3.56",
          "65.4\n74.5\n75.8": "75.0\n73.4\n77.9\n76.8\n79.9\n78.5"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Narani et al.\n[8]\nNarani et al.\n[8]\nHajibabai et.\n[57]\nChung et al.\n[5]\nXie et al.\n[32]\nXie et al.\n[32]\nOkabe et al.\n[15]\nBian et al.\n[12]\nIndia et al.\n[21]\nKye et al.\n[19]\nCai et al.\n[53]\nYadav et al.* [43]": "Proposed\nProposed\nProposed\nProposed\nProposed\nProposed\nProposed\nProposed",
          "i-Vector+PLDA\nVGG\nResNet20\nResNet50\nThin-ResNet\nThin-ResNet\nx-Vector\nResNet50\nVGG\nResnet34\nResNet34\nThin-ResNet": "VGG\nVGG\nSEResNet\nSEResNet\nThin-ResNet\nThin-ResNet\nThin-ResNet\nThin-ResNet",
          "Softmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax*": "Softmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax",
          "–\n–\n–\n–\n–\n–\nSoft Attention\nSelf Attention\nMulti-head Attention\nSupervised Attention\nSelf Attention\nCBAM": "FEFA(LC)\nFEFA(FC)\nFEFA(LC)\nFEFA(FC)\nFEFA(LC)\nFEFA(FC)\nFEFA(LC)\nFEFA(FC)",
          "–\n1024\n128\n512\n512\n512\n1500\n512\n1024\n256\n128\n512": "1024\n1024\n512\n512\n512\n512\n512\n512",
          "–\nTAP\nTAP\nTAP\nTAP\nGhostVLAD\nAP\nAP\nAP\nAP\nAP\nGhostVLAD": "TAP\nTAP\nTAP\nTAP\nTAP\nTAP\nGhostVLAD\nGhostVLAD",
          "VoxCeleb1\nVoxCeleb1\nVoxCeleb1\nVoxCeleb2\nVoxCeleb2\nVoxCeleb2\nVoxCeleb1\nVoxCeleb2\nVoxCeleb2\nVoxCeleb2\nVoxCeleb1\nVoxCeleb2": "VoxCeleb2\nVoxCeleb2\nVoxCeleb2\nVoxCeleb2\nVoxCeleb2\nVoxCeleb2\nVoxCeleb2\nVoxCeleb2",
          "8.8\n7.8\n4.30\n3.95\n10.48\n3.22\n3.85\n5.4\n3.19\n4.75\n4.40\n3.10": "7.43\n7.10\n3.48\n3.43\n5.40\n5.32\n2.78\n2.61"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Benchmark": "FEFA(LC)",
          "VGG [?]\nResNet\n[32]\nSEResNet": "VGG\nVGG\nResNet\nResNet\nSEResNet\nSEResNet",
          "–\n–\n–": "-4.7\n-2.3\n-13.6\n-4.3\n-25.6\n-25.5",
          "7.8\n3.22\n4.81": "7.43\n7.62\n2.78\n3.08\n3.48\n3.58"
        },
        {
          "Benchmark": "FEFA(FC)",
          "VGG [?]\nResNet\n[32]\nSEResNet": "VGG\nVGG\nResNet\nResNet\nSEResNet\nSEResNet",
          "–\n–\n–": "-8.9\n-6.9\n-18.9\n-9.6\n-28.6\n-27.8",
          "7.8\n3.22\n4.81": "7.10\n7.26\n2.61\n2.91\n3.43\n3.47"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chernykh et al.\n[51]\nRamet et al.\n[36]\nMirsamadi et al.\n[37]\nTarantino et al.\n[35]": "Proposed\nProposed\nProposed\nProposed\nProposed\nProposed",
          "RNN\nRNN\nRNN\nTransformer": "VGG\nVGG\nSEResNet\nSEResNet\nResNet\nResNet",
          "–\nSelf Attention\nLocal Attention\nSelf Attention": "FEFA(LC)\nFEFA(FC)\nFEFA(LC)\nFEFA(FC)\nFEFA(LC)\nFEFA(FC)",
          "MFCC\nMFCC+LLD\nMFCC+LLD\nMFCC+LLD": "Spect 256\nSpect 256\nSpect 256\nSpect 256\nSpect 256\nSpect 256",
          "Yes\nNo\nNo\nNo": "No\nNo\nNo\nNo\nNo\nNo",
          "54\n59.6\n58.8\n63.8": "56.70\n58.23\n62.28\n63.92\n62.32\n63.97",
          "–\n–\n–\n–": "56.83\n58.41\n62.43\n64.02\n62.46\n64.09"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Benchmark": "FEFA(LC)",
          "VGG\nResNet\nSEResNet": "VGG\nVGG\nResNet\nResNet\nSEResNet\nSEResNet",
          "–\n–\n–": "+8.04\n+5.48\n+4.36\n+3.09\n+4.11\n+3.25",
          "52.48\n59.72\n59.82": "56.70\n55.36\n62.32\n61.57\n62.28\n61.63"
        },
        {
          "Benchmark": "FEFA(FC)",
          "VGG\nResNet\nSEResNet": "VGG\nVGG\nResNet\nResNet\nSEResNet\nSEResNet",
          "–\n–\n–": "+10.95\n+10.25\n+7.11\n+6.54\n+6.85\n+6.73",
          "52.48\n59.72\n59.82": "58.23\n57.86\n63.97\n63.63\n63.92\n63.85"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Normal": "",
          "w/o FEFA\n100db\n3.40\n+5.5\nw/o FEFA\n50db\n3.85\n+19.5\nw/o FEFA\n20db\n4.82\n+49.6": "+ FEFA\n100db\n2.61\n0\n+ FEFA\n50db\n2.78\n+6.5\n+ FEFA\n20db\n3.10\n+18.7"
        },
        {
          "Normal": "Uniform",
          "w/o FEFA\n100db\n3.40\n+5.5\nw/o FEFA\n50db\n3.85\n+19.5\nw/o FEFA\n20db\n4.82\n+49.6": "w/o FEFA\n100db\n3.32\n+3.1\nw/o FEFA\n50db\n3.48\n+8.0\nw/o FEFA\n20db\n3.96\n+22.9"
        },
        {
          "Normal": "",
          "w/o FEFA\n100db\n3.40\n+5.5\nw/o FEFA\n50db\n3.85\n+19.5\nw/o FEFA\n20db\n4.82\n+49.6": "+ FEFA\n100db\n2.61\n0\n+ FEFA\n50db\n2.80\n+7.2\n+ FEFA\n20db\n3.10\n+18.7"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speaker recognition by machines and humans: A tutorial review",
      "authors": [
        "J Hansen",
        "T Hasan"
      ],
      "year": "2015",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "3",
      "title": "Joint factor analysis versus eigenchannels in speaker recognition",
      "authors": [
        "P Kenny",
        "G Boulianne",
        "P Ouellet",
        "P Dumouchel"
      ],
      "year": "2007",
      "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)"
    },
    {
      "citation_id": "4",
      "title": "A deep neural network for short-segment speaker recognition",
      "authors": [
        "A Hajavi",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "A deep neural network for short-segment speaker recognition"
    },
    {
      "citation_id": "5",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "J Chung",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Voxceleb2: Deep speaker recognition"
    },
    {
      "citation_id": "6",
      "title": "Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (addog)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "J Ding",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "VoxCeleb: A Large-Scale Speaker Identification Dataset",
      "authors": [
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "VoxCeleb: A Large-Scale Speaker Identification Dataset"
    },
    {
      "citation_id": "9",
      "title": "Representation learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "E Laksana",
        "L.-P Morency",
        "S Scherer"
      ],
      "year": "2016",
      "venue": "Representation learning for speech emotion recognition"
    },
    {
      "citation_id": "10",
      "title": "An attentive survey of attention models",
      "authors": [
        "S Chaudhari",
        "V Mithal",
        "G Polatkan",
        "R Ramanath"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)"
    },
    {
      "citation_id": "11",
      "title": "Attention in natural language processing",
      "authors": [
        "A Galassi",
        "M Lippi",
        "P Torroni"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "12",
      "title": "Self-attention based speaker recognition using cluster-range loss",
      "authors": [
        "T Bian",
        "F Chen",
        "L Xu"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "13",
      "title": "Deep Speaker Embeddings for Short-Duration Speaker Verification",
      "authors": [
        "G Bhattacharya",
        "M Alam",
        "P Kenny"
      ],
      "year": "2017",
      "venue": "Deep Speaker Embeddings for Short-Duration Speaker Verification"
    },
    {
      "citation_id": "14",
      "title": "Seq2seq attentional siamese neural networks for text-dependent speaker verification",
      "authors": [
        "Y Zhang",
        "M Yu",
        "N Li",
        "C Yu",
        "J Cui",
        "D Yu"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Attentive statistics pooling for deep speaker embedding",
      "authors": [
        "K Okabe",
        "T Koshinaka",
        "K Shinoda"
      ],
      "year": "2018",
      "venue": "Attentive statistics pooling for deep speaker embedding"
    },
    {
      "citation_id": "16",
      "title": "How to improve your speaker embeddings extractor in generic toolkits",
      "authors": [
        "H Zeinali",
        "L Burget",
        "J Rohdin",
        "T Stafylakis",
        "J Cernocky"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Deep Neural Network Embeddings with Gating Mechanisms for Text-Independent Speaker Verification",
      "authors": [
        "L You",
        "W Guo",
        "L.-R Dai",
        "J Du"
      ],
      "year": "2019",
      "venue": "Deep Neural Network Embeddings with Gating Mechanisms for Text-Independent Speaker Verification"
    },
    {
      "citation_id": "18",
      "title": "Self Multi-Head Attention for Speaker Recognition",
      "authors": [
        "P Safari",
        "J Hernando"
      ],
      "year": "2019",
      "venue": "Self Multi-Head Attention for Speaker Recognition"
    },
    {
      "citation_id": "19",
      "title": "Supervised attention for speaker recognition",
      "authors": [
        "S Kye",
        "J Chung",
        "H Kim"
      ],
      "year": "2021",
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "20",
      "title": "Graph attention networks for speaker verification",
      "authors": [
        "J.-W Jung",
        "H.-S Heo",
        "H.-J Yu",
        "J Chung"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Double multi-head attention for speaker verification",
      "authors": [
        "M India",
        "P Safari",
        "J Hernando"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Memory layers with multi-head attention mechanisms for text-dependent speaker verification",
      "authors": [
        "V Mingote",
        "A Miguel",
        "A Ortega",
        "E Lleida"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Self-attentive speaker embeddings for text-independent speaker verification",
      "authors": [
        "Y Zhu",
        "T Ko",
        "D Snyder",
        "B Mak",
        "D Povey"
      ],
      "year": "2018",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "24",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "25",
      "title": "Free spoken digit dataset",
      "authors": [
        "Z Jackson"
      ],
      "year": "2020",
      "venue": "Free spoken digit dataset"
    },
    {
      "citation_id": "26",
      "title": "Analysis of feature extraction and channel compensation in a gmm speaker recognition system",
      "authors": [
        "L Burget",
        "P Matejka",
        "P Schwarz",
        "O Glembek",
        "J Cernocky"
      ],
      "year": "2007",
      "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)"
    },
    {
      "citation_id": "27",
      "title": "A svm/hmm system for speaker recognition",
      "authors": [
        "W Campbell"
      ],
      "year": "2003",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "28",
      "title": "Training universal background models for speaker recognition",
      "authors": [
        "M Omar",
        "J Pelecanos"
      ],
      "year": "2010",
      "venue": "Odyssey"
    },
    {
      "citation_id": "29",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "Speaker recognition using neural networks and conventional classifiers",
      "authors": [
        "K Farrell",
        "R Mammone",
        "K Assaleh"
      ],
      "year": "1994",
      "venue": "IEEE Transactions on Speech and Audio Processing"
    },
    {
      "citation_id": "31",
      "title": "Emotion recognition in speech using neural networks",
      "authors": [
        "J Nicholson",
        "K Takahashi",
        "R Nakatsu"
      ],
      "year": "2000",
      "venue": "Neural Computing & Applications"
    },
    {
      "citation_id": "32",
      "title": "Utterance-level Aggregation For Speaker Recognition In The Wild",
      "authors": [
        "W Xie",
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "Speech emotion classification using attention-based lstm",
      "authors": [
        "Y Xie",
        "R Liang",
        "Z Liang",
        "C Huang",
        "C Zou",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)"
    },
    {
      "citation_id": "34",
      "title": "Direct modelling of speech emotion from raw speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Direct modelling of speech emotion from raw speech"
    },
    {
      "citation_id": "35",
      "title": "Self-attention for speech emotion recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Self-attention for speech emotion recognition"
    },
    {
      "citation_id": "36",
      "title": "Context-aware attention mechanism for speech emotion recognition",
      "authors": [
        "G Ramet",
        "P Garner",
        "M Baeriswyl",
        "A Lazaridis"
      ],
      "year": "2018",
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "37",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "An end-to-end visual-audio attention network for emotion recognition in user-generated videos",
      "authors": [
        "S Zhao",
        "Y Ma",
        "Y Gu",
        "J Yang",
        "T Xing",
        "P Xu",
        "R Hu",
        "H Chai",
        "K Keutzer"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "39",
      "title": "Multi-resolution multi-head attention in deep speaker embedding",
      "authors": [
        "Z Wang",
        "K Yao",
        "X Li",
        "S Fang"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "40",
      "title": "H-vectors: Utterance-level speaker embedding using a hierarchical attention model",
      "authors": [
        "Y Shi",
        "Q Huang",
        "T Hain"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "41",
      "title": "Attention-based models for text-dependent speaker verification",
      "authors": [
        "F Rezaur Rahman Chowdhury",
        "Q Wang",
        "I Moreno",
        "L Wan"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "42",
      "title": "Cnn with phonetic attention for text-independent speaker verification",
      "authors": [
        "T Zhou",
        "Y Zhao",
        "J Li",
        "Y Gong",
        "J Wu"
      ],
      "year": "2019",
      "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "43",
      "title": "Frequency and temporal convolutional attention for text-independent speaker recognition",
      "authors": [
        "S Yadav",
        "A Rai"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Conference of the North American Chapter"
    },
    {
      "citation_id": "45",
      "title": "Show, attend and tell: Neural image caption generation with visual attention",
      "authors": [
        "K Xu",
        "J Ba",
        "R Kiros",
        "K Cho",
        "A Courville",
        "R Salakhudinov",
        "R Zemel",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "46",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "47",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "48",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Deep Residual Learning for Image Recognition"
    },
    {
      "citation_id": "49",
      "title": "Speaker Recognition and Evaluation",
      "year": "2016",
      "venue": "Speaker Recognition and Evaluation"
    },
    {
      "citation_id": "50",
      "title": "Analysis of length normalization in endto-end speaker verification system",
      "authors": [
        "W Cai",
        "J Chen",
        "M Li"
      ],
      "year": "2018",
      "venue": "Analysis of length normalization in endto-end speaker verification system"
    },
    {
      "citation_id": "51",
      "title": "Emotion recognition from speech with recurrent neural networks",
      "authors": [
        "V Chernykh",
        "P Prikhodko"
      ],
      "year": "2017",
      "venue": "Emotion recognition from speech with recurrent neural networks",
      "arxiv": "arXiv:1701.08071"
    },
    {
      "citation_id": "52",
      "title": "Conformer: Convolution-augmented transformer for speech recognition",
      "authors": [
        "A Gulati",
        "J Qin",
        "C.-C Chiu",
        "N Parmar",
        "Y Zhang",
        "J Yu",
        "W Han",
        "S Wang",
        "Z Zhang",
        "Y Wu"
      ],
      "year": "2020",
      "venue": "Conformer: Convolution-augmented transformer for speech recognition"
    },
    {
      "citation_id": "53",
      "title": "Exploring the encoding layer and loss function in end-to-end speaker and language recognition system",
      "authors": [
        "W Cai",
        "J Chen",
        "M Li"
      ],
      "year": "2018",
      "venue": "Odyssey The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "54",
      "title": "Residual neural networks for speech recognition",
      "authors": [
        "H Vydana",
        "A Vuppala"
      ],
      "year": "2017",
      "venue": "25th European Signal Processing Conference"
    },
    {
      "citation_id": "55",
      "title": "A ResNet-50-based convolutional neural network model for language ID identification from speech recordings",
      "authors": [
        "G Celano"
      ],
      "year": "2021",
      "venue": "Third Workshop on Computational Typology and Multilingual NLP"
    },
    {
      "citation_id": "56",
      "title": "End-to-end speech recognition in agglutinative languages",
      "authors": [
        "O Mamyrbayev",
        "K Alimhan",
        "B Zhumazhanov",
        "T Turdalykyzy",
        "F Gusmanova"
      ],
      "year": "2020",
      "venue": "Asian Conference on Intelligent Information and Database Systems"
    },
    {
      "citation_id": "57",
      "title": "Unified hypersphere embedding for speaker recognition",
      "authors": [
        "M Hajibabaei",
        "D Dai"
      ],
      "year": "2018",
      "venue": "Unified hypersphere embedding for speaker recognition",
      "arxiv": "arXiv:1807.08312"
    },
    {
      "citation_id": "58",
      "title": "Siamese capsule network for end-to-end speaker recognition in the wild",
      "authors": [
        "A Hajavi",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "59",
      "title": "Fluentnet: End-to-end detection of stuttered speech disfluencies with deep learning",
      "authors": [
        "T Kourkounakis",
        "A Hajavi",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "60",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "IEEE conference on computer vision and pattern recognition(CVPR)"
    },
    {
      "citation_id": "61",
      "title": "Cyclical learning rates for training neural networks",
      "authors": [
        "L Smith"
      ],
      "year": "2017",
      "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "62",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "63",
      "title": "Speech emotion recognition using spectrogram & phoneme embedding",
      "authors": [
        "P Yenigalla",
        "A Kumar",
        "S Tripathi",
        "C Singh",
        "S Kar",
        "J Vepa"
      ],
      "year": "2018",
      "venue": "Speech emotion recognition using spectrogram & phoneme embedding"
    },
    {
      "citation_id": "64",
      "title": "Deep temporal models using identity skip-connections for speech emotion recognition",
      "authors": [
        "J Kim",
        "G Englebienne",
        "K Truong",
        "V Evers"
      ],
      "year": "2017",
      "venue": "the 25th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "65",
      "title": "Area attention",
      "authors": [
        "Y Li",
        "L Kaiser",
        "S Bengio",
        "S Si"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning(ICML)"
    },
    {
      "citation_id": "66",
      "title": "Inner attention based recurrent neural networks for answer selection",
      "authors": [
        "B Wang",
        "K Liu",
        "J Zhao"
      ],
      "year": "2016",
      "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics"
    }
  ]
}