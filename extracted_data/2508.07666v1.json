{
  "paper_id": "2508.07666v1",
  "title": "Towards Multimodal Sentiment Analysis Via Contrastive Cross-Modal Retrieval Augmentation And Hierachical Prompts",
  "published": "2025-08-11T06:27:40Z",
  "authors": [
    "Xianbing Zhao",
    "Shengzun Yang",
    "Buzhou Tang",
    "Ronghuan Jiang"
  ],
  "keywords": [
    "Multimodal sentiment analysis",
    "Multimodal retrival augmentation",
    "Prompt learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal sentiment analysis is a fundamental problem in the field of affective computing. Although significant progress has been made in cross-modal interaction, it remains a challenge due to the insufficient reference context in cross-modal interactions. Current cross-modal approaches primarily focus on leveraging modality-level reference context within a individual sample for cross-modal feature enhancement, neglecting the potential cross-sample relationships that can serve as sample-level reference context to enhance the cross-modal features. To address this issue, we propose a novel multimodal retrieval-augmented framework to simultaneously incorporate inter-sample modalitylevel reference context and cross-sample sample-level reference context to enhance the multimodal features. In particular, we first design a contrastive cross-modal retrieval module to retrieve semantic similar samples and enhance target modality. To endow the model to capture both inter-sample and intrasample information, we integrate two different types of prompts, modality-level prompts and sample-level prompts, to generate modality-level and sample-level reference contexts, respectively. Finally, we design a cross-modal retrieval-augmented encoder that simultaneously leverages modality-level and sample-level reference contexts to enhance the target modality. Extensive experiments demonstrate the effectiveness and superiority of our model on two publicly available datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "M Ultimodal content, primarily consisting of text, visual, and acoustic, is widely present in our daily lives. It is crucial for computers to analyze and understand these multimodal data. Multimodal sentiment analysis is an important and fundamental problem in the field of affective computing, which has attracted significant attention from researchers and made substantial progress. Previous multimodal sentiment analysis studies can be broadly divided into two categories: multimodal interaction learning methods and multimodal representation learning methods. The former learns enhanced multimodal features through cross-modal interaction learning  [1] -  [5] , while the latter learns fine-grained representations by Fig.  1 . Previous cross-modal interaction paradigm that solely rely on modality-level reference context to enhance target modality produce erroneous sentiment polarity. In contrast, our method simultaneously utilizes both modality-level and sample-level reference contexts to enhance target modality, resulting in the correct sentiment polarity. modeling the geometric relationships in the cross-modal representation space  [6] -  [11] . These approach effectively addresses key limitations of multimodal interaction learning, such as the alignment and fusion of cross-modal information.\n\nDespite achieving significant progress, these methods still suffer from important drawbacks. These approaches learn enhanced multimodal features that are limited to the information within individual sample, overlooking the rich reference context information provided by similar samples. The reference context offered by similar samples can provide more richer information for multimodal feature enhancement compared to intra-sample multimodal interaction learning  [12] -  [16] . As illustrated in Figure  1 , the multimodal sentiment analysis model that only considers modality-level reference context within the individual sample generates incorrect sentiment polarity. By additionally incorporating sample-level reference context of intra-samples, the multimodal sentiment analysis model produces the correct sentiment polarity. Therefore, it would be interesting to investigate i) how to introduce cross-sample reference context into multimodal interaction learning, ii) how to efficiently facilitate the integration of inter-sample and intra-sample reference contexts to learn enhanced multimodal features.\n\nTo tackle these downsides, we propose a novel Towards Multimodal Sentiment Analysis via Contrastive Cross-modal Retrieval Augmentation and Hierarchical Prompts, namely TC 2 RAHP, which is the first unified multimodal sentiment analysis framework that can introduce and incorporate modality-level and sample-level reference context to enhance target modality. As shown in Figure  2 , we first design a contrastive cross-modal retrieval module to learn the reference context of similar samples. This module learns positive and negative samples similar to the target modality under the guidance of positive and negative sentiment semantics. We leverage contrastive learning to constrain the model to learn samples that are semantically similar to the target modality, rather than just feature-wise similarity. Positive samples are selected as sample-level reference contexts to enhance the target modality. To facilitate inter-sample and intra-sample interaction learning, we design a prompt-based cross-modal reference context generation module. This module includes two different types of prompts, modality-level prompts and sample-level prompts, which map cross-modal and crosssample information into modality-level and sample-level reference context. We design a cross-modal retrieval-enhanced encoder that leverages modality-level and sample-level reference contexts to enhance the target modality and improve its representational ability, thereby increasing the accuracy of sentiment polarity prediction. Extensive experiments on two publicly available multimodal datasets, CMU-MOSI and CMU-MOSEI, validate the effectiveness and superiority of our model. The main contributions of this work are threefold:\n\n‚Ä¢ We introduce a cross-modal retrieval-augmented interaction framework for multimodal sentiment analysis, which is capable of learning cross-modal and cross-sample information to enhance the target modality. ‚Ä¢ We design a contrastive cross-modal retrieval module that retrieves positive and negative samples relative to the target modality and strengthens the semantics of these samples.\n\n‚Ä¢ We design a prompt-based cross-modal reference context generation module that maps inter-sample and intrasample information into modality-level and sample-level reference contexts. Additionally, we customize a crossmodal retrieval-augmented encoder that utilizes these reference contexts to enhance the target modality.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Multimodal Sentiment Analysis",
      "text": "Multimodal Sentiment Analysis refers to the process of comprehensively analyzing and understanding human sentiment by leveraging multiple modalities  [17] ,  [18] . Current research focuses primarily on two aspects: multimodal representation learning and multimodal fusion.\n\nThe former continuously explores semantics  [19] ,  [20]  through feature transformation to obtain more fine-grained feature representations. Zadeh et al.  [21]  further explored the local information of the features by reshaping and concatenating the tensors, as well as establishing associations between the different tensors. Liu et al.  [22]  deconstructed the matrix to transform it into a low-rank matrix, capturing feature information in the altered vector space. Yang et al.  [7]  decomposed the features into fine-grained shared and private features through adversarial methods, capturing information along these two dimensions. Yu et al.  [6]  synthesized global information to generate sentiment labels, which then guided further feature extraction. The latter projects multiple nonaligned local features into the same feature space through interactions between different modalities and obtains global sentiment information within this feature space  [23] . Tsai et al.  [1]  first projected multiple local features into the same vector space through cross-modal attention and then integrated global sentiment information through self-attention. Han et al.  [17]  designed a hierarchical structure to layerwise integrate information from various modalities, as well as to fuse information across modalities and within individual modalities, for sentiment analysis. Yu et al.  [24]  assigned modality-specific information to each modality to guide the extraction of features and the fusion of global information. Zhang et al.  [25]  identified the modules with weak information extraction capabilities and prioritized training these modules through targeted prompts. Li et al.  [26]  noticed the noise caused by modal imbalance and mitigated it by adjusting the distance between modalities in the semantic space to perform feature denoising.\n\nWith the development of LLMs and MLLMs  [27] , many studies have begun to explore the application of LLMs and MLLMs in multimodal sentiment analysis tasks  [28] ,  [29] , demonstrating superior zero-shot performance. LLMs such as ChatGPT  [30]  and LLama  [31]  have shown excellent sentiment analysis results by capturing richer feature representations in text. MLLMs such as Gemini-V  [32]  and BLIP-2  [33]  possess stronger cross-modal interaction capabilities, allowing better utilization of information from different modalities.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Retrieval Augmentation",
      "text": "Retrieval Augmentation refers to the process of incorporating relevant external information into the current data by retrieval, thus improving the representation capacity of existing data  [12] ,  [15] ,  [16] ,  [34] ,  [35] . Long et al.  [13]  added an image library and a retrieval module to a basic image feature extraction module, improving image classification capabilities by introducing additional image information. Xu et al.  [36]  improved the performance of the document classifier by mapping the query data and the retrieved data to the same feature space for matching. Ram et al.  [14]  enhanced the semantics of the text by incorporating the retrieved textual information into the input while keeping the model structure fixed. Lewis et al.  [37]  combined retrieval augmentation techniques with large models and provided a widely applicable fine-tuning framework, achieving excellent results. Rezaei et al.  [38]  designed an evaluation score to balance the richness and accuracy of generated content during multi-round retrieval. Yang et al.  [39]  constructed a knowledge base based on information from existing sequences, and utilized the retrieved knowledge to interact with the current sequence, thereby guiding predictions for future sequences.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this section, we customize three modules, as illustrated in Figure  2 . Specifically, we first introduce the contrastive crossmodal retrieval module in Section III-B to learn the reference  context from external samples. Next, we present the promptbased cross-modal reference context generation module in Section III-C, which utilizes modality-level prompts and samplelevel prompts to generate mappings for intra-sample and intersample information. Finally, in Section III-D, we design the cross-modal retrieval-augmented encoder, which integrates the generated intra-sample and inter-sample information into the target modality to learn enhanced multimodal features.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "A. Overall Framework",
      "text": "Figure  2  illustrates the workflow of our model. Taking the target modality as the text modality as an example. The workflow of the proposed model is as follows: Given the i-th video clip, we use the pre-trained models to extract text  [40] , visual  [41] , and acoustic  [42]  features, denoted as x {t,v,a} i ‚àà D, respectively. D denotes the whole dataset. We obtain the feature set {x {t,v,a} j } ‚àà D, i Ã∏ = j as the reference pool. Under the guidance of sentiment labels, we retrieve the most similar positive and negative samples relative to the target modality x t i from the reference pool, denoted as {x t nt , x t pt }, {x t nv , x t pv } and {x t na , x t pa }. We use contrastive loss L ccrl to constrain the target modality and the retrieved samples. In the prompt-based cross-modal generation network, modality-level prompts P mt and intra-sample features x {t,v,a} i are input into the modality generation network f Œ∏m * to obtain modality-level reference context x tm i . Sample-level prompt P st and intersample features x t p {t,v,a} are input into the sample generation network f Œ∏s * to obtain sample-level reference context x ts i . The modality-level and sample-level reference context information x tm i and x ts i are respectively input into the cross-modal retrieval-augmented encoder to enhance the target modality x t i , thereby obtaining the enhanced features xtm i and xts i . Following the same pipeline, we can also obtain (x vm i ,x vs i ) and (x am i ,x as i ). Finally, the enhanced multimodal features are fused to obtain x f inal for sentiment polarity prediction.Subsequent sections provide details of the proposed components.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Contrastive Cross-Modal Retrieval",
      "text": "Previous multimodal sentiment analysis methods  [1] ,  [25] ,  [26]  primarily explore intra-sample multimodal interaction learning to enhance the features of the target modality, neglecting the reference information provided by external samples. To this end, we propose a contrastive cross-modal retrieval module that selects reference samples from external samples and provides richer reference context for feature enhancement of the target modality. This module is capable of effectively retrieving valid reference samples from external samples and optimizing the semantic of these reference samples with the guidance of sentiment category. Specifically, given a mini-batch containing M ‚àà D samples as reference pool {x {t,v,a} j } ‚àà M, we use cosine similarity as the similarity measure to retrieve the samples with the highest feature similarity. The samples with matching sentiment labels to the target modality m Œ± ‚àà {t, v, a} are selected as positive samples x {t,v,a} pm Œ≤ , m Œ≤ ‚àà {t, v, a}, while those with opposing sentiment labels to the target modality are selected as negative samples x {t,v,a} nm Œ≤ , m Œ≤ ‚àà {t, v, a}. The cross-modal retrieval similarity is calculated as follows:\n\nwhere x mŒ± i and x m Œ≤ j represent the features of the target modality and the reference sample, respectively. F represents the cosine similarity function. The y i and y j represent the sentiment label of samples i and j at training stage, respectively. Note that, during the testing phase, the model is already able to effectively distinguish the semantics of positive and negative samples, so label guidance is not required. We can retrieve the corresponding positive and negative reference sample pairs {x mŒ± pm Œ≤ , x mŒ± nm Œ≤ } for each modality according to the Eq. 2. Formally,\n\nwhere the superscript m Œ± represents the target modality, the subscript m Œ≤ represents the retrieved modality, and p and n denote positive and negative samples, respectively. For example, x t pv represents the most similar visual modality retrieved from external positive samples with the text modality as the target modality. Only leveraging the similarity between the target modality and external samples makes it difficult to learn semantic associations, as the target modality may have high similarity with external samples that possess opposite sentiment polarities. We design a contrastive loss to reduce the distance between the target modality and the positive reference sample features, while increasing the distance between the target modality and the negative reference sample features. The contrastive cross-modal retrieval loss is formulated as follows:\n\n(5) where m Œ± , m Œ≤ ‚àà {t, a, v} denotes modality, d represents the l 2 norm constraint between the two feature spaces, and Œ≥ denotes the upper bound of the distance metric between the target modality and the negative reference sample. The contrastive constraint ensures that the retrieved samples are not only feature-wise similar, but more importantly, semantically similar in terms of sentiment.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Prompt-Based Cross-Modal Reference Context Generation",
      "text": "Multimodal data often exhibits gaps in feature space distributions  [24] ,  [43] . Previous methods address this issue by designing bridging neural networks to perform modality-tomodality mapping  [8] . Specifically, in scenarios with missing modality  [44] , these methods use prompts to bridge the gap and generate the missing modality, as prompts can learn crossmodal mapping information. Based on this observation, we propose using prompts to transform cross-modal reference samples into representations of the target modality's reference context. Modality-level prompts and sample-level prompts are used for generating modality-level and sample-level reference contexts, respectively. We refer to these prompts as hierarchical prompts, which can be formalized as: P m = {P mt , P mv , P ma } (6) P s = {P st , P sv , P sa }  (7)  where P m and P s represent modality-level and sample-level prompts, respectively. The initialization  [45]  form of the prompt is as follows:\n\nwhere f an in represents the number of input neurons, which is the feature dimension of the prompt. The symbol a represents the negative slope parameter with a value of 5, and b is a constant with a value of 6. The symbol U represents the uniform distribution, indicating that weights are sampled symmetrically within the interval. We inject prompt P m and P s into functions f Œ∏m (‚Ä¢) and f Œ∏s (‚Ä¢) with parameters Œ∏ m and Œ∏ s , respectively, to transform the reference samples into the corresponding modality-level reference context. This can be formalized as:\n\nx as = f Œ∏s tva‚Üía ([P sa , f Œ∏s t‚Üía (x a pt ), f Œ∏s v‚Üía (x a pv ), f Œ∏s a‚Üía (x a pa )])\n\nwhere x ts , x vs and x as represent the sample-level reference context, which is used to enhance the features of the target modality. The x t p {t,v,a} , x v p {t,v,a} , and x a p {t,v,a} represents the positive reference samples retrieved from external samples, as defined in Eq. 3 of Section III-B. Similarly, we also transform the intra-sample reference modality into the samplelevel reference context to enhance the features of the target modality. Formally, this can be represented as:\n\n) where x tm , x vm , and x am are the modality-level reference contexts used to enhance the features of the target modality. The x t , x v , and x a represent the multimodal features of individual intra-sample that need to be enhanced target modalities, respectively. Since both intra-sample and inter-sample multimodal information need to be transformed into reference contexts to enhance the target modality, the prompt-based transformation function effectively facilitates the conversion of cross-modal information.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Cross-Modal Retrieval-Augmented Encoder",
      "text": "To efficiently integrate intra-sample and inter-sample reference context into the target modality, we design the crossmodal retrieval-augmented encoder (CRE). It consists of an intra-sample cross-modal augmented encoder (Inter-CAE) and an inter-sample cross-modal augmented encoder (Intra-CAE). The target modality and reference context are first passed through the self-attention  [46]  layer of Inter-CAE and Intra-CAE. Then the intra-sample cross-modal augmented encoder uses modality-level reference contexts to enhance the target modality, while the inter-sample cross-modal augmented encoder uses sample-level reference contexts to reinforce the target modality. Formally,\n\nx * s = Intra-CRE(x * , x * s ), * ‚àà {t, v, a}\n\nwhere x * m and x * s represent the enhanced target multimodal features that leverage modality-level reference context x * m and sample-level reference context x * s , respectively. The w {inter,inter} {q * ,k * ,v * } denotes parameter matrix. LN and FFN represent layer normalization and feed-forward network and are used to handle enhanced target multimodal features. Finally, these enhanced target multimodal features are fused and passed through several linear layers and activation functions for sentiment polarity prediction.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Optimization Objective",
      "text": "The optimization objective consists of two parts: the multimodal sentiment analysis task loss and the contrastive crossmodal retrieval loss. We use MSE Loss L msa for the multimodal sentiment analysis task. The total loss is calculated as follows.\n\nL total = L msa + ŒªL ccrl  (18)  where Œª denotes the hyper-parameter.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Datasets",
      "text": "In this paper, following previous work, we conducted experiments on two datasets: CMU-MOSI and CMU-MOSEI, to evaluate the performance of our model.\n\nCMU-MOSI  [47] . The CMU-MOSI dataset serves as a widely recognized benchmark for sentiment analysis across multiple modalities, including acoustic, text, and visual. It consists of 2,199 YouTube video clips, each of which has been manually labeled with a sentiment intensity rating on a scale from -3 (indicating extreme negativity) to +3 (indicating extreme positivity).\n\nCMU-MOSEI  [48] . CMU-MOSEI is a sentiment analysis dataset collected from YouTube. As an extension of CMU-MOSI, CMU-MOSEI shares a similar annotation standard for sentiment scores and encompasses a more diverse range of video categories, comprising a total of 22,856 video segments.\n\nB. Experimental Settings a) Evaluation Protocols.: Following previous work, we employed four evaluation metrics to assess the model's performance: Binary Classification Accuracy (Acc2), F1-Score, Mean Absolute Error (MAE), and Pearson Correlation (Corr). Specifically, we converted the regression predictions into binary values for computing Acc-2. Higher values indicate better performance for all metrics except MAE.\n\nb) Implementation Details.: We trained our proposed model using the PyTorch library on a single NVIDIA A100 GPU. The AdamW optimizer is configured with a batch size of 8. For training on the CMU-MOSI dataset, the number of epochs was set to 50, while for the CMU-MOSEI dataset, it was set to 20. The learning rate was fixed at 1e-5. The upper bound of the distance metric Œ≥ was set to 50.\n\nC. Baselines a) MSA Models.: We compared our proposed model with state-of-the-art baseline models in the task of multimodal sentiment analysis to justify its effectiveness. TFN  [21]  expands at the tensor level to capture both intra-modal and inter-modal information. LMF  [22]  decomposes the model weights into low-rank factors for tensor fusion. MULT  [1]  leverages crossmodal attention to incorporate information from other modalities into the current modality. MISA  [43]  divides the modality features into more fine-grained shared and exclusive features. Self-MM  [6]  generates labels through a self-supervised approach and further employs them to facilitate the interaction of multimodal information. MMIM  [17]  captures sentiment information through multidimensional interactions between modalities. FDMER  [7]  optimizes the parameters of the encoders for identifying private and shared features through adversarial learning. AMML  [49]  integrates the information learned from single-modal tasks into the multimodal context. ConKI  [24]  incorporates modality-specific information for each modality. HyDiscGAN  [50]  aligns different modalities and generates simulated multimodal features from the obtained information to achieve privacy protection. MOFN  [51]  proposes an alignment method that enriches the representation of video frame features to achieve video restoration. DMD  [9]  addresses the modality imbalance issue using a crossmodal distillation method. ConFEDE  [10]  decomposes the features and enhances the information representation through contrastive learning. DTN  [11]  decouples the features of a single modality and explores the commonality and diversity between different modalities.\n\nb) LLMs and MLLMs.: We selected several highperforming large language models from the benchmark tests  [52]  of multimodal large language models as baselines, such as Chatgpt  [30] , LLama2  [31] , Flan-T5  [53] , Qwen-VLs  [54] , BLIP-2  [33] , InstructBLIP  [55] , GPT-4V  [56] , Claude3-V  [57] , and Gemini-V  [32] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Performance Comparison",
      "text": "The performance comparisons are summarized in Table  I . By analyzing this table, we gained the following observations: 1) Among traditional multimodal sentiment analysis methods,  MOFN outperforms previous methods on both datasets by introducing a carefully designed compensation module to enhance video frames. Specifically, this compensation module aligns video frames, which are subsequently processed by a fusion module for reconstruction, thus obtaining frames of improved visual quality. This result indicates that fully leveraging the modality-level reference context is essential in sentiment analysis. 2) Among LLM and MLLM based methods, GPT-4V and Gemini-V outperforms the compared baselines. Compared with MOFN, GPT-4V and Gemini-V obtains relative Acc2 gains with 4.02% and 0.82%, respectively. This result indicates that LLMs possess representational capabilities that traditional sentiment analysis models cannot surpass.\n\nOur proposed model outperforms all baseline models across all evaluation metrics on both the CMU-MOSI and CMU-MOSEI datasets. Compared with GPT-4V and Gemini-V, our proposed model obtains relative Acc2 gains with 0.4% and 0.44%, respectively. The reason is that our model not only focuses on the sample-level reference context but also incorporates the modality-level reference context. This indicates that simultaneously attending to both sample-level and modality-level reference contexts, and performing fine-grained interactions, can further enhance the model's capability.\n\nBeyond comparing the results under Acc-2, we also report performance under the Acc-7 metric, as shown in Figures  3  and 4 . The experimental results demonstrate that our model consistently outperforms all open-source large models and most closed-source models in terms of Acc-7. Notably, the closed-source models GPT-4v and GPT-4o were evaluated only on a subset of the dataset, which further highlights the effectiveness of our proposed model.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "E. Ablation Study",
      "text": "To gain a deeper understanding of our model's performance, we conducted ablation tests according to the following scheme.   II .\n\nCompared with our model, w/o MMG causes a greater performance drop than w/o SMG. The reason is that the modality-level reference context shares the same semantic space as the target sample, which reveals that the more similar the semantic spaces are, the more useful the information obtained through interaction, thereby better enhancing the target sample. Moreover, the performance drop of w/o (M)ICAE and w/o (S)ICAE can be observed, revealing that enabling the target sample to interact with both the modality-level and sample-level reference contexts effectively enhances the feature representations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "F. Cross-Modal Retrieval Sample Visualization",
      "text": "Apart from achieving outstanding performance, the key advantage of TC 2 RAHP is that it can introduce the samplelevel reference context to enhance target sample through contrastive cross-modal retrieval module. To this end, we visualized the process of retrieving positive and negative samples for the target modality during the training phase.\n\nBut none of those things matter its a fun movie anyway.\n\nAnd um fun at the end.\n\nThe first one was a lot better.\n\nTarget sample: And um fun at the end.\n\nThe first one was a lot better. Specifically, we selected specific samples, using the text modality as an example, and visualized the changes in the retrieved positive and negative samples as the training steps increased, along with the corresponding similarity values. The results are displayed in Figure  5 . From Figure  5 , we gained the following observations. 1) In the initial stage, the similarity of positive and negative samples relative to the target modality is similar, but their semantic information is opposite. 2) As the training steps increase, the similarity of positive samples to the target modality increases, while the similarity of negative samples to the target modality decreases.\n\n3) The retrieved positive and negative samples are dynamically changing. This result indicates that relying solely on similarity-based retrieval may lead to samples with opposite semantics, which could negatively impact the model. After adding the contrastive cross-modal retrieval constraint, the similarity of samples with opposing semantics to the target modality is significantly reduced. This strongly demonstrates the effectiveness of the contrastive cross-modal retrieval module.\n\nTo gain deeper insights into the contrastive cross-modal retrieval module, we explained the changes in the similarity of specific target modality and the retrieved positive and negative samples as the training steps increase, as shown in Figure  6 . We could observe that the similarity between the target modality and the fixed retrieved positive samples increases, while the similarity with the negative samples exhibits an opposite trend. This result further confirms the effectiveness of the contrastive cross-modal retrieval module. This result further demonstrates the effectiveness of the contrastive crossmodal retrieval module, as it ensures that the model retrieves samples that are similar to the target modality both in terms of features and semantics, thereby enhancing the model's performance. In addition to visualizing the retrieval process, we also compare different contrastive learning strategies. As shown in Figure  7 , our proposed L ccr achieves comparable performance to the widely used InfoNCE loss, with a slight advantage.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "G. Hierarchical Prompt Analysis",
      "text": "To validate the effectiveness of hierarchical prompt, we conducted experiments to observe the impact of prompt length on model performance. The results are shown in Figure  8 . The left and right y-axes represent binary accuracy and the contribution rate of prompt units, respectively. The prompt unit contribution rate measures the contribution of prompt units of unit length to the model's performance. From Figure  8 , we could observe that the model's performance increases initially with the length of the prompt, then slightly decreases. The unit prompt contribution rate is decreasing. The model achieves the best performance when the prompt length is 128. This may be due to the fact that as the prompt length increases, the model gradually learns more information from both modalitylevel and sample-level reference contexts. The shorter prompt length fails to carry enough reference context information, while the longer prompt length causes the model to focus on trivial details, leading to a decline in performance. This result proves the effectiveness of modality-level and samplelevel prompts, as well as the fact that an appropriate prompt length can enhance model performance.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "H. Hyper-Parameter Analysis",
      "text": "To explore the impact of the hyperparameter Œª, we conducted experiments by increasing it from 0.0002 to 0.0018. The left y-axis corresponds to Acc2 and the right y-axis corresponds to the initial absolute difference between ŒªL ccr and L msa . Figure  9  displayed the experimental results. We could observe that the absolute difference between the two task losses first decreases and then increases, while the model performance initially improves and then declines. This can be attributed to the significant gap in task losses, indicating that the sub-tasks were not fully optimized. It also highlights the importance of the sub-task design, particularly the optimization of the contrastive cross-modal retrieval loss.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "I. Case Study",
      "text": "To qualitatively validate the effectiveness of our proposed model, we displayed several typical results, as shown in Figure  10 , which are made using only modality-level reference context and those made using both modality-level reference context and sample-level reference context. Based on these experimental results, we could observe that solely utilizing sample-level reference context to enhance the modality may lead to inaccurate or even incorrect sentiment polarity. The additional incorporation of sample-level reference context guides the model to output the correct sentiment polarity. This qualitative experimental result highlights the necessity and effectiveness of introducing sample-level reference context.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we present a multimodal retrieval-enhanced framework for multimodal sentiment analysis tasks. It is the",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Retrieved Content Prediction Label",
      "text": "But all in all i just thought it was really over overly sad.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Case",
      "text": "I mean that sounds really harsh. first work to explore the direct integration of external samples to enhance modalities through the retrieval mechanism. Specifically, we first design a cross-modal contrastive retrieval module that retrieves samples semantically similar to the target modality. We then introduce modality-level and sample-level prompts, leveraging inter-sample and intra-sample information to generate modality-level and sample-level reference contexts, respectively. Finally, we design a cross-modal retrievalenhanced encoder that utilizes modality-level and sample-level reference contexts to enhance the target modality, improving its representational capacity. Extensive experiments demonstrate the effectiveness and superiority of the proposed model.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Previous cross-modal interaction paradigm that solely rely on",
      "page": 1
    },
    {
      "caption": "Figure 1: , the multimodal sentiment analysis model",
      "page": 1
    },
    {
      "caption": "Figure 2: , we first design a",
      "page": 2
    },
    {
      "caption": "Figure 2: Specifically, we first introduce the contrastive cross-",
      "page": 2
    },
    {
      "caption": "Figure 2: Schematic illustration of the proposed TC2RAHP framework with three components, Cross-modal Contrastive Retrieval module, Prompt-based Cross-",
      "page": 3
    },
    {
      "caption": "Figure 2: illustrates the workflow of our model. Taking",
      "page": 3
    },
    {
      "caption": "Figure 3: The performance of our proposed model with several state-of-the-art",
      "page": 6
    },
    {
      "caption": "Figure 4: The performance of our proposed model with several state-of-the-art",
      "page": 6
    },
    {
      "caption": "Figure 5: Visualization of the changes in the positive and negative samples",
      "page": 7
    },
    {
      "caption": "Figure 6: The evolution of similarity between the target sample and the",
      "page": 7
    },
    {
      "caption": "Figure 5: From Figure 5, we gained the",
      "page": 7
    },
    {
      "caption": "Figure 6: We could observe that the similarity between the target",
      "page": 7
    },
    {
      "caption": "Figure 7: Comparison between our proposed Lccr and InfoNCE loss on the",
      "page": 8
    },
    {
      "caption": "Figure 8: The analysis of the impact of prompt length on model performance",
      "page": 8
    },
    {
      "caption": "Figure 7: , our proposed Lccr achieves comparable",
      "page": 8
    },
    {
      "caption": "Figure 8: The left and right y-axes represent binary accuracy and the",
      "page": 8
    },
    {
      "caption": "Figure 9: The experiment analyzing the impact of hyperparameter Œª on model",
      "page": 8
    },
    {
      "caption": "Figure 9: displayed the experimental results. We",
      "page": 8
    },
    {
      "caption": "Figure 10: , which are made using only modality-level reference",
      "page": 8
    },
    {
      "caption": "Figure 10: Case study on CMU-MOSI dataset. The model predicts values",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modality-level\nreference context": "Sample-level\nreference context"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Positive Sample": "Negative Sample"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ùë•ùëé\nùëñ": "ùë•ùë£\nùëñ"
        },
        {
          "ùë•ùëé\nùëñ": "ùë•ùë°\nùëñ"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Negative Samples\nùë•ùë°\nùëõùëé ÔÄÆÔÄÆÔÄÆ\nùë•ùë°\nùëõùë£ ÔÄÆÔÄÆÔÄÆ\nùë•ùë°\nùëõùë° ÔÄÆÔÄÆÔÄÆ": "Positive Samples\nùë•ùë°\nùëùùëé ÔÄÆÔÄÆÔÄÆ\nùë•ùë°\nùëùùë£ ÔÄÆÔÄÆÔÄÆ\nùë•ùë°"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Targetsample: But none of those things matter\n+1.40 its a funmovie anyway.": "Retrieval Retrieval Retrieval\nRetrieval positive samples Retrieval positive samples Retrieval positive samples\nAnd um en f d u . nat the Si 0 m .3 il 2 ar 8 ity + L 1 a .2 be 0 l Woul k d i d b s e t g o r o e . atfor Si 0 m . i 3 la 9 r 4 ity + L 1 a . b 4 e 0 l And um en f d u . nat the Si 0 m . i 4 la 7 r 6 ity + L 1 a . b 2 e 0 l\n0.069+1.80 0.075+0.40 0.132+1.80\n0.051+1.30 0.086+2.40 0.117+1.30\nRetrieval negative samples Retrieval negative samples Retrieval negative samples\nThe first one was SimilarityLabel The first one was SimilarityLabel I think the film SimilarityLabel\na lot better. 0.299 -0.80 a lot better. 0.239-0.80 was awful. 0.178 -2.00\n0.059-0.80 0.042-0.80 0.031 -1.60\n0.050-1.20 0.043-1.20 0.036-0.60"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for computational linguistics. Meeting"
    },
    {
      "citation_id": "2",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "W Rahman",
        "M Hasan",
        "S Lee",
        "A Zadeh",
        "C Mao",
        "L.-P Morency",
        "E Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the conference. Association for computational linguistics. Meeting"
    },
    {
      "citation_id": "3",
      "title": "Mtag: Modal-temporal attention graph for unaligned human multimodal language sequences",
      "authors": [
        "J Yang",
        "Y Wang",
        "R Yi",
        "Y Zhu",
        "A Rehman",
        "A Zadeh",
        "S Poria",
        "L.-P Morency"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter"
    },
    {
      "citation_id": "4",
      "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
      "authors": [
        "F Lv",
        "X Chen",
        "Y Huang",
        "L Duan",
        "G Lin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "5",
      "title": "Bi-bimodal modality fusion for correlation-controlled multimodal sentiment analysis",
      "authors": [
        "W Han",
        "H Chen",
        "A Gelbukh",
        "A Zadeh",
        "L -P. Morency",
        "S Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 international conference on multimodal interaction"
    },
    {
      "citation_id": "6",
      "title": "Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis",
      "authors": [
        "W Yu",
        "H Xu",
        "Z Yuan",
        "J Wu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "7",
      "title": "Disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "D Yang",
        "S Huang",
        "H Kuang",
        "Y Du",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM international conference on multimedia"
    },
    {
      "citation_id": "8",
      "title": "Tmmda: A new token mixup multimodal data augmentation for multimodal sentiment analysis",
      "authors": [
        "X Zhao",
        "Y Chen",
        "S Liu",
        "X Zang",
        "Y Xiang",
        "B Tang"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM Web Conference 2023"
    },
    {
      "citation_id": "9",
      "title": "Decoupled multimodal distilling for emotion recognition",
      "authors": [
        "Y Li",
        "Y Wang",
        "Z Cui"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "10",
      "title": "Confede: Contrastive feature decomposition for multimodal sentiment analysis",
      "authors": [
        "J Yang",
        "Y Yu",
        "D Niu",
        "W Guo",
        "Y Xu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "11",
      "title": "Disentanglement translation network for multimodal sentiment analysis",
      "authors": [
        "Y Zeng",
        "W Yan",
        "S Mai",
        "H Hu"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "12",
      "title": "Retrieval-augmented generation for large language models: A survey",
      "authors": [
        "Y Gao",
        "Y Xiong",
        "X Gao",
        "K Jia",
        "J Pan",
        "Y Bi",
        "Y Dai",
        "J Sun",
        "H Wang"
      ],
      "venue": "Retrieval-augmented generation for large language models: A survey"
    },
    {
      "citation_id": "13",
      "title": "Retrieval augmented classification for long-tail visual recognition",
      "authors": [
        "A Long",
        "W Yin",
        "T Ajanthan",
        "V Nguyen",
        "P Purkait",
        "R Garg",
        "A Blair",
        "C Shen",
        "A Van Den",
        "Hengel"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "14",
      "title": "In-context retrieval-augmented language models",
      "authors": [
        "O Ram",
        "Y Levine",
        "I Dalmedigos",
        "D Muhlgay",
        "A Shashua",
        "K Leyton-Brown",
        "Y Shoham"
      ],
      "year": "2023",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "15",
      "title": "Retrieval-augmented diffusion models",
      "authors": [
        "A Blattmann",
        "R Rombach",
        "K Oktay",
        "J M√ºller",
        "B Ommer"
      ],
      "year": "2022",
      "venue": "Proceedings of the 36th International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "16",
      "title": "Cross-modal retrieval augmentation for multi-modal classification",
      "authors": [
        "S Gur",
        "N Neverova",
        "C Stauffer",
        "S.-N Lim",
        "D Kiela",
        "A Reiter"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "17",
      "title": "Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
      "authors": [
        "W Han",
        "H Chen",
        "S Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Y.-H Tsai",
        "P Liang",
        "A Zadeh",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "International Conference on Representation Learning"
    },
    {
      "citation_id": "19",
      "title": "Multimodal sentiment analysis with word-level fusion and reinforcement learning",
      "authors": [
        "M Chen",
        "S Wang",
        "P Liang",
        "T Baltru≈°aitis",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "20",
      "title": "Acformer: An aligned and compact transformer for multimodal sentiment analysis",
      "authors": [
        "D Zong",
        "C Ding",
        "B Li",
        "J Li",
        "K Zheng",
        "Q Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM international conference on multimedia"
    },
    {
      "citation_id": "21",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "22",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "Excavating multimodal correlation for representation learning",
      "authors": [
        "S Mai",
        "Y Sun",
        "Y Zeng",
        "H Hu"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "24",
      "title": "Conki: Contrastive knowledge injection for multimodal sentiment analysis",
      "authors": [
        "Y Yu",
        "M Zhao",
        "S -A. Qi",
        "F Sun",
        "B Wang",
        "W Guo",
        "X Wang",
        "L Yang",
        "D Niu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "25",
      "title": "Modal feature optimization network with prompt for multimodal sentiment analysis",
      "authors": [
        "X Zhang",
        "W Wei",
        "S Zou"
      ],
      "year": "2025",
      "venue": "Proceedings of the 31st International Conference on Computational Linguistics"
    },
    {
      "citation_id": "26",
      "title": "t-hne: A text-guided hierarchical noise eliminator for multimodal sentiment analysis",
      "authors": [
        "Z Li",
        "L Li"
      ],
      "year": "2025",
      "venue": "Proceedings of the 31st International Conference on Computational Linguistics"
    },
    {
      "citation_id": "27",
      "title": "A survey of multimodel large language models",
      "authors": [
        "Z Liang",
        "Y Xu",
        "Y Hong",
        "P Shang",
        "Q Wang",
        "Q Fu",
        "K Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 3rd International Conference on Computer"
    },
    {
      "citation_id": "28",
      "title": "Mixtral of experts",
      "year": "2023",
      "venue": "Mixtral of experts"
    },
    {
      "citation_id": "29",
      "title": "Gemma -google's new open llm",
      "authors": [
        "Google"
      ],
      "venue": "Gemma -google's new open llm"
    },
    {
      "citation_id": "30",
      "title": "Chatgpt: Large-scale language model fine-tuned for conversational applications",
      "authors": [
        "Openai"
      ],
      "venue": "Chatgpt: Large-scale language model fine-tuned for conversational applications"
    },
    {
      "citation_id": "31",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "H Touvron",
        "L Martin",
        "K Stone",
        "P Albert",
        "A Almahairi",
        "Y Babaei",
        "N Bashlykov",
        "S Batra",
        "P Bhargava",
        "S Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "32",
      "title": "Gemini: our largest and most capable ai model",
      "authors": [
        "Google"
      ],
      "year": "2023",
      "venue": "Gemini: our largest and most capable ai model"
    },
    {
      "citation_id": "33",
      "title": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models"
    },
    {
      "citation_id": "34",
      "title": "Joint and individual matrix factorization hashing for large-scale cross-modal retrieval",
      "authors": [
        "D Wang",
        "Q Wang",
        "L He",
        "X Gao",
        "Y Tian"
      ],
      "year": "2020",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "35",
      "title": "Active retrieval augmented generation",
      "authors": [
        "Z Jiang",
        "F Xu",
        "L Gao",
        "Z Sun",
        "Q Liu",
        "J Dwivedi-Yu",
        "Y Yang",
        "J Callan",
        "G Neubig"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "36",
      "title": "Weakly-supervised scientific document classification via retrieval-augmented multi-stage training",
      "authors": [
        "R Xu",
        "Y Yu",
        "J Ho",
        "C Yang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "37",
      "title": "Retrievalaugmented generation for knowledge-intensive nlp tasks",
      "authors": [
        "P Lewis",
        "E Perez",
        "A Piktus",
        "F Petroni",
        "V Karpukhin",
        "N Goyal",
        "H K√ºttler",
        "M Lewis",
        "W -T. Yih",
        "T Rockt√§schel"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "38",
      "title": "Vendi-rag: Adaptively trading-off diversity and quality significantly improves retrieval augmented generation with llms",
      "authors": [
        "M Rezaei",
        "A Dieng"
      ],
      "year": "2025",
      "venue": "Vendi-rag: Adaptively trading-off diversity and quality significantly improves retrieval augmented generation with llms",
      "arxiv": "arXiv:2502.11228"
    },
    {
      "citation_id": "39",
      "title": "Timerag: Boosting llm time series forecasting via retrieval-augmented generation",
      "authors": [
        "S Yang",
        "D Wang",
        "H Zheng",
        "R Jin"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "40",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "41",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "42",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "43",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "44",
      "title": "Multimodal prompt learning with missing modalities for sentiment analysis and emotion recognition",
      "authors": [
        "Z Guo",
        "T Jin",
        "Z Zhao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "45",
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "46",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "≈Å Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "47",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "48",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "49",
      "title": "Learning to learn better unimodal representations via adaptive multimodal meta-learning",
      "authors": [
        "Y Sun",
        "S Mai",
        "H Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "Hydiscgan: a hybrid distributed cgan for audio-visual privacy preservation in multimodal sentiment analysis",
      "authors": [
        "Z Wu",
        "Q Zhang",
        "D Miao",
        "K Yi",
        "W Fan",
        "L Hu"
      ],
      "year": "2024",
      "venue": "Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "51",
      "title": "Mofn: Multi-offset-flow-based network for video restoration and enhancement",
      "authors": [
        "Y Chen",
        "Y Wang",
        "Y Liu"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)"
    },
    {
      "citation_id": "52",
      "title": "Mm-instructeval: Zero-shot evaluation of (multimodal) large language models on multimodal reasoning tasks",
      "authors": [
        "X Yang",
        "W Wu",
        "S Feng",
        "M Wang",
        "D Wang",
        "Y Li",
        "Q Sun",
        "Y Zhang",
        "X Fu",
        "S Poria"
      ],
      "year": "2024",
      "venue": "Mm-instructeval: Zero-shot evaluation of (multimodal) large language models on multimodal reasoning tasks",
      "arxiv": "arXiv:2405.07229"
    },
    {
      "citation_id": "53",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "H Chung",
        "L Hou",
        "S Longpre",
        "B Zoph",
        "Y Tay",
        "W Fedus",
        "Y Li",
        "X Wang",
        "M Dehghani",
        "S Brahma"
      ],
      "year": "2024",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "54",
      "title": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "authors": [
        "J Bai",
        "S Bai",
        "S Yang",
        "S Wang",
        "S Tan",
        "P Wang",
        "J Lin",
        "C Zhou",
        "J Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "arxiv": "arXiv:2308.12966"
    },
    {
      "citation_id": "55",
      "title": "Instructblip: Towards general-purpose visionlanguage models with instruction tuning",
      "authors": [
        "W Dai",
        "J Li",
        "D Li",
        "A Tiong",
        "J Zhao",
        "W Wang",
        "B Li",
        "P Fung",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Instructblip: Towards general-purpose visionlanguage models with instruction tuning"
    },
    {
      "citation_id": "56",
      "title": "Gpt-4v(ision) system card",
      "authors": [
        "Openai"
      ],
      "venue": "Gpt-4v(ision) system card"
    },
    {
      "citation_id": "57",
      "title": "Meet claude",
      "authors": [
        "Anthropic"
      ],
      "year": "2024",
      "venue": "Meet claude"
    }
  ]
}