{
  "paper_id": "2406.07900v1",
  "title": "Exploring Self-Supervised Multi-View Contrastive Learning For Speech Emotion Recognition With Limited Annotations",
  "published": "2024-06-12T06:06:55Z",
  "authors": [
    "Bulat Khaertdinov",
    "Pedro Jeuris",
    "Annanda Sousa",
    "Enrique Hortal"
  ],
  "keywords": [
    "speech emotion recognition",
    "self-supervised learning",
    "contrastive learning",
    "sparse annotations"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent advancements in Deep and Self-Supervised Learning (SSL) have led to substantial improvements in Speech Emotion Recognition (SER) performance, reaching unprecedented levels. However, obtaining sufficient amounts of accurately labeled data for training or fine-tuning the models remains a costly and challenging task. In this paper, we propose a multiview SSL pre-training technique that can be applied to various representations of speech, including the ones generated by large speech models, to improve SER performance in scenarios where annotations are limited. Our experiments, based on wav2vec 2.0, spectral and paralinguistic features, demonstrate that the proposed framework boosts the SER performance, by up to 10% in Unweighted Average Recall, in settings with extremely sparse data annotations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is a crucial task of Affective Computing that gained a significant amount of research attention in the last decades. Speech serves as a key marker for effective emotion recognition, encompassing diverse acoustic, prosodic, and other voice-related information and accounting for inter-speaker differences  [1] . During the last ten years, Speech Emotion Recognition (SER) algorithms have been significantly improved due to the rapid development of Deep Learning architectures. The earlier methods of the decade were based on the end-to-end supervised Deep Learning models exploiting prosodic or spectral features  [2, 3, 4] , or raw audio waveforms  [5] . In the last couple of years, the research focus has been shifting towards exploiting Transformer-based large speech models, such as wav2vec 2.0  [6] , WavLM  [7] , and HuBERT  [8] , pre-trained via Self-Supervised Learning frameworks  [9, 10, 11, 12] .\n\nOne of the key challenges always associated with emotion recognition is collecting data with trustworthy annotations  [13] . Furthermore, emotion recognition systems could be deployed in various scenarios requiring data collection in natural settings and utilizing specific emotional models that are not covered in open access data  [14, 15, 16] . In this case, acquiring a dataset even with hundreds of samples containing effectively elicited emotions and accurate annotations is an extremely challenging and time-consuming process  [17] . Deep learning models trained from scratch typically require large amounts of accurately annotated data to achieve satisfactory performance, whereas large pre-trained models can be fine-tuned with less, but still significant, amounts of annotated data.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "* Authors Contributed Equally To This Work",
      "text": "In this paper, motivated by these challenges, we introduce multi-view contrastive SSL pre-training that can be applied on top of various audio features (views), including paralinguistic cues, spectral representations, and features extracted by large speech models pre-trained on ASR datasets. The contributions of this work can be summarized as follows:\n\n• The introduced framework, denoted as Pairwise-CL, aims to pre-train encoders on multiple speech views for further finetuning with sparsely annotated data. Pre-training is based on contrastive SSL loss computed between representations of speech views in a pairwise fashion. Specifically, the encoders from the selected views aim to align representations of each utterance in the projected latent space. • The proposed framework can be adapted to any combination and number of views. The experiments in this paper were conducted on three types of views, namely wav2vec 2.0 features  [6] , mel spectrograms and eGeMAPS-88  [2] . • We analyze the representations learnt from each view and quantify their alignment using projection-weighted Canonical Correlation Analysis (PWCCA)  [18] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pairwise-Cl: Multi-View Contrastive Learning",
      "text": "In the last years, contrastive Self-Supervised Learning has shown promising results in multi-modal and multi-view pretraining in different domains  [19, 20, 21, 22, 23] . The main idea lies in maximizing similarities between different representations of the same instance in a projected latent space while contrasting them to other instances. The pre-training strategy introduced in this paper is inspired by Contrastive Multiview Coding (CMC)  [19]  suggested for multi-view image representation learning. Namely, we propose using normalized temperaturescaled cross-entropy loss (NT-Xent)  [24]  in between pairs of view-level representations corresponding to different audio features. We denote the proposed framework as Pairwise-CL. Formally, assume there is a mini-batch of size N with features from\n\nl ∈ Xi from i-th view to a vector of size di. The view-level representation dimensionality di is based on the encoder architecture processing the view. Then, the features from each view are mapped to the space where contrastive loss is computed using separate projection networks gi : Φi → Λ ⊂ R D , i.e. z i l = gi(fi(x i l )). Thus, the set of projected representations can be written as {z 1  l , z 2 l , . . . , z K l } N l=1 . A pair of projected representations z i l and z j l is considered positive as they correspond to views of the same l-th instance in a mini-batch. The NT-Xent loss l i→j l treating i-th view from arXiv:2406.07900v1 [cs.CL] 12 Jun 2024  l-th example as an anchor can be computed as follows:\n\nwhere δ(z i l , z j l ) = exp(\n\n) and s(•) is the cosine similarity function  [24] . Therefore, the total loss aggregated for the whole mini-batch of views i and j can be averaged as:\n\nFurthermore, each instance l in a mini-batch is represented by K different views. In the proposed Pairwise-CL, we compute losses between all pairs of views, and average them:\n\nwhere C(K, 2) is a number of possible pairs from K views. Therefore, the proposed loss function aims to maximize the similarities for multi-view representations {z 1  l , z 2 l , . . . , z K l } corresponding to the same l-th instance.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Utilizing The Proposed Framework",
      "text": "The proposed pre-training framework can be applied to any number of speech views. In this study, we evaluate the framework using a combination of three views, namely wav2vec 2.0 features, eGeMAPS-88 low-level descriptors, and mel-scale spectrograms, as shown in Figure  1 . This choice is based on their ability to capture different characteristics of speech  [25] . Pre-training. Representations from each view are processed by a view-specific projection network before computing a pairwise contrastive loss as shown in Figure  2 . Thus, the encoders are trained on unlabeled audio signals to align representations of  views from corresponding instances by maximizing cosine similarities among them. We highlight that our approach is only focused on pre-training the view-level encoders, also referred to as downstream architectures for large speech models  [12] . Thus, during pre-training, the wav2vec 2.0 model is frozen and used as a feature extraction method, unlike in relevant studies exploring tuning wav2vec 2.0 parameters  [26] . Fine-tuning. Each of the view-level encoders can be fine-tuned by adding a classifier on top of the learnt representations (Figure  1b ). During fine-tuning, the view-level encoders can be either frozen or further tuned via backpropagation using a supervision signal from labeled speech instances.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Implementation Details",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data",
      "text": "The experiments in this study are based on the IEMOCAP dataset  [27]  frequently exploited in the SER literature. The data is collected with 10 subjects in 5 sessions (2 subjects per session). In particular, we use two versions of the dataset in this paper. First, the full version of the dataset, which we refer to as IEMOCAP-10, contains about 10,000 audio samples with 10 distinct emotion annotations. In recent research studies  [9, 11]  a subset of this dataset with 5,531 samples 1 and 4 emotions (neutral, angry, sad, and happy merged with excited), which we denote as IEMOCAP-4, is commonly used. We use IEMOCAP-10 without labels for pre-training purposes, whereas IEMOCAP-4 is mainly used for fine-tuning. We exploit a leave-one-sessionout cross-validation (5-fold) protocol consistent between pretraining and fine-tuning data in order to prevent data leakage. In each cross-validation iteration, one session is used for testing purposes, another session is used for validation and early stopping, and three remaining sessions are used for training. We use Unweighted Average Recall (UAR) and Weighted Accuracy (WA) as metrics for SER.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Views And Feature Encoders",
      "text": "We applied our multi-view pre-training strategy to three views of audio signals downsampled to 16,000 Hz. Each of these views is processed with a view-specific backbone architecture. wav2vec 2.0. We use a base version of the wav2vec 2.0 model  [6]  pre-trained on the LibriSpeech dataset  [28]   chaudio 2 . We trim or pad all audio inputs to a 15-second length  [9]  before feeding them to the model. The generated features are passed through the view-level encoder as proposed in  [9] .\n\nIn particular, the outputs of the CNN and transformer blocks are averaged with learnable weights and passed through a twolayer pointwise 1D-CNN, that outputs vectors of size 128.\n\nMel-scale spectrograms. The mel spectrograms are extracted with a 25-millisecond window length and a 10-millisecond hop  [29, 30] . We employed 64 mel filterbanks covering frequencies from 60 Hz to 7800 Hz. We trim or pad all audio inputs to a 15-second length before generating spectrograms. The obtained spectrograms are then fed to a CNN backbone with three layers. eGeMAPS-88. The extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS-88)  [2]  contains 88 derived parameters related to frequency (pitch, jitter), energy, and spectrum aggregated for the whole utterance. We generated these features using the opensmile 3 package. A two-layered MLP with 256 and 128 neurons is used as a view-level encoder.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Pre-Training And Fine-Tuning",
      "text": "During pre-training, we used an MLP with 2 layers of size 256 and 128 as a projection head. The models are pre-trained for 100 epochs with a per-view batch size of 128 and early stopping after 30 epochs with no improvement in validation loss. In the fine-tuning stage, the projection head is dropped and the features are directly passed to a linear classification head with softmax activation. For both pre-training and fine-tuning, we use the Adam optimization algorithm with an initial learning rate of 0.001. During fine-tuning, we decrease the learning rate by a factor of 0.9 after every 5 epochs with no improvement in validation UAR. Pre-training and fine-tuning have been conducted using Nvidia Quadro RTX 5000 GPU (16GB VRAM) with features extracted in advance. With this setup, Pairwise-CL pretraining takes approximately 13 minutes per epoch, whereas fine-tuning time varies based on the used view: 2 seconds for eGEMAPS, 10 seconds for spectrograms, and about 3 minutes for wav2vec 2.0 representations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluations",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fully Annotated Dataset And Temperature",
      "text": "Grid search for temperature. First, we conduct experiments to identify the optimal value of temperature τ in the contrastive loss function (Equation  1 ). We pre-trained view-level encoders on IEMOCAP-10 with τ ∈ {0.1, 0.25, 0.  Figure  3 : UAR for fine-tuning with limited amounts of labeled data: * -statistically significant differences, ns -not significant.\n\neach view with the same view-level encoder and classifier architectures. As can be seen from the table, the pre-trained models obtain higher performance in terms of UAR and Weighted Accuracy on wav2vec 2.0 and eGeMAPS-88 features. Besides, pre-trained model performance (with temperature τ = 0.5) is comparable when using mel spectrograms. These results demonstrate that the proposed pre-training strategy, in some cases, can further improve performance when large annotated datasets are available for both pre-training and fine-tuning. Besides, the models pre-trained with temperature τ = 0.5 achieve the highest or the second-highest results for almost all views and metrics. Thus, these pre-training settings will be further explored in the subsequent experiments. Fine-tune or freeze? To evaluate the feature representations learnt by encoders on the SSL task only, we compare tuned and frozen encoders during fine-tuning in Table  2 . As can be seen, the framework with frozen encoders is less effective. In particular, fine-tuning the view-level encoder on top of the wav2vec features leads to the largest improvement (almost 8% UAR), compared to the frozen view-level encoder. However, it is worth mentioning that the proposed pre-training allows us to obtain about 50-55% UAR for all views without tuning the encoders with labels. The gap between the models is less notable (about 3% UAR) for eGeMAPS-88 and mel spectrograms.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Limited Annotated Data",
      "text": "Pairwise-CL vs Supervised. The main motivation of our study is to suggest a pre-training strategy for settings with small amounts of labeled data. Thus, we simulate the scenario with limited annotated data available for fine-tuning by using p ∈ {2%, 5%, 10%, 25%} of training data from each class in IEMOCAP-4. We fine-tune the pre-trained encoder and train the supervised encoder from scratch models 10 times for each proportion of annotations p. In Figure  3 , we report the average UAR values obtained for each p along with 95% confidence intervals. Besides, we conduct the Mann-Whitney U-test (α = 0.05) to check for statistically significant differences between the supervised and pre-trained models.\n\nAccording to the obtained metrics, the proposed pretraining strategy significantly improves UAR for all three views in cases with extremely limited annotations (p ∈ {2%, 5%}). In these cases, the fine-tuning data amounts to approximately 100 and 250 labeled examples per training set (3 session folds). The performance gaps are particularly high for handcrafted features, where improvements reach up to 10-15% in UAR. For spectral features, supervised models outperform pre-trained ones starting from 10% of annotations available, whereas, for eGeMAPS, the pre-training strategy is beneficial for all values of p.\n\nPre-training data distribution. In the previous experiment, the models were pre-trained on IEMOCAP-10 which contains 10 emotions, from which 5 (happy and excited are merged) are presented in the fine-tuning IEMOCAP-4 dataset. Thus, the remaining emotions are not relevant for fine-tuning. Even though such a scenario represents a realistic case when only some parts of the dataset are annotated, it is interesting to explore how the distribution of pre-training data affects the performance on downstream emotions. In particular, we conduct pre-training on IEMOCAP-4 containing target emotions only. Furthermore, we pre-train another set of encoders on the remaining part of IEMOCAP with out-of-distribution emotions only. We compare both sets of models after fine-tuning them with sparse annotations on IEMOCAP-4 (Figure  4 ). On average, models pretrained on target distribution data show comparable or better performance, with statistically significant differences observed for wav2vec 2.0 at 2% of annotations, spectrograms at 5% and 10%, and eGeMAPS at 10% and 25%. Nevertheless, the gaps in performance for the most sparse annotations are generally smaller compared to the ones reported in Figure  3 . Thus, model pre-training with target emotions is beneficial but does not lead to large improvements when annotations are limited.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "View-Level Representations And Alignment",
      "text": "Figure  5  demonstrates the representations learnt by pre-trained and supervised view-level encoders in a two-dimensional space using t-SNE  [31] . The supervised models were trained on fully annotated IEMOCAP-4. The illustrated data points correspond to the unseen test subjects from the last cross-validation fold.\n\nThe proposed pre-training strategy aims to align representations of different audio signal views. We utilize a projectionweighted Canonical Correlation Analysis (PWCCA) to quantify their alignment. PWCCA has been introduced in  [18]  as a technique for identifying common structures in features and exploring the similarities between deep representations. Table  3  compares the PWCCA scores obtained for representations of viewlevel encoders after pre-training to those of supervised models trained independently. The highest level of alignment is obtained for the combination of wav2vec 2.0 and spectral views. Interestingly, even though the PWCCA scores are comparable for pairs with eGeMAPS, there are significant gains in performance for this view after pre-training according to Figure  3c .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduced a lightweight contrastive SSL strategy to refine representations of speech in SER settings with sparsely annotated data. Specifically, we evaluated the strategy for three types of views, namely eGeMAPS-88, mel spectrograms, and wav2vec",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "8. Supplementary Materials",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "8.1. Number Of Parameters",
      "text": "In Table  5 , we present the number of frozen and trainable parameters in the utilized models during pre-training and finetuning. As highlighted in Section 2.2, we did not tune the parameters of wav2vec 2.0 and used it as a feature encoder. Thus, the number of frozen parameters in all models exploiting this architecture is no less than the number of wav2vec 2.0 parameters (94.5 million). It can be seen, that the proposed pre-training method outperforms plain fine-tuning on limited data (Figure  3a  from the paper) by tuning a small number of parameters corresponding to the view-level encoder applied on top of wav2vec 2.0 features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fully-Annotated Fine-Tuning: Extended Results",
      "text": "In Tables  1  and 2  from the paper, we demonstrated the summary of the fine-tuning results averaged over unseen test folds in leave-one-session-out cross-validation settings of IEMOCAP-4. Specifically, we tried out different temperature values and freezing or tuning the view-level encoders during fine-tuning.\n\nIn Table  4  (next page), we present a more thorough summary of the results given all possible combinations between these hyperparameters along with the average metrics obtained on validation and test sessions. Furthermore, we computed the performance ranks (1 -highest metric score, 9 -lowest) of models for each metric and data split, and averaged them for validation and test. The average ranks are presented in the last two columns of the table. As can be seen, the highest ranks on both validation (2.33) and test (1.83) data correspond to the model pre-trained with temperature τ = 0.5, which have been further used in the experiments with sparse annotations (Section 4.2 from the paper).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Randomly Initialized Representations",
      "text": "In Figure  5  from the paper, we visualize the feature representations produced by the view-level encoder after pre-training and supervised learning from scratch. As a baseline, in Figure  6 , we also demonstrate the t-SNE scatter plot right after random initialization of vier-level encoders, i.e. before any type of training has been applied to them. According to the figure, eGeMAPS-88 and wav2vec 2.0 representations have some initial structure. For wav2vec 2.0, this can be explained by the fact that this method has already been pre-trained on raw speech, whereas eGeMAPS-88 is a set of features that extract handcrafted features meaningful for recognizing emotions.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed multi-view SSL framework for speech",
      "page": 2
    },
    {
      "caption": "Figure 1: This choice is based on",
      "page": 2
    },
    {
      "caption": "Figure 2: Thus, the encoders are",
      "page": 2
    },
    {
      "caption": "Figure 2: Pairwise contrastive loss calculation. Representa-",
      "page": 2
    },
    {
      "caption": "Figure 1: b). During fine-tuning, the view-level encoders can be either",
      "page": 2
    },
    {
      "caption": "Figure 3: UAR for fine-tuning with limited amounts of labeled",
      "page": 3
    },
    {
      "caption": "Figure 4: Comparison of model pre-trained on datasets with",
      "page": 4
    },
    {
      "caption": "Figure 3: , we report the av-",
      "page": 4
    },
    {
      "caption": "Figure 4: ). On average, models pre-",
      "page": 4
    },
    {
      "caption": "Figure 3: Thus, model",
      "page": 4
    },
    {
      "caption": "Figure 5: Representations from the test set projected onto the",
      "page": 4
    },
    {
      "caption": "Figure 5: demonstrates the representations learnt by pre-trained",
      "page": 4
    },
    {
      "caption": "Figure 5: from the paper, we visualize the feature represen-",
      "page": 6
    },
    {
      "caption": "Figure 6: , we also demonstrate the t-SNE scatter plot right after ran-",
      "page": 6
    },
    {
      "caption": "Figure 6: Representations of randomly initialized view-level en-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Results for pre-trained models with frozen",
      "data": [
        {
          "τ\nMethod": "Supervised\n-",
          "wav2vec 2.0\nUAR\nWA": "62.11\n59.43",
          "Spectral\nUAR\nWA": "53.39\n52.09",
          "eGeMAPS-88\nUAR\nWA": "49.97\n48.1"
        },
        {
          "τ\nMethod": "0.1\n0.25\nPairwise-CL\n0.5\n1.0",
          "wav2vec 2.0\nUAR\nWA": "62.24\n61.51\n61.59\n62.38\n63.11\n61.36\n62.35\n60.88",
          "Spectral\nUAR\nWA": "51.2\n49.94\n50.69\n50.17\n53.37\n51.97\n51.97\n49.61",
          "eGeMAPS-88\nUAR\nWA": "49.69\n51.38\n50.77\n47.99\n52.96\n49.16\n50.34\n48.43"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Bimodal emotion recognition through audio-visual cues",
      "authors": [
        "E Ghaleb"
      ],
      "year": "2021",
      "venue": "Bimodal emotion recognition through audio-visual cues"
    },
    {
      "citation_id": "3",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "4",
      "title": "Autoencoder-based unsupervised domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "5",
      "title": "An experimental study of speech emotion recognition based on deep convolutional neural networks",
      "authors": [
        "W Zheng",
        "J Yu",
        "Y Zou"
      ],
      "year": "2015",
      "venue": "2015 international conference on affective computing and intelligent interaction (ACII)"
    },
    {
      "citation_id": "6",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "7",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "8",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "11",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu",
        "I Gat",
        "M Damasceno",
        "H Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Speech self-supervised representations benchmarking: a case for larger probing heads",
      "authors": [
        "S Zaiem",
        "Y Kemiche",
        "T Parcollet",
        "S Essid",
        "M Ravanelli"
      ],
      "year": "2023",
      "venue": "Speech self-supervised representations benchmarking: a case for larger probing heads",
      "arxiv": "arXiv:2308.14456"
    },
    {
      "citation_id": "14",
      "title": "A review on five recent and near-future developments in computational processing of emotion in the human voice",
      "authors": [
        "D Schuller",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "15",
      "title": "Personalized estimation of engagement from videos using active learning with deep reinforcement learning",
      "authors": [
        "O Rudovic",
        "H Park",
        "J Busche",
        "B Schuller",
        "C Breazeal",
        "R Picard"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "16",
      "title": "A multimodal hierarchical approach to speech emotion recognition from audio and text",
      "authors": [
        "P Singh",
        "R Srivastava",
        "K Rana",
        "V Kumar"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition in adaptive virtual reality settings: Challenges and opportunities",
      "authors": [
        "S Mousavi",
        "B Khaertdinov",
        "P Jeuris",
        "E Hortal",
        "D Andreoletti",
        "S Giordano"
      ],
      "year": "2023",
      "venue": "CEUR Workshop Proceedings"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "19",
      "title": "Insights on representational similarity in neural networks with canonical correlation",
      "authors": [
        "A Morcos",
        "M Raghu",
        "S Bengio"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "20",
      "title": "Contrastive multiview coding",
      "authors": [
        "Y Tian",
        "D Krishnan",
        "P Isola"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "21",
      "title": "Contrastive learning with cross-modal knowledge mining for multimodal human activity recognition",
      "authors": [
        "R Brinzea",
        "B Khaertdinov",
        "S Asteriadis"
      ],
      "year": "2022",
      "venue": "2022 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "22",
      "title": "Multi-level feature learning for contrastive multi-view clustering",
      "authors": [
        "J Xu",
        "H Tang",
        "Y Ren",
        "L Peng",
        "X Zhu",
        "L He"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Context or knowledge is not always necessary: A contrastive learning framework for emotion recognition in conversations",
      "authors": [
        "G Tu",
        "B Liang",
        "R Mao",
        "M Yang",
        "R Xu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "24",
      "title": "Imagebind: One embedding space to bind them all",
      "authors": [
        "R Girdhar",
        "A El-Nouby",
        "Z Liu",
        "M Singh",
        "K Alwala",
        "A Joulin",
        "I Misra"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "26",
      "title": "Exploration of a selfsupervised speech model: A study on emotional corpora",
      "authors": [
        "Y Li",
        "Y Mohamied",
        "P Bell",
        "C Lai"
      ],
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "27",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "29",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Contrastive learning of general-purpose audio representations",
      "authors": [
        "A Saeed",
        "D Grangier",
        "N Zeghidour"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural networks: the official journal of the International Neural Network Society"
    },
    {
      "citation_id": "32",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}