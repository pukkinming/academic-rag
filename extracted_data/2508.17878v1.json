{
  "paper_id": "2508.17878v1",
  "title": "Enhancing Speech Emotion Recognition With Multi-Task Learning And Dynamic Feature Fusion",
  "published": "2025-08-25T10:32:24Z",
  "authors": [
    "Honghong Wang",
    "Jing Deng",
    "Fanqin Meng",
    "Rong Zheng"
  ],
  "keywords": [
    "speech emotion recognition",
    "multi-task learning",
    "class imbalance"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This study investigates fine-tuning self-supervised learning (SSL) models using multi-task learning (MTL) to enhance speech emotion recognition (SER). The framework simultaneously handles four related tasks: emotion recognition, gender recognition, speaker verification, and automatic speech recognition. An innovative co-attention module is introduced to dynamically capture the interactions between features from the primary emotion classification task and auxiliary tasks, enabling context-aware fusion. Moreover, We introduce the Sample Weighted Focal Contrastive (SWFC) loss function to address class imbalance and semantic confusion by adjusting sample weights for difficult and minority samples. The method is validated on the Categorical Emotion Recognition task of the Speech Emotion Recognition in Naturalistic Conditions Challenge, showing significant performance improvements.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recently, the rapid advancement of artificial intelligence has significantly improved the role of speech in human-computer interaction (HCI)  [1] . As a crucial element of HCI, Speech Emotion Recognition (SER) has emerged as a key area of research in multimodal emotion analysis, with the goal of accurately identifying and interpreting the speaker's emotional state.\n\nSER currently encounters several challenges. A major challenge is the scarcity of high-quality emotion-labeled datasets. Furthermore, the emotional features of speech are dynamic and varying over time, with valuable information distributed unevenly in the domain of time and frequency  [2] . Traditional deep learning models, such as convolutional neural networks (CNNs)  [3] , are constrained by a fixed perceptual field, making it difficult to accurately capture emotional cues, which often leads to subpar recognition performance. To address these challenges, researchers have increasingly turned to self-supervised learning (SSL) models, leading to the development of powerful end-to-end speech representation models like wav2vec 2.0  [4] , WavLM  [5] , and HuBERT  [6] . These models improve emotion recognition by extracting high-dimensional representations from raw waveform signals, leveraging large-scale unsupervised pre-training to capture rich semantic, phonetic, and paralinguistic information  [7] .\n\nSince paralinguistic and acoustic features are believed to carry personalized emotional information  [8] , researchers have increasingly turned to multi-task learning (MTL) approaches  [9, 10] , which simultaneously train the SER task alongside these auxiliary tasks. By sharing parameters across multiple tasks, the model can better extract emotional information from speech features, resulting in significant improvements in SER performance  [11] . However, traditional MTL methods often fail to fully take advantage of the representations in each auxiliary task. As a result, integrating information from paralinguistic factors, such as speaker identity, gender, and speech content, with the emotion branch has been shown to further enhance emotion recognition performance  [12] .\n\nThe objective of the Speech Emotion Recognition in Naturalistic Conditions Challenge  [13]  is to predict the emotional state of speakers in real-life scenarios. This challenge consists of two tracks: (1) Categorical Emotion Recognition and (2) Emotional Attributes Prediction. The Categorical Emotion Recognition track involves classifying each sample into one of eight emotional categories: anger, happiness, sadness, fear, surprise, contempt, disgust, and neutral. As shown in Figure  1 , the training and validation data for the challenge are typically imbalanced. This paper presents a SER model that employs a MTL approach. The model incorporates a co-attention module to integrate emotion, gender, speaker identity, and speech content, enabling it to capture the relationships between emotional information and other auxiliary factors. To address the challenges of recognizing a limited number of categories and mitigating semantic confusion caused by the long-tailed distribution of emotion categories in the challenge data, we introduce the Sample Weighted Focal Contrastive (SWFC) loss function  [15] . We have performed a comprehensive evaluation and demonstrated the effectiveness of the proposed approach. Recognizing the varying importance of different feature layers in the SSL model for different downstream tasks, we employ a learnable weighting sum method  [16] . This method adaptively integrates the hidden representations from all layers of the SSL model at the utterance level to generate a unified representation. The emotion, gender and speaker recognition branches take the global representations encoded by the attentive statistics pooling layer  [16]  as input, while the automatic speech recognition (ASR) branch uses the high-dimensional representations of the SSL model directly. The multi-task branching network then predicts emotion, gender, speaker, and ASR outputs, respectively. Notably, the emotion recognition branch incorporates a Co-attention module that facilitates dynamic interaction between emotion features and other auxiliary task features.\n\nDuring training, the emotion, gender, and speaker branches are optimized using a cross-entropy loss function, while the ASR branch uses a connectionist temporal classification (CTC) loss function  [17] . To further enhance performance, we introduce a SWFC loss function, which operates on the global representation output from the SSL model. This loss function enables the SSL model to focus more on challenging, less frequent samples during fine-tuning. The overall objective function L of the model is defined as:\n\nwhere α and β are hyperparameter for each auxiliary task.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Multi-Task Learning Network",
      "text": "The proposed MTL network includes a primary emotion recognition task and three auxiliary tasks. The emotion, gender, and speaker recognition branches each utilize a two-layer linear classification network, with each layer comprising Layer-Norm, a ReLU activation function, and Dropout. Specifically, the ASR branch incorporates an LSTM network to enhance temporal modeling capabilities before the two-layer linear classification network. Inspired by  [8]  ,A unique feature of our approach is the dynamic fusion of hidden layer representations as SSL features. In addition, we employ a co-attention mechanism to interactively fuse the outputs of the first linear layer from each auxiliary task with the corresponding features from the emotion recognition task. This fusion process enables the emotion recognition task to be effectively guided by auxiliary task information, with the final predictions generated through a linear classifier.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Fusion With Co-Attention",
      "text": "The co-attention module begins by mapping the features from each branch to the same dimension using a linear projection.\n\nThen it applies a dot-product attention mechanism to compute the interaction weights between the emotional features and each auxiliary task feature. Specifically, separate attention score matrices are calculated between the emotional features and the speaker, gender, and speech content features. These auxiliary features are weighted and aggregated through Softmax normalization. Finally, the dynamically fused multi-task representations are combined with the original emotional features, resulting in enhanced emotion representations that incorporate complementary cross-task information.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Swfc Loss",
      "text": "This study leaves behind a novel loss function, SWFC, specifically designed to mitigate class imbalance in classification tasks. SWFC builds upon the traditional Focal Contrastive Loss by introducing a dual weight adjustment mechanism that dynamically modifies both the focusing parameter and sample weight. This enables the simultaneous optimization of hard sample mining and class balance within a contrastive learning framework. The proposed method promotes the clustering of similar samples while enhancing the separation of dissimilar ones by constructing positive and negative pairs in the feature space. The focusing parameter directs the model's attention to hard-to-classify samples, while the sample weight modulates the penalty for underrepresented classes, thereby reducing model bias caused by long-tailed distributions. The SWFC loss is defined as follows:\n\nwhere wi is the category weight of sample i, sij is the dot product similarity between sample i and sample j, τ is the sample parameter controlling the intensity of the penalty, and γ is the focusing parameter.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "The experiments in this study employ data from the MSP-Podcast Challenge  [18] , a large, naturalistic speech emotion corpus consisting of speech segments collected from an audiosharing website. The dataset is divided into training, development, and test sets, with 66992, 25258, and 3200 utterances, respectively. These utterances represent a range of emotions, including anger (A), happiness (H), sadness (S), fear (F), surprise (U), contempt (C), disgust (D), and neutral (N). To improve the precision of transcription in the original dataset, we retranscribe the text using the Whisper-large v3 model 1 .\n\nTo assess the generalizability of the model, we also incorporate additional test data from the Emotiw2018 2 and MEIJU2025 Track1 English datasets 3 . Model performance is evaluated using three metrics: F1-Macro, F1-Micro, and Accuracy. These metrics offer a comprehensive assessment of the model's performance in SER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation Details",
      "text": "This study is based on the PyTorch  [19] , with the MSP-Podcast Challenge serving as the baseline for comparative experiments. We utilize WavLM-large with a feature dimension of 1024 for our experiments. The experiments are conducted on an NVIDIA A100 40GB GPU cluster, with a batch size of 16. The model is trained for a total of 60 epochs, using an initial learning rate of 1e-5 and the Adam optimizer  [20]  for parameter updates.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparasion With Baseline",
      "text": "As shown in Table  1 , the results of the comparative analysis reveal significant improvements in performance metrics for the proposed approach compared to the baseline model on all three test sets. The baseline model relies on single-task fine-tuning, where the SSL model is only fine-tuned for emotion recognition. In contrast, the MTL approach develops in this study achieves notable improvements across all test sets: F1 Macro increases by 5.66%, 3.20%, and 5.14%, while Accuracy and F1 Micro improve by 3.22%, 3.67%, and 4.57%, respectively. Importantly, our method consistently outperforms the baseline by over 3% on both core evaluation metrics-F1 Macro and Accuracy-demonstrating the effectiveness of the MTL framework in enhancing SER performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study On Highlighted Modules",
      "text": "This section presents systematic ablation experiments to assess the impact of each innovative component. As shown in Table 2, three comparison groups are created: (1) a pure MTL framework, (2) a MTL network without the co-attention mechanism, and (3) a baseline model that only implements the SWFC loss. Due to submission limits, evaluations are conducted on the Emotiw2018 and MEIJU2025 test sets, with the top and bottom rows of data for each experiment corresponding to these datasets.\n\nThe quantitative results demonstrates the positive contributions of each component. Compared to the baseline, the 1 https://huggingface.co/openai/whisper-large-v3 2 https://sites.google.com/view/Emotiw2018 3 https://ai-s2-lab.github.io/MEIJU2025-website/ 28.17 28.17\n\nSWFC loss alone improves F1-Macro by 0.81%-1.00% , highlighting its effectiveness in mitigating category imbalance. The full multi-task framework, however, achieves a substantial performance improvement of 3.01%-4.86%, highlighting the effectiveness of cross-task knowledge transfer. Notably, the addition of the co-attention mechanism boosts F1-Macro by 0.33%-2.81%, demonstrating the importance of dynamically integrating speaker characteristics, gender attributes, and ASR information as a key driver of performance improvement.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Analysis With Different Auxiliary Task Combination",
      "text": "In this section, we aim to explore how cross-task correlations influence emotion recognition. To achieve this, we conduct an ablation experiment involving task combinations, as the official test sets lacked any labeling. As a result, our experiments are limited to the Emotiw2018 and MEIJU2025 test sets. Notably, due to training on the challenge dataset, Speaker IDs are unique and do not overlap across the dataset. So Speaker Verification achieved an accuracy of 0 on both test sets, and these results are not presented here. As shown in Table  3 , experiments on the Emotiw2018 and MEIJU2025 cross-domain test sets reveal a decrease in emotion recognition accuracy compared to the baseline when either Speaker Verification or Gender Classification is used as a secondary task. This suggests that these tasks may not have a strong association with emotion recognition. However, when all three auxiliary tasks-ASR, Gender Classification, and Speaker Verification are combined, the model achieves optimal performance. Despite the suboptimal results of the gender and speaker classification tasks when considered individually, the ASR task, The evaluation results demonstrate that the learnable weighting mechanism improves F1-Macro by 1.67%, 1.91%, and 2.18% across the three test sets, respectively. These results highlight the complementary role of paralinguistic information across different transformer layers. The weighting module automatically assigns contribution weights to each hidden layer through end-to-end optimization, effectively addressing the issue of phonetic paralinguistic information dilution caused by deep semantic features. This mechanism provides an optimized aggregation path for hierarchical acoustic representations, significantly enhancing the model's ability to capture emotionally relevant acoustic cues.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "This study presents an innovative solution for the Inter-speech2025 Natural Scene Speech Emotion Recognition Challenge. The proposed MTL framework combines emotion recognition, gender classification, speaker verification, and speech recognition. It dynamically models the interactions between emotional and auxiliary task features through a collaborative attention mechanism, enabling context-aware feature fusion. To address challenges in recognizing minority class samples and distinguishing semantically similar samples, we introduce a sample-weighted focus-contrast loss function. This function strengthens the learning of hard-to-distinguish and minority class samples by dynamically adjusting sample weights, effectively mitigating issues related to category imbalance and semantic confusion. Experimental results demonstrate that the proposed approach significantly enhances emotion recognition performance. In future work, we plan to explore the interrelationships between additional speech paralinguistic features and emotion, and further improve emotion recognition through novel cross-feature fusion techniques.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Distribution of the challenge training and validation",
      "page": 1
    },
    {
      "caption": "Figure 2: , our research framework consists of three",
      "page": 2
    },
    {
      "caption": "Figure 2: Network structure of our proposed MTL method.",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dengjing,\n{wanghonghong,": "Abstract",
          "mengfanqin,\nzhengrong}@fosafer.com": "features,\nresulting in significant\nimprovements in SER perfor-"
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "mance [11]. However,\ntraditional MTL methods often fail\nto"
        },
        {
          "dengjing,\n{wanghonghong,": "This\nstudy investigates fine-tuning self-supervised learn-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "fully take advantage of\nthe representations\nin each auxiliary"
        },
        {
          "dengjing,\n{wanghonghong,": "ing (SSL) models using multi-task learning (MTL) to enhance",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "task. As a result,\nintegrating information from paralinguistic"
        },
        {
          "dengjing,\n{wanghonghong,": "speech emotion recognition (SER). The framework simultane-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "factors,\nsuch as speaker\nidentity, gender, and speech content,"
        },
        {
          "dengjing,\n{wanghonghong,": "ously handles four\nrelated tasks:\nemotion recognition, gender",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "with the emotion branch has been shown to further enhance"
        },
        {
          "dengjing,\n{wanghonghong,": "recognition, speaker verification, and automatic speech recog-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "emotion recognition performance [12]."
        },
        {
          "dengjing,\n{wanghonghong,": "nition. An innovative co-attention module is introduced to dy-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "The objective of the Speech Emotion Recognition in Natu-"
        },
        {
          "dengjing,\n{wanghonghong,": "namically capture the interactions between features\nfrom the",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "ralistic Conditions Challenge [13]\nis to predict\nthe emotional"
        },
        {
          "dengjing,\n{wanghonghong,": "primary emotion classification task and auxiliary tasks,\nen-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "state of\nspeakers\nin real-life scenarios.\nThis challenge con-"
        },
        {
          "dengjing,\n{wanghonghong,": "abling context-aware fusion. Moreover, We introduce the Sam-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "sists of\ntwo tracks:\n(1) Categorical Emotion Recognition and"
        },
        {
          "dengjing,\n{wanghonghong,": "ple Weighted Focal Contrastive (SWFC)\nloss\nfunction to ad-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "(2) Emotional Attributes Prediction. The Categorical Emotion"
        },
        {
          "dengjing,\n{wanghonghong,": "dress class imbalance and semantic confusion by adjusting sam-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "Recognition track involves classifying each sample into one of"
        },
        {
          "dengjing,\n{wanghonghong,": "ple weights for difficult and minority samples. The method is",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "eight emotional categories: anger, happiness, sadness, fear, sur-"
        },
        {
          "dengjing,\n{wanghonghong,": "validated on the Categorical Emotion Recognition task of\nthe",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "prise, contempt, disgust, and neutral. As shown in Figure 1,"
        },
        {
          "dengjing,\n{wanghonghong,": "Speech Emotion Recognition in Naturalistic Conditions Chal-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "the training and validation data for the challenge are typically"
        },
        {
          "dengjing,\n{wanghonghong,": "lenge, showing significant performance improvements.",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "",
          "mengfanqin,\nzhengrong}@fosafer.com": "imbalanced."
        },
        {
          "dengjing,\n{wanghonghong,": "Index Terms: speech emotion recognition, multi-task learning,",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "class imbalance",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "1.\nIntroduction",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "Recently,\nthe rapid advancement of artificial\nintelligence has",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "significantly improved the role of speech in human-computer",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "interaction (HCI)\n[1].\nAs a crucial element of HCI, Speech",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "Emotion Recognition (SER) has emerged as a key area of\nre-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "search in multimodal emotion analysis, with the goal of accu-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "rately identifying and interpreting the speaker’s emotional state.",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "SER currently encounters several challenges. A major chal-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "lenge is the scarcity of high-quality emotion-labeled datasets.",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "Furthermore, the emotional features of speech are dynamic and",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "varying over\ntime, with valuable information distributed un-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "evenly in the domain of\ntime and frequency [2].\nTraditional",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "deep learning models,\nsuch as convolutional neural networks",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "(CNNs)\n[3], are constrained by a fixed perceptual field, mak-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "ing it difficult to accurately capture emotional cues, which often",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "leads to subpar recognition performance. To address these chal-",
          "mengfanqin,\nzhengrong}@fosafer.com": "the challenge training and validation\nFigure 1: Distribution of"
        },
        {
          "dengjing,\n{wanghonghong,": "lenges, researchers have increasingly turned to self-supervised",
          "mengfanqin,\nzhengrong}@fosafer.com": "sets across the eight emotional labels."
        },
        {
          "dengjing,\n{wanghonghong,": "learning (SSL) models,\nleading to the development of power-",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "ful end-to-end speech representation models like wav2vec 2.0",
          "mengfanqin,\nzhengrong}@fosafer.com": ""
        },
        {
          "dengjing,\n{wanghonghong,": "[4], WavLM [5],\nand HuBERT [6].\nThese models\nimprove",
          "mengfanqin,\nzhengrong}@fosafer.com": "This paper presents a SER model\nthat employs a MTL ap-"
        },
        {
          "dengjing,\n{wanghonghong,": "emotion recognition by extracting high-dimensional\nrepresen-",
          "mengfanqin,\nzhengrong}@fosafer.com": "proach.\nThe model\nincorporates a co-attention module to in-"
        },
        {
          "dengjing,\n{wanghonghong,": "tations from raw waveform signals,\nleveraging large-scale un-",
          "mengfanqin,\nzhengrong}@fosafer.com": "tegrate emotion, gender, speaker\nidentity, and speech content,"
        },
        {
          "dengjing,\n{wanghonghong,": "supervised pre-training to capture rich semantic, phonetic, and",
          "mengfanqin,\nzhengrong}@fosafer.com": "enabling it\nto capture the relationships between emotional\nin-"
        },
        {
          "dengjing,\n{wanghonghong,": "paralinguistic information [7].",
          "mengfanqin,\nzhengrong}@fosafer.com": "formation and other auxiliary factors. To address the challenges"
        },
        {
          "dengjing,\n{wanghonghong,": "Since paralinguistic and acoustic features are believed to",
          "mengfanqin,\nzhengrong}@fosafer.com": "of recognizing a limited number of categories and mitigating se-"
        },
        {
          "dengjing,\n{wanghonghong,": "carry personalized emotional\ninformation [8], researchers have",
          "mengfanqin,\nzhengrong}@fosafer.com": "mantic confusion caused by the long-tailed distribution of emo-"
        },
        {
          "dengjing,\n{wanghonghong,": "increasingly turned to multi-task learning (MTL) approaches",
          "mengfanqin,\nzhengrong}@fosafer.com": "tion categories in the challenge data, we introduce the Sample"
        },
        {
          "dengjing,\n{wanghonghong,": "[9, 10], which simultaneously train the SER task alongside these",
          "mengfanqin,\nzhengrong}@fosafer.com": "Weighted Focal Contrastive (SWFC)\nloss\nfunction [15]. We"
        },
        {
          "dengjing,\n{wanghonghong,": "auxiliary tasks.\nBy sharing parameters across multiple tasks,",
          "mengfanqin,\nzhengrong}@fosafer.com": "have performed a comprehensive evaluation and demonstrated"
        },
        {
          "dengjing,\n{wanghonghong,": "the model can better extract emotional information from speech",
          "mengfanqin,\nzhengrong}@fosafer.com": "the effectiveness of the proposed approach."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2. Proposed Method": "2.1. Model Architecture",
          "2.2. Multi-task Learning Network": "The proposed MTL network includes a primary emotion recog-"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "nition task and three\nauxiliary tasks.\nThe\nemotion, gender,"
        },
        {
          "2. Proposed Method": "As shown in Figure 2, our research framework consists of three",
          "2.2. Multi-task Learning Network": ""
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "and speaker\nrecognition branches each utilize a two-layer\nlin-"
        },
        {
          "2. Proposed Method": "key stages:\nfeature extraction, pooling, and classification. The",
          "2.2. Multi-task Learning Network": ""
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "ear classification network, with each layer comprising Layer-"
        },
        {
          "2. Proposed Method": "network follows an end-to-end structure, where raw speech is",
          "2.2. Multi-task Learning Network": ""
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "Norm, a ReLU activation function, and Dropout. Specifically,"
        },
        {
          "2. Proposed Method": "first up-sampled using a convolutional\nfeature encoder. Then,",
          "2.2. Multi-task Learning Network": ""
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "the ASR branch incorporates an LSTM network to enhance"
        },
        {
          "2. Proposed Method": "deep feature extraction is performed by a Transformer Encoder.",
          "2.2. Multi-task Learning Network": ""
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "temporal modeling capabilities before the two-layer linear clas-"
        },
        {
          "2. Proposed Method": "Recognizing the varying importance of different feature layers",
          "2.2. Multi-task Learning Network": ""
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "sification network.\nInspired by [8] ,A unique feature of our ap-"
        },
        {
          "2. Proposed Method": "in the SSL model for different downstream tasks, we employ a",
          "2.2. Multi-task Learning Network": ""
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "proach is\nthe dynamic fusion of hidden layer\nrepresentations"
        },
        {
          "2. Proposed Method": "learnable weighting sum method [16]. This method adaptively",
          "2.2. Multi-task Learning Network": ""
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "as SSL features.\nIn addition, we employ a co-attention mech-"
        },
        {
          "2. Proposed Method": "integrates the hidden representations from all layers of the SSL",
          "2.2. Multi-task Learning Network": ""
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "anism to interactively fuse the outputs of\nthe first\nlinear\nlayer"
        },
        {
          "2. Proposed Method": "model at the utterance level to generate a unified representation.",
          "2.2. Multi-task Learning Network": ""
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "from each auxiliary task with the corresponding features from"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "the emotion recognition task. This fusion process enables the"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "emotion recognition task to be effectively guided by auxiliary"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "task information, with the final predictions generated through a"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "linear classifier."
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "2.3. Feature Fusion with Co-attention"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "The co-attention module begins by mapping the features from"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "each branch to the same dimension using a linear projection."
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "Then it applies a dot-product attention mechanism to compute"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "the interaction weights between the emotional features and each"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "auxiliary task feature. Specifically, separate attention score ma-"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "trices are calculated between the emotional\nfeatures and the"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "speaker, gender, and speech content\nfeatures. These auxiliary"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "features are weighted and aggregated through Softmax normal-"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "ization.\nFinally,\nthe dynamically fused multi-task representa-"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "tions are combined with the original emotional features, result-"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "ing in enhanced emotion representations that\nincorporate com-"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "plementary cross-task information."
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "2.4.\nSWFC Loss"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "This study leaves behind a novel\nloss function, SWFC, specif-"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "ically designed to mitigate\nclass\nimbalance\nin classification"
        },
        {
          "2. Proposed Method": "Figure 2: Network structure of our proposed MTL method.",
          "2.2. Multi-task Learning Network": ""
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "tasks. SWFC builds upon the traditional Focal Contrastive Loss"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "by introducing a dual weight adjustment mechanism that dy-"
        },
        {
          "2. Proposed Method": "The emotion, gender and speaker recognition branches take",
          "2.2. Multi-task Learning Network": "namically modifies both the\nfocusing parameter\nand sample"
        },
        {
          "2. Proposed Method": "the global\nrepresentations\nencoded by the\nattentive\nstatistics",
          "2.2. Multi-task Learning Network": "weight.\nThis enables\nthe simultaneous optimization of hard"
        },
        {
          "2. Proposed Method": "pooling layer [16] as input, while the automatic speech recog-",
          "2.2. Multi-task Learning Network": "sample mining and class balance within a contrastive learn-"
        },
        {
          "2. Proposed Method": "nition (ASR) branch uses the high-dimensional representations",
          "2.2. Multi-task Learning Network": "ing framework. The proposed method promotes the clustering"
        },
        {
          "2. Proposed Method": "of\nthe SSL model directly. The multi-task branching network",
          "2.2. Multi-task Learning Network": "of similar samples while enhancing the separation of dissimi-"
        },
        {
          "2. Proposed Method": "then predicts emotion, gender, speaker, and ASR outputs,\nre-",
          "2.2. Multi-task Learning Network": "lar ones by constructing positive and negative pairs in the fea-"
        },
        {
          "2. Proposed Method": "spectively. Notably,\nthe emotion recognition branch incorpo-",
          "2.2. Multi-task Learning Network": "ture space. The focusing parameter directs the model’s atten-"
        },
        {
          "2. Proposed Method": "rates a Co-attention module that facilitates dynamic interaction",
          "2.2. Multi-task Learning Network": "tion to hard-to-classify samples, while the sample weight mod-"
        },
        {
          "2. Proposed Method": "between emotion features and other auxiliary task features.",
          "2.2. Multi-task Learning Network": "ulates the penalty for underrepresented classes,\nthereby reduc-"
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "ing model bias caused by long-tailed distributions. The SWFC"
        },
        {
          "2. Proposed Method": "During training, the emotion, gender, and speaker branches",
          "2.2. Multi-task Learning Network": ""
        },
        {
          "2. Proposed Method": "",
          "2.2. Multi-task Learning Network": "loss is defined as follows:"
        },
        {
          "2. Proposed Method": "are optimized using a cross-entropy loss\nfunction, while the",
          "2.2. Multi-task Learning Network": ""
        },
        {
          "2. Proposed Method": "ASR branch uses a connectionist temporal classification (CTC)",
          "2.2. Multi-task Learning Network": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Comparison between Baseline and Proposed models",
      "data": [
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "on three test sets."
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Test Set\nModels\nF1 Macro\nF1 Micro\nAccuracy"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Baseline\n29.83\n33.84\n33.84"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "MSP-podcast"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "35.49\n36.75\n36.75\nProposed"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Baseline\n26.21\n38.75\n38.75"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Emotiw2018"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "29.41\n41.42\n41.42\nProposed"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Baseline\n16.79\n23.60\n23.60"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "MEIJU2025"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "21.93\n28.17\n28.17\nProposed"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Table 2: Ablation study of MTL,Co-Attention,and SWFC Loss,"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "where Emotiw2018 is above and MEIJU2025 is below."
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Approach\nF1 Macro\nF1 Micro\nAccuracy"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "26.21\n38.75\n38.75"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Baseline"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "16.79\n23.60\n23.60"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "27.02\n38.84\n38.84"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Ours w/o MTL"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "17.79\n23.38\n23.38"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "28.89\n40.41\n40.41"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Ours w/o Co-attention"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "20.60\n26.36\n26.36"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "29.22\n41.06\n41.06"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Ours w/o SWFC Loss"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "21.65\n27.66\n27.66"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "29.41\n41.42\n41.42"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Ours"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "21.93\n28.17\n28.17"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "SWFC loss alone improves F1-Macro by 0.81%–1.00% , high-"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "lighting its effectiveness in mitigating category imbalance. The"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "full multi-task framework, however, achieves a substantial per-"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "formance improvement of 3.01%–4.86%, highlighting the ef-"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "fectiveness\nof\ncross-task\nknowledge\ntransfer.\nNotably,\nthe"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "addition of\nthe co-attention mechanism boosts F1-Macro by"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "0.33%–2.81%, demonstrating the importance of dynamically"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "integrating speaker characteristics, gender attributes, and ASR"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "information as a key driver of performance improvement."
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "3.3.3. Analysis with Different Auxiliary Task Combination"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "In this section, we aim to explore how cross-task correlations"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "influence emotion recognition. To achieve this, we conduct an"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "ablation experiment involving task combinations, as the official"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "test sets lacked any labeling. As a result, our experiments are"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "limited to the Emotiw2018 and MEIJU2025 test sets. Notably,"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "due to training on the challenge dataset, Speaker IDs are unique"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "and do not overlap across the dataset. So Speaker Verification"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "achieved an accuracy of 0 on both test sets, and these results are"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "not presented here."
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "As shown in Table 3, experiments on the Emotiw2018 and"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "MEIJU2025 cross-domain test sets reveal a decrease in emo-"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "tion recognition accuracy compared to the baseline when ei-"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "ther Speaker Verification or Gender Classification is used as a"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "secondary task. This suggests that\nthese tasks may not have a"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "strong association with emotion recognition. However, when all"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "three auxiliary tasks—ASR, Gender Classification, and Speaker"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Comparison between Baseline and Proposed models",
      "data": [
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "on three test sets."
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Test Set\nModels\nF1 Macro\nF1 Micro\nAccuracy"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Baseline\n29.83\n33.84\n33.84"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "MSP-podcast"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "35.49\n36.75\n36.75\nProposed"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Baseline\n26.21\n38.75\n38.75"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Emotiw2018"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "29.41\n41.42\n41.42\nProposed"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Baseline\n16.79\n23.60\n23.60"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "MEIJU2025"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "21.93\n28.17\n28.17\nProposed"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Table 2: Ablation study of MTL,Co-Attention,and SWFC Loss,"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "where Emotiw2018 is above and MEIJU2025 is below."
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Approach\nF1 Macro\nF1 Micro\nAccuracy"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "26.21\n38.75\n38.75"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Baseline"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "16.79\n23.60\n23.60"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "27.02\n38.84\n38.84"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Ours w/o MTL"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "17.79\n23.38\n23.38"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "28.89\n40.41\n40.41"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Ours w/o Co-attention"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "20.60\n26.36\n26.36"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "29.22\n41.06\n41.06"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Ours w/o SWFC Loss"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "21.65\n27.66\n27.66"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "29.41\n41.42\n41.42"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Ours"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "21.93\n28.17\n28.17"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "SWFC loss alone improves F1-Macro by 0.81%–1.00% , high-"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "lighting its effectiveness in mitigating category imbalance. The"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "full multi-task framework, however, achieves a substantial per-"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "formance improvement of 3.01%–4.86%, highlighting the ef-"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "fectiveness\nof\ncross-task\nknowledge\ntransfer.\nNotably,\nthe"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "addition of\nthe co-attention mechanism boosts F1-Macro by"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "0.33%–2.81%, demonstrating the importance of dynamically"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "integrating speaker characteristics, gender attributes, and ASR"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "information as a key driver of performance improvement."
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "3.3.3. Analysis with Different Auxiliary Task Combination"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "In this section, we aim to explore how cross-task correlations"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "influence emotion recognition. To achieve this, we conduct an"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "ablation experiment involving task combinations, as the official"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": ""
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "test sets lacked any labeling. As a result, our experiments are"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "limited to the Emotiw2018 and MEIJU2025 test sets. Notably,"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "due to training on the challenge dataset, Speaker IDs are unique"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "and do not overlap across the dataset. So Speaker Verification"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "achieved an accuracy of 0 on both test sets, and these results are"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "not presented here."
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "As shown in Table 3, experiments on the Emotiw2018 and"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "MEIJU2025 cross-domain test sets reveal a decrease in emo-"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "tion recognition accuracy compared to the baseline when ei-"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "ther Speaker Verification or Gender Classification is used as a"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "secondary task. This suggests that\nthese tasks may not have a"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "strong association with emotion recognition. However, when all"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "three auxiliary tasks—ASR, Gender Classification, and Speaker"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "Verification are combined,\nthe model achieves optimal perfor-"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "mance. Despite the suboptimal results of the gender and speaker"
        },
        {
          "Table 1: Comparison between Baseline and Proposed models": "classification tasks when considered individually, the ASR task,"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": ""
        },
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": ""
        },
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": "SER"
        },
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": "✓"
        },
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": "✓"
        },
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": ""
        },
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": "✓"
        },
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": "✓"
        },
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": "✓"
        },
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": "✓"
        },
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": "✓"
        },
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": ""
        },
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": "✓"
        },
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": "✓"
        },
        {
          "Table 3: Task Combination Analysis on Emotiw2018 and MEIJU2025 Datasets.": "✓"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "✓\n×\n×": "✓\n✓\n✓",
          "✓\n23.18\n–\n–": "✓\n28.17\n98.80\n25.56"
        },
        {
          "✓\n×\n×": "Table 4: Comparison results on three test sets using only the last",
          "✓\n23.18\n–\n–": ""
        },
        {
          "✓\n×\n×": "",
          "✓\n23.18\n–\n–": "recognition.\nIt dynamically models\nthe interactions between"
        },
        {
          "✓\n×\n×": "hidden state features and the features obtained using a learn-",
          "✓\n23.18\n–\n–": ""
        },
        {
          "✓\n×\n×": "",
          "✓\n23.18\n–\n–": "emotional and auxiliary task features\nthrough a collaborative"
        },
        {
          "✓\n×\n×": "able weighted fusion strategy. Last denotes the last hidden state",
          "✓\n23.18\n–\n–": ""
        },
        {
          "✓\n×\n×": "",
          "✓\n23.18\n–\n–": "attention mechanism,\nenabling context-aware\nfeature\nfusion."
        },
        {
          "✓\n×\n×": "and Learnable denotes the weighted fusion features.",
          "✓\n23.18\n–\n–": ""
        },
        {
          "✓\n×\n×": "",
          "✓\n23.18\n–\n–": "To address challenges\nin recognizing minority class\nsamples"
        },
        {
          "✓\n×\n×": "",
          "✓\n23.18\n–\n–": "and distinguishing semantically similar samples, we introduce"
        },
        {
          "✓\n×\n×": "Test Set\nFeatures Type F1 Macro F1 Micro Accuracy",
          "✓\n23.18\n–\n–": "a\nsample-weighted focus-contrast\nloss\nfunction.\nThis\nfunc-"
        },
        {
          "✓\n×\n×": "",
          "✓\n23.18\n–\n–": "tion strengthens the learning of hard-to-distinguish and minority"
        },
        {
          "✓\n×\n×": "Last\n33.82\n36.00\n36.00",
          "✓\n23.18\n–\n–": ""
        },
        {
          "✓\n×\n×": "MSP-podcast",
          "✓\n23.18\n–\n–": "class samples by dynamically adjusting sample weights, effec-"
        },
        {
          "✓\n×\n×": "35.49\n36.75\n36.75\nLearnable",
          "✓\n23.18\n–\n–": ""
        },
        {
          "✓\n×\n×": "",
          "✓\n23.18\n–\n–": "tively mitigating issues related to category imbalance and se-"
        },
        {
          "✓\n×\n×": "Last\n27.50\n39.02\n39.02",
          "✓\n23.18\n–\n–": "mantic confusion.\nExperimental\nresults demonstrate that\nthe"
        },
        {
          "✓\n×\n×": "Emotiw2018",
          "✓\n23.18\n–\n–": ""
        },
        {
          "✓\n×\n×": "29.41\n41.42\n41.42\nLearnable",
          "✓\n23.18\n–\n–": "proposed approach significantly enhances emotion recognition"
        },
        {
          "✓\n×\n×": "",
          "✓\n23.18\n–\n–": "performance.\nIn future work, we plan to explore the inter-"
        },
        {
          "✓\n×\n×": "Last\n19.75\n26.63\n26.63",
          "✓\n23.18\n–\n–": ""
        },
        {
          "✓\n×\n×": "MEIJU2025",
          "✓\n23.18\n–\n–": "relationships between additional speech paralinguistic features"
        },
        {
          "✓\n×\n×": "21.93\n28.17\n28.17\nLearnable",
          "✓\n23.18\n–\n–": ""
        },
        {
          "✓\n×\n×": "",
          "✓\n23.18\n–\n–": "and emotion, and further improve emotion recognition through"
        },
        {
          "✓\n×\n×": "",
          "✓\n23.18\n–\n–": "novel cross-feature fusion techniques."
        },
        {
          "✓\n×\n×": "supported by its primary auxiliary function, provided acoustic",
          "✓\n23.18\n–\n–": "5. Acknowledgements"
        },
        {
          "✓\n×\n×": "features that significantly enhanced the model’s robustness.",
          "✓\n23.18\n–\n–": ""
        },
        {
          "✓\n×\n×": "",
          "✓\n23.18\n–\n–": "This work is supported by the National Key Research and De-"
        },
        {
          "✓\n×\n×": "",
          "✓\n23.18\n–\n–": "velopment Program of China (No.2022YFF0608504)."
        },
        {
          "✓\n×\n×": "3.3.4.\nLast HiddenState vs. FusionState",
          "✓\n23.18\n–\n–": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This work is supported by the National Key Research and De-": "velopment Program of China (No.2022YFF0608504)."
        },
        {
          "This work is supported by the National Key Research and De-": ""
        },
        {
          "This work is supported by the National Key Research and De-": "6. References"
        },
        {
          "This work is supported by the National Key Research and De-": ""
        },
        {
          "This work is supported by the National Key Research and De-": "[1]\nT. M. Wani, T. S. Gunawan, S. A. A. Qadri, M. Kartiwi, and E."
        },
        {
          "This work is supported by the National Key Research and De-": ""
        },
        {
          "This work is supported by the National Key Research and De-": "Ambikairajah, “A comprehensive review of speech emotionrecog-"
        },
        {
          "This work is supported by the National Key Research and De-": "nition systems,” IEEE access, vol. 9, pp. 47 795–47 814,2021."
        },
        {
          "This work is supported by the National Key Research and De-": ""
        },
        {
          "This work is supported by the National Key Research and De-": "[2] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-"
        },
        {
          "This work is supported by the National Key Research and De-": ""
        },
        {
          "This work is supported by the National Key Research and De-": "manian,\n“Deformer:\nDecomposing\npre-trained\ntrans-"
        },
        {
          "This work is supported by the National Key Research and De-": "formers\nforfaster\nquestion\nanswering,”\narXiv\npreprint"
        },
        {
          "This work is supported by the National Key Research and De-": "arXiv:2005.00697,2020."
        },
        {
          "This work is supported by the National Key Research and De-": "[3] H. S. Kumbhar and S. U. Bhandari, “Speech emotion recogni-"
        },
        {
          "This work is supported by the National Key Research and De-": "tion using mfcc features and lstm network,” in 2019 5th Interna-"
        },
        {
          "This work is supported by the National Key Research and De-": "tional Conference on Computing, Communication, Control and"
        },
        {
          "This work is supported by the National Key Research and De-": "Automa-tion (ICCUBEA). IEEE, 2019, pp. 1–3."
        },
        {
          "This work is supported by the National Key Research and De-": ""
        },
        {
          "This work is supported by the National Key Research and De-": "[4] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec2.0:"
        },
        {
          "This work is supported by the National Key Research and De-": "A framework\nfor\nself-supervised\nlearning\nof\nspeech\nrepre-"
        },
        {
          "This work is supported by the National Key Research and De-": "sentations,” Advances\nin\nneural\ninformation\nprocessing\nsys-"
        },
        {
          "This work is supported by the National Key Research and De-": ""
        },
        {
          "This work is supported by the National Key Research and De-": "tems,vol. 33, pp. 12 449–12 460, 2020."
        },
        {
          "This work is supported by the National Key Research and De-": ""
        },
        {
          "This work is supported by the National Key Research and De-": "[5]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen,\nJ. Li,N."
        },
        {
          "This work is supported by the National Key Research and De-": "Kanda, T. Yoshioka, X. Xiao et al., “Wavlm: Large-scale self-"
        },
        {
          "This work is supported by the National Key Research and De-": ""
        },
        {
          "This work is supported by the National Key Research and De-": "supervised pre-training for\nfull stack speech processing,” IEEE-"
        },
        {
          "This work is supported by the National Key Research and De-": "Journal of Selected Topics in Signal Processing, vol. 16, no. 6,pp."
        },
        {
          "This work is supported by the National Key Research and De-": "1505–1518, 2022."
        },
        {
          "This work is supported by the National Key Research and De-": "[6] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhut-"
        },
        {
          "This work is supported by the National Key Research and De-": ""
        },
        {
          "This work is supported by the National Key Research and De-": "dinov, and A. Mohamed, “Hubert:\nSelf-supervised speech rep-"
        },
        {
          "This work is supported by the National Key Research and De-": ""
        },
        {
          "This work is supported by the National Key Research and De-": "resentation\nlearning\nby masked\nprediction\nof\nhidden\nunits,”"
        },
        {
          "This work is supported by the National Key Research and De-": "IEEE/ACM Transactions on Audio, Speech, and Language Pro-"
        },
        {
          "This work is supported by the National Key Research and De-": "cessing, vol. 29, pp. 3451–3460, 2021."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "manian,\n“Deformer:\nDecomposing\npre-trained\ntrans-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "formers\nforfaster\nquestion\nanswering,”\narXiv\npreprint"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "arXiv:2005.00697,2020."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "[8]\nZ. Zhang, W. Xu, Z. Dong, K. Wang, Y. Wu,\nJ. Peng, R."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "Wang,and D.-Y. Huang, “Paralbench: a large-scale benchmark for"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "com-putational paralinguistics over acoustic foundation models,”"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "IEEETransactions on Affective Computing, 2024."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "[9] H. Shi, M. Mimura, L. Wang, J. Dang, and T. Kawahara, “Time-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "domain\nspeech\nenhancement\nassisted\nby multi-resolution\nfre-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "quency encoder and decoder,” in ICASSP 2023-2023 IEEE Inter-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "national Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "(ICASSP). IEEE, 2023, pp. 1–5."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "[10]\n. Pan and W.-Q. Zhang, “Multi-task learning based end-to-end"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "speaker\nrecognition,”\nin Proceedings of\nthe 2019 2nd Interna-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "tional Conference\non Signal Processing\nand Machine Learn-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "ing,2019, pp. 56–61."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "[11]\nS. Ghosh, U. Tyagi, S. Ramaneswaran, H. Srivastava,\nand D."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "Manocha,\n“Mmer: Multimodal multi-task learning for\nspeech"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "emotion recognition,” arXiv preprint arXiv:2203.16794, 2022."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "[12]\nZ. Wan,\nZ. Qiu, Y. Liu,\nand W.-Q. Zhang,\n“Metadata-\nen-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "hanced speech emotion recognition: Augmented residual\ninte-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "gration and co-attention in two-stage fine-tuning,” arXiv preprint"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "arXiv:2412.20707, 2024."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "[13]\nT. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman,\nand C."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "Finn,“Gradient surgery for multi-task learning,” Advances in Neu-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "ral\nInformation Processing Systems,\nvol.\n33,\npp.\n5824–5836,"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "2020."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "[14] A. Reddy Naini, L. Goncalves, A. N. Salman, P. Mote,\nI. R."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "¨Ulgen, T. Thebaud, L. Velazquez, L. P. Garcia, N. Dehak, B. Sis-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "man, and C. Busso, “The interspeech 2025 challenge on speech"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "emotion recognition in naturalistic\nconditions,”\nin Interspeech"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "2025, vol.Under\nsubmission, Rotterdam, The Netherlands, Au-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "gust 2025."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "[15]\nT.\nShi\nand\nS.-L. Huang,\n“Multiemo:\nAn\nattention-based"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "correlation-aware multimodal\nfusion\nframework\nfor\nemotion"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "recognition in conversations,” in Proceedings of the 61st Annual"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "Meeting of\nthe Association for Computational Linguistics (Vol-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "ume 1: Long Papers), 2023, pp. 14 752–14 766."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "[16] K. Okabe,\nT. Koshinaka,\nand K.\nShinoda,\n“Attentive\nstatis-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "tics\npooling\nfor\ndeep\nspeaker\nembedding,”\narXiv\npreprint"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "arXiv:1803.10963, 2018."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "[17] A. Graves,\nS.\nFern´andez,\nF. Gomez,\nand\nJ.\nSchmidhuber,"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "“Con-nectionist\ntemporal classification:\nlabelling unsegmented"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "se-quence data with recurrent neural networks,” in Proceedings"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "of the 23rd international conference on Machine learning, 2006,"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "pp.369–376."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "[18] R. Lotfian and C. Busso, “Building naturalistic emotionally bal-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "anced speech corpus by retrieving emotional speech from exist-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "ing podcast\nrecordings,” IEEE Transactions on Affective Com-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "puting,vol. 10, no. 4, pp. 471–483, 2017."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "[19] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,T."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "Killeen, Z. Lin, N. Gimelshein, L. Antiga et al.,\n“Pytorch:An"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "imperative style, high-performance deep learning library,” Ad-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "vances in neural information processing systems, vol. 32, 2019."
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "[20]\nZ. Zhang, “Improved adam optimizer for deep neural networks,”in"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "2018 IEEE/ACM 26th international symposium on quality of ser-"
        },
        {
          "[7] Q. Cao, H. Trivedi, A. Balasubramanian,\nand N. Balasubra-": "vice (IWQoS). IEEE, 2018, pp. 1–2."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A comprehensive review of speech emotionrecognition systems",
      "authors": [
        "T Wani",
        "T Gunawan",
        "S Qadri",
        "M Kartiwi",
        "E Ambikairajah"
      ],
      "year": "2021",
      "venue": "IEEE access"
    },
    {
      "citation_id": "3",
      "title": "Deformer: Decomposing pre-trained transformers forfaster question answering",
      "authors": [
        "Q Cao",
        "H Trivedi",
        "A Balasubramanian",
        "N Balasubramanian"
      ],
      "year": "2020",
      "venue": "Deformer: Decomposing pre-trained transformers forfaster question answering",
      "arxiv": "arXiv:2005.00697"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using mfcc features and lstm network",
      "authors": [
        "H Kumbhar",
        "S Bhandari"
      ],
      "year": "2019",
      "venue": "2019 5th International Conference on Computing, Communication, Control and Automa-tion (ICCUBEA)"
    },
    {
      "citation_id": "5",
      "title": "wav2vec2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE-Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Deformer: Decomposing pre-trained transformers forfaster question answering",
      "authors": [
        "Q Cao",
        "H Trivedi",
        "A Balasubramanian",
        "N Balasubramanian"
      ],
      "year": "2020",
      "venue": "Deformer: Decomposing pre-trained transformers forfaster question answering",
      "arxiv": "arXiv:2005.00697"
    },
    {
      "citation_id": "9",
      "title": "Paralbench: a large-scale benchmark for com-putational paralinguistics over acoustic foundation models",
      "authors": [
        "Z Zhang",
        "W Xu",
        "Z Dong",
        "K Wang",
        "Y Wu",
        "J Peng",
        "R Wang",
        "D.-Y Huang"
      ],
      "year": "2024",
      "venue": "IEEETransactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Timedomain speech enhancement assisted by multi-resolution frequency encoder and decoder",
      "authors": [
        "H Shi",
        "M Mimura",
        "L Wang",
        "J Dang",
        "T Kawahara"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Multi-task learning based end-to-end speaker recognition",
      "authors": [
        "W.-Q Pan",
        "Zhang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 2nd International Conference on Signal Processing and Machine Learning"
    },
    {
      "citation_id": "12",
      "title": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "U Tyagi",
        "S Ramaneswaran",
        "H Srivastava",
        "D Manocha"
      ],
      "year": "2022",
      "venue": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "arxiv": "arXiv:2203.16794"
    },
    {
      "citation_id": "13",
      "title": "Metadata-enhanced speech emotion recognition: Augmented residual integration and co-attention in two-stage fine-tuning",
      "authors": [
        "Z Wan",
        "Z Qiu",
        "Y Liu",
        "W.-Q Zhang"
      ],
      "year": "2024",
      "venue": "Metadata-enhanced speech emotion recognition: Augmented residual integration and co-attention in two-stage fine-tuning",
      "arxiv": "arXiv:2412.20707"
    },
    {
      "citation_id": "14",
      "title": "Gradient surgery for multi-task learning",
      "authors": [
        "T Yu",
        "S Kumar",
        "A Gupta",
        "S Levine",
        "K Hausman",
        "C Finn"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "The interspeech 2025 challenge on speech emotion recognition in naturalistic conditions",
      "authors": [
        "A Reddy Naini",
        "L Goncalves",
        "A Salman",
        "P Mote",
        "I ¨ulgen",
        "T Thebaud",
        "L Velazquez",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2025",
      "venue": "Interspeech 2025"
    },
    {
      "citation_id": "16",
      "title": "Multiemo: An attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations",
      "authors": [
        "T Shi",
        "S.-L Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "17",
      "title": "Attentive statistics pooling for deep speaker embedding",
      "authors": [
        "K Okabe",
        "T Koshinaka",
        "K Shinoda"
      ],
      "year": "2018",
      "venue": "Attentive statistics pooling for deep speaker embedding",
      "arxiv": "arXiv:1803.10963"
    },
    {
      "citation_id": "18",
      "title": "Con-nectionist temporal classification: labelling unsegmented se-quence data with recurrent neural networks",
      "authors": [
        "A Graves",
        "S Fern´andez",
        "F Gomez",
        "J Schmidhuber"
      ],
      "year": "2006",
      "venue": "Proceedings of the 23rd international conference on Machine learning"
    },
    {
      "citation_id": "19",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Pytorch:An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Improved adam optimizer for deep neural networks",
      "authors": [
        "Z Zhang"
      ],
      "year": "2018",
      "venue": "2018 IEEE/ACM 26th international symposium on quality of service (IWQoS)"
    }
  ]
}