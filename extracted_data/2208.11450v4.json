{
  "paper_id": "2208.11450v4",
  "title": "Vistanet: Visual Spoken Textual Additive Net For Interpretable Multimodal Emotion Recognition",
  "published": "2022-08-24T11:35:51Z",
  "authors": [
    "Puneet Kumar",
    "Sarthak Malik",
    "Balasubramanian Raman",
    "Xiaobai Li"
  ],
  "keywords": [
    "Affective Computing",
    "Emotion and Sentiment Analysis",
    "Speech-Text-Image Signals",
    "Information Fusion",
    "Interpretable AI"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper proposes a multimodal emotion recognition system, VIsual Spoken Textual Additive Net (VISTANet), to classify emotions reflected by input containing image, speech, and text into discrete classes. A new interpretability technique, K-Average Additive exPlanation (KAAP), has been developed that identifies important visual, spoken, and textual features leading to predicting a particular emotion class. The VISTANet fuses information from image, speech, and text modalities using a hybrid of intermediate and late fusion. It automatically adjusts the weights of their intermediate outputs while computing the weighted average. The KAAP technique computes the contribution of each modality and corresponding features toward predicting a particular emotion class. To mitigate the insufficiency of multimodal emotion datasets labelled with discrete emotion classes, we have constructed the IIT-R MMEmoRec dataset consisting of images, corresponding speech and text, and emotion labels ('angry,' 'happy,' 'hate,' and 'sad'). The VISTANet has resulted in an overall emotion recognition accuracy of 80.11% on the IIT-R MMEmoRec dataset using visual, spoken, and textual modalities, outperforming single or dual-modality configurations. The code and data can be accessed at github.com/MIntelligence-Group/MMEmoRec.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Unimodal Emotion Recognition",
      "text": "In unimodal emotion recognition, individual modalities like speech, text, and images are utilized to detect emotions. Speech Emotion Recognition (SER) systems traditionally extract speech features such as cepstrum coefficients, voice tone, prosody, and pitch, key for identifying emotions  [13] . These features help categorize highkey emotions like happiness and anger from low-key ones such as sadness and despair  [14] . However, the manual crafting of acoustic features and difficulties in parameter estimation pose challenges in developing robust SER systems  [15] . Recent advances using spectrogram features and attention mechanisms have enhanced SER's effectiveness  [16] ,  [17] , with CNNs for spectrogram processing  [18]  and RNN-based techniques showing promising results  [7] .\n\nText Emotion Recognition (TER) analyzes emotions portrayed by transcripts from online platforms like YouTube, Facebook, and Twitter  [19] . Attention mechanisms using graphs  [20] , transformer models  [19] , word embedding techniques from tweets  [21] , and graph network-based multimodal fusion  [22]  are leading methods for TER. Sequence-based CNNs and the integration of semantic and emotional information enhance the modelling of textual emotions  [23] ,  [24] . Image Emotion Recognition (IER) primarily uses facial expressions, benefiting from techniques like face localization, micro-expression analysis, and landmark tracking, and both traditional and advanced deep learning approaches  [5] ,  [25] . Despite advancements, IER faces challenges from deep learning techniques, underscoring the need for continued research  [5] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "Emotion analysis using a single modality may not fully capture the emotional context  [4] . Various modalities have distinct statistical properties, and understanding the inter-relationships between them is crucial for recognizing complex emotions  [2] , leading researchers to focus on multimodal emotion analysis  [26] . Existing approaches include interpreting emotions from speech and text using activation vectors  [27] , and acoustic and textual analyses  [28] . Dual RNNs extract speech and text information for emotion recognition  [29] , with transformers-based models being fine-tuned to enhance performance  [30] . Systems employing multiple modalities often utilize information fusion and end-to-end approaches for effective integration  [31] ,  [32] . Studies involving visual and textual data have explored semantic reasoning networks  [33]  and multi-task architectures designed to address missing modalities  [34] . Additionally, co-memory-based networks have been developed for sentiment recognition  [35] . The adoption of pre-trained transformers has significantly advanced multimodal emotion recognition (MER) efforts  [36] ,  [37] .\n\nMultimodal emotion recognition emphasizes integrating feature extraction and fusion techniques across visual, speech, and textual modalities. Transformer-based models play a central role due to their ability to handle complex intermodal interactions. Ma et al. introduced a self-distilling transformer model enhancing emotion recognition accuracy in conversational contexts  [19] , demonstrating transformers' significant performance enhancement by leveraging multimodal data. Zhang et al. provided a systematic review of deep learning approaches for MER, underscoring the transformative impact of integrating modalities through advanced architectures like transformers  [38] . Fan et al. developed transformer-based networks for depression detection, highlighting the adaptability of this technology in affective computing  [39] . Advancements continue with methods integrating fused speech and visual features  [40] ,  [41] , modality-specific frameworks  [42] , knowledge-embedded models for deeper analysis  [43] ,  [44] , and emotion analysis using self-supervised learning and feature correlation analysis  [45] ,  [46] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Explainable And Interpretable Emotion Analysis",
      "text": "Explainability describes an algorithm's mechanism for a specific output, while interpretability concerns understanding a model's output in context and its functional design  [47] ,  [?] . The opaque nature of deep learning has driven the emergence of explainable AI  [48] . Ribeiro et al.  [49]  developed perturbation-based methods to identify which input components drive outputs, and Shrikumar et al.  [50]  traced individual neuron contributions via backpropagation. Existing interpretability techniques fall into three categories-attributionbased, perturbation-based, and backpropagation-based methods with the latter two being subsets of attribution approaches. These attribution-based methods focus on local interpretability of individual instances. Attribution methods compute feature relevance via Shapley values  [51] , which underpin the SHAP framework  [48] ,  [8] . However, exact Shapley computation is combinatorial, requiring evaluation of 2 n subsets for n features, motivating approximations such as KernelSHAP  [48]  and sampling-based Shapley values  [52] . Another interpretability approach, DeepSHAP builds on DeepLIFT and requires direct access to internal neuron activations and gradients-making it model-dependent and necessitating separate explainers per modality  [50] . Such gradient-based attributions cannot be uniformly applied to transformer-based text backbones like BERT  [53] .\n\nPerturbation techniques involve making small alterations to inputs and observing their impact on the model's behaviour  [54] . Local Interpretable Model-agnostic Explanations (LIME)  [55]  is a widely used perturbation technique that generates new data by perturbing the original instance and weights it based on proximity. Although applicable to any machine learning model, LIME's reliance on generating new data can lead to computational overhead. Backpropagation-based techniques compute attributions by iteratively backpropagating through the network. Saliency Maps  [56]  and Gradient-weighted Class Activation Map (Grad-CAM)  [57]  are prominent examples. Grad-CAM generates highlights of important input features by focusing on the last convolutional layer, thereby offering insights into model decisions  [58] .\n\nAs highlighted, techniques like LIME, SHAP, and Grad-CAM have challenges: LIME is computationally costly, Grad-CAM struggles with minor input changes, and SHAP, though robust, is mainly for visual modalities. These issues led to the development of our KAAP interpretability technique for multimodal emotion recognition. Unlike traditional attention-based methods that highlight features without quantifying impact  [6] ,  [37] ,  [59] , KAAP offers a quantitative analysis of feature influence. It uses a perturbation-based method to evaluate contributions, providing precise insights into how modalities and features affect outcomes. This approach addresses limitations in attention-based models and enhances interpretability in multimodal emotion analysis.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Proposed Work",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Compilation",
      "text": "The IIT-R MMEmoRec dataset contains generic images (facial, human, non-human objects), speech utterances, text transcripts, emotion labels ('angry,' 'happy,' 'hate,' and 'sad'), and the probability of each emotion class. In contrast, other multimodal datasets like the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [9]  and the Multimodal EmotionLines Dataset (MELD)  [60]  contain only facial images/videos. Moreover, the IIT-R MMEmoRec dataset demonstrates greater diversity than IEMOCAP and MELD. It features faces in 70.64% of its images, compared to 97.86% in the other datasets, showing a higher variety of non-facial images. Additionally, the IIT-R MMEmoRec dataset includes 80 YOLOv8 objects  [61] , with 30 different ones appearing in over 1% of the images, whereas the IEMOCAP and MELD datasets contain 72 objects, with 18 different ones appearing in more than 1% of the images. Furthermore, 14.48% of its images feature unidentified YOLOv8 objects, while this figure is only 1.2% for the other datasets, underscoring its broader diversity. It has been constructed on top of the 'Balanced Twitter for Sentiment Analysis' (B-T4SA) dataset  [12] , which contains images, text, and sentiment labels ('positive,' 'negative,' 'neutral'). The IIT-R MMEmoRec dataset has discrete emotion labels for image, text, and speech modalities and has been constructed as described.\n\n• The text from the BT4SA dataset is pre-processed by removing links, special characters, and tags, and then the cleaned text is converted to speech using the pre-trained state-of-the-art text-to-speech (TTS) model, DeepSpeech3  [62] . The rationale for using TTS is based on recent studies showing that TTS models produce high-quality speech, which can serve as a reliable approximation of natural speech  [62] ,  [63] .\n\n• The image and speech components are passed through a pre-trained VGG model for IER and SER, while the text component is passed through a Bidirectional Encoder Representations from Transformers (BERT) model for TER. The VGG was trained on the Flickr & Instagram (FI)  [64]  dataset and the IEMOCAP  [9]  dataset for IER and SER, respectively, while the BERT was trained on the ISEAR dataset  [65]  for TER. Prediction probabilities for each emotion class are obtained per modality. For recognition, we employed models distinct from those used in dataset construction, utilizing VGG (trained on ImageNet) for visual and speech modalities, and BERT (uncased L-12 H-768 A-12) for the textual modality. • The prediction probabilities are averaged to obtain each sample's ground-truth emotion, ensuring that the chosen ground truth is the one that is supported by the majority of modalities. Fig.  1  shows an example of emotion label determination, whereas Table  1  describes the IIT-R MMEmoRec dataset's class-wise distribution. The probabilities for each emotion class given by each modality are shown. The 'happy' class has an average prediction probability of 0.500 compared to 0.233 for 'angry,' 0.133 for 'hate,' and 0.133 for 'sad.' The final emotion label for the sample is determined as 'happy.' • The data is segregated according to classes, and the samples having an average prediction probability of less than the threshold confidence value of 0.55 times the maximum probability for the corresponding class are discarded. The threshold confidence is determined in Section 3.1.1.\n\n• The four emotion classes, 'angry,' 'happy,' 'hate,' and 'sad,' are common in various datasets of different modalities  considered in this work. Samples labelled as 'excitement' were merged with 'happy,' as excitement, categorized under 'surprise' in Plutchik's wheel of emotions  [66] , shares its positive valence. Similarly, samples labelled as 'disgust' were re-labelled as 'hate,' aligning with their shared high arousal while 'sad' denotes low arousal. The final dataset contains 112455 samples with 53317 labelled as 'angry,' 44980 as 'happy,' and 10327 & 3831 as 'sad' and 'hate' respectively. Table  2  shows samples from the IIT-R MMEmoRec dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Determining Threshold Confidence Value",
      "text": "The B-T4SA dataset comprises 470586 samples labelled as 'positive,' 'negative,' and 'neutral.' Samples labelled 'neutral' were removed and the remaining 312664 samples after manual cleaning were mapped to discrete emotion labels ('angry,' 'happy,' 'hate,' 'sad') after processing the image, speech, and text components through respective emotion recognition models. This refined collection is referred to as Set A + Set B. Set A, containing 112455 samples with a high degree of confidence in their emotion classification, is designated as the final IIT-R MMEmoRec dataset. Conversely, the additional 200209 samples in Set B are with a lower degree of confidence. Both sets A and B of the dataset along with VISTANet's code can be accessed at github.com/MIntelligence-Group/MMEmoRec. To establish the confidence threshold for Set A, these factors are considered:\n\n• Modality-Wise Agreement: The chosen threshold should ensure the final emotion class with the highest f inal prob to have a minimum probability of 0.51, affirming a decisive classification without inter-modality conflict. For instances, where one modality supports a secondary emotion class, the threshold ensures that the f inal prob of this class remains below that of the primary emotion class supported by the other two modalities. It mandates a threshold greater than 0.51. • Dataset Size: Observations from Fig.  2a , indicate that until a threshold of 0.37, a significant number of samples are retained, suggesting a potential compromise in label confidence.\n\nBetween thresholds of 0.37 and 0.6, there is a steep decline in sample retention, indicating a more stringent filtering of data quality. Above a threshold of 0.6, very few samples are retained. Hence, a dataset between 0.37 and 0.6 should be selected to have a balance of dataset size and quality.\n\nTABLE 2: A few samples from IIT-R MMEmoRec dataset. Here, 'Img Prob,' 'Sp Prob,' 'Txt Prob,' and 'Final Prob' are image, speech, text, and final prediction probabilities, whereas angry, happy, hate, and sad emotion labels are denoted as 0, 1, 2 & 3 respectively.\n\n• Class Distribution Consistency: It is imperative to maintain a class distribution in the resultant dataset that mirrors that of the original B-T4SA dataset. Fig.  2b  shows that maintaining the threshold up to 0.33 preserves this distribution optimally, with thresholds up to 0.55 still acceptable before the distribution significantly diverges. Given these considerations, a threshold of 0.55 emerges as the most effective choice, balancing high confidence in data labels, adequate sample retention, and class distribution's preservation.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Human Evaluation",
      "text": "The IIT-R MMEmoRec dataset was evaluated by eight people. Two human readers (one male, one female) recorded the text components as speech, and evaluators compared these to the machine-synthesized versions on a 0 -100 contextual-similarity scale. They also rated how well each modality matched the annotated emotions. Average scores are shown in Table  3 , where S ss-hs represents the percentage of evaluators who found the synthetic speech (ss) similar to human speech (hs). S ss & S hs denote the percentages of synthetic and human speech components portraying the annotated emotion. S i and S t indicate the agreement of the image and text with the annotated emotion, respectively. S ss-i-t and S hs-i-t reflect the agreement across all three modalities considering synthetic and human speech. We had two readers read the text of the data samples and called their output human-synthesized speech. 60.72% of evaluators found",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Anthropomorphic Score Based Evaluation",
      "text": "The Anthropomorphic Score, proposed by Jaiswal et al.  [67] , quantifies the human-like quality of synthesized speech. It represents the SER accuracy ratio between synthesized speech and real speech. We assessed the reliability of the IIT-R MMEmoRec dataset's speech component synthesized via TTS using this metric.\n\nTo validate our approach, we tested it on two multimodal emotion recognition datasets: IEMOCAP  [9]  and MELD  [60] , conducting SER using their real speech and speech synthesized from their text via DeepVoice3. The computed Anthropomorphic Scores for both datasets averaged 0.94, confirming that TTS-generated speech reliably approximates real speech.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vistanet",
      "text": "The proposed system, VISTANet's architecture, is shown in Fig.  3 , which has been decided based on the ablation studies discussed in Section 5. The three modalities are fed into two types of networks: pretrained and simpler networks. The intuition behind this approach is to build a fully automated multimodal emotion classifier by including various modalities in all possible combinations and learning their weights while training without any human intervention. The proposed system contains P i and S i for image, P s and S s for speech, and P t and S t for text, denoting pretrained and simpler networks, respectively. The input speech has been converted to a log-mel spectrogram before being fed into the network. A combination of complex pre-trained models has been employed with simpler, adaptable models to enhance the system's efficiency and adaptability. This setup mirrors a dynamic where a structured, rule-following member (complex pretrained model) provides robust foundational knowledge, guiding a flexible, adaptable member (simpler model). This arrangement allows the simpler models to adapt and apply these insights to new scenarios, thus maintaining the unique identity of each modality while optimizing overall system responsiveness. By leveraging the strengths of both model types, VISTANet ensures that learning is not only comprehensive but also sufficiently flexible, allowing for an effective integration of insights across different modalities.\n\nThe idea of the proposed method is inspired by emotion theories and related cognitive studies  [65] ,  [68]  on how the human brain perceives emotions from complex environmental stimuli. The integration of complex pre-trained models with simpler models is designed to mimic the hierarchical and layered processing of emotions in the human brain  [69] ,  [70] , effectively capturing both emotional contents and general (non-emotional) information as described in Appraisal Theories  [65]  and Dimensional Theories  [68] . The human brain processes all information at the lower level neural pathways, including general information such as color, shape, pitch, etc., regardless of whether they are emotion-related or not. At a later phase, our brain integrates related attributes to form emotions or feelings. For example, a dark scene with a highpitched sound could produce 'fear'. Building on this foundation, VISTANet emulates how humans integrate multiple modalities, i.e., the pre-trained models perceive generic information, and the simpler but specific models focus on emotional attributes such as action units, speech tones, emotional words, etc., which are fused and their contributions are dynamically adjusted, mirroring the human tendency to prioritize certain channels over others depending on their informativeness for perceived emotions  [71] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Intermediate Fusion Phase",
      "text": "The images of dimension (128, 128, 3) are resized to (224, 224, 3) before being fed into P i and S i respectively. P i comprises a VGG16 model  [72]  followed by a 512-dimensional dense layer, while S i contains three convolution layers with 64, 128, and 256 filters of size (3, 3), followed by a dense layer of 512 dimensions. The spectrogram of size (128, 128, 1) from the speech input is initially processed through a convolution layer with 3 filters of size (3, 3) to enhance its feature extraction capability and to expand the channel depth to 3, making it compatible with the VGG16 model. This processed spectrogram is then further analyzed by P s and S s , which consists of the architecture as P i and S i respectively.\n\nThe text input is processed by T i , which includes a BERT model  [73] , and T s , which consists of an embedding layer followed by an LSTM layer with 64 units. Both T i and T s lead into 512dimensional dense layers. In the intermediate fusion step, all pairs of pre-trained and simpler networks from different modalities are combined using a 'WeightedAdd' layer that we have defined. This results in six distinct combinations, each processed through two dense layers with 1024 neurons, providing classification outcomes based on each pair. Equation 1 illustrates all possible pairings from the combination of pre-trained and simpler networks, ensuring that the networks in each pair do not belong to the same modality. Pairwise fusion at the intermediate stage leverages the distinct strengths of each modality pair, enhancing the model's ability to capture nuanced intermodal interactions and improve emotion recognition accuracy before final integration.\n\nwhere O 1 , O 2 , O 3 , O 4 , O 5 , and O 6 represent the classification outputs for the pairs (P i , S s ), (P i , S t ), (P s , S i ), (P s , S t ), (P t , S i ), and (P t , S s ), respectively. The 'WeightedAdd' layer ensures that during training, the weight of any weighted addition is learned using back-propagation without any human intervention. Each weight in the 'WeightedAdd' layer is randomly initialized and passed from the softmax layer, giving us positive values used as final weights and learned during training.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Late Fusion Phase",
      "text": "In this phase, the information from various modalities' all possible pairs is combined in a hybrid manner. The intermediate classification outputs obtained from above Eq. 1 are passed from another 'WeightedAdd' layer, which combines these outputs dynamically,  giving us the final output O as depicted in Eq. 2. The output O is passed from a dense layer with dimensions equal to the number of emotion classes, i.e., four.\n\nwhere",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Kaap",
      "text": "This Section proposes a novel multimodal interpretability technique, K-Average Additive exPlanation (KAAP), depicted in Fig.  4 . It computes the importance of each modality and its features while predicting a particular emotion class. Most of the existing interpretability techniques do not apply to speech and multimodal emotion recognition. Moreover, the most frequently used and accepted interpretability technique for images and text is Shapley Additive exPlanations (SHAP)  [48] , which is an approximation of Shapley values  [51] . It requires O(n 2 ) computational time complexity, whereas KAAP requires a time of O(k 2 ) where k <= n is a given hyper-parameter. Moreover, KAAP applies to multimodal emotion analysis and a single modality or a combination of any two modalities.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Calculating K-Explanable Values",
      "text": "For a model with k features {f 1 , f 2 , . . . , f k }, the K-exPlanable (KP) value of feature f i , denoted kp fi , represents its importance. Fig.  5  depicts an example calculation. Consider four nodes: Node 1 with no feature, i.e., NULL; Node 2 with a single feature f i ; Node 3 containing all remaining features from Node 1, i.e., {f 1 , f 2 , . . . , f (i-1) , f (i+1) , . . . , f k }; and Node 4 with all the features {f 1 , f 2 , . . . , f (i-1) , f i , f (i+1) , . . . , f k }. The 'Marginal Contribution' of an edge connecting Node i and Node j is defined as the difference between the prediction probabilities when using their respective features. For a given predicted label c, the marginal contribution of the feature f i from Node 1 to Node 2 is calculated as per Eq. 3, where prob {fi} is the probability of label c calculated by using only feature f i and setting all other features to zero.\n\nwhere ϕ denotes a null feature. The overall importance of f i is determined by calculating the weighted average of all 'marginal contributions' of f i as shown in Eq. 4.\n\nThe weights w 12 and w 34 must satisfy two conditions: i) their sum equals one to normalize the weights; ii) w 34 must be (k -1) times w 12 , reflecting the fact that M C fi,{fi} represents the contribution of adding f i to an empty set, while M C fi,{f1,f2,...,f k } considers its contribution to a nearly complete set of features. These relations are formulated in Eq. 5 and calculated as shown in Eq. 6.\n\nEq. 7 show the KP values calculated using Eqs. 4 and 6.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Calculating Kaap Values",
      "text": "The KAAP values are calculated to determine the importance of each modality and its features, enabling a detailed analysis of how individual and grouped elements contribute to emotion recognition. The information from image, text, and speech modalities is formatted continuously; for instance, a single pixel cannot alone  First, the input of size l is divided into k parts, where k is a hyperparameter decided through the ablation study in Section 5.4. These k parts correspond to the k features of the input. Then, for a feature group f i , KP fi (k) values are computed for the given value of k using Eq 6. It represents how a group of features f i will perform compared to all remaining groups. However, these groups can vary in size, i.e., k can have various values that lead to different groups and thus to different KP values from groups of different sizes, thus affecting the original features' importance. To deal with this issue, the weighted average of all the KP values is taken in Eq. 8 for j ∈ {2, 3 . . . , k} where weights are equal to the number of features in that group of features. k = 1 is ignored here as the whole input as one feature will not make any sense.\n\nFor input image and speech spectrogram, both of width 128 and height 128, their KP values for a given k are calculated by dividing the input into k parts along both axes. As a matrix defines image and speech spectrogram, this gives us a k * k feature group. The equation for calculating the KAAP values for the above two inputs is given by Eq. 9 where l denotes the input length for text and w denotes the input width of the image or spectrogram feature matrices, each serving as a fixed normalization bound. It gives us a matrix showing the importance of each pixel for a given image and speech input. This matrix directly represents the importance of the image. At the same time, for speech input, the values are averaged along the frequency axis to reduce the KAAP value matrix to the time axis, hence giving importance to speech at a given time.\n\nFor input text, the division is done such that each text word is considered a feature, as the emotion can only be defined by a word, not a single letter, as discussed above. Then, the text is divided into k parts, and as a linear array can represent text, the KAAP values are calculated using Eq. 8. Also, the value of k used for image, speech, and text modalities have been determined as 7, 7, and 5, respectively, in Section 5.4.2. Furthermore, the modalities' importance defined by symbols υ, δ, and τ for visual, spoken, and textual features, respectively, are computed assuming that image, speech, and text are three distinct features and calculating each modality's KAAP value for k = 3. While evaluating one modality, the others are perturbed to zero.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Implementation",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setup",
      "text": "The network training for the proposed system has been carried out on Nvidia Tesla V100 GPU, whereas the testing & evaluation have been done on an Intel(R) Core(TM) i7-8700 Ubuntu machine with 64-bit OS and 3.70 GHz, 16GB RAM.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Training Strategy And Hyperparameter Setting",
      "text": "The model training has been performed using a batch-size of 64, with data partitioned into training, validation, and testing sets at ratios of 70%, 15%, and 15%, respectively, and evaluated using 5-fold cross-validation, Adam optimizer, ReLU activation function with a learning rate of 1 × 10 -4 and ReduceLROnPlateau learning rate scheduler with a patience value of 2. The baselines and proposed models converged in terms of validation loss in 10 to 15 epochs. As a safe upper bound, the models have been trained for 50 epochs with EarlyStopping  [74]  with patience values of 5. The loss function is the average of categorical focal loss  [75]  and categorical cross-entropy loss. Accuracy and CohenKappa  [76]  have been analyzed for the model evaluation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baselines And Proposed Models",
      "text": "The 'Image + Speech + Text' configuration described in Section 5.4.1 is taken as Baseline 1. In Baselines 2-6, this simple fusion is replaced by each of the six pairwise fusion schemes defined in Eq. 10, allowing us to isolate and evaluate the contribution of every modality pair. All baselines share the same featureextraction backbones, classification head, and employ KAAP for interpretability and are made on a common idea, as described below. Firstly, all three modalities are fed into P i , P s , S i , S s , T i and T s as described in Section 3.2, and are then passed from a dense layer of 512 neurons, resulting in a 512-dimensional outputs which are then combined using 'WeightedAdd' to give three outputs. The following strategy is being followed for combining them: any pretrained network must be combined with another simpler network. At least one combination must contain the network from different modalities because if all the modalities combine with themselves, then such a combination will not lead to any information exchange. Thus, six such configurations are possible, as described in Eq. 10.\n\nConfiguration (#1) is discarded because it fails to meet the requirement that at least one combination must include different modalities. Configurations (#2), (#3), and (#6) are partiallycomplete as they involve networks from the same modalities, while (#4) and (#5) are complete. This strategy reveals two issues: i) only two out of five baselines are complete; ii) different datasets require different modal strengths. To adapt to various datasets and scenarios, the VISTANet system is introduced. It merges the outputs of baselines 2-6, avoids self-combinations, and dynamically adjusts weights to meet dataset and scenario needs.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Quantitative Results",
      "text": "The VISTANet has achieved emotion recognition accuracies of 95.99%, 75.13% and 80.11% for Set A, Set B and the overall dataset described in Section 3.1.1. The detailed results, along with the results of various baseline models, are shown in Table  4 . VISTANet and baseline models employ KAAP for interpretability, yielding an average inference time of approximately 26.8 seconds per sample. Replacing KAAP with SHAP in VISTANet increased the inference time to 43.9 seconds i.e. a 1.63 times slow-down while keeping the classification metrics (Acc, F1, CK, P, R) unchanged. Similar slowdown was observed for the baseline models on replacing KAAP with SHAP for interpretability.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Qualitative Results",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Fig. 6 Shows Sample Emotion Classification & Interpretation Results.",
      "text": "The key speech and image features for emotion classification are identified, with corresponding words highlighted. In the waveform, yellow and blue indicate the most and least important features, respectively. Speech and text are observed to be the most contributing to predicting the 'angry' and 'hate' classes, while image and text equally influence the 'happy' and 'sad' classes.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results Comparison",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion Recognition Results' Comparison",
      "text": "Table  5  summarizes the emotion recognition results on the IIT-R MMEmoRec, IEMOCAP  [9] , and MELD  [60]  datasets for VIS-TANet and state-of-the-art (SOTA) multimodal emotion recognition methods. It is important to note that VISTANet processes images for visual modality input, in contrast to some SOTA methods that utilize video inputs. To adapt SOTA methods for the IIT-R MMEmoRec dataset, we replicated the same image to match the frame requirements of different methods. For instance, while MER-MULTI uses the average of all frames, we utilized the original image features directly. Conversely, the Multimedia Information Bottleneck (MIB) method aligns frames with the number of words in the text, requiring us to copy the image features as many times as there are words. Self-distillation (SDT) for emotion recognition in conversations (ERC)  [19]  and graph neural network (GNN) for ERC  [77]  focus on processing conversations by considering past emotions to predict current ones, in contrast to our work on multimodal emotion recognition which predicts each data instance's emotion independently. We implemented these methods on the IEMOCAP and MELD datasets, maintaining the same conditions aimed at classifying emotions independently of the previous context. However, the lack of documented preprocessing steps for these datasets prevented us from preparing the MMEmoRec dataset in the necessary format, thus, SDT and GNN methods were applied only to IEMOCAP and MELD.\n\nAs observed from Table  5 , VISTANet either outperforms or closely competes with the SOTA methods across all datasets. This underscores its capability to handle diverse emotion recognition scenarios effectively. Additionally, the successful application of SOTA emotion recognition methods on the IIT-R MMEmoRec TABLE 4: Emotion recognition results on Set A, Set B, and overall for the IIT-R MMEmoRec dataset. Baseline 1 uses simple image + speech + text (IST) fusion while baselines 2-6 use the pairwise fusion configurations from Eq. 10. Here, 'Acc,' 'F1,' 'CK,' 'P,' 'R' denote accuracy, F1-score, CohenKappa score, precision, and recall. The ↑ indicates that their higher values are better while the highest value is in bold. 'T' denotes average per-sample interpretability inference time in seconds. The ↓ indicates that its lower values are better.   dataset further validates its reliability and usefulness. Furthermore, the experiments conducted on the IIT-R MMEmoRec dataset are speaker-dependent, as all speech samples were generated using the TTS strategy described in Section 3.2. In contrast, the experiments on the IEMOCAP and MELD datasets are speaker-independent.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Sentiment Classification Results' Comparison",
      "text": "The IIT-R MMEmoRec dataset has been constructed from the B-T4SA dataset in this paper; hence, there are no existing emotion recognition results for it. However, sentiment classification (into 'neutral,' 'negative,' and 'positive' classes) results on the B-T4SA dataset are available in the literature, which have been compared with VISTANet's sentiment classification results in Table  6 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Approach Modality Accuracy",
      "text": "Cross-Modal Learning  [12]  V + T 51.30% Multimodal Sentiment Analysis  [11]  V + T 60.42% Hybrid Fusion  [81]  V + T 86.70% Automated ML  [82]  V + T 95.19% VISTANet V + S + T 96.59%",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ablation Studies",
      "text": "The ablation studies have been performed on set A to determine the threshold confidence for data construction, appropriate network configuration for VISTANet, and suitable k values for KAAP.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Study 1: Determining Baselines And Proposed System'S Architecture",
      "text": "To begin with, the emotion recognition has been performed for a single modality at a time, i.e., separate IER, SER, and TER using pre-trained VGG models  [72]  for Image & speech and BERT  [73]  for text. The performance has been evaluated in terms of Accuracy, CohenKappa metric (CK), F1 score, Precision, and Recall and summarized in Table  7 . The CK metric measures whether the distribution of the predicted class is in line with the ground truth. Next, we move on to the combination of two modalities. The chosen two modalities are fed into respective pre-trained models and then passed from a dense layer with 512 neurons. Then the information from these modalities is added using the 'WeightedAdd' layer defined in 3.2.1. This output is next passed from three dense layers of size 1024, 1024, and 4 neurons, which then classifies the emotion. An additional evaluation confirmed that combining two modalities outperforms individual modalities.\n\nFinally, the information from all three modalities is combined and fed into their respective pre-trained models and is then passed from a dense layer of size 512, which is then passed from a 'WeightedAdd' layer; the output of this layer is passed from 3 dense layers as in the combination of two modalities. Combining all three modalities has performed better than the remaining models in all the evaluation metrics. As observed during the experiments above, combining the information from the complementary modalities has led to better emotion recognition performance. Hence, the baselines and proposed model have been formulated with all three modalities and various information fusion mechanisms in Section 4.3. This procedure was applied to all three modalities, and the results showing the effect of increasing k are visualized in Fig.  7 . For image & speech, the value converges to 1 at k = 7, while for text, the optimal value of k is 5.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Ablation Study 3: Performance For Missing Modalities",
      "text": "In real-life scenarios, some data samples may be missing information about one of the modalities. The VISTANet has been evaluated for such scenarios. We formulated four use cases with image, speech, text, or no modality missing and divided the test dataset into randomly selected equal parts accordingly. Then the information of the missing modality has been overridden to null, and VISTANet has been evaluated for emotion recognition. As per Table  8 , the emotion recognition performance for missing no modality aligns with the results observed in Section5.2. Further, missing image modality information has caused the least dip in the performance. Moreover, the information from speech and text modalities combined has resulted in an accuracy of 82.59%, whereas including all the modalities resulted in 95.90% accuracy. These observations are consistent with Section 5.4.1, which reports that IER performance was lower than that of TER and SER.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ablation Study 4: Classwise Modality Pair Contribution",
      "text": "Fig.  6  provides sample-level qualitative results. This study analyzes dataset-wide, quantitative impact of each modality pair's impact on each emotion. The six weights from the 'WeightedAdd' layer that combines outputs O 1 -O 6 in Eq. 2 are recorded for each test sample. Then the average is computed both overall and for each emotion class. Table  9  shows the average weight contribution per pair, with weights L1-normalised to sum to 1.0 per row.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Discussion",
      "text": "While many multimodal tasks prioritize a single input, VISTANet learns optimal fusion weights across all six modality pairs for balanced integration. The bi-modal configuration uses two active modalities; in contrast, the tri-modal case with a missing modality still processes three inputs (two real and one zeroed), leading to different fusion dynamics and performance outcomes. Leveraging the IIT R MMEmoRec dataset's complementary cues, the network learns context-linked emotions and continuously corrects bias introduced by averaged unimodal labels. Image is the most complex and diverse modality (wide variations in size, hue, saturation, angle, brightness, objects and backgrounds), whereas speech spectrograms and text embeddings are more structured and less diverse. Removing text or speech eliminates structural information the model expects, causing larger accuracy drops than zeroing the image (the image-only setup yields only 60.44% accuracy, and the \"Missing Image\" case sees a smaller overall drop than omitting speech or text). Human evaluators verified label consistency and audio quality. Ablation studies and missing-modality experiments confirm the benefit of integrating all signals. Finally, the KAAP technique quantifies each modality's and feature's influence and offers a tool for future advances in multimedia emotion analysis.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "The proposed system, VISTANet, performs emotion recognition by considering the information from the image, speech & text modalities. It combines the information from these modalities in a hybrid manner of intermediate and late fusion and determines their weights automatically. It has resulted in better performance on including image, speech & text modalities than including only one or two of these modalities. The proposed interpretability technique, KAAP, identifies each modality's contribution and important features toward predicting a particular emotion class. The future research plan includes transforming emotional content from one modality to another. We will also work on controllable emotion generation, where the output contains the desired emotional tone.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows an example of emotion label determination,",
      "page": 3
    },
    {
      "caption": "Figure 1: Example of emotion label determination.",
      "page": 3
    },
    {
      "caption": "Figure 2: a, indicate that until",
      "page": 3
    },
    {
      "caption": "Figure 2: b shows that maintaining the",
      "page": 4
    },
    {
      "caption": "Figure 2: Determining threshold confidence for dataset construction.",
      "page": 4
    },
    {
      "caption": "Figure 3: Schematic architecture of the proposed multimodal emotion recognition system. Here, Pm & Sm denote the pre-trained &",
      "page": 6
    },
    {
      "caption": "Figure 4: It computes the importance of each modality and its features",
      "page": 6
    },
    {
      "caption": "Figure 5: depicts an example calculation. Consider four nodes: Node",
      "page": 6
    },
    {
      "caption": "Figure 4: Schematic representation of the proposed interpretability technique. The symbols ki, ks, and kt represent number of image,",
      "page": 7
    },
    {
      "caption": "Figure 5: Sample model for KP values computation.",
      "page": 7
    },
    {
      "caption": "Figure 6: shows sample emotion classification & interpretation results.",
      "page": 8
    },
    {
      "caption": "Figure 6: Sample results. ‘P’ & ‘GT:’ predicted & ground-truth labels. ‘Score:’ visual (υ), speech (δ) & textual (τ) modalities’ importance.",
      "page": 9
    },
    {
      "caption": "Figure 7: Ablation Study 2: Determining appropriate k values.",
      "page": 10
    },
    {
      "caption": "Figure 6: provides sample-level qualitative results. This study analyzes",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conv(64)": "Conv(128)"
        },
        {
          "Conv(64)": "Conv(256)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Embedding": "LSTM(64)"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Baseline 1 (IST + KAAP",
      "venue": "Baseline 1 (IST + KAAP"
    },
    {
      "citation_id": "2",
      "title": "Multimodal Machine Learning: A Survey and Taxonomy",
      "authors": [
        "T Baltrušaitis",
        "C Ahuja"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Emotion Recognition from Unimodal to Multimodal Analysis: A Review",
      "authors": [
        "K Ezzameli",
        "H Mahersia"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "4",
      "title": "Recognizing Induced Emotions of Movie Audiences from Multimodal Information",
      "authors": [
        "M Muszynski",
        "L Tian"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "MGEED: A Multimodal Genuine Emotion and Expression Detection Database",
      "authors": [
        "Y Wang",
        "H Yu",
        "W Gao",
        "Y Xia",
        "C Nduka"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Interpretable Explainability in Facial Emotion Recognition and Gamification for Data Collection",
      "authors": [
        "K Shingjergji",
        "D Iren"
      ],
      "year": "2022",
      "venue": "Int. Conf. on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "7",
      "title": "EMERSK-Explainable Multimodal Emotion Recognition with Situational Knowledge",
      "authors": [
        "M Palash",
        "B Bhargava"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "8",
      "title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations",
      "authors": [
        "N Majumder",
        "S Poria"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "9",
      "title": "Towards Interpretable Facial Emotion Recognition",
      "authors": [
        "S Malik",
        "P Kumar",
        "B Raman"
      ],
      "year": "2021",
      "venue": "The 12th Indian Conference on Computer Vision, Graphics and Image Processing"
    },
    {
      "citation_id": "10",
      "title": "IEMOCAP: Interactive Emotional dyadic MOtion CAPture data",
      "authors": [
        "C Busso"
      ],
      "year": "2008",
      "venue": "Language Resources & Evaluation"
    },
    {
      "citation_id": "11",
      "title": "Recognizing Induced Emotions of Movie Audiences: Are Induced and Perceived Emotions The Same",
      "authors": [
        "L Tian",
        "M Muszynski"
      ],
      "year": "2017",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "12",
      "title": "A Multimodal Approach to Image Sentiment Analysis",
      "authors": [
        "A Gaspar",
        "L Alexandre"
      ],
      "year": "2019",
      "venue": "Springer International Conference on Intelligent Data Engineering and Automated Learning"
    },
    {
      "citation_id": "13",
      "title": "Cross-Media Learning for Image Sentiment Analysis in the Wild",
      "authors": [
        "L Vadicamo",
        "F Carrara"
      ],
      "year": "2017",
      "venue": "ICCV Workshops"
    },
    {
      "citation_id": "14",
      "title": "A deep Interpretable Representation Learning Method for Speech Emotion Recognition",
      "authors": [
        "E Jing",
        "Y Liu"
      ],
      "year": "2023",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "15",
      "title": "Emotion Transplantation Through Adaptation in HMM-based Speech Synthesis",
      "authors": [
        "J Lorenzo-Trueba",
        "R Barra-Chicote"
      ],
      "year": "2015",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "16",
      "title": "Explainable Convolutional Neural Network with Facial Emotion Enabled Music Recommender System",
      "authors": [
        "R Udendhran"
      ],
      "year": "2022",
      "venue": "4th Int. Conf. on Information Management and Machine Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Learning Discriminative Features from Spectrograms using Center Loss for SER",
      "authors": [
        "D Dai",
        "Z Wu"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "18",
      "title": "Focus-Attention-Enhanced Crossmodal Transformer with Metric Learning for MM Speech Emo Recognition",
      "authors": [
        "K Kim",
        "N Cho"
      ],
      "year": "2023",
      "venue": "Focus-Attention-Enhanced Crossmodal Transformer with Metric Learning for MM Speech Emo Recognition"
    },
    {
      "citation_id": "19",
      "title": "Learning Salient Features for Speech Emotion Recognition using Convolutional Neural Networks",
      "authors": [
        "Q Mao",
        "M Dong"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "A Transformer-based Model With SDT for Multimodal ERC",
      "authors": [
        "H Ma"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "GA2MIF: Graph and Attention based Two-Stage Multi-Source Information Fusion for Conversational Emotion Detection",
      "authors": [
        "J Li",
        "X Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Explainable Emotion Recognition from Tweets using Deep Learning and Word Embedding Models",
      "authors": [
        "A Abubakar"
      ],
      "year": "2022",
      "venue": "IEEE 19th India Council International Conference"
    },
    {
      "citation_id": "23",
      "title": "GraphCFC: A Directed Graph based Cross-Modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition",
      "authors": [
        "J Li",
        "X Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "24",
      "title": "An Effective Approach for Emotion Detection in Multimedia Text using Sequence based Convolutional Network",
      "authors": [
        "K Shrivastava"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "25",
      "title": "Semantic Emotion Neural Net for Emotion Recognition from Text",
      "authors": [
        "E Batbaatar"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "26",
      "title": "Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-Related Applications",
      "authors": [
        "C Corneanu",
        "M Simón"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Exploring the Contextual Factors Affecting Multimodal Emotion Recognition in Videos",
      "authors": [
        "P Bhattacharya",
        "R Gupta",
        "Y Yang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Interpretability for Multimodal Emotion Recognition using Concept Activation Vectors",
      "authors": [
        "A Asokan",
        "N Kumar",
        "A Ragam",
        "S Shylaja"
      ],
      "year": "2022",
      "venue": "Int. Joint Conf. on Neural Networks"
    },
    {
      "citation_id": "29",
      "title": "Multimodal Emotion Recognition with High-level Speech and Text Features",
      "authors": [
        "M Makiuchi",
        "K Uto",
        "K Shinoda"
      ],
      "year": "2021",
      "venue": "Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "30",
      "title": "Multimodal Speech Emotion Recognition using Audio and Text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "31",
      "title": "Jointly Fine Tuning 'BERT-Like' Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "authors": [
        "S Siriwardhana",
        "A Reis",
        "R Weerasekera"
      ],
      "year": "2020",
      "venue": "Jointly Fine Tuning 'BERT-Like' Self Supervised Models to Improve Multimodal Speech Emotion Recognition"
    },
    {
      "citation_id": "32",
      "title": "Fusing Audio, Visual and Textual Clues for Sentiment Analysis from Multimodal Content",
      "authors": [
        "S Poria",
        "E Cambria"
      ],
      "year": "2016",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "33",
      "title": "End-to-End Multimodal Emotion Recognition Using Deep Neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Multimodal Emotion Classification with Multi-Level Semantic Reasoning Network",
      "authors": [
        "T Zhu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "A Multimodal Fusion Emotion Recognition Method based on Multitask Learning and Attention Mechanism",
      "authors": [
        "J Xie",
        "J Wang"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "36",
      "title": "A Co-Memory Network for Multimodal Sentiment Analysis",
      "authors": [
        "N Xu",
        "W Mao",
        "G Chen"
      ],
      "year": "2018",
      "venue": "ACM International Conference on Research & Development in Information Retrieval"
    },
    {
      "citation_id": "37",
      "title": "Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers",
      "authors": [
        "M Shayaninasab",
        "B Babaali"
      ],
      "year": "2024",
      "venue": "Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers",
      "arxiv": "arXiv:2402.07327"
    },
    {
      "citation_id": "38",
      "title": "Multimodal Information Bottleneck: Learning Minimal Sufficient Unimodal and Multimodal Representations",
      "authors": [
        "S Mai",
        "Y Zeng",
        "H Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "39",
      "title": "Deep Learning-based Multimodal Emotion Recognition: A Systematic Review of Recent Advancements and Future Prospects",
      "authors": [
        "S Zhang",
        "Y Yang"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "40",
      "title": "Transformer-Based Multimodal Feature Enhancement Networks for Depression Detection Integrating Video, Audio and Remote PPG Signals",
      "authors": [
        "H Fan"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "41",
      "title": "FV2ES: A Fully End2End Multimodal System For Fast Yet Effective Video Emotion Recognition Inference",
      "authors": [
        "Q Wei",
        "X Huang",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Broadcasting"
    },
    {
      "citation_id": "42",
      "title": "UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition",
      "authors": [
        "G Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "43",
      "title": "Multimodal Speech Emotion Recognition Using Modality-Specific Self-Supervised Frameworks",
      "authors": [
        "R Patamia"
      ],
      "year": "2023",
      "venue": "International Conference on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "44",
      "title": "MemoBERT: Pre-training Model With Prompt-based Learning for MM Emotion Recognition",
      "authors": [
        "J Zhao"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "45",
      "title": "Two Birds With One Stone: Knowledge-Embedded Temporal Convolutional Transformer for Depression Detection and Emotion Recognition",
      "authors": [
        "W Zheng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "SoundNet: Learning Sound Representations from Unlabeled Video",
      "authors": [
        "Y Aytar",
        "C Vondrick",
        "A Torralba"
      ],
      "year": "2016",
      "venue": "SoundNet: Learning Sound Representations from Unlabeled Video"
    },
    {
      "citation_id": "47",
      "title": "Multimodal Emotion Recognition by Fusing Correlation Features of Speech-Visual",
      "authors": [
        "C Guanghui",
        "Z Xiaoping"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "48",
      "title": "Towards the Explainability of Multimodal Speech Emotion Recognition",
      "authors": [
        "P Kumar",
        "V Kaushik"
      ],
      "year": "2021",
      "venue": "Towards the Explainability of Multimodal Speech Emotion Recognition"
    },
    {
      "citation_id": "49",
      "title": "A Unified Approach to Interpreting Model Predictions",
      "authors": [
        "S Lundberg",
        "S.-I Lee"
      ],
      "year": "2017",
      "venue": "A Unified Approach to Interpreting Model Predictions"
    },
    {
      "citation_id": "50",
      "title": "Beyond Human: Deep Learning, Explainability and Representation",
      "authors": [
        "M Fazi"
      ],
      "year": "2020",
      "venue": "Theory, Culture & Society"
    },
    {
      "citation_id": "51",
      "title": "Learning Important Features Through Propagating Activation Differences",
      "authors": [
        "A Shrikumar"
      ],
      "year": "2017",
      "venue": "ICML"
    },
    {
      "citation_id": "52",
      "title": "A Value for n-person Games, Contributions to the Theory of Games II",
      "authors": [
        "L Shapley"
      ],
      "year": "1953",
      "venue": "A Value for n-person Games, Contributions to the Theory of Games II"
    },
    {
      "citation_id": "53",
      "title": "Polynomial Calculation of The Shapley Value based on Sampling",
      "authors": [
        "J Castro"
      ],
      "year": "2009",
      "venue": "Polynomial Calculation of The Shapley Value based on Sampling"
    },
    {
      "citation_id": "54",
      "title": "An Interpretability Illusion for BERT",
      "authors": [
        "T Bolukbasi"
      ],
      "year": "2021",
      "venue": "An Interpretability Illusion for BERT",
      "arxiv": "arXiv:2104.07143"
    },
    {
      "citation_id": "55",
      "title": "Understanding Deep Networks Via Extremal Perturbations and Smooth Masks",
      "authors": [
        "R Fong",
        "M Patrick"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "56",
      "title": "Why Should I Trust You? Explaining Predictions of Any Classifier",
      "authors": [
        "M Ribeiro",
        "S Singh",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "International Conference on Knowledge Discovery & Data mining"
    },
    {
      "citation_id": "57",
      "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
      "authors": [
        "K Simonyan"
      ],
      "year": "2014",
      "venue": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps"
    },
    {
      "citation_id": "58",
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "authors": [
        "R Selvaraju"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "59",
      "title": "The (Un)reliability of Saliency Methods",
      "authors": [
        "P.-J Kindermans"
      ],
      "year": "2019",
      "venue": "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning"
    },
    {
      "citation_id": "60",
      "title": "MER 2023: Multi-Label Learning, Modality Robustness, and Semi-Supervised Learning",
      "authors": [
        "Z Lian"
      ],
      "venue": "ACM MM, 2023"
    },
    {
      "citation_id": "61",
      "title": "MELD: A Multimodal Multi-Party Dataset For Emotion Recognition in Conversations",
      "authors": [
        "S Poria"
      ],
      "year": "2018",
      "venue": "MELD: A Multimodal Multi-Party Dataset For Emotion Recognition in Conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "62",
      "title": "YOLOv8 Guide: Step-by-Step Implementation of the Latest YOLO Architecture",
      "authors": [
        "Viso",
        "Ai"
      ],
      "year": "2024",
      "venue": "YOLOv8 Guide: Step-by-Step Implementation of the Latest YOLO Architecture"
    },
    {
      "citation_id": "63",
      "title": "DeepVoice 3: Scaling Text-to-Speech with Convolutional Sequence Learning",
      "authors": [
        "W Ping",
        "K Peng"
      ],
      "year": "2018",
      "venue": "ICLR"
    },
    {
      "citation_id": "64",
      "title": "WaveNet: A Generative Model for Raw Audio",
      "authors": [
        "A Oord"
      ],
      "year": "2016",
      "venue": "WaveNet: A Generative Model for Raw Audio",
      "arxiv": "arXiv:1609.03499"
    },
    {
      "citation_id": "65",
      "title": "Building a Large Scale Dataset for Image Emotion Recognition: the Fine Print and the Benchmark",
      "authors": [
        "Q You",
        "J Luo",
        "H Jin",
        "J Yang"
      ],
      "year": "2016",
      "venue": "AAAI"
    },
    {
      "citation_id": "66",
      "title": "Evidence For Universality and Cultural Variation of Differential Emotion Response Patterning",
      "authors": [
        "K Scherer",
        "H Wallbott"
      ],
      "year": "1994",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "67",
      "title": "The Nature of Emotions: Human Emotions Have Deep Evolutionary Roots, a Fact That May Explain Their Complexity and Provide Tools for Clinical Practice",
      "authors": [
        "R Plutchik"
      ],
      "year": "2001",
      "venue": "Journal Storage Digital Library's American scientist Journal"
    },
    {
      "citation_id": "68",
      "title": "A Generative Adversarial Network Based Ensemble Technique For Automatic Evaluation of Machine Synthesized Speech",
      "year": "2019",
      "venue": "Asian Conference on Pattern Recognition"
    },
    {
      "citation_id": "69",
      "title": "A Circumplex Model of Affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "70",
      "title": "Neural Systems for Recognizing Emotion",
      "authors": [
        "R Adolphs"
      ],
      "year": "2002",
      "venue": "Current Opinion in Neurobiology"
    },
    {
      "citation_id": "71",
      "title": "On the Relationship Between Emotion and Cognition",
      "authors": [
        "L Pessoa"
      ],
      "year": "2008",
      "venue": "Nature Reviews Neuroscience"
    },
    {
      "citation_id": "72",
      "title": "The Perception of Emotions by Ear and by Eye",
      "authors": [
        "B Gelder",
        "J Vroomen"
      ],
      "year": "2000",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "73",
      "title": "Very Deep ConvNet for Large-Scale Image Recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very Deep ConvNet for Large-Scale Image Recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "74",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "75",
      "title": "Early Stopping -But When?",
      "authors": [
        "L Prechelt"
      ],
      "year": "1998",
      "venue": "Neural Networks: Tricks of the trade"
    },
    {
      "citation_id": "76",
      "title": "Focal Loss for Dense Object Detection",
      "authors": [
        "T.-Y Lin",
        "P Goyal"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "77",
      "title": "Cohen's Kappa Coefficient as a Performance Measure for Feature Selection",
      "authors": [
        "S Vieira",
        "U Kaymak",
        "J Sousa"
      ],
      "year": "2010",
      "venue": "The 18th IEEE International Conference on Fuzzy Systems"
    },
    {
      "citation_id": "78",
      "title": "Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation",
      "authors": [
        "F Chen"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "79",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "year": "2019",
      "venue": "EMNLP"
    },
    {
      "citation_id": "80",
      "title": "DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai"
      ],
      "year": "2021",
      "venue": "ACL"
    },
    {
      "citation_id": "81",
      "title": "MM-DFN: Multimodal Dynamic Fusion Network For Emotion Recognition In Conversations",
      "authors": [
        "D Hu"
      ],
      "year": "2022",
      "venue": "Int. Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "82",
      "title": "Hybrid Fusion based Approach for Multimodal Emotion Recognition with Insufficient Labeled Data",
      "authors": [
        "P Kumar",
        "V Khokher",
        "Y Gupta",
        "B Raman"
      ],
      "year": "2021",
      "venue": "ICIP"
    },
    {
      "citation_id": "83",
      "title": "An AutoML-based Approach to Multimodal Image Sentiment Analysis",
      "authors": [
        "V Lopes"
      ],
      "year": "2021",
      "venue": "IJCNN"
    },
    {
      "citation_id": "84",
      "title": "Learning to Predict Crisp Boundaries",
      "authors": [
        "R Deng"
      ],
      "year": "2018",
      "venue": "15th European Conference on Computer Vision"
    }
  ]
}