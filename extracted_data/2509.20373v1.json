{
  "paper_id": "2509.20373v1",
  "title": "Speaker Style-Aware Phoneme Anchoring For Improved Cross-Lingual Speech Emotion Recognition",
  "published": "2025-09-19T21:03:21Z",
  "authors": [
    "Shreya G. Upadhyay",
    "Carlos Busso",
    "Chi-Chun Lee"
  ],
  "keywords": [
    "speech emotion recognition",
    "domain adaptation",
    "cross-lingual",
    "transfer learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Cross-lingual speech emotion recognition (SER) remains a challenging task due to differences in phonetic variability and speaker-specific expressive styles across languages. Effectively capturing emotion under such diverse conditions requires a framework that can align the externalization of emotions across different speakers and languages. To address this problem, we propose a speaker-style aware phoneme anchoring framework that aligns emotional expression at the phonetic and speaker levels. Our method builds emotionspecific speaker communities via graph-based clustering to capture shared speaker traits. Using these groups, we apply dual-space anchoring in speaker and phonetic spaces to enable better emotion transfer across languages. Evaluations on the MSP-Podcast (English) and BIIC-Podcast (Taiwanese Mandarin) corpora demonstrate improved generalization over competitive baselines and provide valuable insights into the commonalities in cross-lingual emotion representation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is essential for emotionally intelligent human-computer interaction, with applications in dialogue systems, virtual assistants, mental health monitoring, and customer service  [1] [2] [3] [4] [5] . While promising in controlled conditions, SER performance often drops in real-world settings due to mismatches in language, speaker traits, or recording environments. This challenge has led to growing interest in cross-domain or cross-lingual SER, which aims to build models that generalize emotion recognition across languages and domains  [6] [7] [8] . To mitigate domain shift, approaches such as adversarial training, feature normalization, and contrastive learning have been explored  [9] [10] [11] . Anchoring-based approaches have recently gained attention as effective strategies for improving cross-domain or crosslingual generalization in SER  [12] [13] [14] [15] . These methods align emotionally meaningful subspaces, such as phoneme-level features across corpora with different linguistic properties. By identifying linguistic units that consistently convey emo-tion across languages as anchors, these methods guide models to learn stable and transferable emotion representations. This targeted anchoring mitigates domain mismatch and enhances both the robustness and interpretability of emotion features, making it effective for cross-lingual SER.\n\nWhile anchoring-based methods have shown promising results, they often rely on features extracted from large pretrained or self-supervised learning (SSL) models such as WavLM  [16]  and HuBERT  [17]  to represent phonetic content. Although these models provide rich and powerful embeddings, the learned representations are inherently entangled. They capture a mixture of phonetic structure, speaker identity, prosody, and even environmental noise. This complexity makes it difficult to discern which specific information is being anchored across corpora. It raises questions such as whether we are aligning phonemes, vocal style, or both. Such ambiguity limits interpretability and makes it challenging to exert fine-grained control over the transfer process, particularly in cross-lingual SER where aligning emotion-relevant cues is essential for effective generalization.\n\nEmotional expression in speech depends not only on what is said, represented by phonemes and lexical units, but also on how it is spoken. The expressive realization of emotion is shaped by speaker-specific prosodic cues, vocal dynamics, and articulatory styles, which vary across individuals and cultures  [14, 18] . While phonemes carry important emotional information  [19, 20] , their realization can differ depending on speaker characteristics and phonological structure of the language  [21] [22] [23] . These variations are conveyed through differences in pitch, intensity, and voice quality, all of which influence how emotion is embedded in speech  [19] . Therefore, a more interpretable and controllable SER system must consider both the phoneme-level content and the speaker style with which it is delivered, especially in cross-lingual settings.\n\nRecent trends in speech and vision have shown that disentangled representations improve interpretability and control in tasks involving complex factors such as identity, style, and content. In particular, many studies in voice conversion  [24, 25] , speech synthesis  [26] , and emotional speech editing  [27]  explicitly separate different representation spaces to enable more controllable and adaptive modeling. In this study, we adopt a dual-encoder setup based on FreeVC  [24] ,\n\nwhere one encoder models phonetic content and the other captures speaker-related characteristics. Although speaker encoders in VC models are primarily designed for speaker identification (ID), recent findings suggest that speaker embeddings trained for ID tasks still encode emotion-dependent expressivity, leading to clustering patterns that vary with emotion  [28] . By using these encoders independently, our framework captures emotion-relevant cues from both phoneme and speaker-style dimensions. This setup allows us to investigate how shared phonemes, when articulated with similar expressive patterns can support emotion transfer across languages. We argue that effective cross-lingual emotion recognition requires aligning both what is spoken (phonetic content) and how it is expressed (speaker style).\n\nIn this work, we propose the speaker-style aware phoneme anchoring (SAPA), a novel framework for improving crosslingual SER. We evaluate SAPA on two diverse corpora: the MSP-Podcast (American English) and BIIC-Podcast (Taiwanese Mandarin) corpora. First, we analyze how emotional cues align across languages in speaker and phoneme spaces. Despite linguistic differences, speakers form emotion-specific clusters, and some phonemes appear to carry emotional cues across corpora. These observations motivate our dualspace anchoring strategy, where the model learns from both phonetic-level acoustic patterns and speaker-style variations. By guiding learning through these complementary spaces, SAPA captures stable emotional cues that generalize across languages. In cross-lingual transfer tasks, SAPA outperforms strong baselines, achieving 59.25% of unweighted average recall (UAR) when training with the MSP-Podcast corpus and testing with the BIIC-Podcast corpus and 56.54% UAR for the reverse, highlighting the effectiveness of speaker-style informed phoneme anchoring.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Embedding-Level Commonality",
      "text": "In this section, we analyze the emotion-specific commonalities across the two languages by examining both the speaker style space and the phonetic representation space.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Affective Naturalistic Corpora",
      "text": "The MSP-Podcast (MSP-P)  [29]  corpus comprises 324 hours of emotional American English speech (version 1.12), collected from audio-sharing platforms. Its large scale and emotionally balanced dialogues from diverse speakers make it a valuable resource for SER research. The corpus includes annotations for primary emotions, secondary emotions, and emotional attributes. In this study, we focus on four primary emotion categories: neutral, happiness, anger, and sadness, using a total of 49,018 samples with predefined training, validation, and test splits. The corpus includes phonetic information for all samples.\n\nThe BIIC-Podcast (BIIC-P)  [30]  corpus is a SER dataset (version 1.0) in Taiwanese Mandarin. It comprises 157 hours of podcast speech and adopts a data collection methodology similar to that of the MSP-P corpus. The dataset includes annotations for primary and secondary emotions, along with three emotional attributes. We utilize 22,799 samples focused on four primary emotional categories, using the predefined train-validation-test splits provided by the dataset creators. For phonetic information, we apply the phone alignment approach described in Upadhyay et al.  [30] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speaker-Style Encoded Commonality",
      "text": "To capture speaker-level variations in emotional expression across corpora, we adopt a graph-based similarity clustering approach  [31] [32] [33] . This method allows us to effectively uncover communities of speakers who share similar expressive styles. We construct the graph using embeddings from the speaker encoder of the FreeVC architecture  [24] , which captures speaker-specific traits such as prosody, voice quality, and articulation style. These embeddings form the basis for identifying speaker clusters that are emotion-consistent.\n\nLet S = {z\n\ns , . . . , z\n\ns } be the set of speaker embeddings corresponding to N speakers, extracted specifically from utterances labeled with a target emotion (e.g., anger). We construct an undirected, weighted graph G = (V, E), where, each node v i ∈ V corresponds to a speaker embedding z (i) s . Each edge e ij ∈ E is weighted by the cosine similarity between the speaker embeddings as shown in Eq. 1. To focus on meaningful relationships, we retain only edges with similarity above a threshold τ = 0.7.\n\nWe then apply Louvain clustering  [34]  to identify speaker communities, where each cluster C k groups speakers with similar emotional speaking styles across languages.\n\nTo assess the quality of our emotion-specific graph-based speaker clusters, we visualize a subset of 15 speakers each from the MSP-P and BIIC-P corpora on the pre-constructed similarity graph for each major emotion. We plot the nodes using distinct shapes to represent the datasets (circles for MSP-P and squares for BIIC-P) and use colors to indicate cluster membership. As shown in Figure  1 , speakers with similar emotional expression styles tend to group together, even across languages, suggesting that the graph captures emotion-driven similarities that generalize cross-lingually.\n\nFigure  1  shows that several clusters include speakers from both corpora, supporting our hypothesis that emotional vocal traits exhibit cross-lingual alignment. Additionally, the clustering patterns vary across emotions, some speakers group together under one emotion (e.g., sadness) but diverge under another (e.g., anger), indicating that the embeddings capture emotion-specific articulatory differences. These observations suggest that our graph-based approach can effectively model  To further validate the emotion-relevance of these clusters, we compute the modularity score Q for each emotion graph, which quantifies how well the graph partitions into dense, emotion-consistent communities.\n\nwhere A ij is the edge weight between nodes i and j, k i is the degree of node i, m is the total edge weight in the graph, and δ(c i , c j ) is 1 if nodes i and j belong to the same cluster and 0 otherwise. The modularity score Q ranges from 0 to 1, with higher values indicating better-defined and more cohesive community structures. The modularity scores for different emotions are presented in Table  1 . As shown, all considered emotions yield relatively high modularity values, indicating that speaker expressions are consistent and form well-defined clusters. This supports the use of emotion-specific speaker groupings to guide our dual-space phoneme anchoring and suggests that certain emotions offer more stable cues for effective crosslingual alignment.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Phoneme-Encoded Commonality",
      "text": "To assess phoneme alignment across corpora, we analyze the content encoder outputs from the FreeVC-based model  [24] , focusing on phonemes that appear in both datasets under the same emotion. While earlier studies  [12, 15]  highlight the effectiveness of phoneme-level anchoring in cross-lingual SER, our objective is to verify whether these shared phonemes can serve as stable anchors in our framework, supporting emotion transfer between languages.\n\nTo analyze phoneme-level embeddings, we extract phonetic representations from the content encoder of the FreeVCbased model  [24] . We perform phoneme-centered segmentation by taking a fixed 120 ms window around the midpoint of each phoneme segment, following the approach in Upadhyay et al.  [12] . These embeddings are associated with specific emotions across both corpora, for example, z /a/,anger c represents the embedding for the phoneme /a/ in the context of anger. We identify phonemes that are shared and common across corpora within the same emotion (e.g., /a/ and /i/ under anger) and compute cosine similarity between their embeddings to assess alignment across languages using Equation  3 .\n\nThe similarity score sim provides a direct measure of phoneme-embedded commonality across corpora within the same emotional context. This similarity serves as a direct measure of phoneme-level alignment between corpora within the same emotional context.  Overall, our analyses across the speaker and phonetic spaces reveal key insights that support cross-lingual emotion transfer. The speaker-style graph shows that speakers from different languages often cluster together under the same emotion, indicating that expressive vocal traits transcend linguistic boundaries and can be grouped based on shared emotional articulation. Complementarily, the phoneme-level analysis shows that certain phonemes consistently encode emotional information across corpora, pointing to stable phonetic patterns that generalize across languages. These insights form the foundation of our dual-space anchoring approach, demonstrating that both speaker style and phoneme content offer transferable emotional cues essential for improving generalization in cross-lingual SER.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Speaker Style-Aware Phoneme Anchoring",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dual-Space Guided Phoneme Anchoring",
      "text": "We propose a dual-space framework that models each speech utterance using two encoders: a content encoder for phonetic structure and a speaker encoder for expressive speaker style relevant to emotion. Both are based on the FreeVC architecture  [24] , producing robust phoneme-and speakerlevel embeddings. This setup enables cross-corpus alignment in both spaces, guided by emotion-specific speaker communities. Phoneme-space alignment captures consistent emotional variability across corpora, while speaker-space alignment clusters speakers with similar expressive styles. Given a speech segment x, we extract two latent representations: a phonetic embedding z c = f c (x) ∈ R dc and a speaker embedding z s = f s (x) ∈ R ds , where f c (•) and f s (•) denote the content and speaker encoders. The content embedding captures phoneme identity, while the speaker embedding encodes prosody, pitch, and vocal dynamics relevant to the speaker's emotions. To prepare input segments, we use the phoneme-level forced alignments and extract 240 ms windows centered around each phoneme, following the same setup used in Section 2.3. These segments are passed through both encoders to obtain fine-grained phoneme and speaker embeddings. The resulting embeddings serve as inputs to our anchoring strategy, where we enforce emotion-consistent alignment in both phonetic and speaker-style spaces.\n\nTo guide learning, we define two triplet losses-one in each latent space. The phoneme-space loss L p encourages embeddings of the same phoneme, spoken with the same emotion by speakers in similar expressive-style clusters, to remain close:\n\nwhere z A c is the anchor (target phoneme embedding), z P c is a positive sample from the same phoneme-emotion cluster, and z N c is a negative drawn from a different emotion but the same phoneme class. The speaker-space triplet loss L s ensures that speakers exhibiting similar expressive behavior under emotion are closely grouped in the speaker latent space:\n\nHere, z A s and z P s belong to speakers in the same emotion-style cluster, while z N s is from a different cluster. Together, these losses promote cross-lingual alignment in both phoneme articulation and emotional speaking style. The margins α and β in the triplet losses define how much farther negative samples should be from the anchor compared to positive ones. We set α = 0.4 for the phoneme space and β = 0.6. These values are chosen based on validation performance on multiobjective loss balancing in emotion recognition.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Emotion Classification",
      "text": "For four category emotion classification, we concatenate the pooled phonetic and speaker embeddings:\n\nThe fused representation, obtained by combining the phonetic and speaker embeddings, is fed into our SER model architecture. For emotion classification, we adopt a transformer-based encoder followed by four fully connected layers, consistent with the architecture used in Upadhyay et al.  [12] . This model is designed to predict one of the four emotion categories using a softmax output layer. The training objective for SER is computed using the standard cross-entropy loss.\n\nwhere C is the number of emotion classes. The final training objective integrates the SER loss with the two anchoring losses:\n\nwhere λ 1 and λ 2 are the weighting coefficients for the phoneme-space and speaker-space losses, respectively. In our experiments, we set both values to λ 1 = 0.5 and λ 2 = 0.5.\n\nFigure  2  shows our proposed speaker style-aware phoneme anchoring architecture.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Settings",
      "text": "The training is optimized using the Adam optimizer in conjunction with stochastic gradient descent. A learning rate of 0.0001 and a decay factor of 0.001 are applied to support effective training. Models are trained for up to 70 epochs with a batch size of 64, and early stopping is employed to mitigate overfitting. The SER models are trained for multi-class classification across four primary emotional categories. Crossentropy loss and triplet loss are used as the cost functions, while unweighted average recall (UAR) serves as the evaluation metric. To assess the proposed approach, the corpora are split into predefined training, validation, and testing sets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison With Baselines",
      "text": "As baselines for comparison, we consider three established models that align with the core ideas of our proposed approach. The first is an ensemble learning method  [35] , which aggregates predictions from multiple diverse models to improve emotion recognition performance, particularly in crosslingual settings (denoted as Ensemble). The second is a fewshot learning approach  [7] , which enables rapid adaptation to target languages or domains using only a limited amount of labeled emotional data (Few-shot). Lastly, we include the phonetic-constraint based anchoring (PC) method  [12] , which leverages a shared phonetic representation space to guide emotion recognition across corpora. The cross-lingual SER performance results are presented in Table  4 . We denote <source corpus>→<target corpus> to indicate that the models are trained with the source corpus and tested with the target corpus. For both MSP-P→BIIC-P Table  4 : Cross-lingual SER performance in terms of UAR (%) across baseline models and ablation studies, averaged over 10 runs. Statistical significance is reported for comparisons between the baseline models and the proposed SAPA model, denoted by asterisks (* for p < 0.1, ** for p < 0.05).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "4-Category Cl-Ser",
      "text": "MSP-P→BIIC-P BIIC-P→MSP-P Ensemble  [35]  53.90** 52.86 ** Few-Shot  [7]  54.93** 53.35** PC  [12]  58.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Analysis",
      "text": "To evaluate the effectiveness of our proposed SAPA method for cross-lingual SER, we conduct a series of ablation studies. In the full SAPA setup, we use a fused representation of speaker and phonetic embeddings. To isolate the contribution of each component, we first experiment with models trained using only speaker embeddings (Only-S) and only phonetic features (Only-P). Furthermore, while SAPA anchors both the speaker and phoneme spaces jointly, we explore the impact of anchoring on a single space. Specifically, under the same fusion and speaker clustering framework, we compare variants where only the speaker space is anchored (SAPA-Only-S) and where only the phoneme space is anchored (SAPA-Only-P). These ablations help disentangle the role each representation plays in cross-lingual emotion recognition performance.\n\nThe ablation results for the proposed SAPA framework are shown in Table  4 . These experiments evaluate the contribution of individual components by selectively removing or isolating the speaker and phoneme spaces. Using only speaker embeddings (Only-S) results in the lowest performance, with 51.58% for MSP-P→BIIC-P and 49.93% for BIIC-P→MSP-P, highlighting that speaker information alone is insufficient for robust emotion recognition. In contrast, using only phoneme embeddings (Only-P) improves performance to 56.37% and 54.86%, indicating that phoneme structure carries more discriminative emotional cues across languages.\n\nFurther, we also analyze the impact of anchoring in individual spaces. SAPA-Only-S, which anchors only in the speaker space, achieves 57.46% and 55.33%, while SAPA-Only-P (anchoring only in the phoneme space) achieves 58.81% and 56.01% for the two tasks, respectively. These results show that anchoring in either space provides benefits for cross-lingual SER. However, the full SAPA model, which anchors both speaker and phoneme spaces, achieves the best performance with 59.25% and 56.54%. These findings support our central hypothesis that speaker-style aware phoneme anchoring enables more generalizable cross-lingual SER.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Speaker Group Transferability Analysis",
      "text": "To evaluate the effectiveness of the proposed SAPA model and specifically assess the impact of speaker-style-based grouping, we conduct a speaker group transferability analysis on the BIIC-P target corpus under the MSP-P→BIIC-P crosscorpus setting. For each emotion-specific speaker cluster graph, we aggregate all test utterances belonging to speakers within that cluster and compute the average emotion recognition accuracy over these samples.\n\nTo establish fair baselines, we compare our emotionspecific graph (w/ Emo) against two alternative grouping strategies. The first baseline uses an emotion-agnostic graph (w/o Emo), where speakers are clustered using a global graph built without considering emotion categories. The second baseline is a random grouping (Rand), where we randomly sample an equal number of utterances from speakers across the target corpus. This random sampling is repeated with multiple seeds, and the reported accuracy is averaged across runs for stability. As shown in Table  5 , the emotion-specific graph-based clustering (w/ Emo) consistently outperforms both the emotionagnostic clustering (w/o Emo) and the random baseline (Rand). Comparing w/ Emo to w/o Emo reveals that incorporating emotional context into speaker grouping leads to better emotion generalization, for example, in the case of anger, w/ Emo achieves 72.4% accuracy compared to 70.1% for w/o Emo. This suggests that clustering speakers without considering emotional categories limits the model's ability to align expressive characteristics.\n\nWhen comparing w/ Emo to the Rand baseline, the benefit becomes even more pronounced. Again, using anger as an example, w/ Emo outperforms Rand by +6.8%, indicating that randomly grouping speakers, even with equal-sized samples, fails to capture meaningful emotional structure. This pattern persists across other expressive emotions like happiness, where w/ Emo clusters enable stronger cross-corpus alignment. These results validate that both emotion-aware clustering and structured speaker grouping are essential for effective cross-lingual SER, particularly under speaker-styleaware phoneme anchoring.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "This work introduces the speaker-style aware phoneme anchoring (SAPA) framework for cross-lingual SER. Through analysis of speaker and phoneme spaces across two diverse language corpora, we find common emotion-related patterns in both speaker expressive styles and phonetic realizations. Leveraging this, SAPA employs a dual anchoring strategy to align phoneme and speaker embeddings under emotion-specific contexts. The resulting improvements in cross-lingual SER performance confirm the benefit of modeling both speaker style and phoneme-level cues. Future work will focus on expanding the emotion label set, incorporating greater speaker diversity, and exploring advanced adaptation techniques to further improve cross-lingual generalization. We also plan to validate the proposed framework on additional languages to assess its broader applicability.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , speakers with",
      "page": 2
    },
    {
      "caption": "Figure 1: shows that several clusters include speakers from",
      "page": 2
    },
    {
      "caption": "Figure 1: Speakers from the MSP-P and BIIC-P corpora are clustered using graph-based methods for each emotion category.",
      "page": 3
    },
    {
      "caption": "Figure 2: shows our proposed speaker style-aware phoneme",
      "page": 4
    },
    {
      "caption": "Figure 2: Proposed speaker-style aware phoneme anchoring (SAPA) architecture for cross-lingual SER.",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Cross-lingual SER performance in terms of UAR",
      "data": [
        {
          "MSP-P→BIIC-P": "53.90**\n54.93**\n58.14*",
          "BIIC-P→MSP-P": "52.86 **\n53.35**\n55.49*"
        },
        {
          "MSP-P→BIIC-P": "59.25",
          "BIIC-P→MSP-P": "56.54"
        },
        {
          "MSP-P→BIIC-P": "51.58\n56.37",
          "BIIC-P→MSP-P": "49.93\n54.86"
        },
        {
          "MSP-P→BIIC-P": "57.46\n58.8 1",
          "BIIC-P→MSP-P": "55.33\n56.01"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Behavioral signal processing: Deriving human behavioral informatics from speech and language",
      "authors": [
        "Shrikanth Narayanan",
        "Panayiotis G Georgiou"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "3",
      "title": "Using Emotion to Gain Rapport in a Spoken Dialog System",
      "authors": [
        "J Acosta"
      ],
      "year": "2009",
      "venue": "Using Emotion to Gain Rapport in a Spoken Dialog System"
    },
    {
      "citation_id": "4",
      "title": "Speech based emotion classification framework for driver assistance system",
      "authors": [
        "Ashish Tawari",
        "Mohan Trivedi"
      ],
      "year": "2010",
      "venue": "2010 IEEE Intelligent Vehicles Symposium"
    },
    {
      "citation_id": "5",
      "title": "Real-life emotion-related states detection in call centers: A crosscorpora study",
      "authors": [
        "L Devillers",
        "C Vaudable",
        "C Chastagnol"
      ],
      "venue": "Interspeech 2010, Makuhari, Japan, September 2010"
    },
    {
      "citation_id": "6",
      "title": "Formulating emotion perception as a probabilistic model with application to categorical emotion classification",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "7",
      "title": "Semi-supervised speech emotion recognition with ladder networks",
      "authors": [
        "Srinivas Parthasarathy",
        "Carlos Busso"
      ],
      "year": "2020",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "8",
      "title": "Crosscorpus speech emotion recognition based on few-shot learning and domain adaptation",
      "authors": [
        "Youngdo Ahn",
        "Sung Lee",
        "Jong Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "9",
      "title": "Zero-shot speech emotion recognition using generative learning with reconstructed prototypes",
      "authors": [
        "Xinzhou Xu",
        "Jun Deng",
        "Zixing Zhang",
        "Zhen Yang",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (ADDoG)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "A conditional cycle emotion gan for cross corpus speech emotion recognition",
      "authors": [
        "Bo-Hao Su",
        "Chi-Chun Lee"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "13",
      "title": "Phonetic anchor-based transfer learning to facilitate unsupervised cross-lingual speech emotion recognition",
      "authors": [
        "S Upadhyay",
        "L Martinez-Lucas",
        "B.-H Su",
        "W.-C Lin",
        "W.-S Chien",
        "Y.-T Wu",
        "W Katz",
        "C Busso",
        "C.-C Lee"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "A layer-anchoring strategy for enhancing cross-lingual speech emotion recognition",
      "authors": [
        "S Upadhyay",
        "C Busso",
        "C.-C Lee"
      ],
      "year": "2024",
      "venue": "Interspeech 2024, Kos Island"
    },
    {
      "citation_id": "15",
      "title": "Mouth articulation-based anchoring for improved crosscorpus speech emotion recognition",
      "authors": [
        "S Upadhyay",
        "A Salman",
        "C Busso",
        "C.-C Lee"
      ],
      "year": "2025",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2025)"
    },
    {
      "citation_id": "16",
      "title": "Phonetically-anchored domain adaptation for crosslingual speech emotion recognition",
      "authors": [
        "S Upadhyay",
        "L Martinez-Lucas",
        "W Katz",
        "C Busso",
        "C.-C Lee"
      ],
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "WavLM: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao",
        "J Wu",
        "L Zhou",
        "S Ren",
        "Y Qian",
        "Y Qian",
        "J Wu",
        "M Zeng",
        "X Yu",
        "F Wei"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "Y.-H Tsai",
        "B Bolte",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Prosody in context: A review",
      "authors": [
        "Jennifer Cole"
      ],
      "year": "2015",
      "venue": "Cognition and Neuroscience"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition based on phoneme classes",
      "authors": [
        "C Lee",
        "S Yildirim",
        "M Bulut",
        "A Kazemzadeh",
        "C Busso",
        "Z Deng",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2004",
      "venue": "8th International Conference on Spoken Language Processing (ICSLP 04)"
    },
    {
      "citation_id": "21",
      "title": "Modeling phonetic pattern variability in favor of the creation of robust emotion classifiers for real-life applications",
      "authors": [
        "Bogdan Vlasenko",
        "Dmytro Prylipko",
        "Ronald Böck",
        "Andreas Wendemuth"
      ],
      "year": "2014",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "22",
      "title": "Cultural perspectives on the linguistic representation of emotion and emotion events",
      "authors": [
        "Carien Gün R Semin",
        "Sharda Görts",
        "Astrid Nandram",
        "Semin-Goossens"
      ],
      "year": "2002",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "23",
      "title": "Investigation of speaker group-dependent modelling for recognition of affective states from speech",
      "authors": [
        "Ingo Siegert",
        "David Philippou-Hübner",
        "Kim Hartmann",
        "Ronald Böck",
        "Andreas Wendemuth"
      ],
      "year": "2014",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "24",
      "title": "Phonemes convey embodied emotion",
      "authors": [
        "Christine Sp Yu",
        "Michael Mcbeath",
        "Arthur Glenberg"
      ],
      "year": "2021",
      "venue": "Handbook of Embodied Psychology"
    },
    {
      "citation_id": "25",
      "title": "FreeVC: Towards highquality text-free one-shot voice conversion",
      "authors": [
        "Jingyi Li",
        "Weiping Tu",
        "Li Xiao"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "VQMIVC: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion",
      "authors": [
        "Disong Wang",
        "Liqun Deng",
        "Yu Yeung",
        "Xiao Chen",
        "Xunying Liu",
        "Helen Meng"
      ],
      "year": "2021",
      "venue": "VQMIVC: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion",
      "arxiv": "arXiv:2106.10132"
    },
    {
      "citation_id": "27",
      "title": "Disentangling correlated speaker and noise for speech synthesis via data augmentation and adversarial factorization",
      "authors": [
        "Wei-Ning Hsu",
        "Yu Zhang",
        "Ron Weiss",
        "Yu-An Chung",
        "Yuxuan Wang",
        "Yonghui Wu",
        "James Glass"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "EmoTalk: Speech-driven emotional disentanglement for 3d face animation",
      "authors": [
        "Ziqiao Peng",
        "Haoyu Wu",
        "Zhenbo Song",
        "Hao Xu",
        "Xiangyu Zhu",
        "Jun He",
        "Hongyan Liu",
        "Zhaoxin Fan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "29",
      "title": "Revealing emotional clusters in speaker embeddings: A contrastive learning strategy for speech emotion recognition",
      "authors": [
        "I Ülgen",
        "Z Du",
        "C Busso",
        "B Sisman"
      ],
      "year": "2024",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024)"
    },
    {
      "citation_id": "30",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "An intelligent infrastructure toward large scale naturalistic affective speech corpora collection",
      "authors": [
        "S Upadhyay",
        "W.-S Chien",
        "B.-H Su",
        "L Goncalves",
        "Y.-T Wu",
        "A Salman",
        "C Busso",
        "C.-C Lee"
      ],
      "year": "2023",
      "venue": "International Conference on Affective Computing and Intelligent Interaction (ACII 2023)"
    },
    {
      "citation_id": "32",
      "title": "Speech emotion recognition via graph-based representations",
      "authors": [
        "Anastasia Pentari",
        "George Kafentzis",
        "Manolis Tsiknakis"
      ],
      "year": "2024",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "33",
      "title": "Speech emotion recognition based on graph-lstm neural network",
      "authors": [
        "Yan Li",
        "Yapeng Wang",
        "Xu Yang",
        "Sio-Kei Im"
      ],
      "year": "2023",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "34",
      "title": "Compact graph architecture for speech emotion recognition",
      "authors": [
        "Amir Shirian",
        "Tanaya Guha"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "35",
      "title": "I-Louvain: An attributed graph clustering method",
      "authors": [
        "David Combe",
        "Christine Largeron",
        "Mathias Géry",
        "Előd Egyed-Zsigmond"
      ],
      "year": "2015",
      "venue": "Advances in Intelligent Data Analysis XIV: 14th International Symposium"
    },
    {
      "citation_id": "36",
      "title": "Cross corpus multilingual speech emotion recognition using ensemble learning",
      "authors": [
        "Wisha Zehra",
        "Abdul Javed",
        "Zunera Jalil",
        "Thippa Habib Ullah Khan",
        "Gadekallu"
      ],
      "year": "2021",
      "venue": "Complex & Intelligent Systems"
    }
  ]
}