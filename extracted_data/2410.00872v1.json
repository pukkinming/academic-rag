{
  "paper_id": "2410.00872v1",
  "title": "Do Music Generation Models Encode Music Theory?",
  "published": "2024-10-01T17:06:30Z",
  "authors": [
    "Megan Wei",
    "Michael Freeman",
    "Chris Donahue",
    "Chen Sun"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Music foundation models possess impressive music generation capabilities. When people compose music, they may infuse their understanding of music into their work, by using notes and intervals to craft melodies, chords to build progressions, and tempo to create a rhythmic feel. To what extent is this true of music generation models? More specifically, are fundamental Western music theory concepts observable within the \"inner workings\" of these models? Recent work proposed leveraging latent audio representations from music generation models towards music information retrieval tasks (e.g. genre classification, emotion recognition), which suggests that high-level musical characteristics are encoded within these models. However, probing individual music theory concepts (e.g. tempo, pitch class, chord quality) remains under-explored. Thus, we introduce SynTheory, a synthetic MIDI and audio music theory dataset, consisting of tempos, time signatures, notes, intervals, scales, chords, and chord progressions concepts. We then propose a framework to probe for these music theory concepts in music foundation models (Jukebox and MusicGen) and assess how strongly they encode these concepts within their internal representations. Our findings suggest that music theory concepts are discernible within foundation models and that the degree to which they are detectable varies by model size and layer. *: Equal contribution.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "State-of-the-art text-to-music generative models  [1] [2] [3]  exhibit impressive generative capabilities. Past work suggests that internal representations of audio extracted from music generative models encode information relating to high-level concepts (e.g. genre, instruments, or emotion)  [4] [5] [6] [7] . However, it remains unclear if they also capture underlying symbolic music concepts (e.g. tempo or chord progressions)  [8] .\n\nWe aim to investigate whether and to what extent stateof-the-art music generation models encode music theory concepts in their internal representations. Confirming this could enable the creative alteration of these concepts, providing artists with new methods towards more detailed and lower-level control  [9]  (e.g. changing the key of a song or editing a particular chord in a chord progression). Furthermore, by benchmarking these foundation models, we identify potential avenues for improvement towards stronger concept encoding. Our approach builds upon work in probing and editing concepts in language models, which have shown promise in identifying emergent representations in autoregressive models and editing factual knowledge  [9] [10] [11] [12] . While previous work probed music generation models for high-level concepts, such as emotion, genre, and tagging  [4] [5] [6] [7] , we focus on uncovering representations of more granular music theory concepts. Although existing datasets such as HookTheory  [13]  contain rich music theory annotations, their association with copyrighted music potentially complicates their use.\n\nOur first contribution is a framework that generates diagnostic datasets for probing music theory concepts in music generation models, allowing programmatic control over concept selection while mitigating the presence of potential distractors. Our synthetic music theory dataset, Syn-Theory, consists of seven music concepts based on Western music theory: tempo, time signatures, notes, intervals, scales, chords, and chord progressions. SynTheory serves as a customizable, copyright-free, and scalable approach towards generating diagnostic music clips for probing music generative models trained on real-world data.\n\nOur second contribution is the analysis of two state-ofthe-art music generative models Jukebox  [3]  and Music-Gen  [1]  with our SynTheory benchmark. We extract representations for the concepts defined in SynTheory from Mu-sicGen and Jukebox and assess whether these models encode meaningful representations of these concepts. To analyze the internal representations of these models, we train probing classifiers  [14]  based on ground truth music theory concept labels. A higher classification accuracy implies that these models learn internal representations that \"understand\" music theory concepts, which can be decoded by a multi-layer perceptron (MLP) or a linear model.\n\nOur results show that music foundation models encode meaningful representations of music theory concepts. These representations vary across different sections of the model (audio codecs, decoder LMs), different layers within the decoder LMs, and different model sizes. Furthermore, the nature of the concepts, from time-varying (e.g. chord progressions) to stationary (e.g. notes, chords), influence the performance of these models across these Figure  1 . Overview of our SynTheory benchmark and our Jukebox and MusicGen probing setup. Our SynTheory benchmark consists of Rhythmic (tempos and time signatures) and Tonal (notes, intervals, scales, chords, and chord progressions) concepts. We assess whether music foundation models (Jukebox and MusicGen) encode these music theory concepts within their internal representations. For each task from the SynTheory dataset, we extract representations from the music foundation model. We pass an audio input, embodying the concept (e.g. Perfect 4th), into a pretrained foundation model. The audio codec tokenizes the audio into discrete audio tokens. Then, it passes these tokens into a decoder language model. From there, we extract the representations. We then train a probe classifier (linear or two-layer MLP) on these representations to predict the corresponding class (e.g. pitch class, intervals, or chords) for each SynTheory concept.\n\ntasks. We hope our insights on probing music foundation models, along with the synthetic music data generation framework, encourage and facilitate future endeavors on symbolic controllability in music generative models.\n\nFor reproducibility, we upload our SynTheory dataset to Hugging Face 1 , release the code for dataset generation, embedding extraction, probing, and evaluation on GitHub 2 , and showcase audio samples on our website 3 .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "The success of large language models (LLMs)  [15] [16] [17] [18]  has sparked new research on probing their internal representations to measure their understanding of linguistic concepts  [19, 20]  and world knowledge  [11, 12, 21]  as well as editing the encoded knowledge to make LLMs more faithful to factual knowledge  [9, 10] . Studies have shown that LLMs can encode grounded representations on color  [22] , direction  [23] , and auditory representations  [24] . Thus, we investigate if music generative models, which often share similar model architectures and training objectives as LLMs, are able to encode abstract concepts from highlevel music tags (e.g. genre, emotion) to fine-grained music theory knowledge (e.g. tempo, chords).\n\nRecent work has shown promise towards uncovering conceptual representations from probing audio and music generative models and leveraging different music foundation model architectures towards music understanding tasks. Castellon and Donahue et al.  [4]  propose using representations from language models trained on codified audio towards downstream MIR tasks as a better alternative to conventional tagging models. The authors train probing classifiers on Jukebox representations on music tagging, genre identification, key identification, and emotion recognition tasks. These results demonstrate the effec-1 https://huggingface.co/datasets/meganwei/ syntheory 2 https://github.com/brown-palm/syntheory 3 https://brown-palm.github.io/music-theory tiveness of using internal model representations for downstream MIR tasks. Koo et al.  [7]  focus primarily on probing MusicGen's attention heads in instrument recognition tasks, benchmarking against the tasks highlighted in  [4] , and propose leveraging these representations for inferencetime control. Other works  [5, 6]  assess the impact of model architectures and self-supervised approaches towards music understanding tasks. However, prior work primarily uses real-world data, which is often concept-entangled and potentially subject to copyright concerns. For example, some of these works use Giantsteps-MTG and Giantsteps  [25] , which are datasets of primarily electronic dance music with tempo and key annotations, obtained from Beatport. Won et al.  [5]  use HookTheory for chord recognition, where they focus on major and minor chord identification for each pitch class. The authors also use Harmonix Set  [26]  and GTZAN  [27]  for beat and downbeat detection. In the language modality, the authors of ChatMusician  [28]  produce a multi-choice question answering dataset, MusicTheoryBench, with expert annotation from a professional college music teacher. MusicTheoryBench aims to assess the music understanding capabilities of LLMs through natural language alone. To the best of our knowledge, there is a lack of music theory probing benchmarks in the audio domain that are labeled in detail, copyright-free, and scalable, prior to our proposed SynTheory.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Syntheory: Synthetic Dataset Of Music Theory Concepts",
      "text": "We design seven datasets to capture isolated music theory concepts -similar to synthetic audio for ear training. Musicians may \"train their ear\" to recognize music concepts like intervals or chord quality in an isolated setting before advancing to the harder, more entangled case that arises in non-pedagogical music. Assessing concept recognition through isolated concepts mitigates the possibility that one intuits or guesses the answer from its context. Literature on instrument-specific absolute pitch in humans corroborates the notion that timbral information may be exploited in identifying a different concept like pitch class  [29] . As such, our dataset is designed to remove or reduce features that may correlate with a concept, but are not strictly necessary for identifying it. Our intent is a more pointed assessment towards theoretical concepts as abstract ideas rather than as acoustically realized audio. A more practical motivation for this work is that extracting such low-level, isolated concepts from existing datasets may require nontrivial engineering or domain expert labor. It may even be impossible to disentangle all overlapping concepts. Music stem isolation and concept isolation are distinct; an isolated instrument in a multi-track recording may still exhibit several, intricately intertwined theory concepts. It is not clear how to \"unmix\" such concepts once they are blended.\n\nInstead of attempting to disentangle several concepts from existing audio, SynTheory implements this \"ear training\" quiz setting by explicitly producing individual concepts. Each of the seven datasets ablates a single musical feature while fixing all others, thereby isolating it to a degree not typically found in recorded music. These ablated concepts consist of tempo, time signatures, notes, intervals, scales, chords, and chord progressions. We adopt isolation as a design choice to mitigate context that may be exploited in deep learning models as \"shortcuts\", i.e. heuristics that correlate with concepts most of the time but do not truly encode the concept.\n\nUsing this music theory concept-driven synthesis design, we construct label-balanced and copyright-free data. The synthetic approach avoids annotation errors present in other contemporary MIR datasets. For example, the HookTheory data processing step for SheetSage  [13]  required ad-hoc time-alignment of the expert annotations. In the released SheetSage dataset, 17, 980/26, 175 (68.7%) samples required more precise time alignment. While our synthetic data is no substitute for real music data, to our knowledge, no other dataset so strictly isolates each concept to this level of granularity.\n\nSynTheory contains two categories: tonal and rhythmic. We make this distinction for stronger concept isolation; we wish to keep the rhythm samples tonally consistent and the tonal samples rhythmically consistent. For each tonal dataset, we voice the same MIDI data through 92 distinct instruments. The selection of instrument voices is fixed, making the distribution of timbres sufficiently diverse but also class-balanced. Each instrument corresponds to one of the 128 canonical MIDI program codes and is voiced through the TimGM6mb.sf2  [30]  soundfont. A MIDI \"program\" is a specific instrument preset. The canonical program set includes many instruments, e.g. \"Acoustic Grand Piano\", \"Flute\", etc. We exclude programs that are polyphonic, sound effects (e.g. \"Bird Tweet\", \"Gun Shot\"), and highly articulate. A highly articulate program has some unchangeable characteristic (e.g. pitch bending) that destabilizes its pitch. For each rhythmic dataset, we define five metronome-like timbral settings. Each setting uses one of the distinct instruments: \"Woodblock Light\",",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Concept",
      "text": "Total We present an overview of these datasets and their sizes.\n\n\"Woodblock Dark\", \"Taiko\", \"Synth Drum\", and the MIDI drum-kit, following the voicing done in Sheetsage  [13] .\n\nEach setting produces a distinct sound on the upbeat and the downbeats, which defines the time signature concept. One can extend or alter these configurations using the SynTheory codebase. We provide a framework that enables declarative and programmatic MIDI construction in musical semantics, audio export in any soundfont, and dataset construction for use in our framework.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Syntheory-Rhythmic",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Tempo",
      "text": "We voice integer tempi from 50 to 210 BPM (beats per minute) inclusive in 4 4 time. To ensure diverse start times, we produce five random offset times per sample. There are (5 CLICK SETTING • 161 TEMPO • 5 OFFSET ) = 4, 025 samples in total.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Time Signature",
      "text": "We voice the following time signatures: 2 2 , 4 2 , 4 3 , 8 3 , 4 4  , 8 6 , 8 9 , and 8\n\n12 at a fixed tempo of 120 BPM. To add acoustic variation, we add three levels of reverb from completely dry to spacious. We find empirically that this acoustic perturbation increases the difficulty of the probing task. Like the Tempo dataset, we produce ten random offset times per sample. There are (8",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Syntheory-Tonal",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Notes",
      "text": "We voice all twelve Western temperament pitch classes, in nine octaves, using 92 instruments. The note is played in quarter notes at a tempo of 120 BPM, with no distinction between the upbeat or downbeat. There are (12 PITCH CLASS • 9 OCTAVE • 92 INSTRUMENT ) = 9, 936 configurations. However, there are only 9, 848 distinct samples because 88 configurations at extreme registers are unvoiceable in our soundfont. These silent samples are listed for completeness in our Hugging Face dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Intervals",
      "text": "We vary the root note, number of half-steps, instrument, and play style (unison, up, and down). To retain consistent rhythm, the up and down styles repeat four times throughout the sample while the unison play style repeats eight times. There are (12 PITCH CLASS •12 HALF-STEP • 92 INSTRUMENT • 3 PLAY STYLE ) = 39, 744 samples.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Scales",
      "text": "We voice seven Western music modes (Ionian, Dorian, Phrygian, Lydian, Mixolydian, Aeolian, and Locrian) in all root notes, in 92 instruments, and in two play styles (ascending or descending). The register is constant; we select root notes close to middle C. There are",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Chords",
      "text": "We voice triads of all twelve root notes, four chord qualities (major, minor, augmented, and diminished), 92 instruments, and three inversions (root position, first inversion, and second inversion). The chord is struck at each quarter note at 120 BPM. Like the Scales dataset, we fix the register close to middle C. There are  (12",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Chord Progressions",
      "text": "We select 19 four-chord progressions, with ten in the major mode and nine in the natural minor mode:\n\n• Major:\n\n(I-IV-V-I), (I-IV-vi-V), (I-V-vi-IV), (I-vi-IV-V), (ii-V-I-vi), (IV-I-V-vi), (IV-V-iii-vi), (V-IV-I-V), (V-vi-IV-I), (vi-IV-I-V)\n\n• Natural Minor: (i-ii • -v-i), (i-III-iv-i), (i-iv-v-i),",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "(I-Vi-Iii-Vii), (I-Vi-Vii-I), (I-Vi-Vii-Iii), (I-Vii-Vi-Iv), (Iv-Vii-I-I), (Vii-Vi-Vii-I)",
      "text": "We vary the root note of the key and instrument. Each chord is played at each quarter notes at 120 BPM. There are (19 PROGRESSION • 12 KEY ROOT • 92 INSTRUMENT ) = 20, 976 samples.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "We describe our approach to assess how well the internal representations of music generative models (MusicGen and Jukebox) and handcrafted audio features (mel spectrograms, MFCC, and chroma) encode music theory concepts.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation",
      "text": "A \"probe\" is a simple classifier, often a linear model, trained on the activations of a neural network  [14] . Accurate performance of such classifiers suggests that information relevant to the class exists in the latent representations within the network. As such, probes may be used as a proxy for measuring a model's \"understanding\" or encoding of abstract concepts. Motivated by the use of probes to discover linguistic structure and semantics in NLP  [31]  and more recently in MIR  [4] , we use probes to assess whether music theory concepts are discernable in the internal representations of foundation models.\n\nWe adopt the same probing paradigm as  [4]  and frame concept understanding as multiclass classification for discrete concepts (notes, intervals, scales, chords, chord progressions, and time signatures) and regression for continuous concepts (tempo). We train linear and two-layer MLP probes on the embeddings of the internal representations of Jukebox and MusicGen and the handcrafted features.\n\nEach probe is trained independently for its corresponding concept task. That is, the probe trained to identify notes from Jukebox embeddings will not be used to identify intervals, for example.\n\nFor the classification tasks, we measure the accuracy of our trained probes on the following SynTheory tasks:\n\n• Notes (  12\n\n(i-VI-VII-III), (i-VII-VI-IV), (iv-VII-i-i), and (VII-vi-VII-i)\n\n• Time Signatures (8): 2 2 , 4 2 , 4 3 , 8 3 , 4 4 , 8 6 , 8 9 , and 8 12\n\nThese tasks are trained on a 70% train, 15% test, and 15% validation split, using the Adam optimizer and Cross Entropy loss.\n\nFor the Tempo dataset, we train a regression probe, over the 161 tempo values. To increase complexity in the probing task and test generalization to unseen BPMs, the training set consists of the middle 70% of the BPMs. The test and validation sets consist of the top 15% BPMs and the bottom 15% BPMs, randomly shuffled and split in half. We use MSE loss and report the R 2 score.\n\nTo select the best performing probe for each concept, we perform a grid search across various hyperparameters for the MusicGen audio codec, mel spectrogram, MFCC, chroma, and aggregate handcrafted features, following  [4] :\n\n• Data Normalization: {True, False} • Model Type: {Linear, two-layer MLP with 512 hidden units and ReLU activation} • Batch Size: {64, 256} • Learning Rate: {10 -5 , 10 -4 , 10 -3 } • Dropout: {0.25, 0.5, 0.75} • L2 Weight Decay: {off, 10 -4 , 10 -3 } For the decoder language models (MusicGen small, medium, and large and Jukebox), we use a fixed set of hyperparameters and select the probe with the best performing layer for each concept, in the interest of computational efficiency:\n\n• Data Normalization: True • Model Type: two-layer MLP with 512 hidden units and ReLU activation • Batch Size: 64 • Learning Rate: 10 -3 • Dropout: 0.5 • L2 Weight Decay: off We selected these hyperparameters from the best overall performing probe by fixing a layer in the decoder LMs and performing a hyperparameter search, following the sweep approach outlined in  [4] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Representations",
      "text": "We extract representations from two text-to-music foundation models, Jukebox  [3]  and MusicGen  [1]  and benchmark them against three handcrafted, spectral features following  [4] : mel spectrograms, mel-frequency cepstral coefficients (MFCC), and constant-Q chromagrams (chroma). These handcrafted features are common in traditional methods of MIR and are a more interpretable baseline against the embeddings of the pre-trained music generative models.\n\nJukebox consists of a VQ-VAE model that codifies audio waveforms into discrete codes at a lower sample rate and a language model that generates codified audio with a transformer decoder. We trim each audio sample to four seconds and convert it to mono. Using Jukemirlib  [4] , we pass the audio through the frozen audio encoder and through the decoder language model. Due to resource constraints, we downsample the activation to a target rate of half that in  [4] , using the Librosa FFT algorithm  [32] . Then, we meanpool the representations across time to reduce the dimensionality of the embeddings, resulting a dimension of (72, 4800) per sample, where 72 is the number of layers and 4800 is the dimension of the activations. To further reduce the dimensions, we adopt a layer selection process similar to  [4] . We train the probe classifiers with fixed hyperparameters on the music concept tasks as described in Section 4.1. For each concept, we select the layer that results in the highest probing score. This results in a final dimension of 4800 for each Jukebox representation.\n\nMusicGen consists of a pretrained convolutional autoencoder (EnCodec)  [33] , a pretrained T5 text encoder, and an acoustic transformer decoder. We resample the audio to 32 kHz (the sampling rate used in the EnCodec model) trim to four seconds, convert to mono, and pass the audio through the frozen EnCodec audio codec. We do not pass text through the text encoder, as we focus on audio representations. We then extract representations from several regions of the model: the final layer of the audio codec before residual vector quantization and the hidden states of the decoder language model. The number of decoder hidden states vary based on the model size: small (24 layers), medium (48 layers), and large (48 layers).\n\nFor our four second audio clips, the audio codec representations are of dimension  (128, 200) , where 128 is the dimension of the activation after the final layer of the au-Figure  2 . Probing evaluation metrics averaged across all SynTheory concepts over the model layers of Jukebox and MusicGen decoder models. The probing evaluation metric is R 2 for tempos and accuracy for the rest of the SynTheory concepts (notes, intervals, scales, chords, chord progressions, and time signatures). Features extracted from deeper layers generally perform better, with a slight dropoff near the final layers. dio codec and 200 is the sequence length. We meanpool the values of the representations across time, resulting in a final dimension of 128 for the MusicGen audio codec.\n\nThe decoder hidden states for the small, medium, and large MusicGen models have dimensions  (24, 200, 1024) ,  (48, 200, 1536) , and (48, 200, 2048) respectively, where the first axis corresponds to the number of layers, second corresponds to sequence length, and third corresponds to hidden size. To reduce the dimensions, we meanpool across time, resulting in dimensions of (24, 1024) for Mu-sicGen small decoder, (48, 1536) for MusicGen medium decoder, and (48, 2048) for MusicGen large decoder. Similar to our approach with Jukebox, we select the most optimal layer for each decoder model size based on the probing scores. After selecting the best performing layer per concept and model size, the dimensions of the representations are 1024 for MusicGen small, 1536 for MusicGen medium, and 2048 for MusicGen large.\n\nWe visualize results from probing across layers per model (MusicGen and Jukebox) averaged across concepts in Figure  2 .\n\nWe extract the handcrafted features (mel spectrograms, mel-frequency cepstral coefficients, and constant-Q chromagrams) with librosa  [32] . Similar to  [4] , we concatenate the mean and standard deviation across time of these features along with their first-and second-order discrete differences. To obtain an aggregate representation of the handcrafted features, we concatenate the mel spectrogram, chroma, and MFCC features and obtain their mean and standard deviation across time and their first-and secondorder differences.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Discussion",
      "text": "Overall, we observe that music generation models do encode music theory concepts in their internal representa- Decoder LM (Small, Medium, and Large) and Jukebox models, we report the metrics of the best performing probe for each task using layer selection. We also report an average performance across all concepts for each model/feature.\n\ntions. Jukebox performs consistently well across all Syn-Theory tasks. MusicGen Decoder LMs also exhibit competitive performance across our benchmark. Interestingly, we observe that MusicGen Decoder (Small) outperforms its larger counterparts. This finding contradicts traditional discussions on scaling laws and the claims in  [1]  that larger MusicGen models better \"understand\" text prompts and produce better quantitative and subjective scores. Figure  2  further highlights this finding, showing that MusicGen Decoder (Small) maintains consistently high probing scores across all layers compared to that of its larger counterparts. Meanwhile, larger MusicGen models display a steep performance drop in the initial layers, followed by a gradual increase and then a slight decline in the final layers. This pattern suggests that the smaller model may have developed a more efficient encoding of music theory concepts within its representations. While these models perform well on other concepts, MusicGen slightly underperforms on the notes dataset. We hypothesize this may be due to isolated notes being less prominent as intervals, scales, and chords in real-world music. This reveals the challenge in distinguishing the most fundamental building blocks of music.\n\nPretrained music decoder LMs generally outperform MusicGen Audio Codec representations and individual handcrafted features. MusicGen Audio Codec exhibits poorer overall performance, since these codecs were trained to reconstruct fine-grained, low-level details localized by time.\n\nChroma features, which encode pitch class information, perform comparably well on tonal tasks, but slightly underperform on rhythmic tasks. Notably, chroma features outperform MusicGen Decoder LMs on stationary harmonic tasks (notes, scales, and chords) but lag behind on dynamic harmonic tasks (chord progressions and intervals).\n\nThe aggregate handcrafted features perform comparably to MusicGen Decoder LMs. This suggests that harder music concept understanding benchmarks should address concepts latent in foundation models but not easily encoded in handcrafted features. These harder benchmarks may include entangled concepts, such as probing for both chord progression type and tempo in a chord progression samples of varying tempos. Probing for compositional tasks could reveal differences in concept encoding between model representations and handcrafted features.\n\nWhile these models have shown that they encode representations for music theory in audio, some state-of-theart music foundation models primarily rely on text controls. In a future study, we plan to probe for language representations in music generation models to improve textcontrollable methods.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "We introduce SynTheory, a synthetic dataset of music theory concepts, that is concept-isolated, copyright-free, and scalable. We use SynTheory to evaluate the degree to which music theory concepts are encoded in state-of-theart music generative models. Our experiments suggest that music theory concepts are indeed discernible within the latent representations of these models. We believe this will allow us to understand how to isolate and manipulate such concepts, which advances towards controllable generation and music theory benchmarks. We encourage the community to build more challenging probing datasets with our framework to further understand the relationship between symbolic and audio-based music generation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ethics Statement",
      "text": "Our work aims to understand if music generation models encode music theory concepts in their internal representations. Our dataset may be used to assess music generation models and may be applied towards fine-grained, musictheory based controllable generation.\n\nOur custom dataset, SynTheory, is based on elementary Western music theory concepts and is generated programmatically. The data does not infringe copyright of musical writers or performers.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of our SynTheory benchmark and our Jukebox and MusicGen probing setup. Our SynTheory bench-",
      "page": 2
    },
    {
      "caption": "Figure 2: Probing evaluation metrics averaged across all",
      "page": 5
    },
    {
      "caption": "Figure 2: We extract the handcrafted features (mel spectrograms,",
      "page": 5
    },
    {
      "caption": "Figure 2: further highlights this finding, showing that MusicGen De-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "ABSTRACT\ncould enable the creative alteration of these concepts, pro-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "viding artists with new methods towards more detailed and"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "Music foundation models possess impressive music gen-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "lower-level control [9] (e.g. changing the key of a song or"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "eration capabilities. When people compose music,\nthey"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "editing a particular chord in a chord progression). Further-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "may infuse their understanding of music into their work,"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "more, by benchmarking these foundation models, we iden-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "by using notes and intervals to craft melodies, chords to"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "tify potential avenues\nfor\nimprovement\ntowards\nstronger"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "build progressions, and tempo to create a rhythmic feel. To"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "concept\nencoding.\nOur\napproach builds upon work in"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "what extent is this true of music generation models? More"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "probing and editing concepts in language models, which"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "specifically, are fundamental Western music theory con-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "have shown promise in identifying emergent\nrepresenta-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "cepts observable within the “inner workings” of these mod-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "tions in autoregressive models and editing factual knowl-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "els? Recent work proposed leveraging latent audio repre-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "edge\n[9–12]. While previous work probed music gen-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "sentations\nfrom music generation models\ntowards music"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "eration models for high-level concepts, such as emotion,"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "information retrieval\ntasks (e.g. genre classification, emo-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "genre, and tagging [4–7], we focus on uncovering repre-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "tion recognition), which suggests that high-level musical"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "sentations of more granular music theory concepts.\nAl-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "characteristics are encoded within these models. However,"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "though existing datasets such as HookTheory [13] contain"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "probing individual music\ntheory concepts\n(e.g.\ntempo,"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "rich music theory annotations, their association with copy-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "pitch class, chord quality) remains under-explored. Thus,"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "righted music potentially complicates their use."
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "we introduce SynTheory, a synthetic MIDI and audio mu-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "Our first contribution is a framework that generates di-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "sic theory dataset, consisting of\ntempos,\ntime signatures,"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "agnostic datasets for probing music theory concepts in mu-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "notes,\nintervals,\nscales,\nchords,\nand chord progressions"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "sic generation models, allowing programmatic control over"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "concepts. We then propose a framework to probe for these"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "concept selection while mitigating the presence of poten-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "music theory concepts in music foundation models (Juke-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "tial distractors. Our synthetic music theory dataset, Syn-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "box and MusicGen) and assess how strongly they encode"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "Theory, consists of seven music concepts based on West-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "these concepts within their\ninternal\nrepresentations. Our"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "ern music theory:\ntempo, time signatures, notes, intervals,"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "findings suggest that music theory concepts are discernible"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "scales, chords, and chord progressions. SynTheory serves"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "within foundation models and that the degree to which they"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "as a customizable, copyright-free, and scalable approach"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "are detectable varies by model size and layer."
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "towards generating diagnostic music clips for probing mu-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "sic generative models trained on real-world data."
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "1.\nINTRODUCTION"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "Our second contribution is the analysis of two state-of-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "State-of-the-art\ntext-to-music generative models [1–3] ex-\nthe-art music generative models Jukebox [3] and Music-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "hibit\nimpressive generative capabilities.\nPast work sug-\nGen [1] with our SynTheory benchmark. We extract repre-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "gests that\ninternal representations of audio extracted from\nsentations for the concepts defined in SynTheory from Mu-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "music generative models encode information relating to\nsicGen and Jukebox and assess whether these models en-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "high-level\nconcepts\n(e.g.\ngenre,\ninstruments,\nor\nemo-\ncode meaningful representations of these concepts. To an-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "tion) [4–7]. However,\nit remains unclear if they also cap-\nalyze the internal representations of these models, we train"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "ture underlying symbolic music concepts (e.g.\ntempo or\nprobing classifiers [14] based on ground truth music theory"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "chord progressions) [8].\nconcept\nlabels. A higher classification accuracy implies"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "We aim to investigate whether and to what extent state-\nthat\nthese models learn internal\nrepresentations that “un-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "of-the-art music generation models encode music theory\nderstand” music theory concepts, which can be decoded"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "concepts in their internal representations. Confirming this\nby a multi-layer perceptron (MLP) or a linear model."
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "Our\nresults\nshow that music\nfoundation models\nen-\n*: Equal contribution."
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "code meaningful representations of music theory concepts."
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "These\nrepresentations\nvary\nacross\ndifferent\nsections\nof"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "© M. Wei, M. Freeman, C. Donahue, and C. Sun. Licensed"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "the model\n(audio codecs, decoder LMs), different\nlayers"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "under a Creative Commons Attribution 4.0 International License (CC BY"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "within the decoder LMs, and different model sizes.\nFur-"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "4.0). Attribution: M. Wei, M. Freeman, C. Donahue, and C. Sun, “Do"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "thermore,\nthe nature of\nthe concepts,\nfrom time-varying"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "music generation models encode music theory?”,\nin Proc."
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "Int. Society for Music Information Retrieval Conf., San Francisco, United\n(e.g. chord progressions) to stationary (e.g. notes, chords),"
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "States, 2024."
        },
        {
          "meganwei@brown.edu,\nmichael_freeman@alumni.brown.edu": "influence the performance of\nthese models across\nthese"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tempo,": "time"
        },
        {
          "tempo,": "signature,"
        },
        {
          "tempo,": "note,"
        },
        {
          "tempo,": ""
        },
        {
          "tempo,": "interval,"
        },
        {
          "tempo,": ""
        },
        {
          "tempo,": "scale,"
        },
        {
          "tempo,": ""
        },
        {
          "tempo,": "chord,"
        },
        {
          "tempo,": ""
        },
        {
          "tempo,": "chord"
        },
        {
          "tempo,": ""
        },
        {
          "tempo,": "progression"
        },
        {
          "tempo,": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "minor 6th\nprogression": "minor 6th \ntokens"
        },
        {
          "minor 6th\nprogression": "SynTheory Dataset\nFoundation Model Representations\nExperiments"
        },
        {
          "minor 6th\nprogression": "Figure 1. Overview of our SynTheory benchmark and our Jukebox and MusicGen probing setup. Our SynTheory bench-"
        },
        {
          "minor 6th\nprogression": "mark consists of Rhythmic (tempos and time signatures) and Tonal (notes,\nintervals, scales, chords, and chord progres-"
        },
        {
          "minor 6th\nprogression": "sions) concepts. We assess whether music foundation models (Jukebox and MusicGen) encode these music theory concepts"
        },
        {
          "minor 6th\nprogression": "within their internal representations. For each task from the SynTheory dataset, we extract representations from the music"
        },
        {
          "minor 6th\nprogression": "foundation model. We pass an audio input, embodying the concept (e.g. Perfect 4th),\ninto a pretrained foundation model."
        },
        {
          "minor 6th\nprogression": "The audio codec tokenizes the audio into discrete audio tokens.\nThen,\nit passes these tokens into a decoder\nlanguage"
        },
        {
          "minor 6th\nprogression": "model.\nFrom there, we extract\nthe representations. We then train a probe classifier\n(linear or\ntwo-layer MLP) on these"
        },
        {
          "minor 6th\nprogression": "representations to predict the corresponding class (e.g. pitch class, intervals, or chords) for each SynTheory concept."
        },
        {
          "minor 6th\nprogression": "tasks. We hope our\ninsights on probing music founda-\ntiveness of using internal model representations for down-"
        },
        {
          "minor 6th\nprogression": "tion models, along with the synthetic music data genera-\nstream MIR tasks. Koo et al. [7] focus primarily on prob-"
        },
        {
          "minor 6th\nprogression": "tion framework, encourage and facilitate future endeavors\ning MusicGen’s attention heads in instrument recognition"
        },
        {
          "minor 6th\nprogression": "on symbolic controllability in music generative models.\ntasks, benchmarking against\nthe tasks highlighted in [4],"
        },
        {
          "minor 6th\nprogression": "and propose leveraging these representations for inference-\nFor\nreproducibility, we upload our SynTheory dataset"
        },
        {
          "minor 6th\nprogression": "time control. Other works [5,6] assess the impact of model\nto Hugging Face 1 ,\nrelease the code for dataset genera-"
        },
        {
          "minor 6th\nprogression": "architectures and self-supervised approaches towards mu-\ntion,\nembedding extraction,\nprobing,\nand evaluation on"
        },
        {
          "minor 6th\nprogression": "sic understanding tasks.\nGitHub 2 , and showcase audio samples on our website 3 ."
        },
        {
          "minor 6th\nprogression": "However,\nprior work primarily uses\nreal-world data,"
        },
        {
          "minor 6th\nprogression": "which is often concept-entangled and potentially subject to"
        },
        {
          "minor 6th\nprogression": "2. RELATED WORK"
        },
        {
          "minor 6th\nprogression": "copyright concerns. For example, some of these works use"
        },
        {
          "minor 6th\nprogression": "The success of large language models (LLMs) [15–18] has\nGiantsteps-MTG and Giantsteps [25], which are datasets"
        },
        {
          "minor 6th\nprogression": "sparked new research on probing their\ninternal\nrepresen-\nof primarily electronic dance music with tempo and key"
        },
        {
          "minor 6th\nprogression": "tations\nto measure their understanding of\nlinguistic con-\nannotations, obtained from Beatport. Won et al.\n[5] use"
        },
        {
          "minor 6th\nprogression": "cepts [19, 20] and world knowledge [11, 12, 21] as well as\nHookTheory for chord recognition, where they focus on"
        },
        {
          "minor 6th\nprogression": "editing the encoded knowledge to make LLMs more faith-\nmajor and minor chord identification for each pitch class."
        },
        {
          "minor 6th\nprogression": "ful\nto factual knowledge [9, 10]. Studies have shown that\nThe authors also use Harmonix Set [26] and GTZAN [27]"
        },
        {
          "minor 6th\nprogression": "LLMs can encode grounded representations on color [22],\nfor beat and downbeat detection. In the language modality,"
        },
        {
          "minor 6th\nprogression": "direction [23],\nand auditory representations\n[24].\nThus,\nthe authors of ChatMusician [28] produce a multi-choice"
        },
        {
          "minor 6th\nprogression": "we investigate if music generative models, which often\nquestion answering dataset, MusicTheoryBench, with ex-"
        },
        {
          "minor 6th\nprogression": "share similar model architectures and training objectives\npert annotation from a professional college music teacher."
        },
        {
          "minor 6th\nprogression": "as LLMs, are able to encode abstract concepts from high-\nMusicTheoryBench aims to assess the music understand-"
        },
        {
          "minor 6th\nprogression": "level music tags (e.g. genre, emotion) to fine-grained mu-\ning capabilities of LLMs through natural\nlanguage alone."
        },
        {
          "minor 6th\nprogression": "sic theory knowledge (e.g.\ntempo, chords).\nTo the best of our knowledge, there is a lack of music the-"
        },
        {
          "minor 6th\nprogression": "Recent work has\nshown promise towards uncovering\nory probing benchmarks in the audio domain that are la-"
        },
        {
          "minor 6th\nprogression": "conceptual\nrepresentations\nfrom probing audio and mu-\nbeled in detail, copyright-free, and scalable, prior\nto our"
        },
        {
          "minor 6th\nprogression": "sic generative models and leveraging different music foun-\nproposed SynTheory."
        },
        {
          "minor 6th\nprogression": "dation model architectures\ntowards music understanding"
        },
        {
          "minor 6th\nprogression": "tasks. Castellon and Donahue et al. [4] propose using rep-"
        },
        {
          "minor 6th\nprogression": "3.\nSYNTHEORY: SYNTHETIC DATASET OF"
        },
        {
          "minor 6th\nprogression": "resentations from language models trained on codified au-"
        },
        {
          "minor 6th\nprogression": "MUSIC THEORY CONCEPTS"
        },
        {
          "minor 6th\nprogression": "dio towards downstream MIR tasks as a better alternative"
        },
        {
          "minor 6th\nprogression": "We design seven datasets to capture isolated music theory\nto conventional\ntagging models.\nThe authors train prob-"
        },
        {
          "minor 6th\nprogression": "concepts – similar to synthetic audio for ear training. Mu-\ning classifiers on Jukebox representations on music tag-"
        },
        {
          "minor 6th\nprogression": "sicians may “train their ear” to recognize music concepts\nging, genre identification, key identification, and emotion"
        },
        {
          "minor 6th\nprogression": "like intervals or chord quality in an isolated setting before\nrecognition tasks.\nThese\nresults demonstrate the\neffec-"
        },
        {
          "minor 6th\nprogression": "advancing to the harder, more entangled case that arises"
        },
        {
          "minor 6th\nprogression": "1 https://huggingface.co/datasets/meganwei/"
        },
        {
          "minor 6th\nprogression": "in non-pedagogical music. Assessing concept recognition"
        },
        {
          "minor 6th\nprogression": "syntheory"
        },
        {
          "minor 6th\nprogression": "through isolated concepts mitigates the possibility that one\n2 https://github.com/brown-palm/syntheory"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: SynTheory contains seven synthetic datasets,",
      "data": [
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "rather than as acoustically realized audio. A more practical"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "motivation for this work is that extracting such low-level,"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "isolated concepts from existing datasets may require non-"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "trivial engineering or domain expert labor.\nIt may even be"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "impossible to disentangle all overlapping concepts. Mu-"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "sic stem isolation and concept isolation are distinct; an iso-"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "lated instrument in a multi-track recording may still exhibit"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "several,\nintricately intertwined theory concepts.\nIt\nis not"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "clear how to “unmix” such concepts once they are blended."
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "Instead of attempting to disentangle several concepts"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "from existing audio, SynTheory implements this “ear train-"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "ing” quiz setting by explicitly producing individual con-"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "cepts.\nEach of\nthe seven datasets ablates a single musi-"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "cal feature while fixing all others,\nthereby isolating it\nto a"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "degree not\ntypically found in recorded music. These ab-"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "lated concepts consist of tempo, time signatures, notes, in-"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "tervals, scales, chords, and chord progressions. We adopt"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "isolation as a design choice to mitigate context\nthat may"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "be exploited in deep learning models as “shortcuts”,\ni.e."
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "heuristics that correlate with concepts most of the time but"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "do not truly encode the concept."
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "Using this music theory concept-driven synthesis de-"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "sign, we construct\nlabel-balanced and copyright-free data."
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "The synthetic approach avoids annotation errors present"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "in other contemporary MIR datasets.\nFor example,\nthe"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "HookTheory data processing step for SheetSage [13]\nre-"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "quired ad-hoc time-alignment of the expert annotations. In"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "the released SheetSage dataset, 17, 980/26, 175 (68.7%)"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "samples required more precise time alignment. While our"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "synthetic data is no substitute for\nreal music data,\nto our"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "knowledge, no other dataset so strictly isolates each con-"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "cept to this level of granularity."
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "SynTheory contains two categories:\ntonal and rhythmic."
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "We make this distinction for stronger concept isolation; we"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "wish to keep the rhythm samples\ntonally consistent and"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "the tonal samples rhythmically consistent. For each tonal"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "dataset, we voice the same MIDI data through 92 distinct"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "instruments.\nThe selection of\ninstrument voices is fixed,"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "making the distribution of timbres sufficiently diverse but"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "also class-balanced. Each instrument corresponds to one"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "of\nthe 128 canonical MIDI program codes and is voiced"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "through the TimGM6mb.sf2 [30]\nsoundfont.\nA MIDI"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "“program” is a specific instrument preset.\nThe canoni-"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "cal program set\nincludes many instruments, e.g.\n“Acous-"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "tic Grand Piano”, “Flute”, etc. We exclude programs that"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "are polyphonic,\nsound effects (e.g.\n“Bird Tweet”, “Gun"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "Shot”), and highly articulate. A highly articulate program"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "has some unchangeable characteristic (e.g. pitch bending)"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "that destabilizes its pitch. For each rhythmic dataset, we"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "define five metronome-like timbral settings. Each setting"
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": ""
        },
        {
          "assessment\ntowards theoretical concepts as abstract\nideas": "uses one of the distinct\ninstruments: “Woodblock Light”,"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: SynTheory contains seven synthetic datasets,",
      "data": [
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "rates the notion that timbral information may be exploited"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "in identifying a different concept\nlike pitch class [29]. As"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "such, our dataset\nis designed to remove or reduce features"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "that may correlate with a concept, but are not strictly nec-"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "essary for\nidentifying it.\nOur\nintent\nis\na more pointed"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "assessment\ntowards theoretical concepts as abstract\nideas"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "rather than as acoustically realized audio. A more practical"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "motivation for this work is that extracting such low-level,"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "isolated concepts from existing datasets may require non-"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "trivial engineering or domain expert labor.\nIt may even be"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "impossible to disentangle all overlapping concepts. Mu-"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "sic stem isolation and concept isolation are distinct; an iso-"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "lated instrument in a multi-track recording may still exhibit"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "several,\nintricately intertwined theory concepts.\nIt\nis not"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "clear how to “unmix” such concepts once they are blended."
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "Instead of attempting to disentangle several concepts"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "from existing audio, SynTheory implements this “ear train-"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "ing” quiz setting by explicitly producing individual con-"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "cepts.\nEach of\nthe seven datasets ablates a single musi-"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "cal feature while fixing all others,\nthereby isolating it\nto a"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "degree not\ntypically found in recorded music. These ab-"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "lated concepts consist of tempo, time signatures, notes, in-"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "tervals, scales, chords, and chord progressions. We adopt"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "isolation as a design choice to mitigate context\nthat may"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "be exploited in deep learning models as “shortcuts”,\ni.e."
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "heuristics that correlate with concepts most of the time but"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "do not truly encode the concept."
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "Using this music theory concept-driven synthesis de-"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "sign, we construct\nlabel-balanced and copyright-free data."
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "The synthetic approach avoids annotation errors present"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "in other contemporary MIR datasets.\nFor example,\nthe"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "HookTheory data processing step for SheetSage [13]\nre-"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "quired ad-hoc time-alignment of the expert annotations. In"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "the released SheetSage dataset, 17, 980/26, 175 (68.7%)"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "samples required more precise time alignment. While our"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "synthetic data is no substitute for\nreal music data,\nto our"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "knowledge, no other dataset so strictly isolates each con-"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "cept to this level of granularity."
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "SynTheory contains two categories:\ntonal and rhythmic."
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "We make this distinction for stronger concept isolation; we"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "wish to keep the rhythm samples\ntonally consistent and"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "the tonal samples rhythmically consistent. For each tonal"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "dataset, we voice the same MIDI data through 92 distinct"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "instruments.\nThe selection of\ninstrument voices is fixed,"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "making the distribution of timbres sufficiently diverse but"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "also class-balanced. Each instrument corresponds to one"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "of\nthe 128 canonical MIDI program codes and is voiced"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "through the TimGM6mb.sf2 [30]\nsoundfont.\nA MIDI"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "“program” is a specific instrument preset.\nThe canoni-"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "cal program set\nincludes many instruments, e.g.\n“Acous-"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "tic Grand Piano”, “Flute”, etc. We exclude programs that"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "are polyphonic,\nsound effects (e.g.\n“Bird Tweet”, “Gun"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "Shot”), and highly articulate. A highly articulate program"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "has some unchangeable characteristic (e.g. pitch bending)"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "that destabilizes its pitch. For each rhythmic dataset, we"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "define five metronome-like timbral settings. Each setting"
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": ""
        },
        {
          "on instrument-specific absolute pitch in humans corrobo-": "uses one of the distinct\ninstruments: “Woodblock Light”,"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "more recently in MIR [4], we use probes to assess whether"
        },
        {
          "3.2.2\nIntervals": "We vary the root note, number of half-steps,\ninstrument,",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "music theory concepts are discernable in the internal rep-"
        },
        {
          "3.2.2\nIntervals": "and play style (unison, up,\nand down).\nTo retain con-",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "resentations of foundation models."
        },
        {
          "3.2.2\nIntervals": "sistent\nrhythm,\nthe up and down styles repeat\nfour\ntimes",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "We adopt\nthe same probing paradigm as [4] and frame"
        },
        {
          "3.2.2\nIntervals": "throughout\nthe sample while the unison play style repeats",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "concept understanding as multiclass classification for dis-"
        },
        {
          "3.2.2\nIntervals": "eight times. There are (12 PITCH CLASS ·12 HALF-STEP ·",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "crete concepts (notes,\nintervals, scales, chords, chord pro-"
        },
        {
          "3.2.2\nIntervals": "92 INSTRUMENT\n· 3 PLAY STYLE ) = 39, 744 samples.",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "gressions, and time signatures) and regression for continu-"
        },
        {
          "3.2.2\nIntervals": "3.2.3\nScales",
          "discover linguistic structure and semantics in NLP [31] and": "ous concepts (tempo). We train linear and two-layer MLP"
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "probes on the embeddings of\nthe internal\nrepresentations"
        },
        {
          "3.2.2\nIntervals": "We voice\nseven Western music modes\n(Ionian, Dorian,",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "of Jukebox and MusicGen and the handcrafted features."
        },
        {
          "3.2.2\nIntervals": "Phrygian, Lydian, Mixolydian, Aeolian, and Locrian)\nin",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "Each probe is trained independently for its correspond-"
        },
        {
          "3.2.2\nIntervals": "all\nroot notes,\nin 92 instruments, and in two play styles",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "ing concept task. That is, the probe trained to identify notes"
        },
        {
          "3.2.2\nIntervals": "(ascending or descending). The register is constant; we se-",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "from Jukebox embeddings will not be used to identify in-"
        },
        {
          "3.2.2\nIntervals": "·\nlect\nroot notes close to middle C. There are (7 MODE",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "tervals, for example."
        },
        {
          "3.2.2\nIntervals": "12 ROOT NOTE\n· 92 INSTRUMENT\n· 2 PLAY STYLE ) =",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "For the classification tasks, we measure the accuracy of"
        },
        {
          "3.2.2\nIntervals": "15, 456 samples.",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "our trained probes on the following SynTheory tasks:"
        },
        {
          "3.2.2\nIntervals": "3.2.4 Chords",
          "discover linguistic structure and semantics in NLP [31] and": "• Notes (12): C, C#, D, D#, E, F, F#, G, G#, A, A#,"
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "and B"
        },
        {
          "3.2.2\nIntervals": "We voice triads of all\ntwelve root notes, four chord quali-",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "•\nIntervals\n(12): minor 2nd, Major 2nd, minor 3rd,"
        },
        {
          "3.2.2\nIntervals": "ties (major, minor, augmented, and diminished), 92 instru-",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "Major 3rd, Perfect 4th, Tritone, Perfect 5th, minor"
        },
        {
          "3.2.2\nIntervals": "ments, and three inversions (root position, first\ninversion,",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "6th, Major 6th, minor 7th, Major 7th, and Perfect"
        },
        {
          "3.2.2\nIntervals": "and second inversion). The chord is struck at each quar-",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "octave"
        },
        {
          "3.2.2\nIntervals": "ter note at 120 BPM. Like the Scales dataset, we fix the",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "• Scales\n(7):\nIonian,\nDorian,\nPhrygian,\nLydian,"
        },
        {
          "3.2.2\nIntervals": "·\nregister close to middle C. There are (12 ROOT NOTE",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "Mixolydian, Aeolian, and Locrian"
        },
        {
          "3.2.2\nIntervals": "4 CHORD QUALITY ·92 INSTRUMENT ·3 INVERSION ) =",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "• Chords\n(4): Major, Minor, Diminished, and Aug-"
        },
        {
          "3.2.2\nIntervals": "13, 248 samples.",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "mented"
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "(I–IV–V–I),\n(I–IV–vi–V),\n• Chord Progressions\n(19):"
        },
        {
          "3.2.2\nIntervals": "3.2.5 Chord Progressions",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "(I–V–vi–IV),\n(I–vi–IV–V),\n(ii–V–I–vi),\n(IV–I–V–vi),"
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "(IV–V–iii–vi),\n(V–IV–I–V),\n(V–vi–IV–I),\n(vi–IV–I–V),"
        },
        {
          "3.2.2\nIntervals": "We select 19 four-chord progressions, with ten in the major",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "(i–ii◦–v–i),\n(i–III–iv–i),\n(i–iv–v–i),\n(i–VI–III–VII),"
        },
        {
          "3.2.2\nIntervals": "mode and nine in the natural minor mode:",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "(i–VI–VII–i),\n(i–VI–VII–III),\n(i–VII–VI–IV),"
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "(iv–VII–i–i), and (VII–vi–VII–i)"
        },
        {
          "3.2.2\nIntervals": "(I–IV–V–I),\n(I–IV–vi–V),\n(I–V–vi–IV),\n• Major:",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "(I–vi–IV–V),\n(ii–V–I–vi),\n(IV–I–V–vi),\n(IV–V–iii–vi),",
          "discover linguistic structure and semantics in NLP [31] and": "22\n• Time Signatures (8):\n, 4\n2, 4\n3, 8\n3, 4\n4, 8\n6, 8\n9, and 8"
        },
        {
          "3.2.2\nIntervals": "(V–IV–I–V), (V–vi–IV–I), (vi–IV–I–V)",
          "discover linguistic structure and semantics in NLP [31] and": "These tasks are trained on a 70% train, 15% test, and 15%"
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "validation split, using the Adam optimizer and Cross En-"
        },
        {
          "3.2.2\nIntervals": "(i–ii◦–v–i),\n(i–III–iv–i),\n(i–iv–v–i),\n• Natural Minor:",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "tropy loss."
        },
        {
          "3.2.2\nIntervals": "(i–VI–III–VII),\n(i–VI–VII–i),\n(i–VI–VII–III),",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "(i–VII–VI–IV), (iv–VII–i–i), (VII–vi–VII–i)",
          "discover linguistic structure and semantics in NLP [31] and": "For the Tempo dataset, we train a regression probe, over"
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "the 161 tempo values. To increase complexity in the prob-"
        },
        {
          "3.2.2\nIntervals": "We\nvary\nthe\nroot\nnote\nof\nthe\nkey\nand\ninstrument.",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "ing task and test generalization to unseen BPMs, the train-"
        },
        {
          "3.2.2\nIntervals": "Each\nchord\nis\nplayed\nat\neach\nquarter\nnotes\nat\n120",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "ing set consists of the middle 70% of the BPMs. The test"
        },
        {
          "3.2.2\nIntervals": "(19 PROGRESSION\n· 12 KEY ROOT\n·\nBPM. There\nare",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "and validation sets consist of the top 15% BPMs and the"
        },
        {
          "3.2.2\nIntervals": "92 INSTRUMENT ) = 20, 976 samples.",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "bottom 15% BPMs,\nrandomly shuffled and split\nin half."
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "We use MSE loss and report the R2 score."
        },
        {
          "3.2.2\nIntervals": "4. EXPERIMENTS",
          "discover linguistic structure and semantics in NLP [31] and": "To select\nthe best performing probe for each concept,"
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "we perform a grid search across various hyperparameters"
        },
        {
          "3.2.2\nIntervals": "We describe our approach to assess how well\nthe inter-",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "for the MusicGen audio codec, mel spectrogram, MFCC,"
        },
        {
          "3.2.2\nIntervals": "nal representations of music generative models (MusicGen",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "chroma, and aggregate handcrafted features, following [4]:"
        },
        {
          "3.2.2\nIntervals": "and Jukebox) and handcrafted audio features\n(mel\nspec-",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "• Data Normalization: {True, False}"
        },
        {
          "3.2.2\nIntervals": "trograms, MFCC, and chroma) encode music theory con-",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "• Model Type: {Linear, two-layer MLP with 512 hid-"
        },
        {
          "3.2.2\nIntervals": "cepts.",
          "discover linguistic structure and semantics in NLP [31] and": ""
        },
        {
          "3.2.2\nIntervals": "",
          "discover linguistic structure and semantics in NLP [31] and": "den units and ReLU activation}"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• Data Normalization: True": "• Model Type:"
        },
        {
          "• Data Normalization: True": "and ReLU activation"
        },
        {
          "• Data Normalization: True": "• Batch Size: 64"
        },
        {
          "• Data Normalization: True": "• Learning Rate: 10−3"
        },
        {
          "• Data Normalization: True": "• Dropout: 0.5"
        },
        {
          "• Data Normalization: True": "• L2 Weight Decay: off"
        },
        {
          "• Data Normalization: True": "We selected these hyperparameters from the best overall"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• Learning Rate: 10−3": "• Dropout: 0.5"
        },
        {
          "• Learning Rate: 10−3": "• L2 Weight Decay: off"
        },
        {
          "• Learning Rate: 10−3": "We selected these hyperparameters from the best overall"
        },
        {
          "• Learning Rate: 10−3": "performing probe by fixing a layer in the decoder LMs and"
        },
        {
          "• Learning Rate: 10−3": "performing a hyperparameter search, following the sweep"
        },
        {
          "• Learning Rate: 10−3": "approach outlined in [4]."
        },
        {
          "• Learning Rate: 10−3": "4.2 Model representations"
        },
        {
          "• Learning Rate: 10−3": ""
        },
        {
          "• Learning Rate: 10−3": "We extract representations from two text-to-music founda-"
        },
        {
          "• Learning Rate: 10−3": ""
        },
        {
          "• Learning Rate: 10−3": "tion models,\nJukebox [3] and MusicGen [1] and bench-"
        },
        {
          "• Learning Rate: 10−3": ""
        },
        {
          "• Learning Rate: 10−3": "mark\nthem against\nthree\nhandcrafted,\nspectral\nfeatures"
        },
        {
          "• Learning Rate: 10−3": ""
        },
        {
          "• Learning Rate: 10−3": "following\n[4]:\nmel\nspectrograms, mel-frequency\ncep-"
        },
        {
          "• Learning Rate: 10−3": ""
        },
        {
          "• Learning Rate: 10−3": "stral coefficients\n(MFCC), and constant-Q chromagrams"
        },
        {
          "• Learning Rate: 10−3": ""
        },
        {
          "• Learning Rate: 10−3": "(chroma). These handcrafted features are common in tra-"
        },
        {
          "• Learning Rate: 10−3": ""
        },
        {
          "• Learning Rate: 10−3": "ditional methods of MIR and are a more interpretable base-"
        },
        {
          "• Learning Rate: 10−3": ""
        },
        {
          "• Learning Rate: 10−3": "line against\nthe embeddings of the pre-trained music gen-"
        },
        {
          "• Learning Rate: 10−3": "erative models."
        },
        {
          "• Learning Rate: 10−3": "Jukebox consists of a VQ-VAE model\nthat codifies au-"
        },
        {
          "• Learning Rate: 10−3": "dio waveforms into discrete codes at a lower sample rate"
        },
        {
          "• Learning Rate: 10−3": "and a language model that generates codified audio with a"
        },
        {
          "• Learning Rate: 10−3": "transformer decoder. We trim each audio sample to four"
        },
        {
          "• Learning Rate: 10−3": "seconds and convert\nit\nto mono.\nUsing Jukemirlib [4],"
        },
        {
          "• Learning Rate: 10−3": "we pass the audio through the frozen audio encoder and"
        },
        {
          "• Learning Rate: 10−3": "through the decoder language model. Due to resource con-"
        },
        {
          "• Learning Rate: 10−3": "straints, we downsample the activation to a target\nrate of"
        },
        {
          "• Learning Rate: 10−3": "half\nthat\nin [4],\nusing the Librosa FFT algorithm [32]."
        },
        {
          "• Learning Rate: 10−3": "Then, we meanpool\nthe representations across time to re-"
        },
        {
          "• Learning Rate: 10−3": "duce the dimensionality of the embeddings, resulting a di-"
        },
        {
          "• Learning Rate: 10−3": "mension of (72, 4800) per sample, where 72 is the number"
        },
        {
          "• Learning Rate: 10−3": "of layers and 4800 is the dimension of the activations. To"
        },
        {
          "• Learning Rate: 10−3": "further reduce the dimensions, we adopt a layer selection"
        },
        {
          "• Learning Rate: 10−3": "process similar to [4]. We train the probe classifiers with"
        },
        {
          "• Learning Rate: 10−3": "fixed hyperparameters on the music concept\ntasks as de-"
        },
        {
          "• Learning Rate: 10−3": "scribed in Section 4.1.\nFor each concept, we select\nthe"
        },
        {
          "• Learning Rate: 10−3": "layer that results in the highest probing score. This results"
        },
        {
          "• Learning Rate: 10−3": "in a final dimension of 4800 for each Jukebox representa-"
        },
        {
          "• Learning Rate: 10−3": "tion."
        },
        {
          "• Learning Rate: 10−3": "MusicGen consists of a pretrained convolutional auto-"
        },
        {
          "• Learning Rate: 10−3": "encoder (EnCodec) [33], a pretrained T5 text encoder, and"
        },
        {
          "• Learning Rate: 10−3": "an acoustic transformer decoder. We resample the audio"
        },
        {
          "• Learning Rate: 10−3": "to 32 kHz (the sampling rate used in the EnCodec model)"
        },
        {
          "• Learning Rate: 10−3": "trim to four seconds, convert\nto mono, and pass the audio"
        },
        {
          "• Learning Rate: 10−3": "through the frozen EnCodec audio codec. We do not pass"
        },
        {
          "• Learning Rate: 10−3": "text\nthrough the text encoder, as we focus on audio rep-"
        },
        {
          "• Learning Rate: 10−3": "resentations. We then extract representations from several"
        },
        {
          "• Learning Rate: 10−3": "regions of the model:\nthe final layer of the audio codec be-"
        },
        {
          "• Learning Rate: 10−3": "fore residual vector quantization and the hidden states of"
        },
        {
          "• Learning Rate: 10−3": "the decoder language model. The number of decoder hid-"
        },
        {
          "• Learning Rate: 10−3": "den states vary based on the model size: small (24 layers),"
        },
        {
          "• Learning Rate: 10−3": "medium (48 layers), and large (48 layers)."
        },
        {
          "• Learning Rate: 10−3": ""
        },
        {
          "• Learning Rate: 10−3": "For our four second audio clips,\nthe audio codec repre-"
        },
        {
          "• Learning Rate: 10−3": "sentations are of dimension (128, 200), where 128 is the"
        },
        {
          "• Learning Rate: 10−3": "dimension of the activation after the final\nlayer of the au-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chord": "",
          "Time": ""
        },
        {
          "Chord": "Progressions",
          "Time": "Signatures"
        },
        {
          "Chord": "0.971",
          "Time": "1.000"
        },
        {
          "Chord": "0.942",
          "Time": "0.911"
        },
        {
          "Chord": "0.870",
          "Time": "0.883"
        },
        {
          "Chord": "0.901",
          "Time": "0.905"
        },
        {
          "Chord": "0.330",
          "Time": "0.677"
        },
        {
          "Chord": "0.723",
          "Time": "0.827"
        },
        {
          "Chord": "0.872",
          "Time": "0.688"
        },
        {
          "Chord": "0.869",
          "Time": "0.672"
        },
        {
          "Chord": "0.868",
          "Time": "0.833"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "Aggregate Handcrafted\n0.941\n0.997\n0.972\n0.992\n0.868\n0.947\n0.833\n0.936"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "Table 2. We report probing results on the SynTheory dataset for the Jukebox LM, MusicGen Decoder LM (Small, Medium,"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "and Large), MusicGen Audio Codec models as well as handcrafted features\n(Mel Spectrogram, MFCC, Chroma, and"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "Aggregate Handcrafted). For the tempos dataset, we report the R2 score from the regression probe. For all other concepts"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "(notes,\nintervals, scales, chords, chord progressions, and time signatures), we report\nthe probing classifier accuracy. For"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "MusicGen Audio Codec, Mel Spectrogram, MFCC, Chroma, and Aggregate Handcrafted, we report\nthe metrics of\nthe"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "best performing probe for each task using the best validation performance from our hyperparameter search. For MusicGen"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "Decoder LM (Small, Medium, and Large) and Jukebox models, we report the metrics of the best performing probe for each"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "task using layer selection. We also report an average performance across all concepts for each model/feature."
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "The aggregate handcrafted features perform compara-\ntions.\nJukebox performs consistently well across all Syn-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "bly to MusicGen Decoder LMs. This suggests that harder\nTheory tasks. MusicGen Decoder LMs also exhibit com-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "music concept understanding benchmarks should address\npetitive performance across our benchmark.\nInterestingly,"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "concepts\nlatent\nin foundation models but not easily en-\nwe observe that MusicGen Decoder\n(Small) outperforms"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "coded in handcrafted features. These harder benchmarks\nits larger counterparts. This finding contradicts traditional"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "may include entangled concepts, such as probing for both\ndiscussions on scaling laws and the claims in [1] that larger"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "chord progression type and tempo in a chord progression\nMusicGen models better “understand” text prompts and"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "samples of varying tempos.\nProbing for compositional\nproduce better quantitative and subjective scores. Figure 2"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "tasks could reveal differences in concept encoding between\nfurther highlights this finding, showing that MusicGen De-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "model representations and handcrafted features.\ncoder\n(Small) maintains consistently high probing scores"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "across all layers compared to that of its larger counterparts."
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "While these models have shown that\nthey encode rep-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "Meanwhile,\nlarger MusicGen models display a steep per-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "resentations for music theory in audio, some state-of-the-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "formance drop in the initial\nlayers, followed by a gradual"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "art music foundation models primarily rely on text con-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "increase and then a slight decline in the final\nlayers. This"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "trols.\nIn a future study, we plan to probe for language rep-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "pattern suggests that\nthe smaller model may have devel-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "resentations in music generation models to improve text-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "oped a more efficient encoding of music theory concepts"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "controllable methods."
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "within its representations."
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "While these models perform well on other concepts,"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "MusicGen slightly underperforms on the notes dataset. We"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "hypothesize this may be due to isolated notes being less"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "prominent as\nintervals,\nscales,\nand chords\nin real-world\n6. CONCLUSION"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "music.\nThis\nreveals\nthe challenge in distinguishing the"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "most fundamental building blocks of music."
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "We introduce SynTheory, a synthetic dataset of music the-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "Pretrained music decoder LMs generally outperform"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "ory concepts,\nthat\nis concept-isolated, copyright-free, and"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "MusicGen Audio Codec\nrepresentations\nand\nindividual"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "scalable. We use SynTheory to evaluate\nthe degree\nto"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "handcrafted features.\nMusicGen Audio Codec\nexhibits"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "which music theory concepts are encoded in state-of-the-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "poorer\noverall\nperformance,\nsince\nthese\ncodecs were"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "art music generative models. Our experiments suggest that"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "trained to reconstruct fine-grained,\nlow-level details local-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "music theory concepts are indeed discernible within the la-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "ized by time."
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "tent representations of these models. We believe this will"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "allow us to understand how to isolate and manipulate such\nChroma features, which encode pitch class information,"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "concepts, which advances towards controllable generation\nperform comparably well on tonal tasks, but slightly under-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "and music theory benchmarks. We encourage the commu-\nperform on rhythmic tasks. Notably, chroma features out-"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "nity to build more challenging probing datasets with our\nperform MusicGen Decoder LMs on stationary harmonic"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "framework to further understand the relationship between\ntasks (notes, scales, and chords) but lag behind on dynamic"
        },
        {
          "Chroma\n0.954\n0.820\n0.989\n0.994\n0.869\n0.847\n0.672\n0.878": "symbolic and audio-based music generation.\nharmonic tasks (chord progressions and intervals)."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "able Machine Learning for Speech and Audio (XAI-"
        },
        {
          "7. ETHICS STATEMENT": "Our work aims to understand if music generation models",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "SA), 2024."
        },
        {
          "7. ETHICS STATEMENT": "encode music theory concepts in their internal representa-",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "tions. Our dataset may be used to assess music generation",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "[8] G. Brunner, Y. Wang, R. Wattenhofer, and J. Wiesen-"
        },
        {
          "7. ETHICS STATEMENT": "models and may be applied towards fine-grained, music-",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "danger, “Jambot: Music theory aware chord based gen-"
        },
        {
          "7. ETHICS STATEMENT": "theory based controllable generation.",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "eration of polyphonic music with lstms,” in 2017 IEEE"
        },
        {
          "7. ETHICS STATEMENT": "Our custom dataset, SynTheory, is based on elementary",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "29th International Conference on Tools with Artificial"
        },
        {
          "7. ETHICS STATEMENT": "Western music theory concepts and is generated program-",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "Intelligence (ICTAI).\nIEEE, 2017, pp. 519–526."
        },
        {
          "7. ETHICS STATEMENT": "matically. The data does not infringe copyright of musical",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "writers or performers.",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "[9] K. Li, O. Patel, F. Viégas, H. Pfister, and M. Watten-"
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "berg,\n“Inference-time intervention:\nEliciting truthful"
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "answers from a language model,” in Advances in Neu-"
        },
        {
          "7. ETHICS STATEMENT": "8. ACKNOWLEDGEMENTS",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "ral Information Processing Systems, 2024."
        },
        {
          "7. ETHICS STATEMENT": "We would like to thank Professor Cheng-Zhi Anna Huang,",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "Professor Daniel Ritchie, Professor David Bau, Profes-",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "[10] K. Meng, D. Bau, A. Andonian,\nand Y. Belinkov,"
        },
        {
          "7. ETHICS STATEMENT": "sor Jacob Andreas, Tian Yun, Nate Gillman, and Calvin",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "“Locating and editing factual associations in GPT,” in"
        },
        {
          "7. ETHICS STATEMENT": "Luo for their fruitful discussions and feedback towards this",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "Information Processing Systems,"
        },
        {
          "7. ETHICS STATEMENT": "work. This project is partially supported by Samsung. We",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "2022."
        },
        {
          "7. ETHICS STATEMENT": "would also like to thank the Center\nfor Computation and",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "Visualization at Brown University for their computational",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "[11] K. Li, A. K. Hopkins, D. Bau, F. Viégas, H. Pfister, and"
        },
        {
          "7. ETHICS STATEMENT": "resources towards this project. Finally, we greatly appreci-",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "M. Wattenberg, “Emergent world representations: Ex-"
        },
        {
          "7. ETHICS STATEMENT": "ate the insightful questions and thoughtful feedback from",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "ploring a sequence model\ntrained on a synthetic task,”"
        },
        {
          "7. ETHICS STATEMENT": "the reviewers.",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "in The Eleventh International Conference on Learning"
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "Representations, 2023."
        },
        {
          "7. ETHICS STATEMENT": "9. REFERENCES",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "[12] T. Yun, Z. Zeng, K. Handa, A. V. Thapliyal, B. Pang,"
        },
        {
          "7. ETHICS STATEMENT": "[1]\nJ. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Syn-",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "E. Pavlick, and C. Sun, “Emergence of abstract state"
        },
        {
          "7. ETHICS STATEMENT": "naeve, Y. Adi, and A. Défossez, “Simple and control-",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "representations\nin embodied sequence modeling,”\nin"
        },
        {
          "7. ETHICS STATEMENT": "lable music generation,” in Thirty-seventh Conference",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "Proceedings of\nthe Conference on Empirical Methods"
        },
        {
          "7. ETHICS STATEMENT": "on Neural Information Processing Systems, 2023.",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "in Natural Language Processing, 2023."
        },
        {
          "7. ETHICS STATEMENT": "[2] A. Agostinelli,\nT.\nI. Denk,\nZ. Borsos,\nJ.\nEngel,",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "[13] C. Donahue, J. Thickstun, and P. Liang, “Melody tran-"
        },
        {
          "7. ETHICS STATEMENT": "M. Verzetti,\nA.\nCaillon,\nQ. Huang,\nA.\nJansen,",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "scription via generative pre-training,” in International"
        },
        {
          "7. ETHICS STATEMENT": "A. Roberts, M. Tagliasacchi, M. Sharifi, N. Zeghi-",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "Society for Music Information Retrieval, 2022."
        },
        {
          "7. ETHICS STATEMENT": "dour,\nand C. Frank,\n“MusicLM: Generating music",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "from text,” arXiv preprint arXiv:2301.11325, 2023.",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "[14] G. Alain and Y. Bengio, “Understanding intermediate"
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "layers using linear classifier probes,” in International"
        },
        {
          "7. ETHICS STATEMENT": "[3] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford,",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "Conference of Learning Representations, 2016."
        },
        {
          "7. ETHICS STATEMENT": "and I. Sutskever,\n“Jukebox: A generative model\nfor",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "music,” arXiv preprint arXiv:2005.00341, 2020.",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "[15]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,"
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "“Bert: Pre-training of deep bidirectional\ntransformers"
        },
        {
          "7. ETHICS STATEMENT": "[4] R. Castellon, C. Donahue, and P. Liang, “Codified au-",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "for language understanding,” in Association for Com-"
        },
        {
          "7. ETHICS STATEMENT": "dio language modeling learns useful\nrepresentations",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "putational Linguistics, 2019."
        },
        {
          "7. ETHICS STATEMENT": "for music information retrieval,” in International So-",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "ciety for Music Information Retrieval, 2021.",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "[16] A. Radford,\nJ. Wu, R. Child, D. Luan, D. Amodei,"
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "I. Sutskever et al., “Language models are unsupervised"
        },
        {
          "7. ETHICS STATEMENT": "[5] M. Won,\nY\n.-N. Hung,\nand D.\nLe,\n“A founda-",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "multitask learners,” OpenAI blog, vol. 1, no. 8, p. 9,"
        },
        {
          "7. ETHICS STATEMENT": "arXiv\npreprint\ntion model\nfor music\ninformatics,”",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "2019."
        },
        {
          "7. ETHICS STATEMENT": "arXiv:2311.03318, 2023.",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "[17] T. B. Brown,\nB. Mann,\nN. Ryder, M.\nSubbiah,"
        },
        {
          "7. ETHICS STATEMENT": "[6] Y. Li, R. Yuan, G. Zhang, Y. Ma, X. Chen, H. Yin,",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,"
        },
        {
          "7. ETHICS STATEMENT": "C. Lin, A. Ragni, E. Benetos, N. Gyenge, R. Dan-",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss,"
        },
        {
          "7. ETHICS STATEMENT": "nenberg, R. Liu, W. Chen, G. Xia, Y. Shi, W. Huang,",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "G. Krueger,\nT. Henighan,\nR. Child, A. Ramesh,"
        },
        {
          "7. ETHICS STATEMENT": "Y\n. Guo, and J. Fu, “Mert: Acoustic music understand-",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen,"
        },
        {
          "7. ETHICS STATEMENT": "ing model with large-scale self-supervised training,”",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "E. Sigler, M. Litwin, S. Gray, B. Chess,\nJ. Clark,"
        },
        {
          "7. ETHICS STATEMENT": "arXiv preprint arXiv:2306.00107, 2023.",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": ""
        },
        {
          "7. ETHICS STATEMENT": "",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "C. Berner, S. McCandlish, A. Radford,\nI. Sutskever,"
        },
        {
          "7. ETHICS STATEMENT": "[7]\nJ. Koo, G. Wichern, F. G. Germain, S. Khurana, and",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "and D. Amodei, “Language models are few-shot learn-"
        },
        {
          "7. ETHICS STATEMENT": "J. Le Roux,\n“Understanding and controlling genera-",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "in Neural\nInformation Processing\ners,”\nin Advances"
        },
        {
          "7. ETHICS STATEMENT": "tive music transformers by probing individual attention",
          "heads,” IEEE ICASSP Satellite Workshop on Explain-": "Systems, 2020."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "Frontiers\nof\ninstrument-specific\nabsolute\npitch,”"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "in\nPsychology,\nvol.\n11,\n2020.\n[Online].\nAvail-"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "able: https://www.frontiersin.org/journals/psychology/"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "articles/10.3389/fpsyg.2020.560877"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "[19]",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "[30] T. Brechbill, “Timidity++,” 2004. [Online]. Available:"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "https://timbrechbill.com/saxguru/Timidity.php"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "[31] M. E. Peters, M. Neumann, M.\nIyyer, M. Gardner,"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "[20]",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "C. Clark, K. Lee, and L. Zettlemoyer, “Deep contex-"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "the\ntualized word representations,” in Proceedings of"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "2018 Conference of\nthe North American Chapter of"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "the Association for Computational Linguistics: Hu-"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "man Language Technologies, Volume 1 (Long Papers),"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "2018."
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "[21] T. Yun, C. Sun,",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "[32] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar,"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "E. Battenberg, and O. Nieto, “librosa: Audio and music"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "signal analysis in python.” in SciPy, 2015, pp. 18–24."
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "[33] A. Défossez, J. Copet, G. Synnaeve, and Y. Adi, “High"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "[22] M. Abdou, A. Kulmizev, D. Hershcovich, S. Frank,",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "arXiv\npreprint\nfidelity\nneural\naudio\ncompression,”"
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": "arXiv:2210.13438, 2022."
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "[23] R. Patel and E. Pavlick, “Mapping language models to",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "[24]",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "[25] P. Knees, Á. Faraldo, P. Herrera, R. Vogl, S. Böck,",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "[26] O. Nieto, M. C. McCallum, M. Davies, A. Robert-",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "[27] U. Marchand, Q. Fresnel, and G. Peeters, “GTZAN-",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "[28] R. Yuan, H. Lin, Y. Wang, Z. Tian, S. Wu, T. Shen,",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        },
        {
          "[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,": "",
          "[29] L.\nReymore\nand\nN.\nC.\nHansen,\n“A\ntheory": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Simple and controllable music generation",
      "authors": [
        "J Copet",
        "F Kreuk",
        "I Gat",
        "T Remez",
        "D Kant",
        "G Synnaeve",
        "Y Adi",
        "A Défossez"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "3",
      "title": "MusicLM: Generating music from text",
      "authors": [
        "A Agostinelli",
        "T Denk",
        "Z Borsos",
        "J Engel",
        "M Verzetti",
        "A Caillon",
        "Q Huang",
        "A Jansen",
        "A Roberts",
        "M Tagliasacchi",
        "M Sharifi",
        "N Zeghidour",
        "C Frank"
      ],
      "year": "2023",
      "venue": "MusicLM: Generating music from text",
      "arxiv": "arXiv:2301.11325"
    },
    {
      "citation_id": "4",
      "title": "Jukebox: A generative model for music",
      "authors": [
        "P Dhariwal",
        "H Jun",
        "C Payne",
        "J Kim",
        "A Radford",
        "I Sutskever"
      ],
      "year": "2020",
      "venue": "Jukebox: A generative model for music",
      "arxiv": "arXiv:2005.00341"
    },
    {
      "citation_id": "5",
      "title": "Codified audio language modeling learns useful representations for music information retrieval",
      "authors": [
        "R Castellon",
        "C Donahue",
        "P Liang"
      ],
      "year": "2021",
      "venue": "Codified audio language modeling learns useful representations for music information retrieval"
    },
    {
      "citation_id": "6",
      "title": "A foundation model for music informatics",
      "authors": [
        "M Won",
        "Y.-N Hung",
        "D Le"
      ],
      "year": "2023",
      "venue": "A foundation model for music informatics",
      "arxiv": "arXiv:2311.03318"
    },
    {
      "citation_id": "7",
      "title": "Mert: Acoustic music understanding model with large-scale self-supervised training",
      "authors": [
        "Y Li",
        "R Yuan",
        "G Zhang",
        "Y Ma",
        "X Chen",
        "H Yin",
        "C Lin",
        "A Ragni",
        "E Benetos",
        "N Gyenge",
        "R Dannenberg",
        "R Liu",
        "W Chen",
        "G Xia",
        "Y Shi",
        "W Huang",
        "Y Guo",
        "J Fu"
      ],
      "year": "2023",
      "venue": "Mert: Acoustic music understanding model with large-scale self-supervised training",
      "arxiv": "arXiv:2306.00107"
    },
    {
      "citation_id": "8",
      "title": "Understanding and controlling generative music transformers by probing individual attention heads",
      "authors": [
        "J Koo",
        "G Wichern",
        "F Germain",
        "S Khurana",
        "J Roux"
      ],
      "venue": "IEEE ICASSP Satellite Workshop on Explainable Machine Learning for Speech and Audio (XAI-SA)"
    },
    {
      "citation_id": "9",
      "title": "Jambot: Music theory aware chord based generation of polyphonic music with lstms",
      "authors": [
        "G Brunner",
        "Y Wang",
        "R Wattenhofer",
        "J Wiesendanger"
      ],
      "year": "2017",
      "venue": "2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI)"
    },
    {
      "citation_id": "10",
      "title": "Inference-time intervention: Eliciting truthful answers from a language model",
      "authors": [
        "K Li",
        "O Patel",
        "F Viégas",
        "H Pfister",
        "M Wattenberg"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "Locating and editing factual associations in GPT",
      "authors": [
        "K Meng",
        "D Bau",
        "A Andonian",
        "Y Belinkov"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "12",
      "title": "Emergent world representations: Exploring a sequence model trained on a synthetic task",
      "authors": [
        "K Li",
        "A Hopkins",
        "D Bau",
        "F Viégas",
        "H Pfister",
        "M Wattenberg"
      ],
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations"
    },
    {
      "citation_id": "13",
      "title": "Emergence of abstract state representations in embodied sequence modeling",
      "authors": [
        "T Yun",
        "Z Zeng",
        "K Handa",
        "A Thapliyal",
        "B Pang",
        "E Pavlick",
        "C Sun"
      ],
      "year": "2023",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Melody transcription via generative pre-training",
      "authors": [
        "C Donahue",
        "J Thickstun",
        "P Liang"
      ],
      "year": "2022",
      "venue": "International Society for Music Information Retrieval"
    },
    {
      "citation_id": "15",
      "title": "Understanding intermediate layers using linear classifier probes",
      "authors": [
        "G Alain",
        "Y Bengio"
      ],
      "year": "2016",
      "venue": "International Conference of Learning Representations"
    },
    {
      "citation_id": "16",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "17",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "18",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "19",
      "title": "Exploring the limits of transfer learning with a unified textto-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "20",
      "title": "Bert rediscovers the classical nlp pipeline",
      "authors": [
        "I Tenney",
        "D Das",
        "E Pavlick"
      ],
      "year": "2019",
      "venue": "Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "What do you learn from context? probing for sentence structure in contextualized word representations",
      "authors": [
        "I Tenney",
        "P Xia",
        "B Chen",
        "A Wang",
        "A Poliak",
        "R Mccoy",
        "N Kim",
        "B Van Durme",
        "S Bowman",
        "D Das"
      ],
      "year": "2019",
      "venue": "Proceedings of the 7th International Conference on Learning Representations"
    },
    {
      "citation_id": "22",
      "title": "Does vision-andlanguage pretraining improve lexical grounding",
      "authors": [
        "T Yun",
        "C Sun",
        "E Pavlick"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "23",
      "title": "Can language models encode perceptual structure without grounding? a case study in color",
      "authors": [
        "M Abdou",
        "A Kulmizev",
        "D Hershcovich",
        "S Frank",
        "E Pavlick",
        "A Søgaard"
      ],
      "year": "2021",
      "venue": "Proceedings of the 25th Conference on Computational Natural Language Learning"
    },
    {
      "citation_id": "24",
      "title": "Mapping language models to grounded conceptual spaces",
      "authors": [
        "R Patel",
        "E Pavlick"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "25",
      "title": "What do language models hear? probing for auditory representations in language models",
      "authors": [
        "J Ngo",
        "Y Kim"
      ],
      "year": "2024",
      "venue": "What do language models hear? probing for auditory representations in language models"
    },
    {
      "citation_id": "26",
      "title": "Two data sets for tempo estimation and key detection in electronic dance music annotated from user corrections",
      "authors": [
        "P Knees",
        "Á Faraldo",
        "P Herrera",
        "R Vogl",
        "S Böck",
        "F Hörschläger",
        "M Goff"
      ],
      "year": "2015",
      "venue": "Two data sets for tempo estimation and key detection in electronic dance music annotated from user corrections"
    },
    {
      "citation_id": "27",
      "title": "The harmonix set: Beats, downbeats, and functional segment annotations of western popular music",
      "authors": [
        "O Nieto",
        "M Mccallum",
        "M Davies",
        "A Robertson",
        "A Stark",
        "E Egozy"
      ],
      "year": "2019",
      "venue": "International Society for Music Information Retrieval"
    },
    {
      "citation_id": "28",
      "title": "GTZANrhythm: Extending the GTZAN test-set with beat, downbeat and swing annotations",
      "authors": [
        "U Marchand",
        "Q Fresnel",
        "G Peeters"
      ],
      "year": "2015",
      "venue": "ISMIR 2015 Late-Breaking Session"
    },
    {
      "citation_id": "29",
      "title": "Chatmusician: Understanding and generating music intrinsically with llm",
      "authors": [
        "R Yuan",
        "H Lin",
        "Y Wang",
        "Z Tian",
        "S Wu",
        "T Shen",
        "G Zhang",
        "Y Wu",
        "C Liu",
        "Z Zhou",
        "Z Ma",
        "L Xue",
        "Z Wang",
        "Q Liu",
        "T Zheng",
        "Y Li",
        "Y Ma",
        "Y Liang",
        "X Chi",
        "R Liu",
        "Z Wang",
        "P Li",
        "J Wu",
        "C Lin",
        "Q Liu",
        "T Jiang",
        "W Huang",
        "W Chen",
        "E Benetos",
        "J Fu",
        "G Xia",
        "R Dannenberg",
        "W Xue",
        "S Kang",
        "Y Guo"
      ],
      "year": "2024",
      "venue": "Chatmusician: Understanding and generating music intrinsically with llm",
      "arxiv": "arXiv:2402.16153"
    },
    {
      "citation_id": "30",
      "title": "A theory of instrument-specific absolute pitch",
      "authors": [
        "L Reymore",
        "N Hansen"
      ],
      "year": "2020",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2020.560877"
    },
    {
      "citation_id": "31",
      "title": "Timidity++",
      "authors": [
        "T Brechbill"
      ],
      "year": "2004",
      "venue": "Timidity++"
    },
    {
      "citation_id": "32",
      "title": "Deep contextualized word representations",
      "authors": [
        "M Peters",
        "M Neumann",
        "M Iyyer",
        "M Gardner",
        "C Clark",
        "K Lee",
        "L Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "33",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "SciPy"
    },
    {
      "citation_id": "34",
      "title": "High fidelity neural audio compression",
      "authors": [
        "A Défossez",
        "J Copet",
        "G Synnaeve",
        "Y Adi"
      ],
      "year": "2022",
      "venue": "High fidelity neural audio compression",
      "arxiv": "arXiv:2210.13438"
    }
  ]
}