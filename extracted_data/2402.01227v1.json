{
  "paper_id": "2402.01227v1",
  "title": "Staa-Net: A Sparse And Transferable Adversarial Attack For Speech Emotion Recognition",
  "published": "2024-02-02T08:46:57Z",
  "authors": [
    "Yi Chang",
    "Zhao Ren",
    "Zixing Zhang",
    "Xin Jing",
    "Kun Qian",
    "Xi Shao",
    "Bin Hu",
    "Tanja Schultz",
    "Björn W. Schuller"
  ],
  "keywords": [
    "Speech emotion recognition",
    "adversarial attacks",
    "sparsity",
    "transferability",
    "efficiency",
    "end-to-end"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech contains rich information on the emotions of humans, and Speech Emotion Recognition (SER) has been an important topic in the area of human-computer interaction. The robustness of SER models is crucial, particularly in privacy-sensitive and reliability-demanding domains like private healthcare. Recently, the vulnerability of deep neural networks in the audio domain to adversarial attacks has become a popular area of research. However, prior works on adversarial attacks in the audio domain primarily rely on iterative gradient-based techniques, which are time-consuming and prone to overfitting the specific threat model. Furthermore, the exploration of sparse perturbations, which have the potential for better stealthiness, remains limited in the audio domain. To address these challenges, we propose a generator-based attack method to generate sparse and transferable adversarial examples to deceive SER models in an end-to-end and efficient manner. We evaluate our method on two widely-used SER datasets, Database of Elicited Mood in Speech (DEMoS) and Interactive Emotional dyadic MOtion CAPture (IEMOCAP), and demonstrate its ability to generate successful sparse adversarial examples in an efficient manner. Moreover, our generated adversarial examples exhibit model-agnostic transferability, enabling effective adversarial attacks on advanced victim models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "S PEECH Emotion Recognition (SER) has been widely applied in many areas in human-computer interaction  [1] , such as diagnosis of depression and bipolar disorder  [2] , computer games  [3] , and intelligent call centres  [4] . Recently, due to the rapid development of efficient computing resources and advanced deep learning methods, Deep Neural Networks (DNNs) have achieved good performance in SER  [5] -  [12] . From the perspective of the input types, there are primarily two categories of DNNs for SER. One category utilises spectrum-based features  [5] ,  [6]  while the other processes audio recordings in an end-to-end manner  [7] ,  [12] . More recently, foundation models (e. g., wav2vec 2.0  [13] , or HuBERT  [14] ) pre-trained on large relative datasets (e. g., Librispeech  [15] ) have been successfully fine-tuned on smaller dataset for the specific task at hand  [10] ,  [11] .\n\nEven though adversarial attacks were first demonstrated and have been extensively studied in the image domain  [16] -  [19] , recent research has found that an adversary also poses significant security and privacy threats to the audio domain, such as automatic speech recognition  [20] -  [23] , speaker recognition  [24] -  [27] , and classification of acoustic scenes and events  [28] . Moreover, adversarial attacks on SER can also cause severe issues. For instance, these attacks can be used to spread toxic or hateful speech on social media platforms or online gaming platforms, compromising public safety. Additionally, malicious actors can manipulate speech content to deceive automatic speech recognition systems and further manipulate SER models, potentially leading to the propagation of harmful content  [29] . Furthermore, in the context of mental illness pre-screening, targeted attacks on speech data could result in incorrect diagnoses and inappropriate treatment for patients  [2] . Given these concerns, enhancing the robustness of SER models has emerged as a crucial research area. At present, however, there are relatively few studies  [30] ,  [31]  on adversarial attacks for SER.\n\nPrevious adversarial attack techniques in the audio domain have limitations in terms of their practicality and transferability. White-box approaches (e. g.,  [20] ,  [23] ,  [25] ,  [32] ) are often less realistic in the real-world since the adversary has access to all information of the attacked models; iterative gradient-based attacks often suffer from low transferability (e. g.,  [33] ,  [34] ), which can be attributed to the optimisation of perturbations using the gradient information of the victim model with respect to a specific input. Additionally, most existing attacks on audio tasks impose constraints on the l 2  [20] ,  [26]  or l ∞  [22] -  [25] ,  [27]  norms of the adversarial perturbations, as the l 0 norm is a typical NP-hard problem  [18] ,  [35] . However, sparse perturbations have the potential to enhance the stealthiness of audio attacks and provide insights into the robustness of DNNs in the audio domain, making l 0 -constrained attackers worth exploring. Nonetheless, directly adapting existing l 0 -constrained attackers from the image domain to audio faces two challenges. Firstly, the potential high dimensionality of 1-D timesequential audio signals can hinder the efficiency of the attacker. Secondly, the sequential information inherent in audio may not be effectively extracted by such attackers.\n\nTo address the aforementioned challenges, we propose STAA-Net, a generator-based adversarial attack method for end-to-end speech SER. Our approach leverages an adjusted Wave-U-Net-like generator to generate sparse audio adversarial perturbations in a single forward pass, enabling efficient and transferable attacks. We validate the effectiveness of STAA-Net through experiments on two widely-used emotional speech datasets. To the best of our knowledge, this study is the first to explore sparse adversarial attacks in the audio domain. Moreover, this work alleviates the scarcity of adversarial attack studies in SER.\n\nThe rest of the paper is organised as follows. Section 2 provides an overview of related studies on SER and adversarial attacks in the audio domain. Section 3 depicts the detailed methodology. Section 4 presents the datasets applied and experimental setups. Section 5 describes the results and provides an analysis of the findings. Finally, Section 6 concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "End-To-End Ser",
      "text": "Even though many SER systems with conventional machine learning techniques utilise hand-crafted acoustic features (e. g., Mel-frequency cepstral coefficients (MFCC) features) as input, the selection of these features can introduce bias and take extra time. In recent years, there has been a growing interest in DNNs that directly process raw audio signals, bypassing the need for manual feature extraction and potentially offering more comprehensive representations for SER tasks. Train-from-scratch Models. Tzirakis et al.  [7]  (denoted as Emo18) proposed a convolution recurrent neural network structure initially for continuous emotion recognition (e. g., arousal and valence). Emo18 is composed of 3 convolutional layers for feature extraction from the raw audio signal and 2-layer Long Short-Term Memory (LSTM) module for contextual dependencies. Zhao et al.  [5]  (denoted as Zhao19) proposed a similar network for discrete emotion recognition, and it was composed of 4 convolutional layers and 2 stacked LSTM layers. Emo18 and Zhao19 are also utilised and compared in  [36]  for the audio modality. Zhang et al.  [37]  employed an attention mechanism and a multi-task learning strategy to enhance the robustness of audio representations for emotion recognition. Sun et al.  [38]  utilised a gender information block besides the residual CNN block to improve the recognition accuracy. Tzirakis et al.  [39]  fused the high-level semantic information from Word2Vec and Speech2Vec models and low-level paralinguistic features extracted by CNN blocks for better performance. Foundation Models. By pre-training on a large amount of data, foundation models learn robust representations that capture both acoustic and linguistic properties of speech, enabling it to transfer knowledge effectively to various speech processing tasks.\n\nWav2vec 2.0 comprises a Convolutional Neural Network (CNN) module that serves as the feature encoder for latent speech representations, along with a Transformer module that captures global contextual dependencies. Wav2vec 2.0 adopts a self-supervised learning approach, where the model is trained on a massive amount of speech data with a contrastive objective to learn discriminative representations. A Wav2vec 2.0 model  [13]  has been widely adopted in SER research  [9] ,  [11] ,  [12] ,  [40] ,  [41]  because of its remarkable capability in extracting representations.\n\nThe Hidden-Unit BERT (HuBERT)  [14]  applies an architecture similar to the one of wav2vec 2.0. HuBERT also applies selfsupervised learning but with additional auxiliary tasks (e. g., framewise features predictions), which promotes the model's ability to learn combined acoustic and language features over the raw speech data. Morais et al.  [42]  fine-tuned HuBERT as upstream model to provide generated utterance embeddings for emotion classification.\n\nWavLM  [43]  builds upon the success of self-supervised pretraining in speech processing and aims to tackle full-stack speech processing tasks by leveraging large-scale unlabelled data. To better capture the sequence information in the audio, WavLM further extends the HuBERT approach by employing a gated relative position bias in the Transformer structure and augmenting the training data with an utterance mixing strategy. Feng et al.  [44]  applied the WavLM for embeddings extraction and also explored its trustworthiness.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Adversarial Attacks In The Audio Domain",
      "text": "Adversarial attacks in the audio domain can be categorised into two main types: iterative gradient-based attacks and generatorbased attacks. Iterative gradient-based attacks typically operate in a white-box setting, utilising gradient information from the victim model to iteratively find minimal perturbations that can deceive the model. Carlini et al.  [20]  used a white-box iterative optimisation-based attack to turn any audio waveform into any target transcriptions. Neekhara et al.  [21]  discovered audio-agnostic universal quasi-imperceptible adversarial perturbation through iteratively optimising the normalised Levenshtein distance for automatic speech recognition systems. Kim et al.  [23]  found the transferability of adversarial examples is related to the noise sensitivity and proposed a noise injected attack method to generate transferable adversarial examples by iteratively injecting additive noise during the gradient ascent process. Zhang et al.  [24]  employed a Projected Gradient Descent (PGD) attack with the momentum method to generate text-independent adversarial perturbations for speaker verification systems (SVS). Chen et al.  [26]  attacked SVS with a Fast Gradient Sign Method (FGSM) attack, PGD attack, Carlini-Wagner (CW) attack, and FAKEBOB  [25]  attack to address the optimisation problem.\n\nRecently, generative models such as Generative Adversarial Networks (GAN)  [45]  and autoencoders  [46]  have shown promise in generating adversarial perturbations. Compared to iterative gradientbased attackers, generator-based methods focus on learning the distributions of the training data, resulting in more transferable perturbations. Xie et al.  [28]  proposed a target attack approach on various audio tasks by concatenating the target class embedding feature map with the intermediate feature map of the generator.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "In this section, we first formulate the research problem in Section 3.1, and then introduce the proposed approach in Section 3.2. The description of the loss functions is finally given in Section 3.3.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Problem Formulation",
      "text": "We denote an original audio as x, its ground truth emotional class as y, added adversarial perturbation as δ, the victim SER model as f , and the corresponding adversarial example x adv = x + δ. In the un-targeted attack scenario applied in this work, we aim to:\n\nwhere ϵ is a pre-defined hyper-parameter that promotes the imperceptibility of the added adversarial perturbation. However, directly solving the above problem is NP-hard. Inspired by  [35] ,  [47]  in the image domain, we factorise the sparse perturbation δ to element-wise product of two vectors as follows:\n\nwhere v ∈ R N denotes the perturbation magnitudes, m ∈ {0, 1}\n\nN represents the perturbation locations, N describes the number of time frames of an audio waveform, and ⊗ denotes the elementwise product. In the training procedure, l 1 regulisation is applied on m to prompt a sparse perturbation. The above two vectors are optimised separately: one module generates v and the other module produces m. Because m is a binary vector, where the value on the time frame i is perturbed if m i = 1 and unperturbed if m i = 0, it cannot be directly optimised with gradient back-propagation. To address this issue, a 0-1 random quantisation operation is applied.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Approach",
      "text": "To introduce the proposed approach, the overall architecture is first given in Section 3.2.1, followed by the description of the two parts: i) Wave-U-Net and ii) Perturbation Magnitudes and Positions. Finally, we describe the training procedure of the generator.\n\n... ...",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Overall Architecture",
      "text": "The proposed framework is depicted in Fig.  1 . Since our framework operates on an end-to-end basis, the original raw audio is fed into the generator to produce the perturbation magnitudes and positions. The perturbation magnitudes aim to limit the values of the perturbations at each frame, while the perturbation positions are targeted to be sparse. Next, the sparse perturbations are calculated by multiplying the perturbation magnitudes with the positions. Afterwards, the sparse perturbations are added to the original audio samples to create adversarial audio samples. The training of the generator aims to fool the local threat SER models and the generated adversarial audio samples can also be transferred to attack other unseen targeted SER models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Wave-U-Net",
      "text": "U-Net  [48]  was originally developed for accurate and efficient biomedical image segmentation. It utilises a symmetric encoderdecoder structure with skip connections. WaveNet  [49] , on the other hand, is a deep generative model specifically designed for speech synthesis. It leverages the repeated application of dilated convolutions, where the dilation factors increases exponentially, to model the long-term dependencies in audio signals. However, its high memory consumption, attributed to the high sampling rate of audio, has limited its application in real-time scenarios.\n\nWave-U-Net  [50]  extends the U-Net architecture to tackle audio source separation tasks. It combines the benefits of U-Net's encoder-decoder structure and WaveNet's ability to capture longterm dependencies. As depicted in Figure  2 , in the down-sampling stage, Wave-U-Net reduces the temporal resolution by discarding features for every other time step; in the up-sampling stage, linear interpolation is employed to up-sample the feature maps while also concatenating higher-level features with local features. This approach enables Wave-U-Net to capture longer-term dependencies in audio signals.\n\nAs described in Section 1, many previous studies on audio adversarial attacks have predominantly relied on the gradient information of parameters of f with respect to x, leading to limited cross-model transferability. To address these challenges, we propose the adoption of Wave-U-Net  [50]  as the generator in this work with one down-sampling block and two up-sampling blocks: UB 1 is for perturbation magnitudes and UB 2 controls the perturbation locations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Perturbation Magnitudes And Positions",
      "text": "To generate imperceptible and sparse adversarial audio samples, we divide the final perturbation into magnitudes and positions. On the one hand, the magnitudes control the perturbation values to ensure they are extremely small, thereby making them non-distinguishable from real data. On the other hand, the positions further mitigate the negative impact on audio quality from perturbations by reducing the number of frames affected.\n\nPerturbation Magnitudes. After UB 1 , a clip operation is applied to bound the perturbation value into {-ϵ, +ϵ}, where ϵ is a pre-defined hyper-parameter to constrain the l ∞ norm. The clip operation then leads to the perturbation magnitudes.\n\nPerturbation Positions. To generate sparse perturbations, we use quantisation to convert the output of UB 2 into binary representations as the perturbation positions. Specifically, if denoting the output of UB 2 as ∂, in order to transfer it into a discrete vector m ∈ {0, 1}\n\nN , we pass ∂ into a binary quantisation as:\n\nwhere γ is a hyper-parameter. The binary quantisation described above performs well during inference, but it can encounter issues with gradient vanishing during training  [51] . To address this, a randomisation approach is introduced to ensure the gradual convergence of ∂ i towards either 1 or 0. Specifically, a random number t is generated from a uniform distribution on the interval [0, 1). If t ≥ 0.5, the binary quantisation operation is applied; otherwise, the binary quantisation is not performed.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Training Of The Generator",
      "text": "The training procedure involves feeding the adversarial example x adv into the threat SER model f for emotion classification. During this process, the generator's parameters are optimised to identify and manipulate significant patterns within the training data. These patterns possess a level of generalisability, which potentially promotes the model-independence of the crafted adversarial examples. By leveraging these generalised patterns, the crafted adversarial audio x adv can effectively deceive and attack other models. We carefully choose one of the end-to-end train-from-scratch models (i. e., Emo18) and one of the pre-trained foundation models (i. e., wav2vec 2.0) as local threat model to supervise the generator training. Notably, if the generated sparse adversarial examples, under the guidance of relatively simpler threat model, can still fool more advanced victim models successfully in an efficient manner, it would signify the strong capability of the adversary. Meanwhile, it provides insights into the vulnerabilities and robustness of advanced models when facing adversarial attacks, showcasing the importance of developing defense mechanisms against such attacks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Loss Functions Design",
      "text": "The training of the generator is guided by a combination of losses. Specifically, an adversarial loss is used to successfully attack the targeted SER model; a magnitude loss aims to make the adversarial perturbations imperceptible; a sparsity loss encourages sparse perturbations; and a quantisation loss bridges the performance gap between training and inference.\n\nAdversarial Loss. We employ the Carlini and Wagner (C&W)  [52]  loss as the primary objective function for crafting adversarial audio examples. The C&W loss aims to increase the probability of misclassification as follows:\n\nwhere ϕ is a confidence pre-defined hyper-parameter to control the attack strength. Magnitude Loss. Following the approach of previous works  [28] ,  [53] , we incorporate an l 2 norm regularisation term to ensure that the generated adversarial perturbation remains imperceptible while allowing control over the strength of the attack. This is achieved by adding the l 2 norm on the clipped perturbation, as shown in the following equation:\n\nSparsity Loss. Directly constraining the l 0 norm on the added perturbation δ is NP-hard as discussed beforehand. By factorising the δ perturbation magnitude v and perturbation locations m, we can control the sparsity of δ with the l 1 norm of m, since it only contains values 0 and 1, where 1 means the value is perturbed. Therefore, we have the sparsity loss as follows:\n\nIn this way, the sparsity of the final perturbation depends on how the generator converges. Quantisation Loss. As explained in Section 3.2.1, during the training phase, the decision to perform binary quantisation is determined by a random number sampled from a uniform distribution.\n\nIn contrast, during inference, binary quantisation is always applied. Consequently, there can be a performance disparity between the generator during training and inference. To mitigate this gap, we introduce the quantisation loss, which is defined as follows:\n\nOverall Loss. The overall loss is calculated as the weighted sum of the aforementioned losses, expressed by the equation:\n\nwhere λ m , λ s , and λ q represent the weights assigned to the magnitude loss, sparsity loss, and quantisation loss, respectively. These weights allow for fine-tuning the influence of each component in the overall loss function. By optimising the overall loss, the local threat model guides the training of the generator to generate sparse, imperceptible perturbations that achieve a high attack success rate (ASR) (definition can be found in Section 4.3). DEMoS: The DEMoS dataset  [54]  used in this study comprises approximately 7.7 hours of Italian emotional speech recordings. It involves a total of 68 speakers, including 23 females and 45 males. Without considering the 332 neutral speech samples as previous works  [9] ,  [30] , we employ the 9, 365 speech samples (average duration: 2.86 seconds ± standard deviation: 1.26 seconds), which are categorised into seven classes: anger, disgust, fear, guilt, happiness, sadness, and surprise. We split the dataset into 40 % training, 40 % validation, 30 % testing in a speaker-independent manner. The detailed emotion distribution can be found in our previous work  [9] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Implementations",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "IEMOCAP: The Interactive Emotional dyadic MOtion CAPture (IEMOCAP) database  [55]  consists of approximately 12 hours of English audio-visual recordings. Five pairs of actors (a female and a male) participate five recording sessions (i. e., 1-5) respectively by either improvising affective scenarios or performing theatrical scripts. The recorded dialogues are further manually segmented into utterances, which are categorised by at least three annotators into different emotional states, i. e., anger, disgust, excited state, fear, frustration, happiness, neutral state, sadness, and surprise.\n\nSimilar to prior works on IEMOCAP  [6] ,  [56] ,  [57] , only four classes are included in this work to mitigate the class imbalance, including anger, happiness, neutral state, sadness. Moreover, to better compare with prior works  [12] ,  [56] ,  [57] , we merge the class excited state into happiness. As a result, in total 5, 531 audio samples are applied in this work (average duration: 4.55 seconds ± standard deviation: 3.23 seconds). The majority of prior works do not set a validation dataset explicitly and perform 5-fold crossvalidation  [12] ,  [56] ,  [57] . In order to maintain consistency with our experiments on DEMoS and considering that the applied adversarial attack baselines are time-consuming, we randomly select three sessions (Session 1, 2, and 5) as the training set, one session (Session 4) as the validation set, and the remaining session (Session 3) as the test set. The emotion distribution is described in Table  1 .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Applied Adversarial Attack Baselines",
      "text": "Many prior works focus on adversarial attacks with l 2 or l ∞ constrains, perturbing all values on the time-frame axis, whereas sparse adversarial attacks with l 0 constrain target only a few values to fool the victim model.\n\nProjected Gradient Decent (PGD). Projected Gradient Decent is proposed in  [17]  and it iteratively applies small perturbations to the input data based on the gradient of the loss function with respect to the input.\n\nIn each iteration, the magnitude of the perturbation is controlled by a step size α and after each iteration, there is a projection operation clip(•) to make sure the generated current adversarial examples are within a pre-defined range (i. e., ϵ-ball).\n\nUsually, PGD is considered as an l 2 -constrained attack. However, in this work, to generate sparse adversarial perturbations, we introduce the sparsity constraint directly. Specifically, in each iteration, we only perturb a pre-defined number of positions along the time frames of the audio, while keeping the remaining positions unchanged.\n\nSparseFool. SparseFool proposed in  [18]  is a geometry inspired sparse attack method on an image. By estimating the impact of each pixel on the model's decision boundary with a linear approximation, only the pixels with the highest impact are selected for perturbation. The selected pixels are modified under a sparsity constraint (i. e., ϵ) in each iteration.\n\nOne-Pixel Attack. One pixel attack was proposed in  [19]  with the target to attack models through perturbing just one pixel of the image. Its optimisation-based method has the objective of finding the optimal pixel value and location to lead to the mis-classfication of the attacked model. The optimisation algorithms applied the most are evolutionary strategies. The one-pixel attack also shows some transferability in  [58] . In this work, we adjust the one-pixel attack method to the 1-D dimension of audio signals and a constraint of the perturbation magnitude ϵ is also applied for imperceptibility of perturbation.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experimental Settings",
      "text": "Audio Pre-processing. All audio recordings are down-sampled to 16 kHz for faster processing. For the DEMoS dataset, the audio lengths are unified to the maximum duration of 6.0 seconds by repeating shorter samples to match the desired length. As for the IEMOCAP dataset, since there is a huge difference between the maximum duration (34.1 seconds) and the duration at the 90-th percentile (8.7 seconds), all audio durations are unified to 8.7 seconds by removing extra signals and repeating shorter samples as necessary. Models Preparation. In this work, we experiment with four widely applied end-to-end models with raw audio waves as input: Emo18, Zhao19, wav2vec 2.0, and WavLM. The batch size is 8 and the model development is supervised by cross-entropy loss. The Emo18 and Zhao19 models are trained from scratch. The training process is optimised with the Adam optimiser with an initial learning rate of 1e -3 and stopped after 30 epochs. The wav2vec 2.0 and WavLM models used in this study have been pre-trained on the 960-hour Librispeech corpus  [15] . For the fine-tuning of SER, the feature encoder is frozen, and the classification head consisting of two linear layers is added to make predictions based on the learnt representations. The fine-tuning procedure employs the Adam optimiser with an initial learning rate of 3e -5, and is stopped after 20 epochs. Implementations of Applied Adversarial Attack Baselines. The PGD, SparseFool, and one-pixel attack 12 methods are adjusted based on their pytorch implementations. For PGD, the number of perturbed locations on the time axis of the audio waveform is set to achieve comparable sparsity with the proposed STAA-Net. The one-pixel attack method specifically sets this number as 1, while SparseFool dynamically determines the number of perturbed locations. To maintain consistency, a maximum of 20 iteration steps is set for all three methods. Additionally, to maintain stealthiness as in STAA-Net, the perturbation bound ϵ is set to 0.05 for the DEMoS and 0.01 for IEMOCAP datasets. Generator Training. According to their model complexity (number of parameters), we divide the above four models into two sub-groups: Emo18 (1.30 M) and Zhao19 (1.01 M), Wav2vec 2.0 (90.37 M) and WavLM (90.38 M). In our study, we carefully select Emo18 and Wav2vec 2.0 as the local threat models to guide the training of our generator. These models provide a solid foundation for supervising the generation of adversarial examples. Additionally, we utilise Zhao19 and WavLM as the victim models to better evaluate the effectiveness of the transferred adversarial examples.\n\nThe weights assigned to the magnitude loss λ m , sparsity loss λ s , and quantisation loss λ q are set as 1e -3, 1e -4, and 1e -4, respectively. As for the binary quantisation, the γ is set as 0.5. When training the generator using end-to-end train-from-scratch models on the IEMOCAP dataset, we observe that the performance 1. https://github.com/Harry24k/adversarial-attacks-pytorch 2. https://github.com/DebangLi/one-pixel-attack-pytorch of Emo18 is not as good as of the pre-trained models. Consequently, we decide to lower the weights for λ m , λ s and λ q by a factor of 0.1. For the clip operation, we set the bound ϵ as 0.05 for DEMoS and 0.01 for IEMOCAP. This operation ensures that the attack on the IEMOCAP dataset is relatively less potent compared to the attack on the DEMoS dataset.\n\nThe batch size is set to 8 and we utilise the 'Adam' optimiser with an initial learning rate of 1e -4. The learning rate is decayed by a factor of 0.5 every 5 epochs to facilitate convergence. The generator training process is stopped after 20 epochs. Evaluations Metrics. (1) Unweighted Average Recall (UAR) is utilised as the standard evaluation metric to mitigate the class imbalance issue  [59] , apart from accuracy (i. e., weighted average recall). (  2 ) Attack Success Rate (ASR) calculates the ratio of the number of adversarial examples causing a miss-classification to the number of total adversarial ones, describing the fooling power of attackers. (  3 ) Signal-to-Noise Ratio (SNR) expressed in decibels (dB) measures the relative noise level of perturbation δ i to the original audio\n\nmax(δi) . The larger the SNR, the more imperceptible the added perturbation. According to  [60] , an SNR (dB) value close to 20 or larger can be regarded as human imperceptible.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ser Models Results",
      "text": "The Emo18 and Zhao19 models were trained from scratch, while the Wav2vec 2.0 and WavLM models were fine-tuned on the DEMoS dataset. The performance of these models is summarised in Table  2 . In comparison to previous works on DEMoS, our applied models demonstrate comparable or even superior performances.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Models",
      "text": "Validation Test Chang et al.  [9]  74.9 / 70.0 85.9 / 78.9 Ren et al.  [61]  -/ 87. For the IEMOCAP dataset, we assess the performance of our applied models in comparison to the state-of-the-art (SOTA) 4class emotion recognition methods, as shown in Table  3 . It is worth noting that the majority of previous works  [11]  on IEMOCAP employed a dataset split where one session was used as the test dataset and the remaining four sessions were used for training. Some studies (e. g.,  [44] ,  [62] ) also utilised 5-fold cross-validation. While our fine-tuned results are comparable to previous works, it is important to consider that differences in performance may arise due to variations in the dataset split. It is important to highlight that this work primarily focuses on the models' performance in the adversarial attack settings, rather than solely on IEMOCAP.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Attack Results",
      "text": "As shown in Table  4 , with Emo18 and wav2vec 2.0 as the threat model to train the generator, the produced adversarial examples are used to test on Emo18 and wav2vec 2.0 as white-box attack and   [60] . However, the one-pixel attack with wav2vec 2.0 as threat model achieves higher SNR (i. e., around 37 dB), which might be caused by the randomness of just one value on the time frame axis.\n\nIn terms of speed, the proposed generator-based method demonstrates rapid generation of adversarial examples (i. e., 0.01 seconds), which is more than 100 times faster than the PGD attacker when the threat model is Emo18 and about 73 times faster on the validation dataset and 57 times faster on the test dataset when the threat model is wav2vec 2.0. Notably, when the attacker is SparseFool and one-pixel, the speed is quite slow, from 8.33 seconds to 418.36 seconds to generate one single adversarial examples. Our proposed STAA-Net is quite efficient in this regard. This is essential because in practical attack scenarios, attackers would prefer to quickly generate the adversarial perturbation using mobile devices and inject it into the victim's ongoing speech since attackers usually do not have the opportunities to record and modify the whole speech in real time. This requires a highly efficient method with low computational complexity to craft robust adversarial perturbations within a limited time budget.\n\nSparsity and attack performances sometimes are trade-offs, but the proposed STAA-Net can achieve a balance between them. In Table  4 , when both the threat model and victim model are Emo18, we can see that val ASR 82.71% and test ASR 78.50% by STAA-Net is considerably better than those of PGD, SparseFool, and onepixel with sparser purturbations (i. e., val sparsity 12.50% and test sparsity 6.25%). By limiting the number of perturbation locations to be one, the one-pixel attacker fails to attack any of the audio samples in DEMoS. When attacking other models with Emo18 as the threat model, STAA-Net achieves better transferability. Specifically, the validation and test ASR on Zhao19, wav2vec 2.0, and WavLM are the best, compared with other attackers respectively. Notably, even though the threat model is relatively simple (i. e., Emo18), when using generated adversarial examples to attack more advanced models (i. e., wav2vec 2.0 and WavLM), the ASR drops but is still quite high. This further proves the effectiveness of the proposed STAA-Net. When the threat model is wav2vec 2.0, if the victim model is also wav2vec 2.0, we can find that ASRs achieved by STAA-Net (i. e., val ASR 80.02% and test ASR 76.09% ) are lower than the ones by PGD (i. e., val ASR 98.46% and test ASR 99.05%). However, the transferability of the  STAA-Net is better and the sparsity of the perturbation is quite lower (i. e., validation sparsity 11.88% vs 22.29% and test sparsity 12.93% vs 23.13%). Interestingly, when the threat model (i. e., wav2vec 2.0) is relatively more advanced than the victim model, the ASR increases a little bit. This might be caused by the strong capability of wav2vec 2.0 for latent representations extraction.\n\nTo further validate the effectiveness of STAA-Net, we extend our experiments on the second emotion dataset IEMOCAP and the corresponding results can be found in Table  5 . In terms of speed, the proposed STAA-Net is the fastest when generating adversarial examples. For SNR, the STAA-Net generates imperceptible adversarial examples, especially when the threat model is wav2vec 2.0. As for the sparsity of the adversarial examples, the STAA-Netgenerated adversarial examples are the most sparse, whereas the SparseFool generates quite dense perturbations. When the attack is white-box and the threat model is Emo18, even though the PGD attacker achieves better test ASR (i. e., 80.28%), the perturbation is denser and transferability is worse. When the threat model is wav2vec 2.0, we can see that adversarial examples generated by STAA-Net have the best transferability and sparsity of perturbations. Sparsefool also fails to generate sparse perturbations.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "We further assess the contributions of the main components within our proposed framework. To this end, we selected Emo18 and wav2vec 2.0 as the local threat models, and the DEMoS dataset as our experimental dataset. The results are indicated in Table  6 . The generation of adversarial examples maintains a consistent speed, and there is only little observed alteration in the SNR. Effects of Factorisation. In order to assess the effectiveness of the factorisation approach, we remove the up-sampling block UB 2 in Figure  1  from our architecture, which controls the perturbation locations, and solely use the output of the other up-sampling block UB 1 to generate the final perturbation v. Specifically, we retained the magnitude loss L mag , but calculated the sparsity loss as L spa = ∥v∥ 1 , while the quantisation loss was omitted. The results, presented in Table  6 , demonstrate that the generated adversarial examples achieve 100% sparsity. This outcome suggests that directly applying l 1 regularisation alone does not yield sparse solutions in our study. Furthermore, this ablation study emphasises the significance of UB 2 in effectively controlling the locations of the perturbations within the generated adversarial examples. Effects of Magnitude Loss. The purpose of using magnitude loss is two-fold: one is to control the attack strength and the second is to alleviate extremely large perturbations. Without the magnitude loss, the generator is not stable and can produce perturbation with all values as 0 with Emo18 as local threat model on the test dataset of DEMoS as shown in Table  6 . Furthermore, it leads to poor ASR performances. Effects of Sparsity Loss. Without the sparsity loss, the generated adversarial perturbation is fully dense (i. e., sparsity = 100 %). Specifically, when the threat model is Emo18 and the adversarial examples are transferred to attack wav2vec 2.0 and WavLM, the test ASR drops considerably. Effects of Quantisation Loss. Without the quantisation loss, we can see that the sparsity increases, especially when the threat model is wav2vec 2.0.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "The proposed STAA-Net, while offering advantages in generating sparse and transferable perturbations efficiently, has several limitations that should be taken into account. Firstly, the selection and fine-tuning of weights for different losses, such as λ m , λ s , and λ q , require careful consideration. Finding the optimal balance between these weights is crucial for achieving desired results. Secondly, although many prior works  [21] ,  [24] ,  [26]  on adversarial attacks in the audio domain primarily use SNR to evaluate the imperceptibility of the adversarial perturbation, there may be scenarios where stealthiness is more critical than ASR. In such cases, additional  evaluation metrics, such as human evaluation, can provide a more comprehensive assessment. Lastly, the availability and quantity of training data for the generator can impact its performance. Insufficient or imbalanced data may limit the generator's ability to learn effectively and generate high-quality adversarial examples.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In conclusion, the field of speech emotion recognition (SER) lacks sufficient research on adversarial attacks, with most existing attacks in the audio domain primarily focusing on l 2 or l ∞ norm constraints. To address this gap, we proposed STAA-Net, a generator-based attacker that efficiently generates transferable and sparse adversarial perturbations in an end-to-end manner. We trained the generator using Emo18 and WavLM as threat models and produced adversarial examples in a single forward pass. The generated adversarial examples were then used to attack the considered Zhao19 and wav2vec 2.0 models. Experimental results on the DEMoS and IEMOCAP datasets demonstrated the effectiveness of STAA-Net in achieving a balance between sparsity, speed, imperceptibility, transferability, and attack success rate.\n\nIn terms of future research directions, there are several directions that can be explored. Firstly, while this work primarily focused on non-targeted adversarial attacks, it would be valuable to investigate targeted attacks and evaluate the efficiency of the generator in producing transferable and sparse audio adversarial examples that are specifically designed to deceive a particular victim model or class. Secondly, there is room for exploring the applicability and performance in other audio tasks, such as automatic speech recognition. Investigating the effectiveness of the method across different audio domains would help assess its versatility and generalisation capabilities. Thirdly, exploring potential defense mechanisms against such adversarial attacks is an interesting area to investigate, which can contribute to the overall robustness of SER models. Lastly, the exploration of automatic weight determination for different loss components is a worthwhile avenue for future investigation.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall architecture of the proposed STAA-Net.",
      "page": 3
    },
    {
      "caption": "Figure 2: Wave-U-Net architecture. →describes down- and up-sampling;",
      "page": 3
    },
    {
      "caption": "Figure 1: Since our framework",
      "page": 3
    },
    {
      "caption": "Figure 2: , in the down-sampling",
      "page": 3
    },
    {
      "caption": "Figure 3: Comparison of the waveforms and log Mel spectrograms of",
      "page": 7
    },
    {
      "caption": "Figure 1: from our architecture, which controls the perturbation",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "Liu et al.\n[10]",
          "Validation": "- / -",
          "Test": "64.8 / -"
        },
        {
          "Models": "Pepino et al.\n[40]",
          "Validation": "- / -",
          "Test": "- / 67.2"
        },
        {
          "Models": "Chen et al.\n[12]",
          "Validation": "- / -",
          "Test": "- / 74.3"
        },
        {
          "Models": "Santoso et al.\n[62]",
          "Validation": "- / -",
          "Test": "- / 75.9"
        },
        {
          "Models": "Emo18",
          "Validation": "54.03 / 52.81",
          "Test": "51.26 / 52.40"
        },
        {
          "Models": "Zhao19",
          "Validation": "53.93 / 54.17",
          "Test": "52.48 / 52.74"
        },
        {
          "Models": "Wav2vec 2.0",
          "Validation": "67.12 / 68.07",
          "Test": "66.46/ 66.74"
        },
        {
          "Models": "WavLM",
          "Validation": "68.28 / 67.08",
          "Test": "67.07 / 66.90"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "Chang et al.\n[9]",
          "Validation": "74.9 / 70.0",
          "Test": "85.9 / 78.9"
        },
        {
          "Models": "Ren et al.\n[61]",
          "Validation": "-/ 87.5",
          "Test": "-/ 86.7"
        },
        {
          "Models": "Ren et al.\n[8]",
          "Validation": "-/ 91.8",
          "Test": "-/ 91.4"
        },
        {
          "Models": "Emo18",
          "Validation": "78.18 / 77.39",
          "Test": "79.31 / 78.80"
        },
        {
          "Models": "Zhao19",
          "Validation": "76.40 / 75.71",
          "Test": "72.39 / 72.10"
        },
        {
          "Models": "Wav2vec 2.0",
          "Validation": "92.99 / 92.82",
          "Test": "91.01 / 91.01"
        },
        {
          "Models": "WavLM",
          "Validation": "92.48 / 92.30",
          "Test": "92.26 / 92.41"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 5: Intermsofspeed,",
      "data": [
        {
          "Threat Model": "",
          "Attacker": "",
          "SNR (dB)": "Val",
          "Sparsity (%)": "Val",
          "Speed (s)": "Val",
          "Emo18 (%)": "Val",
          "Zhao19 (%)\nWav2vec 2.0 (%)\nWavLM (%)": "Val"
        },
        {
          "Threat Model": "Emo18",
          "Attacker": "PGD\nSparseFool\nOne-Pixel\nSTAA-Net",
          "SNR (dB)": "16.87\n16.64\n–\n16.86",
          "Sparsity (%)": "14.58\n95.43\n–\n12.50",
          "Speed (s)": "1.10\n18.90\n29.88\n0.01",
          "Emo18 (%)": "77.85\n12.87\n0.00\n82.71",
          "Zhao19 (%)\nWav2vec 2.0 (%)\nWavLM (%)": "67.09\n9.67\n0.00\n84.95"
        },
        {
          "Threat Model": "Wav2vec 2.0",
          "Attacker": "PGD\nSparseFool\nOne-Pixel\nSTAA-Net",
          "SNR (dB)": "16.87\n16.69\n36.88\n16.87",
          "Sparsity (%)": "22.29\n99.61\n1 (p)\n11.88",
          "Speed (s)": "0.73\n90.64\n82.93\n0.01",
          "Emo18 (%)": "79.93\n53.40\n0.00\n82.84",
          "Zhao19 (%)\nWav2vec 2.0 (%)\nWavLM (%)": "81.11\n53.88\n0.00\n84.44"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Threat Model": "",
          "Attacker": "",
          "SNR (dB)": "Val",
          "Sparsity (%)": "Val",
          "Speed (s)": "Val",
          "Emo18 (%)": "Val",
          "Zhao19 (%)\nWav2vec 2.0 (%)\nWavLM (%)": "Val"
        },
        {
          "Threat Model": "Emo18",
          "Attacker": "PGD\nSparseFool\nOne-Pixel\nSTAA-Net",
          "SNR (dB)": "23.83\n15.69\n–\n22.99",
          "Sparsity (%)": "35.96\n99.96\n–\n24.34",
          "Speed (s)": "2.20\n162.58\n40.50\n0.01",
          "Emo18 (%)": "47.14\n9.70\n0.00\n58.68",
          "Zhao19 (%)\nWav2vec 2.0 (%)\nWavLM (%)": "44.42\n9.70\n0.00\n53.15"
        },
        {
          "Threat Model": "Wav2vec 2.0",
          "Attacker": "PGD\nSparseFool\nOne-Pixel\nSTAA-Net",
          "SNR (dB)": "26.34\n27.51\n40.83\n24.74",
          "Sparsity (%)": "8.27\n98.41\n1 (p)\n3.12",
          "Speed (s)": "0.78\n247.38\n61.64\n0.01",
          "Emo18 (%)": "17.75\n26.77\n0.00\n33.95",
          "Zhao19 (%)\nWav2vec 2.0 (%)\nWavLM (%)": "19.50\n24.64\n0.00\n27.64"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Threat Model": "",
          "Attacker": "",
          "SNR (dB)": "Val",
          "Sparsity (%)": "Val",
          "Speed (s)": "Val",
          "Emo18 (%)": "Val",
          "Zhao19 (%)\nWav2vec 2.0 (%)\nWavLM (%)": "Val"
        },
        {
          "Threat Model": "Emo18",
          "Attacker": "w/o factorisation\nw/o magnitude loss\nw/o sparsity loss\nw/o quantisation loss\nSTAA-Net",
          "SNR (dB)": "16.92\n16.86\n16.96\n16.87\n16.86",
          "Sparsity (%)": "100.00\n12.50\n100.00\n12.48\n12.50",
          "Speed (s)": "0.01\n0.01\n0.01\n0.01\n0.01",
          "Emo18 (%)": "82.99\n83.26\n82.90\n83.14\n82.71",
          "Zhao19 (%)\nWav2vec 2.0 (%)\nWavLM (%)": "80.33\n86.34\n80.02\n85.31\n84.95"
        },
        {
          "Threat Model": "Wav2vec 2.0",
          "Attacker": "w/o factorisation\nw/o magnitude loss\nw/o sparsity loss\nw/o quantisation loss\nSTAA-Net",
          "SNR (dB)": "16.88\n16.87\n16.91\n16.87\n16.87",
          "Sparsity (%)": "100.00\n22.10\n100.00\n20.40\n11.88",
          "Speed (s)": "0.01\n0.01\n0.01\n0.01\n0.01",
          "Emo18 (%)": "82.77\n82.74\n83.35\n83.02\n82.84",
          "Zhao19 (%)\nWav2vec 2.0 (%)\nWavLM (%)": "84.38\n85.10\n84.47\n85.62\n84.44"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "2",
      "title": "Speech technology for healthcare: Opportunities, challenges, and state of the art",
      "authors": [
        "S Latif",
        "J Qadir",
        "A Qayyum",
        "M Usama",
        "S Younis"
      ],
      "year": "2021",
      "venue": "IEEE Reviews in Biomedical Engineering"
    },
    {
      "citation_id": "3",
      "title": "Audiovisual analysis for recognising frustration during Game-Play: Introducing the multimodal game frustration database",
      "authors": [
        "M Song",
        "Z Yang",
        "A Baird",
        "E Parada-Cabaleiro",
        "Z Zhang",
        "Z Zhao",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proc. ACII"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition method for call/contact centre systems",
      "authors": [
        "M Płaza",
        "R Kazała",
        "Z Koruba",
        "M Kozłowski",
        "M Lucińska",
        "K Sitek",
        "J Spyrka"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using deep 1D & 2D CNN LSTM networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition from 3D log-Mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "7",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "8",
      "title": "Fast yet effective speech emotion recognition with self-distillation",
      "authors": [
        "Z Ren",
        "T Nguyen",
        "Y Chang",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "9",
      "title": "Knowledge transfer for on-device speech emotion recognition with neural structured learning",
      "authors": [
        "Y Chang",
        "Z Ren",
        "T Nguyen",
        "K Qian",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP, Rhodes island"
    },
    {
      "citation_id": "10",
      "title": "Dual-TBNet: Improving the robustness of speech features via dual-transformer-bilstm for speech emotion recognition",
      "authors": [
        "Z Liu",
        "X Kang",
        "F Ren"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP, Rhodes island"
    },
    {
      "citation_id": "13",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "14",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "16",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "I Goodfellow",
        "J Shlens",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "17",
      "title": "Towards deep learning models resistant to adversarial attacks",
      "authors": [
        "A Madry",
        "A Makelov",
        "L Schmidt",
        "D Tsipras",
        "A Vladu"
      ],
      "year": "2018",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "18",
      "title": "SparseFool: A few pixels make a big difference",
      "authors": [
        "A Modas",
        "S.-M Moosavi-Dezfooli",
        "P Frossard"
      ],
      "year": "2019",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "19",
      "title": "One pixel attack for fooling deep neural networks",
      "authors": [
        "J Su",
        "D Vargas",
        "K Sakurai"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Evolutionary Computation"
    },
    {
      "citation_id": "20",
      "title": "Audio adversarial examples: Targeted attacks on speech-to-text",
      "authors": [
        "N Carlini",
        "D Wagner"
      ],
      "year": "2018",
      "venue": "Proc. SPW"
    },
    {
      "citation_id": "21",
      "title": "Universal adversarial perturbations for speech recognition systems",
      "authors": [
        "P Neekhara",
        "S Hussain",
        "P Pandey",
        "S Dubnov",
        "J Mcauley",
        "F Koushanfar"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "SPECPATCH: Human-in-the-loop adversarial audio spectrogram patch attack on speech recognition",
      "authors": [
        "H Guo",
        "Y Wang",
        "N Ivanov",
        "L Xiao",
        "Q Yan"
      ],
      "year": "2022",
      "venue": "Proc. CCS"
    },
    {
      "citation_id": "23",
      "title": "Generating transferable adversarial examples for speech classification",
      "authors": [
        "H Kim",
        "J Park",
        "J Lee"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Attack on practical speaker verification system using universal adversarial perturbations",
      "authors": [
        "W Zhang",
        "S Zhao",
        "L Liu",
        "J Li",
        "X Cheng",
        "T Zheng",
        "X Hu"
      ],
      "year": "2021",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "25",
      "title": "Who is real Bob? Adversarial attacks on speaker recognition systems",
      "authors": [
        "G Chen",
        "S Chenb",
        "L Fan",
        "X Du",
        "Z Zhao",
        "F Song",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "Proc . 2021 IEEE SP"
    },
    {
      "citation_id": "26",
      "title": "AS2T: Arbitrary source-to-target adversarial attack on speaker recognition systems",
      "authors": [
        "G Chen",
        "Z Zhao",
        "F Song",
        "S Chen",
        "L Fan",
        "Y Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Dependable and Secure Computing"
    },
    {
      "citation_id": "27",
      "title": "PhoneyTalker: An out-of-the-box toolkit for adversarial example attack on speaker recognition",
      "authors": [
        "M Chen",
        "L Lu",
        "Z Ba",
        "K Ren"
      ],
      "year": "2022",
      "venue": "Proc. INFOCOM"
    },
    {
      "citation_id": "28",
      "title": "Enabling fast and universal audio adversarial attack using generative model",
      "authors": [
        "Y Xie",
        "Z Li",
        "C Shi",
        "J Liu",
        "Y Chen",
        "B Yuan"
      ],
      "year": "2021",
      "venue": "Proc. AAAI, Virtual Event"
    },
    {
      "citation_id": "29",
      "title": "Poster: Adversarial examples for hate speech classifiers",
      "authors": [
        "R Oak"
      ],
      "year": "2019",
      "venue": "Proc. CCS"
    },
    {
      "citation_id": "30",
      "title": "Enhancing transferability of black-box adversarial attacks via lifelong learning for speech emotion recognition models",
      "authors": [
        "Z Ren",
        "J Han",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "31",
      "title": "Robust federated learning against adversarial attacks for speech emotion recognition",
      "authors": [
        "Y Chang",
        "S Laridi",
        "Z Ren",
        "G Palmer",
        "B Schuller",
        "M Fisichella"
      ],
      "year": "2022",
      "venue": "Robust federated learning against adversarial attacks for speech emotion recognition"
    },
    {
      "citation_id": "32",
      "title": "Imperceptible, robust, and targeted adversarial examples for automatic speech recognition",
      "authors": [
        "Y Qin",
        "N Carlini",
        "G Cottrell",
        "I Goodfellow",
        "C Raffel"
      ],
      "year": "2019",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "33",
      "title": "Voiceprint mimicry attack towards speaker verification system in smart home",
      "authors": [
        "L Zhang",
        "Y Meng",
        "J Yu",
        "C Xiang",
        "B Falk",
        "H Zhu"
      ],
      "year": "2020",
      "venue": "Proc. INFOCOM"
    },
    {
      "citation_id": "34",
      "title": "Adversarial attack and defense strategies for deep speaker recognition systems",
      "authors": [
        "A Jati",
        "C.-C Hsu",
        "M Pal",
        "R Peri",
        "W Abdalmageed",
        "S Narayanan"
      ],
      "year": "2021",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "35",
      "title": "Transferable sparse adversarial attack",
      "authors": [
        "Z He",
        "W Wang",
        "J Dong",
        "T Tan"
      ],
      "year": "2022",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "36",
      "title": "End-to-end multimodal affect recognition in real-world environments",
      "authors": [
        "P Tzirakis",
        "J Chen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "37",
      "title": "Attention-augmented end-to-end multi-task learning for emotion prediction from speech",
      "authors": [
        "Z Zhang",
        "B Wu",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "38",
      "title": "End-to-end speech emotion recognition with gender information",
      "authors": [
        "T.-W Sun"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "39",
      "title": "Speech emotion recognition using semantic information",
      "authors": [
        "P Tzirakis",
        "A Nguyen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "40",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "41",
      "title": "Speech emotion recognition with multi-task learning",
      "authors": [
        "X Cai",
        "J Yuan",
        "R Zheng",
        "L Huang",
        "K Church"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "42",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Morais"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "43",
      "title": "WavLM: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "TrustSER: On the trustworthiness of fine-tuning pre-trained speech embeddings for speech emotion recognition",
      "authors": [
        "T Feng",
        "R Hebbar",
        "S Narayanan"
      ],
      "year": "2023",
      "venue": "TrustSER: On the trustworthiness of fine-tuning pre-trained speech embeddings for speech emotion recognition",
      "arxiv": "arXiv:2305.11229"
    },
    {
      "citation_id": "45",
      "title": "Generative adversarial networks",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2020",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "46",
      "title": "Extracting and composing robust features with denoising autoencoders",
      "authors": [
        "P Vincent",
        "H Larochelle",
        "Y Bengio",
        "P.-A Manzagol"
      ],
      "year": "2008",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "47",
      "title": "Sparse adversarial attack via perturbation factorization",
      "authors": [
        "Y Fan",
        "B Wu",
        "T Li",
        "Y Zhang",
        "M Li",
        "Z Li",
        "Y Yang"
      ],
      "year": "2020",
      "venue": "Proc. ECCV, Virtual Event"
    },
    {
      "citation_id": "48",
      "title": "Singing voice separation with deep U-Net convolutional networks",
      "authors": [
        "A Jansson",
        "E Humphrey",
        "N Montecchio",
        "R Bittner",
        "A Kumar",
        "T Weyde"
      ],
      "year": "2017",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "49",
      "title": "WaveNet: A generative model for raw audio",
      "authors": [
        "A Van Den Oord",
        "S Dieleman",
        "H Zen",
        "K Simonyan",
        "O Vinyals",
        "A Graves",
        "N Kalchbrenner",
        "A Senior",
        "K Kavukcuoglu"
      ],
      "year": "2016",
      "venue": "Proc. ISCA Workshop on SSW9"
    },
    {
      "citation_id": "50",
      "title": "Wave-U-Net: A multi-scale neural network for end-to-end audio source separation",
      "authors": [
        "S Daniel Stoller",
        "Sebastian Ewert"
      ],
      "year": "2018",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "51",
      "title": "Improved gradient-based adversarial attacks for quantized networks",
      "authors": [
        "K Gupta",
        "T Ajanthan"
      ],
      "year": "2022",
      "venue": "Proc. AAAI"
    },
    {
      "citation_id": "52",
      "title": "Towards evaluating the robustness of neural networks",
      "authors": [
        "N Carlini",
        "D Wagner"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Symposium on SP"
    },
    {
      "citation_id": "53",
      "title": "AdvPulse: Universal, synchronization-free, and targeted audio adversarial attacks via subsecond perturbations",
      "authors": [
        "Z Li",
        "Y Wu",
        "J Liu",
        "Y Chen",
        "B Yuan"
      ],
      "year": "2020",
      "venue": "Proc. CCS, Virtual Event"
    },
    {
      "citation_id": "54",
      "title": "DEMoS: An italian emotional speech corpus",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "55",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "56",
      "title": "Fusing ASR outputs in joint training for speech emotion recognition",
      "authors": [
        "Y Li",
        "P Bell",
        "C Lai"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "57",
      "title": "An adversarial training based speech emotion classifier with isolated gaussian regularization",
      "authors": [
        "C Fu",
        "C Liu",
        "C Ishi",
        "H Ishiguro"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "58",
      "title": "On the transferability of adversarial perturbation attacks against fingerprint based authentication systems",
      "authors": [
        "S Marrone",
        "C Sansone"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "59",
      "title": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing",
      "authors": [
        "B Schuller",
        "A Batliner"
      ],
      "year": "2013",
      "venue": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing"
    },
    {
      "citation_id": "60",
      "title": "SirenAttack: Generating adversarial audio for end-to-end acoustic systems",
      "authors": [
        "T Du",
        "S Ji",
        "J Li",
        "Q Gu",
        "T Wang",
        "R Beyah"
      ],
      "year": "2020",
      "venue": "Proc. ASIA CCS"
    },
    {
      "citation_id": "61",
      "title": "Generating and protecting against adversarial attacks for deep speech-based emotion recognition models",
      "authors": [
        "Z Ren",
        "A Baird",
        "J Han",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "62",
      "title": "Speech emotion recognition based on attention weight correction using word-level confidence measure",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    }
  ]
}