{
  "paper_id": "2407.00119v2",
  "title": "Efficient Long-Distance Latent Relation-Aware Graph Neural Network For Multi-Modal Emotion Recognition In Conversations",
  "published": "2024-06-27T15:54:12Z",
  "authors": [
    "Yuntao Shou",
    "Wei Ai",
    "Jiayi Du",
    "Tao Meng",
    "Haiyan Liu",
    "Nan Yin"
  ],
  "keywords": [
    "Graph Neural Network",
    "Multi-modal Emotion Recognition",
    "Relation-aware",
    "Efficiency",
    "Information Fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The task of multi-modal emotion recognition in conversation (MERC) aims to analyze the genuine emotional state of each utterance based on the multi-modal information in the conversation, which is crucial for conversation understanding. Existing methods focus on using graph neural networks (GNN) to model conversational relationships and capture contextual latent semantic relationships. However, due to the complexity of GNN, existing methods cannot efficiently capture the potential dependencies between long-distance utterances, which limits the performance of MERC. In this paper, we propose an Efficient Long-distance Latent Relation-aware Graph Neural Network (ELR-GNN) for multi-modal emotion recognition in conversations. Specifically, we first use pre-extracted text, video and audio features as input to Bi-LSTM to capture contextual semantic information and obtain low-level utterance features. Then, we use low-level utterance features to construct a conversational emotion interaction graph. To efficiently capture the potential dependencies between long-distance utterances, we use the dilated generalized forward push algorithm to precompute the emotional propagation between global utterances and design an emotional relation-aware operator to capture the potential semantic associations between different utterances. Furthermore, we combine early fusion and adaptive late fusion mechanisms to fuse latent dependency information between speaker relationship information and context. Finally, we obtain high-level discourse features and feed them into MLP for emotion prediction. Extensive experimental results show that ELR-GNN achieves state-of-the-art performance on the benchmark datasets IEMOCAP and MELD, with running times reduced by 52% and 35%, respectively. In addition, ELR-GNN can effectively improve the accuracy of the MERC task by capturing and fusing the latent semantic relationships between utterances.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multi-modal emotion recognition in conversations (ME-RC) has received research attention  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  due to its wide application in the fields of intelligent customer service and emotion analysis  [11] , humancomputer interaction (HCI)  [12] , and security monitoring  [13] . For instance, in HCI, MERC can help computers better understand the emotional state of human users, thereby enabling more intelligent interactions and improving user experience. Unlike traditional non-conversational or unimodal emotion recognition tasks  [14] , MERC requires identifying the speaker's genuine emotions based on textual, auditory, and visual information in the conversation utterances  [15] .\n\nThe current mainstream research methods mainly use RNN, Transformer, and GCN to model conversation context and multi-modal information in MERC. For example, DialogueRNN  [16]  uses a sequential approach to track conversation context and captures the most important emotional features through a memory mechanism. Although RNN-based methods can model the speaker's contextual information, they have limited memory ability for longdistance conversations, which limits the application of RNN in MERC tasks  [17] . To solve the above problems, the Transformer architecture  [18]  is proposed to model longdistance context dependencies in MERC. For instance, CT-1 arXiv:2407.00119v2  [cs. LG] 31 Aug 2024\n\nNet  [19]  builds a Single Transformer and Cross Transformer to capture long-distance context dependencies and realize intra-module and inter-module information interaction for emotion recognition. However, methods based on Transformer architecture ignore conversational relationship information between speakers, which limits the model's emotion recognition performance  [20, 21] . To tackle this limitation, many GCN methods have been proposed to model interaction information between speakers. For example, DialogueGCN  [22]  uses a graph structure to model conversation context and uses GCN to learn conversation graphs to achieve semantic understanding and emotional recognition of conversations. In addition, LR-GCN  [23]  believes that the context latent dependencies of utterences should also be considered. LR-GCN uses multi-head attention to construct multiple full association graphs to model potential conversational relationships, and then uses GCN to learn latent relationships to achieve emotion recognition. However, limited by the complexity of GCN, these methods usually adopt a fixed window size strategy and then fully connect the utterances within the window to construct a conversation graph, which significantly limits the ability to obtain long-distance contextual information.\n\nInspired by LR-GCN, we also use GCN to model dialogue relationship information between speakers for MERC. Furthermore, long-distance context potential dependencies can provide more information for emotion classification and help reveal the genuine emotion of utterances. Therefore, how to comprehensively consider long-distance contextual dependencies while ensuring that the number of model parameters does not increase dramatically remains a challenge.\n\nIn this paper, we propose an Efficient Long-distance Latent Relation-aware Graph Neural Network (ELR-GNN) for multi-modal emotion recognition in conversation. Specifically, we first use RoBERTa, 3D-CNN, and openSMILE to perform pre-feature extraction of text, video, and audio features, respectively. Next, we use Bi-LSTM to capture contextual semantic information and obtain low-level utterance features. We then use low-level utterence features to construct a speaker graph. In the constructed speaker relationship graph, low-level utterence features are used as node features, while dialogue relationship information between speakers is used for edge construction. To capture the latent dependency information between long-distance contexts, we use the graph random neural network algorithm to randomly sample top-k nodes for information extraction. In addition, we combine early fusion and adaptive late fusion mechanisms to simultaneously fuse speaker relationship information and latent dependency information between contexts. Finally, we fine-grained obtained high-level utterance features and fed them into the MLP and softmax function for emotion prediction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "• We Propose A Novel Efficient Long-Distance Latent",
      "text": "Relation-aware Graph Neural Network (ELR-GNN) for MER. ELR-GCN not only considers conversational relationship information between speakers, but also captures long-distance context latent dependency information.\n\n• We propose a graph random neural network architecture in which long-distance latent dependencies between utterances are captured by randomly sampling top-k node features. Furthermore, we combine early fusion and adaptive late fusion mechanisms to simultaneously exploit speaker information and context's latent dependency information during information propagation.\n\n• We perform extensive experiments on two publicly available datasets to verify the effectiveness of the ELR-GNN method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Modal Emotion Recognition",
      "text": "Single-modality emotion recognition may be limited. For example, text-based emotion recognition alone may not capture the emotional cues in speech and facial expressions  [24] . Multimodal emotion recognition (MER) can integrate multiple information sources to improve the accuracy and robustness of emotion recognition.\n\nCurrent mainstream MER research mainly focuses on RNN, Transformer and GCN. For instance, DialogueRNN  [16]  modeled individual speakers and uses three different GRUs to achieve more effective correlations between speakers. DialogueGCN  [22]  was proposed to solve the problem that RNN-based methods cannot consider the dialogue relationship between speakers. DialogueGCN improves the performance of MER by modeling the interactive relationship between speakers through the inherent properties of the graph structure and using graph convolution operations to transfer contextual semantic information. TL-ERC  [25]  used transfer learning methods to solve problems in supervised learning that require large amounts of high-quality annotated data. CTNet  [19]  proposed a multi-modal learning framework, which achieves cross-modal contextual semantic information interaction by building a single Transformer and a cross Transformer.\n\nHowever, current mainstream methods only consider contextual semantic information, latent dependencies of local utterances, and conversational relationships between speakers, and their focus is on exploring the semantic information between utterances and the correlation between speakers. The above approach ignores latent dependencies of the global context, which limits the performance of MER.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Scalable Graph Neural Network",
      "text": "The current mainstream scalable GNN methods include three types of methods: 1) Node sampling strategy: Accelerate the aggregation process of node features by sampling nodes. The representative methods include GraphSAGE  [26] , FastGCN  [27]  and LADIES  [28] . The main idea of GraphSage is to update the representation of each node through multiple rounds of neighbor sampling and aggregation of neighbor node information, thereby capturing the structure and relationships between nodes in graph data. FastGCN proposes a structured graph  node sampling strategy, which selects sampling nodes by considering the structural information of the graph to retain important structural features of the graph. This sampling strategy can preserve the graph information as much as possible while ensuring sampling efficiency. LADIES adopts an adaptive density modeling method to capture local and global information by learning the density distribution of neighbors around a node. LADIES can effectively update the representation of nodes to one that takes into account both local and global information. 2) Graph partitioning method: Divide the original large graph into several small subgraphs and run GNN on the subgraphs. The mainstream graph partitioning methods include Cluster-GCN  [29]  and GraphSAINT  [30] . The mainstream graph partitioning methods include Cluster-GCN and GraphSA-INT. Cluster-GCN divides the original large-scale graph data into multiple subgraphs, each subgraph contains a part of nodes and corresponding edges, thereby reducing computational and memory overhead. GraphSAINT processes large-scale graph data through graph sampling and iterative coarsening. 3) Matrix approximation method: Accelerate feature propagation by decoupling feature propagation and nonlinear transformation. SGC simplifies the nonlinear activation function in traditional graph convolutional networks, retaining only graph convolution operations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Head Attention",
      "text": "The multi-head attention in MER can help the model effectively capture the correlation information between different modalities and adaptively focus on the most important parts for the emotion classification task. For example, TEMMA  [31]  proposesd a multi-modal multi-head attention for MER to comprehensively consider the complementarity and redundancy between modalities. TE-MMA can realize the semantic information interaction between modalities and capture the temporal dependence within the modalities. GA2MIF  [32]  constructed a multi-head directed graph attention network and a multi-head pairwise cross-modal attention network respectively to achieve contextual semantic information extraction and crossmodal information fusion. EEANet  [33]  used a multi-head self-attention mechanism to capture the discriminative features in contextual semantic information that are most suitable for emotion classification.\n\nWe apply an attention mechanism to the utterance features obtained through graph convolution operations to calculate the correlation between contexts and capture the utterances with the strongest emotional features among the global context latent dependencies. Our method ELR-GNN can simultaneously consider contextual semantic information, interaction information between speakers, and latent dependency information of the global context.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "The overall framework of the ELR-GNN proposed in this paper is shown in Fig.  1 . ELR-GNN consists of four stages, including sequential contextual feature extraction, graph construction, long-distance contextual latent relationship exploration, and information fusion. In the following subsections, we describe these four key parts in detail.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Sequential Context Information Extraction",
      "text": "The speaker's emotional state is not only related to the textual semantic information at the current moment, but also related to the previous contextual semantic information. Therefore, we use Bi-LSTM to capture contextual semantic information in multi-modal features to more accurately understand the speaker's emotional changes. The formula of LSTM is defined as follows:\n\nwhere u t i represents the concatenated multi-modal features, h t i represents the hidden layer state, r t represents the input gate, z t represents the forgetting gate, C t represents the cell state, and ⊙ represents Hadamard product, W is a learnable network parameter.\n\nBi-LSTM is composed of forward and reverse LSTM, and its formula is defined as follows:\n\nwhere ht i is obtained by concatenating the contextual semantic features extracted by forward and reverse LSTM.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Graph Construction",
      "text": "We use the inherent properties of the graph structure to construct a speaker relationship graph, in which the contextual semantic features extracted through Bi-LSTM are used as node features of the graph, and the dialogue relationships between speakers are used as edges. Specifically, given a speaker dialogue graph G = {W, V, E, R}, where the node v i (v i ∈ V) is composed of contextual semantic features (i.e., hi ), the edge e ij = 1(e ij ∈ R) indicates that there is a conversation relationship between node v i and node v j , otherwise e ij = 0, ω ij (ω ij ∈ W, 0 ≤ ω ij ≤ 1) represents the weight of edge e ij , and r ∈ R represents the edge relationship.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Long-Distance Latent Context Relationship Extraction",
      "text": "Unlike previous work that set the context window size to 10 (i.e., the number of nodes), to capture long-distance latent dependencies of contexts, we adopt a larger context window to explore potential correlations between contexts. Specifically, we first construct an original graph G with a larger context window, the generalized forward push algorithm is then used to calculate the propagation matrix of the row vectors, and top-k sparsification is used to further reduce the training time of the network, so as to comprehensively consider the latent correlation of the context.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Propagation Matrix",
      "text": "We use a mixed-order matrix of feature propagation to aggregate neighbor node information of different orders in the graph to obtain long-distance contextual latent dependency information. The formula of the propagation matrix is defined as follows:\n\nwhere w n ≥ 0 and N n=0 w n = 1, A is the adjacency matrix, and D is the degree matrix. The propagation matrix can fuse different orders of neighbor node information and capture important contextual potential dependency information by adjusting the weights.\n\nThen we aggregate the node features and update the node features, defined as follows:\n\nwhere z υ ∼ Bernoulli(1 -δ), Π is the row vectors of the node s, N π s is the indices of non-zero value of Π s . Through Eq. 4, we can solve the problem of slow inference speed caused by the high computational complexity of GCN and achieve rapid training of the model. Therefore, we can construct larger graphs to capture long-distance context latent dependencies. However, Π s is actually a difficult estimation problem. To address the problem, We use a two-stage estimation step for calculation, which includes Generalized Forward Push (GFP) and Top-k sparsification. First, GFP gives the error bound of Π s , and then Top-k sparsification only retains top-k elements to achieve faster calculation speed.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Generalized Forward Push",
      "text": "Since the row-normalized adjacency matrix D -1 A is also an inverse random walk transition probability matrix on G, we design an efficient GFP estimation algorithm to estimate Π s . The key step of GFP is to accelerate the random walk probability diffusion process through pruning operation. Specifically, we first give two initial vectors q (n) ∈ R |V | and r (n) ∈ R |V | , and both q (0) and r (0) are initialized to e (s) , where e (s) = 1 and e (v) = 0 for s ̸ = v. Furthermore, q (n) = 0, r (n) = 0, 1 ≤ n ≤ N . Then, the GFP algorithm begins to iteratively update the q (n) and r (n) vectors through r",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Top-K Sparsification",
      "text": "To reduce the computational complexity of GCN, we perform top-k sparsification on Πs to accelerate model training. The core idea of Top-k sparsification is to retain only the top-k largest elements of Π, and set other elements to 0. Therefore, Π(k) has only k non-zero elements, which preserves the most important emotion features in the latent dependencies of the context.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Learnable Information Propagation",
      "text": "Therefore, we introduce a learnable parameter W to achieve dimensionality reduction of multi-modal features while improving the learning ability of the model. The formula is defined as follows:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Auxiliary Information Module",
      "text": "Graph random neural networks can effectively extract dialogue relationship information between speakers and long-distance context potential dependency information, but it is easy to ignore some discriminative original fullemotion features. Therefore, we use AIM to extract and fuse higher-level emotional features, adaptively aggregating original emotional features, speaker relationship information, and long-distance context potential dependency information.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Feature Extractor (Aim-Fe)",
      "text": "Multimodal data are characterized by noise and high dimensionality. To achieve denoising and capture discriminative emotional features in multi-modal data, we introduce gated convolutional networks to capture auxiliary information. In the gated convolutional network, we use sigmoid and tanh functions, which can retain the most important emotional feature information and improve the nonlinear fitting ability of the model. The formula of the gated convolutional network is defined as follows:\n\nwhere Conv1D represents 1D convolution operations, ⊙ represents Hadama product.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Late Adaptive Fusion (Aim-Laf)",
      "text": "To capture finer-grained semantic information in multimodal data, early and late adaptive fusion mechanisms are combined to capture auxiliary information with finegrained emotional features. Specifically, late fusion fuses highly abstract time and space information, ignoring detailed information. Therefore, the combination of early and late adaptive fusion mechanisms proposed in this paper can more effectively capture more discriminative emotional features adaptively from multi-modal data.\n\nIn the early fusion process, we map the contextual features Z C through the gated convolutional network and the latent features Z G through the graph random neural network to the same dimension, obtain zg and zc and fuse them. The formula is defined as follows: zf = Ω( zg , zc )  (7)  where Ω(•) represents summation average operation. Then we use a FCN to achieve feature dimensionality reduction and obtain z g , z c , and z f . Then we use the attention mechanism to obtain the corresponding attention score as follows:\n\nwhere q represents the query matrix, W and b are the learnable parameters. Likewise, e c and e f are calculated using Eq. 8. Then we use the softmax function to normalize the attention coefficient as follows:\n\nFinally, we perform a weighted sum of z g , z c and z f to obtain the final emotional feature vector representation. The formula is defined as follows:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Training",
      "text": "The final emotional feature vector z with contextual semantic information, dialogue relationship information between speakers, and long-distance latent dependency information is fed into the MLP with residual connections for feature conversion, and then use the softmax layer to get the probability of C-class emotion category:\n\nwhere W z , b z , W Z , b Z is the learnable parameters. We then obtain the index of the maximum emotion probability by using the argmax function. ŷ(j) = argmax(P (j) ) (\n\nFinally, we use cross-entropy loss to complete the optimization of the model:\n\nwhere M represents the number of dialogues, and L i represents the number of utterances in the i-th dialogue.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We evaluate the ELR-GNN model proposed in this paper on two benchmark datasets, IEMOCAP  [34]  and MELD  [35] . All these data sets contain three modal data sets: text, video, and audio.\n\nIEMOCAP is a public dataset widely used in emotion recognition research. This dataset was created by the Sippy team at the University of Southern California and aims to provide detailed annotations of emotional interactions and speech/non-verbal behaviors. The IEMO-CAP dataset emotionally annotates speech and video, including six emotion categories: happy, sad, angry, excited, frustrated, and neutral. Emotional annotation is accomplished through consistent annotation of data by multiple evaluators. The IEMOCAP dataset contains text, audio, and video data from 10 different actors. Each actor participated in a series of emotional interaction tasks.\n\nMELD is an open multi-modal dataset for emotion analysis research. It was created by researchers at the University of Toronto to advance research into natural language and speech emotion recognition. The MELD data set contains data in three modalities: text, video and audio. The text is the script text from the movie dialogue. The MELD dataset contains annotations for six emotion categories: joy, sadness, anger, fear, surprise and neutral. These emotion annotations are performed independently by multiple annotators.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baselines And Evaluation Metrics",
      "text": "bc-LSTM  [36]  performs final emotion recognition by extracting the sequential context information of the utterance, which is context-sensitive.\n\nText-CNN  [37]  uses convolution filters to extract local semantic information from utterances, which is contextindependent.\n\nMFN  [38]  designs a multi-view learning mechanism to capture view-specific and cross-view semantic information, but MFN does not consider contextual information.\n\nCMN  [39]  achieves the fusion of speaker information and multi-modal features by introducing an attention mechanism.\n\nICON  [40]  uses GRU to extract contextual information of multi-modal features and uses attention layers to achieve the fusion of multi-modal semantic information.\n\nDialogueRNN  [16]  constructs three different gating units to achieve the extraction and fusion of speaker information, emotional information and global information.\n\nDialogueGCN  [22]  DialogueGCN constructs a speaker relationship graph by using contextual semantic features, and utilizes contextual semantic information and speaker relationship information to achieve emotion classification.\n\nConGCN  [41]  treats multimodal features as node features in the graph and utilizes heterogeneous graphs to model conversational relationship information between speakers.\n\nLR-GCN  [23]  captures the latent dependencies between contexts by constructing multiple graphs and constructs densely connected layers to extract speaker relationship information and structural information of the graph.\n\nAGHMN  [42]  uses BiGRU to fuse the correlation information between historical contexts and uses the attention mechanism to give higher weight to important context information.\n\nBiERU  [43]  uses emotion recurrent units and emotion feature extractors to extract contextual semantic information respectively. and refine contextual emotion feature vectors.\n\nEmoBERTa  [44]  uses RoBERTa to extract sequential contextual semantic information from text. This method does not use multi-modal data.\n\nLFM  [45]  uses low-rank decomposition to effectively reduce the dimensionality disaster problem that occurs during the fusion process of multi-modal features.\n\nRGAT  [46]  integrates position encoding information into the graph attention network to improve the model's context understanding ability.\n\nCoMPM  [47]  uses a pre-trained model to extract pretrained context memory information and combines it with the context model to understand the global contextual emotional features in a fine-grained manner.\n\nCOGMEN  [48]  improves the representation ability of emotional feature vectors by building context GCN to extract global and local context information and fuse them.\n\nDER-GCN  [49]  improves the model's emotional representation capabilities by constructing speaker relationship graphs and event graphs.\n\nA-DMN  [50]  A-DMN comprehensively considers the intra-speaker and inter-speaker contextual information, and uses GRU to achieve cross-modal feature fusion.\n\nCTNet  [19]  realizes semantic information interaction within and between modalities by building Single Transformer and Cross Transformer.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison With The State-Of-The-Art Methods",
      "text": "To verify the superiority of the ELR-GNN method proposed in this paper, we report the experimental results of ELR-GNN and other comparative methods on the IEMO-CAP and MELD data sets. Experimental results are presented in Tables  1  and 2 .\n\nIEMOCAP: As shown in Table  1 , the multi-modal emotion recognition method proposed in this paper achieved the best emotion recognition effect on the IEMOCAP data set, with an average accuracy of 70.6% and an average F1 value of 70.9%. ELR-GCN proposes an effective modeling method of long-distance context latent dependencies for multi-modal emotion recognition. In addition, ELR-GCN also combines early and adaptive late fusion methods to achieve the capture of fine-grained emotional features. Among other comparison methods, the emotion recognition effect of DER-GCN is slightly lower than that of ELR-GNN, with an average accuracy of 69.7% and an average F1 value of 69.4%. Although DER-GCN comprehensively considers event relationships and dialogue relationships between speakers to enhance the model's emotional understanding, it ignores latent context dependencies. The emotion recognition effect of LR-GCN is lower than ELR-GNN and DER-GCN, with an average accuracy of 68.5% and an average F1 value of 68.3%. Although LR-GCN considers latent dependencies between contexts, due to the high computational complexity of GCN, LR-GCN can only capture local latent dependencies. The emotion recognition effects of other comparison methods are lower   than ELR-GNN. Likewise, none of them take into account potential dependencies on context. Overall, the accuracy of ELR-GNN on the happy emotion analogy is much higher than that of other comparison algorithms, while the accuracy of other emotion categories is also relatively close to that of other comparison algorithms. In addition, the F1 value of ELR-GNN on the happy and excited emotional analogies is much higher than that of other comparison algorithms. At the same time, the F1 value of ELR-GNN on other emotional categories is also relatively close to other comparison algorithms. The experimental results prove the superiority of the ELR-GNN method proposed in this paper.\n\nMELD: As shown in Table  2 , The ELR-GNN method proposed in this article has the best emotion recognition effect on the MELD data set, with an average accuracy of 68.7% and an average F1 value of 69.9%. The emotion recognition effect of DER-GCN is second, with an average accuracy of 69.7% and an average F1 value of 69.4%. The emotion recognition effect of LR-GCN is lower than that of ELR-GNN and DER-GCN, with an average accuracy of 68.5% and an average F1 value of 68.3%. The emotion recognition effects of other comparison methods are relatively poor, and the average accuracy and F1 value are lower than ELR-GNN. The performance improvement may be attributed to ELR-GNN's ability to capture longdistance contextual latent dependencies and fine-grained fusion of dialogue relationships between speakers, contextual latent dependencies and contextual semantic information. Overall, the accuracy of ELR-GNN on the neutral, fear, sadness, joy, and disgust emotion analogy is much higher than that of other comparison algorithms, while the accuracy of other emotion categories is also relatively close to that of other comparison algorithms. In addition, the F1 value of ELR-GNN on the neutral, fear, sadness, joy, and anger emotional analogies is much higher than that of other comparison algorithms. At the same time, the F1 value of ELR-GNN on other emotional categories is also relatively close to other comparison algorithms. In addition, we find that ELR-GNN has better emotion recognition effects on the minority emotions fear and disgust, with relatively high accuracy and F1 value. The experimental results prove the superiority of the ELR-GNN method proposed in this paper.\n\nIn addition, to intuitively illustrate that the running time of the ELR-GNN method proposed in this paper is better than other comparative methods, we statistics in Table  3  the running time of other comparative methods of the ELR-GNN method on the IEMOCAP and MELD data sets. As shown in Table  3 , the running time of the ELR-GNN method proposed in this paper on the IEMOCAP and MELD data sets is 41s and 91s respectively, which is significantly better than other comparison methods. The running times of DialogueGCN are 58s and 127s respectively, which are lower than LR-GCN and DER-GCN, but the emotion recognition effect is relatively poor. The running times of LR-GCN are 87s and 142s respectively. The running times of DER-GCN are 125s and 189s respectively. The experimental results prove the efficiency and effectiveness of the ELR-GNN method proposed in this paper.\n\nTable  3 . We tested the running time of the ELR-GNN method proposed in this paper and other comparative methods on the IEMO-CAP and MELD data sets. In particular, ELR-GNN sets rmax to 10 -5 and neighbor size to 64.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Analysis Of The Experimental Results",
      "text": "To intuitively understand the ability of the feeling model for each emotion category, we analyzed the emotion classification of ELR-GNN and LR-GCN on the test set. Fig.  2  shows the confusion matrix of ELR-GNN and LR-GCN for emotion classification on IEMOCAP and MELD data sets.\n\nOverall, on the IEMOCAP data set, ELR-GCN has a higher number of correct classifications for each emotion category than LR-GCN. On the MELD dataset, ELR-GCN has a higher number of correct classifications than LR-GCN in most emotional categories. The performance   improvement may be attributed to ELR-GNN's ability to understand the semantic representation of each emotion category in a fine-grained manner.\n\nOn the IEMOCAP dataset, the confusion matrix shows that ELR-GNN easily misclassifies happy emotions into excited emotions. Similarly, LR-GCN also easily misclassifies happy emotions into excited emotions, and even the number of misclassifications is greater than that of ELR-GNN. We speculate that this is because the semantics of happy emotions and excited emotions are relatively similar, and the model cannot differentiate between these two types of emotions in a fine-grained manner. In addition, we also find that ELR-GNN easily misclassifies neutral emotions into frustated emotions.\n\nOn the MELD data set, the confusion matrix shows that ELR-GNN has a very poor classification effect on disgust and fear emotions, and can only correctly classify a few samples. This is because the number of disgust and fear emotion categories is relatively small, and the data set has a serious imbalance problem, which leads to deviations in the model's emotional understanding ability. The number of correct classifications of ELR-GNN on neutral emotions is very large, and there are very few misclassified samples. Experimental results prove that ELR-GNN has a relatively strong ability to understand neutral emotional categories.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "4. Ablation Study",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "4.5.1. Importance Of The Modalities",
      "text": "To verify the importance of the three modal features of text, video and audio for ELR-GNN, we conducted ablation experiments on the IEMOCAP and MELD data sets to compare the performance of the combination of different modal features. The experimental results are shown in Table 4. In single-modal experiments, ELR-GNN with text modality features has the best emotion recognition effect. The average accuracy on the IEMOCAP and MELD data sets are 64.1% and 63.5%, respectively, and the average F1 value is 63.9% and 62.4%, respectively. The emotion recognition effect of ELR-GNN with audio modal features is second, with average accuracy rates of 61.1% and 62.7% on the IEMOCAP and MELD data sets, and average F1 values of 60.8% and 62.0% respectively. ELR-GNN with video modality features has the worst emotion recognition effect, with average accuracy rates of 59.4% and 60.1% on the IEMOCAP and MELD data sets, and average F1 values of 59.7% and 61.4% respectively. Experimental results show that text features contain the most emotional semantic information. In the dual-modal experiment, ELR-GNN with text and audio modal features has the best emotion recognition effect. The average accuracy on the IEMO-CAP and MELD data sets are 65.0% and 64.1%, respectively, and the average F1 values are are 64.4% and 63.2%, respectively. Experimental results demonstrate the effectiveness of multimodal features.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Parameter Analysis",
      "text": "We tested the impact of the maximum neighborhood size and parameter r max in ELR-GNN on the accuracy and running time of emotion recognition. As shown in Figs.  3(a) , and 3(b), we tested the impact of different neighborhood sizes and r max on emotion recognition accuracy on the IEMOCAP and MELD datasets. Experimental results show that when r = 10 -5 , ELR-GNN has the best emotion recognition effect. When r = 10 -4 , the emotion recognition effect of ELR-GNN is second. When r = 10 -3 , ELR-GNN has the worst emotion recognition effect. Furthermore, as the size of the neighborhood continues to increase, the model's emotion recognition performance also improves. Experimental results demonstrate the necessity of capturing long-range latent context dependencies.\n\nAs shown in Figs.  3(c ), and 3(d), We also calculated the impact of different neighborhood sizes on running time and emotion recognition accuracy. Experimental results show that as the neighborhood size increases, the running time of the model also increases, but it is lower than the running time of LR-GCN and DER-GCN. In addition, as the neighborhood size increases, the emotion recognition effect of the model also improves.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a novel Efficient Long-distance Latent Relation-aware Graph Neural Network (ELR-GNN) for multi-modal emotion recognition. Specifically, we first use RoBERTa, 3D-CNN and openSMILE to perform prefeature extraction of text, video and audio features respectively. Next, we use Bi-LSTM to capture contextual semantic information and obtain low-level utterence features. We then use low-level utterence features to construct a speaker graph. In the constructed speaker relationship graph, low-level utterence features are used as node features, while dialogue relationship information between speakers is used for edge construction. To capture the latent dependency information between long-distance contexts, we use the graph random neural network algorithm to randomly sample top-k nodes for information extraction. In addition, we combine early fusion and adaptive late fusion mechanisms to simultaneously fuse speaker relationship information and latent dependency information between contexts. On the IEMOCAP and MELD data sets, the ELR-GNN method proposed in this paper is better than other comparative methods, and the experimental results prove the superiority of the ELR-GNN method.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Credit Authorship Contribution Statement",
      "text": "",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of ELR-GCN for milti-modal emotion recognition.",
      "page": 3
    },
    {
      "caption": "Figure 1: ELR-GNN consists of four",
      "page": 4
    },
    {
      "caption": "Figure 2: Confusion matrix of ELR-GNN and LR-GNN classification on IEMOCAP and MELD datasets.",
      "page": 8
    },
    {
      "caption": "Figure 2: shows the confusion matrix of ELR-GNN and LR-GCN",
      "page": 8
    },
    {
      "caption": "Figure 3: We tested the impact of the maximum neighborhood size and parameter rmax in ELR-GNN on the accuracy and running time of",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "Happy\nSad\nNeutral\nAngry\nExcited\nFrustrated\nAverage(w)"
        },
        {
          "Methods": "",
          "IEMOCAP": "Acc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1"
        },
        {
          "Methods": "TextCNN\nbc-LSTM\nMFN\nCMN\nLFM\nICON\nA-DMN\nDialogueGCN\nRGAT\nAGHMN\nBiERU\nCoMPM\nEmoBERTa\nCOGMEN\nCTNet\nLR-GCN\nDER-GCN\nELR-GCN",
          "IEMOCAP": "27.7 29..8\n57.1 53.8\n34.3 40.1\n61.1 52.4\n46.1 50.0\n62.9 55.7\n48.9 48.1\n29.1 34.4\n57.1 60.8\n54.1 51.8\n57.0 56.7\n51.1 57.9\n67.1 58.9\n55.2 54.9\n24.0 34.1\n65.6 70.5\n55.5 52.1\n72.3 66.8\n64.3 62.1\n67.9 62.5\n60.1 59.9\n25.0 30.3\n55.9 62.4\n52.8 52.3\n61.7 59.8\n55.5 60.2\n71.1 60.6\n56.5 56.1\n25.6 33.1\n75.1 78.8\n58.5 59.2\n64.7 65.2\n80.2 71.8\n61.1 58.9\n63.4 62.7\n22.2 29.9\n58.8 64.6\n62.8 57.4\n64.7 63.0\n58.9 63.4\n67.2 60.8\n59.1 58.5\n43.1 50.6\n69.4 76.8\n63.0 62.9\n63.5 56.5\n88.3 77.9\n53.3 55.7\n64.6 64.3\n40.6 42.7\n89.1 84.5\n62.0 63.5\n67.5 64.1\n65.5 63.1\n64.1 66.9\n65.2 64.1\n60.1 51.6\n78.8 77.3\n60.1 65.4\n70.7 63.0\n78.0 68.0\n64.3 61.2\n65.0 65.2\n48.3 52.1\n68.3 73.3\n61.6 58.4\n57.5 61.9\n68.1 69.7\n67.1 62.3\n63.5 63.5\n54.2 31.5\n80.6 84.2\n64.7 60.2\n67.9 65.7\n62.8 74.1\n61.9 61.3\n66.1 64.7\n59.9 60.7\n78.0 82.2\n60.4 63.0\n70.2 59.9\n85.8 78.2\n62.9 59.5\n67.7 67.2\n56.9 56.4\n79.1 83.0\n64.0 61.5\n70.6 69.6\n86.0 78.0\n63.8 68.7\n67.3 67.3\n57.4 51.9\n81.4 81.7\n65.4 68.6\n69.5 66.0\n83.3 75.3\n63.8 68.2\n68.2 67.6\n47.9 51.3\n78.0 79.9\n69.0 65.8\n72.9 67.2\n85.3 78.7\n52.2 58.8\n68.0 67.5\n54.2 55.5\n81.6 79.1\n59.1 63.8\n69.4 69.0\n76.3 74.0\n68.2 68.9\n68.5 68.3\n60.7 58.8\n75.9 79.8\n66.5 61.5\n71.3 72.1\n71.1 73.3\n66.1 67.8\n69.7 69.4\n64.7 62.9\n75.7 80.8\n66.2 62.4\n70.7 70.0\n76.8 78.6\n67.9 68.1\n70.6 70.9"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "MELD": "Neutral\nSurprise\nFear\nSadness\nJoy\nDisgust\nAnger\nAverage(w)"
        },
        {
          "Methods": "",
          "MELD": "Acc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1"
        },
        {
          "Methods": "TextCNN\nbc-LSTM\nDialogueRNN\nDialogueGCN\nRGAT\nCoMPM\nEmoBERTa\nConGCN\nA-DMN\nLR-GCN\nDER-GCN\nELR-GCN",
          "MELD": "76.2 74.9\n43.3 45.5\n4.6 3.7\n18.2 21.1\n46.1 49.4\n8.9 8.3\n35.3 34.5\n56.3 55.0\n78.4 73.8\n46.8 47.7\n3.8 5.4\n22.4 25.1\n51.6 51.3\n4.3 5.2\n36.7 38.4\n57.5 55.9\n72.1 73.5\n54.4 49.4\n1.6 1.2\n23.9 23.8\n52.0 50.7\n1.5 1.7\n41.0 41.5\n56.1 55.9\n70.3 72.1\n42.4 41.7\n3.0 2.8\n20.9 21.8\n44.7 44.2\n6.5 6.7\n39.0 36.5\n54.9 54.7\n76.0 78.1\n40.1 41.5\n3.0 2.4\n32.1 30.7\n68.1 58.6\n4.5 2.2\n40.0 44.6\n60.3 61.1\n78.3 82.0\n48.3 49.2\n1.7 2.9\n35.9 32.3\n71.4 61.5\n3.1 2.8\n42.2 45.8\n64.1 65.3\n78.9 82.5\n50.2 50.2\n1.8 1.9\n33.3 31.2\n72.1 61.7\n9.1 2.5\n43.3 46.4\n64.1 65.2\n46.8 45.4\n10.6 8.8\n8.7 8.1\n53.1 54.6\n76.7 75.2\n28.5 26.3\n50.3 48.4\n59.4 58.7\n76.5 78.9\n56.2 55.3\n8.2 8.6\n22.1 24.9\n59.8 57.4\n1.2 3.4\n41.3 40.9\n61.5 60.4\n76.7 80.0\n53.3 55.2\n0.0 0.0\n49.6 35.1\n68.0 64.4\n10.7 2.7\n48.0 51.0\n65.7 65.6\n76.8 80.6\n50.5 51.0\n14.8 10.4\n56.7 41.5\n69.3 64.3\n17.2 10.3\n52.5 57.4\n66.8 66.1\n80.2 83.6\n36.8 35.4\n19.2 13.1\n80.2 83.6\n76.5 69.7\n55.6 13.0\n52.1 57.7\n68.7 69.9"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Smin: Semi-supervised multi-modal interaction network for conversational emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "S Yang",
        "K Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "3",
      "title": "A lowrank matching attention based cross-modal feature fusion method for conversational emotion recognition",
      "authors": [
        "Y Shou",
        "X Cao",
        "D Meng",
        "B Dong",
        "Q Zheng"
      ],
      "year": "2023",
      "venue": "A lowrank matching attention based cross-modal feature fusion method for conversational emotion recognition",
      "arxiv": "arXiv:2306.17799"
    },
    {
      "citation_id": "4",
      "title": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2023",
      "venue": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "arxiv": "arXiv:2312.06337"
    },
    {
      "citation_id": "5",
      "title": "A multi-message passing framework based on heterogeneous graphs in conversational emotion recognition",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "J Du",
        "H Liu",
        "K Li"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "6",
      "title": "Der-gcn: Dialog and event relation-aware graph convolutional neural network for multimodal dialog emotion recognition",
      "authors": [
        "W Ai",
        "Y Shou",
        "T Meng",
        "K Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "7",
      "title": "Adversarial representation with intra-modal and inter-modal graph contrastive learning for multimodal emotion recognition",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "K Li"
      ],
      "year": "2023",
      "venue": "Adversarial representation with intra-modal and inter-modal graph contrastive learning for multimodal emotion recognition",
      "arxiv": "arXiv:2312.16778"
    },
    {
      "citation_id": "8",
      "title": "Revisiting multimodal emotion learning with broad state space models and probability-guidance fusion",
      "authors": [
        "Y Shou",
        "T Meng",
        "F Zhang",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "Revisiting multimodal emotion learning with broad state space models and probability-guidance fusion",
      "arxiv": "arXiv:2404.17858"
    },
    {
      "citation_id": "9",
      "title": "A twostage multimodal emotion recognition model based on graph contrastive learning",
      "authors": [
        "W Ai",
        "F Zhang",
        "T Meng",
        "Y Shou",
        "H Shao",
        "K Li"
      ],
      "year": "2023",
      "venue": "2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS)"
    },
    {
      "citation_id": "10",
      "title": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "authors": [
        "T Meng",
        "F Zhang",
        "Y Shou",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "arxiv": "arXiv:2404.17862"
    },
    {
      "citation_id": "11",
      "title": "Attention driven fusion for multi-modal emotion recognition",
      "authors": [
        "D Priyasad",
        "T Fernando",
        "S Denman",
        "S Sridharan",
        "C Fookes"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "13",
      "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "V Chudasama",
        "P Kar",
        "A Gudmalwar",
        "N Shah",
        "P Wasnik",
        "N Onoe"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Exploring temporal representations by leveraging attention-based bidirectional lstm-rnns for multi-modal emotion recognition",
      "authors": [
        "C Li",
        "Z Bao",
        "L Li",
        "Z Zhao"
      ],
      "year": "2020",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "15",
      "title": "Object detection in medical images based on hierarchical transformer and mask mechanism",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "C Xie",
        "H Liu",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "16",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition in conversation based on a dynamic complementary graph convolutional network",
      "authors": [
        "Z Yang",
        "X Li",
        "Y Cheng",
        "T Zhang",
        "X Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "B Zhang",
        "Y Zhang",
        "B Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Graphcfc: A directed graph based cross-modal feature complementation approach for multimodal conversational emotion recognition",
      "authors": [
        "J Li",
        "X Wang",
        "G Lv",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "Graphunet: Graph make strong encoders for remote sensing segmentation",
      "authors": [
        "Y Shou",
        "W Ai",
        "T Meng",
        "F Zhang",
        "K Li"
      ],
      "year": "2023",
      "venue": "2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS)"
    },
    {
      "citation_id": "22",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2020",
      "venue": "EMNLP-IJCNLP 2019-2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference"
    },
    {
      "citation_id": "23",
      "title": "Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "W Li",
        "D Song",
        "W Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "24",
      "title": "Structure aware multi-graph network for multi-modal emotion recognition in conversations",
      "authors": [
        "D Zhang",
        "F Chen",
        "J Chang",
        "X Chen",
        "Q Tian"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition in conversations with transfer learning from generative conversation modeling",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Zimmermann",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Emotion recognition in conversations with transfer learning from generative conversation modeling",
      "arxiv": "arXiv:1910.04980"
    },
    {
      "citation_id": "26",
      "title": "Inductive representation learning on large graphs, Advances in neural information processing systems",
      "authors": [
        "W Hamilton",
        "Z Ying",
        "J Leskovec"
      ],
      "year": "2017",
      "venue": "Inductive representation learning on large graphs, Advances in neural information processing systems"
    },
    {
      "citation_id": "27",
      "title": "Fastgcn: Fast learning with graph convolutional networks via importance sampling",
      "authors": [
        "J Chen",
        "T Ma",
        "C Xiao"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "28",
      "title": "Layerdependent importance sampling for training deep and large graph convolutional networks",
      "authors": [
        "D Zou",
        "Z Hu",
        "Y Wang",
        "S Jiang",
        "Y Sun",
        "Q Gu"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "29",
      "title": "Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks",
      "authors": [
        "W.-L Chiang",
        "X Liu",
        "S Si",
        "Y Li",
        "S Bengio",
        "C.-J Hsieh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "30",
      "title": "Graphsaint: Graph sampling based inductive learning method",
      "authors": [
        "H Zeng",
        "H Zhou",
        "A Srivastava",
        "R Kannan",
        "V Prasanna"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "31",
      "title": "Transformer encoder with multimodal multi-head attention for continuous affect recognition",
      "authors": [
        "H Chen",
        "D Jiang",
        "H Sahli"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2020.3037496"
    },
    {
      "citation_id": "32",
      "title": "Ga2mif: Graph and attention based two-stage multi-source information fusion for conversational emotion detection",
      "authors": [
        "J Li",
        "X Wang",
        "G Lv",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Deep feature extraction and attention fusion for multimodal emotion recognition",
      "authors": [
        "Z Yang",
        "D Li",
        "F Hou",
        "Y Song",
        "Q Gao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs"
    },
    {
      "citation_id": "34",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "35",
      "title": "A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-tics"
    },
    {
      "citation_id": "36",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "37",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "38",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "39",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "40",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "41",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "D Zhang",
        "L Wu",
        "C Sun",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "42",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "W Jiao",
        "M Lyu",
        "I King"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "43",
      "title": "Bieru: Bidirectional emotional recurrent unit for conversational sentiment analysis",
      "authors": [
        "W Li",
        "W Shao",
        "S Ji",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "44",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "T Kim",
        "P Vossen"
      ],
      "year": "2021",
      "venue": "Computing Research Repository"
    },
    {
      "citation_id": "45",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Bagher",
        "L.-P Zadeh",
        "Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL"
    },
    {
      "citation_id": "46",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "T Ishiwatari",
        "Y Yasuda",
        "T Miyazaki",
        "J Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "47",
      "title": "Compm: Context modeling with speaker's pretrained memory tracking for emotion recognition in conversation",
      "authors": [
        "J Lee",
        "W Lee"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "48",
      "title": "Cogmen: Contextualized gnn based multimodal emotion recognition",
      "authors": [
        "A Joshi",
        "A Bhat",
        "A Jain",
        "A Singh",
        "A Modi"
      ],
      "year": "2022",
      "venue": "Cogmen: Contextualized gnn based multimodal emotion recognition"
    },
    {
      "citation_id": "49",
      "title": "Der-gcn: Dialogue and event relation-aware graph convolutional neural network for multimodal dialogue emotion recognition",
      "authors": [
        "W Ai",
        "Y Shou",
        "T Meng",
        "K Li"
      ],
      "year": "2023",
      "venue": "Der-gcn: Dialogue and event relation-aware graph convolutional neural network for multimodal dialogue emotion recognition",
      "arxiv": "arXiv:2312.10579"
    },
    {
      "citation_id": "50",
      "title": "Adapted dynamic memory network for emotion recognition in conversation",
      "authors": [
        "S Xing",
        "S Mai",
        "H Hu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}