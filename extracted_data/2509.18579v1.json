{
  "paper_id": "2509.18579v1",
  "title": "Teaching Audio Models To Reason: A Unified Framework For Source-And Layer-Wise Distillation",
  "published": "2025-09-23T02:58:16Z",
  "authors": [
    "Runyan Yang",
    "Yuke Si",
    "Yingying Gao",
    "Junlan Feng",
    "Chao Deng",
    "Shilei Zhang"
  ],
  "keywords": [
    "Knowledge Distillation",
    "Audio Reasoning",
    "LLM Distillation",
    "modality-specific KD"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "While large audio language models excel at tasks like ASR and emotion recognition, they still struggle with complex reasoning due to the modality gap between audio and text as well as the lack of structured intermediate supervision. To address this, we propose a unified knowledge distillation framework to transfer reasoning capabilities from a high-capacity textual teacher model to a student audio models while preserving its acoustic competence. Our method introduces two key dimensions: source-wise distillation, which leverages both textual and acoustic teachers to provide complementary modalityspecific supervision; and layer-wise distillation, which aligns teacher signals with appropriate student layers to improve transfer efficiency. This dual-dimensional strategy enables fine-grained control over the distillation process, effectively bridging the gap between symbolic reasoning and speech representations. Experimental results show significant improvements in audio reasoning performance, demonstrating the effectiveness of our framework as a reasoning transfer solution for audio modeling.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recent advances in large audio language models (LALMs) have improved performance on tasks such as automatic speech recognition, speech translation, and emotion recognition  [1, 2, 3] . However, their ability to perform complex reasoning over spoken content remains limited. Compared with text-based large language models, audio models face difficulties in multi-step reasoning due to the modality gap between audio and text as well as the lack of structured intermediate supervision during training.\n\nTo overcome the reasoning limitations of audio models, recent studies have explored large audio reasoning models (LARMs) that integrate structured prompting, chain-ofthought supervision, or reward shaping into audio models, † Equal contribution * Corresponding author enabling audio models to emulate step-wise reasoning similar to LLMs  [4, 5, 6] . While these approaches improve performance on complex auditory reasoning tasks, they typically require large-scale instruction tuning and substantial computational resources, limiting their practicality and scalability in real-world applications. These challenges call for a more efficient and scalable solution to endow audio models with reasoning abilities. Knowledge distillation (KD) provides a natural solution by transferring skills from high-capacity teacher models to student models  [7, 8, 9] . While KD has proven effective in textual domains, its use for structured reasoning in audio models remains underexplored. Moreover, conventional KD techniques assume fixed teacher sources and static supervision layer, which are not suited for the modality gap and representational hierarchy inherent in audio reasoning tasks.\n\nIn this work, we propose a unified and fine-grained distillation framework to teach audio models to reason by decoupling the supervision process into two dimensions: sourcewise, and layer-wise distillation. Source-wise distillation considers the origin and modality of the teacher model. The textual teacher offers strong capabilities in symbolic reasoning and commonsense inference, while the acoustic teacher provides modality-consistent supervision grounded in audio representation. We explore two source selection strategies. The first strategy employs only a textual teacher and avoids the input modality mismatch by aligning textual audio descriptions with raw audio. The second strategy leverages both audio and textual teachers, allowing the student to jointly learn from audio and text representations with complementary guidance. Layer-wise distillation addresses the architectural alignment between teacher and student, enabling the student to absorb relevant information at the most effective depths. We analyze how teacher modality and reasoning depth interact to guide supervision placement.\n\nTogether, these two dimensions form a reasoning-aware distillation framework tailored for audio models. Our experiments show that modeling source-wise and layer-wise interactions leads to significant improvements in reasoning accuracy, offering new insights into transferring reasoning abilities from LLMs to LALMs.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Llm Textual Teacher",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Lalm Student",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Lalm Acoustic Teacher",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Source-Wise Distillation",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Large Audio Reasoning Models (LARMs). LARMs are Large Audio Language Models (LALMs) that leverages the advanced reasoning capabilities of LLMs to understand complex queries with audio inputs. GAMA  [4]  obtain complex reasoning abilities through instruction-tuning on LALM, by which the model is encouraged to analyze audio event according to the context such as other scene elements and world knowledge. CompA  [10]  focuses on the compositional reasoning capacity of LALMs that attempts to understand the interrelationships, such as order of occurrence and attributebinding, among acoustic events in an audio. Audio-CoT  [5]  is the first exploration that integrates Chain-of-Thought (CoT) reasoning into LALMs to enhance their reasoning ability across auditory modalities. Audio-Reasoner  [11]  is finetuned on Qwen2-Audio with structured CoT training. R1-AQA  [6]  adopts reinforcement learning to improve the reasoning performance of the audio question answering (AQA) task. SARI  [12]  compares explicit vs. implicit reasoning and structured vs. unstructured thinking process for LARMs. Audio Flamingo 3  [13]  supports on-demand thinking and long audio understanding and reasoning. Audio-Thinker  [14]  considers the question of when and how to think and incorporates multiple think rewards related to task complexity, the overall consistency and quality of the reasoning process, exhibiting State-of-the-Art performance on diverse benchmarks. Distillation of Large Language Models. In LLMs scenarios, standard knowledge distillation objective becomes sub-optimal since the teacher model contains many more modes than student model. Therefore, more and more work is starting to consider the feedback from student model. Lion  [7]  is an adversarial distillation framework that incorporates the feedback of the student model and leverages the versatile role adaptability of LLMs, in which the teacher model is prompted to identify and generate \"hard\" instructions for student model to boost its proficiency iteratively. To prevent the student model from overestimating the low-probability regions of the teacher distribution due to the asymmetric nature of the Kullback-Leibler divergence (KLD), MiniLLM  [8]  adopts reverse KLD (RKL) to replace the forward KLD objective. Similarly, DISTILLM  [9]  introduces skew KLD (SKL), f-DISTILL  [15]  proposes Jenson-Shannon distillation (JSD), DISTILLM-2  [16]  integrates SKL and SRKL and achieves faster convergence and greater effectiveness. Besides the distillation loss, some work focuses on the distillation process in a white-box manner. Distilling step-by-step  [17]  adopts LLM to extract rationales as additional supervision for training small models within a multi-task framework. DDK  [18]  controls the composition of the distillation dataset according the performance differences between the teacher and student models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "The proposed distillation framework is illustrated in Fig.  1 . Through Source-wise distillation, the LALM student utilizes the knowledge of the LLM textual teacher and the LALM acoustic teacher together. Through Layer-wise distillation, the teachers guide the student using information at various depths. We organize this section as follows: first, we describe the textualization of audio, which is the foundation for textual distillation; then, we illustrate layer-wise distillation in the context of textual distillation; finally, we introduce the acoustic distillation approach and the joint training objective.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Textualization Of Audio",
      "text": "A key challenge we face is that the textual teacher cannot directly process audio inputs. To bridge this modality gap, we design a textualization method that converts audio into textual descriptions. This enables the textual teacher to operate in its native modality while still providing reasoning supervision aligned with the audio.\n\nTo construct textual audio descriptions, we utilize the CoTA dataset  [11] , which was introduced to improve the reasoning ability of LALMs with structured CoT training. The CoTA dataset is denoted by\n\nwhere N is the dataset size. Each sample contains an audio input x, a textual question q, a four-stage reasoning trace r = {r (j) | 1 ≤ j ≤ 4} consisting of (1) planning, (2) caption, (3) reasoning, and (4) summary, as well as a final answer a. The reasoning task is to predict r and a given x and q. We instruct an LALM to extract from the reasoning trace a concise audio description d, which captures audio content including essential information that supports subsequent reasoning. This process yields a textualized dataset:\n\nThe prompt we use to instruct the LALM is presented below:\n\nYou are an excellent audio analyst. Next, you will receive an audio and a question about this audio. You will also receive an reasoning trace, which involves some absolutely correct information about this audio. Your task is to analyze the audio content and generate a detailed textual description that includes all information from the audio relevant to the questionanswering task, such that another model, which only processes text and does not have access to the original audio, can accurately answer the question based solely on your description. The audio description you provide should not be in conflict with the information from the given reasoning trace. Your description may include the following aspects:\n\n1. What the speaker(s) said (verbatim or summarized); 2. If there are multiple speakers, identify them and indicate the order of their speech; 3. Speaking tone, emotion, and emphasis (if helpful for understanding the question); 4. Key facts, background information, and reasoning cues mentioned in the audio; 5. Significant pauses, hesitations, or emphasis in speech if relevant; 6. Any background or environmental sounds that might be relevant (e.g., car sounds, music). Do not add unrelated subjective interpretations or opinions-just objectively reconstruct everything in the audio that could assist in answering the question. Below is the audio and its corresponding question and reasoning trace: Here is the audio.\n\nHere is the question: **Question** Here is the reasoning trace: **Reasoning trace** Please output a textual description of the audio that is suitable for answering the question:",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Layer-Wise Kd",
      "text": "The conventional knowledge distillation method minimizes a divergence measure, e.g., Kullback-Leibler divergence (KLD) or Jensen-Shannon divergence (JSD), between the teacher's and student's predictive distributions at their top layers. Taking the textual distillation as an example, the objective for each output step t is:\n\nwhere y t = {r, a} t is the token that the model predicts, and KD (•∥•) is the divergence measure.\n\nSimilarly to  [19, 20, 21] , in our distillation framework, the knowledge of the teacher model is distilled not only to the student's top-layer, but also to the student's each layer's representations. This layer-wise distillation allows the student to capture hierarchical feature representations learned by the teacher, leading to richer and more structured knowledge transfer.\n\nSince the number of layers in the textual teacher model may not be an integer multiple of that in the student model, it is not always feasible to align layers by simple skipping.\n\nInstead, we align them proportionally: for the l S i -th student layer, its corresponding teacher layer index l T i is determined by\n\nwhere L S and L T are the numbers of layers of the student and teacher model, respectively. The layer-wise KD training objective for each layer at each output step is:\n\nwhere h T i,t ∈ R D T and h S i,t ∈ R D S are the hidden representations of the textual teacher model's l T i -th layer and the student model's l S i -th layer, respectively. W i ∈ R D T ×D S is a layer-specific learnable matrix, which maps h T i,t to the same dimension as h S i,t . α layer is a scaling hyperparameter. The training objective for the whole output text token sequence is\n\nwhere T y are a set of token indices that the model predicts.\n\nWe also propose an additional setting in which one layer is distilled every k layers (1-in-k), as an intermediate approach between distilling all layers and distilling only the top layer. This approach is referred to as skip-layer distillation. The training objective is defined as follows:",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Acoustic Kd",
      "text": "We additionally perform representation distillation on the hidden states corresponding to the input audio tokens, in order to preserve the model's ability to process acoustic representations and thus maintain fundamental acoustic capability. We refer to a frozen snapshot of the pre-trained LALM student taken before distillation, S0, as the acoustic teacher. As the LALMs does not yield logit outputs at the time steps corresponding to audio tokens, we only perform hidden-state distillation for acoustic KD. The acoustic distillation loss is\n\nwhere T x is the set of token positions corresponding to the input audio, and h S0 i,t denotes the hidden representation at the l S i -th layer of the acoustic teacher.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Joint Training Objective",
      "text": "By combining above KD objectives and a supervised finetuning objective, we define the final joint training loss as:\n\nwhere L SFT is the conventional cross-entropy loss used for supervised fine-tuning of the student model. α ac and α SFT are weight coefficients (hyperparameters).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "For training, we utilize the CoTA dataset  [11] . The audio description introduced in Section 3.1 is generated using Qwen2.5-Omni-7B  [22]  with greedy search. For evaluation, we mainly assess our method on an open-source audio question answering (AQA) benchmark MMAU (v05.15.25, test-mini subset)  [23] , which covers multiple domains (sound, music, and speech), various reasoning / information extraction skills, and different difficulty levels. We also present results on speech emotion recognition (SER) benchmark IEMOCAP (session 5)  [24]  as supplementary reference. No evaluation data are used in model training.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Training Setup",
      "text": "In our knowledge distillation framework, we adopt Qwen2.5-Omni-7B thinker  [22]  as the student model, initialized from its pre-trained parameters, and employ Qwen3-8B  [25]  as the textual teacher model. The Transformer layer numbers of the student and the textual teacher are 28 and 36, respectively. We train the model for 3 epochs, setting the maximum learning rate to 1e-5. α layer , α ac , and α SFT are set to 0.05, 0.05, and 0.5, respectively. We adopt JSD as the KD divergence measure because it is symmetric and bounded, and it yields more stable training than KLD. Model training is performed on 8 NVIDIA A800 (80GB) GPUs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Inference Setup",
      "text": "During inference, we use the same generation parameters across all experimental settings: temperature = 0.6, top-k = 5, and top-p = 0.5. For more precise and reliable evaluation for AQA, we standardize the final answer generated by the LALM to fit MMAU's evaluation script. We discard the generated reasoning trace in the evaluation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Analysis",
      "text": "Experimental results are illustrated in Table  1 . Baseline refers to the results reproduced using the original Qwen2.5-Omni-7B model. We report accuracies for AQA and GID. For SER, we evaluate unweighted accuracy (UA), which averages accuracies over classes (happy, anger, sad, and neutral).\n\nThe results indicate that simple supervised fine-tuning (SFT-only) does not yield consistent gains over the baseline. While SFT slightly improves AQA performance on speech questions, it degrades results on sound-related questions and SER, suggesting that naive SFT introduces catastrophic forgetting across heterogeneous speech tasks.\n\nIncorporating knowledge distillation provides more stable improvements. Top-layer txt KD surpasses SFT-only on both AQA and SER, though its gains on AQA remain limited, highlighting the insufficiency of relying solely on the final representation. Layer-wise txt KD further boosts AQA accuracy, reaching the best performance on speech-related questions (75.68%), but at the cost of degraded SER. This suggests that fully distillation at all depths can overfit textual reasoning ability tasks while neglecting audio-related abilities. As expected, Skip-layer txt KD (1-in-7) achieves intermediate performance between top-layer KD and layer-wise KD.\n\nFinally, combining Layer-wise txt KD and ac KD yields the overall best performance on AQA (average 73.30%). Comparing to Layer-wise txt KD + SFT, the incorporation of acoustic distillation brings substantial improvements on sound AQA (+4.51%) and SER (+6.38%), indicating that it helps maintain the model's abilities to perceive and analyze low-level acoustic features. It is also observed that our LALMs trained using the CoTA dataset underperform the baseline in SER performance. This is because CoT reasoning may leverage semantic cues, which can occasionally misguide the model's inference.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we propose a fine-grained distillation framework to equip audio models with reasoning abilities. Our approach introduces source-wise and layer-wise supervision to address the modality gap and architectural misalignment between teacher and student models. By leveraging complementary strengths of textual and acoustic teachers and aligning their signals with appropriate student layers, our method enables more effective knowledge transfer. Experiments demonstrate that the dual-dimensional strategy significantly improves reasoning performance, offering a new solution for transferring reasoning capabilities from LLMs to LALMs.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed teacher-student distillation framework",
      "page": 2
    },
    {
      "caption": "Figure 1: Through Source-wise distillation, the LALM student utilizes",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "2The State Key Laboratory of Multimedia Information Processing, Peking University, Beijing, China"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "enabling audio models to emulate step-wise reasoning similar"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "to LLMs [4, 5, 6]. While these approaches improve perfor-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "mance on complex auditory reasoning tasks,\nthey typically"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "require large-scale instruction tuning and substantial compu-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "tational resources, limiting their practicality and scalability in"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "real-world applications."
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "These challenges call\nfor a more efficient and scalable"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "solution\nto\nendow audio models with\nreasoning\nabilities."
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "Knowledge distillation (KD) provides a natural solution by"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "transferring skills from high-capacity teacher models to stu-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "dent models\n[7, 8, 9]. While KD has proven effective in"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "textual domains,\nits use\nfor\nstructured reasoning in audio"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "models remains underexplored. Moreover, conventional KD"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "techniques assume fixed teacher sources and static supervi-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "sion layer, which are not\nsuited for\nthe modality gap and"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "representational hierarchy inherent in audio reasoning tasks."
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "In this work, we propose a unified and fine-grained distil-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "lation framework to teach audio models to reason by decou-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "source-\npling the supervision process into two dimensions:"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "wise, and layer-wise distillation. Source-wise distillation con-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "siders the origin and modality of the teacher model. The tex-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "tual\nteacher offers strong capabilities in symbolic reasoning"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "and commonsense inference, while the acoustic teacher pro-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "vides modality-consistent supervision grounded in audio rep-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "resentation. We explore two source selection strategies. The"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "first strategy employs only a textual teacher and avoids the in-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "put modality mismatch by aligning textual audio descriptions"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "with raw audio. The second strategy leverages both audio and"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "textual teachers, allowing the student to jointly learn from au-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "dio and text\nrepresentations with complementary guidance."
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "Layer-wise distillation addresses the architectural alignment"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "between teacher and student, enabling the student\nto absorb"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "relevant information at the most effective depths. We analyze"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "how teacher modality and reasoning depth interact\nto guide"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "supervision placement."
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "Together,\nthese two dimensions form a reasoning-aware"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "distillation framework tailored for audio models. Our exper-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "iments show that modeling source-wise and layer-wise inter-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "actions leads to significant\nimprovements in reasoning accu-"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "racy, offering new insights into transferring reasoning abilities"
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": ""
        },
        {
          "1Jiutian Artificial Intelligence Research Institute, China Mobile, Beijing, China": "from LLMs to LALMs."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Audio + Text prompt"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Source 2: audio-level distillation"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "LALM"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Audio\nAudio\nAcoustic"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Teacher"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Distillation"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "LALM\nPlanning\nPlanning"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Loss"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Student"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Calculation\nCaption\nCaption"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "LLM"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Reasoning"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Reasoning\nTextual"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Summary\nSummary"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Teacher"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Answer\nAnswer"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Audio + Text prompt\nTextual audio descriptions + Text prompt"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Source 1: text-level alignment"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Source-wise distillation"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "(a) Top-layer distillation\n(b) Skip-layer distillation\n(c)  Layer-wise distillation"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Layer-wise distillation"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Fig. 1. Proposed teacher-student distillation framework"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "2. RELATED WORK"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Large Audio Reasoning Models\n(LARMs).\nLARMs are"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Large Audio Language Models (LALMs)\nthat\nleverages the"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "advanced reasoning capabilities of LLMs to understand com-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "plex queries with audio inputs. GAMA [4] obtain complex"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "reasoning abilities through instruction-tuning on LALM, by"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "which the model\nis encouraged to analyze audio event ac-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "cording to the context such as other scene elements and world"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "knowledge. CompA [10]\nfocuses on the compositional\nrea-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "soning capacity of LALMs\nthat attempts\nto understand the"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "interrelationships, such as order of occurrence and attribute-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "binding, among acoustic events in an audio. Audio-CoT [5] is"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "the first exploration that\nintegrates Chain-of-Thought\n(CoT)"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "reasoning into LALMs\nto enhance\ntheir\nreasoning ability"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "across\nauditory modalities.\nAudio-Reasoner\n[11]\nis fine-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "tuned on Qwen2-Audio with structured CoT training.\nR1-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "AQA [6] adopts reinforcement\nlearning to improve the rea-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "soning performance of the audio question answering (AQA)"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "task. SARI [12] compares explicit vs.\nimplicit reasoning and"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "structured vs. unstructured thinking process for LARMs. Au-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "dio Flamingo 3 [13] supports on-demand thinking and long"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "audio understanding and reasoning. Audio-Thinker [14] con-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "siders the question of when and how to think and incorporates"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "multiple think rewards related to task complexity,\nthe overall"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "consistency and quality of\nthe reasoning process, exhibiting"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "State-of-the-Art performance on diverse benchmarks."
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Distillation of Large Language Models.\nIn LLMs sce-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "narios,\nstandard\nknowledge\ndistillation\nobjective\nbecomes"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "sub-optimal\nsince\nthe\nteacher model\ncontains many more"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": "and teacher model, respectively. The layer-wise KD training"
        },
        {
          "where LS and LT are the numbers of layers of the student": "objective for each layer at each output step is:"
        },
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": "(cid:17)\n(cid:16)"
        },
        {
          "where LS and LT are the numbers of layers of the student": "(cid:13)(cid:13)(cid:13)\nhS\n,\n(5)\nLlayer,i = KD\nWihT\ni,t\ni,t"
        },
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": "where hT\nand hS\ni,t ∈ RDT\ni,t ∈ RDS are the hidden repre-"
        },
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": "-th layer and the\nsentations of the textual\nteacher model’s lT"
        },
        {
          "where LS and LT are the numbers of layers of the student": "i"
        },
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": "student model’s lS\ni -th layer, respectively. Wi ∈ RDT ×DS is a"
        },
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": "layer-specific learnable matrix, which maps hT\nto the same"
        },
        {
          "where LS and LT are the numbers of layers of the student": "i,t"
        },
        {
          "where LS and LT are the numbers of layers of the student": "dimension as hS\nis a scaling hyperparameter.\nThe\nαlayer"
        },
        {
          "where LS and LT are the numbers of layers of the student": "i,t."
        },
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": "training objective for the whole output text token sequence is"
        },
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": "(cid:32)\n(cid:33)"
        },
        {
          "where LS and LT are the numbers of layers of the student": "LS(cid:88)"
        },
        {
          "where LS and LT are the numbers of layers of the student": "(cid:88) t\n,\n(6)\nLtxt =\nLtop,t + αlayer\nLlayer,i"
        },
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": "i=1\n∈Ty"
        },
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": "where Ty are a set of token indices that the model predicts."
        },
        {
          "where LS and LT are the numbers of layers of the student": "We also propose an additional setting in which one layer is"
        },
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": "distilled every k layers (1-in-k), as an intermediate approach"
        },
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": "between distilling all\nlayers and distilling only the top layer."
        },
        {
          "where LS and LT are the numbers of layers of the student": ""
        },
        {
          "where LS and LT are the numbers of layers of the student": "This approach is\nreferred to as\nskip-layer distillation.\nThe"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "a. The reasoning task is to predict r and a given x and q. We"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "instruct an LALM to extract from the reasoning trace a con-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "cise audio description d, which captures audio content includ-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "ing essential information that supports subsequent reasoning."
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "This process yields a textualized dataset:"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "(2)\nDtext = {(di, qi, ri, ai)}N"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "i=1 ."
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "The prompt we use to instruct the LALM is presented below:"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "You are an excellent audio analyst. Next, you will receive an audio and a"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "question about this audio. You will also receive an reasoning trace, which"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "involves some absolutely correct information about this audio. Your task"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "is to analyze the audio content and generate a detailed textual descrip-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "tion that includes all information from the audio relevant to the question-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "answering task, such that another model, which only processes text and"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "does not have access\nto the original audio, can accurately answer\nthe"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "question based solely on your description.\nThe audio description you"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "provide should not be in conflict with the information from the given rea-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "soning trace."
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Your description may include the following aspects:"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "1. What\nthe speaker(s) said (verbatim or summarized); 2.\nIf\nthere are"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "multiple speakers,\nidentify them and indicate the order of their speech;"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "3.\nSpeaking tone, emotion, and emphasis (if helpful\nfor understanding"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "the question); 4. Key facts, background information, and reasoning cues"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "mentioned in the audio; 5. Significant pauses, hesitations, or emphasis"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "in speech if relevant; 6. Any background or environmental sounds that"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "might be relevant (e.g., car sounds, music)."
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Do not add unrelated subjective interpretations or opinions—just objec-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "tively reconstruct everything in the audio that could assist\nin answering"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "the question."
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Below is the audio and its corresponding question and reasoning trace:"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Here is the audio."
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Here is the question: **Question**"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Here is the reasoning trace: **Reasoning trace**"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "Please output a textual description of the audio that is suitable for answer-"
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": ""
        },
        {
          "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.": "ing the question:"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": "where LSFT is the conventional cross-entropy loss used for"
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": "supervised fine-tuning of\nthe student model. αac and αSFT"
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": "are weight coefficients (hyperparameters)."
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": "4. EXPERIMENTS"
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": "4.1. Datasets"
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": "For\ntraining, we utilize\nthe CoTA dataset\n[11].\nThe\nau-"
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": "dio description introduced in Section 3.1 is generated using"
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": "Qwen2.5-Omni-7B [22] with greedy search.\nFor\nevalua-"
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": "tion, we mainly assess our method on an open-source audio"
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": ""
        },
        {
          "(9)\nLjoint = Ltxt + αacLac + αSFTLSFT,": "question answering (AQA) benchmark MMAU (v05.15.25,"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "Layer-wise txt KD + SFT\n49.65"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "75.68 / 72.50"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "Qwen2.5-Omni-7B [22] with greedy search.\nFor\nevalua-",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "75.38 / 70.36"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "tion, we mainly assess our method on an open-source audio",
          "70.87 / 70.96": "Layer-wise txt KD + ac KD + SFT\n56.03"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "74.17 / 73.30"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "question answering (AQA) benchmark MMAU (v05.15.25,",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "test-mini subset) [23], which covers multiple domains (sound,",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "questions,\nit degrades results on sound-related questions and"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "music, and speech), various reasoning /\ninformation extrac-",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "SER, suggesting that naive SFT introduces catastrophic for-"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "tion skills,\nand different difficulty levels. We also present",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "getting across heterogeneous speech tasks."
        },
        {
          "dio description introduced in Section 3.1 is generated using": "results\non\nspeech\nemotion\nrecognition\n(SER)\nbenchmark",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "Incorporating knowledge distillation provides more sta-"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "IEMOCAP (session 5) [24] as supplementary reference. No",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "ble improvements.\nTop-layer txt KD surpasses SFT-only on"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "evaluation data are used in model training.",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "both AQA and SER, though its gains on AQA remain limited,"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "highlighting the insufficiency of\nrelying solely on the final"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "4.2. Model Training Setup",
          "70.87 / 70.96": "representation. Layer-wise txt KD further boosts AQA accu-"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "racy,\nreaching the best performance on speech-related ques-"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "In our knowledge distillation framework, we adopt Qwen2.5-",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "tions (75.68%), but at\nthe cost of degraded SER. This sug-"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "Omni-7B thinker [22] as the student model,\ninitialized from",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "gests that fully distillation at all depths can overfit textual rea-"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "its pre-trained parameters, and employ Qwen3-8B [25] as the",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "soning ability tasks while neglecting audio-related abilities."
        },
        {
          "dio description introduced in Section 3.1 is generated using": "textual teacher model. The Transformer layer numbers of the",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "As expected, Skip-layer txt KD (1-in-7) achieves intermediate"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "student and the textual teacher are 28 and 36, respectively.",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "performance between top-layer KD and layer-wise KD."
        },
        {
          "dio description introduced in Section 3.1 is generated using": "We train the model\nfor 3 epochs,\nsetting the maximum",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "Finally, combining Layer-wise txt KD and ac KD yields"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "to 0.05,\nlearning rate to 1e-5. αlayer, αac, and αSFT are set",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "the\noverall\nbest\nperformance\non AQA (average\n73.30%)."
        },
        {
          "dio description introduced in Section 3.1 is generated using": "0.05, and 0.5,\nrespectively. We adopt JSD as the KD diver-",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "Comparing to Layer-wise txt KD + SFT,\nthe incorporation"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "gence measure because it\nis symmetric and bounded, and it",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "of acoustic distillation brings\nsubstantial\nimprovements on"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "yields more stable training than KLD. Model\ntraining is per-",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "sound AQA (+4.51%)\nand SER (+6.38%),\nindicating that"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "formed on 8 NVIDIA A800 (80GB) GPUs.",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "it helps maintain the model’s abilities\nto perceive and an-"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "alyze low-level acoustic features.\nIt\nis also observed that"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "4.3. Model Inference Setup",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "our LALMs\ntrained using the CoTA dataset underperform"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "the baseline in SER performance. This is because CoT rea-"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "During inference, we use\nthe\nsame generation parameters",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "soning may leverage semantic cues, which can occasionally"
        },
        {
          "dio description introduced in Section 3.1 is generated using": "across all experimental settings:\ntemperature = 0.6,\ntop-k =",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "",
          "70.87 / 70.96": "misguide the model’s inference."
        },
        {
          "dio description introduced in Section 3.1 is generated using": "5,\nand top-p = 0.5.\nFor more precise and reliable evalua-",
          "70.87 / 70.96": ""
        },
        {
          "dio description introduced in Section 3.1 is generated using": "tion for AQA, we standardize the final answer generated by",
          "70.87 / 70.96": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "2025."
        },
        {
          "6. REFERENCES": "[1] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "[14]\nShu Wu, Chenxing Li, Wenfu Wang, Hao Zhang, Hualei Wang,"
        },
        {
          "6. REFERENCES": "Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang,\n“Salmonn:",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "Meng Yu, and Dong Yu,\n“Audio-thinker: Guiding audio lan-"
        },
        {
          "6. REFERENCES": "Towards generic hearing abilities for large language models,”",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "guage model when and how to think via reinforcement\nlearn-"
        },
        {
          "6. REFERENCES": "arXiv preprint arXiv:2310.13289, 2023.",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "ing,” arXiv preprint arXiv:2508.08039, 2025."
        },
        {
          "6. REFERENCES": "[2]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "[15] Yuqiao Wen, Zichao Li, Wenyu Du,\nand Lili Mou,\n“F-"
        },
        {
          "6. REFERENCES": "ing Wang, “Pengi: An audio language model for audio tasks,”",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "divergence minimization for sequence-level knowledge distil-"
        },
        {
          "6. REFERENCES": "Advances in Neural Information Processing Systems, vol. 36,",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "lation,” arXiv preprint arXiv:2307.15190, 2023."
        },
        {
          "6. REFERENCES": "pp. 18090–18108, 2023.",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "[16]\nJongwoo Ko, Tianyi Chen, Sungnyun Kim, Tianyu Ding, Lum-"
        },
        {
          "6. REFERENCES": "[3] Runyan Yang, Huibao Yang, Xiqing Zhang, Tiantian Ye, Ying",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "ing Liang,\nIlya Zharkov, and Se-Young Yun,\n“Distillm-2: A"
        },
        {
          "6. REFERENCES": "Liu, Yingying Gao, Shilei Zhang, Chao Deng,\nand Junlan",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "arXiv\ncontrastive approach boosts\nthe distillation of\nllms,”"
        },
        {
          "6. REFERENCES": "Feng, “Polyspeech: Exploring unified multitask speech models",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "preprint arXiv:2503.07067, 2025."
        },
        {
          "6. REFERENCES": "arXiv preprint\nfor competitiveness with single-task models,”",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "arXiv:2406.07801, 2024.",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "[17] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan"
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna,"
        },
        {
          "6. REFERENCES": "[4]\nSreyan Ghosh,\nSonal Kumar, Ashish\nSeth,\nChandra Ki-",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "Chen-Yu Lee, and Tomas Pfister, “Distilling step-by-step! out-"
        },
        {
          "6. REFERENCES": "ran Reddy Evuru, Utkarsh Tyagi,\nS Sakshi, Oriol Nieto,",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "performing larger language models with less training data and"
        },
        {
          "6. REFERENCES": "Ramani Duraiswami,\nand Dinesh Manocha,\n“Gama:\nA",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "smaller model sizes,” arXiv preprint arXiv:2305.02301, 2023."
        },
        {
          "6. REFERENCES": "large\naudio-language model with\nadvanced\naudio\nunder-",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "arXiv preprint\nstanding and complex reasoning abilities,”",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "[18]\nJiaheng Liu, Chenchen Zhang, Jinyang Guo, Yuanxing Zhang,"
        },
        {
          "6. REFERENCES": "arXiv:2406.11768, 2024.",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "Haoran Que, Ken Deng, Jie Liu, Ge Zhang, Yanan Wu, Con-"
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "gnan Liu, et al.,\n“Ddk: Distilling domain knowledge for effi-"
        },
        {
          "6. REFERENCES": "[5] Ziyang Ma, Zhuo Chen, Yuping Wang, Eng Siong Chng,",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "cient large language models,” Advances in Neural Information"
        },
        {
          "6. REFERENCES": "and Xie Chen,\n“Audio-cot:\nExploring\nchain-of-thought",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "Processing Systems, vol. 37, pp. 98297–98319, 2024."
        },
        {
          "6. REFERENCES": "arXiv preprint\nreasoning in large\naudio language model,”",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "arXiv:2501.07246, 2025.",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "[19]\nSiqi Sun, Yu Cheng, Zhe Gan,\nand Jingjing Liu,\n“Patient"
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "arXiv\nknowledge distillation for bert model compression,”"
        },
        {
          "6. REFERENCES": "[6] Gang Li,\nJizhong Liu, Heinrich Dinkel, Yadong Niu,\nJunbo",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "preprint arXiv:1908.09355, 2019."
        },
        {
          "6. REFERENCES": "Zhang, and Jian Luan,\n“Reinforcement\nlearning outperforms",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "supervised fine-tuning: A case study on audio question answer-",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "[20] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,"
        },
        {
          "6. REFERENCES": "ing,” arXiv preprint arXiv:2503.11197, 2025.",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "Antoine Chassang, Carlo Gatta,\nand Yoshua Bengio,\n“Fit-"
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "arXiv preprint\nnets: Hints\nfor\nthin deep nets. arxiv 2014,”"
        },
        {
          "6. REFERENCES": "[7] Yuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang,",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "arXiv:1412.6550, 2014."
        },
        {
          "6. REFERENCES": "“Lion: Adversarial distillation of proprietary large language",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "models,” arXiv preprint arXiv:2305.12870, 2023.",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "[21] Kai Zhang, Jinqiu Li, Bingqian Wang, and Haoran Meng, “Au-"
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "tocorrelation matrix knowledge distillation:\nA task-specific"
        },
        {
          "6. REFERENCES": "[8] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang, “Minillm:",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "distillation method for bert models,”\nApplied Sciences, vol."
        },
        {
          "6. REFERENCES": "arXiv\nKnowledge\ndistillation\nof\nlarge\nlanguage models,”",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "14, no. 20, 2024."
        },
        {
          "6. REFERENCES": "preprint arXiv:2306.08543, 2023.",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "[22]\nJin Xu, Zhifang Guo,\nJinzheng He, Hangrui Hu, Ting He,"
        },
        {
          "6. REFERENCES": "[9]\nJongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun,",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "Shuai Bai, Keqin Chen,\nJialin Wang, Yang Fan, Kai Dang,"
        },
        {
          "6. REFERENCES": "“Distillm: Towards streamlined distillation for large language",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "arXiv\npreprint\net\nal.,\n“Qwen2.5-omni\ntechnical\nreport,”"
        },
        {
          "6. REFERENCES": "models,” arXiv preprint arXiv:2402.03898, 2024.",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "arXiv:2503.20215, 2025."
        },
        {
          "6. REFERENCES": "[10]\nSreyan Ghosh, Ashish Seth, Sonal Kumar, Utkarsh Tyagi,",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "[23]\nS Sakshi, Utkarsh Tyagi,\nSonal Kumar, Ashish Seth, Ra-"
        },
        {
          "6. REFERENCES": "Chandra Kiran Evuru, S Ramaneswaran, S Sakshi, Oriol Ni-",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "maneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami,"
        },
        {
          "6. REFERENCES": "eto, Ramani Duraiswami, and Dinesh Manocha, “Compa: Ad-",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "Sreyan Ghosh,\nand Dinesh Manocha,\n“Mmau: A massive"
        },
        {
          "6. REFERENCES": "dressing the gap in compositional reasoning in audio-language",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "multi-task\naudio\nunderstanding\nand\nreasoning\nbenchmark,”"
        },
        {
          "6. REFERENCES": "models,” arXiv preprint arXiv:2310.08753, 2023.",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "arXiv preprint arXiv:2410.19168, 2024."
        },
        {
          "6. REFERENCES": "[11] Zhifei Xie, Mingbao\nLin,\nZihang\nLiu,\nPengcheng Wu,",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "[24] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe"
        },
        {
          "6. REFERENCES": "Shuicheng Yan,\nand Chunyan Miao,\n“Audio-reasoner:\nIm-",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N"
        },
        {
          "6. REFERENCES": "proving reasoning capability in large audio language models,”",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:"
        },
        {
          "6. REFERENCES": "arXiv preprint arXiv:2503.02318, 2025.",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "Lan-\nInteractive emotional dyadic motion capture database,”"
        },
        {
          "6. REFERENCES": "[12] Cheng Wen,\nTingwei Guo,\nShuaijiang\nZhao, Wei\nZou,",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "guage resources and evaluation, vol. 42, no. 4, pp. 335–359,"
        },
        {
          "6. REFERENCES": "and Xiangang Li,\n“Sari:\nStructured\naudio\nreasoning\nvia",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "2008."
        },
        {
          "6. REFERENCES": "arXiv preprint\ncurriculum-guided reinforcement\nlearning,”",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "arXiv\npreprint\n[25] Qwen Team,\n“Qwen3\ntechnical\nreport,”"
        },
        {
          "6. REFERENCES": "arXiv:2504.15900, 2025.",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": "arXiv:2505.09388, 2025."
        },
        {
          "6. REFERENCES": "[13] Arushi Goel, Sreyan Ghosh,\nJaehyeon Kim, Sonal Kumar,",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "Zhifeng Kong, Sang-gil Lee, Chao-Han Huck Yang, Ramani",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "Duraiswami, Dinesh Manocha, Rafael Valle, et al.,\n“Audio",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        },
        {
          "6. REFERENCES": "flamingo 3: Advancing audio intelligence with fully open large",
          "audio language models,”\narXiv preprint arXiv:2507.08128,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang"
      ],
      "year": "2023",
      "venue": "Salmonn: Towards generic hearing abilities for large language models",
      "arxiv": "arXiv:2310.13289"
    },
    {
      "citation_id": "3",
      "title": "Pengi: An audio language model for audio tasks",
      "authors": [
        "Soham Deshmukh",
        "Benjamin Elizalde",
        "Rita Singh",
        "Huaming Wang"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "Polyspeech: Exploring unified multitask speech models for competitiveness with single-task models",
      "authors": [
        "Runyan Yang",
        "Huibao Yang",
        "Xiqing Zhang",
        "Tiantian Ye",
        "Ying Liu",
        "Yingying Gao",
        "Shilei Zhang",
        "Chao Deng",
        "Junlan Feng"
      ],
      "year": "2024",
      "venue": "Polyspeech: Exploring unified multitask speech models for competitiveness with single-task models",
      "arxiv": "arXiv:2406.07801"
    },
    {
      "citation_id": "5",
      "title": "Gama: A large audio-language model with advanced audio understanding and complex reasoning abilities",
      "authors": [
        "Sreyan Ghosh",
        "Sonal Kumar",
        "Ashish Seth",
        "Chandra Kiran Reddy",
        "Utkarsh Evuru",
        "S Tyagi",
        "Oriol Sakshi",
        "Ramani Nieto",
        "Dinesh Duraiswami",
        "Manocha"
      ],
      "year": "2024",
      "venue": "Gama: A large audio-language model with advanced audio understanding and complex reasoning abilities",
      "arxiv": "arXiv:2406.11768"
    },
    {
      "citation_id": "6",
      "title": "Audio-cot: Exploring chain-of-thought reasoning in large audio language model",
      "authors": [
        "Ziyang Ma",
        "Zhuo Chen",
        "Yuping Wang",
        "Eng Siong Chng",
        "Xie Chen"
      ],
      "year": "2025",
      "venue": "Audio-cot: Exploring chain-of-thought reasoning in large audio language model",
      "arxiv": "arXiv:2501.07246"
    },
    {
      "citation_id": "7",
      "title": "Reinforcement learning outperforms supervised fine-tuning: A case study on audio question answering",
      "authors": [
        "Gang Li",
        "Jizhong Liu",
        "Heinrich Dinkel",
        "Yadong Niu",
        "Junbo Zhang",
        "Jian Luan"
      ],
      "year": "2025",
      "venue": "Reinforcement learning outperforms supervised fine-tuning: A case study on audio question answering",
      "arxiv": "arXiv:2503.11197"
    },
    {
      "citation_id": "8",
      "title": "Lion: Adversarial distillation of proprietary large language models",
      "authors": [
        "Yuxin Jiang",
        "Chunkit Chan",
        "Mingyang Chen",
        "Wei Wang"
      ],
      "year": "2023",
      "venue": "Lion: Adversarial distillation of proprietary large language models",
      "arxiv": "arXiv:2305.12870"
    },
    {
      "citation_id": "9",
      "title": "Minillm: Knowledge distillation of large language models",
      "authors": [
        "Yuxian Gu",
        "Li Dong",
        "Furu Wei",
        "Minlie Huang"
      ],
      "year": "2023",
      "venue": "Minillm: Knowledge distillation of large language models",
      "arxiv": "arXiv:2306.08543"
    },
    {
      "citation_id": "10",
      "title": "Distillm: Towards streamlined distillation for large language models",
      "authors": [
        "Jongwoo Ko",
        "Sungnyun Kim",
        "Tianyi Chen",
        "Se-Young Yun"
      ],
      "year": "2024",
      "venue": "Distillm: Towards streamlined distillation for large language models",
      "arxiv": "arXiv:2402.03898"
    },
    {
      "citation_id": "11",
      "title": "Compa: Addressing the gap in compositional reasoning in audio-language models",
      "authors": [
        "Sreyan Ghosh",
        "Ashish Seth",
        "Sonal Kumar",
        "Utkarsh Tyagi",
        "Chandra Kiran Evuru",
        "S Ramaneswaran",
        "S Sakshi",
        "Oriol Nieto",
        "Ramani Duraiswami",
        "Dinesh Manocha"
      ],
      "year": "2023",
      "venue": "Compa: Addressing the gap in compositional reasoning in audio-language models",
      "arxiv": "arXiv:2310.08753"
    },
    {
      "citation_id": "12",
      "title": "Audio-reasoner: Improving reasoning capability in large audio language models",
      "authors": [
        "Zhifei Xie",
        "Mingbao Lin",
        "Zihang Liu",
        "Pengcheng Wu",
        "Shuicheng Yan",
        "Chunyan Miao"
      ],
      "year": "2025",
      "venue": "Audio-reasoner: Improving reasoning capability in large audio language models",
      "arxiv": "arXiv:2503.02318"
    },
    {
      "citation_id": "13",
      "title": "Sari: Structured audio reasoning via curriculum-guided reinforcement learning",
      "authors": [
        "Cheng Wen",
        "Tingwei Guo",
        "Shuaijiang Zhao",
        "Wei Zou",
        "Xiangang Li"
      ],
      "year": "2025",
      "venue": "Sari: Structured audio reasoning via curriculum-guided reinforcement learning",
      "arxiv": "arXiv:2504.15900"
    },
    {
      "citation_id": "14",
      "title": "Audio flamingo 3: Advancing audio intelligence with fully open large audio language models",
      "authors": [
        "Arushi Goel",
        "Sreyan Ghosh",
        "Jaehyeon Kim",
        "Sonal Kumar",
        "Zhifeng Kong",
        "Sang-Gil Lee",
        "Chao-Han Huck Yang",
        "Ramani Duraiswami",
        "Dinesh Manocha",
        "Rafael Valle"
      ],
      "year": "2025",
      "venue": "Audio flamingo 3: Advancing audio intelligence with fully open large audio language models",
      "arxiv": "arXiv:2507.08128"
    },
    {
      "citation_id": "15",
      "title": "Audio-thinker: Guiding audio language model when and how to think via reinforcement learning",
      "authors": [
        "Shu Wu",
        "Chenxing Li",
        "Wenfu Wang",
        "Hao Zhang",
        "Hualei Wang",
        "Meng Yu",
        "Dong Yu"
      ],
      "year": "2025",
      "venue": "Audio-thinker: Guiding audio language model when and how to think via reinforcement learning",
      "arxiv": "arXiv:2508.08039"
    },
    {
      "citation_id": "16",
      "title": "Fdivergence minimization for sequence-level knowledge distillation",
      "authors": [
        "Yuqiao Wen",
        "Zichao Li",
        "Wenyu Du",
        "Lili Mou"
      ],
      "year": "2023",
      "venue": "Fdivergence minimization for sequence-level knowledge distillation",
      "arxiv": "arXiv:2307.15190"
    },
    {
      "citation_id": "17",
      "title": "Distillm-2: A contrastive approach boosts the distillation of llms",
      "authors": [
        "Jongwoo Ko",
        "Tianyi Chen",
        "Sungnyun Kim",
        "Tianyu Ding",
        "Luming Liang",
        "Ilya Zharkov",
        "Se-Young Yun"
      ],
      "year": "2025",
      "venue": "Distillm-2: A contrastive approach boosts the distillation of llms",
      "arxiv": "arXiv:2503.07067"
    },
    {
      "citation_id": "18",
      "title": "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes",
      "authors": [
        "Cheng-Yu Hsieh",
        "Chun-Liang Li",
        "Chih-Kuan Yeh",
        "Hootan Nakhost",
        "Yasuhisa Fujii",
        "Alexander Ratner",
        "Ranjay Krishna",
        "Chen-Yu Lee",
        "Tomas Pfister"
      ],
      "year": "2023",
      "venue": "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes",
      "arxiv": "arXiv:2305.02301"
    },
    {
      "citation_id": "19",
      "title": "Ddk: Distilling domain knowledge for efficient large language models",
      "authors": [
        "Jiaheng Liu",
        "Chenchen Zhang",
        "Jinyang Guo",
        "Yuanxing Zhang",
        "Haoran Que",
        "Ken Deng",
        "Jie Liu",
        "Ge Zhang",
        "Yanan Wu",
        "Congnan Liu"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "Patient knowledge distillation for bert model compression",
      "authors": [
        "Siqi Sun",
        "Yu Cheng",
        "Zhe Gan",
        "Jingjing Liu"
      ],
      "year": "2019",
      "venue": "Patient knowledge distillation for bert model compression",
      "arxiv": "arXiv:1908.09355"
    },
    {
      "citation_id": "21",
      "title": "Fitnets: Hints for thin deep nets",
      "authors": [
        "Adriana Romero",
        "Nicolas Ballas",
        "Samira Kahou",
        "Antoine Chassang",
        "Carlo Gatta",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Fitnets: Hints for thin deep nets",
      "arxiv": "arXiv:1412.6550"
    },
    {
      "citation_id": "22",
      "title": "Autocorrelation matrix knowledge distillation: A task-specific distillation method for bert models",
      "authors": [
        "Kai Zhang",
        "Jinqiu Li",
        "Bingqian Wang",
        "Haoran Meng"
      ],
      "year": "2024",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "23",
      "title": "Qwen2.5-omni technical report",
      "authors": [
        "Jin Xu",
        "Zhifang Guo",
        "Jinzheng He",
        "Hangrui Hu",
        "Ting He",
        "Shuai Bai",
        "Keqin Chen",
        "Jialin Wang",
        "Yang Fan",
        "Kai Dang"
      ],
      "year": "2025",
      "venue": "Qwen2.5-omni technical report",
      "arxiv": "arXiv:2503.20215"
    },
    {
      "citation_id": "24",
      "title": "Mmau: A massive multi-task audio understanding and reasoning benchmark",
      "authors": [
        "Utkarsh Sakshi",
        "Sonal Tyagi",
        "Ashish Kumar",
        "Ramaneswaran Seth",
        "Oriol Selvakumar",
        "Ramani Nieto",
        "Sreyan Duraiswami",
        "Dinesh Ghosh",
        "Manocha"
      ],
      "year": "2024",
      "venue": "Mmau: A massive multi-task audio understanding and reasoning benchmark",
      "arxiv": "arXiv:2410.19168"
    },
    {
      "citation_id": "25",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "26",
      "title": "Qwen3 technical report",
      "authors": [
        "Qwen Team"
      ],
      "year": "2025",
      "venue": "Qwen3 technical report",
      "arxiv": "arXiv:2505.09388"
    }
  ]
}