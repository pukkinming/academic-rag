{
  "paper_id": "2510.18016v1",
  "title": "Vibed-Net: Video Based Engagement Detection Network Using Face-Aware And Scene-Aware Spatiotemporal Cues * A Preprint",
  "published": "2025-10-20T18:48:25Z",
  "authors": [
    "Prateek Gothwal",
    "Deeptimaan Banerjee",
    "Ashis Kumer Biswas"
  ],
  "keywords": [
    "Student Engagement Detection",
    "EfficientNet",
    "Spatiotemporal Modeling",
    "Dual-Stream Architecture"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Engagement detection in online learning environments is vital for improving student outcomes and personalizing instruction. We present ViBED-Net (Video-Based Engagement Detection Network), a novel deep learning framework designed to assess student engagement from video data using a dual-stream architecture. ViBED-Net captures both facial expressions and full-scene context by processing facial crops and entire video frames through EfficientNetV2 for spatial feature extraction. These features are then analyzed over time using two temporal modeling strategies: Long Short-Term Memory (LSTM) networks and Transformer encoders. Our model is evaluated on the DAiSEE dataset, a large-scale benchmark for affective state recognition in e-learning. To enhance performance on underrepresented engagement classes, we apply targeted data augmentation techniques. Among the tested variants, ViBED-Net with LSTM achieves 73.43% accuracy, outperforming existing state-of-the-art approaches. ViBED-Net demonstrates that combining face-aware and scene-aware spatiotemporal cues significantly improves engagement detection accuracy. Its modular design allows flexibility for application across education, user experience research, and content personalization. This work advances video-based affective computing by offering a scalable, high-performing solution for real-world engagement analysis. The source code for this project is available on https:// github.com/prateek-gothwal/ViBED-Net.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Detecting student engagement in online learning environments is a critical yet challenging task in the field of educational technology. As virtual classrooms become increasingly prevalent, the ability to monitor and respond to students' attentiveness in real time has become essential for maintaining instructional quality and personalized learning. Engagement detection from video data, in particular, presents a rich but underexplored opportunity-not only in education, but across domains such as entertainment, marketing, and gaming, where understanding user attention and affective responses can provide actionable insights. For instance, content creators can leverage engagement detection to identify the most captivating segments of their films or games, enabling targeted improvements and audience personalization  [10; 14] .\n\nThe emergence of datasets such as DAiSEE  [3]  and CMOSE  [15]  has facilitated progress in this area by providing annotated video data for training and evaluating models. DAiSEE, in particular, has become a benchmark for developing affective state recognition systems in educational settings. Among the recent advancements, the method proposed by Malekshahi et al.  [6]  currently achieves state-of-the-art performance on the DAiSEE dataset by leveraging a generalizable deep learning framework capable of capturing both spatial and temporal cues from video data. This work surpasses earlier hybrid architectures such as EfficientNetB7 combined with TCN, LSTM, and Bi-LSTM  [8] , demonstrating enhanced generalization and accuracy in learner engagement detection.\n\nIn this paper, we present a novel two-stream architecture called ViBED-Net (Video Based Engagement Detection Network) that combines EfficientNetV2 with temporal modeling modules-Long Short-Term Memory (LSTM) networks and Transformer encoders-to detect student engagement from video with state-of-the-art performance on the DAiSEE dataset. Our approach processes both the facial region and the full video frame simultaneously, capturing fine-grained emotional cues and contextual scene information. The EfficientNetV2 backbone ensures efficient and highresolution spatial feature extraction, while the LSTM and Transformer modules model temporal dependencies crucial for understanding engagement over time. This dual-stream and multi-temporal modeling strategy significantly outperforms existing benchmarks, demonstrating the effectiveness and flexibility of ViBED-Net for real-world engagement analysis applications.\n\nBy addressing a crucial gap in video-based affective computing, our work contributes a powerful, efficient, and extensible framework for multi-domain engagement analysis, with promising applications in online learning analytics, user experience research, and human-computer interaction.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Literature Review",
      "text": "Student engagement detection has gained substantial traction in educational technology, especially with the widespread adoption of online learning platforms. A significant contribution to this domain is the DAiSEE dataset, which provides labeled videos capturing affective states such as engagement, boredom, confusion, and excitement in e-learning settings. As one of the earliest datasets of its kind, DAiSEE has served as a benchmark for numerous deep learning approaches targeting video-based engagement recognition  [3] .\n\nBuilding on this foundation, Selim et al. introduced a hybrid model combining EfficientNetB7 with Temporal Convolutional Networks (TCN), LSTM, and Bi-LSTM for engagement detection. Their approach, evaluated on the DAiSEE dataset, demonstrated strong performance by leveraging both spatial and temporal modeling capabilities  [8] . More recently, Malekshahi et al. proposed a general deep learning model that achieved state-of-the-art accuracy on DAiSEE by capturing engagement-related spatiotemporal patterns in a scalable and modular manner  [6] .\n\nAnother influential work, Do I Have Your Attention?, introduced a large-scale engagement dataset and established baseline models. The authors further validated their approach on DAiSEE, providing insights into cross-dataset generalization and the robustness of attention-based engagement predictors  [10] .\n\nIn parallel, the CMOSE dataset emerged as a high-quality, multimodal resource that addresses limitations in labeling granularity and modality diversity. By incorporating eye gaze, posture, and facial expression modalities with synchronized high-quality labels, CMOSE enables more holistic modeling of student engagement  [15] .\n\nCollectively, these studies underscore the importance of both spatial and temporal modeling in engagement prediction. However, many prior architectures focus primarily on either facial expressions or coarse global features, often neglecting the combined value of both streams. Our proposed model, ViBED-Net, addresses this gap through a dual-stream architecture that integrates EfficientNetV2 with both LSTM and Transformer-based temporal encoders. This allows it to simultaneously capture facial micro-expressions and full-scene context, achieving superior performance on DAiSEE and offering broader applicability across domains such as content creation, marketing analytics, and user experience research.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Daisee",
      "text": "The DAiSEE (Dataset for Affective States in E-Environments) dataset is a large-scale video corpus curated to support research on affective computing in e-learning environments. It comprises 9,068 video snippets, each approximately 10 seconds long, collected from 112 participants in naturalistic online learning conditions. Each clip is annotated for four affective states-engaged, bored, confused, and excited-at four ordinal intensity levels: very low, low, high, and very high. Annotations were obtained via crowdsourcing and refined using majority voting to ensure label quality. The dataset includes diverse variations in lighting, facial occlusion, and head pose, making it suitable for training models that generalize to real-world conditions. Due to its scale, label richness, and ecological validity, DAiSEE has become a benchmark for video-based engagement detection in academic settings  [3] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Daisee",
      "text": "The DAiSEE (Dataset for Affective States in E-Environments) dataset is a large-scale video corpus curated to support research on affective computing in e-learning environments. It comprises 9,068 video snippets, each approximately 10 seconds long, collected from 112 participants in naturalistic online learning conditions. Each clip is annotated for four affective states-engaged, bored, confused, and excited-at four ordinal intensity levels: very low, low, high, and very high. Annotations were obtained via crowdsourcing and refined using majority voting to ensure label quality. The dataset includes diverse variations in lighting, facial occlusion, and head pose, making it suitable for training models that generalize to real-world conditions. Due to its scale, label richness, and ecological validity, DAiSEE has become a benchmark for video-based engagement detection in academic settings  [3] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Methodology",
      "text": "In this study, we introduce a dual-stream deep learning architecture designed to detect student engagement levels from video data. Our approach addresses two primary challenges: (1) the class imbalance in datasets like DAiSEE, particularly for \"very low\" and \"low\" engagement classes, and (2) the need to effectively model temporal dynamics inherent in video sequences.\n\nTo mitigate class imbalance, we employ targeted data augmentation techniques. These include salt-and-pepper noise, random translations, brightness and contrast adjustments, Gaussian blur, horizontal flips, and elastic transformations. Such augmentations have been shown to enhance model robustness and generalization, especially in scenarios with limited data for certain classes.\n\nFor feature extraction, we utilize EfficientNetV2, a convolutional neural network known for its balance between accuracy and computational efficiency. We extract two sets of features from each video frame: one from the entire frame to capture contextual information, and another from the cropped facial region to focus on fine-grained emotional cues. These features are precomputed and stored to expedite the training process.\n\nThe extracted features are then input into two separate temporal modeling modules. By default, we use Long Short-Term Memory (LSTM) networks, which are adept at capturing temporal dependencies in sequential data and modeling the progression of engagement levels over time. Additionally, we also experimented with a Transformer-based architecture as an alternative temporal encoder. Transformers, with their self-attention mechanism, are capable of capturing longrange dependencies and offer a parallelizable alternative to recurrent architectures. The outputs from these temporal models are concatenated and passed through a Multi-Layer Perceptron (MLP) to produce the final engagement level prediction. EfficientNet is a family of convolutional neural networks introduced by Tan and Le  [11] , known for achieving high accuracy with significantly fewer parameters and FLOPs compared to traditional architectures like ResNet or Inception. The core innovation behind EfficientNet is the compound scaling method, which uniformly scales the network's depth, width, and input resolution using a set of fixed scaling coefficients. This enables more efficient use of model capacity and computational resources without requiring manual tuning of each dimension.\n\nIn our proposed architecture, we utilize EfficientNetV2, an enhanced version of the original that incorporates fused MBConv layers and progressive learning strategies for improved training speed and parameter efficiency  [12] . Its balance between inference time and representational power makes it particularly suitable for large-scale video processing.\n\nFor each 10-second video clip in the DAiSEE dataset, we uniformly sample 60 frames to capture temporal variations in engagement. These frames are processed through two parallel EfficientNetV2 streams. The first stream receives the entire video frame as input, capturing global context such as posture, body language, and background. The second stream uses OpenCV-based face detection to crop the facial region from each frame, allowing the model to focus on fine-grained facial expressions and micro-movements crucial for affective analysis.\n\nIn both streams, we remove the classification head of EfficientNetV2 and extract features from the penultimate layer, resulting in a feature vector of size 1028 per frame. With 60 frames per video, this produces a final feature sequence of shape (60, 1028) for each stream. These feature arrays are precomputed and saved as pickle files to reduce computational overhead during training, allowing the LSTM components to operate directly on the extracted spatiotemporal representations.\n\nEach of these feature sequences is then passed to a dedicated LSTM/Transformer network for temporal modeling, as described in the following subsection.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Temporal Modeling Using Lstm And Transformer",
      "text": "While EfficientNetV2 extracts powerful spatial features from individual frames, engagement is inherently a temporal concept that unfolds over a sequence of video frames. To model this temporal evolution, we explore two distinct approaches: Long Short-Term Memory (LSTM) networks and a Transformer-based encoder. Both are designed to capture temporal dependencies, but with different mechanisms and computational trade-offs. In practice, we compare both variants during training to assess performance trade-offs in terms of accuracy, generalization, and computational cost.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Lstm Architecture",
      "text": "LSTMs are a type of recurrent neural network (RNN) well-suited for capturing long-term dependencies in sequential data. They mitigate the vanishing gradient problem of traditional RNNs through the use of internal memory cells and gating mechanisms-namely the input, forget, and output gates-which allow selective retention and updating of information over time  [4] . This makes them highly effective for modeling the subtle progression of engagement cues across consecutive frames.\n\nIn our architecture, we use two independent LSTM networks: one for the full-frame features and one for the cropped facial region. Each LSTM receives a sequence of 60 feature vectors (one per frame), with an input size of 1028, corresponding to the output dimensionality from EfficientNetV2. The hidden size is set to 1024, with 1 layer and a dropout rate of 0.1 applied during training to prevent overfitting. The final hidden states from both LSTMs (one per stream) are extracted after processing the last time step and concatenated to form a unified representation for classification.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Transformer Architecture",
      "text": "In addition to LSTMs, we also experiment with a Transformer-based encoder, inspired by its success in various video understanding tasks  [13] . Unlike recurrent networks, Transformers use a self-attention mechanism to weigh the importance of each frame in the sequence relative to all others. This allows the model to capture long-range temporal dependencies in parallel, making it both flexible and scalable.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Our Transformer Model Consists Of:",
      "text": "• A positional encoding module, which injects temporal order information into the input sequence (since Transformers lack inherent sequential bias)  [13] .\n\n• A stack of 2 Transformer encoder layers, each with 8 attention heads, a hidden dimension of 1024, and feed-forward layers of size 2048.\n\n• Layer normalization and residual connections applied after each sub-layer to stabilize training  [13] .\n\nThe input to the Transformer is the same 60 × 1028 sequence of features per stream. We apply separate Transformer encoders for the full-frame and face streams. After processing, the final token representations (either mean pooled or using a learnable [CLS]-like token) from each stream are concatenated and passed to the same MLP classifier as used with LSTMs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Self-Attention Mechanism",
      "text": "The core of the Transformer encoder is the scaled dot-product attention, which computes the relevance of each input frame (or token) to every other frame. Given a sequence of input feature vectors X ∈ R T ×d , where T = 60 is the number of frames and d = 1028 is the input dimension, the inputs are linearly projected into three matrices:\n\nwhere W Q , W K , and W V are learnable projection matrices, and Q, K, and V represent the queries, keys, and values respectively.\n\nThe self-attention output is computed as:\n\nHere, d k is the dimensionality of the key vectors (typically d k = d/h for h attention heads), and the softmax function is applied row-wise to generate attention weights that sum to 1.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Multi-Head Attention",
      "text": "To capture diverse patterns from different subspaces, we use multi-head attention, which runs h attention operations in parallel and concatenates their results:\n\nwhere\n\nW O is a learnable output projection matrix that combines the individual attention head outputs. This enables the model to learn relationships at multiple levels of granularity.\n\nFeed-Forward Network. Each Transformer encoder block also includes a position-wise feed-forward network applied independently to each token:\n\nResidual connections and layer normalization are applied after both the attention and feed-forward layers to ensure stable training.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Classification Using Mlp",
      "text": "After temporal modeling via the dual-stream LSTM networks, we obtain two fixed-length feature vectors-each of dimension 1024-representing the temporal summaries of the full-frame and face-based input sequences. These two vectors are concatenated to form a single combined feature vector of size 2048, capturing both scene-level and facial temporal dynamics in a unified representation.\n\nThis concatenated vector is then passed to a Multi-Layer Perceptron (MLP) classifier to predict the final engagement level. The MLP consists of a fully connected layer that reduces the feature dimensionality from 2048 to 512, followed by a ReLU activation function and a dropout layer with a dropout rate of 0.3 to prevent overfitting. A final fully connected layer maps the 512-dimensional representation to the number of engagement classes (e.g., 4 in the DAiSEE dataset: Very Low, Low, High, Very High). The full MLP pipeline can be seen in Fig.  3 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Figure 3: Mlp Architecture",
      "text": "This architecture allows the network to effectively combine and process both spatial and temporal features to classify student engagement levels with high accuracy. The entire classifier is trained using the cross-entropy loss function in conjunction with the AdamW optimizer, as described in the training section.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Details",
      "text": "The proposed architecture is trained in a supervised fashion using the cross-entropy loss, which is commonly applied to multi-class classification tasks. The loss for a single sample is calculated as shown in Equation (  6 ):\n\nwhere C is the number of classes, y i is the ground-truth one-hot encoded label, and ŷi is the predicted probability for class i. This loss penalizes the model proportionally to the confidence in incorrect predictions.\n\nFor optimization, we use the AdamW optimizer, which improves upon Adam by decoupling the weight decay from the gradient update. The parameter update rule for each step t is given in Equation (  7 ):\n\nwhere η is the learning rate (set to 0.001), mt and vt are the bias-corrected first and second moment estimates of the gradients, ϵ is a small constant for numerical stability, and λ is the weight decay coefficient.\n\nTraining is performed using a mini-batch size of 32 over 40 epochs. To improve training efficiency, we precompute and cache the spatial features extracted from EfficientNetV2 as tensors of shape 60 × 1028 per video. These sequences are fed into the temporal and classification modules of the model.\n\nThe total loss minimized during training is the average cross-entropy loss over the N training samples, as expressed in Equation (  8 ):\n\nValidation accuracy is monitored across epochs to ensure that the model generalizes well to unseen data.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "To evaluate the performance of our proposed engagement detection framework, we report the results using four standard classification metrics: Accuracy, Precision, Recall, and F1 score. These metrics provide a comprehensive understanding of how well the model performs, particularly in imbalanced class settings such as the DAiSEE dataset.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "• Accuracy: The proportion of total predictions that were correct.\n\n• Precision: The proportion of positive identifications that were actually correct. Precision = T P T P + F P\n\n• Recall: The proportion of actual positives that were correctly identified.\n\n• F1-Score: The harmonic mean of precision and recall, providing a balanced measure between the two.\n\nThese metrics are computed per class and averaged using a macro-average strategy to give equal weight to each class. This approach is especially useful for datasets with class imbalance. A closer look at the per-class metrics reveals important nuances. The LSTM model exhibits perfect precision (1.00) for the \"Very Low\" engagement class, meaning that every prediction it makes for this class is correct. However, its recall is only 0.25, indicating that it fails to identify 75% of the actual \"Very Low\" instances. This suggests the model is conservative in its predictions for this minority class, prioritizing high confidence over high detection rates. In contrast, the model achieves a strong balance between precision and recall for the \"Low\" (0.80 F1-score) and \"Very High\" (0.74 F1-score) classes, indicating robust performance. The \"High\" engagement class shows slightly lower but still competitive performance with a 0.72 F1-score.\n\nThe Transformer-based model, with an overall accuracy of 62.39%, struggles significantly with the less-represented classes. It completely fails to identify any \"Very Low\" instances (0.00 precision and recall) and shows very low recall (0.13) for the \"Low\" engagement class. This suggests that the self-attention mechanism, without a recurrent structure, may be less effective at capturing the subtle and sparse temporal cues associated with disengagement in this dataset. While its performance on the \"High\" engagement class is reasonable (0.67 F1-score), its inability to handle class imbalance effectively makes it a less viable solution compared to the LSTM variant.\n\nThese findings are further corroborated by the confusion matrix for the LSTM model, shown in Fig.  4 , which visualizes the misclassifications between classes. The matrix clearly shows that most errors occur between adjacent engagement levels (e.g., \"Low\" vs. \"High\"), which is expected given the subjective nature of engagement annotation.\n\nFinally, Table  2  situates our results within the broader landscape of existing methods. The ViBED-Net (LSTM) model not only surpasses the Transformer variant but also outperforms all previously reported state-of-the-art models on the DAiSEE dataset, establishing a new benchmark for video-based engagement detection. CNN-RNN Baseline (DAiSEE)  [3]  53.9 I3D (Inflated 3D ConvNet)  [16]  52.35 DFSTN (SE-ResNet-50 + LSTM + GALN)  [5]  58.84 ResNet + TCN Hybrid  [1]  63.9 EfficientNetB7 + LSTM  [8]  67.48 BiusFPN + ICCSA  [7]  68.16 General Model for Learner Engagement  [6]  68.57 Affect-driven Ordinal Engagement  [2]  67.4 VisioPhysioENet  [9]  63.09\n\nProposed ViBED-Net (Transformer) 62.39 Proposed ViBED-Net (LSTM) 73.43",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduced ViBED-Net, a novel dual-stream deep learning framework for video-based student engagement detection. By leveraging both face-aware and scene-aware spatiotemporal cues through parallel EfficientNetV2 feature extractors and LSTM-based temporal modeling, our approach achieved a new state-of-the-art accuracy of 73.43% on the challenging DAiSEE dataset. Our results underscore the significant performance gains from combining fine-grained facial expression analysis with broader contextual information. The superior performance of the LSTM variant compared to the Transformer-based model suggests that recurrent architectures remain highly effective for modeling the nuanced temporal evolution of engagement in this context.\n\nDespite its strong performance, our work has several limitations. First, while achieving high precision on the \"Very Low\" engagement class, the model's recall was low, indicating a difficulty in identifying all instances of this underrepresented class. This highlights the ongoing challenge of class imbalance in affective computing datasets. Second, our current framework relies on pre-computed features, which, while efficient for training, may not be ideal for fully real-time, end-to-end deployment. Finally, the model's robustness has been validated on a single dataset and assumes a relatively consistent, front-facing camera view, which may not hold true in all real-world learning environments.\n\nFuture research can extend this work in several promising directions. One key avenue is the integration of additional modalities to create a more holistic model of student engagement. Incorporating eye-gaze tracking, for instance, could provide a powerful, direct measure of visual attention. Furthermore, enhancing the model to accommodate different camera placements and viewing angles would significantly improve its practical applicability. This could be achieved by training on more diverse data or by incorporating view-invariant feature learning techniques. Finally, exploring self-supervised learning on large, unlabeled video datasets could help learn more generalizable representations of human behavior, which could then be fine-tuned for engagement detection, potentially improving cross-dataset performance and overall robustness.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Acknowledgment",
      "text": "This material is based upon work supported by the National Science Foundation under Grant No. 2329919.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Frames from DAiSEE dataset",
      "page": 3
    },
    {
      "caption": "Figure 2: ViBED-Net Architecture",
      "page": 4
    },
    {
      "caption": "Figure 3: Figure 3: MLP Architecture",
      "page": 6
    },
    {
      "caption": "Figure 4: , which visualizes",
      "page": 8
    },
    {
      "caption": "Figure 4: Confusion matrix EfficientNet+LSTM on DAiSEE test set.",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Per-Class Performance Comparison of LSTM and Transformer Models",
      "page": 8
    },
    {
      "caption": "Table 1: presents a detailed per-class performance comparison between the LSTM-based and Transformer-based variants",
      "page": 8
    },
    {
      "caption": "Table 2: situates our results within the broader landscape of existing methods. The ViBED-Net (LSTM) model",
      "page": 8
    },
    {
      "caption": "Table 2: Accuracy Comparison with Existing Approaches on the DAiSEE Dataset",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Improving state-of-the-art in detecting student engagement with resnet and tcn hybrid network",
      "authors": [
        "Ali Abedi",
        "Shehroz Khan"
      ],
      "year": "2021",
      "venue": "2021 18th Conference on Robots and Vision (CRV)",
      "doi": "10.1109/CRV52889.2021.00028"
    },
    {
      "citation_id": "2",
      "title": "Affect-driven ordinal engagement measurement from video",
      "authors": [
        "Ali Abedi",
        "S Shehroz",
        "Khan"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "3",
      "title": "Towards user engagement recognition in the wild",
      "authors": [
        "Abhay Gupta",
        "D' Arjun",
        "Kamal Cunha",
        "Vineeth Awasthi",
        "Balasubramanian",
        "Daisee"
      ],
      "year": "2016",
      "venue": "Towards user engagement recognition in the wild",
      "arxiv": "arXiv:1609.01885"
    },
    {
      "citation_id": "4",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "5",
      "title": "Deep facial spatiotemporal network for engagement prediction in online learning",
      "authors": [
        "Jiacheng Liao",
        "Yan Liang",
        "Jiahui Pan"
      ],
      "year": "2021",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "6",
      "title": "A general model for detecting learner engagement: implementation and evaluation",
      "authors": [
        "Somayeh Malekshahi",
        "Javad Kheyridoost",
        "Omid Fatemi"
      ],
      "year": "2024",
      "venue": "A general model for detecting learner engagement: implementation and evaluation",
      "arxiv": "arXiv:2405.04251"
    },
    {
      "citation_id": "7",
      "title": "Student engagement analysis using bi-up sampling feature pyramid network with inter cross coordinate self attention",
      "authors": [
        "I Jeena Naveen",
        "Ajay Jacob",
        "Mandava"
      ],
      "year": "2025",
      "venue": "Journal of Wireless Mobile Networks, Ubiquitous Computing, and Dependable Applications"
    },
    {
      "citation_id": "8",
      "title": "Students engagement level detection in online e-learning using hybrid efficientnetb7 together with tcn, lstm, and bi-lstm",
      "authors": [
        "Tasneem Selim",
        "Islam Elkabani",
        "Mohamed Abdou"
      ],
      "year": "2022",
      "venue": "Students engagement level detection in online e-learning using hybrid efficientnetb7 together with tcn, lstm, and bi-lstm"
    },
    {
      "citation_id": "9",
      "title": "Multimodal Engagement Detection using Visual and Physiological Signals",
      "authors": [
        "Alakhsimar Singh",
        "Nischay Verma",
        "Kanav Goyal",
        "Amritpal Singh",
        "Puneet Kumar",
        "Xiaobai Li",
        "Visio-Physioenet"
      ],
      "year": "2024",
      "venue": "Multimodal Engagement Detection using Visual and Physiological Signals",
      "arxiv": "arXiv:2409.16126"
    },
    {
      "citation_id": "10",
      "title": "Do i have your attention: A large scale engagement prediction dataset and baselines",
      "authors": [
        "Monisha Singh",
        "Ximi Hoque",
        "Donghuo Zeng",
        "Yanan Wang",
        "Kazushi Ikeda",
        "Abhinav Dhall"
      ],
      "year": "2023",
      "venue": "Proceedings of the 25th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "11",
      "title": "Rethinking model scaling for convolutional neural networks",
      "authors": [
        "Mingxing Tan",
        "Quoc Le",
        "Efficientnet"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "12",
      "title": "Efficientnetv2: Smaller models and faster training",
      "authors": [
        "Mingxing Tan",
        "Quoc Le"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "13",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "The faces of engagement: Automatic recognition of student engagementfrom facial expressions",
      "authors": [
        "Jacob Whitehill",
        "Zewelanji Serpell",
        "Yi-Ching Lin",
        "Aysha Foster",
        "Javier Movellan"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "CMOSE: Comprehensive Multi-Modality Online Student Engagement Dataset with High-Quality Labels",
      "authors": [
        "Chi-Hsuan Wu",
        "Shih-Yang Liu",
        "Xijie Huang",
        "Xingbo Wang",
        "Rong Zhang",
        "Luca Minciullo",
        "Kai Wong",
        "Kenny Yiu",
        "Kwang-Ting Kwan",
        "Cheng"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "An novel end-to-end network for automatic student engagement recognition",
      "authors": [
        "Hao Zhang",
        "Xiaofan Xiao",
        "Tao Huang",
        "Sanya Liu",
        "Yu Xia",
        "Jia Li"
      ],
      "year": "2019",
      "venue": "2019 IEEE 9th International Conference on Electronics Information and Emergency Communication (ICEIEC)",
      "doi": "10.1109/ICEIEC.2019.8784507"
    }
  ]
}