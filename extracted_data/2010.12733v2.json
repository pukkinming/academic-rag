{
  "paper_id": "2010.12733v2",
  "title": "Learning Fine-Grained Cross Modality Excitement For Speech Emotion Recognition",
  "published": "2020-10-24T01:17:58Z",
  "authors": [
    "Hang Li",
    "Wenbiao Ding",
    "Zhongqin Wu",
    "Zitao Liu"
  ],
  "keywords": [
    "speech emotion recognition",
    "human-computer interaction",
    "multimodal learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition is a challenging task because the emotion expression is complex, multimodal and fine-grained. In this paper, we propose a novel multimodal deep learning approach to perform fine-grained emotion recognition from reallife speeches. We design a temporal alignment mean-max pooling mechanism to capture the subtle and fine-grained emotions implied in every utterance. In addition, we propose a cross modality excitement module to conduct sample-specific adjustment on cross modality embeddings and adaptively recalibrate the corresponding values by its aligned latent features from the other modality. Our proposed model is evaluated on two well-known real-world speech emotion recognition datasets. The results demonstrate that our approach is superior on the prediction tasks for multimodal speech utterances, and it outperforms a wide range of baselines in terms of prediction accuracy. Further more, we conduct detailed ablation studies to show that our temporal alignment mean-max pooling mechanism and cross modality excitement significantly contribute to the promising results. In order to encourage the research reproducibility, we make the code publicly available at https://github.com/tal-ai/FG_CME.git.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion plays an important role in human communication. It has significant effects on information transmission and human interactions. The expression of emotion is usually multimodal, including voice, speech content, facial expressions, etc. With the development of machine learning and deep learning, a large number of speech interaction systems have emerged, such as voice assistants  [1, 2, 3] , intelligent tutoring systems  [4, 5] , etc. Speech emotion recognition is particularly useful to this kind of systems to better understand the content of the speech and generate a natural response based on the recognized emotion  [6, 7, 8] .\n\nVarious types of machine learning methods have been applied to improve the performance of speech emotion recognition. However, the majority of them are not sufficient for accurately detecting speech emotions from human due to the following two challenges. First, emotion is multimodal. Both the vocal sound and the linguistic content express emotions. It is insufficient to only consider information from single modality when conducting emotion recognition. For example, sentences expressed in different tones will show different emotions even with the same content. Second, emotion expression is finegrained. The emotion of a sentence is often expressed by specific words or voice fragments. The utterance-level emotion\n\nThe corresponding author: Zitao Liu. recognition approaches treat every words equally and may fail to capture fine-grained and subtle emotions.\n\nRecently, a large number of approaches have been developed to address above challenges  [9, 10, 11, 12] . Researchers have jointly combined multiple modalities for speech emotion recognition and achieved better results compared to methods that only consider information from single modality  [9, 11] . For example, Schuller et al. combined acoustic features and linguistic information in a hybrid architecture to identify emotional key phrases, and assessed the emotional salience of verbal cues from both phoneme sequences and words  [9] . Yoon et al. built deep neural networks to learn vocal representations and text representations and concatenated them for emotion classification  [11] . In spite of the success of above methods, there is still large room for improvement. Most existing approaches are based on utterance-level fusion, such as directly concatenating acoustic and text features. These methods pay little attention to fine-grained multimodal emotional information and are not able to capture subtle emotions in real-life speech conversations. Therefore, it is very necessary to conduct fine-grained learning of spoken speeches and make full use of relationships between acoustic and semantic modalities.\n\nTo overcome above challenges, we propose a novel multimodal neural framework to perform fine-grained emotion recognition. Instead of simply using utterance-level features, we propose a temporal alignment mean-max pooling operation and a cross modality excitation mechanism to capture the interrelations between different modalities. We have conducted extensive experiments on two public available emotion recognition datasets, i.e., Interactive Emotional Dyadic Motion Capture database (IEMOCAP) 1 and Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) 2 . The results show that our method outperforms all baselines.\n\nOverall this paper makes the following contributions:\n\n• We conduct fine-grained learning with a temporal alignment mean-max pooling operation and cross-modality excitement mechanism to obtain plentiful cross modality information from both voice fragments and sentences.\n\n• We design and evaluate our approach quantitatively by using two well-known real-world emotion recognition datasets. Besides, we present detailed ablation studies to demonstrate the effectiveness of each component in our proposed model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Speech emotion recognition has been studied for decades in both the machine learning and speech communities  [13, 14, 15, 16, 17] . Following the mainstream direction, researchers have extracted hand-craft features from audio data and applied them to classic supervised learning methods such as support vector machines  [15] , hidden Markov models  [13] , Gaussian mixture models  [17] , Tree-Based models  [16] , etc. With recent advances of deep learning, various neural networks have been used in the task of speech emotion recognition  [14, 18, 19, 20] . Han et al. extracted speech signal features from audio and used deep neural networks to classify speech emotion  [18] . Badshah et al. extracted raw spectrograms features and used convolutional neural network (CNN) to extract high-level features to build the model  [19] . Due to the sequential structure of audio, Trigeorgis et al. proposed an end-to-end model that employs long short-term memory (LSTM) network to capture the contextual information  [14] . Mirsamadi et al. used recurrent neural networks with local attention mechanism to automatically discover emotionally relevant features from speech and provided more accurate predictions  [20] .\n\nIn addition to tackle the speech emotion recognition problem by directly applying deep neural networks on audio datasets, multimodal learning frameworks that jointly consider emotions implied in different modalities  [10, 21, 22] . For instance, Tzirakis et al. provided a multimodal system to perform an end-to-end spontaneous emotion prediction task from speech and visual data  [21] . Lee et al. proposed an attention model for multimodal emotion recognition from speech and text data, and provided an effective method to learn the correlation between two output feature vectors from separate yet jointly trained CNNs  [22] .\n\nThe closest work to our research is Xu's  [10] , where they proposed a fine-grained method to learn alignment between the original speech and the recognized text with attention mechanism. Our approach is different from that: (1) we propose a temporal mean-max alignment pooling method to combine each word and its corresponding voice instead of attention mechanism; and (2) we design a cross modality excitement module to enhance the interactions between the two modalities.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Our Approach",
      "text": "The architecture of our purposed model is shown in Figure .1, which is composed of four key components: (1) unimodal embedding module, which captures the acoustic and semantic information respectively; (2) temporal alignment mean-max pooling, which aggregates the acoustic embedding for each word based on its corresponding speech frames, (3) cross modality excitement module, which converts unimodal information into a fine-grained multimodal representation through the cross modality excitement mechanism; and (4) multimodal prediction, which utilizes a bidirectional LSTM network to model the sequential information within the entire speech sentence.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Unimodal Embedding",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Acoustic Embedding",
      "text": "For each utterance, we first transform it into n frames {fi} n i=1 of width 25ms and step 10ms. We extract low-level features, i.e., x a i ∈ R p , from each frame fi and the utterance-level feature is\n\nThen we extract the context-aware acoustic embedding Xa 1 ∈ R q×n by feeding X a into a multi-layer 1-d CNN, i.e., Xa 1 = CNN(X a ). p and q represent the dimensions of low-level features and the extracted acoustic representations respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Semantic Embedding",
      "text": "For each utterance, we obtain its transcription by an automatic speech recognition (ASR) service. Then, we extract the linguistic representation X s ∈ R l×m via pre-trained language models  [23] . m denotes the number of words in the ASR transcription. After that, we apply a linear transformation operator W s ∈ R t×l on X s to obtain the fine-tuned semantic embeddings Z s ∈ R t×m , i.e., Z s = W s X s , which are used in the downstream emotion recognition task.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Temporal Alignment Mean-Max Pooling",
      "text": "To capture the subtle emotion expression implied in the utterance, we conduct the fine-grained recognition by extracting the underlying word-level acoustic representation. Hence, we propose a temporal alignment mean-max pooling to aggregate the frame-level acoustic representation Xa . Instead of the simply averaging all the frame-level acoustic embeddings like mean pooling operator, our temporal alignment pooling operator uses a word-level binary alignment matrix A to only select relevant frames' acoustic features for each corresponding word. The binary alignment matrix A ∈ R n×m is a block-wise diagonal matrix, which is calculated by taking an orthographic transcription of an audio file and generating a time-aligned version using a pronunciation dictionary to look up phones for words. The temporal aligned word-level acoustic representation\n\n] can be obtained as follows:\n\nwhere a•,j represents the jth row of alignment matrix A, z a i represents the acoustic feature for ith frame. [•, •] denotes the concatenation operation, and represents the element-wise product.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross Modality Excitation",
      "text": "To learn the nonlinear interactions between the acoustic and semantic modalities, similar to  [24] , we develop a cross modality excitation (CME) module. The CME module serves as samplespecific activations and is learned for each embedding dimension by a self-gating mechanism based on dimensional dependence. Specifically, we employ a simple gating mechanism with a sigmoid activation, i.e., E a = δ(W a Z s ), E a ∈ R q×m and E s = δ(W s Z a ), E s ∈ R t×m , where δ(•) denotes the sigmoid function and W a ∈ R q×t , W s ∈ R t×q represent linear projection operator for the aligned acoustic and semantic representations respectively. E a and E s are the excitation matrixs, which act as the weights adapted to the corresponding features. Then the temporal aligned word-level acoustic and semantic representations are adaptively calibrated by the corresponding cross modality excitation matrices and the cross modality adapted representations Za and Zs are computed as Za = E a Z a and Zs = E s Z s .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Prediction",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Let Za",
      "text": "i and zs i be the acoustic and semantic representations of ith word in the original utterance, i.e., Za = {z a i } m i=1 and Zs = {z s i } m i=1 . Each word's multimodal representation gi is computed by concatenating za i and zs i , i.e., gi = [z a i , zs i ]. In the multimodal prediction layer, we utilize a bi-directional LSTM to capture the word-level sequential patterns on top of the multimodal representation of each word gi. The resulting hidden state of hi is the concatenation of hidden representations from both directions, i.e., hi = [ -→ hi, ←hi]. After that, a max-pooling layer is applied to aggregate the sequential information for all the hidden states his in the utterance sequence, i.e., h * i = maxpool({hi} m i=1 ). Finally, we use a two-layer fully-connected feed forward network (FCN) to conduct the final predictions, i.e.,\n\nwhere pi is the probabilistic vector that indicates the final probabilities of class memberships of utterance i. In this work, we use the multi-class cross-entropy loss to optimize the prediction accuracy, which is defined as follows:\n\nwhere p i,k is the kth element of pi, and y i,k = 1 if the ith sample belongs to the kth class.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iemocap",
      "text": "IEMOCAP consists of five sessions of dyadic conversations between different pairs of actors. The total number of utterances in IEMOCAP is 10,039. For each utterance, the audio, transcriptions, video and actor's motion-capturing recordings are collected and emotion labels are annotated by the majority voting results from three experienced evaluators. Following the similar experiment settings with prior studies  [10, 11] , we use four emotions (angry, happy, neutral and sad) for classification evaluation. Besides that, we perform the five-fold cross validation and the average results are reported.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ravdess",
      "text": "RAVDESS is a validated multimodal database of emotional speeches and songs, which includes 9 hours of speeches and 1,440 utterance. Unlike the IEMOCAP, the speech content of each utterance in RAVDESS is the same and actors express the different emotions through their different tones. The label of each utterance is validated by multiple raters and the intensities of emotions are conducted based on raters' responses on emotion strength. In our experiment, we use the same four emotions (angry, happy, neutral and sad) for classification. The five-fold cross validation is also incorporated for the robustness of the final conclusions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baselines",
      "text": "To evaluate the effectiveness of our proposed framework, we carefully choose the following state-of-the-art emotion classification approaches as our baselines: (1) Acoustic-CNN+LSTM(CNN + LSTM a ): We train the similar model proposed by Satt et al., which uses CNN for extracting acoustic embeddings and applies the LSTM layer at the top for aggregating utterance embedding  [27] .\n\n(2) Acoustic-LSTM+Attention(LSTM + Attn a ): A bidirectional LSTM is used for generating the acoustic embeddings from the frame-wise low-level features. An attention layer is used for the final aggregation  [20] . (  4 ) Semantic-LSTM+Attention(LSTM + Attn s ): Similar to LSTM + Att a ) but instead only the text features are used.\n\n(5) Multimodal-Utterance+Concat(UttConcat m ): Two individual bi-directional LSTMs are used as unimodal encoders for acoustic and semantic features and the unimodal embeddings are concatenated at utterance level for the final prediction  [11] .  (6)  Multimodal-AttentionFusion(AttnFusion m ): Similar to Utt + Concat m but instead applying the multihead attention for fusing the multimodal features at word level  [10] .  (7)  Multimodal-MultihopAttention(MHA m ): Similar to UttConcat m and AttnFusion m , except that a multi-hop attention mechanism is used for the inference of correlations between two modalities  [28] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "For the acoustic embeddings, we first transform sample's audio signal into frames with 25ms and step size 10ms. Then, we use a Python Library  [29]  to extract the 40-dimensional filterbank features from each frame. The 1d CNN network we used for embedding the acoustic features has 3 layers with kernel sizes (5, 2, 2) and the stride size of each kernel is set to 1 for keeping the length of the frame sequences. The number of the filters are set to (64,128,128) and the dimension of the output acoustic embedding Z a is 256, which is generated by concatenating the corresponding word level mean-pooling and max-pooling embeddings.\n\nFor the semantic embeddings, we use a 300-dimensional pre-trained GloVe embedding  [23]  for mapping the words into the fixed-length vectors for each utterance transcription. The dimension of linear operator W s we used for fine-tuning the pre-trained word embeddings is 256. To implement the model, we use 256 hidden units in bidirectional LSTM, i.e. d h i , in the ). To train our model, we use Adam optimizer  [30]  with learning rate of 0.0005.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results & Analysis",
      "text": "We evaluate and compare the performance of different method based on two widely used metrics for emotion classification: weighted accuracy (WA) that is the overall classification accuracy and unweighted accuracy (UA) that is the average recall over the emotion categories. The results of our experiments show that our approach outperforms all other methods on both IEMOCAP and RAVDESS. Specifically, from Table .1, we find the following results: (1) When comparing the unimodal based models (CNN + LSTM a , LSTM + Attn a ,LSTM + Attn s ) with the multimodal ones (UttConcat m , AttnFusion m , MHA m ), we find that the multimodal features provide a huge boost for the models' performance on both WA and UA. (2) Comparing UttConcat m and MHA m with AttnFusion m , the fusion of multimodal features in a fine-grained manner provides the model with a strong capability to capture the subtle emotions expressed within the utterance level.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Studies",
      "text": "Apart from applying the comparisons above, we also present the ablation results of different key components in our proposed algorithm in Table .1. Overall, each component of our proposed model plays an important role in improving the model's performance on both datasets. For model without acoustic excitement (w/o Excite a ), we find its performance drops dramatically in RAVDESS, which is consistent with our expectations since samples in the RAVDESS share the similar text inputs. Comparing model without any excitement (w/o Excite m ) with model without either acoustic excitement or semantic excitement (w/o Excite s ), we observe that the model's performance is boosted by both excitements and the best performance is achieved by using both excitement modules simultaneously. At last, we replace our pre-calculated word-level binary alignment matrix A with the CTC predicting alignment matrix (w/ CTC Align)  [31] , we find the model's performance suffers a great loss, which indicates that the pre-calculated alignment matrix A is more compatible with our proposed algorithm.\n\nTo provide a better view about the effectiveness of each components, we use t-SNE to visualize the embeddings of validation samples from RAVDESS generated by different models in Fig.  2 . From the figure, we observe that embeddings of different emotion category's samples from the model without both excitement modules is hard to be separated. After employing the CME block, the intra-distance between different categories is enlarged, which makes it easy for the final classifier to generate a robustness split boundaries for each class.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a novel multimodal deep learning framework to perform fine-grained learning of voice and text in speeches. Through applying the temporal alignment operator combining with the cross modality excitation module, we successfully generated a powerful multimodal representation for each utterance in a fine-grained manner. The experiments on two open source English emotion datasets: IEMOCAP and RAVDESS demonstrates the effectiveness of our purposed algorithm. Besides, the influence of each component of our model is presented via the detailed ablation studies and analysis.",
      "page_start": 1,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed neural framework.",
      "page": 2
    },
    {
      "caption": "Figure 2: Visualization of embeddings for validation samples from RAVDESS with different ablated components. (a) w/o both excitement",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Experimental results on IEMOCAP and RAVDESS",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Alexa, siri, cortana, and more: an introduction to voice assistants",
      "authors": [
        "M Hoy"
      ],
      "year": "2018",
      "venue": "Medical reference services quarterly"
    },
    {
      "citation_id": "3",
      "title": "Cross-corpus acoustic emotion recognition from singing and speaking: A multi-task learning approach",
      "authors": [
        "B Zhang",
        "E Provost",
        "G Essl"
      ],
      "year": "2016",
      "venue": "IEEE"
    },
    {
      "citation_id": "4",
      "title": "A feature fusion method based on extreme learning machine for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "J Dang",
        "L Zhang",
        "H Guan"
      ],
      "year": "2018",
      "venue": "2018 IEEE ICASSP"
    },
    {
      "citation_id": "5",
      "title": "etutor: Online learning for personalized education",
      "authors": [
        "C Tekin",
        "J Braun",
        "M Van Der Schaar"
      ],
      "year": "2015",
      "venue": "ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Supervised domain adaptation for emotion recognition from speech",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2015",
      "venue": "ICASSP"
    },
    {
      "citation_id": "7",
      "title": "Human language technology: Opportunities and challenges",
      "authors": [
        "M Ostendorf",
        "E Shriberg",
        "A Stolcke"
      ],
      "year": "2005",
      "venue": "ICASSP"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Q Jin",
        "C Li",
        "S Chen",
        "H Wu"
      ],
      "year": "2015",
      "venue": "ICASSP"
    },
    {
      "citation_id": "9",
      "title": "Computationallyefficient endpointing features for natural spoken interaction with personal-assistant systems",
      "authors": [
        "H Arsikere",
        "E Shriberg",
        "U Ozertem"
      ],
      "year": "2014",
      "venue": "ICASSP"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2004",
      "venue": "ICASSP"
    },
    {
      "citation_id": "11",
      "title": "Learning alignment for multimodal emotion recognition from speech",
      "authors": [
        "H Xu",
        "H Zhang",
        "K Han",
        "Y Wang",
        "Y Peng",
        "X Li"
      ],
      "year": "2019",
      "venue": "Learning alignment for multimodal emotion recognition from speech",
      "arxiv": "arXiv:1909.05645"
    },
    {
      "citation_id": "12",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using deep neural network considering verbal and nonverbal speech sounds",
      "authors": [
        "K.-Y Huang",
        "C.-H Wu",
        "Q.-B Hong",
        "M.-H Su",
        "Y.-H Chen"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "14",
      "title": "Hidden markov modelbased speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2003",
      "venue": "ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "ICASSP"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition using support vector machines",
      "authors": [
        "T Seehapoch",
        "S Wongthanavasu"
      ],
      "year": "2013",
      "venue": "2013 5th international conference on Knowledge and smart technology (KST)"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition using a hierarchical binary decision tree approach",
      "authors": [
        "C.-C Lee",
        "E Mower",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition using gaussian mixture vector autoregressive models",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2007",
      "venue": "ICASSP"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Fifteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition from spectrograms with deep convolutional neural network",
      "authors": [
        "A Badshah",
        "J Ahmad",
        "N Rahim",
        "S Baik"
      ],
      "year": "2017",
      "venue": "2017 international conference on platform technology and service"
    },
    {
      "citation_id": "21",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "22",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Convolutional attention networks for multimodal emotion recognition from speech and text data",
      "authors": [
        "C Lee",
        "K Song",
        "J Jeong",
        "W Choi"
      ],
      "year": "2018",
      "venue": "ACL 2018"
    },
    {
      "citation_id": "24",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of EMNLP"
    },
    {
      "citation_id": "25",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "27",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess)",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "The ryerson audio-visual database of emotional speech and song (ravdess)"
    },
    {
      "citation_id": "28",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "S Yoon",
        "S Byun",
        "S Dey",
        "K Jung"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "pyaudioanalysis: An open-source python library for audio signal analysis",
      "authors": [
        "T Giannakopoulos"
      ],
      "year": "2015",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0144610"
    },
    {
      "citation_id": "31",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations"
    },
    {
      "citation_id": "32",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    }
  ]
}