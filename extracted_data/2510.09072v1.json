{
  "paper_id": "2510.09072v1",
  "title": "Emotion-Disentangled Embedding Alignment For Noise-Robust And Cross-Corpus Speech Emotion Recognition",
  "published": "2025-10-10T07:17:07Z",
  "authors": [
    "Upasana Tiwari",
    "Rupayan Chakraborty",
    "Sunil Kumar Kopparapu"
  ],
  "keywords": [
    "speech emotion",
    "latent subspace",
    "partial least square",
    "noisy samples",
    "cross-corpus"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Effectiveness of speech emotion recognition in real-world scenarios is often hindered by noisy environments and variability across datasets. This paper introduces a two-step approach to enhance the robustness and generalization of speech emotion recognition models through improved representation learning. First, our model employs EDRL (Emotion-Disentangled Representation Learning) to extract class-specific discriminative features while preserving shared similarities across emotion categories. Next, MEA (Multiblock Embedding Alignment) refines these representations by projecting them into a joint discriminative latent subspace that maximizes covariance with the original speech input. The learned EDRL-MEA embeddings are subsequently used to train an emotion classifier using clean samples from publicly available datasets, and are evaluated on unseen noisy and cross-corpus speech samples. Improved performance under these challenging conditions demonstrates the effectiveness of the proposed method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) is a vital area of research aimed at inferring the emotional state of a speaker, enabling machines to understand and respond to human emotions from speech signals. Accurate emotion detection supports the development of empathetic virtual assistants  [25] , responsive customer service agents  [3] , and other AI systems that interact naturally and contextually  [22] .\n\nDespite recent progress in SER  [33] , models often struggle with robustness and generalization, especially when exposed to unseen noise and cross-corpus conditions at inference time. These scenarios, where the model is tested on speech samples differing significantly from the clean, in-domain training data, reveal critical limitations in existing systems. Traditional approaches typically rely on fixed representations and static features that do not adapt well to real-world variations, including environmental distortions and dataset shifts.\n\nA major obstacle lies in the variability of emotional expression across different datasets, shaped by cultural, linguistic, and speaker-specific differences  [31, 4, 28, 5] . In parallel, background noise, such as babble, ambient disturbances, or acoustic corruption further degrades speech quality, making reliable feature extraction increasingly difficult  [14, 12, 19, 20] . Existing SER methods often fail to generalize across such challenging acoustic and data conditions.\n\nTo address these limitations, we propose a two-step approach for robust and generalizable representation learning. Motivated from  [34] , first, we introduce an Emotion-Disentangled Representation Learning (EDRL) framework that extracts class-specific discriminative features while retaining emotion-shared structures across categories. This disentangled representation promotes expressiveness while preserving generalizability, even when emotional cues vary across content and speakers.\n\nSecond, we employ Multiblock Embedding Alignment (MEA) to project the EDRL-derived embeddings into a joint latent space that aligns closely with the original speech input. MEA enhances the discriminative capacity of these features by maximizing shared covariance across blocks, allowing the model to better distinguish emotional states even under noisy or cross-corpus conditions.\n\nThe learned EDRL-MEA embeddings are then used to train an emotion classifier using clean samples from the IEMOCAP dataset. Evaluation is conducted on unseen noisy and cross-corpus test samples, demonstrating marked improvements in robustness and generalization over conventional methods. By jointly learning emotion-specific representations and refining them through projection alignment, our approach improves SER performance in diverse and unpredictable acoustic environments.\n\nThe key contributions of this paper are:\n\n-We make use of a two-stage SER framework based on EDRL-MEA that creates robust emotion embeddings effective in both clean and unseen noisy, cross-corpus scenarios. -The EDRL-MEA architecture acts as a pre-trained embedding generator without requiring any fine-tuning, domain adaptation, or data augmentation, enabling simplicity alongside improved generalization. -Our method effectively captures emotion-specific discriminative patterns and refines them through embedding alignment, making it suitable for real-world, variable conditions.\n\nThe remainder of the paper is organized as follows: Section 2 reviews related work. Section 3 details our proposed EDRL-MEA methodology. Section 4 presents experiments and analysis. Section 5 concludes the paper.\n\n2 Literature Review marily relied on classical machine learning methods such as Hidden Markov Models (HMM), Gaussian Mixture Models (GMM), and Support Vector Machines (SVM)  [27, 29, 9] . These traditional systems often required extensive preprocessing and manual feature engineering to extract relevant acoustic and prosodic cues.\n\nFeature extraction remains a critical component of SER. Commonly used features include prosodic attributes (e.g., pitch, intensity), voice quality parameters, and spectral descriptors  [13] . Among spectral features, Mel-Frequency Cepstral Coefficients (MFCCs) are widely used. For example,  [17]  employed MFCCs with 39 coefficients as input to a Long Short-Term Memory (LSTM) network for emotion classification. Convolutional Neural Networks (CNNs) have also been utilized to extract high-level features from spectrograms  [36, 37] . In particular, Deep Stride CNNs (DSCNNs), which replace pooling layers with strided convolutions, have been shown to improve emotion recognition accuracy  [36, 26] .\n\nDespite these advancements, SER systems continue to struggle with two major challenges: (1) generalization to unseen cross-corpus data, and (2) robustness under realistic noisy conditions during inference. Many existing approaches attempt to mitigate noise sensitivity using methods such as speech enhancement  [38] , noise reduction  [30] , feature compensation  [8] , or robust feature extraction techniques  [19, 20] . However, these approaches often fall short when applied to dynamic and unpredictable acoustic environments.\n\nSimilarly, the diversity across emotional speech corpora-including differences in language, culture, recording conditions, and speaker demographics-poses a significant obstacle to cross-corpus generalization. Several techniques have been explored to bridge this gap. These include corpus-based normalization  [32] , domain adaptation strategies such as Universum learning  [10] , and adversarial learning for unsupervised and semi-supervised adaptation  [2, 18] .\n\nWhile these methods have yielded improvements, they often require additional adaptation stages, access to target domain data, or complex training schemes. This motivates the need for a more streamlined and generalizable approach to SER that is robust against unseen noise and cross-corpus variation without reliance on explicit adaptation.\n\nTo this end, our work introduces a two-stage embedding learning strategy: Emotion-Disentangled Representation Learning (EDRL) to capture emotionspecific yet generalizable features, followed by Multiblock Embedding Alignment MEA to refine and align those features in a shared latent space. Together, EDRL and MEA enable the model to learn robust and transferable representations, improving SER performance in both noisy and cross-corpus settings without requiring target-domain fine-tuning or data augmentation.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion-Disentangled Representation Learning (Edrl)",
      "text": "Emotion-Disentangled Representation Learning (EDRL) aims to transform raw speech input X into a structured embedding space that captures both class- specific emotional traits and shared characteristics across different emotion categories (as depicted in Figure  1(a) ). This dual representation facilitates learning of discriminative features while preserving generalizable patterns useful for robust cross-corpus and noisy-condition generalization.\n\nLet:\n\n-X = {X 1 , X 2 , . . . , X C } denote the speech input grouped by emotion class c ∈ {1, . . . , C}, -C be the number of emotion classes, with X c containing the samples from class c.\n\nFor each class c, we define an emotion-specific block B c consisting of two parallel encoders:\n\n-An intra-class encoder (independent branch encoder È intra in Figure  1(a) )\n\nintra ) that learns discriminative features unique to class c, -An inter-class encoder (similarity branch encoder È inter in Figure  1(a) )\n\ninter , θinter ) that extracts features shared across emotion categories, with θinter being shared across all classes.\n\nThese encoders produce the following latent representations:\n\nEach block B c functions as an autoencoder, where the encoded features are decoded to reconstruct the input. The inter-class latent space is shared, enabling alignment across classes for similarity-aware learning.\n\nTraining involves minimizing the reconstruction loss:\n\ninter (X c ) The loss L c r includes: 1. A cosine similarity loss between original and reconstructed embeddings, 2. A Kullback-Leibler divergence term encouraging compact, disentangled representations.\n\nThe joint representation for class c is obtained by concatenating the intraand inter-class embeddings:\n\ninter ] These embeddings are passed to the next stage for global alignment.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Multiblock Embedding Alignment (Mea)",
      "text": "Given the combined embeddings { Z c } C c=1 , the goal of Multiblock Embedding Alignment (MEA) is to project them into a common latent space that captures both within-class cohesion and between-class similarity structure.\n\nWe employ Multiblock Partial Least Squares (MBPLS) to perform this alignment. MBPLS maximizes the covariance between the learned emotion embeddings and the original input features while minimizing redundancy across blocks.\n\nLet:\n\n-Z = [ Z 1 , . . . , Z C ] denote the concatenated embeddings, -X represent the original speech input, -K be the number of latent variables (LVs) to be extracted.\n\nFor each latent variable k = 1, . . . , K, MBPLS computes:\n\n-Score vectors t sk from Z c and u k from X, -Loading vectors p k and v k for the respective components.\n\nEmbeddings are iteratively updated via deflation:\n\nAfter K iterations, the projected outputs are:\n\nThese satisfy the following reconstruction relations:\n\nThe final MEA transformation is defined as:\n\nwhere X ′ denotes the aligned embedding capturing both class structure and its relationship to the original speech signal.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Final Classification",
      "text": "The final representation X ′ obtained from the EDRL + MEA pipeline is fed into a classifier:\n\nwhere ĉ is the predicted emotion class and Ω denotes the classifier parameters. The complete pipeline is summarized in Algorithm 1.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Algorithm 1 Edrl-Mea: Robust Emotion Representation Learning",
      "text": "Require: Speech data X = {X 1 , X 2 , . . . , X C }, where X c denotes samples from class c, with C total classes Ensure: Robust emotion embeddings X ′ for classification 1: Initialize: Parameters θ c intra , θ c inter , θinter for all c ∈ {1, . . . , C} 2: for all classes c = 1 to C do 3:\n\nExtract intra-class embedding:\n\nExtract inter-class embedding:\n\nForm embedding:\n\nMinimize reconstruction loss:\n\n7: end for 8: Input to MEA: Embeddings Z = { Z 1 , . . . , Z C } and original input X 9: Apply MBPLS projection: X ′ = ϕmea( Z, X) 10: Emotion Classification: ĉ = arg maxc P (c | X ′ , Ω)",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Setup",
      "text": "We performed the evaluation of our proposed approach using intra-corpus as well as inter-corpus setup in clean and noisy environments, respectively. In real life conversations, e.g. in call center help-desk or mental-health screening, emotions are mostly interpreted as positive or negative in dimensional space. That is why in this paper we choose to experimentally validate our proposed approach in arousal and valence dimensions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Database And Biomarker Extraction",
      "text": "SER Database: We use Interactive emotional dyadic motion capture (IEMO-CAP) database  [7] , wherein samples were recorded when two participants conversing in two different scenarios, namely scripted and improvised. In scripted sessions, the speakers were asked to memorize the scripts and rehearse, whereas in improvised they were asked to improvise some hypothetical situations that were designed to elicit the specific emotions. Each samples are annotated by participants themselves and by several evaluators in 10 emotion categories as well as in a-v-d dimensional space on a scale of 1 to 5. In this work, we perform the binary-class SER in dimensional emotion space. We consider the average of all evaluators score as a final rating given to each sample. Furthermore, we construct the binary labels + (≥ λ) and -(< λ) on the final rating score with λ = 2.5. Thus each sample, had one of two labels in the v-a-space, namely, v+ or v-; a+ or a-.\n\nFor inter-corpus evaluation, we combined two audio emotion datasets, namely, (A) Berlin Emotional Database (Emo-DB)  [6]  and, (B) the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [23] . To match with our train setup, while performing the inter-corpus evaluation in dimensional space, we consider four categorical emotion classes Anger, Happy, Neutral and Sad that are mapped into a-v space. This is done by labeling Anger and Sad as v-, Happy and Neutral as v+, Sad and Neutral as a-and Anger and Happy as a+. This resulted into data distribution of v+: 438, v-: 573 a+: 582, a-: 429. Noise Database: In order to create a noisy test data, we use recorded noises from Indian Noise Database (iNoise) database  [16]  to corrupt the clean test utterances for both inter-corpus and intra-corpus setup. We used total five types of noises, out of which 3 noises are indoor, namely, Indoor_workplace, Indoor_cafteria, Indoor_home, represented as Noise_1, Noise_2 and Noise_3, respectively; and 2 outdoor noises, namely, Outdoor_travel-bus, Outdoor_street, represented as Noise_4 and Noise_5, respectively. All these noises are used to corrupt clean test samples at 5 SNR levels (0dB, 5dB, 10dB, 15dB, 20dB). This is to be noted that the choice of noise types are made in such a way that the environments are closely relevant to real life scenarios. Acoustic Biomarker Extraction: We extract 88 acoustic features from each audio file with extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) using eGeMAPSv01a  [11]  configuration file of the OpenSMILE toolkit  [1] . There are a total of 18 acoustic features, namely Pitch, Jitter, Shimmer, formant related energy, MFCCs, also known as low-level descriptors (LLDs); and high-level descriptors (HLDs) are computed (mean, standard deviation, skewness, kurtosis, extremes, linear regressions, etc.) for each of those LLDs.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Edrl-Mea Configuration And Training",
      "text": "We implement EDRL with two emotion blocks (B 1 and B 2 ) as shown in Figure  1(b) . ae for both intra and inter branch consists of 3 layers with relu activation, namely, È, L and D. To keep the ae compact, we stacked only single È, L and D layers in each branch. We tried different setups for selecting the number of hidden neurons in each layer; (a) setup-1: same number of neurons in È, L and D; (b) setup-2: compressed latent space with number of hidden neurons in L as half of that in È and D; and (c) setup-3: expanded latent space with number of hidden neurons in L as twice of that in È and D. Each of the above mentioned setups are tried with N/2, N , 2N , and 4N number of hidden neurons, where N is dimension of the input vector. We found setup-3 with 2N neurons to be working best. Further, the decoded output from both the branches are concatenated and fed to the final dense layer with linear activation and N neurons. The final output of each EDRL block is the combined representation learnt per emotion class. So, we get one N -dimension EDRL output vectors for each block. L of the similarity branches from two blocks are tied together (by sharing weights) unlike the independent branches of the two block. We hypothesize that this process of learning the two branches helps the model capture not only the emotion class specific properties but also similarities among the different emotion classes. As an example, assume X κ be the training set that consists of two class data X 1 κ and X 2 κ . During training, X 1 κ is input to block B 1 (in an epoch) while X 2 κ is input to B 2 in a sequence. At each epoch e, both B 1 and B 2 are trained. While the shared latent space weights are updated by training each block for an epoch, the block layer weights are updated only once per epoch when that block sees an input. The EDRL is implemented in Keras  [15]  with adam optimizer and customised loss. To prevent overfitting, we use Keras EarlyStopping that monitors validation loss to guide EDRL training. We use a python package, mbpls, to implement the MEA with two data blocks consisting of N -dimensional combined embeddings learnt from both B 1 and B 2 of EDRL. Note that MEA is trained on EDRL output and maps them to a common latent subspace. The target vector of MEA is the original train data itself. From these two data blocks, MEA predicts a N -dimensional vector, such that respective contribution of each emotion block is retained. Finally, this emotion class embedding is used for emotion classification.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion Classification",
      "text": "We split the IEMOCAP data into train set (80%) and test set (20%) for training and intra-corpus testing, respectively. Further 10% of the train data is used for validation for EDRL-MEA training. IEMOCAP dataset consists of v+: 2952, v-: 2483; a+: 3480, a-: 1995 samples. We adopted majority class undersampling using RandomUnderSampler technique (from sklearn python package) over the train set to overcome the class imbalance across the v-a emotion dimensions. Unlike conventional approach of data balancing which uses minority class oversampling, we opted for majority class undersampling to avoid synthetically generated samples to be used in training. We build two SER systems, (1) Baseline SER system using the features mentioned in Section 4.1 and Random Forest (rf) as the final stage classifier; (2) EDRL-MEA based SER system that uses the reconstructed embedding X ′ ( as represented in Figure  1 ) learnt using the proposed approach to perform the classification using rf. We use rf for the final emotion recognition task because of it's superior performance compared to other standard classifiers like svm, knn, and ann.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Results And Analysis",
      "text": "We evaluate the proposed EDRL-MEA approach for SER using intra-and intercorpus test data from clean as well as noisy environments separately, for both v and a dimensions (as shown in Table  1 , 2). We use rf as the final stage classifier in all our experiments. We perform grid search to fix the rf parameters n_estimators and n_depth for each of our experimental setup independently, with grid of n_estimators = (i * 10), where 50 ≤ i ≤ 500, and n_depth = (2 * i), where 1 ≤ i ≤ 20. It is to be noted that both Baseline and EDRL-MEA system are trained using IEMOCAP clean samples from the training set. Furthermore, the trained clean model (for Baseline vs EDRL-MEA) is evaluated in four different setup. We discuss each experimental setup in brief details as below.\n\n1. Intra-corpus Clean: Both Baseline and EDRL-MEA is tested using clean test set from IEMOCAP. As shown in Table  1 , EDRL-MEA surpasses the Baseline in terms of F1 score, with absolute improvement of 2.4% and 3.9% for a and v, respectively. 2. Intra-corpus Noisy: Firstly, noisy test data is prepared by corrupting IEMO-CAP test set using 5 noise-types from iNoise dataset at 5 SNR levels (as discussed in Section 4.1). We report average F1-score over 5 SNR levels for each noise-type as seen in Table  1 . EDRL-MEA shows an absolute improvement over Baseline for both a-v emotions in terms of F1 scores across all 5 noise-types. There is an absolute improvement of (a:1.92%, v:3.66%), (a:1.48%, v:1.94%), (a:2.08%, v:3.86%), (a:2.66%, v:2.44%) and (a:3.88%, v:2.88%) using noisy test set corrupted with Noise_1, Noise_2, Noise_3, Noise_4 and Noise_5, respectively. The SER performance using EDRL-MEA not only surpasses the Baseline in clean intra-corpus setup, but also shows a significant improvement over Baseline in noisy environment and cross-corpus testing, clearly demonstrates the usefulness of the proposed approach. It is to be noted that in this paper we are not aiming for any multi-conditioning based model adaptation to address the noise aspect in the speech data. We show the effectiveness of the learnt embeddings with proposed EDRL-MEA in both cross-corpus and noisy environment settings, restricting the model training only with the clean data. As can be seen, the resultant embeddings capture intra-and inter-class characteristics, benefit the final-stage classifier with an improved SER performance, through better generalization on cross-corpus data and more robustness to the unseen noises. Please note that we make no effort to compare our results with existing work  [35, 24, 21]  on SER in dimensional emotion space, due to the mismatch in experimental setup as compared to ours.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces an effective two-stage framework for robust speech emotion recognition (SER) under cross-corpus and noisy conditions. Our approach integrates Emotion-Disentangled Representation Learning (EDRL) to simultaneously capture emotion-specific and shared inter-class patterns through parallel intra-and inter-class encoding pathways. This disentanglement encourages the model to learn discriminative yet generalizable embeddings that are less sensitive to corpus-specific or noise-related artifacts. To further enhance robustness, we incorporate Multiblock Embedding Alignment (MEA) using Multiblock Partial Least Squares (MBPLS), which aligns the learned embeddings with the original input space. This projection mechanism preserves both intra-class distinctiveness and inter-class consistency, ensuring that the embeddings remain semantically meaningful even under distributional shifts. Experimental results validate that the proposed EDRL+MEA pipeline significantly outperforms competitive baselines in both cross-corpus and noisy evaluation setups. These findings demonstrate the effectiveness of our method in mitigating the adverse effects of unseen noise and corpus variability, a critical requirement for real-world SER systems. Our work contributes a generalizable and noise-resilient modeling paradigm, paving the way for more reliable affective computing applications in diverse and unconstrained environments.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: EDRL-MEA architecture for 2 classes.",
      "page": 4
    },
    {
      "caption": "Figure 1: (a)). This dual representation facilitates learning",
      "page": 4
    },
    {
      "caption": "Figure 1: ) learnt using the",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , 2). We use rf as the final stage",
      "data": [
        {
          "Environment Noise_type": "",
          "a": "",
          "v": ""
        },
        {
          "Environment Noise_type": "Clean",
          "a": "77.7",
          "v": "66.7"
        },
        {
          "Environment Noise_type": "Noisy",
          "a": "52.72",
          "v": ""
        },
        {
          "Environment Noise_type": "",
          "a": "52.26",
          "v": ""
        },
        {
          "Environment Noise_type": "",
          "a": "52.78",
          "v": ""
        },
        {
          "Environment Noise_type": "",
          "a": "51.6",
          "v": ""
        },
        {
          "Environment Noise_type": "",
          "a": "50.82",
          "v": "47.46"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 1: , 2). We use rf as the final stage",
      "data": [
        {
          "Environment Noise_type": "",
          "a": "",
          "v": ""
        },
        {
          "Environment Noise_type": "Clean",
          "a": "56.8",
          "v": "54.1"
        },
        {
          "Environment Noise_type": "Noisy",
          "a": "54.26",
          "v": ""
        },
        {
          "Environment Noise_type": "",
          "a": "52.2",
          "v": "50.76"
        },
        {
          "Environment Noise_type": "",
          "a": "50.32",
          "v": ""
        },
        {
          "Environment Noise_type": "",
          "a": "45.52",
          "v": ""
        },
        {
          "Environment Noise_type": "",
          "a": "42.8",
          "v": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "2",
      "title": "we care\": Improving code mixed speech emotion recognition in customer-care conversations",
      "authors": [
        "N Abhishek",
        "P Bhattacharyya"
      ],
      "year": "2023",
      "venue": "we care\": Improving code mixed speech emotion recognition in customer-care conversations"
    },
    {
      "citation_id": "3",
      "title": "Cross-corpus speech emotion recognition based on few-shot learning and domain adaptation",
      "authors": [
        "Y Ahn",
        "S Lee",
        "J Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "4",
      "title": "A study on crosscorpus speech emotion recognition and data augmentation",
      "authors": [
        "N Braunschweiler",
        "R Doddipatla",
        "S Keizer",
        "S Stoyanchev"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "5",
      "title": "A Database of German Emotional Speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "6",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "Front-end feature compensation and denoising for noise robust speech emotion recognition",
      "authors": [
        "R Chakraborty",
        "A Panda",
        "M Pandharipande",
        "S Joshi",
        "S Kopparapu"
      ],
      "year": "2019",
      "venue": "Front-end feature compensation and denoising for noise robust speech emotion recognition",
      "doi": "10.21437/Interspeech.2019-2243"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition: Features and classification models",
      "authors": [
        "L Chen",
        "X Mao",
        "Y Xue",
        "L Cheng"
      ],
      "year": "2012",
      "venue": "Digital signal processing"
    },
    {
      "citation_id": "9",
      "title": "Universum autoencoderbased domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "X Xu",
        "Z Zhang",
        "S Frühholz",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "10",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "11",
      "title": "A survey of speech emotion recognition in natural environment",
      "authors": [
        "M Fahad",
        "A Ranjan",
        "J Yadav",
        "A Deepak"
      ],
      "year": "2021",
      "venue": "Digital signal processing"
    },
    {
      "citation_id": "12",
      "title": "Analysis of emotional speech-a review",
      "authors": [
        "P Gangamohan",
        "S Kadiri",
        "B Yegnanarayana"
      ],
      "year": "2016",
      "venue": "Toward Robotic Socially Believable Behaving Systems-Volume I: Modeling Emotions"
    },
    {
      "citation_id": "13",
      "title": "A review on speech emotion recognition: a survey, recent advances, challenges, and the influence of noise",
      "authors": [
        "S George",
        "P Ilyas"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "14",
      "title": "KERAS: The python deep learning library",
      "authors": [
        "Keras"
      ],
      "year": "2019",
      "venue": "KERAS: The python deep learning library"
    },
    {
      "citation_id": "15",
      "title": "inoise indian noise database",
      "authors": [
        "S Kopparapu",
        "I Sheikh",
        "V Thanneeru"
      ],
      "year": "2020",
      "venue": "inoise indian noise database",
      "doi": "10.21227/w3xm-jn45"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition using mfcc features and lstm network",
      "authors": [
        "H Kumbhar",
        "S Bhandari"
      ],
      "year": "2019",
      "venue": "2019 5th international conference on computing, communication, control and automation (ICCUBEA)"
    },
    {
      "citation_id": "17",
      "title": "Multi-task semi-supervised adversarial autoencoding for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective computing"
    },
    {
      "citation_id": "18",
      "title": "Not all features are equal: Selection of robust features for speech emotion recognition in noisy environments",
      "authors": [
        "S Leem",
        "D Fulford",
        "J Onnela",
        "D Gard",
        "C Busso"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Selective acoustic feature enhancement for speech emotion recognition with noisy speech",
      "authors": [
        "S Leem",
        "D Fulford",
        "J Onnela",
        "D Gard",
        "C Busso"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Contrastive unsupervised learning for speech emotion recognition",
      "authors": [
        "M Li",
        "B Yang",
        "J Levy",
        "A Stolcke",
        "V Rozgic",
        "S Matsoukas",
        "C Papayiannis",
        "D Bone",
        "C Wang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Self context-aware emotion perception on humanrobot interaction",
      "authors": [
        "Z Lin",
        "F Cruz",
        "E Sandoval"
      ],
      "year": "2024",
      "venue": "Self context-aware emotion perception on humanrobot interaction"
    },
    {
      "citation_id": "22",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "23",
      "title": "Learning an arousal-valence speech front-end network using media data in-the-wild for emotion recognition",
      "authors": [
        "C Lu",
        "J Li",
        "C Lee"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "24",
      "title": "Affective social anthropomorphic intelligent system",
      "authors": [
        "M Mamun",
        "H Abdullah",
        "M Alam",
        "M Hassan",
        "M Uddin"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-023-14597-6"
    },
    {
      "citation_id": "25",
      "title": "A cnn-assisted enhanced audio signal processing for speech emotion recognition",
      "authors": [
        "Kwon Mustaqeem"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L De Silva"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "27",
      "title": "Analysis of deep learning architectures for cross-corpus speech emotion recognition",
      "authors": [
        "J Parry",
        "D Palaz",
        "G Clarke",
        "P Lecomte",
        "R Mead",
        "M Berger",
        "G Hofer"
      ],
      "year": "2019",
      "venue": "Analysis of deep learning architectures for cross-corpus speech emotion recognition"
    },
    {
      "citation_id": "28",
      "title": "Emotion recognition from speech with gaussian mixture models & via boosted gmm",
      "authors": [
        "P Patel",
        "A Chaudhari",
        "R Kale",
        "M Pund"
      ],
      "year": "2017",
      "venue": "International Journal of Research In Science & Engineering"
    },
    {
      "citation_id": "29",
      "title": "Spectral and cepstral audio noise reduction techniques in speech emotion recognition",
      "authors": [
        "J Pohjalainen",
        "F Fabien Ringeval",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Proceedings of the 24th ACM International Conference on Multimedia",
      "doi": "10.1145/2964284.2967306"
    },
    {
      "citation_id": "30",
      "title": "Analyzing the influence of different speech data corpora and speech features on speech emotion recognition: A review. Speech Communication p",
      "authors": [
        "T Rathi",
        "M Tripathy"
      ],
      "year": "2024",
      "venue": "Analyzing the influence of different speech data corpora and speech features on speech emotion recognition: A review. Speech Communication p"
    },
    {
      "citation_id": "31",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "M Wöllmer",
        "A Stuhlsatz",
        "A Wendemuth",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Speech emotion recognition: two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Commun. ACM",
      "doi": "10.1145/3129340"
    },
    {
      "citation_id": "33",
      "title": "Joint class learning with self similarity projection for EEG emotion recognition",
      "authors": [
        "U Tiwari",
        "R Chakraborty",
        "S Kopparapu",
        "S Natarajan",
        "I Bhattacharya",
        "R Singh",
        "A Kumar",
        "S Ranu",
        "K Bali"
      ],
      "year": "2024",
      "venue": "Proceedings of the 7th Joint International Conference on Data Science & Management of Data (11th ACM IKDD CODS and 29th COMAD)",
      "doi": "10.1145/3632410.3632417"
    },
    {
      "citation_id": "34",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "35",
      "title": "Speech emotion recognition using convolution neural networks and deep stride convolutional neural networks",
      "authors": [
        "T Wani",
        "T Gunawan",
        "S Qadri",
        "H Mansor",
        "M Kartiwi",
        "N Ismail"
      ],
      "year": "2020",
      "venue": "2020 6th International Conference on Wireless and Telematics (ICWT)"
    },
    {
      "citation_id": "36",
      "title": "Speech emotion recognition using spectrogram & phoneme embedding",
      "authors": [
        "P Yenigalla",
        "A Kumar",
        "S Tripathi",
        "C Singh",
        "S Kar",
        "J Vepa"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "37",
      "title": "Using speech enhancement preprocessing for speech emotion recognition in realistic noisy conditions",
      "authors": [
        "H Zhou",
        "J Du",
        "Y Tu",
        "C Lee"
      ],
      "year": "2020",
      "venue": "Using speech enhancement preprocessing for speech emotion recognition in realistic noisy conditions",
      "doi": "10.21437/Interspeech.2020-2472"
    }
  ]
}