{
  "paper_id": "2306.07848v10",
  "title": "Gemo-Clap: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining For Accurate Speech Emotion Recognition",
  "published": "2023-06-13T15:28:10Z",
  "authors": [
    "Yu Pan",
    "Yanni Hu",
    "Yuguang Yang",
    "Wen Fei",
    "Jixun Yao",
    "Heng Lu",
    "Lei Ma",
    "Jianjun Zhao"
  ],
  "keywords": [
    "Speech emotion recognition",
    "contrastive language-audio pretraining",
    "gender-attribute-enhanced"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Contrastive cross-modality pretraining has recently exhibited impressive success in diverse fields, whereas there is limited research on their merits in speech emotion recognition (SER). In this paper, we propose GEmo-CLAP, a kind of gender-attribute-enhanced contrastive language-audio pretraining (CLAP) method for SER. Specifically, we first construct an effective emotion CLAP (Emo-CLAP) for SER, using pretrained text and audio encoders. Second, given the significance of gender information in SER, two novel multi-task learning based GEmo-CLAP (ML-GEmo-CLAP) and soft label based GEmo-CLAP (SL-GEmo-CLAP) models are further proposed to incorporate gender information of speech signals, forming more reasonable objectives. Experiments on IEMOCAP indicate that our proposed two GEmo-CLAPs consistently outperform Emo-CLAP with different pre-trained models. Remarkably, the proposed WavLM-based SL-GEmo-CLAP obtains the best WAR of 83.16%, which performs better than state-of-the-art SER methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As one of the crucial parts in human-machine interaction, speech emotion recognition (SER) has garnered widespread attention of researchers cross academia and industry  [1] .\n\nOver the previous decade, with tremendous progress of deep learning techniques, numerous deep neural network (DNN) based SER methods  [2, 3]  have been proposed and achieved better performance compared with traditional SER methods. However, the SER field still faces one critical issue which is the lack of sufficient available dataset  [4] , since collecting and annotating speech emotion data is expensive and time-consuming. To this end, several attempts have been made  [5] [6] [7]  to utilize selfsupervised learning (SSL) based pre-trained models to achieve emotion recognition. For instance, chen et al.  [5]  introduced three fine-tuning approaches to optimize the Wav2Vec 2.0  [8]  for SER. Morais et al.  [6]  presented a novel upstream-downstream † denotes the corresponding author. architecture based modular SER system, which used Hubert  [9]  and Wav2Vec 2.0 based SSL features for emotion recognition. Gat et al.  [7]  proposed a gradient-based adversarial learning framework which normalized the voiceprint information from Hubert features for better recognition performance.\n\nDespite obtaining impressive performance, these approaches still have several flaws. First, these methods generally adopt supervised learning based methods to fine-tune pre-trained models, while neglecting research on contrastive learning based approaches. Second, these methods overlook gender information of speech signals, which has a significant impact on SER.\n\nHence in this work, we delve into contrastive learning based speech emotion modeling approaches, and introduce GEmo-CLAP which is a kind of gender-attribute-augmented contrastive language-audio pretraining (CLAP) method for SER. The main contributions are three-fold. First, we build a baseline emotion CLAP (Emo-CLAP) that employs contrastive learning to uncover feature representations across various data pairs. As shown in the Fig.  1 , the proposed Emo-CLAP can leverage pre-trained audio and text encoders, enabling the alignment of features from these two modalities into approximately the same feature space. Second, considering the importance of gender information for SER, we further propose a novel multitask learning based GEmo-CLAP (ML-GEmo-CLAP) and a novel soft label based GEmo-CLAP (SL-GEmo-CLAP), which integrate gender and emotion information of speech signals.\n\nIn this way, the proposed two GEmo-CLAPs can form more reasonable objectives, thereby enhancing their recognition performance. Third, we perform experiments on the challenging IEMOCAP  [10]  corpus to verify the effectiveness of our proposed CLAP-based methods, using one pre-trained text encoder Roberta  [11] , as well as four pre-trained audio encoders, i.e., Wav2vec 2.0, Hubert, WavLM  [12] , and Data2vec  [13] . Results show that our proposed two GEmo-CLAPs consistently outperform the baseline, while also achieving the best recognition results as opposed to state-of-the-art (SOTA) SER approaches.\n\nTo the best of our knowledge, this study marks the first systematic utilization of contrastive cross-modality pretraining to achieve SER. Besides, we aspire for this work to serve as a cornerstone for large-scale generative speech models applicable to text-to-speech synthesis or voice conversion task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "SER aims to identify the emotional state of different speakers based on their speech signals, which has been widely employed in various practical scenarios  [1] . Although current SOTA DNNbased SER systems  [2] [3] [4] [5] [6] [7]  have achieved great performance, there are still several knotty problems need to be solved. First, from the aspect of data, the collection and annotation of speech emotion data is difficult than the collection of image and text data, which means trained SER models will inevitably encounter unseen data during deployment, thus placing high demands on the overall performance of SER methods. Second, from the perspective of attribute distribution of speech signals, there are too many mutually influenced speech attributes (emotion, speaker, gender, language, and etc) in human speech  [14] , which pose great challenges on speech emotion modeling.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Contrastive Cross-Modality Pretraining",
      "text": "Contrastive cross-modality pretraining methods have recently been extensively applied in the various fields  [15] [16] [17] [18] . Taking a comprehensive view, all of these methods strive to acquire versatile visual-linguistic or acoustic-linguistic features via contrastive learning. For example, Radford et al.  [15]  first proposed a novel contrastive language-image pretraining model that learns directly from raw texts about images, which can loosen the restricted supervision of traditional gold-labels. Afterward, Elizalde et al.  [16]  and Wu et al.  [17]  presented two contrastive learning based language-audio pretraining methods to achieve a unified feature representation of audio and text modalities. Meng et al.  [18]  introduced a contrastive acoustic-linguistic module to capture style-associated text features to improve the speaking style of text-to-speech systems.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emo-Clap",
      "text": "Unlike previous studies that employed CLAP for retrieval tasks, this work casts CLAP for the classification task, namely SER.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training Phase Of Emo-Clap",
      "text": "During training, our proposed Emo-CLAP aims to minimize the distance between paired data within the same class and maximize the distance between data pairs of different classes via contrastive learning, which is depicted in Fig.  1 .\n\nIn detail, assume input audio-text data pairs are {X a i , X t i }, where i ∈ [0, N ] and N is the batch size. During training, Emo-CLAP first extract the text features F t and audio features F a via pre-trained audio encoder f a (•) and text encoder f t (•):\n\nwhere F a ∈ R N ×T ×Da , F t ∈ R N ×L×Dt , T and D a are the time length and hidden state dimension of speech signal, L and D t are the sequence length and hidden state dimension of text. Next, we apply mean pooling on the temporal dimension of F a to get processed audio features F a ∈ R N ×Da , and use the first element of each sequence of F t as processed text features F t ∈ R N ×Dt . Then, two multi-layer linear projection modules, i.e., M LP a (•) and M LP t (•) are designed to bring the processed audio and text features into a joint multi-modal space with the same dimension of D, and their corresponding similarity matrices C a and C t are measured as:\n\nwhere E a ∈ R N ×D and E t ∈ R N ×D denote captured audio and text embeddings, ε a and ε t are temperature hyper-parameters. Afterward, Kullback-Leibler divergence based contrastive loss (KL-loss) is used to train Emo-CLAP with the guidance of the inputs' emotional ground truth matrix M e ∈ R N ×N . It is noteworthy that if true labels between different data pairs are identical within the same batch, their corresponding ground truths are set to 1. Otherwise, their ground truths are set to 0. Therefore, the final total loss of Emo-CLAP is defined as:\n\nwhere S(•) denotes softmax function, l S(•) denotes log softmax function, i and j represent the indexes of the row and column of the two-dimensional matrix.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Inference Phase Of Emo-Clap",
      "text": "During inference, we first build natural language supervisions T C = {T C 1 , ..., T C N } based on emotion classes C = {C 1 , ..., C N }. Then, for a given audio data Xa , its category is the best match between its embeddings and natural language supervision embeddings T C i , computed by their cosine similarity.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Gemo-Clap",
      "text": "Instinctively, the gender information is beneficial for speech emotion modeling, since the speech signals of male and female generally manifest considerable disparities in terms of pitch, tone, energy, and so forth. Therefore, we further optimize the baseline Emo-CLAP and propose two GEmo-CLAP approaches, i.e., ML-GEmo-CLAP and SL-GEmo-CLAP, which integrate the gender and emotion information of given speech signals, as outlined below in detail.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ml-Gemo-Clap",
      "text": "In order to leverage the gender attribute of speech signals, we first propose a novel ML-GEmo-CLAP model, which employs multi-task learning to incorporate gender information into Emo-CLAP. Its overall architecture is illustrated in Fig.  2 . To be specific, our proposed ML-GEmo-CLAP mainly consists of two parts, which are the audio pipeline and shared text pipeline. Overall, the proposed ML-GEmo-CLAP is trained via multi-task learning on top of the baseline Emo-CLAP. As a result, its final loss L M T otal can be defined as:\n\nwhere L E T otal is the KL-loss of emotion attribute, L G T otal is the KL-loss of gender attribute, α e is a parameter to adjust L E T otal and L G T otal . In our case, α e is basically set to 0.8 or 0.9. As a consequence, with a more reasonable objective, the proposed ML-GEmo-CLAP model is capable of enhancing its SER performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sl-Gemo-Clap",
      "text": "Although ML-GEmo-CLAP can effectively utilize gender and emotion information of human speech, its training requires more computational resources than the baseline Emo-CLAP, which limits its usage to some extent. Thus, we further present a novel SL-GEmo-CLAP model based on Emo-CLAP, which leverages gender and emotion attributes of speech signals to create a more reasonable ground truth matrix M . Fig.  3  displays its overall architecture.\n\nFig.  3 : Overview of the proposed SL-GEmo-CLAP.\n\nMore precisely, if the gender categories of different data pairs in the same batch are identical, their corresponding gender ground truth are designated as 1. Conversely, their value are set to 0. Thus, the final ground truth matrix M is formulated as:\n\nwhere α e is a hyper-parameter to adjust and scale M e and M g . In our case, α e is normally set to 0.8 or 0.9. Accordingly, with a more feasible ground truth matrix, the proposed SL-GEmo-CLAP is able to improve its recognition performance as well.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Settings",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Databases",
      "text": "As one of the most challenging corpora in SER, IEMOCAP is composed of five sessions, each involving speech from one male and one female speakers.\n\nIn this work, we likewise select four categories (angry, happy+excited, sad, and neutral) which is the same as recent studies to make fair comparisons. In addition, a standard 5-fold cross-validation is adopted to evaluate our proposed methods, with one session left out as the test set in each fold.  From Table  2 , we can observe that all of our proposed methods achieve superior recognition performance compared with SOTA SER methods, which validates the effectiveness of our proposed methods. Notably, the proposed SL-GEmo-CLAP obtains the best WAR of 81.43%, which outperforms other SOTA SER approaches.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparison Of Our Proposed Clap-Based Approaches",
      "text": "Then, we compare the performance of the proposed three CLAPbased methods on IEMOCAP, which is illustrated in Table  1 .\n\nIt can be easily seen that both SL-GEmo-CLAP and ML-GEmo-CLAP consistently perform better than the baseline Emo-CLAP, which proves the effectiveness and robustness of our gender-attribute-enhanced strategy. In addition, SL-GEmo-CLAP and ML-GEmo-CLAP exhibit similar performance. Take WavLM-based models as an example. Our Emo-CLAP obtains the the best UAR of 81.12% and the secondary best WAR of 79.28%. In contrast, the proposed ML-GEmo-CLAP attain the best WAR of 80.94% and the secondary best UAR of 82.30%, which performs better than Emo-CLAP by 1.66% and 1.18%. Meanwhile, the proposed SL-GEmo-CLAP achieves the best WAR of 81.43% and the best UAR of 83.16%, which outperforms the baseline by 2.15% and 2.04%.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparison Of Different Audio Encoders",
      "text": "Third, we compare and analyze the performance of different pretrained audio encoders within our proposed CLAP-based SER architecture.\n\nAs can be observed in Table  1 , when using the pre-trained Roberta-base model as the text encoder, the WavLM-based CLAP approaches generally obtain the best recognition results under all paradigms. In addition, Hubert-based CLAP methods achieve the secondary best recognition performance. Furthermore, Wav2vec2-based CLAP methods normally perform better than Data2vec-based CLAP methods.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this study, we introduce GEmo-CLAP, which is a kind of gender-attribute-enhanced contrastive language-audio pretraining method. To be precise, a baseline Emo-CLAP model is first built, which is able to leverage pre-trained models from both audio and text modalities. Then, two novel ML-GEmo-CLAP and SL-GEmo-CLAP are further proposed, that incorporate gender information to form more reasonable objectives. Extensive experiments on IEMOCAP show that our proposed SL-GEmo-CLAP and ML-GEmo-CLAP consistently outperform the baseline Emo-CLAP, while also obtaining the best recognition results as opposed to SOTA SER methods.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the proposed Emo-CLAP can leverage",
      "page": 1
    },
    {
      "caption": "Figure 1: In detail, assume input audio-text data pairs are {Xa",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of the proposed Emo-CLAP model.",
      "page": 2
    },
    {
      "caption": "Figure 2: Fig. 2: Overview of the proposed ML-GEmo-CLAP model.",
      "page": 3
    },
    {
      "caption": "Figure 3: displays its overall",
      "page": 3
    },
    {
      "caption": "Figure 3: Overview of the proposed SL-GEmo-CLAP.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Overall results of our proposed Emo-CLAP, ML-GEmo-CLAP, and SL-GEmo-CLAP on IEMOCAP.",
      "page": 4
    },
    {
      "caption": "Table 2: Recognition comparison of SOTA SER methods on",
      "page": 4
    },
    {
      "caption": "Table 2: , we can observe that all of our proposed meth-",
      "page": 4
    },
    {
      "caption": "Table 1: It can be easily seen that both SL-GEmo-CLAP and ML-",
      "page": 4
    },
    {
      "caption": "Table 1: , when using the pre-trained",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Customer satisfaction estimation in contact center calls based on a hierarchical multi-task model",
      "authors": [
        "A Ando",
        "R Masumura",
        "H Kamiyama"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "3",
      "title": "Lightsernet: A lightweight fully convolutional neural network for speech emotion recognition",
      "authors": [
        "A Aftab",
        "A Morsali",
        "S Ghaemmaghami"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition",
      "authors": [
        "J Ye",
        "X Cheng Wen",
        "Y Wei"
      ],
      "year": "2023",
      "venue": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "6",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Speaker normalization for self-supervised speech emotion recognition",
      "authors": [
        "I Gat",
        "H Aronowitz",
        "W Zhu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "10",
      "title": "Hubert: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "12",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "13",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "A Baevski",
        "W.-N Hsu",
        "Q Xu"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "15",
      "title": "Msac: Multiple speech attribute control method for reliable speech emotion recognition",
      "authors": [
        "Y Pan",
        "Y Yang",
        "Y Huang"
      ],
      "year": "2023",
      "venue": "Msac: Multiple speech attribute control method for reliable speech emotion recognition",
      "arxiv": "arXiv:2308.04025"
    },
    {
      "citation_id": "16",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "17",
      "title": "Clap learning audio concepts from natural language supervision",
      "authors": [
        "B Elizalde",
        "S Deshmukh",
        "M Ismail"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation",
      "authors": [
        "Y Wu",
        "K Chen",
        "T Zhang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Calm: Constrastive cross-modal speaking style modeling for expressive textto-speech synthesis",
      "authors": [
        "Y Meng",
        "X Li",
        "Z Wu"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "20",
      "title": "Key-sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Using auxiliary tasks in multimodal fusion of wav2vec 2.0 and bert for multimodal emotion recognition",
      "authors": [
        "D Sun",
        "Y He",
        "J Han"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "22",
      "title": "Knowledge-aware bayesian co-attention for multimodal emotion recognition",
      "authors": [
        "Z Zhao",
        "Y Wang",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "U Tyagi",
        "S Ramaneswaran"
      ],
      "year": "2022",
      "venue": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "arxiv": "arXiv:2203.16794"
    },
    {
      "citation_id": "24",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "25",
      "title": "A discriminative feature representation method based on cascaded attention network with adversarial strategy for speech emotion recognition",
      "authors": [
        "Y Liu",
        "H Sun",
        "W Guan"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    }
  ]
}