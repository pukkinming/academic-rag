{
  "paper_id": "2404.17862v2",
  "title": "Revisiting Multimodal Emotion Recognition In Conversation From The Perspective Of Graph Spectrum",
  "published": "2024-04-27T10:47:07Z",
  "authors": [
    "Tao Meng",
    "Fuchen Zhang",
    "Yuntao Shou",
    "Wei Ai",
    "Nan Yin",
    "Keqin Li"
  ],
  "keywords": [
    "Graph Representation Learning",
    "Spectral Domain",
    "Multimodal Emotion Recognition",
    "Multimodal Fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Efficiently capturing consistent and complementary semantic features in a multimodal conversation context is crucial for Multimodal Emotion Recognition in Conversation (MERC). Existing methods mainly use graph structures to model dialogue context semantic dependencies and employ Graph Neural Networks (GNN) to capture multimodal semantic features for emotion recognition. However, these methods are limited by some inherent characteristics of GNN, such as over-smoothing and low-pass filtering, resulting in the inability to learn long-distance consistency information and complementary information efficiently. Since consistency and complementarity information correspond to low-frequency and high-frequency information, respectively, this paper revisits the problem of multimodal emotion recognition in conversation from the perspective of the graph spectrum. Specifically, we propose a Graph-Spectrumbased Multimodal Consistency and Complementary collaborative learning framework GS-MCC. First, GS-MCC uses a sliding window to construct a multimodal interaction graph to model conversational relationships and uses efficient Fourier graph operators to extract long-distance high-frequency and low-frequency information, respectively. Then, GS-MCC uses contrastive learning to construct self-supervised signals that reflect complementarity and consistent semantic collaboration with high and low-frequency signals, thereby improving the ability of high and low-frequency information to reflect real emotions. Finally, GS-MCC inputs the collaborative high and low-frequency information into the MLP network and softmax function for emotion prediction. Extensive experiments have proven the superiority of the GS-MCC architecture proposed in this paper on two benchmark data sets. \n CCS CONCEPTS â€¢ Computing methodologies â†’ Discourse, dialogue and pragmatics; Non-negative matrix factorization; â€¢ Theory of computation â†’ Fixed parameter tractability.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "With the continuous development of Human-Computer Interaction (HCI), the multimodal emotion recognition task in conversation (MERC) has recently received extensive research attention  [1, 8, 13, 30, 35, 52, 54] . MERC aims to identify the emotional state of each utterance using textual, acoustic, and visual information in the conversational context  [26, 37, 45, 46, 55] , which is crucial for multimodal conversational understanding and an essential component for building intelligent HCI systems  [14, 34, 36] . As shown in Fig.  1 , MERC needs to recognize the emotion of each multimodal utterance in the conversation.\n\nUnlike traditional unimodal or non-conversational emotion recognition  [1, 10, 12, 44] , MERC requires joint conversational context and multimodal information modeling to achieve consistency and complementary semantic capture within and between modalities  [63] . Fig.  1  gives an example of a multimodal conversation between two people, Ross and Carol, from the MELD dataset. As shown in utterance ğ‘¢ 4 , Carol has a \"Joy\" emotion, which is vaguely reflected in textual features but more evident in visual or auditory features reflecting the complementary semantics between modalities. In addition, it is difficult to identify the emotion of \"Surprise\" from the utterance ğ‘¢ 7 alone. However, due to the potential consistency of conversational emotions, it can be accurately inferred based on previous utterances. Therefore, the key to multimodal conversational emotion recognition is to capture the consistency and complementary semantics between multimodal information by utilizing the conversational context and emotional dependence between speakers to reveal the speaker's genuine emotion.\n\nThe current mainstream research method uses the Transformer  [27, 33, 64, 65]  or GNN  [2, 22, 24, 49]  architecture to model the MERC task. Transformer-based methods mainly learn complex semantic information between multimodal and conversational contexts from global sequence modeling. For example, CTNet  [27]  builds a single Transformer and cross Transformer to capture longdistance context dependencies and realize intra-module and intermodule information interaction to achieve multimodal conversational emotion recognition. Although transformer-based methods have made progress from the perspective of global utterance sequence modeling, this paradigm underestimates the complex emotional interactions between multimodal utterances  [49]  and ignores the multiple relationships between utterances  [5] , which limits the model's emotion recognition performance.\n\nBenefitting from GNN's ability to mine and represent complex relationships  [18, 58, 59] , recent GNN-based methods  [1, 14, 23]  have made significant progress in the MERC task. For instance, MMGCN  [14]  fully connects all utterance nodes of the same modality and connects different modal nodes of the same utterance to build a heterogeneous graph to model the complex semantic relationships between multimodal utterances, then uses a deep spectral domain GNN to capture long-distance contextual information to achieve multimodal conversational emotion recognition. Although these GNN-based methods show promising performance, they still have some common limitations:\n\n(1) Insufficient long-distance dependence perception. Considerable methods  [1, 13, 24, 47]  using sliding windows to limit the length of fully connected utterances and then using GNN to learn multimodal utterance representations to achieve emotion recognition. However, limited by the over-smoothing characteristics of GNN  [29, 56] , usually only two layers can be stacked for capturing semantic information, making it difficult for these methods to capture long-distance emotional dependencies. Although the method  [5, 14]  without a sliding window can enhance the capture of long-distance dependencies, it will cause many nodes with the non-same emotions in the neighborhood, which is not conducive to the representation learning of GNN and puts enormous performance pressure on GNN. Therefore, previous GNN-based methods still have limitations in long-distance dependency capture.\n\n(2) Underutilization of high-frequency features. Many studies have shown that GNN has low-pass filtering characteristics  [4, 38, 57] , which mainly obtain node representation by aggregating the consistency features of the neighborhood (low-frequency information) and suppressing the dissimilarity features of the neighborhood (high-frequency information). However, consistency and dissimilarity features are equally important in the MERC task. When specific modalities express less obvious emotions, information from other modalities is needed to compensate, thereby revealing the speaker's genuine emotions. Inspired by this, M 3 Net  [5]  tried to use high-frequency information to improve the MERC task and improved the emotion recognition effect by directly fusing highand low-frequency features. However, essential differences exist between high and low-frequency features, and direct fusion cannot establish efficient collaboration. Thus, previous GNN-based methods still have limitations in utilizing and collaborating high and low-frequency features.\n\nInspired by the above analysis, to efficiently learn the consistency and complementary semantic information in multimodal conversation, we try to revisit the problem of multimodal emotion recognition in conversation from the perspective of the graph spectrum. Specifically, we propose a Graph-Spectrum-based Multimodal Consistency and Complementary feature collaboration framework GS-MCC. GS-MCC first uses RoBERTa  [31] , OpenSMILE  [11] , and 3D-CNN  [16]  to extract preliminary text and acoustic and visual features. Then, GRU and a fully connected network are used further to encode text, auditory, and visual features to obtain higher-order utterance representation. In order to capture long-distance dependency information more efficiently, a sliding window is used to construct a fully connected graph to model conversational relationships, and an efficient Fourier graph operator is used to extract long-distance high and low-frequency information, respectively. In addition, to promote the collaboration ability of high and lowfrequency information, we use contrastive learning to construct self-supervised signals that reflect complementarity and consistent semantic collaboration with high and low-frequency signals, thereby improving the ability of high and low-frequency information to reflect real emotions. Finally, we input the collaborative high and low-frequency information into the MLP network and softmax function for emotion prediction.\n\nThe contributions of our work are summarized as follows:\n\nâ€¢ We propose an efficient long-distance information learning module that designs Fourier graph operators to build a mixed-layer GNN to capture high and low-frequency information to obtain consistency and complementary semantic dependencies in multimodal conversational contexts. â€¢ We propose an efficient high-and low-frequency information collaboration module that uses contrastive learning to construct self-supervised signals that reflect the collaboration of high-and low-frequency information in terms of complementarity and consistent semantics and improves the ability to distinguish emotions between different frequency information. â€¢ We conducted extensive comparative and ablation experiments on two benchmark data sets, IEMOCAP and MELD. The results show that our proposed method can efficiently capture long-distance context dependencies and improve the performance of MERC.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Human-machine intelligent conversation systems have recently received significant attention and development  [6, 26, 41, 51, 60] , so understanding conversations is crucial. Driven by this, Multimodal Emotion Recognition in Conversation (MERC) has gradually developed into a new research hotspot. Many researchers  [22, 62, 65]  have explored and improved the effect of MERC from the semantic interaction between text, auditory, and visual modal data in conversational contexts. These methods  [7, 28, 63]  agree that the MERC task focuses on better capturing and fusing multimodal semantic information in the conversational context for emotion recognition. Therefore, we will review the literature closely related to the above topics from the two aspects of multimodal conversation context feature capture and fusion.\n\n(1) Multimodal conversational context feature capture. In early work, the MERC task mainly adopted GRU  [35]  or LSTM  [39]  to capture multimodal information in the conversational context. For example, Poria et al.  [39]  proposed a multimodal conversation emotion recognition model based on Bidirectional Long Short-Term Memory (Bi-LSTM), which captures multimodal contextual information at each time step to understand conversational context relationships in sequence data better. Although methods based on GRU or LSTM can model multimodal conversation context, they cannot capture long-distance information dependencies due to limited memory capabilities. For instance, Ma et al.  [33]  used intra-modal and inter-modal Transformers to capture semantic information in a multimodal conversation context and designed a hierarchical gating mechanism to achieve the fusion of multimodal features. Although Transformer-based methods can capture long-distance semantic information through global sequence modeling, they underestimate the complexity of multimodal dialogue semantics. Due to the superiority of GNN in modeling complex relationships, most existing research chooses to use GNN for global semantic capture and has achieved remarkable results. For example, Li et al.  [24]  proposed directed Graph-based Cross-modal Feature Complementation (GraphCFC), which alleviates the heterogeneity gap problem in multimodal fusion by utilizing multiple subspace extractors and pairwise cross-modal complementation strategies. In addition, speaker information is vital in emotion recognition because emotions are usually subjective and individual experiences. Therefore, Ren et al.  [42]  built a graph model to incorporate conversational context information and speaker dependencies, and then introduced a multi-head attention mechanism to explore potential connections between speakers.\n\n(2) Multimodal conversational context feature fusion. Choosing an appropriate multimodal feature fusion strategy is another crucial step in multimodal dialogue emotion recognition  [9, 65] . For example, Zadeh et al.  [61]  proposed Tensor Fusion Network (TFN), has advantages in processing higher-order data structures (such as multi-dimensional arrays) and is therefore better able to preserve relationships between data when integrating multimodal information. So Liu et al.  [32]  proposed a Low-rank Multimodal Fusion (LMF) method. Multimodal fusion is performed using modalityspecific low-order factors by decomposing tensors and weights in parallel. It avoids calculating high-dimensional tensors, reduces memory overhead, and reduces exponential time complexity to linear. Tellamekala et al.  [48]  proposed Calibrated and Ordinal Latent Distribution Fusion (COLD Fusion). The proposed fusion framework involves learning the latent distribution over an unimodal temporal context by constraining the variance through calibration and ordinal ordering. Furthermore, contrastive learning has attracted increasing research attention due to its powerful ability to obtain meaningful representations through alignment fusion. Kim et al.  [19]  introduced a contrastive loss function to facilitate impactful adversarial learning. This approach enables the adversarial learning of weak emotional samples by leveraging strong emotional samples, thereby enhancing the comprehension of intricate emotional elements embedded in intense emotions. Wang et al.  [50]  proposed a multimodal feature fusion framework based on contrastive learning. The framework first improves the ability to capture emotional features through contrastive learning and then uses an attention mechanism to achieve the fusion of multimodal features.\n\nAlthough multimodal conversational emotion recognition has made significant progress by modeling contextual semantic information and feature fusion, the critical role of high-frequency information in MERC has been ignored. To this end, Hu et al.  [14]  proposed a Multimodal Fusion Graph Convolution Network (MMGCN). MMGCN can not only capture high and low-frequency information in multimodal conversations, but also utilizes speaker information to model inter-speaker and intra-speaker dependencies. Similarly, Chen et al.  [5]  modeled MERC from multivariate information and high-and low-frequency information, further improving the effect of multimodal conversational emotion recognition. Nevertheless, as discussed earlier, these methods do not profoundly explore the uses of high and low-frequency signals, ignoring the consistency and complementary synergy between them.\n\nThis paper starts from the perspective of graph spectrum, uses high and low-frequency signals to reconstruct MERC, captures and collaborates consistency and complementary semantic information, respectively, and improves the effect of multimodal conversational emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preliminary 3.1 Feature Extraction",
      "text": "Text Feature Extraction: Word embeddings can capture the semantic relationships between words, making words with similar meanings closer in the embedding space. Inspired by previous work  [9, 20, 43] , we use the RoBERTa model  [31]  to extract text features and the embedding is denoted as ğœ‘ ğ‘¡ . Audio and Vision Feature Extraction: Consistent with previous work  [13, 25, 35] , we employ openSMILE and 3D-CNN for audio and Vision feature extraction, yielding respective embeddings ğœ‘ ğ‘ and ğœ‘ ğ‘£ .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speaker Information Embedding",
      "text": "Speaker information can play an important role in emotion recognition. Emotion is not only related to the characteristic attributes of the utterance but also to the speaker's inherent expression manner. Inspired by previous work  [5, 14, 63] , we incorporate speaker information into each unimodal utterance to obtain an unimodal representation of context and speaker information.\n\nSpecifically, we first sort all speakers by name and then use the one-hot vector ğ‘  ğ‘– to represent the ğ‘–-th speaker. Finally, we perform a unified embedding representation for the speakers to make similar speakers closer together in the embedding space. The embedding of the ğ‘–-th speaker is as follows:\n\nwhereğ‘Š ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ is the trainable weight. In addition, to obtain higherorder feature representation, we utilize bidirectional Gated Recurrent Units (GRU) to encode conversational text features. We have observed in practice that using recursive modules to encode visual and auditory modalities has no positive performance impact. Therefore, we employed a multilayer perceptron with two single hidden layers to encode auditory and visual modalities, respectively. The specific encoding calculation is as follows:\n\n),\n\nwhere ğ‘Š ğ‘ , ğ‘ ğ‘ , ğ‘Š ğ‘£ and ğ‘ ğ‘£ are the learnable parameters of the auditory and visual encoders, respectively. We then add speaker embeddings to obtain speaker-and context-aware unimodal representations:\n\nwhere ğ‘¡, ğ‘, ğ‘£ represent text, audio, and vision modal, respectively.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "Fig.  2  shows the proposed Graph-Spectrum-based Multimodal Consistency and Complementary collaborative learning framework GS-MCC. GS-MCC contains five modules: feature encoding, multimodal interaction graph construction, Fourier graph neural network, contrastive learning, and emotion classification.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Interaction Graph",
      "text": "To model the latent semantic dependencies between multimodal utterances, we adopt a multimodal interaction graph for construction. Instead of fully connecting all nodes of the same modality, we use a sliding window for restriction. Although fully connecting all nodes of the same modality is beneficial to building long-distance semantic dependencies, it will introduce much noise, which is not conducive to subsequent GNN learning. Given a conversation sequence ğ‘ˆ = {ğ‘¢ 1 , ..., ğ‘¢ ğ‘ } with ğ‘ multimodal utterances, under the restriction of the sliding window ğ‘˜, we can construct a multimodal interaction graph ğº ğ‘˜ = ğ‘‰ ğ‘˜ , ğ¸ ğ‘˜ , ğ´ ğ‘˜ , ğ‘‹ ğ‘˜ , where the node ğ‘£ âˆˆ ğ‘‰ ğ‘˜ represents a single-modal utterance and the edge ğ‘’ âˆˆ ğ¸ ğ‘˜ represents two semantic interactive relationships between unimodal utterances, ğ´ ğ‘˜ is the adjacency matrix, and ğ‘‹ ğ‘˜ is the feature matrix. The multimodal semantic interaction graph is constructed as follows:\n\nNodes: Since any utterance ğ‘¢ ğ‘– âˆˆ ğ‘ˆ contains three modal information, we treat each modality in each utterance as an independent node, using text modal node ğ‘¥ ğ‘– ğ‘¡ , auditory modal node ğ‘¥ ğ‘– ğ‘ , and visual modal node ğ‘¥ ğ‘– ğ‘£ represents, and uses the corresponding features ğ‘¥ ğ‘– ğ‘š to represent the initial embedding of the node. The constructed multimodal interaction graph ğº ğ‘˜ has 3ğ‘ nodes. Edges: In order to avoid introducing noise or redundant information, we use a sliding window to limit node connections of the same mode. Specifically, we fully connect the nodes in the same mode within sliding window ğ‘˜. In addition, we connect different modal nodes of the same utterance to construct semantic interactions between modalities. For example, for utterance ğ‘¢ ğ‘– âˆˆ ğ‘ˆ , connections need to be constructed between nodes ğ‘¥ ğ‘– ğ‘¡ , ğ‘¥ ğ‘– ğ‘ , and ğ‘¥ ğ‘– ğ‘£ in different modalities.\n\nEdge Weight Initialization: In order to better capture the similarity between nodes, we use different similarities to determine edge weights for different types of edges. Nodes with higher similarity show more critical information interactions between them. Specifically, for edges coming from nodes of the same modality, since the feature distribution of the nodes is potentially consistent, our calculation method is as follows:\n\nwhere ğ‘¥ ğ‘– ğ‘š and ğ‘¥ ğ‘— ğ‘š represent the feature representations of the ğ‘–-th and ğ‘—-th nodes in the graph. For edges between nodes in different modalities, since the feature distribution of the nodes is not potentially consistent, we use the hyperparameter ğœ™ to optimize the similarity learning between cross-modal nodes. Our approach is computed as follows:\n\n(5)",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Fourier Graph Neural Network",
      "text": "As mentioned above, using a sliding window will limit long-distance dependency learning. This is because traditional GNN has oversmoothing characteristics and cannot stack many layers. Different from the methods used by MMGCN  [14]  and M 3 Net  [5] , this paper is inspired by FourierGNN  [56] , designs efficient Fourier graph operators for high and low-frequency signals, respectively, to capture the long-distance dependency information. Fourier Graph Operator. For a given multimodal interaction graph, ğº ğ‘˜ = ğ‘‰ ğ‘˜ , ğ¸ ğ‘˜ , ğ´ ğ‘˜ , ğ‘‹ ğ‘˜ , where ğ´ ğ‘˜ âˆˆ R 3ğ‘ Ã—3ğ‘ is the adjacency matrix, ğ‘‹ ğ‘˜ âˆˆ R 3ğ‘ Ã—ğ‘‘ is the feature matrix, ğ‘ is the number of multimodal utterances, and ğ‘‘ is the dimension of the feature. According to FourierGNN, we can obtain the Green kernel ğœ… âˆˆ R ğ‘‘ Ã—ğ‘‘ that meets the conditions based on the adjacency matrix ğ´ ğ‘˜ and the weight matrix ğ‘Š âˆˆ R ğ‘‘ Ã—ğ‘‘ , which needs to satisfy the conditions ğœ… [ğ‘–, ğ‘—] = ğœ… [ğ‘– -ğ‘—], ğœ… [ğ‘–, ğ‘—] = ğ´ ğ‘˜ ğ‘– ğ‘— â€¢ ğ‘Š , and ğ‘– and ğ‘— are fall between 1 and 3ğ‘ . Based on the kernel ğœ…, we can obtain the following Fourier graph operator S G :\n\nwhere F is the Discrete Fourier Transform (DFT). According to the graph convolution theory, we can express the graph convolution operation as follows: where ğœƒ G is the learnable parameter and F -1 is the Inverse Discrete Fourier Transform (IDFT). According to the convolution theory and the conditions of FGO, we can expand the frequency domain term in Eq. (  7 ) as follows:\n\nAs seen from Eq. (  8 ) , the graph convolution operation is implemented through the product of FGO and features in the frequency domain. In addition, according to the convolution theory, the convolution of time-domain signals is equal to the product of frequency-domain signals. The product operation in the frequency domain only requires ğ‘‚(ğ‘ ) time complexity, while the convolution operation in the time domain requires ğ‘‚ ğ‘ 2 time complexity. Therefore, an efficient graph neural network can be constructed based on the Fourier graph operator.\n\nTo efficiently capture high-and low-frequency information, we perform targeted optimization on FGO and use the high-pass and low-pass filters to extract complementary and consistent semantic information. The specific filter design is as follows:\n\nwhere ğ¼ is the identity matrix, ğ· G and ğ´ ğ‘‡ are the degree matrix and adjacency matrix of the multimodal interaction graph, respectively, and ğ¿ ğ‘™ and ğ¿ â„ are the low-pass and high-pass filters, respectively. Based on low-pass and high-pass filters, we can obtain the following low and high-frequency Green kernel and Fourier graph operator:\n\nFinally, we can build an ğ‘€-layer Fourier graph neural network based on these efficient Fourier graph operators to capture longdistance high and low-frequency dependency information in multimodal interaction graphs:\n\nwhere ğœ is the activation function, ğ‘ ğ‘™/â„ is the bias parameter, S ğ‘™/â„ Gâ†’ğ‘– is the FGO in the ğ‘–-th layer, ğ‘™, and â„ represent low and high frequencies respectively. By stacking ğ‘€ layers of Fourier graph operators, our model can capture long-distance dependency information and obtain each node's low-frequency feature representation, ğ‘¥ ğ‘™ ğ‘š , and high-frequency feature representation, ğ‘¥ â„ ğ‘š , respectively.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Contrastive Learning",
      "text": "Low-frequency features reflect the trend of slow changes in emotion, while high-frequency features reflect the trend of rapid changes in emotion. To synergize these two features, we employ contrastive learning to build self-supervised signals to promote consistent and complementary semantics learning in multimodal utterances.\n\nInspired by the SpCo  [29]  method, increasing the frequency domain difference between two contrasting views can achieve better contrast learning effects. Unlike SpCo, our contrastive learning is performed directly in the frequency domain and does not rely on data augmentation to generate contrastive views. Specifically, we use a combination of low-frequency contrast learning and highfrequency contrast learning to promote the synergy of the two features. In addition, we only use the strategy of negative sample pairs far away from each other to increase the frequency domain difference between contrasting views and obtain better contrast learning effects.\n\nLFCL: Low Frequency Contrastive Learning. LFCL aims to use low-frequency samples as anchor nodes and all high-frequency nodes as negative samples to construct a self-supervised signal to increase the frequency domain difference between contrast views to obtain better contrast learning effects and promote consistent semantics and complementary semantics learning in multimodal conversations. For each low-frequency anchor node, the self-supervised contrast loss can be defined as:\n\nwhere ğœ is temperature coefficient, ğ‘¥ ğ‘™ ğ‘š is the low-frequency anchor node, and ğ‘¥ â„ğ‘– - ğ‘š is the ğ‘–-th high-frequency negative sample. HFCL: High Frequency Contrastive Learning. HFCL is similar to LFCL, except that HFCL uses high-frequency samples as anchor nodes and all low-frequency nodes as negative samples to construct a self-supervised signal to increase the frequency domain difference between contrasting views. The specific contrast loss can be defined as:\n\nwhere ğ‘¥ â„ ğ‘š is the high-frequency anchor node, and ğ‘¥ ğ‘™ğ‘– - ğ‘š is the ğ‘–-th low-frequency negative sample.\n\nThe overall collaborative contrastive learning loss is the sum of LFCL and HFCL, which can be expressed as L ğ¶ğ¶ğ¿ :\n\nFinally, we use the inverse discrete Fourier transform to convert the high and low-frequency features into time domain features and concatenation the two parts of features to obtain the final embedding representation of the uni-modal utterance node:\n\nwhere ğ‘š âˆˆ {ğ‘¡, ğ‘, ğ‘£ } represents any one of text, auditory and visual modalities.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Emotion Classifier",
      "text": "For modal utterance ğ‘ˆ ğ‘– , we concatenate the features of each modality for emotion classification.\n\nwhere ğ‘Š ğ‘¢ and ğ‘ ğ‘¢ are learnable parameters, and Å·ğ‘– is the predicted emotion label of utterance ğ‘ˆ ğ‘– . Finally, we employ categorical crossentropy loss and contrastive loss for model training.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments 5.1 Implementation Details",
      "text": "Benchmark Datasets and Evaluation Metrics: In our experiments, we used two multimodal datasets, IEMOCAP  [3] , and MELD  [40] , widely used in multimodal emotion recognition. IEMOCAP (Interactive Emotional Dyadic Motion Capture Database) is a multimodal database for emotion recognition and analysis. The IEMO-CAP data set consists of movie dialogue clips and emotional annotations, including voice, video, and emotional annotation data of 10 actors in interactive dialogue scenes. MELD (Multimodal Emotion-Lines Dataset) contains dialogue text from movie and TV show clips.\n\nThe dialogue text contains the characters' speech and the context information of the dialogue. MELD also provides audio recordings and video recordings of conversations. We record the classification accuracy (Acc.) and F1 for each emotion category, as well as the overall weighted average accuracy (W-Acc.) and weighted average F1 (W-F1).\n\nBaseline Methods: We compare several baselines on the IEMO-CAP and MELD datasets, including bc-LSTM  [39] , and A-DMN  [53]  based on RNN architecture, LFM  [32]  based on Low-rank Tensor Fusion network, DialogueGCN  [13] , LR-GCN  [42] , DER-GCN  [1] , MMGCN  [14] , AdaGIN  [49] , RGAT  [15]  and CoMPM  [21]  based on GCN, EmoBERTa  [20] , CTNet  [27]  and COGMEN  [17]  based on Transformer architecture. Experimental Setup: All experiments are conducted using Python 3.8 and PyTorch 1.8 deep learning framework and performed on a single NVIDIA RTX 3090 24G GPU. Our model is trained using AdamW with a learning rate of 1e-5, cross-entropy as the loss function, and a batch size of 32. The optimal parameters of all models were obtained by performing parameter adjustment using the leave-one-out cross-validation method on the validation set.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With The State-Of-The-Art",
      "text": "Table  1  and Table  2  show the emotion recognition effects of the proposed GS-MCC method and the baseline method on the IEMO-CAP and MELD datasets, respectively. On the IEMOCAP dataset, GS-MCC has the best emotion recognition effect, outperforming all comparison baselines, and is 3.3% and 3.2% better than Ada-GIN on W-Acc and W-F1 ,respectively. In addition, GS-MCC has significantly improved Acc and F1 values in some emotion categories. Similarly, compared with all comparison baselines on the MELD data set, GS-MCC also has the best emotion recognition effect, outperforming AdaGIN by 0.5% and 2.2% on W-Acc and W-F1, respectively. Furthermore, AdaGIN is optimal in both Acc and F1 most emotion categories. Experimental results demonstrate the effectiveness of AdaGIN. The performance improvement may be attributed to the proposed method's ability to utilize long-distance contextual semantic information from fully-and low-frequency signals while avoiding the over-smoothing phenomenon of GCN. Furthermore, the proposed GS-MCC has only 2.10M model parameters, which is far lower than DialogueGCN and other GCNbased emotion recognition methods. Experimental results also demonstrate the potential application of our method in efficient computing.    fluctuate considerably. They are difficult to converge, but the loss of GS-MCC without contrastive loss is lower than DialogueGCN. However, GS-MCC with contrastive loss has better convergence, converging around a loss value of 0.9. Experimental results prove that the contrastive learning mechanism plays an essential role in the convergence of the GCN network and can collaborate with high-and low-frequency features for better emotion recognition.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Over-Smoothing Analysis",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "Ablation studies for SE, Fourier GNN, CL. Speaker embedding (SE), Fourier graph neural network (Fourier GNN), and contrastive learning (CL) are the three critical components of our proposed multimodal emotion recognition model. We only remove one proposed module at a time to verify the effectiveness of the component.\n\nIt is worth noting that when Fourier GNN is removed, we use DialogueGCN as the backbone of the model. From the emotion recognition results in Table  3 , we conclude: (1) All the modules we proposed are helpful because no matter which proposed module is deleted, it will cause the emotion recognition performance of the model to decrease. (  2 ) Speaker embedding has a relatively significant impact on the emotion recognition performance of the model because if the speaker embedding information is removed from the IEMOCAP and MELD data sets, the emotion recognition effect of the model will be significantly reduced. The experimental results show that the speaker's embedded information is essential for the model to understand emotions. (3) On the IEMOCAP and MELD datasets, Fourier GNN is more critical than contrastive learning. We speculate that this is because Fourier GNN can capture high and low-frequency signals to provide more useful emotional semantic information, and the contrastive learning mechanism mainly assists Fourier GNN in better achieving complementary and consistent semantic information collaboration.  Ablation studies for multimodal features. We conduct ablation experiments on multimodal features to compare the performance of single-modal, bi-modal, and tri-modal experimental results to explore the importance of each modality. The experimental results are listed in Table  4 . We choose W-Acc and W-F1 as evaluation metrics. In single-modal experiments, text modality features achieved the best performance, which shows that text features play a decisive role in MERC. Video features have the worst emotion recognition effect. We speculate that video features have more noise signals, making it difficult for the model to learn effective emotional feature representation. In bi-modal experiments, all bi-modal emotion recognition effects are better than their single-modal emotion recognition effects. The tri-modal emotion recognition effect is the best among all experiments. The performance improvement may be attributed to the effective fusion of multimodal, complementary semantic information, which can improve the feature representation ability of emotions. Therefore, GS-MCC can effectively utilize the consistent and complementary semantic information in multimodal conversations to improve the emotion recognition effect.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we rethink the problem of multimodal emotion recognition in conversation from the perspective of the graph spectrum, taking into account some shortcomings of existing work and innovations. Specifically, we propose a Graph-Spectrum-based Multimodal Consistency and Complementary feature collaboration framework GS-MCC. First, we combine sliding windows to build a multimodal interaction graph to model the conversational relationship between utterances and speakers. Secondly, we design efficient Fourier graph operators to capture long-distance utterances' consistency and complementary semantic dependencies. Finally, we adopt contrastive learning and construct self-supervised signals with all negative samples to promote the collaboration of the two semantic information. Extensive experiments on two widely used benchmark datasets, IEMOCAP and MELD, demonstrate the effectiveness and efficiency of our proposed method.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , MERC needs to recognize the emotion of each multimodal",
      "page": 1
    },
    {
      "caption": "Figure 1: gives an example of a multimodal conversation between",
      "page": 1
    },
    {
      "caption": "Figure 1: An example of a multimodal conversation from",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the proposed Graph-Spectrum-based Multimodal Con-",
      "page": 4
    },
    {
      "caption": "Figure 2: The overall architecture of the proposed model GS-MCC. Specifically, feature embedding of multimodal utterances and",
      "page": 5
    },
    {
      "caption": "Figure 3: Loss trends during model training and inference",
      "page": 7
    },
    {
      "caption": "Figure 3: shows the results",
      "page": 7
    },
    {
      "caption": "Figure 4: Emotion recognition performance of DialogueGCN",
      "page": 8
    },
    {
      "caption": "Figure 4: shows the experimental comparison results. On the IEMOCAP",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modality Encoding": "GRU 1t ïª u 1t2t3t\nGRU 2t ïª u\nGRU 3t ïª u\nText\n1a ïª u 1a2a3a\nFC ïª2 u\na\n3a ïª u\nAcoustic\n1v ïª u 1v2v3v\nFC 2v ïª u\n3v ïª u\nVisual",
          "Graph Construction": "",
          "Fourier Graph Neural Network": "mïƒ•i=0 hâ†’i\nFGO FGO ++ â€¦\nhÏƒ hÏƒ hÏƒ hâ†’ 0 hâ†’ i hâ†’ M\nHigh frequency\nDFT\nLow frequency\nlÏƒ Ïƒl Ïƒl lâ†’ lâ†’ lâ†’\n0 i M\nFGO FGO ++ â€¦\nïƒ•m\nl\nâ†’i\ni=0",
          "Contrastive Learning": "",
          "Emotion Classifier": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "DER-GCN: Dialog and Event Relation-Aware Graph Convolutional Neural Network for Multimodal Dialog Emotion Recognition",
      "authors": [
        "Wei Ai",
        "Yuntao Shou",
        "Tao Meng",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "2",
      "title": "A Two-Stage Multimodal Emotion Recognition Model Based on Graph Contrastive Learning",
      "authors": [
        "Wei Ai",
        "Fuchen Zhang",
        "Tao Meng",
        "Yuntao Shou",
        "Hongen Shao",
        "Keqin Li"
      ],
      "year": "2023",
      "venue": "2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS)"
    },
    {
      "citation_id": "3",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Not all low-pass filters are robust in graph convolutional networks",
      "authors": [
        "Heng Chang",
        "Yu Rong",
        "Tingyang Xu",
        "Yatao Bian",
        "Shiji Zhou",
        "Xin Wang",
        "Junzhou Huang",
        "Wenwu Zhu"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recognition in conversation",
      "authors": [
        "Feiyu Chen",
        "Jie Shao",
        "Shuyuan Zhu",
        "Heng Tao Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Learning a structural causal model for intuition reasoning in conversation",
      "authors": [
        "Hang Chen",
        "Bingyu Liao",
        "Jing Luo",
        "Wenjing Zhu",
        "Xinyu Yang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "7",
      "title": "Multimodal sentiment analysis based on attentional temporal convolutional network and multi-layer feature fusion",
      "authors": [
        "Hongju Cheng",
        "Zizhen Yang",
        "Xiaoqi Zhang",
        "Yang Yang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Semi-Supervised Multimodal Emotion Recognition with Expression MAE",
      "authors": [
        "Zebang Cheng",
        "Yuxiang Lin",
        "Zhaoru Chen",
        "Xiang Li",
        "Shuyi Mao",
        "Fan Zhang",
        "Daijun Ding",
        "Bowen Zhang",
        "Xiaojiang Peng"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Pankaj Wasnik, and Naoyuki Onoe. 2022. M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Shah"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "A survey of textual emotion recognition and its challenges",
      "authors": [
        "Jiawen Deng",
        "Fuji Ren"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin WÃ¶llmer",
        "BjÃ¶rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Emonet: A transfer learning framework for multi-corpus speech emotion recognition",
      "authors": [
        "Maurice Gerczuk",
        "Shahin Amiriparian",
        "Sandra Ottl",
        "BjÃ¶rn Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "14",
      "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Relationaware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "16",
      "title": "3D convolutional neural networks for human action recognition",
      "authors": [
        "Shuiwang Ji",
        "Wei Xu",
        "Ming Yang",
        "Kai Yu"
      ],
      "year": "2012",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "17",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain",
        "Atin Singh",
        "Ashutosh Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "18",
      "title": "A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges",
      "authors": [
        "Wei Ju",
        "Siyu Yi",
        "Yifan Wang",
        "Zhiping Xiao",
        "Zhengyang Mao",
        "Hourun Li",
        "Yiyang Gu",
        "Yifang Qin",
        "Nan Yin",
        "Senzhang Wang"
      ],
      "year": "2024",
      "venue": "A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges",
      "arxiv": "arXiv:2403.04468"
    },
    {
      "citation_id": "19",
      "title": "Contrastive adversarial learning for person independent facial emotion recognition",
      "authors": [
        "Daeha Kim",
        "Byung Cheol"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with Roberta",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "Computing Research Repository-arXiv"
    },
    {
      "citation_id": "21",
      "title": "CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "22",
      "title": "Revisiting disentanglement and fusion on modality and context in conversational multimodal emotion recognition",
      "authors": [
        "Bobo Li",
        "Hao Fei",
        "Lizi Liao",
        "Yu Zhao",
        "Chong Teng",
        "Tat-Seng Chua",
        "Donghong Ji",
        "Fei Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "23",
      "title": "GA2MIF: graph and attention based two-stage multi-source information fusion for conversational emotion detection",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "24",
      "title": "Graphcfc: A directed graph based cross-modal feature complementation approach for multimodal conversational emotion recognition",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "26",
      "title": "Gcnet: Graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Zheng Lian",
        "Lan Chen",
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "27",
      "title": "CTNet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "28",
      "title": "Multi-task momentum distillation for multimodal sentiment analysis",
      "authors": [
        "Ronghao Lin",
        "Haifeng Hu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Revisiting graph contrastive learning from the perspective of graph spectrum",
      "authors": [
        "Nian Liu",
        "Xiao Wang",
        "Deyu Bo",
        "Chuan Shi",
        "Jian Pei"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "30",
      "title": "EmotionKD: A Cross-Modal Knowledge Distillation Framework for Emotion Recognition Based on Physiological Signals",
      "authors": [
        "Yucheng Liu",
        "Ziyu Jia",
        "Haichao Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "32",
      "title": "Efficient Low-rank Multimodal Fusion With Modality-Specific Factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "33",
      "title": "A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Bo Zhang",
        "Yijia Zhang",
        "Bo Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "34",
      "title": "Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis",
      "authors": [
        "Sijie Mai",
        "Ying Zeng",
        "Shuangjia Zheng",
        "Haifeng Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "36",
      "title": "A multimessage passing framework based on heterogeneous graphs in conversational emotion recognition",
      "authors": [
        "Tao Meng",
        "Yuntao Shou",
        "Wei Ai",
        "Jiayi Du",
        "Haiyan Liu",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "37",
      "title": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "Tao Meng",
        "Yuntao Shou",
        "Wei Ai",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2023",
      "venue": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "arxiv": "arXiv:2312.06337"
    },
    {
      "citation_id": "38",
      "title": "Revisiting graph neural networks: All we have is low-pass filters",
      "authors": [
        "Hoang Nt",
        "Takanori Maehara"
      ],
      "year": "2019",
      "venue": "Revisiting graph neural networks: All we have is low-pass filters",
      "arxiv": "arXiv:1905.09550"
    },
    {
      "citation_id": "39",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "40",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "41",
      "title": "COM: Contrastive Masked-attention model for incomplete multimodal learning",
      "authors": [
        "Shuwei Qian",
        "Chongjun Wang"
      ],
      "year": "2023",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "42",
      "title": "Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "Minjie Ren",
        "Xiangdong Huang",
        "Wenhui Li",
        "Dan Song",
        "Weizhi Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "43",
      "title": "Directed Acyclic Graph Network for Conversational Emotion Recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "44",
      "title": "Adversarial representation with intra-modal and inter-modal graph contrastive learning for multimodal emotion recognition",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Keqin Li"
      ],
      "year": "2023",
      "venue": "Adversarial representation with intra-modal and inter-modal graph contrastive learning for multimodal emotion recognition",
      "arxiv": "arXiv:2312.16778"
    },
    {
      "citation_id": "45",
      "title": "Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Sihan Yang",
        "Keqin Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "46",
      "title": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2023",
      "venue": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "arxiv": "arXiv:2312.05735"
    },
    {
      "citation_id": "47",
      "title": "Revisiting Multi-modal Emotion Learning with Broad State Space Models and Probabilityguidance Fusion",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Fuchen Zhang",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Revisiting Multi-modal Emotion Learning with Broad State Space Models and Probabilityguidance Fusion",
      "arxiv": "arXiv:2404.17858"
    },
    {
      "citation_id": "48",
      "title": "COLD fusion: Calibrated and ordinal latent distribution fusion for uncertainty-aware multimodal emotion recognition",
      "authors": [
        "Mani Kumar Tellamekala",
        "Shahin Amiriparian",
        "W BjÃ¶rn",
        "Elisabeth Schuller",
        "Timo AndrÃ©",
        "Michel Giesbrecht",
        "Valstar"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "49",
      "title": "Adaptive Graph Learning for Multimodal Conversational Emotion Detection",
      "authors": [
        "Geng Tu",
        "Tian Xie",
        "Bin Liang",
        "Hongpeng Wang",
        "Ruifeng Xu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "50",
      "title": "A selffusion network based on contrastive learning for group emotion recognition",
      "authors": [
        "Xingzhi Wang",
        "Dong Zhang",
        "Hong-Zhou Tan",
        "Dah-Jye Lee"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "51",
      "title": "Distribution-consistent modal recovering for incomplete multimodal learning",
      "authors": [
        "Yuanzhi Wang",
        "Zhen Cui",
        "Yong Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "52",
      "title": "Leveraging multi-modal interactions among the intermediate representations of deep transformers for emotion recognition",
      "authors": [
        "Yang Wu",
        "Zhenyu Zhang",
        "Pai Peng",
        "Yanyan Zhao",
        "Bing Qin"
      ],
      "year": "2022",
      "venue": "Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge"
    },
    {
      "citation_id": "53",
      "title": "Adapted dynamic memory network for emotion recognition in conversation",
      "authors": [
        "Songlong Xing",
        "Sijie Mai",
        "Haifeng Hu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "Disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Haopeng Kuang",
        "Yangtao Du",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "55",
      "title": "Emotion Recognition in Conversation Based on a Dynamic Complementary Graph Convolutional Network",
      "authors": [
        "Zhenyu Yang",
        "Xiaoyang Li",
        "Yuhu Cheng",
        "Tong Zhang",
        "Xuesong Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "56",
      "title": "FourierGNN: Rethinking multivariate time series forecasting from a pure graph perspective",
      "authors": [
        "Kun Yi",
        "Qi Zhang",
        "Wei Fan",
        "Hui He",
        "Liang Hu",
        "Pengyang Wang",
        "Ning An",
        "Longbing Cao",
        "Zhendong Niu"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "57",
      "title": "Dynamic hypergraph convolutional network",
      "authors": [
        "Nan Yin",
        "Fuli Feng",
        "Zhigang Luo",
        "Xiang Zhang",
        "Wenjie Wang",
        "Xiao Luo",
        "Chong Chen",
        "Xian-Sheng Hua"
      ],
      "year": "2022",
      "venue": "2022 IEEE 38th International Conference on Data Engineering (ICDE)"
    },
    {
      "citation_id": "58",
      "title": "Messages are never propagated alone: Collaborative hypergraph neural network for time-series forecasting",
      "authors": [
        "Nan Yin",
        "Li Shen",
        "Huan Xiong",
        "Bin Gu",
        "Chong Chen",
        "Xian-Sheng Hua",
        "Siwei Liu",
        "Xiao Luo"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "59",
      "title": "Dynamic spiking graph neural networks",
      "authors": [
        "Nan Yin",
        "Mengzhu Wang",
        "Zhenghan Chen",
        "Giulia De Masi",
        "Huan Xiong",
        "Bin Gu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "60",
      "title": "Counterfactual Explainable Conversational Recommendation",
      "authors": [
        "Dianer Yu",
        "Qian Li",
        "Xiangmeng Wang",
        "Qing Li",
        "Guandong Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "61",
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "62",
      "title": "Dualgats: Dual graph attention networks for emotion recognition in conversations",
      "authors": [
        "Duzhen Zhang",
        "Feilong Chen",
        "Xiuyi Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "63",
      "title": "A Multi-Level Alignment and Cross-Modal Unified Semantic Graph Refinement Network for Conversational Emotion Recognition",
      "authors": [
        "Xiaoheng Zhang",
        "Weigang Cui",
        "Bin Hu",
        "Yang Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "64",
      "title": "TDFNet: Transformer-based Deep-scale Fusion Network for Multimodal Emotion Recognition",
      "authors": [
        "Zhengdao Zhao",
        "Yuhua Wang",
        "Yuezhu Xu",
        "Jiayuan Zhang"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "65",
      "title": "Multimodal Prompt Transformer with Hybrid Contrastive Learning for Emotion Recognition in Conversation",
      "authors": [
        "Shihao Zou",
        "Xianying Huang",
        "Xudong Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    }
  ]
}