{
  "paper_id": "2206.03173v1",
  "title": "Speaker-Guided Encoder-Decoder Framework For Emotion Recognition In Conversation",
  "published": "2022-06-07T10:51:47Z",
  "authors": [
    "Yinan Bao",
    "Qianwen Ma",
    "Lingwei Wei",
    "Wei Zhou",
    "Songlin Hu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The emotion recognition in conversation (ERC) task aims to predict the emotion label of an utterance in a conversation. Since the dependencies between speakers are complex and dynamic, which consist of intra-and inter-speaker dependencies, the modeling of speaker-specific information is a vital role in ERC. Although existing researchers have proposed various methods of speaker interaction modeling, they cannot explore dynamic intraand inter-speaker dependencies jointly, leading to the insufficient comprehension of context and further hindering emotion prediction. To this end, we design a novel speaker modeling scheme that explores intra-and inter-speaker dependencies jointly in a dynamic manner. Besides, we propose a Speaker-Guided Encoder-Decoder (SGED) framework for ERC, which fully exploits speaker information for the decoding of emotion. We use different existing methods as the conversational context encoder of our framework, showing the high scalability and flexibility of the proposed framework. Experimental results demonstrate the superiority and effectiveness of SGED.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversation (ERC) aims to identify the emotion of each utterance in a conversation. Due to its potential applications in creating empathetic dialogue systems  [Zhou et al., 2018] , social media analysis  [Li et al., 2019] , and so on, it has been concerned by a considerable number of researchers recently.\n\nUnlike vanilla emotion recognition of sentences, emotions expressed in a conversation are dynamic and can be affected by many factors such as surrounding conversational context, and speaker dependencies  [Poria et al., 2019b] . Consequently, ERC models require a strong ability to model context, select crucial information, and capture speaker dependencies, which is more challenging. Among all the factors, speaker information is vital for tracking the emotional Figure  1 : (a) Speaker modeling scheme of existing dynamic speakerspecific modeling methods, which is based on speaker identity to explore intra-speaker dependency. (b) Our novel speaker modeling scheme, which uses the speaker state encoder (SSE) to explore intraand inter-speaker dependencies dynamically. ui means the i th utterance in a dialogue and si means the corresponding speaker state vector of ui.\n\ndynamics of conversations, which consists of two important aspects: intra-and inter-speaker dependencies  [Poria et al., 2019a] . Intra-speaker dependency, also known as emotional inertia, deals with the aspect of emotional influence that speakers have on themselves during conversations  [Kuppens et al., 2010] . In contrast, inter-speaker dependency refers to the emotional influences that the counterparts induce into a speaker  [Poria et al., 2019c] . Exploring intra-speaker dependency and inter-speaker dependency jointly is conducive to comprehending the complex dialogue context that implies various emotions.\n\nNumerous approaches have been proposed for ERC with a focus on intra-and inter-speaker dependencies modeling. They can be divided into two categories: static speakerspecific modeling  [Zhang et al., 2019; Ghosal et al., 2019; Li et al., 2020; Shen et al., 2021a]  and dynamic speakerspecific modeling  [Majumder et al., 2019; Hu et al., 2021] . For the static speaker-specific modeling methods, they utilize speaker information attaching to each utterance to specify different connections between utterances. In this way, the injected prior speaker information is static and shows the limited ability to facilitate conversational context modeling. Intuitively, the speaker state that is affected by the intra-and inter-speaker dependencies, changes dynamically during a dialogue. Different states of a certain speaker greatly affect various emotional expressions. Thus, it is necessary to implement dynamic speaker-specific modeling.\n\nAs shown in Figure  1 (a), existing dynamic speaker-specific modeling methods  [Majumder et al., 2019; Hu et al., 2021]  design speaker-specific recurrent neural networks (RNNs) to capture self-dependencies between adjacent utterances of the same speaker. In this scheme, utterances are classified based on speaker identity. However, sequences composed of separate utterances attaching to the same speaker are disjunctive and incomplete, which hinder the comprehension of the context. Moreover, overreliance on the speaker identity hinders the modeling of dynamic inter-speaker dependency. In this way, the model can't explore dynamic interactions between different speakers, which greatly affects the psychology of speakers, and then results in different emotional expressions. Nonetheless, how to explore intra-and inter-speaker dependencies that change dynamically is still unresolved.\n\nTo alleviate the problem, we design a novel speaker modeling scheme that explores intra-and inter-speaker dependencies dynamically. Besides, we propose a speaker-guided encoder-decoder framework (SGED), which can combine with various existing methods. The encoder module consists of two parts: conversational context encoder (CCE) and speaker state encoder (SSE). We use different existing methods which ignore dynamic speaker dependencies modeling as the CCE. For the SSE, we design a novel speaker modeling scheme to generate a dynamic speaker state sequence that depends on the speaker's previous state (intra-speaker dependency) and other neighboring speakers' states (inter-speaker dependency). For the decoder module based on RNN, we use speaker states to guide the decoding of utterances' emotions step by step. Extensive experiments indicate the effectiveness of SGED. In summary, our contributions are three-fold: • To fully exploit the complex interactions between speakers that are vital for context comprehension, we propose a speaker-guided encoder-decoder framework (SGED) for ERC, formulating the modeling of speaker interactions as a flexible component. • We are the first to explore dynamic intra-and inter-speaker dependencies jointly through a subtly designed speaker state encoder, which explores complex interactions between speakers effectively. • Experimental results on three benchmark datasets demonstrate the effectiveness and high scalability of SGED.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Recently, ERC has received increasing attention from the NLP community. Unlike traditional emotion recognition tasks, ERC models should be endowed with the ability of speaker interaction modeling and conversational context modeling.\n\nSpeaker Interaction Modeling According to whether the speaker-specific information is modeled dynamically during the dialogue, speaker modeling methods can be divided into two categories: static and dynamic speaker-specific modeling. For the static methods, some of them  [Ghosal et al., 2019; Zhang et al., 2019]  treat dialogue as a graph and inject prior speaker information as different relations between utterances or treat speakers as nodes in the graph. DialogXL  [Shen et al., 2021a]  changes the masking strategies of self-attention to capture intra-and inter-speaker dependencies. HiTrans  [Li et al., 2020]  exploits an auxiliary task to classify whether two utterances belong to the same speaker to make the model speaker-sensitive. However, these methods inject prior static speaker information into the model, neglecting that speaker states change dynamically according to the varying intra-and inter-speaker dependencies along with the dialogue. By contrast,  DialogueRNN [Majumder et al., 2019]  and Dialogue-CRN  [Hu et al., 2021]     Ghosal et al., 2020] . We use different methods as the context encoder, proving the effectiveness and high scalability of our proposed speaker-guided encoder-decoder framework.\n\n3 Preliminary",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Problem Definition",
      "text": "A conversation containing N utterances is denoted as C = {u 1 , u 2 , ..., u N }. Each utterance u i consists of n i tokens, namely u i = {x i1 , x i2 , ..., x ini }. There are S speakers s 1 , s 2 , ..., s S (S ≥ 2) in a dialogue. Utterance u i is uttered by speaker s φ(ui) where the function φ maps u i to its corresponding speaker. If u i isn't the first utterance spoken by s φ(ui) in the conversation, we denote u ψ(ui) as the last previous utterance expressed by the same speaker where 0 < ψ(u i ) < i. We denote {u j |∀j, ψ(u i ) ≤ j < i} as the local information for u i . ERC task aims to predict the emotion label y i for each utterance u i in the conversation C.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Textual Feature Extraction",
      "text": "By convention, we use the pre-trained language model to extract utterance-level representations. Following  Shen et al [2021b] , we fine-tune the pre-trained language model RoBERTa-Large  [Liu et al., 2019]  on each ERC dataset first and then freeze its parameters while training our proposed framework. Concretely, we first append a special token [CLS] at the beginning of a given utterance u i to obtain the input sequence {[CLS], x i1 , x i2 , ..., x ini }. Afterward, we extract a representation of u i from the [CLS]'s embedding of the last layer with a dimension of 1,024.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "As illustrated in Figure  2 , there are three components in our proposed framework SGED: (1) conversational context en-",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conversational Context Encoder",
      "text": "We take the existing baseline as the conversation context encoder of SGED, which is easily replaceable. 1 For a given dialogue C, we generate the utterance representation vector sequence H = [h 1 , h 2 , ..., h N ] as follows:\n\nwhere Encoder means the replaceable conversational context encoder.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speaker State Encoder",
      "text": "Afterward, we generate a speaker state vector for each utterance based on the utterance representations from the conversational context encoder. The dynamic speaker state has two factors: intra-speaker dependency and inter-speaker dependency. We take the sequence of utterance representations as input and model the two kinds of dependencies simultaneously to obtain the final speaker state vectors sequentially.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Intra-Speaker Dependency Modeling",
      "text": "For a given utterance u i which is uttered by speaker s φ(ui) , we take the last previous speaker state of s φ(ui) and previous context c i = [h 1 , h 2 , ..., h i ] ∈ R h×i as input, modeling the intra-speaker dependency implied in the dialogue. First, we utilize the last previous speaker state vector of φ(ui) and the representation vector of u i to obtain the query vector q intra i as follows:\n\nwhere v ψ(ui) ∈ R h×1 means the last previous speaker state vector of s φ(ui) ; || means the concatenation operation; W intra q ∈ R h×2h and b intra q ∈ R 1 are model parameters. 1 In the experiments, we take different existing methods, namely bc-LSTM+att  [Poria et al., 2017] , DialogueGCN  [Ghosal et al., 2019] , and DAG-ERC  [Shen et al., 2021b] , for the conversational context encoder.\n\nThen, we use q intra i as the query and previous context c i as the key and value to implement the attention mechanism, generating the intra-speaker state vector of u i as follows:\n\nwhere is an element-wise product operation;\n\nis the attention score to aggregate the previous context; v intra i ∈ R 1×h is the intra-speaker state vector of u i . Inter-Speaker Dependency Modeling Simultaneously, we treat the representation vector of u i and the speaker state vectors of local information {u j |∀j, ψ(u i ) ≤ j < i} as input, modeling the latent interspeaker dependency.\n\nWe adopt the following methods to generate the interspeaker state vector of u i :\n\nwhere k i = {v j |∀j, ψ(u i ) ≤ j < i} ∈ R h×l denotes the speaker state vectors of previous local information; means the element-wise product operation;\n\nis the inter-speaker state vector of u i . Finally, we fuse the intra-speaker state vector v intra i and the inter-speaker state vector v inter i to generate the final speaker state vector of u i :\n\nwhere v i ∈ R 1×h is the speaker state vector of u i . Note that if u i is the first utterance expressed by speaker s φ(ui) in the dialogue, we let v i = h i . For the given dialogue C, we obtain the speaker state vector sequence V = {v 1 , v 2 , ..., v N }, which encodes speaker-specific intra-speaker dependency and inter-speaker dependency dynamically.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speaker-Guided Decoder",
      "text": "To fully utilize the vital speaker-specific information implied in the conversation, we use the speaker state vector v i to guide the decoding of u i 's emotion. In addition, inherent relations between emotion tags also facilitate ERC  [Wang et al., 2020] . Therefore, we also use the embedding of the predicted emotion of u i-1 to instruct the decoding of u i 's emotion.\n\nWe subtly design a speaker-guided emotion decoder based on (gated recurrent unit) GRU to decode each utterance's emotion in a given dialogue sequentially.\n\nGiven an utterance u i , we first match the speaker state vector v i with the utterance representation vector h i as follows:\n\nwhere W m ∈ R h×h and b m ∈ R 1 are trainable parameters for feature transformation; m i ∈ R 1×h is the generated match vector. Afterward, we concatenate the match vector m i and the embedding e i-1 of the predicted emotion of u i-1 and then feed it to GRU:\n\nFinally, we take the concatenation of h i ∈ R h×1 and o i ∈ R 1×h as the final representation of u i , and feed it to a feedforward neural network for emotion prediction:\n\nwhere\n\ntrainable parameters; L is the class number of the dataset; ŷi is the predicted label of u i ; E is the look-up table of emotion embeddings which is initialized randomly at the beginning; e i is the embedding of the predicted emotion of u i .\n\nTo train the model, we use cross-entropy loss function:\n\nwhere M is the number of dialogues in the training set; N j is the number of utterances in the j-th dialogue; y j,t is the ground-truth label; θ is the collection of trainable parameters.\n\n5 Experimental Settings",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We evaluate our proposed framework on three ERC benchmark datasets. The statistics of them are shown in Table  1 .\n\nIEMOCAP  [Busso et al., 2008] : A two-party conversation dataset for ERC, which includes 6 types of emotion, that is neutral, happiness, sadness, anger, frustrated, and excited.\n\nSince this dataset has no validation set, we follow  [Shen et al., 2021b]   MELD  [Poria et al., 2019a] : A multi-party conversation dataset collected from the TV series Friends. The emotion labels are: neutral, happiness, surprise, sadness, anger, disgust, and fear.\n\nEmoryNLP  [Zahiri and Choi, 2018] : A multi-party conversation dateset collected from Friends, but varies from MELD in the choice of scenes and emotion labels. There are 7 types of emotion, namely neutral, sad, mad, scared, powerful, peaceful, and joyful. Although all of the datasets contain multimodal information, we only focus on the textual information for experiments. Following existing work  [Shen et al., 2021b] , we choose weighted-average F1 for evaluation metrics.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Compared Methods",
      "text": "We compare our method with the following baselines.\n\nKET  [Zhong et al., 2019]  is a knowledge-enriched transformer that extracts commonsense knowledge based on a graph attention mechanism. COSMIC  [Ghosal et al., 2020]  incorporates different elements of commonsense and leverages them to learn self-speaker dependency.  Dia-logueRNN [Majumder et al., 2019]  uses speaker-specific GRU to model intra-speaker dependency and generate speaker states sequentially in a dynamic manner. Another dynamic speaker-specific modeling method is Dialogue-CRN  [Hu et al., 2021] , which utilizes speaker-level context to model dynamic intra-speaker dependency and designs a multi-turn reasoning module to imitate human cognitive thinking. bc-LSTM+att  [Poria et al., 2017]  is a contextual LSTM network without speaker information modeling, which is based on an attention mechanism to re-weight the features. DialogueGCN  [Ghosal et al., 2019]  is a graphbased method that treats speaker dependencies as static information and designs different relations between utterances. DAG-ERC  [Shen et al., 2021b ] models a dialogue in the form of a directed acyclic graph and utilizes speaker identity to establish connections between utterances.\n\nWe use the same feature extractor RoBERTa  [Liu et al., 2019]  as provided by  Shen et al., [2021b] . We replace the textual feature in the previous methods with our extracted features first. Then, we use previous methods without dynamic speaker-specific information modeling (RoBERTa, bc-LSTM+att, DialogueGCN, DAG-ERC 2  ) as the conversational context encoder of the proposed speaker-guided  encoder-decoder framework, investigating the effectiveness of our speaker modeling scheme.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "We utilize the validation set to tune parameters on each dataset and adopt Adam  [Kingma and Ba, 2015]  as the optimizer. The learning rate and batch size are selected from {1e-5, 5e-5, 1e-4, 5e-4} and {8, 16, 32}, respectively. The dimension of hidden vector h is set to 300 and the dimension of label embedding is set to 100. The feature size for the RoBERTa extractor is 1,024. We use the fine-tuned RoBERTa features provided by  Shen et al., [2021b] . Each training process contains 60 epochs. The results of our proposed SGED framework reported in the experiments are all based on the average score of 5 random runs on the test set.\n\n6 Results and Analysis",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Overall Results",
      "text": "Overall results of our proposed method and the baselines are reported on Table  2 . The results demonstrate that methods combined with our speaker-guided encoder-decoder framework achieve significant improvement.\n\nAs shown in Table  2 , based on the same feature extractor RoBERTa, the framework proposed by us outperforms the other dynamic speaker-specific information modeling methods (DialogueRNN, DialogueCRN) in general. Moreover, SGED + RoBERTa which encodes each utterance independently in the context encoder, achieves comparable results with DialogueRNN+RoBERTa, proving the effectiveness of the SGED proposed by us. Taking different types of existing methods as the conversational context encoder, SGED which combines the novel speaker modeling scheme achieves significant improvement all the time. It indicates the high scalability and flexibility of SGED. Moreover, SGED + DAG-ERC* reaches the new state-of-the-art on the three benchmark datasets, even surpasses COSMIC which utilizes external commonsense knowledge.\n\nMELD and EmoryNLP: On these multi-party conversation datasets, we observe that our SGED framework makes varying degrees of promotion based on different conversational context encoders. For the recurrence-based method bc-LSTM+att and context-independent method RoBERTa, the promotion brought by the SGED framework is about 0.55% on average. However, for the graph-based methods Dia-logueGCN and DAG-ERC, our SGED framework achieves an increase of 1.66% on average. It is because the speaker state encoder of SGED is established based on the conversational context encoder. Thus, the context comprehension ability of the conversational context encoder affects the subsequent dynamic speaker-specific information modeling, and finally influences the decoding of emotion. The stable improvement brought by SGED indicates that the proposed dynamic speaker modeling scheme helps to explore crucial dependencies between speakers and benefits ERC.\n\nIEMOCAP: On the two-party conversation dataset, we observe that the improvement brought by SGED has little correlation with the type of context encoder, which is different from the results on the multi-party conversation datasets. Besides, the improvement of SGED on IEMOCAP is 0.83% on average, which is slightly lower than the improvements on MELD and EmoryNLP. We argue that the dependencies between speakers in multi-party conversations are more complex and intractable. Therefore, it is more necessary to exploit SGED to model speaker-specific states dynamically for multi-party conversations.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Ablation Study",
      "text": "To investigate the effects of different modules in SGED, we conduct several ablation studies on EmoryNLP and IEMO-CAP. We remove 1) intra-speaker dependency in speaker state encoder (SSE), 2) inter-speaker dependency in SSE, 3) SSE, 4) speaker-guided decoder (SGD), and 5) SSE and SGD, respectively. The results are reported on Table  3 . Effect of Different Dynamic Dependencies between Speakers. As shown in Table  3 , the result on EmoryNLP declines when removing either intra-speaker dependency or inter-speaker dependency in the SSE. However, SGED without inter-speaker dependency performs worse than without intra-speaker dependency on IEMOCAP. It proves that both dynamic intra-and inter-speaker dependencies are important for multi-party conversations while dynamic inter-speaker dependency is more important for two-party conversations. Moreover, the result drops sharply when eliminating the entire speaker state encoder (SSE) that explores both intra-and inter-speaker dependencies in a dynamic manner, which indicates that the two kinds of dynamic dependency are complementary to each other. Effect of Speaker-Guided Decoder. To evaluate the effectiveness of the speaker-guided decoder (SGD), we remove it and only use the speaker state vector for prediction. As shown in Table  3 , it is clear that the speaker-guided decoder proposed by us boosts model performance a lot. Removing the SGD not only has a great impact on multi-party conversations (1.28%) but also two-party conversations (1.23%), which proves that the aggregation of speaker state feature and semantic feature benefits emotion recognition in conversation. Effect of the Novel Speaker Modeling Scheme. Our proposed speaker modeling scheme consists of speaker state encoder (SSE) and speaker-guided decoder (SGD). As reported on Tabel 3, removing SSE and SGD simultaneously results in a significant drop. It indicates the effectiveness of the proposed speaker modeling scheme, which can capture dynamic speaker interactions and promote emotion prediction.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Discussion",
      "text": "As shown in Figure  3  (a), we illustrate the results of dialogues with the different number of speakers on MELD, achieved by DialogueGCN and our SGED framework which takes Di-alogueGCN as the conversational context encoder. We observe that our proposed method exceeds DialogueGCN on dialogues with different speaker numbers except 6. Compared with DialogueGCN, our method achieves the greatest promotion on dialogues with 5 speakers. We will take an example with 5 speakers for the case study and conduct an error analysis on dialogues with 6 speakers.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Case Study",
      "text": "In Figure  3  (b), we present an abridged dialogue that has 5 speakers originally and our method recognizes most emotions correctly. For person A, we recognize u 3 as anger correctly though it is a short utterance that is intractable. We argue that the intra-speaker dependency of A captured by our method facilitates the recognition. Moreover, although person A's previous utterances (u 1 , u 2 , u 3 ) all express anger, our method recognizes u 7 as happiness which is obviously different from anger, based on the inter-speaker dependencies among person C (u 5 ), B (u 6 ), and A (u 7 ). For person B, we recognize u 8 as happiness correctly, due to the trade-off of inter-speaker dependency between person A (u 7 ) and B (u 8 ) and intra-speaker dependency of B captured by our method. It indicates that SGED models dynamic speaker states effectively with integration of intra-speaker dependency and inter-speaker dependency.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Error Analysis",
      "text": "In Figure  3  (c), we present the confusion matrix of conversations with 6 speakers excluding the diagonal, achieved by our method. From the heat map, neutral samples are more confused with non-neutral (e.g., happiness) samples, which can be alleviated by adding cues from other modalities. In addition to this, distinguishing similar emotions (e.g., happiness and surprise) still disturbs our method.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a speaker-guided encoder-decoder framework (SGED) for ERC, which captures dynamic intraand inter-speaker dependencies to benefit the understanding of the complex context. In this framework, we propose a novel speaker state encoder (SSE) and design a speakerguided decoder (SGD). SSE is competent at exploring intraand inter-speaker dependencies dynamically. Moreover, we treat different existing methods as the conversational context encoder of SGED, showing its high scalability and flexibility. Experiments on three benchmark datasets prove the superiority and effectiveness of our method. Especially, our method achieves a more significant improvement on the multi-party conversation datasets, indicating its strong ability to model dynamic multi-speaker states during a dialogue.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) Speaker modeling scheme of existing dynamic speaker-",
      "page": 1
    },
    {
      "caption": "Figure 1: (a), existing dynamic speaker-speciﬁc",
      "page": 1
    },
    {
      "caption": "Figure 2: , there are three components in our",
      "page": 2
    },
    {
      "caption": "Figure 2: An overview of SGED. SSE represents the speaker state encoder. As shown in the right part of the ﬁgure, we take u6 as an example",
      "page": 3
    },
    {
      "caption": "Figure 3: (a) The weighted-average F1 score of conversations with the different number of speakers on MELD, achieving by DialogueGCN",
      "page": 6
    },
    {
      "caption": "Figure 3: (a), we illustrate the results of dialogues",
      "page": 6
    },
    {
      "caption": "Figure 3: (b), we present an abridged dialogue that has 5",
      "page": 6
    },
    {
      "caption": "Figure 3: (c), we present the confusion matrix of conversa-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Overall performance on the three datasets. We choose on average. However, for the graph-based methods Dia-",
      "data": [
        {
          "-\n62.95\n63.37": "58.10\n63.02\n64.55",
          "-\n38.28\n38.89": "-\n38.10\n39.73"
        },
        {
          "-\n62.95\n63.37": "63.65\n63.39\n65.46",
          "-\n38.28\n38.89": "39.02\n38.84\n40.24"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Busso"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "2",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Ghosal"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "3",
      "title": "COSMIC: commonsense knowledge for emotion identification in conversations",
      "authors": [
        "Ghosal"
      ],
      "year": "2020",
      "venue": "EMNLP, volume EMNLP 2020 of Findings of ACL"
    },
    {
      "citation_id": "4",
      "title": "ICON: interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Hazarika"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "5",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Hu"
      ],
      "year": "2021",
      "venue": "ACL/IJCNLP"
    },
    {
      "citation_id": "6",
      "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "Jiao"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "7",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "Kingma",
        "P Ba ; Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "ICLR"
    },
    {
      "citation_id": "8",
      "title": "Emotional inertia and psychological maladjustment",
      "authors": [
        "Kuppens"
      ],
      "year": "2010",
      "venue": "Psychological science"
    },
    {
      "citation_id": "9",
      "title": "Towards discriminative representation learning for speech emotion recognition",
      "authors": [
        "Li"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "10",
      "title": "Hitrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "Li"
      ],
      "year": "2020",
      "venue": "COLING"
    },
    {
      "citation_id": "11",
      "title": "Dialoguernn: An attentive RNN for emotion detection in conversations",
      "authors": [
        "Liu"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "12",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Poria"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "13",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Poria"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Poria"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "15",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Shen"
      ],
      "year": "2021",
      "venue": "ACL/IJCNLP"
    },
    {
      "citation_id": "16",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Wang"
      ],
      "year": "2020",
      "venue": "SIGdial"
    },
    {
      "citation_id": "17",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Zahiri",
        "M Choi ; Sayyed",
        "Jinho Zahiri",
        "; Choi",
        "Zhang"
      ],
      "year": "2018",
      "venue": "AAAI, volume WS-18 of AAAI Workshops"
    },
    {
      "citation_id": "18",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "Zhou"
      ],
      "year": "2018",
      "venue": "AAAI"
    }
  ]
}