{
  "paper_id": "2306.09361v3",
  "title": "Mfsn: Multi-Perspective Fusion Search Network For Pre-Training Knowledge In Speech Emotion Recognition",
  "published": "2023-06-12T16:40:07Z",
  "authors": [
    "Haiyang Sun",
    "Fulin Zhang",
    "Yingying Gao",
    "Zheng Lian",
    "Shilei Zhang",
    "Junlan Feng"
  ],
  "keywords": [
    "speech emotion recognition",
    "multiple perspectives",
    "neural architecture search *Equal contribution; **Corresponding authors Choice Cell Continuous-Based Speech-related Emotional Content Extraction Quantization-Based Textual-related Emotional Content Extraction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) is an important research topic in human-computer interaction. Many recent works focus on directly extracting emotional cues through pre-trained knowledge, frequently overlooking considerations of appropriateness and comprehensiveness. Therefore, we propose a novel framework for pre-training knowledge in SER, called Multiperspective Fusion Search Network (MFSN). Considering comprehensiveness, we partition speech knowledge into Textualrelated Emotional Content (TEC) and Speech-related Emotional Content (SEC), capturing cues from both semantic and acoustic perspectives, and we design a new architecture search space to fully leverage them. Considering appropriateness, we verify the efficacy of different modeling approaches in capturing SEC and fills the gap in current research. Experimental results on multiple datasets demonstrate the superiority of MFSN.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Due to its significant contribution to human-computer interaction, Speech Emotion Recognition (SER) has received increasing attention. Researchers have been working towards endowing models with the ability to perceive and recognize emotions akin to humans. While some studies have incorporated various prior knowledge to guide the modeling process, the emergence of highly performant pre-trained models has led many recent works to directly leverage pre-trained knowledge for favorable outcomes. However, due to differences in training objectives, pre-training modeling methods and knowledge that excel in other downstream tasks may not perform well in SER. In this paper, we undertake a consideration and analysis from both the perspectives of appropriateness and comprehensiveness, proposing a pre-training knowledge utilization framework tailored for SER.\n\nFrom the perspective of comprehensiveness, both text and speech play pivotal roles in emotion recognition  [1, 2, 3] . However, obtaining accurate transcriptions in real-world scenarios poses challenges, and directly utilizing semantic information in speech does not enable its comprehensive exploration. Consequently, we categorize emotional cues in speech into two types: Textual-related Emotional Content (TEC), which can be extracted using Automated Speech Recognition (ASR) model to approximate text, and Speech-related Emotional Content (SEC), which can be extracted using pre-trained model to represent acoustic feature.  Directly leveraging both forms of information proves impractical, as transcriptions often contains emotionally charged words that may deviate from the true emotional context, potentially causing misinterpretations. For instance, someone might emphatically say, \"I'm really happy right now\" when they are actually angry. In such instances, individuals recalibrate their perception of emotional words based on acoustic information such as intonation or tone, or others. We follow this principle and leverage speech feature to refine the model's understanding of TEC. In light of the varying significance of multi-level speech information  [4] , the thoughtful selection of the optimal level, along with the fusion operation with TEC, becomes crucial. While exhaustive selection may yield insights, it is a timeconsuming and resource-intensive process. Hence, we propose the use of Neural Architecture Search (NAS) for the efficient accomplishment of the aforementioned tasks. Specifically, we design a noval search space and employ a differentiable search algorithm  [5]  for automated strategy design.\n\nFrom the perspective of appropriateness, many contemporary approaches incorporate prior knowledge to tailor modeling methods for the target task. For instance, many existing pretrained ASR models, grounded in inductive bias towards pronunciation units, introduce quantization operations to enhance the model's perception of text  [6, 7, 8] . Following two stages of pre-training and fine-tuning, these models can construct a constrained set of discrete units. However, there is currently no consensus on the modeling methods for SEC, often relying on pre-training speech models without due consideration  [9, 10] . Given that human emotions are often dimensional and continuous  [11, 12] , we follow this principle and systematically compare the performance of quantitative modeling pre-trained knowledge  [7]  with continuous modeling pre-trained knowledge  [13]  in SER. This not only offers insights into the modeling methods for SEC but also fills current research gaps.\n\nIn conclusion, considering both comprehensiveness and appropriateness, we propose a novel framework for pretraining knowledge in SER, called Multi-perspective Fusion Search Network (MFSN). Illustrated in Figure  1 , MFSN em-arXiv:2306.09361v3 [eess.AS] 26 Jun 2024",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multiple-Perspective Fusion Search Network",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Tec & Sec",
      "text": "In order to comprehensively extract emotional cues from speech, we partition the emotion-related content in speech into two types: Textual-related Emotional Content (TEC) and Speech-related Emotional Content (SEC). As illustrated in Figure  1 , utilizing an ASR pre-trained model based on quantization modeling allows us to obtain TEC, referred to as X t . Similarly, employing a pre-trained model based on continuous modeling with k encoding layers enables us to acquire different levels of speech features, denoted as [X s 1 , ..., X s m , ..., X s k ], where m = k 2 , X s k is SEC. As the intermediate information generated during speech understanding carries different information  [23] , we regard X s 1 , X s m , and X s k as three levels of information. • Due to the constraints of training objective, X s k contains more target-related information.\n\n• To extract X s k from speech, the intermediate layer need to capture deeper information to fully understand content, denoted as X s m . • In contrast, the output of the earlier layer, X s 1 , is closer to the raw speech representation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Search Space",
      "text": "As illustrated in Figure  2 , to efficiently accomplish the selection of multi-level features and fusion operations, we design a novel search space. Firstly, to endow the model with the capability to search different levels of speech features while avoiding potential conflicts among them, during each mini-batch training, Choice Cell randomly samples one level of SEC, denoted as X s i , where i ∈ [1, m, k], for calculation. Secondly, for effective adjustment of TEC and to ensure that X s 1 and X s m do not contain target-related information, we detach X s i before feeding it into Fusion Cells along with X t . Finally, following the previous fusion work, we offer a total of eight operations in the operation pool O of one Fusion Cell, and each operation takes two tensor inputs and yields a output:\n\nAttention, ConcatFC, and ISM operations have corresponding reverse versions. For example, Attentionr(X s i , X t ) = Attention(X t , X s i ). As the number of Fusion Cells, denoted as N f , increases, the quantity of structures in the search space will exponentially grow to 3 * 8 Nc .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Search Algorithm",
      "text": "In the Fusion Cell, we employ a differentiable search algorithm  [5]  to search the fusion operations:\n\nwhere α ∈ R |O| is a trainable weight that measures the importance of different operations, and |O| denotes the number of operations in O. With the assistance of NAS, it is possible to avoid exhaustive training of all 3 * 8 Nc structures and find the optimal adjustment strategy through a single training process. Both X s k and X t f are subsequently used for emotional analysis.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Different Modeling Methods",
      "text": "To choose the most appropriate knowledge for extracting SEC, we conduct a comprehensive analysis of various pre-training modeling methods. The current modeling methods can be broadly categorized into two types: quantization-based reconstruction  [6, 7, 8]  and continuous frame-based reconstruction  [24, 13] . Both approaches aim to understand the content of speech by reconstructing the masked portions. The key difference between these approaches lies in their representation space of speech frames. Formally, we represent the speech as    The objective of the model is to minimize the discrepancy between X[Z] and Y . On the other hand, continuous frame reconstruction method directly use the original frames at the masked positions as the labels, i.e., Y = X[Z].",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [26]  dataset is a widely adopted benchmark for evaluating emotion recognition models. Following prior research, we focus on four emotions: angry, sad, happy, and neutral. To ensure a robust evaluation of our method, we adopted the 5-fold leave-onesession-out and 10-fold leave-one-speaker-out validation strategies to generate results. Surrey Audio-Visual Expressed Emotion (SAVEE)  [27]  dataset consists of recordings from 4 male actors in 7 different emotions. Following prior research, we adopted the 10-fold cross-validation strategies to generate results.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup & Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Main Setup",
      "text": "In this paper, we adopt Co-attention  [9]  as our Emotion Analysis module. Thus, in data preprocessing, in alignment with it, we segmented each speech into several 3-second-long segments and extracted the corresponding spectrogram and MFCC features for these segments. The final prediction for each speech was determined by aggregating the predictions from all its segmented parts.\n\nIn MFSN, we utilize pre-trained knowledge based on continuous modeling from Data2vec  [13]  to extract SEC from speech. For TEC, we retain prior knowledge of pronunciation units and capture it using fine-tuned Wav2vec2  [7] . Addition- ally, we employed an SGD optimizer with a learning rate of 5 to optimize the operation weight α, as described in Section 2.3. For the convenience of analysis, we employ a single Fusion Cell. When dealing with MFSN dual-stream inputs, we double the dimension of the encoding hidden features for both spectrograms and MFCCs. These features are evenly distributed to each stream for co-attention calculations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparison With Existing Works",
      "text": "Following prior works, we conducted performance tests of MFSN on both the IEMOCAP and SAVEE datasets. As depicted in Table  1 , in our experiments on the IEMOCAP dataset, MFSN attains the highest Unweighted Accuracy (UA) score in the Leave-one-session cross-validation strategy, demonstrating competitive Weighted Accuracy (WA) scores even in the presence of class imbalance. Furthermore, within the Leave-onespeaker cross-validation strategy, MFSN consistently achieves the top UA and WA scores. When applied to the broader range of categories in the SAVEE dataset, MFSN exhibits a significant advantage, surpassing the sota in the 10-fold cross-validation. However, as shown in Table  2 , there is still a gap between TEC and BERT. Although MFSN surpass the performance of ASR+BERT, it still lags behind the SAWC  [25] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparison Between Different Modeling Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Unified Model Configuration",
      "text": "As shown in Figure  3 , to mitigate the impact caused by variations in engineering implementations, we employ a unified framework for different modeling methods. The specific parameters of the network are shown in Table  3 . In the quantization modeling method, we imitate Wav2vec2  [7]  and assign B codebooks to all frame features, each containing W codewords, resulting in a total of N = W B clustering centers. The label vector Y is composed of the clustering centers corresponding to X[Z] and the sampled negative examples. In the continuous modeling method, we imitate Data2vec  [13]  and use the mean value of the outputs of the last L encoder layers as the Y . They were trained separately using BCE loss and MSE loss for reconstruction, with a data mask rate set at 65%. Wav2vec2 ✓ 74.0 / 71.9",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "To fully compare the capabilities of various modeling methods in capturing SEC, we not only test the models' capability in discrete emotion recognition but also in dimensional emotion analysis. In Table  4 , we report the MSE metrics of Valence (V), Activation (A), Dominance (D), and report UA, WA.\n\nAs demonstrated by the performance in V, A, and D, in dimensional emotion analysis, knowledge acquired through quantization modeling exhibits notable limitations. The performance significantly improves as the number of quantization units, N , increases. Only when N is large enough can this method approach the performance of continuous modeling. In the continuous modeling, the variation of the encoding layers L as reconstructed labels does not noticeably affect performance.\n\nFor discrete emotion recognition, in the four-class IEMO-CAP task, the quantization modeling shows comparable results to the continuous modeling. However, in more complex emotional categorizations, such as the seven-class SAVEE task, quantization modeling leads to a noticeable performance decline. Similarly, inadequate N numbers restrict model performance, while various settings of continuous modeling exhibit relatively consistent performance.\n\nThe above experimental results demonstrate that, whether in discrete emotion recognition or dimensional emotion analysis, the pre-trained knowledge derived from continuous modeling proves to be a superior choice for extracting SEC.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparison With Different Configurations",
      "text": "We further analyze the importance of each module in MFSN by evaluating various model configurations using the Leave-onesession strategy on IEMOCAP. As shown in Table  5 , consistent with the findings in Section 5.3.2, the pre-trained knowledge based on continuous modeling in Data2vec better captures SEC. However, simply incorporating TEC without adjustment, resulting in no performance improvement and even a slight decrease in the WA metric. Through the adjustment strategy search by MFSN, the model leverages SEC to mitigate bias in TEC, leading to a further improvement in model performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Visualization Of Search Results",
      "text": "As shown in Figure  4 , in various data partition scenarios, the Choice Cell and Fusion Cell automatically select optimal speech features and fusion operations to adjust TEC. To further demonstrate the performance of MFSN on imbalanced datasets, we present the performance confusion matrix of MFSN compared to the Co-attention method using only Wav2vec2 as the TEC Encoder, as shown in Figure  4 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall framework of MFSN.",
      "page": 1
    },
    {
      "caption": "Figure 1: , MFSN em-",
      "page": 1
    },
    {
      "caption": "Figure 2: The search space we designed is divided into two",
      "page": 2
    },
    {
      "caption": "Figure 2: , to efficiently accomplish the selection",
      "page": 2
    },
    {
      "caption": "Figure 3: Unified training framework for different modeling.",
      "page": 3
    },
    {
      "caption": "Figure 3: , to mitigate the impact caused by vari-",
      "page": 3
    },
    {
      "caption": "Figure 4: The visualization of adjustment strategy search results",
      "page": 4
    },
    {
      "caption": "Figure 5: Visualization of confusion matrices.",
      "page": 4
    },
    {
      "caption": "Figure 4: , in various data partition scenarios,",
      "page": 4
    },
    {
      "caption": "Figure 4: 6. Conclusions",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Continuous-Based": "\"\n\"\nSpeech-related Emotional"
        },
        {
          "Continuous-Based": "!!\n!#\n!$"
        },
        {
          "Continuous-Based": "Content Extraction"
        },
        {
          "Continuous-Based": "\"\nChoice"
        },
        {
          "Continuous-Based": "!&"
        },
        {
          "Continuous-Based": "Cell"
        },
        {
          "Continuous-Based": "Emotion"
        },
        {
          "Continuous-Based": "Analysis"
        },
        {
          "Continuous-Based": "Fusion"
        },
        {
          "Continuous-Based": "Cells"
        },
        {
          "Continuous-Based": "Quantization-Based"
        },
        {
          "Continuous-Based": "!%\n%"
        },
        {
          "Continuous-Based": "Textual-related Emotional \n!'"
        },
        {
          "Continuous-Based": "Content Extraction"
        },
        {
          "Continuous-Based": "Data transfer with gradients \nData transfer without gradients"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "Figure 1: The overall framework of MFSN."
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "Directly leveraging both forms of\ninformation proves im-"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "practical, as transcriptions often contains emotionally charged"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "words that may deviate from the true emotional context, poten-"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "tially causing misinterpretations. For instance, someone might"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "emphatically say, ”I’m really happy right now” when they are"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "actually angry.\nIn such instances,\nindividuals recalibrate their"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "perception of emotional words based on acoustic information"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "such as intonation or tone, or others. We follow this principle"
        },
        {
          "Continuous-Based": "and leverage speech feature to refine the model’s understand-"
        },
        {
          "Continuous-Based": "ing of TEC. In light of\nthe varying significance of multi-level"
        },
        {
          "Continuous-Based": "speech information [4],\nthe thoughtful selection of the optimal"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "level, along with the fusion operation with TEC, becomes cru-"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "cial. While exhaustive selection may yield insights, it is a time-"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "consuming and resource-intensive process. Hence, we propose"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "the use of Neural Architecture Search (NAS)\nfor\nthe efficient"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "accomplishment of the aforementioned tasks. Specifically, we"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "design a noval search space and employ a differentiable search"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "algorithm [5] for automated strategy design."
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "From the perspective of appropriateness, many contempo-"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "rary approaches incorporate prior knowledge to tailor modeling"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "methods for\nthe target\ntask.\nFor\ninstance, many existing pre-"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "trained ASR models, grounded in inductive bias towards pro-"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "nunciation units,\nintroduce quantization operations to enhance"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "the model’s perception of text [6, 7, 8]. Following two stages"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "of pre-training and fine-tuning,\nthese models can construct a"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "constrained set of discrete units. However,\nthere is currently"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "no consensus on the modeling methods\nfor SEC, often rely-"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "ing on pre-training speech models without due consideration"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "[9, 10]. Given that human emotions are often dimensional and"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "continuous [11, 12], we follow this principle and systematically"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "compare the performance of quantitative modeling pre-trained"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "knowledge [7] with continuous modeling pre-trained knowl-"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "edge [13]\nin SER. This not only offers insights into the mod-"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "eling methods for SEC but also fills current research gaps."
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "In\nconclusion,\nconsidering\nboth\ncomprehensiveness\nand"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "appropriateness,\nwe\npropose\na\nnovel\nframework\nfor\npre-"
        },
        {
          "Continuous-Based": ""
        },
        {
          "Continuous-Based": "training knowledge\nin SER,\ncalled Multi-perspective Fusion"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Choice Cell": "#"
        },
        {
          "Choice Cell": "!\""
        },
        {
          "Choice Cell": "Fusion Cell\nFusion Cell"
        },
        {
          "Choice Cell": "#"
        },
        {
          "Choice Cell": "!$"
        },
        {
          "Choice Cell": "#"
        },
        {
          "Choice Cell": "!%"
        },
        {
          "Choice Cell": "!\n…"
        },
        {
          "Choice Cell": "…\n…\n!&"
        },
        {
          "Choice Cell": "!!"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "Unselected path\nSelected path\nOperations\n,"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "The search space we designed is divided into two\nFigure 2:"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "parts: Choice Cell and Fusion Cell."
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "ploys continuous-based knowledge to capture SEC and utilizes"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "quantization-based knowledge to capture TEC. Furthermore,\nit"
        },
        {
          "Choice Cell": "possesses the capability to automatically leverage the optimal"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "level speech feature to refine the understanding of TEC. Finally,"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "both features are employed for emotion analysis. The contribu-"
        },
        {
          "Choice Cell": "tions of this paper can be summarized as follows:"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "• We propose a novel framework for using pre-trained knowl-"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "edge in SER, named MFSN. MFSN extract both TEC and"
        },
        {
          "Choice Cell": "SEC from appropriate perspectives.\nIt automatically designs"
        },
        {
          "Choice Cell": "adjustment strategies for TEC within a newly designed search"
        },
        {
          "Choice Cell": "space, comprehensively exploring emotional cues."
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "•\nFor the first time from a modeling perspective, we verify that"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "pre-training methods based on continuous modeling are more"
        },
        {
          "Choice Cell": "suitable for capturing SEC than quantization."
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "• Experimental\nresults on multiple SER benchmark datasets"
        },
        {
          "Choice Cell": "demonstrate that\nthe performance of MFSN achieves state-"
        },
        {
          "Choice Cell": "of-the-art levels."
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "2. Multiple-perspective Fusion Search"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "Network"
        },
        {
          "Choice Cell": "2.1. TEC & SEC"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "In\norder\nto\ncomprehensively\nextract\nemotional\ncues\nfrom"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "speech, we\npartition\nthe\nemotion-related\ncontent\nin\nspeech"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "into two types: Textual-related Emotional Content\n(TEC) and"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "Speech-related Emotional Content (SEC). As illustrated in Fig-"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "ure 1, utilizing an ASR pre-trained model based on quantization"
        },
        {
          "Choice Cell": "modeling allows us to obtain TEC,\nreferred to as X t.\nSimi-"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "larly, employing a pre-trained model based on continuous mod-"
        },
        {
          "Choice Cell": "eling with k encoding layers enables us to acquire different lev-"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "els of speech features, denoted as [X s\n1 , ..., X s\nm, ..., X s\nk], where"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "m = k"
        },
        {
          "Choice Cell": "k is SEC. As the intermediate information generated\n2 , X s"
        },
        {
          "Choice Cell": "during speech understanding carries different information [23],"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "we regard X s\n1 , X s\nm, and X s\nk as three levels of information."
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "• Due\nto the\nconstraints of\ntraining objective, X s\ncontains"
        },
        {
          "Choice Cell": "more target-related information."
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "• To extract X s\nfrom speech,\nthe intermediate layer need to"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "capture deeper\ninformation to fully understand content, de-"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "noted as X s\nm."
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "•\nIn contrast, the output of the earlier layer, X s\n1 , is closer to the"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "raw speech representation."
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "2.2.\nSearch Space"
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": ""
        },
        {
          "Choice Cell": "As illustrated in Figure 2, to efficiently accomplish the selection"
        },
        {
          "Choice Cell": "of multi-level features and fusion operations, we design a novel"
        },
        {
          "Choice Cell": "search space.\nFirstly,\nto endow the model with the capability"
        },
        {
          "Choice Cell": "to search different\nlevels of speech features while avoiding po-"
        },
        {
          "Choice Cell": "tential conflicts among them, during each mini-batch training,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Performance comparison with existing works. Evaluation measures are UA / WA for IEMOCAP and UAR / WAR for SAVEE.": ""
        },
        {
          "Table 1: Performance comparison with existing works. Evaluation measures are UA / WA for IEMOCAP and UAR / WAR for SAVEE.": "Model"
        },
        {
          "Table 1: Performance comparison with existing works. Evaluation measures are UA / WA for IEMOCAP and UAR / WAR for SAVEE.": ""
        },
        {
          "Table 1: Performance comparison with existing works. Evaluation measures are UA / WA for IEMOCAP and UAR / WAR for SAVEE.": "IS09-classification[14]"
        },
        {
          "Table 1: Performance comparison with existing works. Evaluation measures are UA / WA for IEMOCAP and UAR / WAR for SAVEE.": "CME[17]"
        },
        {
          "Table 1: Performance comparison with existing works. Evaluation measures are UA / WA for IEMOCAP and UAR / WAR for SAVEE.": "Co-attention[9]"
        },
        {
          "Table 1: Performance comparison with existing works. Evaluation measures are UA / WA for IEMOCAP and UAR / WAR for SAVEE.": "Prosody2Vec[21]"
        },
        {
          "Table 1: Performance comparison with existing works. Evaluation measures are UA / WA for IEMOCAP and UAR / WAR for SAVEE.": "MFSN"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Prosody2Vec[21]": "MFSN",
          "73.3 / 72.4": "74.0 / 71.9",
          "73.9 / 72.7\nTIM-Net[22]\n77.3 / 79.4": "74.6 / 73.2\nMFSN\n86.0 / 86.3"
        },
        {
          "Prosody2Vec[21]": "",
          "73.3 / 72.4": "",
          "73.9 / 72.7\nTIM-Net[22]\n77.3 / 79.4": "Table 3: Speech encoder architecture."
        },
        {
          "Prosody2Vec[21]": "",
          "73.3 / 72.4": "",
          "73.9 / 72.7\nTIM-Net[22]\n77.3 / 79.4": ""
        },
        {
          "Prosody2Vec[21]": "",
          "73.3 / 72.4": "Emotion",
          "73.9 / 72.7\nTIM-Net[22]\n77.3 / 79.4": "strides\n5, 2, 2, 2, 2, 2, 2"
        },
        {
          "Prosody2Vec[21]": "",
          "73.3 / 72.4": "",
          "73.9 / 72.7\nTIM-Net[22]\n77.3 / 79.4": ""
        },
        {
          "Prosody2Vec[21]": "",
          "73.3 / 72.4": "Analysis",
          "73.9 / 72.7\nTIM-Net[22]\n77.3 / 79.4": ""
        },
        {
          "Prosody2Vec[21]": "",
          "73.3 / 72.4": "",
          "73.9 / 72.7\nTIM-Net[22]\n77.3 / 79.4": "CNN Encoder\nkernel width\n10, 3, 3, 3, 3, 2, 2"
        },
        {
          "Prosody2Vec[21]": "",
          "73.3 / 72.4": "",
          "73.9 / 72.7\nTIM-Net[22]\n77.3 / 79.4": "channel\n512"
        },
        {
          "Prosody2Vec[21]": "",
          "73.3 / 72.4": "",
          "73.9 / 72.7\nTIM-Net[22]\n77.3 / 79.4": "layer\n4"
        },
        {
          "Prosody2Vec[21]": "",
          "73.3 / 72.4": "",
          "73.9 / 72.7\nTIM-Net[22]\n77.3 / 79.4": "embedding dim.\n512"
        },
        {
          "Prosody2Vec[21]": "",
          "73.3 / 72.4": "",
          "73.9 / 72.7\nTIM-Net[22]\n77.3 / 79.4": "Transformer"
        },
        {
          "Prosody2Vec[21]": "",
          "73.3 / 72.4": "",
          "73.9 / 72.7\nTIM-Net[22]\n77.3 / 79.4": "FFN dim.\n2048"
        },
        {
          "Prosody2Vec[21]": "",
          "73.3 / 72.4": "",
          "73.9 / 72.7\nTIM-Net[22]\n77.3 / 79.4": "attention heads\n8"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3: Unified training framework for different modeling.": "",
          "embedding dim.": "",
          "512": ""
        },
        {
          "Figure 3: Unified training framework for different modeling.": "",
          "embedding dim.": "FFN dim.",
          "512": "2048"
        },
        {
          "Figure 3: Unified training framework for different modeling.": "Table 2: Performance comparison with works using text feature.",
          "embedding dim.": "attention heads",
          "512": "8"
        },
        {
          "Figure 3: Unified training framework for different modeling.": "Speech\nText\nIEMOCAP",
          "embedding dim.": "",
          "512": ""
        },
        {
          "Figure 3: Unified training framework for different modeling.": "Model",
          "embedding dim.": "",
          "512": ""
        },
        {
          "Figure 3: Unified training framework for different modeling.": "Feature\nFeature\nLeave-one-session",
          "embedding dim.": "ally, we employed an SGD optimizer with a learning rate of",
          "512": ""
        },
        {
          "Figure 3: Unified training framework for different modeling.": "MFCC\n-\nBLSTM[25]\n61.1 / 64.3",
          "embedding dim.": "5 to optimize the operation weight α, as described in Section",
          "512": ""
        },
        {
          "Figure 3: Unified training framework for different modeling.": "-\nASR+BERT\nBLSTM[25]\n71.8 / 71.9",
          "embedding dim.": "2.3. For the convenience of analysis, we employ a single Fusion",
          "512": ""
        },
        {
          "Figure 3: Unified training framework for different modeling.": "MFCC\nASR+BERT\nSAWC[25]\n76.8 / 76.6",
          "embedding dim.": "Cell. When dealing with MFSN dual-stream inputs, we double",
          "512": ""
        },
        {
          "Figure 3: Unified training framework for different modeling.": "SEC\nTEC\nMSFN\n74.0 / 71.9",
          "embedding dim.": "the encoding hidden features for both spec-",
          "512": ""
        },
        {
          "Figure 3: Unified training framework for different modeling.": "",
          "embedding dim.": "trograms and MFCCs. These features are evenly distributed to",
          "512": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP": ""
        },
        {
          "IEMOCAP": ""
        },
        {
          "IEMOCAP": ""
        },
        {
          "IEMOCAP": "V"
        },
        {
          "IEMOCAP": ""
        },
        {
          "IEMOCAP": "1.07"
        },
        {
          "IEMOCAP": "1.06"
        },
        {
          "IEMOCAP": ""
        },
        {
          "IEMOCAP": "0.95"
        },
        {
          "IEMOCAP": "0.70"
        },
        {
          "IEMOCAP": "0.68"
        },
        {
          "IEMOCAP": ""
        },
        {
          "IEMOCAP": "0.68"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "three levels of features. Red color indicates the best path.": ""
        },
        {
          "three levels of features. Red color indicates the best path.": "Angry"
        },
        {
          "three levels of features. Red color indicates the best path.": ""
        },
        {
          "three levels of features. Red color indicates the best path.": "Sad"
        },
        {
          "three levels of features. Red color indicates the best path.": ""
        },
        {
          "three levels of features. Red color indicates the best path.": "Happy"
        },
        {
          "three levels of features. Red color indicates the best path.": ""
        },
        {
          "three levels of features. Red color indicates the best path.": "Neutral"
        },
        {
          "three levels of features. Red color indicates the best path.": ""
        },
        {
          "three levels of features. Red color indicates the best path.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "tion recognition with co-attention based multi-level acoustic infor-"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "mation,” in IEEE International Conference on Acoustics, Speech"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "and Signal Processing, ICASSP.\nIEEE, 2022, pp. 7367–7371."
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "[10] Y. Wang, A. Boumadane, and A. Heba, “A fine-tuned wav2vec"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "2.0/hubert benchmark for\nspeech emotion recognition,\nspeaker"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "verification and spoken language understanding,” arXiv preprint"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "arXiv:2111.02735, 2021."
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "[11] R. Plutchik, “Emotions: A general psychoevolutionary theory,”"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "Approaches to emotion, vol. 1984, no. 197-219, pp. 2–4, 1984."
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "[12] H. Li, J. Chen, L. Ma, H. Bo, C. Xu, and H. Li, “Dimensional"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "speech emotion recognition review,” Journal of Software, vol. 31,"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "no. 8, pp. 2465–2491, 2020."
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "[13] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu,\nJ. Gu, and M. Auli,"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "“Data2vec: A general framework for self-supervised learning in"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "speech, vision and language,” in International Conference on Ma-"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "chine Learning, ICML.\nPMLR, 2022, pp. 1298–1312."
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "[14]\nL. Tarantino, P. N. Garner, A. Lazaridis et al.,\n“Self-attention"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "for speech emotion recognition.” in Interspeech, 2019, pp. 2578–"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "2582."
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "[15]\nP. Li, Y. Song, I. McLoughlin, W. Guo, and L. Dai, “An attention"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "pooling based representation learning method for speech emotion"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "recognition,” in Interspeech, 2018, pp. 3087–3091."
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "[16] M. Farooq, F. Hussain, N. K. Baloch, F. R. Raja, H. Yu, and Y. B."
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "Zikria, “Impact of feature selection algorithm on speech emotion"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "recognition using deep convolutional neural network,” Sensors,"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "vol. 20, no. 21, p. 6008, 2020."
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "[17] H. Li, W. Ding, Z. Wu, and Z. Liu, “Learning fine-grained cross"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "modality excitement\nfor\nspeech emotion recognition,” in Inter-"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "speech, 2021, pp. 3375–3379."
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "[18] Q. Cao, M. Hou, B. Chen, Z. Zhang, and G. Lu, “Hierarchical"
        },
        {
          "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-": "network based on the fusion of static and dynamic features for"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "rate speech emotion recognition system using twine shuffle pat-"
        },
        {
          "7. References": "[1]\nE. Kim and J. W. Shin, “Dnn-based emotion recognition based on",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "tern and iterative neighborhood component analysis techniques,”"
        },
        {
          "7. References": "bottleneck acoustic features and lexical features,” in IEEE Inter-",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "Knowledge-Based Systems, vol. 211, p. 106547, 2021."
        },
        {
          "7. References": "national Conference on Acoustics, Speech and Signal Processing,",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "ICASSP.\nIEEE, 2019, pp. 6720–6724.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "[20] X. Wen, J. Ye, Y. Luo, Y. Xu, X. Wang, C. Wu, and K. Liu, “Ctl-"
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "mtnet: A novel capsnet and transfer learning-based mixed task net"
        },
        {
          "7. References": "[2] W. Wu, C. Zhang, and P. C. Woodland, “Emotion recognition by",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "for single-corpus and cross-corpus speech emotion recognition,”"
        },
        {
          "7. References": "fusing time synchronous and time asynchronous representations,”",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "in Proceedings of the Thirty-First International Joint Conference"
        },
        {
          "7. References": "in IEEE International Conference on Acoustics, Speech and Sig-",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July"
        },
        {
          "7. References": "nal Processing, ICASSP.\nIEEE, 2021, pp. 6269–6273.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "2022, 2022, pp. 2305–2311."
        },
        {
          "7. References": "[3]\nZ. Lian, B. Liu,\nand J. Tao,\n“Smin:\nSemi-supervised multi-",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "[21]\nL. Qu, T. Li, C. Weber, T. Pekarek-Rosin, F. Ren, and S. Wermter,"
        },
        {
          "7. References": "modal\ninteraction network for conversational emotion recogni-",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "“Disentangling prosody representations with unsupervised speech"
        },
        {
          "7. References": "tion,” IEEE Transactions on Affective Computing, 2022.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "reconstruction,” arXiv preprint arXiv:2212.06972, 2022."
        },
        {
          "7. References": "[4]\nL. Pepino, P. Riera,\nand L. Ferrer,\n“Emotion recognition from",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "[22]\nJ. Ye, X.-C. Wen, Y. Wei, Y. Xu, K. Liu, and H. Shan, “Tem-"
        },
        {
          "7. References": "speech using wav2vec 2.0 embeddings,” in Interspeech, 2021, pp.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "poral modeling matters: A novel\ntemporal emotional modeling"
        },
        {
          "7. References": "3400–3404.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "approach for speech emotion recognition,” in IEEE International"
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "Conference on Acoustics, Speech and Signal Processing, ICASSP."
        },
        {
          "7. References": "[5] H. Liu, K. Simonyan, and Y. Yang, “DARTS: differentiable archi-",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "IEEE, 2023, pp. 1–5."
        },
        {
          "7. References": "tecture search,” in International Conference on Learning Repre-",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "sentations, ICLR, 2019.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "[23]\nL. Pepino, P. Riera,\nand L. Ferrer,\n“Emotion recognition from"
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "speech using wav2vec 2.0 embeddings,” in Interspeech, 2021, pp."
        },
        {
          "7. References": "[6] A. Baevski,\nS. Schneider,\nand M. Auli,\n“vq-wav2vec:\nSelf-",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "3400–3404."
        },
        {
          "7. References": "supervised learning of discrete speech representations,” in Inter-",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "national Conference on Learning Representations, ICLR, 2020.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "[24] A. T. Liu, S.-W. Li, and H.-y. Lee, “Tera: Self-supervised learn-"
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "ing of transformer encoder representation for speech,” IEEE/ACM"
        },
        {
          "7. References": "[7] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "Transactions\non\nAudio,\nSpeech,\nand\nLanguage\nProcessing,"
        },
        {
          "7. References": "2.0: A framework for self-supervised learning of speech repre-",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "vol. 29, pp. 2351–2366, 2021."
        },
        {
          "7. References": "sentations,” Advances in neural\ninformation processing systems,",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "vol. 33, pp. 12 449–12 460, 2020.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "[25]\nJ. Santoso, T. Yamada, K. Ishizuka, T. Hashimoto, and S. Makino,"
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "“Speech emotion recognition based on self-attention weight cor-"
        },
        {
          "7. References": "[8] W. Hsu, B. Bolte, Y. H. Tsai, K. Lakhotia, R. Salakhutdinov,",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "rection for acoustic and text features,” IEEE Access, vol. 10, pp."
        },
        {
          "7. References": "and A. Mohamed, “Hubert: Self-supervised speech representation",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "115 732–115 743, 2022."
        },
        {
          "7. References": "learning by masked prediction of hidden units,” IEEE/ACM Trans-",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "actions on Audio, Speech, and Language Processing, vol. 29, pp.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "[26] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,"
        },
        {
          "7. References": "3451–3460, 2021.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:"
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "Interactive emotional dyadic motion capture database,” Language"
        },
        {
          "7. References": "[9] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emo-",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "resources and evaluation, vol. 42, pp. 335–359, 2008."
        },
        {
          "7. References": "tion recognition with co-attention based multi-level acoustic infor-",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "mation,” in IEEE International Conference on Acoustics, Speech",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "[27]\nP. Jackson and S. Haq, “Surrey audio-visual expressed emotion"
        },
        {
          "7. References": "and Signal Processing, ICASSP.\nIEEE, 2022, pp. 7367–7371.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": "(savee) database,” University of Surrey: Guildford, UK, 2014."
        },
        {
          "7. References": "[10] Y. Wang, A. Boumadane, and A. Heba, “A fine-tuned wav2vec",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "2.0/hubert benchmark for\nspeech emotion recognition,\nspeaker",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "verification and spoken language understanding,” arXiv preprint",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "arXiv:2111.02735, 2021.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "[11] R. Plutchik, “Emotions: A general psychoevolutionary theory,”",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "Approaches to emotion, vol. 1984, no. 197-219, pp. 2–4, 1984.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "[12] H. Li, J. Chen, L. Ma, H. Bo, C. Xu, and H. Li, “Dimensional",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "speech emotion recognition review,” Journal of Software, vol. 31,",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "no. 8, pp. 2465–2491, 2020.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "[13] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu,\nJ. Gu, and M. Auli,",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "“Data2vec: A general framework for self-supervised learning in",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "speech, vision and language,” in International Conference on Ma-",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "chine Learning, ICML.\nPMLR, 2022, pp. 1298–1312.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "[14]\nL. Tarantino, P. N. Garner, A. Lazaridis et al.,\n“Self-attention",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "for speech emotion recognition.” in Interspeech, 2019, pp. 2578–",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "2582.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "[15]\nP. Li, Y. Song, I. McLoughlin, W. Guo, and L. Dai, “An attention",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "pooling based representation learning method for speech emotion",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "recognition,” in Interspeech, 2018, pp. 3087–3091.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "[16] M. Farooq, F. Hussain, N. K. Baloch, F. R. Raja, H. Yu, and Y. B.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "Zikria, “Impact of feature selection algorithm on speech emotion",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "recognition using deep convolutional neural network,” Sensors,",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "vol. 20, no. 21, p. 6008, 2020.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "[17] H. Li, W. Ding, Z. Wu, and Z. Liu, “Learning fine-grained cross",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "modality excitement\nfor\nspeech emotion recognition,” in Inter-",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "speech, 2021, pp. 3375–3379.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "[18] Q. Cao, M. Hou, B. Chen, Z. Zhang, and G. Lu, “Hierarchical",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "network based on the fusion of static and dynamic features for",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "speech emotion recognition,” in IEEE International Conference",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "on Acoustics, Speech and Signal Processing,\nICASSP.\nIEEE,",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        },
        {
          "7. References": "2021, pp. 6334–6338.",
          "[19]\nT. Tuncer,\nS. Dogan,\nand U. R. Acharya,\n“Automated\naccu-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Dnn-based emotion recognition based on bottleneck acoustic features and lexical features",
      "authors": [
        "E Kim",
        "J Shin"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Smin: Semi-supervised multimodal interaction network for conversational emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings"
    },
    {
      "citation_id": "6",
      "title": "DARTS: differentiable architecture search",
      "authors": [
        "H Liu",
        "K Simonyan",
        "Y Yang"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "7",
      "title": "vq-wav2vec: Selfsupervised learning of discrete speech representations",
      "authors": [
        "A Baevski",
        "S Schneider",
        "M Auli"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations, ICLR"
    },
    {
      "citation_id": "8",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "9",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W Hsu",
        "B Bolte",
        "Y Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "12",
      "title": "Emotions: A general psychoevolutionary theory",
      "authors": [
        "R Plutchik"
      ],
      "year": "1984",
      "venue": "Emotions: A general psychoevolutionary theory"
    },
    {
      "citation_id": "13",
      "title": "Dimensional speech emotion recognition review",
      "authors": [
        "H Li",
        "J Chen",
        "L Ma",
        "H Bo",
        "C Xu",
        "H Li"
      ],
      "year": "2020",
      "venue": "Journal of Software"
    },
    {
      "citation_id": "14",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "A Baevski",
        "W.-N Hsu",
        "Q Xu",
        "A Babu",
        "J Gu",
        "M Auli"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning, ICML. PMLR"
    },
    {
      "citation_id": "15",
      "title": "Self-attention for speech emotion recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "16",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "P Li",
        "Y Song",
        "I Mcloughlin",
        "W Guo",
        "L Dai"
      ],
      "year": "2018",
      "venue": "An attention pooling based representation learning method for speech emotion recognition"
    },
    {
      "citation_id": "17",
      "title": "Impact of feature selection algorithm on speech emotion recognition using deep convolutional neural network",
      "authors": [
        "M Farooq",
        "F Hussain",
        "N Baloch",
        "F Raja",
        "H Yu",
        "Y Zikria"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "18",
      "title": "Learning fine-grained cross modality excitement for speech emotion recognition",
      "authors": [
        "H Li",
        "W Ding",
        "Z Wu",
        "Z Liu"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Hierarchical network based on the fusion of static and dynamic features for speech emotion recognition",
      "authors": [
        "Q Cao",
        "M Hou",
        "B Chen",
        "Z Zhang",
        "G Lu"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques",
      "authors": [
        "T Tuncer",
        "S Dogan",
        "U Acharya"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "21",
      "title": "Ctlmtnet: A novel capsnet and transfer learning-based mixed task net for single-corpus and cross-corpus speech emotion recognition",
      "authors": [
        "X Wen",
        "J Ye",
        "Y Luo",
        "Y Xu",
        "X Wang",
        "C Wu",
        "K Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "22",
      "title": "Disentangling prosody representations with unsupervised speech reconstruction",
      "authors": [
        "L Qu",
        "T Li",
        "C Weber",
        "T Pekarek-Rosin",
        "F Ren",
        "S Wermter"
      ],
      "year": "2022",
      "venue": "Disentangling prosody representations with unsupervised speech reconstruction",
      "arxiv": "arXiv:2212.06972"
    },
    {
      "citation_id": "23",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition",
      "authors": [
        "J Ye",
        "X.-C Wen",
        "Y Wei",
        "Y Xu",
        "K Liu",
        "H Shan"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings"
    },
    {
      "citation_id": "25",
      "title": "Tera: Self-supervised learning of transformer encoder representation for speech",
      "authors": [
        "A Liu",
        "S.-W Li",
        "H.-Y Lee"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition based on self-attention weight correction for acoustic and text features",
      "authors": [
        "J Santoso",
        "T Yamada",
        "K Ishizuka",
        "T Hashimoto",
        "S Makino"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "27",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "28",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    }
  ]
}