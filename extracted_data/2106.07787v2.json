{
  "paper_id": "2106.07787v2",
  "title": "Tracing Back Music Emotion Predictions To Sound Sources And Intuitive Perceptual Qualities",
  "published": "2021-06-14T22:49:19Z",
  "authors": [
    "Shreyan Chowdhury",
    "Verena Praher",
    "Gerhard Widmer"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Music emotion recognition is an important task in MIR (Music Information Retrieval) research. Owing to factors like the subjective nature of the task and the variation of emotional cues between musical genres, there are still significant challenges in developing reliable and generalizable models. One important step towards better models would be to understand what a model is actually learning from the data and how the prediction for a particular input is made. In previous work, we have shown how to derive explanations of model predictions in terms of spectrogram image segments that connect to the high-level emotion prediction via a layer of easily interpretable perceptual features. However, that scheme lacks intuitive musical comprehensibility at the spectrogram level. In the present work, we bridge this gap by merging audioLIMEa source-separation based explainer -with mid-level perceptual features, thus forming an intuitive connection chain between the input audio and the output emotion predictions. We demonstrate the usefulness of this method by applying it to debug a biased emotion prediction model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The quest for interpreting the inner workings of \"blackbox\" models and explaining their predictions has led to many recent advances in the area of explainable AI and is becoming an increasingly important staple of all AI subfields. Not only do model explanations help in enhancing trust in the model in applications where its predictions are critical decisions like creditworthiness or medical diagnosis, but they can also often reveal telling signs of algorithmic bias  [1, 2] . While algorithmic decisions in the field of MIR are not as life-critical as medical diagnosis, in today's era of music streaming and recommendations, they can have far-reaching effects on diverse audiences, creators, and artists alike. For instance, machine-learningbased music recommender systems have been shown to exhibit severe biases towards or against certain user groups  [3] . Interpretable explanations of these models or their predictions would be extremely helpful for identifying such biases.\n\nPrevious work on interpretability in MIR has dealt with tasks such as music tagging using self-attention  [4]  and transcription using invertible neural networks  [5] , and post-hoc explanations for music content analysis have been used to understand what a genre classifier  [6]  or a singing voice detector  [7] [8] [9] [10] [11]  have learnt. More recently, audioLIME has been proposed  [12, 13]  and has shown promise in explaining tagging models  [14]  as well as recommendation models  [15] .\n\nHowever, explanations of music emotion recognition systems have received relatively less attention notwithstanding the importance of this task in the areas of musical analysis and recommendation.  [16]  used models trained with different combinations of sound sources to deduce the importance of each source to emotion predictions. We previously proposed using an intermediate layer of mid-level perceptual features  [17]  to explain music emotion recognition models through a linear connection between the intermediate and final layers  [18] . We followed this up with a two-step explanation approach  [19]  to further explain the predictions in the mid-level layer using components from the input spectrogram. This two-level scheme used LIME (Local Interpretable Model-agnostic Explanations)  [13]  to construct explanations for the mid-level predictions in terms of specific patches of the input spectrogram (which were obtained via image segmentation). While this gave us the regions in the spectrogram contributing most to a particular prediction, these regions did not hold any musical meaning by themselves. As a result, it is difficult to comprehend these explanations in terms of meaningful or intuitive concepts.\n\nIn this work, we bridge this gap by merging the audioLIME method, which uses sound sources as explanatory features, with the approach of mid-level features, to obtain comprehensible explanations from the input audio as well as from the perceptual layer. It thus forms an intuitive connection of hierarchical explanations from lowlevel constituent sources of audio to the high-level emotion predictions through the intermediary mid-level layer, all of which have a musical interpretation. We believe that explainability is particularly important for developing better music emotion recognition algorithms since it is often difficult to identify misclassifications and biases in this task because of its inherent subjectivity and inter-annotator variability. As an example of real-life application of our method in understanding the potential cause of bias in an emotion model, we demonstrate how a model that has seen few examples of a genre during training results in a pattern of errors on the test set that contains examples of this genre. We trace this pattern of errors back to a particular mid-level feature and this feature to a particular source in the input. Doing so allows us to predict how the model would change when retrained with a balanced training set. We can then qualitatively verify that the retrained model has in fact changed in the way that we expected from our explanations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Two-Level Explanations",
      "text": "Our proposed two-level system will explain emotion predictions by first tracing them back to the most relevant midlevel features and in a second step explain the intermediate mid-level layer via audio sources. We will first describe each of the parts separately from the lowest level (audio sources) to the highest level (emotion predictions) and then put together all the parts in Section 2.3.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Explaining Via Audio Sources: Audiolime",
      "text": "In order to explain individual mid-level predictions we make use of audioLIME, a recently introduced approach based on LIME  [13]  for interpreting models in MIR  [12, 14] . LIME uses simplified inputs based on a set of human interpretable features (depending on the domain of the task -e.g., superpixels for images) to train a simpler explanation model ùëî in order to explain a more complex, potentially deep model ùëì . Previous approaches based on LIME have used time-, frequency-, or time-frequency segments  [7] , or segments computed by an image segmentation algorithm  [19] . audioLIME introduced a new type of interpretable features: sound sources estimated by a music source separation algorithm. In other words, the audioLIME explanation for a given prediction will be in the form of particular sound sources (and possibly specific temporal segments -which we do not use right now), telling us that it is some sonic aspects of these sources that seem to be influential. In our case, the source separator is the pretrained music source separator spleeter  [20] , thus the explanatory sound sources will be (what the source separator believes are) individual instruments.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Explaining Via Mid-Level Perceptual Features",
      "text": "Mid-level features are perceptual qualities or descriptors that emerge from low-level musical building blocks such as timbre, beat structure, harmony, etc. They are more subjective than the low-level features, thus difficult to model using hand-crafted feature extractors, but musically discernible enough to have many people reach a high agreement in annotations. Examples include qualities such as perceived melodiousness or rhythmic complexity  [17] . They, therefore, form a suitable choice for an intermediary to higher-level concepts like emotion.\n\nThis idea was first used in  [18]  to explain emotion predictions. We use a similar approach here, but we use the more recently introduced receptive-field regularized ResNets  [21, 22]  to model the emotions and the mid-level features. In addition, we learn the emotions and mid-level features from two separate datasets using multi-task learning, i.e., jointly learn to predict mid-level features and high-level emotions, allowing us to be flexible with our train and test domains.\n\nThe mid-level layer is the penultimate layer of the model, with a linear transformation between it and the final (emotion) layer, thus making the connection interpretable. It is learnt end-to-end from audio spectrograms by optimizing on the combined loss from the emotion and mid-level layers.\n\nEmotion predictions can be interpreted by looking at effects plots, which are visual representations of the contribution of each mid-level feature to the final emotion prediction, calculated as the product of the feature value and the weight joining it to the emotion. These indicate the relative influence of the individual features on the final prediction.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Putting It All Together: Intuitive Two-Level Explanations",
      "text": "For a given prediction, we then work backwards. First, we obtain the mid-level explanation of an emotion by computing the effects. The larger the effect of a mid-level feature, the larger is the contribution of that feature to the emotion prediction. Next, we compute the audioLIME explanations for the mid-level feature with the largest effect (we can in principle compute audioLIME explanations for all features to obtain a more diverse explanation, depending on the application). Given these two explanations, we can describe a prediction as being arrived at by the model due to the explanatory mid-level feature, which is in turn most influenced by the input component given by audioLIME.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setup",
      "text": "For our analysis with emotion explanations, we first need to train the \"explainable\" models, which have the penultimate mid-level layer connecting linearly to the final emotion outputs. This section describes the datasets and the training procedure for such models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "For our experiments we are using three different datasets:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mid-Level Perceptual Features Dataset",
      "text": "The Mid-level Perceptual Features Dataset introduced in  [17]  consists of 5000 song snippets with annotations between 1 and 10 for the mid-level descriptors melodiousness, articulation, rhythmic complexity, rhythmic stability, dissonance, tonal stability, and modality (called \"minorness\" here). We use this dataset to train the intermediate layer of our emotion model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Deam: Database For Emotional Analysis In Music",
      "text": "The DEAM dataset  [23]     1 : Emotion prediction performance with our \"explainable\" model trained and tested on difference datasets -P: PMEmo, D: DEAM. The dataset inside the parentheses is the test dataset. The top row is the baseline performance from  [16] . the static emotion annotations, which are continuous values between 0 and 10.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pmemo: Popular Music With Emotional Annotation",
      "text": "The PMEmo dataset  [24]  consists of 794 chorus clips from three different well-known music charts. The songs were annotated by 457 annotators with valence and arousal annotations for dynamic and static. In our experiments, we use static labels, which are continuous values between 0 and 1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Training",
      "text": "The mid-level and emotion model is trained end-to-end using audio spectrograms as inputs and optimizing on the combined loss from the mid-level and emotion layers. The batch size is 16 and contains 8 samples from the Midlevel dataset and 8 samples from either the DEAM or the PMEmo dataset. The loss function is the mean squared error. The learning rate is 10 -3 with cosine annealing, and we perform early stopping on a validation set as regularization. We use the Adam optimizer  [25] .\n\nThe inputs are log-filtered spectrograms (149 bands) of 40-second audio clips peak normalized and sampled at 22.05 kHz with a window size of 2048 samples and a hop length of 704 samples, resulting in 149√ó1252-sized tensors. If a clip is longer than 40 seconds, we take a random snippet, and if it is shorter, it is looped to 40 seconds.\n\nThe labels are scaled to the range [-1, 1] for all three datasets. Therefore, an RMSE of 0.26 would represent 13% error. We split the train and test sets such that they have mutually exclusive sets of artists. A summary of the emotion prediction performance can be found in Table  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Of Explanations",
      "text": "Essentially, there are three targets for empirical evaluation: the two individual components of our two-level explanation framework, and the final composite explanations produced by the model. Regarding the former, the higher level -explanations of emotion predictions in terms of mid-level perceptual features -has already been discussed at length in our previous paper  [18] . We showed how effects plots can give insight into the relative importance of various mid-level qualities. The lower level -using audioLIME to explain mid-level feature predictions via audio sources -is a new concept, and the experiments in the following section are intended to validate it. Empirical evidence for the usefulness of the complete, two-level explanation model, finally, will be presented in the form of a study, in Section 5 below, where we demonstrate how these explanations can help us debug a biased prediction model by gaining insight into what the sources of its problems are.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Explaining Mid-Level Features Via Sound Sources",
      "text": "Evaluating the quality of explanations is a hard task since there is no consensus on what makes a good explanation, with a variety of desired aims and properties proposed in literature  [26] [27] [28] ). We build our evaluation of audioLIME explanations for the mid-level layer on two metrics, (a) fidelity as proposed by Ribeiro et al. along with LIME  [13]  and, (b) complexity, a recently proposed metric for featurebased model explanations  [26] .\n\nFidelity measures how well the local model ùëî (the explainer) approximates the global model ùëì (the model up to the mid-level layer in our case)  [13]  and is computed using the coefficient of determination between the local and global model's predictions as in the original LIME implementation  1  .\n\nIn addition to high fidelity, low complexity is desired. The most complex explanation would be the one where all ùëë features get the same attribution (i.e., all weights ùëî(ùëì, ùë•) ùëñ of the linear explanation model ùëî are the same). The simplest explanation concentrates all attribution on one feature. To measure complexity, a probability distribution ùëÉ ùëî is defined:\n\nComplexity is then defined as the entropy of this distribution  [26] . We compare the complexity per dataset with a random baseline, which is obtained by creating \"random\" explanations with feature weights drawn from a uniform distribution.\n\nFor the analysis, we compute predictions and explanations for all test examples and calculate the above mentioned metrics. The results for one mid-level feature (we picked \"rhythmic stability\" as it is used later on as an example) are summarized in Figures  1  and 2 . We can see in Figure  1a  that the fidelity score (coefficient of determination) is relatively high across all combinations of models and test sets. The median score is 0.86 across all explanations (including all mid-level features), the 25%-quantile is at 0.78. This means that for 50% and 75% of the explanations more than 86%, and 78%, respectively, of the variation in the dependent variable (mid-level prediction) can be predicted using the independent variables (instrument sources).\n\nFigure  1b  shows the computed complexities, compared to a random baseline. Most explanations are far less complex than the random baseline.\n\nThe results shown in the previous figures suggested a relationship between the fidelity and complexity scores.   Therefore we visualized the two metrics for all explanations computed for \"rhythmic stability\" for a model trained on the combined data sets in Figure  2 . Although they seem related on a dataset level, the metrics do not look related when analyzed for each explanation separately, suggesting that indeed both are needed.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Model Debugging",
      "text": "A practical use case of our explanation scheme is demonstrated in this section. We use the two-level explanations to understand why an improperly trained model might be overestimating the valence predictions for one particular genre.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Setup",
      "text": "First, we use a pre-trained tagger  [29]  to predict genre tags for all the tracks in the three datasets mentioned in Section 3.1, since we do not have genre metadata for these datasets. This is only done in order to obtain an estimate of the genre-dependence of the emotion predictions later on.\n\nWe then train two explainable models -one on the DEAM dataset, and one on the combined DEAM and PMEmo dataset. The test set is a fixed but randomly chosen subset of the PMEmo dataset (with a mutually exclusive set of artists from the training set).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Overestimated Valence For Hiphop",
      "text": "When we take the model trained only on DEAM and use it to predict arousal and valence for the entire PMEmo dataset, we observe that the error in valence shows a pattern -overestimations of valence primarily occur in hiphop songs, as shown in Figure  3 .\n\nWe can reason about relatively poor performance for hiphop songs based on the discrepancy between the training and testing sets in terms of genre composition. In Figure  5 , we can see that PMEmo has a large percentage of hiphop songs whereas both DEAM and Mid-level datasets have a small percentage. Since our model has not seen enough hiphop songs during training, it is to be expected that it does not perform well when it encounters hiphop during test. However, a question that is pertinent next iswhat is it about hiphop songs that makes our model overestimate their valence?",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Explaining Valence Overestimations Using Mid-Level Features",
      "text": "To answer this question, we first seek to understand which of the mid-level qualities can be attributed most to high valence predictions. This is the first level of our explanation system. We find these attributions by computing the effects of each mid-level feature on the valence predictions. The effect of a feature is simply the value of that feature multiplied by the weight of the linear connection between it and the target node. In our case, the target is valence and there are seven mid-level features that affect it. We are only interested in relative contribution of each feature, and so we divide each effect by the sum of the absolute values of the effects of all features and take the average across all test songs tagged \"hiphop\". We observe that rhythmic stability has the maximum positive relative effect on the prediction of valence. Therefore, we select rhythmic stability for the next step of explanation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Explaining Rhythmic Stability Using Sources",
      "text": "Once we have selected a mid-level feature as having the most positive relative effect on the valence, we would like to understand what musical constituents in the input can be attributed to positive contribution to that feature. To do this, we take the help of audioLIME and generate source based explanations for rhythmic stability. The sources available in the current implementation of audioLIME 2  are vocals, drums, bass, piano, and other. We find that vocals are a major contributing source for the rhythmic stability predictions for the hiphop songs. For songs tagged as other genres, contributing sources are more distributed.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Re-Training The Model With Target Data",
      "text": "Bringing together our two types of explanations, we can reason that the high valence predictions for hiphop songs is due to overestimation of rhythmic stability, which, in this case, can be attributed to the vocals. While there is a lot of diversity in the style of rapping (the form of vocal delivery predominant in hiphop), it has been noted that rappers typically use stressed syllables and vocal onsets to match the vocals with the underlying rhythmic pulse  [30, 31] . These rhythmic characteristics of vocal delivery (that constitutes \"flow\", and may add metrical layers on top of the beat) contribute strongly to the rhythmic feel of a song. The positive or negative emotion of hiphop songs is mostly contained in the lyrics -the style of vocal performance does not necessarily express or correlate with this aspect of emotion. Therefore, it makes sense that a model which has seen few examples of hiphop during training should wrongly associate the prominent rhythmic vocals of hiphop to high rhythmic stability and in turn high valence. A model that has been trained with hiphop songs included, we expect, would place less importance on rhythmic stability for the prediction of valence, even if the vocals might still contribute significantly to rhythmic stability. Thus, we expect the relative effect of rhythmic stability for valence to decrease in such a model. This is exactly what we observe on a model trained with the combined PMEmo+DEAM dataset. The average relative effects are shown in Figure  4b  and we can see that the relative effect of rhythmic stability has decreased while those of minorness, melody, and tonal stability have increased. Thus, the model changed in a way that was in line with what we expected from the analysis of our two-level explanation method.\n\nLooking at mean overestimations (Figure  6 ) in valence for hiphop and other genres for models trained on DEAM and PMEmo+DEAM shows that valence overestimations of hiphop songs have decreased substantially, without negatively affecting the predictions on other genres 3  .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this paper, we proposed a method to explain music emotion models in an intuitive way using components from low-and mid-levels of the hierarchy of musical concepts by combining audioLIME, which uses input audio sources as explanatory components, with intermediate layer based explanations. We also demonstrated its potential as a tool for model debugging and explaining model behaviour.\n\nThis points us towards exploring this method further and getting more granular explanations as a way of improving the effectiveness of this system for MIR. An immediate next step that we are currently pursuing is to extend audioLIME to provide explanations in the form of temporal segments using semantic music segmentation, along with the sound sources. We are also looking at explaining emotion conveyed in classical piano performances, which pose particular challenges -including the non-availability of training data, where transfer learning of explanatory features becomes necessary  [32] .",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: a that the Ô¨Ådelity score (coefÔ¨Åcient of determina-",
      "page": 3
    },
    {
      "caption": "Figure 1: b shows the computed complexities, compared to",
      "page": 3
    },
    {
      "caption": "Figure 1: Figure 1a shows the computed Ô¨Ådelity (coefÔ¨Åcient of determination ùëÖ2 between the predictions by the global model ùëìand",
      "page": 4
    },
    {
      "caption": "Figure 1: b shows the complexity (entropy of a distribution over the feature",
      "page": 4
    },
    {
      "caption": "Figure 2: A more detailed view on the relationship between the",
      "page": 4
    },
    {
      "caption": "Figure 2: Although they seem",
      "page": 4
    },
    {
      "caption": "Figure 3: Fraction of hiphop songs in quantiles vs the mean",
      "page": 4
    },
    {
      "caption": "Figure 3: We can reason about relatively poor performance for",
      "page": 4
    },
    {
      "caption": "Figure 4: Relative effects of the mid-level features for valence prediction for two models trained on different datasets, but tested on the",
      "page": 5
    },
    {
      "caption": "Figure 5: Compositions of datasets as fraction of songs tagged",
      "page": 5
    },
    {
      "caption": "Figure 6: Mean valence overestimations for two models trained",
      "page": 6
    },
    {
      "caption": "Figure 4: b and we can see that",
      "page": 6
    },
    {
      "caption": "Figure 6: ) in valence",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Emotion prediction performance with our ‚Äúexplain-",
      "page": 3
    },
    {
      "caption": "Table 1: 4. EVALUATION OF EXPLANATIONS",
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Interpretations are Useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge",
      "authors": [
        "L Rieger",
        "C Singh",
        "W Murdoch",
        "B Yu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020"
    },
    {
      "citation_id": "3",
      "title": "Towards Explainable Artificial Intelligence",
      "authors": [
        "W Samek",
        "K M√ºller"
      ],
      "year": "2019",
      "venue": "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, ser. Lecture Notes in Computer Science"
    },
    {
      "citation_id": "4",
      "title": "All The Cool Kids, How Do They Fit In?: Popularity and Demographic Biases in Recommender Evaluation and Effectiveness",
      "authors": [
        "M Ekstrand",
        "M Tian",
        "I Azpiazu",
        "J Ekstrand",
        "O Anuyah",
        "D Mcneill",
        "M Pera"
      ],
      "year": "2018",
      "venue": "Conference on Fairness, Accountability and Transparency"
    },
    {
      "citation_id": "5",
      "title": "Toward Interpretable Music Tagging with Self-Attention",
      "authors": [
        "M Won",
        "S Chun",
        "X Serra"
      ],
      "year": "1906",
      "venue": "CoRR"
    },
    {
      "citation_id": "6",
      "title": "Towards Interpretable Polyphonic Transcription with Invertible Neural Networks",
      "authors": [
        "R Kelz",
        "G Widmer"
      ],
      "year": "2019",
      "venue": "Proceedings of the 20th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "7",
      "title": "Explaining Deep Convolutional Neural Networks on Music Classification",
      "authors": [
        "K Choi",
        "G Fazekas",
        "M Sandler"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "8",
      "title": "Local Interpretable Model-Agnostic Explanations for Music Content Analysis",
      "authors": [
        "S Mishra",
        "B Sturm",
        "S Dixon"
      ],
      "year": "2017",
      "venue": "Proceedings of the 18th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "9",
      "title": "Reliable Local Explanations for Machine Listening",
      "authors": [
        "S Mishra",
        "E Benetos",
        "B Sturm",
        "S Dixon"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "10",
      "title": "Understanding a Deep Machine Listening Model Through Feature Inversion",
      "authors": [
        "S Mishra",
        "B Sturm",
        "S Dixon"
      ],
      "year": "2018",
      "venue": "Proceedings of the 19th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "11",
      "title": "GAN-based Generation and Automatic Selection of Explanations for Neural Networks",
      "authors": [
        "S Mishra",
        "D Stoller",
        "E Benetos",
        "B Sturm",
        "S Dixon"
      ],
      "year": "2019",
      "venue": "CoRR"
    },
    {
      "citation_id": "12",
      "title": "Explaining Predictions of Deep Machine Listening Systems",
      "authors": [
        "S Mishra",
        "B Sturm",
        "S Dixon"
      ],
      "year": "2018",
      "venue": "26th European Signal Processing Conference"
    },
    {
      "citation_id": "13",
      "title": "audioLIME: Listenable Explanations Using Source Separation",
      "authors": [
        "V Haunschmid",
        "E Manilow",
        "G Widmer"
      ],
      "year": "2020",
      "venue": "13th International Workshop on Machine Learning and Music"
    },
    {
      "citation_id": "14",
      "title": "Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
      "authors": [
        "M Ribeiro",
        "S Singh",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "15",
      "title": "Towards Musically Meaningful Explanations Using Source Separation",
      "authors": [
        "V Haunschmid",
        "E Manilow",
        "G Widmer"
      ],
      "year": "2009",
      "venue": "CoRR"
    },
    {
      "citation_id": "16",
      "title": "LEMONS : Listenable Explanations for Music recOmmeNder Systems",
      "authors": [
        "A Melchiorre",
        "V Haunschmid",
        "M Schedl",
        "G Widmer"
      ],
      "year": "2021",
      "venue": "Advances in Information Retrieval -43nd European Conference on IR Research, ECIR 2021 (forthcoming)"
    },
    {
      "citation_id": "17",
      "title": "The Multiple Voices of Musical Emotions: Source Separation for Improving Music Emotion Recognition Models and Their Interpretability",
      "authors": [
        "J De Berardinis",
        "A Cangelosi",
        "E Coutinho"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21st International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "18",
      "title": "A Data-driven Approach to Mid-level Perceptual Musical Feature Modeling",
      "authors": [
        "A Aljanaki",
        "M Soleymani"
      ],
      "year": "2018",
      "venue": "Proceedings of the 19th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "19",
      "title": "Proceedings of the 20th International Society for Music Information Retrieval Conference",
      "authors": [
        "S Chowdhury",
        "A Vall",
        "V Haunschmid",
        "G Widmer"
      ],
      "year": "2019",
      "venue": "Proceedings of the 20th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "20",
      "title": "Twolevel Explanations in Music Emotion Recognition",
      "authors": [
        "V Haunschmid",
        "S Chowdhury",
        "G Widmer"
      ],
      "year": "2019",
      "venue": "Machine Learning for Music Discovery Workshop, ML4MD at ICML2019"
    },
    {
      "citation_id": "21",
      "title": "Spleeter: a Fast and Efficient Music Source Separation Tool with Pre-trained Models",
      "authors": [
        "R Hennequin",
        "A Khlif",
        "F Voituret",
        "M Moussallam"
      ],
      "year": "2020",
      "venue": "Journal of Open Source Software"
    },
    {
      "citation_id": "22",
      "title": "Emotion and Theme Recognition in Music with Frequency-Aware RF-Regularized CNNs",
      "authors": [
        "K Koutini",
        "S Chowdhury",
        "V Haunschmid",
        "H Eghbal-Zadeh",
        "G Widmer"
      ],
      "year": "2019",
      "venue": "MediaEval 2019 Workshop"
    },
    {
      "citation_id": "23",
      "title": "Receptive-Field Regularized CNNs for Music Classification and Tagging",
      "authors": [
        "K Koutini",
        "H Eghbal-Zadeh",
        "V Haunschmid",
        "P Primus",
        "S Chowdhury",
        "G Widmer"
      ],
      "year": "2020",
      "venue": "CoRR"
    },
    {
      "citation_id": "24",
      "title": "Developing a Benchmark for Emotional Analysis of Music",
      "authors": [
        "A Aljanaki",
        "Y.-H Yang",
        "M Soleymani"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "25",
      "title": "The PMEmo Dataset for Music Emotion Recognition",
      "authors": [
        "K Zhang",
        "H Zhang",
        "S Li",
        "C Yang",
        "L Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, ser. ICMR '18"
    },
    {
      "citation_id": "26",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "27",
      "title": "Evaluating and Aggregating Feature-based Model Explanations",
      "authors": [
        "U Bhatt",
        "A Weller",
        "J Moura"
      ],
      "year": "2020",
      "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJ-CAI 2020, C. Bessiere"
    },
    {
      "citation_id": "28",
      "title": "The Mythos of Model Interpretability",
      "authors": [
        "Z Lipton"
      ],
      "year": "2018",
      "venue": "ACM Queue"
    },
    {
      "citation_id": "29",
      "title": "Evaluating the Effectiveness of Explanations for Recommender Systems -Methodological Issues and Empirical Studies on the Impact of Personalization",
      "authors": [
        "N Tintarev",
        "J Masthoff"
      ],
      "year": "2012",
      "venue": "User Model. User Adapt. Interact"
    },
    {
      "citation_id": "30",
      "title": "End-to-end Learning for Music Audio Tagging at Scale",
      "authors": [
        "J Pons",
        "O Nieto",
        "M Prockup",
        "E Schmidt",
        "A Ehmann",
        "X Serra"
      ],
      "year": "2018",
      "venue": "19th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "31",
      "title": "Lyric, rhythm, and non-alignment in the second verse of Kendrick Lamar's \"Momma",
      "authors": [
        "M Ohriner"
      ],
      "year": "2019",
      "venue": "Music Theory Online"
    },
    {
      "citation_id": "32",
      "title": "On the metrical techniques of flow in rap music",
      "authors": [
        "K Adams"
      ],
      "year": "2009",
      "venue": "Music Theory Online"
    },
    {
      "citation_id": "33",
      "title": "Towards explaining expressive qualities in piano recordings: Transfer of explanatory features via acoustic domain adaptation",
      "authors": [
        "S Chowdhury",
        "G Widmer"
      ],
      "year": "2021",
      "venue": "Towards explaining expressive qualities in piano recordings: Transfer of explanatory features via acoustic domain adaptation",
      "arxiv": "arXiv:2102.13479"
    }
  ]
}