{
  "paper_id": "2306.12991v2",
  "title": "Speech Emotion Diarization: Which Emotion Appears When?",
  "published": "2023-06-22T15:47:36Z",
  "authors": [
    "Yingzhi Wang",
    "Mirco Ravanelli",
    "Alya Yacoubi"
  ],
  "keywords": [
    "speech emotion diarization",
    "emotion recognition",
    "Zaion Emotion Dataset",
    "emotion diarization error rate"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) typically relies on utterancelevel solutions. However, emotions conveyed through speech should be considered as discrete speech events with definite temporal boundaries, rather than attributes of the entire utterance. To reflect the fine-grained nature of speech emotions and to unify various finegrained methods under a single objective, we propose a new task: Speech Emotion Diarization (SED). Just as Speaker Diarization answers the question of \"Who speaks when?\", Speech Emotion Diarization answers the question of \"Which emotion appears when?\". To facilitate the evaluation of the performance and establish a common benchmark, we introduce the Zaion Emotion Dataset (ZED), an openly accessible speech emotion dataset that includes non-acted emotions recorded in real-life conditions, along with manually annotated boundaries of emotion segments within the utterance. We provide competitive baselines and open-source the code and the pre-trained models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition has consistently been regarded as a complex task, primarily owing to two factors: the subjective, complex, and temporally fine-grained expression of speech emotion and the difficulty in finding effective feature representations  [1, 2, 3, 4] . In the past few years, the progress in deep learning has contributed to notable improvements in the performance of emotion recognition systems by leveraging highly effective features extracted from deep neural networks  [5, 6, 7] . Rather than exploring the emotional feature representations, the main objective of this paper is to investigate the fine time granularity of speech emotions.\n\nAccording to research in psychology, two of the most popular emotion modeling approaches can be distinguished: the categorical approach and the dimensional approach  [1] . The dimensional approach  [8, 9, 10]  has increasingly gained widespread interest and attention because it can better reflect the complexity of some nonbasic, subtle, and rather complex emotions like thinking and embarrassment. Frame-level solutions  [11, 12]  and datasets  [13, 14, 15]  based on the dimensional approach have been proposed to study the temporal interdependence of emotion states. Compared with the dimensional approach, the categorical approach  [16]  has been the most commonly adopted approach thanks to its universality, intuitiveness, and facility for annotation  [4] . However, in terms of temporal granularity, the majority of the solutions still target utterance-Fig.  1 . Different goals for utterance-level Speech Emotion Recognition (SER) and Speech Emotion Diarization (SED). While the former focuses on recognizing the emotions conveyed in an entire utterance, the latter goes beyond by not only identifying the emotions but also accurately locating their boundaries. level tasks  [2, 4]  and are evaluated on utterance-level datasets and with utterance-level metrics, despite the application of frame-level methodologies.\n\nThis paper focuses on the categorical modeling approach for fine-grained emotion recognition, an area that has received little research attention despite its long-recognized importance  [17, 18] . Our work aims to address a major issue that has hindered extensive research into fine-grained emotion recognition. Specifically, we identified a lack of common benchmarks, with well-defined tasks, evaluation datasets and metrics. To address this gap, we propose a Speech Emotion Diarization (SED) task. While standard speech emotion recognition focuses on identifying the emotion that corresponds to a given utterance, the proposed Speech Emotion Diarization task aims to simultaneously identify the correct emotions and their corresponding boundaries. This task is particularly important for capturing the fine-grained nature of speech emotions and for providing a better causality of the emotion predictions. In the context of industrial applications, the Speech Emotion Diarization task presents an opportunity to gain greater precision in emotion analysis and to track the evolution of emotion over time. A comparison between standard utterance-level Speech Emotion Recognition and the proposed Speech Emotion Diarization can be found in Figure  1 . Inspired by Diarization Error Rate (DER) which is commonly used in Speaker Diarization, we have defined the Emotion Diarization Error Rate (EDER) for the evaluation of the proposed SED task. We also release the Zaion Emotion Dataset (ZED), a fine-grained emotion dataset that is openly accessible to the research community. The ZED dataset includes discrete categorical labels for emotional segments within each utterance, as well as manually annotated bound-aries for each emotion segment.\n\nIn summary, our work brings the following contributions to the field of fine-grained speech emotion recognition:\n\n• We propose a Speech Emotion Diarization task that provides a clear and well-defined benchmark to unify various framelevel methods under a diarization objective.\n\n• We introduce a novel evaluation metric, EDER, that takes into account both the quality of the emotion classification and the accuracy of the detection of emotion boundaries.\n\n• We release the Zaion Emotion Dataset 1 , a high-quality, manually-annotated, fine-grained emotion dataset that includes non-acted/in-the-wild emotions. The dataset is freely available to the research community, providing a common benchmark for evaluation and encouraging further research in this field.\n\n• We open-source the code and the pre-trained models 2 on the popular SpeechBrain toolkit  [19] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "The concept of frame-level processing and dynamic modeling has a long-standing presence in the field of speech emotion recognition. At an early stage,  [20]  investigated dynamic modeling at framelevel for speech emotion recognition, aiming to capture the important information on temporal sub-turn-layers. The frame-level information was classified by a Gaussian Mixture Model to output a frame-wise score. The output scores were added to a super-vector combined with static acoustic features and passed to an SVM for the final utterance-level classification task.\n\nThen, driven by the advancements in deep learning, a widespread incorporation of deep neural networks has been witnessed in framelevel emotion recognition approaches. In  [21]  the authors calculated low-level features for frames and transformed this sequence of features to the sequence of probability distributions over the target emotion labels through a densely connected neural network. Then these probabilities were aggregated into utterance-level features using simple statistics for a final classification.  [22]  proposed a realtime SER system based on end-to-end deep learning, where a Deep Neural Network (DNN) was used to recognize emotions from a one-second frame of raw speech spectrogram. The system was evaluated at utterance-level by fusing frame-level predictions. Both  [23]  and  [24]  noticed the fact that different frames may have different contributions to the overall utterance-level emotion. They proposed attentional pooling strategies on top of DNN frame-level embeddings in order to focus on specific regions of a speech signal that are more emotionally salient. The speech frames were assigned different weights from the attention mechanism based on how emotional they were decided to be. The utilization of frame-wise weights in these works reflects the fact that researchers have begun to put more emphasis on exploring the fine granularity within speech emotions.\n\nFurthermore, some works showed an emerging trend suggesting that a single emotion label is no longer sufficient to adequately describe the emotion complexity of a spoken utterance.  [25]  introduced a frame-based formulation of Speech Emotion Recognition to model intra-utterance dynamics. Instead of using only the utterance-level emotion class, the researchers added silence into the output classes since silence and unvoiced speech were not removed from the input 1 https://zaion.ai/en/resources/zaion-lab-blog/zaion-emotion-dataset/ 2 https://github.com/speechbrain/speechbrain/tree/develop/recipes/ Fig.  2 . Pipeline for speech emotion diarization system. speech. The approach was evaluated with utterance-level accuracies. During inference, the intra-utterance emotion transitions were demonstrated with the posterior class probabilities.  [17]  took into account the problem that even the emotional utterance might contain non-emotional parts. The study designed intra-utterance emotion transition sequences and innovatively used a CTC method to predict the sequence of emotions within one spoken utterance. The emotion sequences were finally collapsed into one emotion label to meet the utterance-level evaluation.\n\nAs can be seen from the aforementioned literature, researchers have consistently applied frame-level methodologies in Speech Emotion Recognition and have acknowledged the significance of intra-utterance dynamics. However, a common constraint is that almost all the mainstream categorical datasets are annotated at utterance-level  [26, 27, 28] . Besides, the absence of a unified finegrained metric for assessing frame-level systems compounds this limitation. Consequently, the majority of the frame-level approaches had to aggregate the frame-level embeddings or predictions into utterance-level for performance evaluation. This practice results in a loss of the fine-grained nature of speech emotions that frame-level methods could potentially capture.\n\nTo address these restrictions, we introduce a fine-grained emotion recognition task together with a fine-grained emotion dataset. In addition, we propose a general metric that allows the evaluation of various frame-level approaches despite different frame lengths utilized. Through our work, we seek to accomplish two principal objectives:\n\n• Reveal the fine temporal granularity brought by frame-level approaches by directly assessing them on a fine-grained task along with a fine-grained dataset and a fine-grained evaluation metric.\n\n• At the same time, establish a common benchmark to compare the intra-utterance dynamic modeling capacities of different frame-level methods.\n\nThe proposed Speech Emotion Diarization task can be considered as a sub-task of the general Speech Emotion Recognition task, which is specially designed to more effectively accommodate framelevel processing/dynamic modeling methodologies.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speech Emotion Diarization",
      "text": "The Speech Emotion Diarization task takes a spoken utterance as input and aims at identifying the presence of a pre-defined set of emotion candidates, while also determining the time intervals in which they appear. In the following, we describe the speech emotion diarization pipeline and the proposed evaluation metric.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speech Emotion Diarization Pipeline",
      "text": "Figure  2  illustrates the proposed pipeline for Speech Emotion Diarization, which consists of three primary modules: front-end processing, frame-wise classification, and post-processing. The frontend processing module incorporates pre-processing techniques such Fig.  3 . The adopted frame-wise classification model is composed of a fine-grained emotional encoder and a frame-wise classifier.\n\nas speech enhancement, dereverberation, source separation, and speech segmentation, which are utilized to improve the quality of the speech recordings and determine the appropriate input lengths before feeding the utterances into the frame-wise classification module. The frame-wise classifier predicts the emotional content on a frame-by-frame basis, allowing for a finer time granularity as opposed to classifying an entire input utterance. A model for framewise emotion classification is proposed in Figure  3 , which employs an emotional encoder followed by a linear classifier. A challenge for fine-grained speech emotion recognition is that different emotional states can vary significantly in duration, with some lasting only a few frames while others could persist for longer periods. To accommodate variable-sized contexts, we adopted a standard self-attention mechanism  [29]  by integrating transformer blocks within the emotional encoder. Specifically, we leveraged modern self-supervised models (e.g., Wav2vec 2.0  [30] , Hubert  [31] , and WavLM  [32] ), which have demonstrated superior performance across various tasks including speech emotion recognition  [6, 33] .\n\nFinally, post-processing techniques can be used to improve the performance of the prediction. For example, the predictions can be filtered based on prior knowledge (e.g., number/position of emotion appearances), or a threshold can be applied to mask low-confidence predictions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Metric",
      "text": "The Emotion Diarization Error Rate (EDER) is specially designed to address the constraint of frame-level methods, which typically rely on utterance-level metrics for evaluation. Inspired by the Diarization Error Rate (DER) used for Speaker Diarization, we define EDER as follows:\n\nAs shown in Figure  4 , the EDER consists of the following components:\n\n• False Alarm (FA): The duration of non-emotional segments that are incorrectly predicted as emotional. • Missed Emotion (ME): The duration of emotional segments that are incorrectly predicted as non-emotional.\n\n• Confusion (CF): The duration of emotional segments that are incorrectly assigned to another (other) emotion(s).\n\n• Overlap (OL): The duration of non-overlapped emotional segments that are predicted to contain other overlapped emotions apart from the correct one.\n\nEstablishing a standard benchmark can be challenging due to the use of different frame lengths in different frame-level solutions. However, by comparing the endpoints of the predicted emotion intervals and the actual emotion intervals, EDER takes the impact of frame lengths into consideration. Commonly, given the equivalent recognition capabilities of the models, the adoption of a smaller frame size provides finer time resolution and can subsequently result in lower EDER. The EDER metric also serves as a motivating factor for frame-level methods to strive for increased frame-wise accuracy as well as finer granularity.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Zaion Emotion Dataset",
      "text": "One of the major challenges in studying fine-grained speech emotion recognition is the limited availability of datasets. To meet this lack, we built and released the Zaion Emotion Dataset (ZED), which includes discrete emotion labels and boundaries of the emotional segments within each utterance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Collection",
      "text": "We first collected emotional YouTube videos that were licensed under the Creative Commons license. These videos were then converted from their original formats to single-channel 16 kHz WAV files. The selected videos encompass a diverse range of emotional and interactive scenes, such as comedy shows, victim interviews, sports commentary, and more. In the selected scenes, the speakers were fully aware of the environment and the context of the conversation. The selected videos contain non-acted emotions recorded in real-life situations. This provides a unique set of challenges that are difficult to replicate in acted and laboratory conditions, offering a significant advantage in terms of dataset realism and relevance for industrial applications. Through the release of the ZED dataset, we aim to support the community in the evaluation and deployment of speech emotion recognition technologies in various real-life use cases.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Design",
      "text": "We included three most frequently identified basic emotions  [34] : happy, sad, angry. We also added neutral to represent the non-Fig.  5 . The four valid transitional sequences of an utterance where a single emotional event occurs  [17] . In the ZED dataset, the \"Emo\" label represents only one of the 3 emotions: happiness, sadness, or anger. The \"Null\" label represents neutral states. emotional states that commonly exist in spoken utterances. In contrast to neutrality, the other three emotions are considered as speech events. Detecting them and their boundaries is the goal of the proposed Speech Emotion Diarization task. This task poses a significant challenge as the boundaries between different emotions are sometimes fuzzy even for human annotators. To alleviate this difficulty, we limited our focus to the utterances that contained only one emotional event. Figure  5  illustrates four possible transitional sequences for an utterance that exhibits only one emotional event  [17] . Each utterance of the designed ZED dataset corresponds to one of the four sequences.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Annotation Protocol",
      "text": "The annotation of the ZED dataset was conducted in a multi-level strategy, involving three main sessions that progressed from coarse to fine granularity:\n\n• utterance-level coarse-grained pre-selection,\n\n• sub-utterance-level segmentation/annotation,\n\n• frame-level boundary annotation.\n\nFirst, the emotional speech recordings were segmented into utterances with a duration between 10 and 20 seconds. Three human experts worked on the utterance-level pre-selection by assigning a discrete emotion label to each utterance. Only those utterances annotated as happy/sad/angry were passed to the second session. In the second session, five professional annotators with linguistic backgrounds performed the sub-utterance-level annotation. The utterances were first manually segmented into sub-utterances (if any), and then a discrete emotion label was assigned to each sub-utterance by the annotators. The annotations from the first session were verified during this process. Finally, a frame-level annotation was carried out to determine the durations of each emotional segment by the five annotators mentioned above. When the emotional boundaries needed to be determined, the annotators were asked to specify a precise timestamp within a frame as an endpoint of the emotion segment. For each session, the assignment of the emotional labels was based on the consensus derived from the annotators' subjective evaluations, a voting system was integrated to deal with the disagreements where the most voted category was selected as the final annotation. Only the utterances or sub-utterances that conform to the four sequences shown in Figure  5  were kept in the ZED dataset. Moreover, to reduce the negative impact of in-utterance silences on the quality of annotation, any utterance with silences longer than 0.2 seconds was excluded.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Dataset Statistics",
      "text": "Table  1  presents the overall statistics for the ZED dataset. It contains 180 utterances ranging from 1 to 15 seconds in duration and covers 73 speakers of different ages and genders. The proportion of the utterances corresponding to the three emotion categories is relatively balanced. Since there can be variations in the length of annotated emotion intervals across different annotators, we accessed the level of agreement by calculating the Fleiss' Kappa over short frames of 0.01 seconds. The obtained frame-wise Fleiss' Kappa is 0.81, which indicates a strong agreement among the five annotators. Additionally, the transcript of the speech content is also provided for each utterance. The ZED dataset is currently a small dataset, making it not ideal for training purposes. However, the inclusion of real-life emotions as well as the high-quality, fine-grained annotations make it an exceptional dataset for evaluation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Set",
      "text": "Since the current ZED dataset is not ideal for training, we constructed the training set using simulated data. We collected five English emotion datasets that are commonly used and annotated at utterance-level: IEMOCAP  [26] , RAVDESS  [27] , Emov-DB  [35] , ESD  [36] , and JL-CORPUS  [37] , and we only selected the recordings that were annotated as happy, sad, angry, and neutral. The selected recordings were resampled to 16 kHz and were pre-processed by a Voice Activity Detection (VAD) module to confirm that no audio files contained silences lasting more than 0.2 seconds. For each processed recording, we assumed that the utterance-level emotional label could be perfectly attributed to the entirety of the utterance. Then, to obtain the training set, we randomly concatenated the processed recordings from the same speaker into longer utterances according to the four sequences shown in Figure  5 , assigning an equal probability of 0.25 to each. At last, a total of over 21 hours of simulated data were collected accounting for both the training and validation sets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "Large models (with about 317 million parameters) of pre-trained Wav2vec 2.0/HuBERT/WavLM were employed as emotional encoders in our experiments. These self-supervised models share the same architecture of the CNN feature encoder, for audios sampled at 16 kHz, this leads to a receptive field of 25ms and a stride of about 20ms between adjacent frames  [30] . Despite the small receptive field, the transformer blocks inside the emotional encoder Each encoded speech unit from the self-supervised model is then classified into one of the four emotion classes through a single fully connected layer. For each utterance of the training set and the ZED dataset, the ground truth frame-wise labels were generated using the same 20ms stride. If two or more adjacent emotions occur within a frame, the frame will be assigned the emotion with the longer-lasting presence.\n\nThe training process followed the methodology outlined in  [6] . The CNN-based feature encoder was frozen while only the transformer blocks were fine-tuned on the downstream frame-wise classification task. Two different schedulers were applied to adjust the learning rate of the self-supervised encoder and the learning rate of the downstream classifier. For both schedulers, an Adam Optimizer was utilized and the learning rates were linearly annealed according to the performance of the validation stage. The self-supervised fine-tuning learning rate and the downstream learning rate were initialized to 10 -5 and 10 -4 . The Negative Log-Likelihood (NLL) loss was calculated over the frame-wise classification and was then optimized.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "The performance of the proposed baseline models is shown in Table 2. The baselines were trained with a 90/10 train-validation split of the constructed training set and were evaluated on the proposed ZED dataset. As the ZED dataset remains fixed and separate from the training/validation set, the evaluation was conducted under a speaker-independent setup. The reported EDER values were averaged across 5 different train-validation splitting seeds. In addition, the standard deviation is also reported to indicate the range of variation.\n\nIt can be inferred from the obtained results that all three models achieve good performance with low EDERs. Specifically, the WavLM-large model outperforms the Wav2vec2.0-large and HuBERT-large models and reaches an EDER of 30.2%, suggesting its better suitability for speech emotion recognition tasks than the other two models.\n\nTo further showcase the dynamic modeling capability of the model, we report the performance of the WavLM-large model on the prediction of the four emotion transitions in Table  3 . For each utterance, if the predicted emotions remain constant across adjacent frames, we fuse these predictions to obtain a shorter emotion sequence. We then compare the obtained sequences with the ground truth sequence shown in Figure  5 . This aims to determine whether the model is able to correctly identify the approximate location of the emotion, i.e., the entirety, beginning, middle, or end of the utterance, and to further provide insights into the model's general capability. The results demonstrate that the model correctly identifies the emotion transition in 42% of the predictions. In particular, we observe a Table  3 . Performance of the WavLM-large model on emotion transition prediction. For each emotion transition in the Zaion Emotion Dataset, we present the total number of utterances (TN), the number of utterances with the correct emotion transition predicted (CN), and the accuracy (ACC) which is equal to CN/T N . decreasing accuracy as the number of emotion transitions increases in the utterance. Especially when the emotional event appears in the middle of the utterance, the model produces poor predictions, which indicates a clear increase in the difficulty of the task. Despite the inherent challenges present in the ZED dataset and the use of simulated training data, the results achieved by our model were satisfactory. Our finding provides evidence of the model's capability on distinguishing emotions at a fine temporal granularity and also validates the feasibility of the proposed Speech Emotion Diarization task on the ZED dataset.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Transition",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we investigated fine-grained speech emotion recognition and proposed a Speech Emotion Diarization task that aims to identify the emotions along with their temporal boundaries. We also introduced the Zaion Emotion Dataset, a freely available finegrained emotion dataset containing real-life emotions. We provided performant baseline models, with the WavLM-large model achieving the lowest EDER of 30.2%. Both the code and the pre-trained models have been open-sourced. Through our work, we hope to further reveal the fine-grained nature of speech emotions and to establish a common benchmark for fine-grained speech emotion recognition approaches through a diarization objective.\n\nIn our future work, we plan to expand the dataset to a larger scale and to include more languages. We also plan to explore the integration of front-end and post-processing techniques, as well as develop novel baseline models with lower EDER.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Different goals for utterance-level Speech Emotion Recogni-",
      "page": 1
    },
    {
      "caption": "Figure 2: Pipeline for speech emotion diarization system.",
      "page": 2
    },
    {
      "caption": "Figure 2: illustrates the proposed pipeline for Speech Emotion Di-",
      "page": 2
    },
    {
      "caption": "Figure 3: The adopted frame-wise classification model is composed of",
      "page": 3
    },
    {
      "caption": "Figure 3: , which employs",
      "page": 3
    },
    {
      "caption": "Figure 4: , the EDER consists of the following compo-",
      "page": 3
    },
    {
      "caption": "Figure 4: The four components of the Emotion Diarization Error Rate",
      "page": 3
    },
    {
      "caption": "Figure 5: The four valid transitional sequences of an utterance where a",
      "page": 4
    },
    {
      "caption": "Figure 5: illustrates four possible transitional sequences",
      "page": 4
    },
    {
      "caption": "Figure 5: were kept in the ZED dataset.",
      "page": 4
    },
    {
      "caption": "Figure 5: , assigning an equal",
      "page": 4
    },
    {
      "caption": "Figure 5: This aims to determine whether",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": "Speech Emotion Recognition (SER)\ntypically relies on utterance-"
        },
        {
          "ABSTRACT": "level solutions. However, emotions conveyed through speech should"
        },
        {
          "ABSTRACT": "be\nconsidered\nas\ndiscrete\nspeech\nevents with\ndefinite\ntemporal"
        },
        {
          "ABSTRACT": "boundaries, rather than attributes of the entire utterance. To reflect"
        },
        {
          "ABSTRACT": "the fine-grained nature of speech emotions and to unify various fine-"
        },
        {
          "ABSTRACT": "grained methods under a single objective, we propose a new task:"
        },
        {
          "ABSTRACT": "Speech Emotion Diarization (SED).\nJust\nas Speaker Diarization"
        },
        {
          "ABSTRACT": "answers the question of “Who speaks when?”, Speech Emotion Di-"
        },
        {
          "ABSTRACT": "arization answers the question of “Which emotion appears when?”."
        },
        {
          "ABSTRACT": "To facilitate the evaluation of the performance and establish a com-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "mon benchmark, we introduce the Zaion Emotion Dataset\n(ZED),"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "an openly accessible speech emotion dataset that includes non-acted"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "emotions recorded in real-life conditions, along with manually an-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "notated boundaries of emotion segments within the utterance. We"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "provide\ncompetitive baselines\nand open-source\nthe\ncode\nand the"
        },
        {
          "ABSTRACT": "pre-trained models."
        },
        {
          "ABSTRACT": "Index Terms— speech emotion diarization, emotion recogni-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "tion, Zaion Emotion Dataset, emotion diarization error rate"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "1.\nINTRODUCTION"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "Speech emotion recognition has\nconsistently been regarded as\na"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "complex task, primarily owing to two factors:\nthe subjective, com-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "plex, and temporally fine-grained expression of speech emotion and"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "the difficulty in finding effective feature representations [1, 2, 3, 4]."
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "In the past few years,\nthe progress in deep learning has contributed"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "to notable improvements in the performance of emotion recognition"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "systems by leveraging highly effective features extracted from deep"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "neural networks [5, 6, 7]. Rather than exploring the emotional fea-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "ture representations, the main objective of this paper is to investigate"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "the fine time granularity of speech emotions."
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "According to research in psychology,\ntwo of\nthe most popular"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "emotion modeling approaches can be distinguished:\nthe categorical"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "approach and the dimensional approach [1].\nThe dimensional ap-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "proach [8, 9, 10] has increasingly gained widespread interest and"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "attention because it can better reflect\nthe complexity of some non-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "basic, subtle, and rather complex emotions like thinking and embar-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "rassment.\nFrame-level solutions [11, 12] and datasets [13, 14, 15]"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "based on the dimensional approach have been proposed to study the"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "temporal\ninterdependence of emotion states.\nCompared with the"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "dimensional approach,\nthe categorical approach [16] has been the"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "most commonly adopted approach thanks to its universality,\nintu-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "itiveness, and facility for annotation [4]. However,\nin terms of tem-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "poral granularity,\nthe majority of the solutions still\ntarget utterance-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "This research is funded by Zaion."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "aries for each emotion segment.": "In summary, our work brings the following contributions to the"
        },
        {
          "aries for each emotion segment.": "field of fine-grained speech emotion recognition:"
        },
        {
          "aries for each emotion segment.": "• We propose a Speech Emotion Diarization task that provides"
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "a clear and well-defined benchmark to unify various frame-"
        },
        {
          "aries for each emotion segment.": "level methods under a diarization objective."
        },
        {
          "aries for each emotion segment.": "• We introduce a novel evaluation metric, EDER, that takes into"
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "account both the quality of the emotion classification and the"
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "accuracy of the detection of emotion boundaries."
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "• We\nrelease\nthe Zaion Emotion Dataset1,\na\nhigh-quality,"
        },
        {
          "aries for each emotion segment.": "manually-annotated,\nfine-grained\nemotion\ndataset\nthat\nin-"
        },
        {
          "aries for each emotion segment.": "cludes non-acted/in-the-wild emotions. The dataset\nis freely"
        },
        {
          "aries for each emotion segment.": "available to the research community, providing a common"
        },
        {
          "aries for each emotion segment.": "benchmark for evaluation and encouraging further\nresearch"
        },
        {
          "aries for each emotion segment.": "in this field."
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "• We open-source the code and the pre-trained models 2 on the"
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "popular SpeechBrain toolkit [19]."
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "2. RELATED WORK"
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "The concept of frame-level processing and dynamic modeling has a"
        },
        {
          "aries for each emotion segment.": "long-standing presence in the field of speech emotion recognition."
        },
        {
          "aries for each emotion segment.": "At an early stage, [20] investigated dynamic modeling at frame-"
        },
        {
          "aries for each emotion segment.": "level for speech emotion recognition, aiming to capture the impor-"
        },
        {
          "aries for each emotion segment.": "tant\ninformation on temporal sub-turn-layers.\nThe frame-level\nin-"
        },
        {
          "aries for each emotion segment.": "formation was classified by a Gaussian Mixture Model\nto output a"
        },
        {
          "aries for each emotion segment.": "frame-wise score. The output scores were added to a super-vector"
        },
        {
          "aries for each emotion segment.": "combined with static acoustic features and passed to an SVM for the"
        },
        {
          "aries for each emotion segment.": "final utterance-level classification task."
        },
        {
          "aries for each emotion segment.": "Then, driven by the advancements in deep learning, a widespread"
        },
        {
          "aries for each emotion segment.": "incorporation of deep neural networks has been witnessed in frame-"
        },
        {
          "aries for each emotion segment.": "level emotion recognition approaches. In [21] the authors calculated"
        },
        {
          "aries for each emotion segment.": "low-level features for frames and transformed this sequence of fea-"
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "tures\nto the\nsequence of probability distributions over\nthe\ntarget"
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "emotion labels through a densely connected neural network. Then"
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "these\nprobabilities were\naggregated\ninto\nutterance-level\nfeatures"
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "using simple statistics for a final classification. [22] proposed a real-"
        },
        {
          "aries for each emotion segment.": "time SER system based on end-to-end deep learning, where a Deep"
        },
        {
          "aries for each emotion segment.": "Neural Network (DNN) was used to recognize\nemotions\nfrom a"
        },
        {
          "aries for each emotion segment.": "one-second frame of raw speech spectrogram. The system was eval-"
        },
        {
          "aries for each emotion segment.": "uated at utterance-level by fusing frame-level predictions. Both [23]"
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "and [24] noticed the fact\nthat different\nframes may have different"
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "contributions to the overall utterance-level emotion. They proposed"
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "attentional pooling strategies on top of DNN frame-level embed-"
        },
        {
          "aries for each emotion segment.": ""
        },
        {
          "aries for each emotion segment.": "dings in order to focus on specific regions of a speech signal that are"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "(EDER)."
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "• Missed Emotion (ME): The duration of emotional segments"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "that are incorrectly predicted as non-emotional."
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "• Confusion (CF): The duration of emotional segments that are"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "incorrectly assigned to another (other) emotion(s)."
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "• Overlap (OL): The duration of non-overlapped emotional"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "segments that are predicted to contain other overlapped emo-"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "tions apart from the correct one."
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": ""
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": ""
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "Establishing a standard benchmark can be challenging due to"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "the use of different frame lengths in different frame-level solutions."
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "However, by comparing the endpoints of the predicted emotion in-"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "tervals and the actual emotion intervals, EDER takes the impact of"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "frame lengths into consideration. Commonly, given the equivalent"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "recognition capabilities of\nthe models,\nthe adoption of a smaller"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "frame size provides finer time resolution and can subsequently result"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "in lower EDER. The EDER metric also serves as a motivating factor"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "for frame-level methods to strive for increased frame-wise accuracy"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "as well as finer granularity."
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": ""
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": ""
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "4. ZAION EMOTION DATASET"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": ""
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": ""
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "One of the major challenges in studying fine-grained speech emotion"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": ""
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "recognition is the limited availability of datasets. To meet this lack,"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": ""
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "we built and released the Zaion Emotion Dataset (ZED), which in-"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": ""
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "cludes discrete emotion labels and boundaries of the emotional seg-"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": ""
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "ments within each utterance."
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": ""
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": ""
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "4.1. Data Collection"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": ""
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "We first collected emotional YouTube videos that were licensed un-"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "der\nthe Creative Commons license.\nThese videos were then con-"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "verted from their original\nformats to single-channel 16 kHz WAV"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "files. The selected videos encompass a diverse range of emotional"
        },
        {
          "Fig. 4. The four components of the Emotion Diarization Error Rate": "and interactive scenes,\nsuch as comedy shows, victim interviews,"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": ""
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": "anger. The ”Null” label represents neutral states."
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": ""
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": "emotional states that commonly exist"
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": ""
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": "trast to neutrality, the other three emotions are considered as speech"
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": ""
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": "events. Detecting them and their boundaries is the goal of the pro-"
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": ""
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": "posed Speech Emotion Diarization task. This task poses a significant"
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": ""
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": "challenge as the boundaries between different emotions are some-"
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": ""
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": "times fuzzy even for human annotators. To alleviate this difficulty,"
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": ""
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": "we limited our focus to the utterances that contained only one emo-"
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": ""
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": "tional event. Figure 5 illustrates four possible transitional sequences"
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": ""
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": "for an utterance that exhibits only one emotional event [17]. Each ut-"
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": ""
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": "terance of the designed ZED dataset corresponds to one of the four"
        },
        {
          "label represents only one of the 3 emotions: happiness, sadness, or": "sequences."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4.4. Dataset Statistics": "Table 1 presents the overall statistics for the ZED dataset. It contains"
        },
        {
          "4.4. Dataset Statistics": "180 utterances ranging from 1 to 15 seconds in duration and covers"
        },
        {
          "4.4. Dataset Statistics": "73 speakers of different ages and genders. The proportion of the ut-"
        },
        {
          "4.4. Dataset Statistics": "terances corresponding to the three emotion categories is relatively"
        },
        {
          "4.4. Dataset Statistics": "balanced.\nSince there can be variations in the length of annotated"
        },
        {
          "4.4. Dataset Statistics": "emotion intervals across different annotators, we accessed the level"
        },
        {
          "4.4. Dataset Statistics": "of agreement by calculating the Fleiss’ Kappa over short frames of"
        },
        {
          "4.4. Dataset Statistics": "0.01 seconds. The obtained frame-wise Fleiss’ Kappa is 0.81, which"
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "indicates a strong agreement among the five annotators. Addition-"
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "ally,\nthe transcript of\nthe speech content\nis also provided for each"
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "utterance."
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "Table 1. A summary of the overall statistics of the ZED dataset."
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "Language\nEnglish"
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "Number of utterances\n180"
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "Total duration\n17 minutes"
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "Number of speakers\n73"
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "Age of speakers\n20 to 70"
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "Gender of speakers\n52% female"
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "% Happy\n34%"
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "% Sad\n37%"
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "% Angry\n29%"
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "The ZED dataset is currently a small dataset, making it not ideal"
        },
        {
          "4.4. Dataset Statistics": "for training purposes. However,\nthe inclusion of real-life emotions"
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "as well as the high-quality, fine-grained annotations make it an ex-"
        },
        {
          "4.4. Dataset Statistics": "ceptional dataset for evaluation."
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": ""
        },
        {
          "4.4. Dataset Statistics": "5. EXPERIMENTS"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Average Emotion Diarization Error Rate (EDER%) and Table3. PerformanceoftheWavLM-largemodelonemotiontran-",
      "data": [
        {
          "Table 2. Average Emotion Diarization Error Rate (EDER%) and": "standard deviation (%) of\nthe proposed baseline models on Zaion",
          "Table 3.\nPerformance of the WavLM-large model on emotion tran-": "sition prediction. For each emotion transition in the Zaion Emotion"
        },
        {
          "Table 2. Average Emotion Diarization Error Rate (EDER%) and": "Emotion Dataset computed over 5 different seeds.",
          "Table 3.\nPerformance of the WavLM-large model on emotion tran-": "Dataset, we present the total number of utterances (TN), the number"
        },
        {
          "Table 2. Average Emotion Diarization Error Rate (EDER%) and": "",
          "Table 3.\nPerformance of the WavLM-large model on emotion tran-": "of utterances with the correct emotion transition predicted (CN), and"
        },
        {
          "Table 2. Average Emotion Diarization Error Rate (EDER%) and": "Emotional Encoder\nEDER%",
          "Table 3.\nPerformance of the WavLM-large model on emotion tran-": ""
        },
        {
          "Table 2. Average Emotion Diarization Error Rate (EDER%) and": "",
          "Table 3.\nPerformance of the WavLM-large model on emotion tran-": "the accuracy (ACC) which is equal to CN/T N ."
        },
        {
          "Table 2. Average Emotion Diarization Error Rate (EDER%) and": "Wav2vec2.0-large\n36.2 ± 1.14",
          "Table 3.\nPerformance of the WavLM-large model on emotion tran-": ""
        },
        {
          "Table 2. Average Emotion Diarization Error Rate (EDER%) and": "",
          "Table 3.\nPerformance of the WavLM-large model on emotion tran-": "Transition\nTN\nCN\nACC"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: Average Emotion Diarization Error Rate (EDER%) and Table3. PerformanceoftheWavLM-largemodelonemotiontran-",
      "data": [
        {
          "standard deviation (%) of": "Emotion Dataset computed over 5 different seeds.",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": "Dataset, we present the total number of utterances (TN), the number"
        },
        {
          "standard deviation (%) of": "",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": "of utterances with the correct emotion transition predicted (CN), and"
        },
        {
          "standard deviation (%) of": "Emotional Encoder",
          "the proposed baseline models on Zaion": "EDER%",
          "sition prediction. For each emotion transition in the Zaion Emotion": ""
        },
        {
          "standard deviation (%) of": "",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": "the accuracy (ACC) which is equal to CN/T N ."
        },
        {
          "standard deviation (%) of": "Wav2vec2.0-large",
          "the proposed baseline models on Zaion": "36.2 ± 1.14",
          "sition prediction. For each emotion transition in the Zaion Emotion": ""
        },
        {
          "standard deviation (%) of": "",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": "Transition"
        },
        {
          "standard deviation (%) of": "HuBERT-large",
          "the proposed baseline models on Zaion": "34.5 ± 0.81",
          "sition prediction. For each emotion transition in the Zaion Emotion": ""
        },
        {
          "standard deviation (%) of": "WavLM-large",
          "the proposed baseline models on Zaion": "30.2 ± 1.60",
          "sition prediction. For each emotion transition in the Zaion Emotion": "happy"
        },
        {
          "standard deviation (%) of": "",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": "null-happy"
        },
        {
          "standard deviation (%) of": "",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": "happy-null"
        },
        {
          "standard deviation (%) of": "",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": "null-happy-null"
        },
        {
          "standard deviation (%) of": "are able to effectively capture long-range dependencies and contex-",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": ""
        },
        {
          "standard deviation (%) of": "tual emotional",
          "the proposed baseline models on Zaion": "information thanks to the self-attention mechanism.",
          "sition prediction. For each emotion transition in the Zaion Emotion": "sad"
        },
        {
          "standard deviation (%) of": "Each encoded speech unit",
          "the proposed baseline models on Zaion": "from the self-supervised model",
          "sition prediction. For each emotion transition in the Zaion Emotion": "null-sad"
        },
        {
          "standard deviation (%) of": "classified into one of the four emotion classes through a single fully",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": "sad-null"
        },
        {
          "standard deviation (%) of": "connected layer. For each utterance of the training set and the ZED",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": "null-sad-null"
        },
        {
          "standard deviation (%) of": "dataset, the ground truth frame-wise labels were generated using the",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": ""
        },
        {
          "standard deviation (%) of": "",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": "angry"
        },
        {
          "standard deviation (%) of": "same 20ms stride.",
          "the proposed baseline models on Zaion": "If two or more adjacent emotions occur within a",
          "sition prediction. For each emotion transition in the Zaion Emotion": ""
        },
        {
          "standard deviation (%) of": "",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": "null-angry"
        },
        {
          "standard deviation (%) of": "frame, the frame will be assigned the emotion with the longer-lasting",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": ""
        },
        {
          "standard deviation (%) of": "",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": "angry-null"
        },
        {
          "standard deviation (%) of": "presence.",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": ""
        },
        {
          "standard deviation (%) of": "",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": "null-angry-null"
        },
        {
          "standard deviation (%) of": "",
          "the proposed baseline models on Zaion": "The training process followed the methodology outlined in [6].",
          "sition prediction. For each emotion transition in the Zaion Emotion": ""
        },
        {
          "standard deviation (%) of": "The CNN-based feature encoder was frozen while only the trans-",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": "Total"
        },
        {
          "standard deviation (%) of": "former blocks were fine-tuned on the downstream frame-wise clas-",
          "the proposed baseline models on Zaion": "",
          "sition prediction. For each emotion transition in the Zaion Emotion": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "the wild,” IEEE transactions on pattern analysis and machine"
        },
        {
          "8. REFERENCES": "[1] Hatice Gunes, Bj¨orn Schuller, Maja Pantic, and Roddy Cowie,",
          "database for audio-visual emotion and sentiment\nresearch in": "intelligence, vol. 43, no. 3, pp. 1022–1040, 2019."
        },
        {
          "8. REFERENCES": "“Emotion representation, analysis and synthesis in continuous",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "[14]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand"
        },
        {
          "8. REFERENCES": "space: A survey,”\nin 2011 IEEE International Conference on",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "Denis Lalanne,\n“Introducing the recola multimodal corpus of"
        },
        {
          "8. REFERENCES": "Automatic Face & Gesture Recognition (FG). IEEE, 2011, pp.",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "remote collaborative and affective interactions,”\nin 2013 10th"
        },
        {
          "8. REFERENCES": "827–834.",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "IEEE international conference and workshops on automatic"
        },
        {
          "8. REFERENCES": "[2] Ruhul Amin Khalil, Edward Jones, Mohammad Inayatullah",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "face and gesture recognition (FG). IEEE, 2013, pp. 1–8."
        },
        {
          "8. REFERENCES": "Babar, Tariqullah Jan, Mohammad Haseeb Zafar, and Thamer",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "[15] Gary McKeown, Michel Valstar, Roddy Cowie, Maja Pantic,"
        },
        {
          "8. REFERENCES": "Alhussain,\n“Speech emotion recognition using deep learning",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "and Marc Schroder,\n“The semaine database: Annotated mul-"
        },
        {
          "8. REFERENCES": "techniques: A review,”\nIEEE Access, vol. 7, pp. 117327–",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "timodal records of emotionally colored conversations between"
        },
        {
          "8. REFERENCES": "117345, 2019.",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "IEEE transactions on affective\na person and a limited agent,”"
        },
        {
          "8. REFERENCES": "[3] Taiba Majid Wani, Teddy Surya Gunawan, Syed Asif Ahmad",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "computing, vol. 3, no. 1, pp. 5–17, 2011."
        },
        {
          "8. REFERENCES": "Qadri, Mira Kartiwi, and Eliathamby Ambikairajah,\n“A com-",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "[16]\nPaul Ekman and Wallace V Friesen,\n“Unmasking the face: A"
        },
        {
          "8. REFERENCES": "prehensive\nreview of\nspeech emotion recognition systems,”",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "guide to recognizing emotions from facial clues.,” 1975."
        },
        {
          "8. REFERENCES": "IEEE Access, vol. 9, pp. 47795–47814, 2021.",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "[4] Mehmet Berkehan Akc¸ay and Kaya O˘guz,\n“Speech emo-",
          "database for audio-visual emotion and sentiment\nresearch in": "[17] Vladimir Chernykh and Pavel Prikhodko,\n“Emotion recog-"
        },
        {
          "8. REFERENCES": "tion recognition: Emotional models, databases,\nfeatures, pre-",
          "database for audio-visual emotion and sentiment\nresearch in": "arXiv\nnition from speech with recurrent neural networks,”"
        },
        {
          "8. REFERENCES": "processing methods,\nsupporting modalities,\nand classifiers,”",
          "database for audio-visual emotion and sentiment\nresearch in": "preprint arXiv:1701.08071, 2017."
        },
        {
          "8. REFERENCES": "Speech Communication, vol. 116, pp. 56–76, 2020.",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "[18] Tianyi Zhang, “On fine-grained temporal emotion recognition"
        },
        {
          "8. REFERENCES": "[5] Xixin Wu, Songxiang Liu, Yuewen Cao, Xu Li, Jianwei Yu,",
          "database for audio-visual emotion and sentiment\nresearch in": "in video: How to trade off recognition accuracy with annota-"
        },
        {
          "8. REFERENCES": "Dongyang Dai, Xi Ma, Shoukang Hu, Zhiyong Wu, Xunying",
          "database for audio-visual emotion and sentiment\nresearch in": "tion complexity?,” 2022."
        },
        {
          "8. REFERENCES": "Liu, et al.,\n“Speech emotion recognition using capsule net-",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "[19] Mirco Ravanelli,\nTitouan\nParcollet,\nPeter\nPlantinga, Aku"
        },
        {
          "8. REFERENCES": "works,” in ICASSP 2019-2019 IEEE International Conference",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nau-"
        },
        {
          "8. REFERENCES": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE,",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "man Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, et al.,"
        },
        {
          "8. REFERENCES": "2019, pp. 6695–6699.",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "arXiv\n“Speechbrain:\nA general-purpose\nspeech\ntoolkit,”"
        },
        {
          "8. REFERENCES": "[6] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab",
          "database for audio-visual emotion and sentiment\nresearch in": "preprint arXiv:2106.04624, 2021."
        },
        {
          "8. REFERENCES": "Heba, “A fine-tuned wav2vec 2.0/hubert benchmark for speech",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "[20] Bogdan Vlasenko, Bj¨orn Schuller, Andreas Wendemuth, and"
        },
        {
          "8. REFERENCES": "emotion recognition, speaker verification and spoken language",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "Gerhard Rigoll,\n“Frame vs.\nturn-level:\nemotion recognition"
        },
        {
          "8. REFERENCES": "understanding,” arXiv preprint arXiv:2111.02735, 2021.",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "from speech considering static and dynamic processing,”\nin"
        },
        {
          "8. REFERENCES": "[7]\nItai Gat, Hagai Aronowitz, Weizhong Zhu, Edmilson Morais,",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "Affective Computing and Intelligent Interaction: Second Inter-"
        },
        {
          "8. REFERENCES": "and Ron Hoory,\n“Speaker normalization for self-supervised",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "national Conference, ACII 2007 Lisbon, Portugal, September"
        },
        {
          "8. REFERENCES": "speech emotion recognition,”\nin ICASSP 2022-2022 IEEE In-",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "12-14, 2007 Proceedings 2. Springer, 2007, pp. 139–147."
        },
        {
          "8. REFERENCES": "ternational Conference on Acoustics, Speech and Signal Pro-",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "[21] Kun Han, Dong Yu, and Ivan Tashev, “Speech emotion recog-"
        },
        {
          "8. REFERENCES": "cessing (ICASSP). IEEE, 2022, pp. 7342–7346.",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "nition using deep neural network and extreme learning ma-"
        },
        {
          "8. REFERENCES": "[8] Didier Grandjean, David Sander, and Klaus R Scherer,\n“Con-",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "chine,” in Interspeech 2014, 2014."
        },
        {
          "8. REFERENCES": "scious emotional experience emerges as a function of multi-",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "[22] Haytham M Fayek, Margaret Lech, and Lawrence Cavedon,"
        },
        {
          "8. REFERENCES": "level, appraisal-driven response synchronization,” Conscious-",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "“Towards\nreal-time\nspeech emotion recognition using deep"
        },
        {
          "8. REFERENCES": "ness and cognition, vol. 17, no. 2, pp. 484–495, 2008.",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "neural networks,” in 2015 9th international conference on sig-"
        },
        {
          "8. REFERENCES": "Journal\n[9]\nJames A Russell,\n“A circumplex model of affect.,”",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "nal processing and communication systems (ICSPCS).\nIEEE,"
        },
        {
          "8. REFERENCES": "of personality and social psychology, vol. 39, no. 6, pp. 1161,",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "2015, pp. 1–5."
        },
        {
          "8. REFERENCES": "1980.",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "[23]\nSeyedmahdad Mirsamadi, Emad Barsoum,\nand Cha Zhang,"
        },
        {
          "8. REFERENCES": "[10] Albert Mehrabian,\n“Pleasure-arousal-dominance: A general",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "“Automatic speech emotion recognition using recurrent neu-"
        },
        {
          "8. REFERENCES": "framework for describing and measuring individual differences",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "ral networks with local\nattention,”\nin 2017 IEEE Interna-"
        },
        {
          "8. REFERENCES": "in temperament,” Current Psychology, vol. 14, pp. 261–292,",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "tional Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "8. REFERENCES": "1996.",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "(ICASSP), 2017, pp. 2227–2231."
        },
        {
          "8. REFERENCES": "[11] Martin W¨ollmer, Bj¨orn Schuller, Florian Eyben, and Gerhard",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "[24]\nPengcheng Li, Yan Song, Ian Vince McLoughlin, Wu Guo, and"
        },
        {
          "8. REFERENCES": "Rigoll,\n“Combining long short-term memory and dynamic",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "Li-Rong Dai, “An attention pooling based representation learn-"
        },
        {
          "8. REFERENCES": "bayesian networks for incremental emotion-sensitive artificial",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "ing method for speech emotion recognition,” 2018."
        },
        {
          "8. REFERENCES": "IEEE Journal of selected topics in signal process-\nlistening,”",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "ing, vol. 4, no. 5, pp. 867–881, 2010.",
          "database for audio-visual emotion and sentiment\nresearch in": "[25] Haytham M Fayek, Margaret Lech, and Lawrence Cavedon,"
        },
        {
          "8. REFERENCES": "[12] Martin W¨ollmer,\nFlorian\nEyben,\nStephan\nReiter,\nBj¨orn",
          "database for audio-visual emotion and sentiment\nresearch in": "“Evaluating deep learning architectures\nfor\nspeech emotion"
        },
        {
          "8. REFERENCES": "Schuller, Cate Cox, Ellen Douglas-Cowie, and Roddy Cowie,",
          "database for audio-visual emotion and sentiment\nresearch in": "recognition,” Neural Networks, vol. 92, pp. 60–68, 2017."
        },
        {
          "8. REFERENCES": "“Abandoning\nemotion\nclasses-towards\ncontinuous\nemotion",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "[26] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe"
        },
        {
          "8. REFERENCES": "recognition with modelling\nof\nlong-range\ndependencies,”",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N"
        },
        {
          "8. REFERENCES": "2008.",
          "database for audio-visual emotion and sentiment\nresearch in": ""
        },
        {
          "8. REFERENCES": "",
          "database for audio-visual emotion and sentiment\nresearch in": "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:"
        },
        {
          "8. REFERENCES": "[13]\nJean Kossaifi, Robert Walecki, Yannis Panagakis,\nJie Shen,",
          "database for audio-visual emotion and sentiment\nresearch in": "Lan-\nInteractive emotional dyadic motion capture database,”"
        },
        {
          "8. REFERENCES": "Maximilian Schmitt, Fabien Ringeval, Jing Han, Vedhas Pan-",
          "database for audio-visual emotion and sentiment\nresearch in": "guage resources and evaluation, vol. 42, no. 4, pp. 335–359,"
        },
        {
          "8. REFERENCES": "dit, Antoine Toisoul, Bj¨orn Schuller, et al.,\n“Sewa db: A rich",
          "database for audio-visual emotion and sentiment\nresearch in": "2008."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "visual database of emotional speech and song (ravdess): A dy-"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "namic, multimodal set of facial and vocal expressions in north"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "american english,”\nPloS one, vol. 13, no. 5, pp. e0196391,"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "2018."
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "[28]\nFelix Burkhardt, Astrid Paeschke, Miriam Rolfes, Walter F"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "Sendlmeier, Benjamin Weiss, et al.,\n“A database of german"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "emotional speech.,”\nin Interspeech, 2005, vol. 5, pp. 1517–"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "1520."
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "[29] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszko-"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "Advances in neural\nPolosukhin,\n“Attention is all you need,”"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "information processing systems, vol. 30, 2017."
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "[30] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "Michael Auli, “wav2vec 2.0: A framework for self-supervised"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "learning of speech representations,” in NeurIPS, 2020."
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "[31] Wei-Ning Hsu,\nBenjamin Bolte,\nYao-Hung Hubert\nTsai,"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "Mohamed,\n“Hubert:\nSelf-supervised speech representation"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "IEEE/ACM\nlearning by masked prediction of hidden units,”"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "Transactions on Audio, Speech, and Language Processing, vol."
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "29, pp. 3451–3460, 2021."
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "[32]\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shu-"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "jie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yosh-"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "ioka, Xiong Xiao, et al., “Wavlm: Large-scale self-supervised"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "pre-training for full stack speech processing,” IEEE Journal of"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505–"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "1518, 2022."
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "[33] Kuo-Hsuan Hung, Szu-wei Fu, Huan-Hsin Tseng, Hsin-Tien"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "Chiang, Yu Tsao,\nand Chii-Wann Lin,\n“Boosting\nself-"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "supervised embeddings for speech enhancement,” Interspeech"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "2022, 2022."
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "[34]\nPaul Ekman, “Universals and cultural differences in facial ex-"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "pressions of emotion.,” in Nebraska symposium on motivation."
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "University of Nebraska Press, 1971."
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "[35] Adaeze Adigwe, No´e Tits, Kevin El Haddad, Sarah Ostadab-"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "bas, and Thierry Dutoit,\n“The emotional voices database: To-"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "wards controlling the emotion dimension in voice generation"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "systems,” arXiv preprint arXiv:1806.09514, 2018."
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "[36] Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li,\n“Seen"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "and unseen emotional style transfer for voice conversion with"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "a new emotional speech dataset,” in ICASSP 2021-2021 IEEE"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "International Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "cessing (ICASSP). IEEE, 2021, pp. 920–924."
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "[37]\nJesin James, Li Tian, and Catherine Watson, “An open source"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "emotional speech corpus for human robot\ninteraction applica-"
        },
        {
          "[27]\nSteven R Livingstone and Frank A Russo, “The ryerson audio-": "tions,” Interspeech 2018, 2018."
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion representation, analysis and synthesis in continuous space: A survey",
      "authors": [
        "Hatice Gunes",
        "Björn Schuller",
        "Maja Pantic",
        "Roddy Cowie"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "Edward Ruhul Amin Khalil",
        "Mohammad Jones",
        "Tariqullah Inayatullah Babar",
        "Mohammad Jan",
        "Thamer Haseeb Zafar",
        "Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "4",
      "title": "A comprehensive review of speech emotion recognition systems",
      "authors": [
        "Taiba Majid Wani",
        "Teddy Surya Gunawan",
        "Syed Asif",
        "Ahmad Qadri",
        "Mira Kartiwi",
        "Eliathamby Ambikairajah"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "Mehmet Berkehan",
        "Akc ¸ay",
        "Kaya Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using capsule networks",
      "authors": [
        "Xixin Wu",
        "Songxiang Liu",
        "Yuewen Cao",
        "Xu Li",
        "Jianwei Yu",
        "Dongyang Dai",
        "Xi Ma",
        "Shoukang Hu",
        "Zhiyong Wu",
        "Xunying Liu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Yingzhi Wang",
        "Abdelmoumene Boumadane",
        "Abdelwahab Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "8",
      "title": "Speaker normalization for self-supervised speech emotion recognition",
      "authors": [
        "Itai Gat",
        "Hagai Aronowitz",
        "Weizhong Zhu",
        "Edmilson Morais",
        "Ron Hoory"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "9",
      "title": "Conscious emotional experience emerges as a function of multilevel, appraisal-driven response synchronization",
      "authors": [
        "David Didier Grandjean",
        "Klaus Sander",
        "Scherer"
      ],
      "year": "2008",
      "venue": "Consciousness and cognition"
    },
    {
      "citation_id": "10",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "11",
      "title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament",
      "authors": [
        "Albert Mehrabian"
      ],
      "year": "1996",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "12",
      "title": "Combining long short-term memory and dynamic bayesian networks for incremental emotion-sensitive artificial listening",
      "authors": [
        "Martin Wöllmer",
        "Björn Schuller",
        "Florian Eyben",
        "Gerhard Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Journal of selected topics in signal processing"
    },
    {
      "citation_id": "13",
      "title": "Abandoning emotion classes-towards continuous emotion recognition with modelling of long-range dependencies",
      "authors": [
        "Martin Wöllmer",
        "Florian Eyben",
        "Stephan Reiter",
        "Björn Schuller",
        "Cate Cox",
        "Ellen Douglas-Cowie",
        "Roddy Cowie"
      ],
      "year": "2008",
      "venue": "Abandoning emotion classes-towards continuous emotion recognition with modelling of long-range dependencies"
    },
    {
      "citation_id": "14",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "Jean Kossaifi",
        "Robert Walecki",
        "Yannis Panagakis",
        "Jie Shen",
        "Maximilian Schmitt",
        "Fabien Ringeval",
        "Jing Han",
        "Vedhas Pandit",
        "Antoine Toisoul",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "15",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Juergen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "16",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "Gary Mckeown",
        "Michel Valstar",
        "Roddy Cowie",
        "Maja Pantic",
        "Marc Schroder"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "17",
      "title": "Unmasking the face: A guide to recognizing emotions from facial clues",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1975",
      "venue": "Unmasking the face: A guide to recognizing emotions from facial clues"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition from speech with recurrent neural networks",
      "authors": [
        "Vladimir Chernykh",
        "Pavel Prikhodko"
      ],
      "year": "2017",
      "venue": "Emotion recognition from speech with recurrent neural networks",
      "arxiv": "arXiv:1701.08071"
    },
    {
      "citation_id": "19",
      "title": "On fine-grained temporal emotion recognition in video: How to trade off recognition accuracy with annotation complexity?",
      "authors": [
        "Tianyi Zhang"
      ],
      "year": "2022",
      "venue": "On fine-grained temporal emotion recognition in video: How to trade off recognition accuracy with annotation complexity?"
    },
    {
      "citation_id": "20",
      "title": "Speechbrain: A general-purpose speech toolkit",
      "authors": [
        "Mirco Ravanelli",
        "Titouan Parcollet",
        "Peter Plantinga",
        "Aku Rouhe",
        "Samuele Cornell",
        "Loren Lugosch",
        "Cem Subakan",
        "Nauman Dawalatabad",
        "Abdelwahab Heba",
        "Jianyuan Zhong"
      ],
      "year": "2021",
      "venue": "Speechbrain: A general-purpose speech toolkit",
      "arxiv": "arXiv:2106.04624"
    },
    {
      "citation_id": "21",
      "title": "Frame vs. turn-level: emotion recognition from speech considering static and dynamic processing",
      "authors": [
        "Bogdan Vlasenko",
        "Björn Schuller",
        "Andreas Wendemuth",
        "Gerhard Rigoll"
      ],
      "year": "2007",
      "venue": "Affective Computing and Intelligent Interaction: Second International Conference"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "Kun Han",
        "Dong Yu",
        "Ivan Tashev"
      ],
      "year": "2014",
      "venue": "Speech emotion recognition using deep neural network and extreme learning machine"
    },
    {
      "citation_id": "23",
      "title": "Towards real-time speech emotion recognition using deep neural networks",
      "authors": [
        "Margaret Haytham M Fayek",
        "Lawrence Lech",
        "Cavedon"
      ],
      "year": "2015",
      "venue": "2015 9th international conference on signal processing and communication systems (ICSPCS)"
    },
    {
      "citation_id": "24",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "Seyedmahdad Mirsamadi",
        "Emad Barsoum",
        "Cha Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "Pengcheng Li",
        "Yan Song",
        "Ian Vince Mcloughlin",
        "Wu Guo",
        "Li-Rong Dai"
      ],
      "year": "2018",
      "venue": "An attention pooling based representation learning method for speech emotion recognition"
    },
    {
      "citation_id": "26",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "Margaret Haytham M Fayek",
        "Lawrence Lech",
        "Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "27",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "28",
      "title": "The ryerson audiovisual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "29",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "30",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "31",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "32",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Boosting selfsupervised embeddings for speech enhancement",
      "authors": [
        "Kuo-Hsuan",
        "Szu-Wei Hung",
        "Huan-Hsin Fu",
        "Hsin-Tien Tseng",
        "Yu Chiang",
        "Chii-Wann Tsao",
        "Lin"
      ],
      "year": "2022",
      "venue": "Boosting selfsupervised embeddings for speech enhancement"
    },
    {
      "citation_id": "35",
      "title": "Universals and cultural differences in facial expressions of emotion.,\" in Nebraska symposium on motivation",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1971",
      "venue": "Universals and cultural differences in facial expressions of emotion.,\" in Nebraska symposium on motivation"
    },
    {
      "citation_id": "36",
      "title": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "authors": [
        "Adaeze Adigwe",
        "Noé Tits",
        "Kevin Haddad",
        "Sarah Ostadabbas",
        "Thierry Dutoit"
      ],
      "year": "2018",
      "venue": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "arxiv": "arXiv:1806.09514"
    },
    {
      "citation_id": "37",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "38",
      "title": "An open source emotional speech corpus for human robot interaction applications",
      "authors": [
        "Jesin James",
        "Li Tian",
        "Catherine Watson"
      ],
      "year": "2018",
      "venue": "An open source emotional speech corpus for human robot interaction applications"
    }
  ]
}