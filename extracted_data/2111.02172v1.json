{
  "paper_id": "2111.02172v1",
  "title": "A Cross-Modal Fusion Network Based On Self-Attention And Residual Structure For Multimodal Emotion Recognition",
  "published": "2021-11-03T12:24:03Z",
  "authors": [
    "Ziwang Fu",
    "Feng Liu",
    "Hanyang Wang",
    "Jiayin Qi",
    "Xiangling Fu",
    "Aimin Zhou",
    "Zhibin Li"
  ],
  "keywords": [
    "multimodal emotion recognition",
    "cross-modal blocks",
    "self-attention",
    "residual structure"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The audio-video based multimodal emotion recognition has attracted a lot of attention due to its robust performance. Most of the existing methods focus on proposing different cross-modal fusion strategies. However, these strategies introduce redundancy in the features of different modalities without fully considering the complementary properties between modal information, and these approaches do not guarantee the non-loss of original semantic information during intra-and inter-modal interactions. In this paper, we propose a novel cross-modal fusion network based on self-attention and residual structure (CFN-SR) for multimodal emotion recognition. Firstly, we perform representation learning for audio and video modalities to obtain the semantic features of the two modalities by efficient ResNeXt and 1D CNN, respectively. Secondly, we feed the features of the two modalities into the cross-modal blocks separately to ensure efficient complementarity and completeness of information through the self-attention mechanism and residual structure. Finally, we obtain the output of emotions by splicing the obtained fused representation with the original representation. To verify the effectiveness of the proposed method, we conduct experiments on the RAVDESS dataset. The experimental results show that the proposed CFN-SR achieves the state-of-the-art and obtains 75.76% accuracy with 26.30M parameters. Our code is available at https://github.com/skeletonNN/CFN-SR.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition has attracted a lot of attention due to its high performance and robustness  [1] , which is applied in various fields such as human-computer interaction and social robotics  [2] . The main goal of multimodal emotion recognition is to obtain human emotion expressions from a video sequence. Humans express their emotions mainly through multiple modalities such as speech  [3] , body gestures  [4] , facial expressions  [5] , and text  [6] . Although many studies have employed more complex modalities, video and audio are still the primary modalities used for this task due to their ability to adequately convey emotion. Therefore, in this work, we focus on the audio-video based multimodal emotion recognition.\n\nAmong the existing researches, multimodal emotion recognition can be classified according to the modal information fusion method: early fusion  [7] , late fusion  [8, 9]  and model fusion  [10, 11] . Early * Corresponding author. fusion is to extract and construct multiple modal data into corresponding modal features before stitching them together into a feature set that integrates each modal feature. Late fusion is to find out the plausibility of each model, and then to coordinate and make joint decisions. Recently, with the development of Transformer  [12]  for natural language processing and computer vision tasks, model fusion is often done using Transformer for cross-modal interactions, with significant performance improvements due to the flexibility of fusion locations. For audio-video emotion recognition,  [13]  introduced Transformer to fuse audio and video representations. Different cross-modal fusion strategies were explored by  [14] .  [10]  proposed a novel representation fusion method, Capsule Graph Convolutional Network(CapsGCN), to use graph capsule networks for audio and video emotion recognition.\n\nHowever, existing methods ignore the complementary information between different modalities, and the final decision often requires the joint decision of the two modality-specific features as well as the fused features. The complementary information ensures that there is an improvement in performance when the fusion module is added. In addition, there is still a significant amount of redundancy in the way existing methods fuse. Some models are only stitched together for the final result output, and the stitched result will have many duplicate representations, and the feature information needs to be filtered to further reduce the redundant features before stitching. Moreover, the existing methods cannot guarantee the information integrity in the feature learning process, and often the learning of intraand inter-modal information will lose some semantic information.\n\nTo solve the above problems, we propose a novel cross-modal fusion network based on self-attention and residual structure (CFN-SR) for multimodal emotion recognition. Specifically, we first perform representation learning for audio modality and video modality. The spatio-temporal structure features of video frame sequences are obtained by ResNeXt  [15] . The MFCC features of audio sequences are obtained by a simple and effective 1D CNN. Secondly, we feed the features into the cross-modal blocks separately, and make the audio modality perform intra-modal feature selection through a selfattention mechanism, which will enable the selected features to interact with the video modality adaptively and efficiently between modalities. The integrity of the original structural features of the video modality can be ensured by the residual structure. Finally, we obtain the output of emotions by splicing the obtained fused representation and the original representation.We apply the model to the RAVDESS  [16]  multimodal emotion recognition dataset, and the experimental results demonstrate that our proposed CFN-SR is more effective, and our method achieves state-of-the-art compared with other models, obtaining an accuracy of 75.76% with a parametric number of 26.30M.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "As shown in Figure  1 , we design a cross-modal fusion network based on self-attention and residual structure. Firstly, we use the 3D CNN to obtain the video features and the 1D CNN to obtain the audio features. Then, we obtain the inter-and intra-modal fusion representations of the two modalities by the cross-modal fusion blocks. Finally, we obtain the output of the emotions by splicing the obtained fusion representation with the original representation. We will describe this process in detail below.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio Encoder",
      "text": "For the audio modality, recent work  [17, 18]  has demonstrated the effectiveness of deep learning methods based on Mel Frequency Cepstrum Coefficient (MFCC) features. We design a simple and efficient 1D CNN to perform MFCC feature extraction. Specifically, we use the feature preprocessed audio modal features as input, denoted as XA. We first pass the features through a 2-layer convolution operation to extract the local features of adjacent audio elements. After that, we use the max-pooling to downsample, compress the features, and remove redundant information. The specific equation is as follows:\n\nwhere BN stands for Batch Normalization, kA is the size of the convolution kernel of modality audio and XA denotes the learned semantic features. We again feed the learned features into a 1D temporal convolution to obtain the higher-order semantic features of the audio. Finally, we flatten the obtained features:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Video Encoder",
      "text": "Video data are dependent in both spatial and temporal dimensions, thus a network with 3D convolutional kernels is needed to learn facial expressions and actions. We consider both the performance and training efficiency of the network and choose the 3D ResNeXt  [15]  network to obtain the spatio-temporal structural features of video modalities. ResNeXt proposes a group convolution strategy between the deep segmentation convolution of ordinary convolutional kernels, and achieves a balance between the two strategies by controlling the number of groups with a simple structure but powerful performance. We use feature preprocessed audio modal features as input, denoted as XV . We obtain the higher-order semantic features of video modalities by this network:\n\nwhere XV denotes the learned semantic features, C, S, H and W are the number of channels, sequence length, height, and width, respectively. After obtaining the higher-order semantic features, we feed them into the cross-modal blocks and fuse the audio feature representations. We believe that the final fused representation obtained contains not only the higher-order semantic features of the video modality, but also the interactive features of the two modalities. After that, we perform downsampling using the average pool to reduce redundant information.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross-Modal Blocks",
      "text": "Through encoding operations for both modalities, we obtain higherorder semantic features for both audio and video modalities. To be able to make the final decision more accurate, we exploit the complementary intra-and inter-modal interaction information between the two modalities. Specifically, we first make the audio modality undergo intramodal representation learning through a self-attention mechanism. This operation allows adaptive learning of higher-order semantic features of the audio modality, making it more focused on features that have a greater impact on the outcome weights. Following Transformer  [12] , we use self-attention to perform feature selection on audio features. The self-attention mechanism is able to reflect the influence of feature neighboring elements, whose query, key, and value are all representations of the same audio modality under different projection spaces. The specific formula is as follows:\n\nwhere\n\nis represented by different projection spaces with different parameter matrices, where i represents the number of layers of transformer attention, i = 1, ..., D.\n\nWe feed the learned weight information into the full connection to obtain the feature adaptive learning results:\n\nwhere LN represents layer normalization, d f represents dimensions of the extracted features. This operation enables the higher-order features of the audio modality to perform feature selection, making it more focused on features that a greater impact on the outcome. Then, we make the automatically selected features and the video modality perform efficient inter-modal interactions. The module accepts input for two modalities, which is called XA âˆˆ R d f and XV âˆˆ R CÃ—SÃ—HÃ—W . We obtain the mapping representations of the features for the two modalities by a linear projection. And then we process the two representations by add and tanh activation function. Finally, the fused representation Xo âˆˆ R CÃ—SÃ—HÃ—W is obtained through sof tmax. We believe that the final fused information contains not only the complementary information of the two modalities, but also the features of the video modality. The specific formula is as follows:\n\nin which Wv âˆˆ R kÃ—C and Wa âˆˆ R kÃ—d f are linear transformation weights, and bv âˆˆ R k is the bias, where k is a pre-defined hyperparameter, and âŠ• represents the broadcast addition operation of a tensor and a vector. This operation enables high-level features to learn complementarily with low-level features, which complement the semantic features of low-level features and express richer information. In this process, to ensure that the information of the video modality is not lost, we ensure the integrity of the original structural features of the video modality through the residual structure.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Classification",
      "text": "Finally, we obtain the output of emotions I = [ Xo, XA] by splicing the obtained fused representation and the original representation.\n\nThe cross-entropy loss is used to optimize the model. The specific equation is shown as follows:\n\nwhere dout is the output dimensions of emotional categories, W1 âˆˆ R d out is the weight vectors, b1 is the bias, y = {y1, y2, ..., yn} T is the one-hot vector of the emotion label, Å· = {Å·1, Å·2, ..., Å·n} T is the predicted probability distribution, n is the number of emotion categories.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [16]  is a multimodal emotion recognition dataset containing 24 actors (12 male, 12 female) of 1440 video clips of short speeches. The dataset is performed when the actors are told the emotion to be expressed, with high quality in terms of both video and audio recordings. Eight emotions are included in the dataset: neutral, calm, happy, sad, angry, fearful, disgust and surprised.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation Details",
      "text": "For the video modality, we extract 30 consecutive images from each video. We crop the face region using the 2D face markers provided for each image and then resize to (224,224). Data augmentation is performed using random cropping, level flipping and normalization methods. For the audio modality, since the first 0.5 seconds usually do not contain sound, we trim the first 0.5 seconds and keep it consistent for the next 2.45 seconds. Following the suggestion of  [19] , we extract the first 13 MFCC features for each cropped audio clip. We perform 5-fold cross-validation on the RAVDESS dataset to provide more robust results. We divide the 24 actors into a training and a test set in a 5:1 ratio. Since the actors' gender is represented by an even or odd number of actor IDs, we enable gender to be evenly distributed by rotating 4 consecutive actor IDs as the test set for each fold of cross-validation. The model is trained using the Adam optimizer  [20]  with a learning rate of 0.001, and the entire training of the model is done on a single NVIDIA RTX 8000. The final accuracy reported is the average accuracy over the 5 folds.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baselines",
      "text": "For this task, we implement multiple recent multimodal fusion algorithms as our baselines. We categorize them into the following:\n\n1. Simple feature concatenation followed by fully connected layers based on  [21]  and MCBP  [22]  are two typical early fusion methods.\n\n2. Averaging and multiplication are the two standard late fusion methods that are adopted as the baselines.\n\n3. Multiplicative layer  [23]  is a late fusion method that adds a down-weighting factor to CE loss to suppress weaker modalities.\n\n4. MMTM  [24]  module allows slow fusion of information between modalities by adding to different feature layers, which allows the fusion of features in convolutional layers of different spatial dimensions.   [21]  Early 71.04 26.87M MCBP  [22]  Early 71.32 51.03M MMTM  [24]  Model 73.12 31.97M MSAF  [25]  Model 74.86 25.94M ERANNs  [26]  Model 74.80 -CFN-SR (Ours) Model 75.76 26.30M\n\n5. MSAF  [25]  module splits each channel into equal blocks of features in the channel direction and creates a joint representation that is used to generate soft notes for each channel across the feature blocks.\n\n6. ERANNs  [26]  is the state-of-the-art method by proposing a new convolutional neural network architecture for audiovideo emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparison To State-Of-The-Art Methods",
      "text": "Table  1  shows the accuracy comparison of the proposed method with baselines. From the table 1, we can see that our model achieves an accuracy of 75.76% with 26.30M number of parameters, reaching the state-of-the-art. Compared to the unimodal baseline, our network accuracy exceeds more than 10%, verifying the importance of multimodal fusion. In addition, the incorporation of the cross-modal fusion block only introduces a 30K number of parameters, but brings a significant performance improvement. Compared with early fusion methods, our method has more than 4% accuracy improvement, which shows that finding the association between video and audio modalities is a difficult task in the early stage. What's more, our proposed CFN-SR has 2.64% improvement over MMTM  [24]  and 0.9% improvement over the best-performing MSFA  [25] , and both have comparable number of parameters. This indicates that our model fully exploits the complementarity between the two modalities and reduces the redundant features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study",
      "text": "Table  2  shows the ablation experiments on the RAVDESS dataset.\n\nTo verify the effectiveness of the cross-modal blocks, we obtain the final sentiment by simply splicing the high-level semantic features of the two modalities. The experimental results show that our crossmodal block leads to a performance improvement of more than 2% with only a 0.4M increase in the number of parameters, which indicates that efficient complementary information from both modalities can have a large impact on the final decision. Meanwhile, we observe that the output of the results by simple splicing will also be better than the early fusion method, which indicates that the early fusion method cannot adequately express the high-level semantic features of both modalities. We further explore the validity of the internal structure of the cross-modal blocks. In the cross-modal blocks, the self-attention mechanism and the residual structure play an important role in the performance of the model. We detach the self-attention mechanism and the residual structure separately which can be seen that selfattention brings more than 2% impact on the final result. This indicates that the audio semantic features we obtained contain redundant information and can be selected by the self-attention mechanism for feature selection to make it efficient and adaptive for inter-modal interaction. In addition, we also see that the residual structure has less impact on the final results, suggesting that the inclusion of the residual structure helps to ensure that the loss of video features is minimized during the interaction. Furthermore, we observe that the current state-of-the-art can be achieved even without the inclusion of the residual structure, which further demonstrates the efficiency of the cross-modal blocks. More, we integrate the audio modality into the video modality as the final fusion result in our model design, denoted as V ->A. We have compared the results of integrating video modality into audio modality, denoted as A->V . We find that there is a 1.6% difference between them, which we attribute to the fact that the self-attention mechanism reduces the redundancy of audio features and is more effective for the results. In parallel, the rich spatio-temporal structure of the video modality also has an impact on the final result output.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a cross-modal fusion network based on self-attention and residual structure (CFN-SR) for multimodal emotion recognition. We design the cross-modal blocks, which fully considers the complementary information of different modalities and makes full use of inter-modal and intra-modal interactions to complete the transmission of semantic features. The inclusion of selfattention mechanism and residual structure can guarantee the efficiency and integrity of information interaction. We validate the effectiveness of the proposed method on the RAVDESS dataset. The experimental results show that the proposed method achieves stateof-the-art and obtains 75.76% accuracy with 26.30M number of parameters. In the future work, we will extend the module to explore efficient interactions between multiple modalities.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of CFN-SR. Left: the ï¬‚ow structure of the whole model, extracting the higher-order semantic features of",
      "page": 2
    },
    {
      "caption": "Figure 1: , we design a cross-modal fusion network based",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "4 Shanghai Key Laboratory of Mental Health and Psychological Crisis Intervention, School of"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "Psychology and Cognitive Science, East China Normal University, Shanghai, China"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "fusion is\nto extract and construct multiple modal data into corre-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "sponding modal\nfeatures before stitching them together\ninto a fea-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "ture set that integrates each modal feature. Late fusion is to ï¬nd out"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "the plausibility of each model, and then to coordinate and make joint"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "decisions. Recently, with the development of Transformer [12] for"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "natural\nlanguage processing and computer vision tasks, model\nfu-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "sion is often done using Transformer\nfor cross-modal\ninteractions,"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "with signiï¬cant performance improvements due to the ï¬‚exibility of"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "fusion locations.\nFor audio-video emotion recognition,\n[13]\nintro-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "duced Transformer to fuse audio and video representations. Differ-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "ent cross-modal fusion strategies were explored by [14].\n[10] pro-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "posed a novel\nrepresentation fusion method, Capsule Graph Con-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "volutional Network(CapsGCN),\nto use graph capsule networks for"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "audio and video emotion recognition."
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": ""
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "However, existing methods ignore the complementary informa-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": ""
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "tion between different modalities, and the ï¬nal decision often re-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": ""
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "quires the joint decision of the two modality-speciï¬c features as well"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": ""
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "as the fused features. The complementary information ensures that"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": ""
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "there is an improvement\nin performance when the fusion module is"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": ""
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "added.\nIn addition,\nthere is still a signiï¬cant amount of redundancy"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": ""
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "in the way existing methods fuse.\nSome models are only stitched"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": ""
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "together for the ï¬nal result output, and the stitched result will have"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": ""
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "many duplicate representations, and the feature information needs to"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": ""
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "be ï¬ltered to further reduce the redundant features before stitching."
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": ""
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "Moreover, the existing methods cannot guarantee the information in-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "tegrity in the feature learning process, and often the learning of intra-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": ""
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "and inter-modal information will lose some semantic information."
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": ""
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "To solve the above problems, we propose a novel cross-modal"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "fusion network based on self-attention and residual structure (CFN-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "SR) for multimodal emotion recognition. Speciï¬cally, we ï¬rst per-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "form representation learning for audio modality and video modality."
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "The spatio-temporal structure features of video frame sequences are"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "obtained by ResNeXt [15]. The MFCC features of audio sequences"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "are obtained by a simple and effective 1D CNN. Secondly, we feed"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "the features into the cross-modal blocks separately, and make the"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "audio modality perform intra-modal feature selection through a self-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "attention mechanism, which will enable the selected features to in-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "teract with the video modality adaptively and efï¬ciently between"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "modalities.\nThe integrity of\nthe original structural\nfeatures of\nthe"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "video modality can be ensured by the residual structure. Finally, we"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "obtain the output of emotions by splicing the obtained fused repre-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "sentation and the original representation.We apply the model\nto the"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "RAVDESS [16] multimodal emotion recognition dataset, and the ex-"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "perimental\nresults demonstrate that our proposed CFN-SR is more"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "effective, and our method achieves state-of-the-art compared with"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "other models, obtaining an accuracy of 75.76% with a parametric"
        },
        {
          "3 Shanghai University of International Business and Economics, Shanghai, China": "number of 26.30M."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "MFCC frames"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "Fig. 1: The overall architecture of CFN-SR. Left:\nthe ï¬‚ow structure of the whole model, extracting the higher-order semantic features of"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "video and audio by ResNeXt and 1D CNN,\nrespectively. Right:\nthe cross-modal\nfusion blocks, which enables the complementarity and"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "completeness of modal interactions to play a role through the introduction of self-attention mechanism and residual structure."
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "2. METHODOLOGY"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "As shown in Figure 1, we design a cross-modal fusion network based"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "(3)\nXA = F latten(BN (ReLU (Conv1D( Ë†XA, kA)))))"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "on self-attention and residual structure. Firstly, we use the 3D CNN"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "to obtain the video features and the 1D CNN to obtain the audio fea-"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "tures. Then, we obtain the inter- and intra-modal fusion representa-"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "2.2. Video Encoder"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "tions of the two modalities by the cross-modal fusion blocks. Finally,"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "we obtain the output of the emotions by splicing the obtained fusion"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "Video data are dependent\nin both spatial and temporal dimensions,"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "representation with the original representation. We will describe this"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "thus a network with 3D convolutional kernels is needed to learn fa-"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "process in detail below."
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "cial expressions and actions. We consider both the performance and"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "training efï¬ciency of the network and choose the 3D ResNeXt [15]"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "2.1. Audio Encoder"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "network to obtain the spatio-temporal\nstructural\nfeatures of video"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "modalities. ResNeXt proposes a group convolution strategy between"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "For the audio modality, recent work [17, 18] has demonstrated the ef-"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "the deep segmentation convolution of ordinary convolutional ker-"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "fectiveness of deep learning methods based on Mel Frequency Cep-"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "nels, and achieves a balance between the two strategies by control-"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "strum Coefï¬cient (MFCC) features. We design a simple and efï¬cient"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "ling the number of groups with a simple structure but powerful per-"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "1D CNN to perform MFCC feature extraction. Speciï¬cally, we use"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "formance. We use feature preprocessed audio modal features as in-"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "the feature preprocessed audio modal features as input, denoted as"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "put, denoted as XV . We obtain the higher-order semantic features"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "XA. We ï¬rst pass the features through a 2-layer convolution oper-"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "of video modalities by this network:"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "ation to extract\nthe local features of adjacent audio elements. After"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "that, we use the max-pooling to downsample, compress the features,"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "and remove redundant\ninformation. The speciï¬c equation is as fol-"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "(4)\nXV = ResN eXt50(XV ) âˆˆ RCÃ—SÃ—HÃ—W"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "lows:"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "Ë†"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "where\ndenotes the learned semantic features, C, S, H and W\nXV"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "Ë†"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "(1)\nXA = BN (ReLU (Conv1D(XA, kA)))\nare the number of channels, sequence length, height, and width, re-"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "spectively. After obtaining the higher-order semantic features, we"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "(2)\nXA = Dropout(BN (M axP ool( Ë†XA)))"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "feed them into the cross-modal blocks and fuse the audio feature"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "representations. We believe that\nthe ï¬nal\nfused representation ob-\nthe\nwhere BN stands for Batch Normalization, kA is the size of"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "Ë†"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "tained contains not only the higher-order semantic features of\nthe\nconvolution kernel of modality audio and\nXA denotes the learned"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "video modality, but also the interactive features of the two modali-\nsemantic features. We again feed the learned features into a 1D tem-"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "ties. After that, we perform downsampling using the average pool to\nporal convolution to obtain the higher-order semantic features of the"
        },
        {
          "ğ‘‹ğ´ âˆˆ ğ‘…ğ‘‡ğ‘  Ã— ğ‘‘ğ‘“": "reduce redundant information.\naudio. Finally, we ï¬‚atten the obtained features:"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "equation is shown as follows:"
        },
        {
          "2.3. Cross-modal Blocks": "Through encoding operations for both modalities, we obtain higher-",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "order semantic features for both audio and video modalities. To be",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "(9)\nprediction = W1I + b1 âˆˆ Rdout"
        },
        {
          "2.3. Cross-modal Blocks": "able to make the ï¬nal decision more accurate, we exploit\nthe com-",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "plementary intra- and inter-modal\ninteraction information between",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "(cid:88) i\nL = âˆ’\n(10)\nyilog(Ë†yi)"
        },
        {
          "2.3. Cross-modal Blocks": "the two modalities.\nSpeciï¬cally, we ï¬rst make the audio modality",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "undergo intramodal representation learning through a self-attention",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "where dout\nis the output dimensions of emotional categories, W1 âˆˆ"
        },
        {
          "2.3. Cross-modal Blocks": "mechanism. This operation allows adaptive learning of higher-order",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "Rdout\nis the weight vectors, b1 is the bias, y = {y1, y2, ..., yn}T"
        },
        {
          "2.3. Cross-modal Blocks": "semantic features of the audio modality, making it more focused on",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "is the one-hot vector of\nthe emotion label, Ë†y = {Ë†y1, Ë†y2, ..., Ë†yn}T"
        },
        {
          "2.3. Cross-modal Blocks": "features that have a greater\nimpact on the outcome weights.\nFol-",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "is the predicted probability distribution, n is the number of emotion"
        },
        {
          "2.3. Cross-modal Blocks": "lowing Transformer\n[12], we use self-attention to perform feature",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "categories."
        },
        {
          "2.3. Cross-modal Blocks": "selection on audio features. The self-attention mechanism is able to",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "reï¬‚ect\nthe inï¬‚uence of feature neighboring elements, whose query,",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "3. EXPERIMENTS"
        },
        {
          "2.3. Cross-modal Blocks": "key, and value are all representations of the same audio modality un-",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "der different projection spaces. The speciï¬c formula is as follows:",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "3.1. Datasets"
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "The Ryerson Audio-Visual Database of Emotional Speech and Song"
        },
        {
          "2.3. Cross-modal Blocks": "QK T",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "âˆš\nself âˆ’ attention(Q, K, V ) = sof tmax(\n)V\n(5)",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "(RAVDESS) [16] is a multimodal emotion recognition dataset con-"
        },
        {
          "2.3. Cross-modal Blocks": "dk",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "taining 24 actors (12 male, 12 female) of 1440 video clips of short"
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "speeches. The dataset is performed when the actors are told the emo-"
        },
        {
          "2.3. Cross-modal Blocks": "where Q, K, V denotes Z [iâˆ’1]\n. Z [iâˆ’1]\nis represented by different\nA\nA",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "tion to be expressed, with high quality in terms of both video and au-"
        },
        {
          "2.3. Cross-modal Blocks": "projection spaces with different parameter matrices, where i repre-",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "dio recordings. Eight emotions are included in the dataset: neutral,"
        },
        {
          "2.3. Cross-modal Blocks": "sents the number of\nlayers of\ntransformer attention,\ni = 1, ..., D.",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "calm, happy, sad, angry, fearful, disgust and surprised."
        },
        {
          "2.3. Cross-modal Blocks": "We feed the learned weight\ninformation into the full connection to",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "obtain the feature adaptive learning results:",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "3.2.\nImplementation Details"
        },
        {
          "2.3. Cross-modal Blocks": "(6)\nXA = LN (Z [i]\nA )) âˆˆ Rdf\nA + F eedf orward(Z [i]",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "For the video modality, we extract 30 consecutive images from each"
        },
        {
          "2.3. Cross-modal Blocks": "where LN represents layer normalization, df represents dimensions",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "video. We crop the face region using the 2D face markers provided"
        },
        {
          "2.3. Cross-modal Blocks": "of\nthe extracted features.\nThis operation enables the higher-order",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "for each image and then resize to (224,224). Data augmentation is"
        },
        {
          "2.3. Cross-modal Blocks": "features of the audio modality to perform feature selection, making it",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "performed using random cropping,\nlevel ï¬‚ipping and normalization"
        },
        {
          "2.3. Cross-modal Blocks": "more focused on features that have a greater impact on the outcome.",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "methods. For the audio modality, since the ï¬rst 0.5 seconds usually"
        },
        {
          "2.3. Cross-modal Blocks": "Then, we make the automatically selected features and the video",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "do not contain sound, we trim the ï¬rst 0.5 seconds and keep it con-"
        },
        {
          "2.3. Cross-modal Blocks": "modality perform efï¬cient\ninter-modal\ninteractions.\nThe module",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "sistent for the next 2.45 seconds. Following the suggestion of [19],"
        },
        {
          "2.3. Cross-modal Blocks": "and\naccepts input\nfor\ntwo modalities, which is called\nXA âˆˆ Rdf",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "we extract the ï¬rst 13 MFCC features for each cropped audio clip."
        },
        {
          "2.3. Cross-modal Blocks": "XV âˆˆ RCÃ—SÃ—HÃ—W . We obtain the mapping representations of the",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "We perform 5-fold cross-validation on the RAVDESS dataset to"
        },
        {
          "2.3. Cross-modal Blocks": "features for the two modalities by a linear projection. And then we",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "provide more robust results. We divide the 24 actors into a training"
        },
        {
          "2.3. Cross-modal Blocks": "process the two representations by add and tanh activation function.",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "and a test set in a 5:1 ratio. Since the actorsâ€™ gender is represented by"
        },
        {
          "2.3. Cross-modal Blocks": "Finally,\nthe fused representation\nXo âˆˆ RCÃ—SÃ—HÃ—W is obtained",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "an even or odd number of actor IDs, we enable gender to be evenly"
        },
        {
          "2.3. Cross-modal Blocks": "through sof tmax. We believe that the ï¬nal fused information con-",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "distributed by rotating 4 consecutive actor IDs as the test set for each"
        },
        {
          "2.3. Cross-modal Blocks": "tains not only the complementary information of the two modalities,",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "fold of cross-validation. The model\nis trained using the Adam opti-"
        },
        {
          "2.3. Cross-modal Blocks": "but also the features of the video modality. The speciï¬c formula is",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "mizer [20] with a learning rate of 0.001, and the entire training of the"
        },
        {
          "2.3. Cross-modal Blocks": "as follows:",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "model\nis done on a single NVIDIA RTX 8000. The ï¬nal accuracy"
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "reported is the average accuracy over the 5 folds."
        },
        {
          "2.3. Cross-modal Blocks": "(7)\nXq = tanh((Wv Ë†XV + bv) + Wa Ë†XA) âˆˆ RkÃ—SÃ—HÃ—W",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "3.3. Baselines"
        },
        {
          "2.3. Cross-modal Blocks": "(8)\nXo = (sof tmax( Ë†Xq) âŠ— Ë†XV ) âŠ• Ë†XV âˆˆ RCÃ—SÃ—HÃ—W",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "For this task, we implement multiple recent multimodal fusion algo-"
        },
        {
          "2.3. Cross-modal Blocks": "are linear transformation\nin which Wv âˆˆ RkÃ—C and Wa âˆˆ RkÃ—df",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "rithms as our baselines. We categorize them into the following:"
        },
        {
          "2.3. Cross-modal Blocks": "weights, and bv âˆˆ Rk is the bias, where k is a pre-deï¬ned hyper-",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "1.\nSimple\nfeature\nconcatenation followed by fully connected"
        },
        {
          "2.3. Cross-modal Blocks": "parameter, and âŠ• represents the broadcast addition operation of a",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "layers based on [21] and MCBP [22] are two typical early"
        },
        {
          "2.3. Cross-modal Blocks": "tensor and a vector.",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "fusion methods."
        },
        {
          "2.3. Cross-modal Blocks": "This operation enables high-level features to learn complemen-",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "tarily with low-level features, which complement\nthe semantic fea-",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "2. Averaging and multiplication are the two standard late fusion"
        },
        {
          "2.3. Cross-modal Blocks": "tures of\nlow-level\nfeatures and express richer\ninformation.\nIn this",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "methods that are adopted as the baselines."
        },
        {
          "2.3. Cross-modal Blocks": "process,\nto ensure that\nthe information of the video modality is not",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "3. Multiplicative layer [23] is a late fusion method that adds a"
        },
        {
          "2.3. Cross-modal Blocks": "lost, we ensure the integrity of the original structural features of the",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "down-weighting factor to CE loss to suppress weaker modal-"
        },
        {
          "2.3. Cross-modal Blocks": "video modality through the residual structure.",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": ""
        },
        {
          "2.3. Cross-modal Blocks": "",
          "The cross-entropy loss is used to optimize the model. The speciï¬c": "ities."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: shows the ablation experiments on the RAVDESS dataset.",
      "data": [
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": ""
        },
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": "cates that the audio semantic features we obtained contain redundant"
        },
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": "information and can be selected by the self-attention mechanism for"
        },
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": "feature selection to make it efï¬cient and adaptive for\ninter-modal"
        },
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": "interaction.\nIn addition, we also see that\nthe residual structure has"
        },
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": "less impact on the ï¬nal results, suggesting that\nthe inclusion of the"
        },
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": "residual structure helps to ensure that\nthe loss of video features is"
        },
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": "minimized during the interaction. Furthermore, we observe that the"
        },
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": "current state-of-the-art can be achieved even without\nthe inclusion"
        },
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": ""
        },
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": "of the residual structure, which further demonstrates the efï¬ciency"
        },
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": ""
        },
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": "of the cross-modal blocks."
        },
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": ""
        },
        {
          "attention brings more than 2% impact on the ï¬nal result. This indi-": "More, we integrate the audio modality into the video modality"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: shows the ablation experiments on the RAVDESS dataset.",
      "data": [
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "for emotion recognition on RAVDESS.",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "Model\nAccuracy\n#Params"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "Model\nFusion Stage\nAccuracy\n#Params",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "CFN-SR\n75.76\n26.30M"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "3D RexNeXt50 (Vid.)\n-\n62.99\n25.88M",
          "Table 2: Ablation study on the RAVDESS dataset.": "w/o Cross-modal Blocks\n73.50\n25.92M"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "1D CNN (Aud.)\n-\n56.53\n0.03M",
          "Table 2: Ablation study on the RAVDESS dataset.": "V->A Cross-modal\n74.15\n25.67M"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "Averaging\nLate\n68.82\n25.92M",
          "Table 2: Ablation study on the RAVDESS dataset.": "A->V Cross-modal\n75.76\n26.30M"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "Multiplicative Î²=0.3\nLate\n70.35\n25.92M",
          "Table 2: Ablation study on the RAVDESS dataset.": "w/o self-attention\n73.86\n26.05M"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "Multiplication\nLate\n70.56\n25.92M",
          "Table 2: Ablation study on the RAVDESS dataset.": "w/o residual\n75.33\n26.30M"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "Concat + FC [21]\nEarly\n71.04\n26.87M",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "MCBP [22]\nEarly\n71.32\n51.03M",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "MMTM [24]\nModel\n73.12\n31.97M",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "performance of the model. We detach the self-attention mechanism"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "MSAF [25]\nModel\n74.86\n25.94M",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "and the residual\nstructure separately which can be seen that\nself-"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "ERANNs [26]\nModel\n74.80\n-",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "attention brings more than 2% impact on the ï¬nal result. This indi-"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "CFN-SR (Ours)\n75.76\n26.30M\nModel",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "cates that the audio semantic features we obtained contain redundant"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "information and can be selected by the self-attention mechanism for"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "feature selection to make it efï¬cient and adaptive for\ninter-modal"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "5. MSAF [25] module splits each channel\ninto equal blocks of",
          "Table 2: Ablation study on the RAVDESS dataset.": "interaction.\nIn addition, we also see that\nthe residual structure has"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "features\nin the channel direction and creates a joint\nrepre-",
          "Table 2: Ablation study on the RAVDESS dataset.": "less impact on the ï¬nal results, suggesting that\nthe inclusion of the"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "sentation that\nis used to generate soft notes for each channel",
          "Table 2: Ablation study on the RAVDESS dataset.": "residual structure helps to ensure that\nthe loss of video features is"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "across the feature blocks.",
          "Table 2: Ablation study on the RAVDESS dataset.": "minimized during the interaction. Furthermore, we observe that the"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "current state-of-the-art can be achieved even without\nthe inclusion"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "6. ERANNs\n[26]\nis\nthe state-of-the-art method by proposing",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "of the residual structure, which further demonstrates the efï¬ciency"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "a new convolutional neural network architecture for audio-",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "of the cross-modal blocks."
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "video emotion recognition.",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "More, we integrate the audio modality into the video modality"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "as the ï¬nal fusion result in our model design, denoted as V ->A. We"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "3.4. Comparison to State-of-the-art Methods",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "have compared the results of integrating video modality into audio"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "modality, denoted as A->V . We ï¬nd that there is a 1.6% difference"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "Table 1 shows the accuracy comparison of the proposed method with",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "between them, which we attribute to the fact\nthat\nthe self-attention"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "baselines. From the table 1, we can see that our model achieves an",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "mechanism reduces the redundancy of audio features and is more"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "accuracy of 75.76% with 26.30M number of parameters,\nreaching",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "effective for the results. In parallel, the rich spatio-temporal structure"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "the state-of-the-art.\nCompared to the unimodal baseline, our net-",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "of the video modality also has an impact on the ï¬nal result output."
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "work accuracy exceeds more than 10%, verifying the importance of",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "multimodal fusion. In addition, the incorporation of the cross-modal",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "fusion block only introduces a 30K number of parameters, but brings",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "4. CONCLUSION"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "a signiï¬cant performance improvement. Compared with early fu-",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "sion methods, our method has more than 4% accuracy improvement,",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "In this paper, we propose a cross-modal\nfusion network based on"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "which shows that ï¬nding the association between video and audio",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "self-attention and residual structure (CFN-SR) for multimodal emo-"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "modalities is a difï¬cult task in the early stage. Whatâ€™s more, our pro-",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "tion recognition. We design the cross-modal blocks, which fully"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "posed CFN-SR has 2.64% improvement over MMTM [24] and 0.9%",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "considers the complementary information of different modalities and"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "improvement over\nthe best-performing MSFA [25], and both have",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "makes full use of inter-modal and intra-modal\ninteractions to com-"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "comparable number of parameters.\nThis indicates that our model",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "plete the transmission of semantic features. The inclusion of self-"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "fully exploits the complementarity between the two modalities and",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "attention mechanism and residual structure can guarantee the efï¬-"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "reduces the redundant features.",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "ciency and integrity of information interaction. We validate the ef-"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "fectiveness of the proposed method on the RAVDESS dataset. The"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "3.5. Ablation Study",
          "Table 2: Ablation study on the RAVDESS dataset.": "experimental results show that\nthe proposed method achieves state-"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "of-the-art and obtains 75.76% accuracy with 26.30M number of pa-"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "Table 2 shows the ablation experiments on the RAVDESS dataset.",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "rameters.\nIn the future work, we will extend the module to explore"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "To verify the effectiveness of the cross-modal blocks, we obtain the",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "",
          "Table 2: Ablation study on the RAVDESS dataset.": "efï¬cient interactions between multiple modalities."
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "ï¬nal sentiment by simply splicing the high-level semantic features",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "of the two modalities. The experimental results show that our cross-",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "modal block leads to a performance improvement of more than 2%",
          "Table 2: Ablation study on the RAVDESS dataset.": "5. ACKNOWLEDGEMENTS"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "with only a 0.4M increase in the number of parameters, which indi-",
          "Table 2: Ablation study on the RAVDESS dataset.": ""
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "cates that efï¬cient complementary information from both modalities",
          "Table 2: Ablation study on the RAVDESS dataset.": "This work is supported by the National Natural Science Foundation"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "can have a large impact on the ï¬nal decision. Meanwhile, we observe",
          "Table 2: Ablation study on the RAVDESS dataset.": "of China (No.82071171), Beijing Municipal Natural Science Foun-"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "that\nthe output of\nthe results by simple splicing will also be better",
          "Table 2: Ablation study on the RAVDESS dataset.": "dation (No.L192026),\nthe Science and Technology Commission of"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "than the early fusion method, which indicates that\nthe early fusion",
          "Table 2: Ablation study on the RAVDESS dataset.": "Shanghai Municipality (No.19511120601),\nthe Scientiï¬c and Tech-"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "method cannot adequately express the high-level semantic features",
          "Table 2: Ablation study on the RAVDESS dataset.": "nological Innovation 2030 Major Projects (No.2018AAA0100902),"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "of both modalities.",
          "Table 2: Ablation study on the RAVDESS dataset.": "the Research Project of Shanghai Science and Technology Commis-"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "We further explore the validity of\nthe internal structure of\nthe",
          "Table 2: Ablation study on the RAVDESS dataset.": "sion (20dz2260300) and the Fundamental Research Funds for\nthe"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "cross-modal blocks.\nIn the cross-modal blocks,\nthe self-attention",
          "Table 2: Ablation study on the RAVDESS dataset.": "Central Universities. We would like to thank the anonymous review-"
        },
        {
          "Table 1: Comparison between multimodal fusion baselines and ours": "mechanism and the residual structure play an important role in the",
          "Table 2: Ablation study on the RAVDESS dataset.": "ers for their valuable suggestions."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "ference on Acoustics, Speech and Signal Processing (ICASSP)."
        },
        {
          "6. REFERENCES": "[1] Wenliang Dai, Samuel Cahyawijaya, Zihan Liu, and Pascale",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "IEEE, 2020, pp. 3507â€“3511."
        },
        {
          "6. REFERENCES": "Fung,\nâ€œMultimodal\nend-to-end sparse model\nfor\nemotion",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "[14] Hengshun Zhou, Debin Meng, Yuanyuan Zhang, Xiaojiang"
        },
        {
          "6. REFERENCES": "recognition,â€ arXiv preprint arXiv:2103.09666, 2021.",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "Peng, Jun Du, Kai Wang, and Yu Qiao,\nâ€œExploring emotion"
        },
        {
          "6. REFERENCES": "[2] Yuanyuan Zhang, Zi-Rui Wang, and Jun Du,\nâ€œDeep fusion:",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "features and fusion strategies for audio-video emotion recogni-"
        },
        {
          "6. REFERENCES": "An attention guided factorized bilinear pooling for audio-video",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "tion,â€\nin 2019 International Conference on Multimodal Inter-"
        },
        {
          "6. REFERENCES": "emotion recognition,â€\nin 2019 International Joint Conference",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "action, 2019, pp. 562â€“566."
        },
        {
          "6. REFERENCES": "on Neural Networks (IJCNN). IEEE, 2019, pp. 1â€“8.",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "[15]\nSaining Xie, Ross Girshick, Piotr DollÂ´ar, Zhuowen Tu, and"
        },
        {
          "6. REFERENCES": "[3] Qi Cao, Mixiao Hou, Bingzhi Chen, Zheng Zhang, and Guang-",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "Kaiming He,\nâ€œAggregated residual\ntransformations for deep"
        },
        {
          "6. REFERENCES": "ming Lu,\nâ€œHierarchical network based on the fusion of static",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "the\nIEEE conference\nneural networks,â€\nin Proceedings of"
        },
        {
          "6. REFERENCES": "and dynamic\nfeatures\nfor\nspeech emotion recognition,â€\nin",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "on computer vision and pattern recognition, 2017, pp. 1492â€“"
        },
        {
          "6. REFERENCES": "ICASSP 2021-2021 IEEE International Conference on Acous-",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "1500."
        },
        {
          "6. REFERENCES": "tics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp.",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "[16]\nSteven R Livingstone and Frank A Russo, â€œThe ryerson audio-"
        },
        {
          "6. REFERENCES": "6334â€“6338.",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "visual database of emotional speech and song (ravdess): A dy-"
        },
        {
          "6. REFERENCES": "[4]\nJinting Wu, Yujia Zhang, Xiaoguang Zhao, and Wenbin Gao,",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "namic, multimodal set of facial and vocal expressions in north"
        },
        {
          "6. REFERENCES": "â€œA generalized zero-shot\nframework for emotion recognition",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "american english,â€\nPloS one, vol. 13, no. 5, pp. e0196391,"
        },
        {
          "6. REFERENCES": "from body gestures,â€ arXiv preprint arXiv:2010.06362, 2020.",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "2018."
        },
        {
          "6. REFERENCES": "[5]\nFernanda HernÂ´andez-Luquin and Hugo Jair Escalante, â€œMulti-",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "[17] Natalia Neverova, Christian Wolf, Graham Taylor, and Florian"
        },
        {
          "6. REFERENCES": "branch deep radial basis function networks for facial emotion",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "Nebout,\nâ€œModdrop:\nadaptive multi-modal gesture recogni-"
        },
        {
          "6. REFERENCES": "recognition,â€ Neural Computing and Applications, pp. 1â€“15,",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "IEEE Transactions on Pattern Analysis and Machine\ntion,â€"
        },
        {
          "6. REFERENCES": "2021.",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "Intelligence, vol. 38, no. 8, pp. 1692â€“1706, 2015."
        },
        {
          "6. REFERENCES": "[6] Usman Naseem,\nImran\nRazzak,\nKatarzyna Musial,\nand",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "[18]\nJianyou Wang, Michael Xue, Ryan Culhane, Enmao Diao, Jie"
        },
        {
          "6. REFERENCES": "Muhammad Imran,\nâ€œTransformer based deep intelligent con-",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "Ding, and Vahid Tarokh,\nâ€œSpeech emotion recognition with"
        },
        {
          "6. REFERENCES": "textual embedding for twitter sentiment analysis,â€ Future Gen-",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "dual-sequence lstm architecture,â€ in ICASSP 2020-2020 IEEE"
        },
        {
          "6. REFERENCES": "eration Computer Systems, vol. 113, pp. 58â€“69, 2020.",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "International Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "cessing (ICASSP). IEEE, 2020, pp. 6474â€“6478."
        },
        {
          "6. REFERENCES": "[7]\nSamarth Tripathi,\nSarthak Tripathi,\nand Homayoon Beigi,",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "â€œMulti-modal emotion recognition on iemocap dataset using",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "[19] Qin Jin, Chengxin Li, Shizhe Chen, and Huimin Wu, â€œSpeech"
        },
        {
          "6. REFERENCES": "deep learning,â€ arXiv preprint arXiv:1804.05788, 2018.",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "emotion recognition with acoustic and lexical\nfeatures,â€\nin"
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "2015 IEEE international conference on acoustics, speech and"
        },
        {
          "6. REFERENCES": "[8]\nShiqing Zhang, Shiliang Zhang, Tiejun Huang, Wen Gao, and",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "signal processing (ICASSP). IEEE, 2015, pp. 4749â€“4753."
        },
        {
          "6. REFERENCES": "Qi Tian, â€œLearning affective features with a hybrid deep model",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "IEEE Transactions on\nfor audioâ€“visual emotion recognition,â€",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "[20] Diederik P Kingma and Jimmy Ba,\nâ€œAdam: A method for"
        },
        {
          "6. REFERENCES": "Circuits and Systems for Video Technology, vol. 28, no. 10, pp.",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "arXiv\npreprint\nstochastic\noptimization,â€\narXiv:1412.6980,"
        },
        {
          "6. REFERENCES": "3030â€“3043, 2017.",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "2014."
        },
        {
          "6. REFERENCES": "[9] Bagus Tris Atmaja and Masato Akagi, â€œMultitask learning and",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "[21]\nJuan DS Ortega, Mohammed\nSenoussaoui,\nEric Granger,"
        },
        {
          "6. REFERENCES": "multistage fusion for dimensional audiovisual emotion recog-",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "Marco Pedersoli, Patrick Cardinal, and Alessandro L Koerich,"
        },
        {
          "6. REFERENCES": "nition,â€ in ICASSP 2020-2020 IEEE International Conference",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "â€œMultimodal fusion with deep neural networks for audio-video"
        },
        {
          "6. REFERENCES": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE,",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "emotion recognition,â€ arXiv preprint arXiv:1907.03196, 2019."
        },
        {
          "6. REFERENCES": "2020, pp. 4482â€“4486.",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "[22] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach,"
        },
        {
          "6. REFERENCES": "[10]\nJiaxing Liu, Sen Chen, Longbiao Wang, Zhilei Liu, Yahui Fu,",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "Trevor Darrell,\nand Marcus Rohrbach,\nâ€œMultimodal\ncom-"
        },
        {
          "6. REFERENCES": "Lili Guo, and Jianwu Dang,\nâ€œMultimodal emotion recogni-",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "pact bilinear pooling for visual question answering and visual"
        },
        {
          "6. REFERENCES": "tion with capsule graph convolutional based representation fu-",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "grounding,â€ arXiv preprint arXiv:1606.01847, 2016."
        },
        {
          "6. REFERENCES": "sion,â€\nin ICASSP 2021-2021 IEEE International Conference",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "[23] Kuan Liu, Yanen Li, Ning Xu, and Prem Natarajan,\nâ€œLearn"
        },
        {
          "6. REFERENCES": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE,",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "arXiv\nto combine modalities\nin multimodal deep learning,â€"
        },
        {
          "6. REFERENCES": "2021, pp. 6339â€“6343.",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "preprint arXiv:1805.11730, 2018."
        },
        {
          "6. REFERENCES": "[11] Licai Sun, Bin Liu, Jianhua Tao, and Zheng Lian, â€œMultimodal",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "[24] Hamid Reza Vaezi\nJoze, Amirreza Shaban, Michael L Iuz-"
        },
        {
          "6. REFERENCES": "cross-and self-attention network for speech emotion recogni-",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "zolino, and Kazuhito Koishida,\nâ€œMmtm: Multimodal\ntransfer"
        },
        {
          "6. REFERENCES": "tion,â€\nin ICASSP 2021-2021 IEEE International Conference",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "module for cnn fusion,â€ in Proceedings of the IEEE/CVF Con-"
        },
        {
          "6. REFERENCES": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE,",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "ference on Computer Vision and Pattern Recognition, 2020, pp."
        },
        {
          "6. REFERENCES": "2021, pp. 4275â€“4279.",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "13289â€“13299."
        },
        {
          "6. REFERENCES": "[12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "[25] Lang\nSu,\nChuqing\nHu,\nGuofa\nLi,\nand\nDongpu\nCao,"
        },
        {
          "6. REFERENCES": "Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polo-",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "arXiv preprint\nâ€œMsaf: Multimodal\nsplit\nattention fusion,â€"
        },
        {
          "6. REFERENCES": "sukhin, â€œAttention is all you need,â€ in Proceedings of the 31st",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "arXiv:2012.07175, 2020."
        },
        {
          "6. REFERENCES": "International Conference on Neural\nInformation Processing",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "Systems, Red Hook, NY, USA, 2017, NIPSâ€™17, p. 6000â€“6010,",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "[26]\nSergey Verbitskiy and Viacheslav Vyshegorodtsev,\nâ€œEranns:"
        },
        {
          "6. REFERENCES": "Curran Associates Inc.",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "Efï¬cient\nresidual\naudio\nneural\nnetworks\nfor\naudio\npattern"
        },
        {
          "6. REFERENCES": "",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": "recognition,â€ arXiv preprint arXiv:2106.01621, 2021."
        },
        {
          "6. REFERENCES": "[13]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        },
        {
          "6. REFERENCES": "Niu,\nâ€œMultimodal\ntransformer fusion for continuous emotion",
          "recognition,â€\nin ICASSP 2020-2020 IEEE International Con-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Multimodal end-to-end sparse model for emotion recognition",
      "authors": [
        "Wenliang Dai",
        "Samuel Cahyawijaya",
        "Zihan Liu",
        "Pascale Fung"
      ],
      "year": "2021",
      "venue": "Multimodal end-to-end sparse model for emotion recognition",
      "arxiv": "arXiv:2103.09666"
    },
    {
      "citation_id": "3",
      "title": "Deep fusion: An attention guided factorized bilinear pooling for audio-video emotion recognition",
      "authors": [
        "Yuanyuan Zhang",
        "Zi-Rui Wang",
        "Jun Du"
      ],
      "year": "2019",
      "venue": "2019 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "4",
      "title": "Hierarchical network based on the fusion of static and dynamic features for speech emotion recognition",
      "authors": [
        "Qi Cao",
        "Mixiao Hou",
        "Bingzhi Chen",
        "Zheng Zhang",
        "Guangming Lu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "A generalized zero-shot framework for emotion recognition from body gestures",
      "authors": [
        "Jinting Wu",
        "Yujia Zhang",
        "Xiaoguang Zhao",
        "Wenbin Gao"
      ],
      "year": "2020",
      "venue": "A generalized zero-shot framework for emotion recognition from body gestures",
      "arxiv": "arXiv:2010.06362"
    },
    {
      "citation_id": "6",
      "title": "Multibranch deep radial basis function networks for facial emotion recognition",
      "authors": [
        "Fernanda HernÃ¡ndez-Luquin",
        "Hugo Escalante"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "7",
      "title": "Transformer based deep intelligent contextual embedding for twitter sentiment analysis",
      "authors": [
        "Usman Naseem",
        "Imran Razzak",
        "Katarzyna Musial",
        "Muhammad Imran"
      ],
      "year": "2020",
      "venue": "Future Generation Computer Systems"
    },
    {
      "citation_id": "8",
      "title": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "authors": [
        "Samarth Tripathi",
        "Sarthak Tripathi",
        "Homayoon Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "arxiv": "arXiv:1804.05788"
    },
    {
      "citation_id": "9",
      "title": "Learning affective features with a hybrid deep model for audio-visual emotion recognition",
      "authors": [
        "Shiqing Zhang",
        "Shiliang Zhang",
        "Tiejun Huang",
        "Wen Gao",
        "Qi Tian"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "10",
      "title": "Multitask learning and multistage fusion for dimensional audiovisual emotion recognition",
      "authors": [
        "Bagus Tris",
        "Masato Akagi"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Multimodal emotion recognition with capsule graph convolutional based representation fusion",
      "authors": [
        "Jiaxing Liu",
        "Sen Chen",
        "Longbiao Wang",
        "Zhilei Liu",
        "Yahui Fu",
        "Lili Guo",
        "Jianwu Dang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Multimodal cross-and self-attention network for speech emotion recognition",
      "authors": [
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao",
        "Zheng Lian"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "14",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "Jian Huang",
        "Jianhua Tao",
        "Bin Liu",
        "Zheng Lian",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Exploring emotion features and fusion strategies for audio-video emotion recognition",
      "authors": [
        "Hengshun Zhou",
        "Debin Meng",
        "Yuanyuan Zhang",
        "Xiaojiang Peng",
        "Jun Du",
        "Kai Wang",
        "Yu Qiao"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "16",
      "title": "Aggregated residual transformations for deep neural networks",
      "authors": [
        "Saining Xie",
        "Ross Girshick",
        "Piotr DollÃ¡r",
        "Zhuowen Tu",
        "Kaiming He"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "17",
      "title": "The ryerson audiovisual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "18",
      "title": "Moddrop: adaptive multi-modal gesture recognition",
      "authors": [
        "Natalia Neverova",
        "Christian Wolf",
        "Graham Taylor",
        "Florian Nebout"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "Jianyou Wang",
        "Michael Xue",
        "Ryan Culhane",
        "Enmao Diao",
        "Jie Ding",
        "Vahid Tarokh"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Qin Jin",
        "Chengxin Li",
        "Shizhe Chen",
        "Huimin Wu"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "21",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "22",
      "title": "Multimodal fusion with deep neural networks for audio-video emotion recognition",
      "authors": [
        "Juan Ds Ortega",
        "Mohammed Senoussaoui",
        "Eric Granger",
        "Marco Pedersoli",
        "Patrick Cardinal",
        "Alessandro Koerich"
      ],
      "year": "2019",
      "venue": "Multimodal fusion with deep neural networks for audio-video emotion recognition",
      "arxiv": "arXiv:1907.03196"
    },
    {
      "citation_id": "23",
      "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
      "authors": [
        "Akira Fukui",
        "Dong Park",
        "Daylen Yang",
        "Anna Rohrbach",
        "Trevor Darrell",
        "Marcus Rohrbach"
      ],
      "year": "2016",
      "venue": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
      "arxiv": "arXiv:1606.01847"
    },
    {
      "citation_id": "24",
      "title": "Learn to combine modalities in multimodal deep learning",
      "authors": [
        "Kuan Liu",
        "Yanen Li",
        "Ning Xu",
        "Prem Natarajan"
      ],
      "year": "2018",
      "venue": "Learn to combine modalities in multimodal deep learning",
      "arxiv": "arXiv:1805.11730"
    },
    {
      "citation_id": "25",
      "title": "Mmtm: Multimodal transfer module for cnn fusion",
      "authors": [
        "Hamid Reza",
        "Vaezi Joze",
        "Amirreza Shaban",
        "Kazuhito Michael L Iuzzolino",
        "Koishida"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Msaf: Multimodal split attention fusion",
      "authors": [
        "Lang Su",
        "Chuqing Hu",
        "Guofa Li",
        "Dongpu Cao"
      ],
      "year": "2020",
      "venue": "Msaf: Multimodal split attention fusion",
      "arxiv": "arXiv:2012.07175"
    },
    {
      "citation_id": "27",
      "title": "Eranns: Efficient residual audio neural networks for audio pattern recognition",
      "authors": [
        "Sergey Verbitskiy",
        "Viacheslav Vyshegorodtsev"
      ],
      "year": "2021",
      "venue": "Eranns: Efficient residual audio neural networks for audio pattern recognition",
      "arxiv": "arXiv:2106.01621"
    }
  ]
}