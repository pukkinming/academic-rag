{
  "paper_id": "2311.11980v1",
  "title": "Leveraging Previous Facial Action Units Knowledge For Emotion Recognition On Faces",
  "published": "2023-11-20T18:14:53Z",
  "authors": [
    "Pietro B. S. Masur",
    "Willams Costa",
    "Lucas S. Figueredo",
    "Veronica Teichrieb"
  ],
  "keywords": [
    "human behavior recognition",
    "emotion recognition",
    "facial unit activation",
    "deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "People naturally understand emotions, thus permitting a machine to do the same could open new paths for human-computer interaction. Facial expressions can be very useful for emotion recognition techniques, as these are the biggest transmitters of non-verbal cues capable of being correlated with emotions. Several techniques are based on Convolutional Neural Networks (CNNs) to extract information in a machine learning process. However, simple CNNs are not always sufficient to locate points of interest on the face that can be correlated with emotions. In this work, we intend to expand the capacity of emotion recognition techniques by proposing the usage of Facial Action Units (AUs) recognition techniques to recognize emotions. This recognition will be based on the Facial Action Coding System (FACS) and computed by a machine learning system. In particular, our method expands over EmotiRAM, an approach for multi-cue emotion recognition, in which we improve over their facial encoding module.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotions are an inherent aspect of a human being's life. They are an indicator of one's inner state, which is constantly communicated through verbal and nonverbal cues in one's communication. The ability to recognize what is being expressed by an individual is vital for communication to happen. In particular, reading someone's emotional state is essential to correspond with them in the correct tone. For example, when talking to someone sad, people act differently than when talking to someone happy.\n\nBeing able to classify the emotions of a user during the interaction with an intelligent system or environment leads to generating useful data for decision-making. Social networks and streaming platforms currently ask the user for their opinion regarding content to generate better recommendations in the future  [5] . However, asking for the input of a user may not always be the best option; users may reply only if they dislike too much or like too much. For some applications, such as, for example, education, safety, healthcare, and entertainment, having insights into the user's perceived emotion are not only helpful but also enabling  [14] . Many components go into building a robust solution for emotion recognition; for instance, the metrics and outputs from such a model must be consistent with psychological principles.\n\nWhen it comes to performing recognition of emotions on faces, another fundamental of psychology can be applied: the Facial action encoding system (FACS)  [3] . FACS is a system capable of translating someone's non-verbal cues into an emotion. It was coded based on empirical observation of subjects' facial and corporal displays while feeling different emotions.\n\nFACS encodes specifically the behaviors behind the activation of AUs; those associated with facial muscles are referred to as facial action units (FAUs). When someone feels an emotion, his body and face respond to that feeling in the form of action units  [4]    [13] . The response intensity varies according to each subject's physiology, but the correlation between emotions and action unit activation is universal. Besides, since this activation is a physiological response, it cannot be posed consistently. Fig.  1  displays some of the AUs activated while the subject feels fear.\n\nTherefore, given the correlation of FAUs and emotions, in this work, we propose to evaluate if deep learning models trained to recognize FAUs could provide a more efficient face encoding than those deployed currently in emotion recognition techniques. We evaluate our work in the well-known CAER-S dataset  [8]  and compare it with a state-of-the-art approach for emotion recognition called EmotiRAM  [2] , specifically against its face encoding stream, which we will refer to as EmotiRAM-f. We also propose experiments to discuss if such models could lead to better explainability of predictions by highlighting which facial cues were used for the prediction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "Emotion recognition. Over the years, techniques for emotion recognition have been proposed based on using more than one nonverbal cue to identify emotion; mainly context representation, which contains descriptions of the context that the person is placed into. The Context-aware emotion recognition networks (CAER-Net)  [8]  is one of the first works to explore this possibility while also publishing the CAER and CAER-S datasets and proposing a public benchmark. The CAER-S dataset was generated by extracting videos and frames from 79 television shows. The main performer would always be somewhat aligned with the camera, and the frame would have contextual information. For CAER-Net, the authors propose two main encoding streams, namely the face encoding stream and the context encoding stream, in which information from each cue would flow into and have features extracted. Although this approach led to significant results, there were still possibilities for improvement regarding the approach for cue extraction.\n\nLater, in work named Emotion recognition on adaptive multi-cues (EmotiRAM)  [2] , the authors propose to extend this pipeline by improving the extraction of representations from context and by proposing an independent stream to process body language. Each cue is processed individually and their representations are fused at the end of the inference. Although proposing a significant improvement, which also led to a significant accuracy, both EmotiRAM and CAER-Net lack a face encoding stream that is capable of extracting deep representations linked to emotion since these methods only search for generic cues in the facial regions.\n\nFacial action units activation. A possible way to extend these works is related to how FAUs are extracted by the current state-of-the-art of this field. Sanchez et al.  [12]  proposes in their work a supervised learning approach for learning AU heatmaps. Their proposed methodology is to estimate the facial landmarks and generate Gaussian maps around the points where AUs are known to cause changes. The inspiration for the approach is related to joint activations of AUs. Thus, it is possible to model AUs more realistically by considering them all when performing regression. The authors employ an Hourglass network  [11]  as a backbone of their model, which achieved significant results.\n\nHowever, relying on pre-defined rules for modeling AU cooccurrence leads to limited generalization. Each face contains a unique setting; taking this into consideration provides a fuller approach to this problem. To contour that, Fan et al. proposes a novel GNN-based technique  [6] . Their framework is such that latent relationships of AUs are automatically learned via establishing semantic correspondences between feature maps. This is achieved by employing a combination of heatmap regression and Semantic Correspondence Convolution (SCC). SCC modules are based on the work by Wang et al.  [15] , which focuses on dynamic graph convolutions in geometry modeling. Intuitively, feature channels that are simultaneously activated are considered to have a latent co-relation; this corresponds to a co-occurrence pattern of AU intensities.\n\nThe method basic framework is built by adding several deconvolutional layers on a ResNet  [7] , which generates the AUs feature maps. Feature maps are then input into SCC modules 3 times. The network outputs one heatmap per AU.\n\nFinally, considering each facial topology when building AU graphs is essential for building a robust GNN-based technique. However, the number of edges between the AUs may also be a very important factor in the representation ability of this graph. Luo et al.  [9]  propose that a single edge between AUs in a graph representation is insufficient for dealing with their complex relations. To address this problem, they propose a strategy that, for each AU pair, encodes a pair (two edges) of multi-dimensional features.\n\nTo achieve their objective and deal with the problems they've encountered with previous approaches to model AUs relationships as graphs, the authors utilize three resources:\n\n1) Modeling a full face representation into an explicitly AUs relationship describing graph through the Attention Node Feature Learning (ANFL) module 2) Modeling the relationship between AUs pair on a multidimensional edge feature graph with Multi-edge Feature Learning (MEFL) module 3) Considering the uniqueness of each facial display by utilizing full face representation as input to the two modules listed above The ANFL module is composed of two blocks: 1) AU-specific Feature Generator (AFG): composed of Fully Connected (FC) and global average polling layers which jointly act as a feature extractor 2) Facial Graph Generator (FGG): computes node similarity via KNN and GCN. This block is only used as a reinforcement to the AFG block; thus it is not used on training MEFL module also has two blocks: 1) Facial Display-specific AU Representation Modeling (FAM) Receives full face representation and facial features from the AFG FC layer and locates activation cues of each AU\n\n2) AU Relationship Modeling (ARM)\n\nThe ARM outputs are utilized to extract features from the located cues which relate to both AUs activations.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Method",
      "text": "Our proposal with this investigation is to improve the accuracy of EmotiRAM-f  [2]  through the knowledge of FAUs. To achieve this, we propose using models trained for this task as a baseline and retraining this model on the CAER-S dataset to perform emotion recognition tasks.\n\nEkman and Friesen  [3]  disclose how emotions can be recognized based on the AUs activation, which is the basis for this evaluation. Furthermore, the emotion displayed by the AUs are non-posed; thus, AUs consists a better source for emotion inference than naive facial observation. Leveraging this observation in constructing this approach, we hypothesize that encoding action units consist of a better source of information than naive facial encoding techniques. This is the postulate we depart from to develop our EmotiRAM-FAU technique.\n\nTherefore, given an image I, we aim to classify this image into a perceived emotion y from a list of possible emotions C ∈ R 7 . For this, we extend the pipeline proposed by Fan et al.  [6]  by adding bottlenecks to manage the flow of representations and the generation of the classification. Following the original implementation pipeline, we use a ResNet  [7]  as a backbone and add de-convolutional layers to upsample the filters and generate the heatmaps. After this, we add a bottleneck sequence using Fully Connected (FC) layers to transcribe this internal representation into a classification C for the given image. The heatmap generated by the pre-trained model has a shape of 10×24×24. This is a fixed shape that is restrained by the output of Fan et al. architecture. The shape is flattened to a resulting 5760 × 1 shape and passed through the following arbitrated FC layers. We reduce the shape of this internal representation using a base-2 approach with 5 FC layers as following: 5760-dim; 2048-dim; 1024-dim; 512-dim; 256-dim and finally 7-dim.\n\nIV. EXPERIMENTS Datasets. For this experimentation, we use a dataset for the task of FAU and another dataset for the emotion recognition task. For FAU we employ DISFA  [10] , a dataset built from the collection of videos from twenty-seven young adults. The participants were recorded while videos aiming to induce spontaneous expressions were shown to them. The recordings were made with two cameras, one on the right and the other one on the left side. The data collected was labeled by FACS experts, frame by frame, to indicate the presence (or absence) and intensity of FAUs. DISFA contains labels for 12 AUs: 5 on the upper face and 7 on the lower face. In joint with BP4D 1    [16] , DISFA is a default benchmark for AUs recognition techniques.\n\nThe CAER benchmark  [8]  comprises videos collected from 79 TV shows, totaling 20,484 clips which were manually annotated with six different emotion labels. CAER-S is a static version of the CAER benchmark. On this version, there is a single frame from the original CAER benchmark video which is annotated in the same way as the videoclip.\n\nTo perform our experiments, we have split CAER-S into three partitions: train, validation, and test. We used pre-trained models for the task of facial action unit activation as a baseline in which we implemented our approach. Our approach consists of inserting a bottleneck of FC layers into the pretrained model and retraining them to recognize emotions. The bottleneck outputs represent the detected emotions.\n\nEnvironment and implementation. We reimplemented EmotiRAM-f as discussed on Costa et al.  [2]  and trained on CAER-S using PyTorch 1.11.0. For the experiments related to FAUs, we reuse the source code published by Luo et al.  [9]   2  , utilizing PyTorch 1.4.0, as stated in their public repository. We perform our experiments on a desktop computer running Ubuntu 20.04 with an RTX 2080 Ti GPU with 24 GB of RAM, and a 4-core processor with 8 threads.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Experimental Settings",
      "text": "We have experimented with three different approaches based on ResNet-50 for FAU recognition. First, the work by Fan et al.  [6] , in which we used a pre-trained model in the BP4D dataset, and also the work by Luo et al. Each of these models is separately inserted on EmotiRAM's architecture in joint with a bottleneck of FC layers, it functions as a replacement for the face-encoding module. The finetuned models are then evaluated in the CAER-S dataset for emotion recognition. Luo et al. technique output is a vector of probabilities for each AU on the dataset. Thus, in our experiments with it, only one FC layer could be utilized.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Experiments On Explainability",
      "text": "Given that we use pre-trained models for the task of AU recognition, we propose an experiment to assess the correlation between the internal representation of AUs and those finetuned for emotions, despite no mechanism that controls which representations to keep being utilized. The pre-trained models we use give output information about AUs. In particular, Luo et al. model outputs a vector of probabilities of the activations of each AU. For this experiment, we only utilize ME-graph models pre-trained on DISFA since it is the only AU dataset we have access to.  utilized. First, we evaluate metrics for the original pre-trained ME-graph model. Then, we evaluate our fine-tuned model, truncating its bottleneck to fit with the network architecture required by Luo et al. code.\n\n2) AU experiments on CAER-S: In this experiment, we extract EmotiRam-FAU's output before and after the bottleneck thus retrieving two outputs: recognized AUs and recognized emotions. We work with data we have access to for building an explainable program based on EmotiRam-FAU. DISFA contains labels for a limited range of AUs; on Luo et al. work, the training process is limited to 8 AUs. Matching the AUs covered by ME-Graph with the emotions provided by the CAER-S dataset, we observe that the only full match between activated AUs and corresponding emotion is Happiness. We label CAER-S happiness data as being (AU6+AU12) and evaluate the correctness of the AU output made by the FAU encoding module after our fine-tuning process. We also perform the same experiments with the original ME-graph pretrained model and use it as a baseline. In the next section, we will display our achieved results and perform some qualitative evaluations over selected images of the CAER-S dataset.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "Here we detail the results achieved on CAER-S in terms of accuracy. Table  I  shows the results we acquired with each technique. As shown, EmotiRAM-FAU improved by 7 percent over Emoti-RAM's face encoding module (EmotiRAM-f) from Costa textitet al.  [2]  in terms of accuracy. This is a significant result and points out that models pre-trained on AUs can provide a better encoding than naive techniques.\n\nThe results achieved with Luo et al. technique were not as good as the others. We believe this occurred because their network outputs are shaped as AUs probabilities, while Fan et al. works with a heatmap richer in features as their output. Thus, when coupled with a bottleneck, Luo et al. output features have little margin to find new vector spaces representing emotions. From now on, we will refer to EmotiRAM-FAU with Luo et al. technique as EmotiRAM-FAU-ME-Graph.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Results On Aus",
      "text": "To evaluate how our model behaved on the AUs recognition task, we performed a test on the DISFA dataset. As previously explained could not work with the BP4D dataset due to our lack of access to the original database.\n\n1) Results on DISFA: As we can see, the F1 score and accuracy metrics have greatly decayed compared to the original approach. There are some reasons we believe may have caused this. As previously observed, CAER-S is an on-the-wild dataset ensembled mainly from TV shows, thus, it is fair to expect some samples to contain posed emotions. Furthermore, since the annotation of CAER-S was not made by FACS experts, we believe that they are far from ideal in terms of correspondence between the real displayed emotion (based on the FACS system) and the labeled emotion. It is also reasonable to suppose that, if our hypothesis is true, there is no reliable way to determine whether CAER-S data provides a balanced distribution of FAUs activations examples. Thus, it is natural to suppose those factors lead our model to suffer some degree of forgetfulness at its original task. We believe that having more reliable training data could greatly benefit our model approach.\n\n2) Qualitative Results on CAER-S: We have utilized data labeled as happiness on the CAER-S dataset and assumed that their corresponding activated AUs were AU6 and AU12. We then made a script to perform an accuracy test based on the emotion and AUs outputs produced by EmotiRAM-FAU.  Fig.  3 . A happiness-labeled image from the CAER-S dataset, which our system correctly labeled. Notice that AU12 is the main responsible for the happiness on this photo, and the AU6 region (eyes), in which our system has low performance, is covered by sunglasses. Fig.  4 . A happiness-labeled image from the CAER-S dataset, which our system mislabeled. Notice that in this sample, the main sign of happiness is given by AU6, the same which our system practically lost the ability to recognize.\n\nfrom the CAER-s dataset, which was correctly labeled for both AUs and emotions by our approach. On the other hand, Fig.  4  displays an image our model failed to recognize as having the correct AUs.\n\nNotice that EmotiRAM-FAU-ME-Graph results at recognizing both AUs were near zero. This comes from the previously discussed forgetfulness suffered by the model. In particular, AU6 was greatly affected by this forgetfulness, as shown in table II. This is the cause of such a low score in recognizing both AUs.\n\nUnfortunately, due to time and accessible data limitations, it was impossible to conduct more extensive and quantitative research on the effectiveness of our system. In addition, the lack of annotated data and emotions compatible with the AUs learned by the ME-Graph technique was a significant drawback to our research. In this work, we have investigated the use of AUs to recognize emotions in ML models. We have shown how models pre-trained to recognize AUs fine-tuned for emotion recognition tasks provide a better basis for emotion recognition than training architectures from scratch.\n\nOur approach is explicitly aimed at emotion recognition. However, we have also investigated how much the final model intermediary representations are still faithful to the original AUs representation. Our results showed that, when fine-tuned for emotions, the model still retains some ability to recognize AUs. This is an important step towards explicable systems for emotion recognition.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: displays some of the AUs activated while",
      "page": 1
    },
    {
      "caption": "Figure 1: The FACS encoding of fear, from [1].",
      "page": 2
    },
    {
      "caption": "Figure 2: Graphical representation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck, represented in red. Then the",
      "page": 4
    },
    {
      "caption": "Figure 3: shows a piece of data",
      "page": 4
    },
    {
      "caption": "Figure 3: A happiness-labeled image from the CAER-S dataset, which our",
      "page": 5
    },
    {
      "caption": "Figure 4: A happiness-labeled image from the CAER-S dataset, which our",
      "page": 5
    },
    {
      "caption": "Figure 4: displays an image our model failed to recognize as having",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Recife, Brazil": "vt@cin.ufpe.br"
        },
        {
          "Recife, Brazil": "Abstract—People\nnaturally\nunderstand\nemotions,\nthus\nper-"
        },
        {
          "Recife, Brazil": "mitting\na machine\nto do\nthe\nsame\ncould open new paths\nfor"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "human-computer\ninteraction.\nFacial\nexpressions\ncan\nbe\nvery"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "useful for emotion recognition techniques, as these are the biggest"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "transmitters of non-verbal cues capable of being correlated with"
        },
        {
          "Recife, Brazil": "emotions. Several\ntechniques are based on Convolutional Neural"
        },
        {
          "Recife, Brazil": "Networks\n(CNNs)\nto extract\ninformation in a machine learning"
        },
        {
          "Recife, Brazil": "process. However,\nsimple CNNs\nare\nnot\nalways\nsufficient\nto"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "locate points of\ninterest on the face that can be correlated with"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "emotions.\nIn this work, we\nintend to\nexpand the\ncapacity\nof"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "emotion recognition techniques by proposing the usage of Facial"
        },
        {
          "Recife, Brazil": "Action Units (AUs) recognition techniques to recognize emotions."
        },
        {
          "Recife, Brazil": "This\nrecognition will\nbe\nbased\non\nthe Facial Action Coding"
        },
        {
          "Recife, Brazil": "System (FACS) and computed by a machine learning system. In"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "particular, our method expands over EmotiRAM, an approach"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "for multi-cue\nemotion recognition,\nin which we\nimprove\nover"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "their facial encoding module."
        },
        {
          "Recife, Brazil": "Index Terms—human behavior\nrecognition,\nemotion recogni-"
        },
        {
          "Recife, Brazil": "tion,\nfacial unit activation, deep learning"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "I.\nINTRODUCTION"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "Emotions are an inherent aspect of a human being’s\nlife."
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "They are an indicator of one’s inner state, which is constantly"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "communicated\nthrough\nverbal\nand\nnonverbal\ncues\nin\none’s"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "communication. The\nability to recognize what\nis being ex-"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "pressed by an individual is vital for communication to happen."
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "In particular,\nreading someone’s\nemotional\nstate\nis\nessential"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "to correspond with them in the\ncorrect\ntone. For\nexample,"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "when talking to someone sad, people act differently than when"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "talking to someone happy."
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "Being able\nto classify the\nemotions of\na user during the"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "interaction with an intelligent system or environment\nleads to"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "generating useful data for decision-making. Social networks"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "and streaming platforms currently ask the user for their opinion"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "regarding content\nto generate better\nrecommendations\nin the"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "future [5]. However, asking for\nthe input of a user may not"
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": ""
        },
        {
          "Recife, Brazil": "979-8-3503-4807-1/23/$31.00 ©2023 IEEE"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "them all when performing regression. The authors employ an": "Hourglass network [11] as a backbone of\ntheir model, which"
        },
        {
          "them all when performing regression. The authors employ an": "achieved significant\nresults."
        },
        {
          "them all when performing regression. The authors employ an": "However, relying on pre-defined rules for modeling AU co-"
        },
        {
          "them all when performing regression. The authors employ an": "occurrence leads to limited generalization. Each face contains"
        },
        {
          "them all when performing regression. The authors employ an": "a unique setting; taking this into consideration provides a fuller"
        },
        {
          "them all when performing regression. The authors employ an": "approach to this problem. To contour that, Fan et al. proposes"
        },
        {
          "them all when performing regression. The authors employ an": "a novel GNN-based technique [6]. Their\nframework is\nsuch"
        },
        {
          "them all when performing regression. The authors employ an": "that\nlatent\nrelationships of AUs are automatically learned via"
        },
        {
          "them all when performing regression. The authors employ an": "establishing semantic correspondences between feature maps."
        },
        {
          "them all when performing regression. The authors employ an": "This\nis\nachieved\nby\nemploying\na\ncombination\nof\nheatmap"
        },
        {
          "them all when performing regression. The authors employ an": "regression and Semantic Correspondence Convolution (SCC)."
        },
        {
          "them all when performing regression. The authors employ an": "SCC modules\nare based on the work by Wang et al.\n[15],"
        },
        {
          "them all when performing regression. The authors employ an": "which focuses on dynamic graph convolutions\nin geometry"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "modeling. Intuitively, feature channels that are simultaneously"
        },
        {
          "them all when performing regression. The authors employ an": "activated\nare\nconsidered\nto\nhave\na\nlatent\nco-relation;\nthis"
        },
        {
          "them all when performing regression. The authors employ an": "corresponds to a co-occurrence pattern of AU intensities."
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "The method basic framework is built by adding several de-"
        },
        {
          "them all when performing regression. The authors employ an": "convolutional layers on a ResNet [7], which generates the AUs"
        },
        {
          "them all when performing regression. The authors employ an": "feature maps. Feature maps are then input\ninto SCC modules"
        },
        {
          "them all when performing regression. The authors employ an": "3 times. The network outputs one heatmap per AU."
        },
        {
          "them all when performing regression. The authors employ an": "Finally, considering each facial\ntopology when building AU"
        },
        {
          "them all when performing regression. The authors employ an": "graphs is essential for building a robust GNN-based technique."
        },
        {
          "them all when performing regression. The authors employ an": "However,\nthe number of edges between the AUs may also be"
        },
        {
          "them all when performing regression. The authors employ an": "a very important\nfactor\nin the\nrepresentation ability of\nthis"
        },
        {
          "them all when performing regression. The authors employ an": "graph. Luo et al. [9] propose that a single edge between AUs"
        },
        {
          "them all when performing regression. The authors employ an": "in a graph representation is insufficient\nfor dealing with their"
        },
        {
          "them all when performing regression. The authors employ an": "complex relations. To address\nthis problem,\nthey propose\na"
        },
        {
          "them all when performing regression. The authors employ an": "strategy that, for each AU pair, encodes a pair (two edges) of"
        },
        {
          "them all when performing regression. The authors employ an": "multi-dimensional\nfeatures."
        },
        {
          "them all when performing regression. The authors employ an": "To\nachieve\ntheir\nobjective\nand\ndeal with\nthe\nproblems"
        },
        {
          "them all when performing regression. The authors employ an": "they’ve encountered with previous approaches to model AUs"
        },
        {
          "them all when performing regression. The authors employ an": "relationships as graphs,\nthe authors utilize three resources:"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "1) Modeling a\nfull\nface\nrepresentation into an explicitly"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "AUs relationship describing graph through the Attention"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "Node Feature Learning (ANFL) module"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "2) Modeling the relationship between AUs pair on a multi-"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "dimensional edge feature graph with Multi-edge Feature"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "Learning (MEFL) module"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "3) Considering the uniqueness of\neach facial display by"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "utilizing\nfull\nface\nrepresentation\nas\ninput\nto\nthe\ntwo"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "modules listed above"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "The ANFL module is composed of\ntwo blocks:"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "1) AU-specific\nFeature Generator\n(AFG):\ncomposed\nof"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "Fully Connected (FC) and global average polling layers"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "which jointly act as a feature extractor"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "2)\nFacial Graph Generator\n(FGG): computes node similar-"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "ity via KNN and GCN. This block is only used as\na"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "reinforcement\nto the AFG block;\nthus it\nis not used on"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "training"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "MEFL module also has two blocks:"
        },
        {
          "them all when performing regression. The authors employ an": ""
        },
        {
          "them all when performing regression. The authors employ an": "1)\nFacial Display-specific AU Representation Modeling"
        },
        {
          "them all when performing regression. The authors employ an": "(FAM) Receives full\nface representation and facial\nfea-"
        },
        {
          "them all when performing regression. The authors employ an": "tures from the AFG FC layer and locates activation cues"
        },
        {
          "them all when performing regression. The authors employ an": "of each AU"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2) AU Relationship Modeling (ARM)": "The ARM outputs are utilized to extract\nfeatures from the",
          "annotated with six different emotion labels. CAER-S is a static": "version of\nthe CAER benchmark. On this version,\nthere is a"
        },
        {
          "2) AU Relationship Modeling (ARM)": "located cues which relate to both AUs activations.",
          "annotated with six different emotion labels. CAER-S is a static": "single frame from the original CAER benchmark video which"
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "is annotated in the same way as the videoclip."
        },
        {
          "2) AU Relationship Modeling (ARM)": "III. METHOD",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "To perform our\nexperiments, we have\nsplit CAER-S into"
        },
        {
          "2) AU Relationship Modeling (ARM)": "Our\nproposal with\nthis\ninvestigation\nis\nto\nimprove\nthe",
          "annotated with six different emotion labels. CAER-S is a static": "three partitions:\ntrain, validation, and test. We used pre-trained"
        },
        {
          "2) AU Relationship Modeling (ARM)": "accuracy of EmotiRAM-f [2] through the knowledge of FAUs.",
          "annotated with six different emotion labels. CAER-S is a static": "models\nfor\nthe\ntask\nof\nfacial\naction\nunit\nactivation\nas\na"
        },
        {
          "2) AU Relationship Modeling (ARM)": "To achieve this, we propose using models trained for this task",
          "annotated with six different emotion labels. CAER-S is a static": "baseline in which we implemented our approach. Our approach"
        },
        {
          "2) AU Relationship Modeling (ARM)": "as a baseline and retraining this model on the CAER-S dataset",
          "annotated with six different emotion labels. CAER-S is a static": "consists of\ninserting a bottleneck of FC layers\ninto the pre-"
        },
        {
          "2) AU Relationship Modeling (ARM)": "to perform emotion recognition tasks.",
          "annotated with six different emotion labels. CAER-S is a static": "trained model and retraining them to recognize emotions. The"
        },
        {
          "2) AU Relationship Modeling (ARM)": "Ekman\nand\nFriesen\n[3]\ndisclose\nhow emotions\ncan\nbe",
          "annotated with six different emotion labels. CAER-S is a static": "bottleneck outputs represent\nthe detected emotions."
        },
        {
          "2) AU Relationship Modeling (ARM)": "recognized based on the AUs\nactivation, which is\nthe basis",
          "annotated with six different emotion labels. CAER-S is a static": "Environment\nand\nimplementation. We\nreimplemented"
        },
        {
          "2) AU Relationship Modeling (ARM)": "for this evaluation. Furthermore,\nthe emotion displayed by the",
          "annotated with six different emotion labels. CAER-S is a static": "EmotiRAM-f as discussed on Costa et al.\n[2] and trained on"
        },
        {
          "2) AU Relationship Modeling (ARM)": "AUs are non-posed; thus, AUs consists a better source for emo-",
          "annotated with six different emotion labels. CAER-S is a static": "CAER-S using PyTorch 1.11.0. For the experiments related to"
        },
        {
          "2) AU Relationship Modeling (ARM)": "tion inference than naive facial observation. Leveraging this",
          "annotated with six different emotion labels. CAER-S is a static": "FAUs, we reuse the source code published by Luo et al.\n[9]"
        },
        {
          "2) AU Relationship Modeling (ARM)": "observation in constructing this approach, we hypothesize that",
          "annotated with six different emotion labels. CAER-S is a static": "2, utilizing PyTorch 1.4.0, as stated in their public repository."
        },
        {
          "2) AU Relationship Modeling (ARM)": "encoding action units consist of a better source of information",
          "annotated with six different emotion labels. CAER-S is a static": "We perform our experiments on a desktop computer\nrunning"
        },
        {
          "2) AU Relationship Modeling (ARM)": "than naive facial encoding techniques. This is the postulate we",
          "annotated with six different emotion labels. CAER-S is a static": "Ubuntu 20.04 with an RTX 2080 Ti GPU with 24 GB of RAM,"
        },
        {
          "2) AU Relationship Modeling (ARM)": "depart\nfrom to develop our EmotiRAM-FAU technique.",
          "annotated with six different emotion labels. CAER-S is a static": "and a 4-core processor with 8 threads."
        },
        {
          "2) AU Relationship Modeling (ARM)": "Therefore, given an image I, we aim to classify this image",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "A. Experimental Settings"
        },
        {
          "2) AU Relationship Modeling (ARM)": "into a perceived emotion y from a list of possible emotions",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "C\n∈\nR7.\nFor\nthis, we\nextend\nthe\npipeline\nproposed\nby",
          "annotated with six different emotion labels. CAER-S is a static": "We have experimented with three different approaches based"
        },
        {
          "2) AU Relationship Modeling (ARM)": "et\nal.\nFan\n[6]\nby\nadding\nbottlenecks\nto manage\nthe\nflow",
          "annotated with six different emotion labels. CAER-S is a static": "on ResNet-50 for FAU recognition. First,\nthe work by Fan et"
        },
        {
          "2) AU Relationship Modeling (ARM)": "of\nrepresentations\nand\nthe\ngeneration\nof\nthe\nclassification.",
          "annotated with six different emotion labels. CAER-S is a static": "al.\n[6],\nin which we used a pre-trained model\nin the BP4D"
        },
        {
          "2) AU Relationship Modeling (ARM)": "Following\nthe\noriginal\nimplementation\npipeline, we\nuse\na",
          "annotated with six different emotion labels. CAER-S is a static": "dataset, and also the work by Luo et al. [9] with a pre-trained"
        },
        {
          "2) AU Relationship Modeling (ARM)": "ResNet\n[7] as a backbone and add de-convolutional\nlayers to",
          "annotated with six different emotion labels. CAER-S is a static": "model\nin the BP4D dataset and another pre-trained model\nin"
        },
        {
          "2) AU Relationship Modeling (ARM)": "upsample the filters and generate the heatmaps. After this, we",
          "annotated with six different emotion labels. CAER-S is a static": "the DISFA dataset. Luo et al. model will be further\nreferred"
        },
        {
          "2) AU Relationship Modeling (ARM)": "add a bottleneck sequence using Fully Connected (FC) layers",
          "annotated with six different emotion labels. CAER-S is a static": "to as ME-graph."
        },
        {
          "2) AU Relationship Modeling (ARM)": "to transcribe this internal representation into a classification C",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "1)\nFan et al.\n[6] model pre-trained on BP4D"
        },
        {
          "2) AU Relationship Modeling (ARM)": "for the given image. The heatmap generated by the pre-trained",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "2) Luo et al.\n[9] model pre-trained on DISFA"
        },
        {
          "2) AU Relationship Modeling (ARM)": "model has a shape of 10×24×24. This is a fixed shape that\nis",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "3) Luo et al.\n[9] model pre-trained on BP4D"
        },
        {
          "2) AU Relationship Modeling (ARM)": "restrained by the output of Fan et al. architecture. The shape",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "Each of these models is separately inserted on EmotiRAM’s"
        },
        {
          "2) AU Relationship Modeling (ARM)": "is flattened to a resulting 5760 × 1 shape and passed through",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "architecture in joint with a bottleneck of FC layers, it functions"
        },
        {
          "2) AU Relationship Modeling (ARM)": "the\nfollowing arbitrated FC layers. We\nreduce\nthe\nshape of",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "as\na\nreplacement\nfor\nthe\nface-encoding module. The fine-"
        },
        {
          "2) AU Relationship Modeling (ARM)": "this internal representation using a base-2 approach with 5 FC",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "tuned models are then evaluated in the CAER-S dataset\nfor"
        },
        {
          "2) AU Relationship Modeling (ARM)": "layers as following: 5760-dim; 2048-dim; 1024-dim; 512-dim;",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "emotion recognition. Luo et al.\ntechnique output\nis a vector"
        },
        {
          "2) AU Relationship Modeling (ARM)": "256-dim and finally 7-dim.",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "of\nprobabilities\nfor\neach AU on\nthe\ndataset. Thus,\nin\nour"
        },
        {
          "2) AU Relationship Modeling (ARM)": "IV. EXPERIMENTS",
          "annotated with six different emotion labels. CAER-S is a static": "experiments with it, only one FC layer could be utilized."
        },
        {
          "2) AU Relationship Modeling (ARM)": "Datasets. For this experimentation, we use a dataset for the",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "B. Experiments on Explainability"
        },
        {
          "2) AU Relationship Modeling (ARM)": "task of FAU and another dataset\nfor\nthe emotion recognition",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "Given that we use pre-trained models\nfor\nthe task of AU"
        },
        {
          "2) AU Relationship Modeling (ARM)": "task. For FAU we employ DISFA [10], a dataset built\nfrom",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "recognition, we propose an experiment to assess the correlation"
        },
        {
          "2) AU Relationship Modeling (ARM)": "the collection of videos from twenty-seven young adults. The",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "between the\ninternal\nrepresentation of AUs\nand those fine-"
        },
        {
          "2) AU Relationship Modeling (ARM)": "participants were\nrecorded while\nvideos\naiming\nto\ninduce",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "tuned for emotions, despite no mechanism that controls which"
        },
        {
          "2) AU Relationship Modeling (ARM)": "spontaneous expressions were shown to them. The recordings",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "representations to keep being utilized. The pre-trained models"
        },
        {
          "2) AU Relationship Modeling (ARM)": "were made with two cameras, one on the right and the other",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "we use give output\ninformation about AUs.\nIn particular, Luo"
        },
        {
          "2) AU Relationship Modeling (ARM)": "one on the left side. The data collected was labeled by FACS",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "et al. model outputs a vector of probabilities of the activations"
        },
        {
          "2) AU Relationship Modeling (ARM)": "experts, frame by frame,\nto indicate the presence (or absence)",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "of each AU. For\nthis experiment, we only utilize ME-graph"
        },
        {
          "2) AU Relationship Modeling (ARM)": "and intensity of FAUs. DISFA contains labels for 12 AUs: 5",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "models pre-trained on DISFA since it\nis the only AU dataset"
        },
        {
          "2) AU Relationship Modeling (ARM)": "on the upper face and 7 on the lower face. In joint with BP4D1",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "we have access to."
        },
        {
          "2) AU Relationship Modeling (ARM)": "[16], DISFA is\na\ndefault\nbenchmark\nfor AUs\nrecognition",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "1) AU experiments on DISFA: To evaluate how much the"
        },
        {
          "2) AU Relationship Modeling (ARM)": "techniques.",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "fine-tuned model still relies on the AUs original representation,"
        },
        {
          "2) AU Relationship Modeling (ARM)": "The CAER benchmark [8] comprises videos collected from",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "we perform experiments on the DISFA dataset. For\nthis end,"
        },
        {
          "2) AU Relationship Modeling (ARM)": "79 TV shows,\ntotaling\n20,484\nclips which were manually",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "",
          "annotated with six different emotion labels. CAER-S is a static": "a publicly available\nimplementation of Luo et al.\ncode was"
        },
        {
          "2) AU Relationship Modeling (ARM)": "1This is another dataset\nthat could fit well\nin our experimentation; however,",
          "annotated with six different emotion labels. CAER-S is a static": ""
        },
        {
          "2) AU Relationship Modeling (ARM)": "we could not gain access to it. Therefore, we use models pre-trained on them.",
          "annotated with six different emotion labels. CAER-S is a static": "2https://github.com/EvelynFan/FAU"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "RESULTS ACHIEVED ON THE CAER-S TEST PARTITION,"
        },
        {
          "TABLE I": "ACCURACY."
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "CAER-S"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "EmotiRAM-f\n70%"
        },
        {
          "TABLE I": "EmotiRAM (face+context)\n87%"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "EmotiRAM (face+context+body)\n89%"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "77%\nEmotiRAM-FAU [BP4D]"
        },
        {
          "TABLE I": "EmotiRAM-FAU-ME-graph [BP4D]\n62%"
        },
        {
          "TABLE I": "EmotiRAM-FAU-ME-graph [DISFA]\n66%"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "bottleneck outputs the value corresponding to the emotion it has predicted.",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "utilized. First, we evaluate metrics for the original pre-trained",
          "represented in red. Then the": "result\nand\npoints\nout\nthat models\npre-trained\non AUs\ncan"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "ME-graph model. Then, we\nevaluate our fine-tuned model,",
          "represented in red. Then the": "provide a better encoding than naive techniques."
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "truncating its bottleneck to fit with the network architecture",
          "represented in red. Then the": "The\nresults\nachieved with Luo et al.\ntechnique were not"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "required by Luo et al. code.",
          "represented in red. Then the": "as good as the others. We believe this occurred because their"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "2) AU experiments on CAER-S:\nIn this experiment, we ex-",
          "represented in red. Then the": "network outputs are shaped as AUs probabilities, while Fan"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "tract EmotiRam-FAU’s output before and after\nthe bottleneck",
          "represented in red. Then the": "et al. works with a heatmap richer in features as their output."
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "thus\nretrieving two outputs:\nrecognized AUs and recognized",
          "represented in red. Then the": "Thus, when coupled with a bottleneck, Luo et al. output\nfea-"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "emotions. We work with data we have access to for building",
          "represented in red. Then the": "tures have little margin to find new vector spaces representing"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "an\nexplainable\nprogram based\non EmotiRam-FAU. DISFA",
          "represented in red. Then the": "emotions. From now on, we will\nrefer\nto EmotiRAM-FAU"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "et\nal.\ncontains\nlabels\nfor\na\nlimited\nrange\nof AUs;\non Luo",
          "represented in red. Then the": "with Luo et al.\ntechnique as EmotiRAM-FAU-ME-Graph."
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "work,\nthe\ntraining\nprocess\nis\nlimited\nto\n8 AUs. Matching",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "A. Results on AUS"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "the AUs covered by ME-Graph with the emotions provided",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "To evaluate how our model behaved on the AUs recognition"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "by the CAER-S dataset, we observe that\nthe only full match",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "task, we performed a test on the DISFA dataset. As previously"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "between activated AUs and corresponding emotion is Happi-",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "explained could not work with the BP4D dataset due to our"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "ness. We label CAER-S happiness data as being (AU6+AU12)",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "lack of access to the original database."
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "and evaluate the correctness of\nthe AU output made by the",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "1) Results on DISFA: Table II display the results achieved"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "FAU encoding module after our fine-tuning process. We also",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "by EmotiRAM-FAU-ME-Graph and compares\nthem with the"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "perform the same experiments with the original ME-graph pre-",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "original Luo et al. model\n(ME-Graph)."
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "trained model and use it as a baseline. In the next section, we",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "As we\ncan see,\nthe F1 score\nand accuracy metrics have"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "will display our achieved results and perform some qualitative",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "greatly decayed compared to the original approach. There are"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "evaluations over selected images of\nthe CAER-S dataset.",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "some reasons we believe may have caused this. As previously"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "V. RESULTS AND DISCUSSION",
          "represented in red. Then the": "observed, CAER-S is an on-the-wild dataset ensembled mainly"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "from TV shows,\nthus,\nit\nis\nfair\nto expect\nsome\nsamples\nto"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "Here we detail\nthe results achieved on CAER-S in terms",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "contain posed emotions. Furthermore, since the annotation of"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "of accuracy. Table I shows the results we acquired with each",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "CAER-S was not made by FACS experts, we believe that\nthey"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "technique.",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "are far from ideal\nin terms of correspondence between the real"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "displayed emotion (based on the FACS system) and the labeled"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "TABLE I",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "emotion. It is also reasonable to suppose that, if our hypothesis"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "IN TERMS OF\nRESULTS ACHIEVED ON THE CAER-S TEST PARTITION,",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "ACCURACY.",
          "represented in red. Then the": "is true,\nthere is no reliable way to determine whether CAER-"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "S data provides\na balanced distribution of FAUs\nactivations"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "CAER-S",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "examples. Thus,\nit\nis natural\nto suppose those factors lead our"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "EmotiRAM-f\n70%",
          "represented in red. Then the": "model\nto suffer\nsome degree of\nforgetfulness\nat\nits original"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "EmotiRAM (face+context)\n87%",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "task. We believe that having more reliable training data could"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "EmotiRAM (face+context+body)\n89%",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "greatly benefit our model approach."
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "77%\nEmotiRAM-FAU [BP4D]",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "EmotiRAM-FAU-ME-graph [BP4D]\n62%",
          "represented in red. Then the": "2) Qualitative Results on CAER-S: We have utilized data"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "EmotiRAM-FAU-ME-graph [DISFA]\n66%",
          "represented in red. Then the": ""
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "labeled as happiness on the CAER-S dataset and assumed that"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "",
          "represented in red. Then the": "their corresponding activated AUs were AU6 and AU12. We"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "As\nshown, EmotiRAM-FAU improved by 7 percent over",
          "represented in red. Then the": "then made a script\nto perform an accuracy test based on the"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "Emoti- RAM’s\nface\nencoding module\n(EmotiRAM-f)\nfrom",
          "represented in red. Then the": "emotion and AUs outputs produced by EmotiRAM-FAU. Table"
        },
        {
          "Fig. 2.\nGraphical\nrepresentation of our architecture. We treat Fan et al. model as a black box and link it with a bottleneck,": "Costa textitet al. [2] in terms of accuracy. This is a significant",
          "represented in red. Then the": "III\nshows our achieved results. Fig 3 shows a piece of data"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ME-Graph [BP4D]\n49%\n81%": "VI. CONCLUSION"
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": ""
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": "A. Considerations"
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": ""
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": ""
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": "In\nthis work, we\nhave\ninvestigated\nthe\nuse\nof AUs\nto"
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": ""
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": "recognize\nemotions\nin ML models. We\nhave\nshown\nhow"
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": "models pre-trained to recognize AUs fine-tuned for emotion"
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": "recognition tasks provide a better basis for emotion recognition"
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": "than training architectures from scratch."
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": "Our\napproach is\nexplicitly aimed at\nemotion recognition."
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": "However, we have also investigated how much the final model"
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": "intermediary representations\nare\nstill\nfaithful\nto the original"
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": "AUs representation. Our\nresults showed that, when fine-tuned"
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": "for emotions,\nthe model still retains some ability to recognize"
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": "AUs. This is an important step towards explicable systems for"
        },
        {
          "ME-Graph [BP4D]\n49%\n81%": "emotion recognition."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUs. This is an important step towards explicable systems for": "emotion recognition."
        },
        {
          "AUs. This is an important step towards explicable systems for": "REFERENCES"
        },
        {
          "AUs. This is an important step towards explicable systems for": ""
        },
        {
          "AUs. This is an important step towards explicable systems for": "[1] Marian\nStewart Bartlett, Gwen Littlewort, Mark G Frank, Claudia"
        },
        {
          "AUs. This is an important step towards explicable systems for": "Lainscsek, Ian R Fasel, Javier R Movellan, et al. Automatic recognition"
        },
        {
          "AUs. This is an important step towards explicable systems for": "of\nfacial\nactions\nin spontaneous\nexpressions.\nJ. Multim., 1(6):22–35,"
        },
        {
          "AUs. This is an important step towards explicable systems for": "2006."
        },
        {
          "AUs. This is an important step towards explicable systems for": "[2] Willams Costa, David Macˆedo, Cleber Zanchettin, Estefan´ıa Talavera,"
        },
        {
          "AUs. This is an important step towards explicable systems for": "Lucas Silva Figueiredo,\nand Veronica Teichrieb.\nA fast multiple\ncue"
        },
        {
          "AUs. This is an important step towards explicable systems for": "Available at SSRN\nfusing approach for human emotion recognition."
        },
        {
          "AUs. This is an important step towards explicable systems for": ""
        },
        {
          "AUs. This is an important step towards explicable systems for": "4255748."
        },
        {
          "AUs. This is an important step towards explicable systems for": "[3]\nEkman, P., & Friesen, and W. V.\nFacial action coding system (facs),"
        },
        {
          "AUs. This is an important step towards explicable systems for": "1978."
        },
        {
          "AUs. This is an important step towards explicable systems for": "American Psychologist,\n[4]\nPaul Ekman.\nFacial expression and emotion."
        },
        {
          "AUs. This is an important step towards explicable systems for": ""
        },
        {
          "AUs. This is an important step towards explicable systems for": "vol. 48, no. 4, 1993, page 384–392."
        },
        {
          "AUs. This is an important step towards explicable systems for": "[5] Mohammed FadhelAljunid and DH Manjaiah. A survey on recommen-"
        },
        {
          "AUs. This is an important step towards explicable systems for": "International\ndation systems for social media using big data analytics."
        },
        {
          "AUs. This is an important step towards explicable systems for": ""
        },
        {
          "AUs. This is an important step towards explicable systems for": "Journal of Latest Trends in Engineering and Technology, pages 48–58,"
        },
        {
          "AUs. This is an important step towards explicable systems for": ""
        },
        {
          "AUs. This is an important step towards explicable systems for": "2017."
        },
        {
          "AUs. This is an important step towards explicable systems for": "[6] Yingruo Fan,\nJacqueline C. K. Lam,\nand Victor O. K. Li.\nFacial"
        },
        {
          "AUs. This is an important step towards explicable systems for": "action unit\nintensity estimation via\nsemantic\ncorrespondence\nlearning"
        },
        {
          "AUs. This is an important step towards explicable systems for": ""
        },
        {
          "AUs. This is an important step towards explicable systems for": "with dynamic graph convolution, 2020."
        },
        {
          "AUs. This is an important step towards explicable systems for": ""
        },
        {
          "AUs. This is an important step towards explicable systems for": "[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren,\nand\nJian Sun.\nDeep"
        },
        {
          "AUs. This is an important step towards explicable systems for": "residual\nlearning for\nimage recognition, 2015."
        },
        {
          "AUs. This is an important step towards explicable systems for": ""
        },
        {
          "AUs. This is an important step towards explicable systems for": "[8]\nJiyoung\nLee,\nSeungryong\nKim,\nSunok\nKim,\nJungin\nPark,\nand"
        },
        {
          "AUs. This is an important step towards explicable systems for": ""
        },
        {
          "AUs. This is an important step towards explicable systems for": "Kwanghoon Sohn. Context-aware emotion recognition networks, 2019."
        },
        {
          "AUs. This is an important step towards explicable systems for": "[9] Cheng Luo, Siyang Song, Weicheng Xie, Linlin Shen, and Hatice Gunes."
        },
        {
          "AUs. This is an important step towards explicable systems for": "Learning multi-dimensional\nedge feature-based AU relation graph for"
        },
        {
          "AUs. This is an important step towards explicable systems for": ""
        },
        {
          "AUs. This is an important step towards explicable systems for": "Proceedings\nof\nthe\nThirty-First\nfacial\naction\nunit\nrecognition.\nIn"
        },
        {
          "AUs. This is an important step towards explicable systems for": ""
        },
        {
          "AUs. This is an important step towards explicable systems for": "International\nJoint Conference on Artificial\nIntelligence.\nInternational"
        },
        {
          "AUs. This is an important step towards explicable systems for": "Joint Conferences on Artificial\nIntelligence Organization,\njul 2022."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "Trinh, and Jeffrey F. Cohn. Disfa: A spontaneous facial action intensity"
        },
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "database.\nIEEE Transactions on Affective Computing, 4(2):151–160,"
        },
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "2013."
        },
        {
          "[10]": "[11] Alejandro Newell, Kaiyu Yang,",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "and\nJia Deng.\nStacked\nhourglass"
        },
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "networks for human pose estimation, 2016."
        },
        {
          "[10]": "[12]",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "Enrique S´anchez-Lozano, Georgios Tzimiropoulos, and Michel F. Val-"
        },
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "star.\nJoint\naction\nunit\nlocalisation\nand\nintensity\nestimation\nthrough"
        },
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "heatmap regression. CoRR, abs/1805.03487, 2018."
        },
        {
          "[10]": "[13]",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "Tassinary, Louis G.,\n, and John T. Cacioppo. Unobservable facial actions"
        },
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "and emotion. Psychological Science, vol. 3, no. 1, 1992, page 28–33."
        },
        {
          "[10]": "[14] C.) Vinola and K.) Vimaladevi. A survey on human emotion recognition",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": ""
        },
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "approaches, databases and applications. ELCVIA : Electronic Letters on"
        },
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "Computer Vision and Image Analysis, pages 24–44, 2015."
        },
        {
          "[10]": "[15] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M.",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": ""
        },
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "Bronstein,\nand Justin M. Solomon.\nDynamic graph cnn for\nlearning"
        },
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "on point clouds, 2019."
        },
        {
          "[10]": "[16] Xing Zhang, Lijun Yin, Jeffrey F. Cohn, Shaun Canavan, Michael Reale,",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": ""
        },
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "Andy Horowitz, Peng Liu, and Jeffrey M. Girard. Bp4d-spontaneous:"
        },
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "a high-resolution spontaneous 3d dynamic facial expression database."
        },
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "Image and Vision Computing, 32(10):692–706, 2014. Best of Automatic"
        },
        {
          "[10]": "",
          "S. Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip": "Face and Gesture Recognition 2013."
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Automatic recognition of facial actions in spontaneous expressions",
      "authors": [
        "Marian Stewart",
        "Gwen Littlewort",
        "G Mark",
        "Claudia Frank",
        "Ian Lainscsek",
        "Javier Fasel",
        "Movellan"
      ],
      "year": "2006",
      "venue": "J. Multim"
    },
    {
      "citation_id": "2",
      "title": "A fast multiple cue fusing approach for human emotion recognition",
      "authors": [
        "Willams Costa",
        "David Macêdo",
        "Cleber Zanchettin",
        "Estefanía Talavera",
        "Lucas Silva Figueiredo",
        "Veronica Teichrieb"
      ],
      "venue": "A fast multiple cue fusing approach for human emotion recognition"
    },
    {
      "citation_id": "3",
      "title": "Facial action coding system (facs)",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Facial action coding system (facs)"
    },
    {
      "citation_id": "4",
      "title": "Facial expression and emotion",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1993",
      "venue": "American Psychologist"
    },
    {
      "citation_id": "5",
      "title": "A survey on recommendation systems for social media using big data analytics",
      "authors": [
        "Mohammed Fadhelaljunid",
        "Manjaiah"
      ],
      "year": "2017",
      "venue": "International Journal of Latest Trends in Engineering and Technology"
    },
    {
      "citation_id": "6",
      "title": "Facial action unit intensity estimation via semantic correspondence learning with dynamic graph convolution",
      "authors": [
        "Yingruo Fan",
        "Jacqueline Lam",
        "O Victor",
        "Li"
      ],
      "year": "2020",
      "venue": "Facial action unit intensity estimation via semantic correspondence learning with dynamic graph convolution"
    },
    {
      "citation_id": "7",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2015",
      "venue": "Deep residual learning for image recognition"
    },
    {
      "citation_id": "8",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2019",
      "venue": "Context-aware emotion recognition networks"
    },
    {
      "citation_id": "9",
      "title": "Learning multi-dimensional edge feature-based AU relation graph for facial action unit recognition",
      "authors": [
        "Cheng Luo",
        "Siyang Song",
        "Weicheng Xie",
        "Linlin Shen",
        "Hatice Gunes"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization"
    },
    {
      "citation_id": "10",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "Mohammad Mahoor",
        "Kevin Bartlett",
        "Philip Trinh",
        "Jeffrey Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Stacked hourglass networks for human pose estimation",
      "authors": [
        "Alejandro Newell",
        "Kaiyu Yang",
        "Jia Deng"
      ],
      "year": "2016",
      "venue": "Stacked hourglass networks for human pose estimation"
    },
    {
      "citation_id": "12",
      "title": "Joint action unit localisation and intensity estimation through heatmap regression",
      "authors": [
        "Enrique Sánchez-Lozano",
        "Georgios Tzimiropoulos",
        "Michel Valstar"
      ],
      "year": "2018",
      "venue": "Joint action unit localisation and intensity estimation through heatmap regression"
    },
    {
      "citation_id": "13",
      "title": "Unobservable facial actions and emotion",
      "authors": [
        "Louis Tassinary",
        "John Cacioppo"
      ],
      "year": "1992",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "14",
      "title": "A survey on human emotion recognition approaches, databases and applications",
      "authors": [
        "C Vinola",
        "K Vimaladevi"
      ],
      "year": "2015",
      "venue": "A survey on human emotion recognition approaches, databases and applications"
    },
    {
      "citation_id": "15",
      "title": "Dynamic graph cnn for learning on point clouds",
      "authors": [
        "Yue Wang",
        "Yongbin Sun",
        "Ziwei Liu",
        "Sanjay Sarma",
        "Michael Bronstein",
        "Justin Solomon"
      ],
      "year": "2019",
      "venue": "Dynamic graph cnn for learning on point clouds"
    },
    {
      "citation_id": "16",
      "title": "Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "Xing Zhang",
        "Lijun Yin",
        "Jeffrey Cohn",
        "Shaun Canavan",
        "Michael Reale",
        "Andy Horowitz",
        "Peng Liu",
        "Jeffrey Girard"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    }
  ]
}