{
  "paper_id": "2312.05447v2",
  "title": "From Static To Dynamic: Adapting Landmark-Aware Image Models For Facial Expression Recognition In Videos",
  "published": "2023-12-09T03:16:09Z",
  "authors": [
    "Yin Chen",
    "Jia Li",
    "Shiguang Shan",
    "Meng Wang",
    "Richang Hong"
  ],
  "keywords": [
    "Dynamic facial expression recognition",
    "model adaptation",
    "transfer learning",
    "emotion ambiguity"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Dynamic facial expression recognition (DFER) in the wild is still hindered by data limitations, e.g., insufficient quantity and diversity of pose, occlusion and illumination, as well as the inherent ambiguity of facial expressions. In contrast, static facial expression recognition (SFER) currently shows much higher performance and can benefit from more abundant highquality training data. Moreover, the appearance features and dynamic dependencies of DFER remain largely unexplored. Recognizing the potential in leveraging SFER knowledge for DFER, we introduce a novel Static-to-Dynamic model (S2D) that leverages existing SFER knowledge and dynamic information implicitly encoded in extracted facial landmark-aware features, thereby significantly improving DFER performance. Firstly, we build and train an image model for SFER, which incorporates a standard Vision Transformer (ViT) and Multi-View Complementary Prompters (MCPs) only. Then, we obtain our video model (i.e., S2D), for DFER, by inserting Temporal-Modeling Adapters (TMAs) into the image model. MCPs enhance facial expression features with landmark-aware features inferred by an off-the-shelf facial landmark detector. And the TMAs capture and model the relationships of dynamic changes in facial expressions, effectively extending the pre-trained image model for videos. Notably, MCPs and TMAs only increase a fraction of trainable parameters (less than +10%) to the original image model. Moreover, we present a novel Emotion-Anchors (i.e., reference samples for each emotion category) based Self-Distillation Loss to reduce the detrimental influence of ambiguous emotion labels, further enhancing our S2D. Experiments conducted on popular SFER and DFER datasets show that we have achieved a new state of the art.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "This paper introduces a novel method to transfer the pretrained image model for DFER efficiently. Therefore, we mainly review the prior works about DFER and efficient transfer learning techniques.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Dynamic Facial Expression Recognition In The Wild",
      "text": "In the early stages, researchers conducted evaluations on datasets in laboratory settings, relying on manually designed features. As deep learning methods gained prominence and large-scale DFER datasets became available in recent years, researchers started employing data-driven techniques to tackle in-the-wild DFER challenges. At present, DFER research methods fall into three main categories. The first category involves using 3D CNN  [21]  to simultaneously model temporal and spatial information. However, this method is computationally expensive, and the models may not be easily scalable to deeper architectures. The second combines 2D CNN with RNN  [4] ,  [22] ,  [23] . It first extracts features from each frame using 2D CNN and then models temporal information using RNN. The third emerging trend in research involves the utilization of Transformer methods. The research conducted by Former-DFER  [8]  utilizes convolution-based spatial Transformer and temporal Transformer to achieve spatiotemporal fusion. STT  [24]  employs ResNet18 to extract features and, in combination with Transformer, jointly learns spatiotemporal features. IAL  [11]  introduces a global convolution-attention block and an intensity-aware loss to differentiate samples based on varying expression intensities. In contrast to the aforementioned approaches, our method leverages the prior knowledge of facial landmark detections  [43]  and SFER data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Parameter-Efficient Transfer Learning",
      "text": "Parameter-efficient transfer learning techniques  [42] ,  [44] -  [47]  are first introduced in natural language processing (NLP) due to the rising computation costs of fully fine-tuning the expanding language models for various downstream tasks. The goal of these techniques is to decrease the number of trainable parameters thus reducing the computational costs, while achieving a comparable or better performance compared to full fine-tuning. Recently, parameter-efficient transfer learning is also introduced in computer vision, and it can be divided into prompt  [48] -  [51]  learning and adapting  [29] -  [32]  learning techniques. The former usually focuses on transfer learning in the same domain (e.g., image-to-image or videoto-video), whereas the latter often targets adapting an image model for video-based tasks. The work most related to ours is AIM  [30] . It utilizes lightweight adapters to modify the vision transformer blocks and adapt a frozen, pre-trained image model for performing video action recognition. However, there are several major differences. Firstly, AIM adapts to downstream tasks by modifying the original transformer layer directly, while our TMA is placed between the adjacent transformer blocks in the form of residual. Secondly, AIM uses an unlearnable self-attention for temporal modeling while our TMA employs a learnable one.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology",
      "text": "This section elaborates on our novel Static-to-Dynamic model (S2D). Rather than devising and fully fine-tuning an existing video model or even training a DFER model from scratch, S2D adapts a landmark-aware image model for facial expression recognition in videos (i.e., DFER) effectively and efficiently. To be specific, we first train a SFER model using static FER datasets, and then merely add and tune a small fraction of adaptation-learning parameters to achieve effective transfer learning for the DFER task. This unique approach\n\nFig.  2 . Overall architecture of the proposed method. Our S2D accepts as input a facial expression image (or facial expression image sequence) X F and a landmark-aware feature (or landmark-aware feature sequence) X L . The facial expression image and landmark-aware feature are embedded with patch embedding layers and fed into the transformer layers {E l } L-1 l=0 borrowed from ViT. Temporal-Modeling Adapter (TMA) is used to capture temporal information T l while Multi-View Complementary Prompter (MCP) uses landmark-aware features to generate guiding prompts P l to enhance the image-level representational ability for both SFER and DFER tasks. Note that the position embedding is added to P 0 after the first MCP block, and TMA is only used for the DFER task. Sg means stop gradient. yields promising performance on DFER benchmarks while simultaneously ensuring parameter efficiency.\n\nAn overall of our model is presented in Fig.  2 , showing our model architecture, computation flow (detailed in section III-A), multi-view input and interaction (detailed in section III-B), and training loss (detailed in section III-D). Section III-C introduces how to expand the image model to an efficient video model in detail.\n\nA. Preliminary 1) Problem Formulation: Given a video clip or a facial image sequence X F with an emotion label Y , the task of DFER is to learn a mapping function F θ (X F ) → Y , where F θ denotes a model and θ represents its learnable parameters. In this work, we employ the facial landmark-aware features X L extracted from X F using an off-the-shelf facial landmark detector as an auxiliary view. Thus, the input of F θ is extended to (X F , X L ). Accordingly, the mapping goal has changed to\n\n2) S2D Model: As illustrated in Fig.  2 , our devised S2D accepts as input the facial image sequence X F ∈ R T ×C×H×W and facial landmark-aware feature sequence\n\n. Here, T , C, H, and W represent the number of frames, channels, width, and height of the facial image sequence, respectively. Firstly, we feed the two flows, X F and X L , into individual patch embedding layers separately. Each input frame is divided into N patches, then mapped and flattened into a D-dimensional latent space. We refer to these embeddings as the facial expression tokens H 0 ∈ R T ×N ×D and facial landmark tokens A 0 ∈ R T ×N ×D . Then, H 0 and a [class] token x class with position embeddings are fed into the transformer layers {E l } L-1 l=0 of Vision Transformer (ViT)  [52] . Note that H 0 and A 0 are firstly sent into the MCP module to generate guiding prompts, while H 0 is sent to the TMA to capture the temporal information. Finally, the learned guiding prompts P ∈ R T ×N ×D and temporal information tokens T ∈ R T ×N ×D are added to the original facial expression tokens in a form of residual:\n\nwhere H l ′ is the prompted tokens, and P l+1 , T l+1 are guiding prompts and temporal information from the (l + 1)th MCP module and TMA module, respectively. Next, H l ′ is fed into the transformer layer E l to extract a more powerful image-level representation:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Image-Level Representation Enhancement",
      "text": "In this section, we will introduce how to enhance imagelevel representational ability through the selection of static facial expression features, the incorporation of facial landmarkaware features, and the generation of guiding prompts.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "1) Selection Of Static Facial Expression Feature:",
      "text": "To harness the image-level representational capacity of the SFER model and gain more robust static facial expression features, we employ AffectNet  [16]  as our pre-training dataset. AffectNet is the existing largest SFER dataset, which contains more than 1M face images from the Internet and about 450,000 manually annotated images. We first train the model on the AffectNet dataset to get a robust expression appearance representation, then fine-tune it on other FER datasets.\n\n2) Facial Landmark-Aware Feature: To bolster image-level representation for both SFER and DFER, we introduce facial landmark-aware features extracted from facial images as an auxiliary view to guide model learning. These facial landmarkaware features are the features in the penultimate stage of landmark detector MobileFaceNet  [43] . And they exhibit sensitivity to key facial regions, such as the mouth, nose, and eyes, enabling the model to focus more on local details relevant to facial expressions. They are used to enhance the model's capacity to represent in-the-wild facial expressions. Furthermore, the process of dynamic facial changes (e.g., muscle movements) is implicitly encoded within the sequence of facial landmark-aware features, which is also used to enhance the model's ability to capture dynamic information of facial expressions in videos.\n\n3) Guiding Prompts Generation: To fully exploit the potential of both facial expression and facial landmark-aware features, we utilize a Multi-View Complementary Prompter (MCP) module to generate guiding prompts, which is proposed in the field of multi-modal object tracking and originally named Modality-Complementary Prompter in  [49] . MCP takes in facial expression tokens represented as H l and facial landmark tokens represented as A l , producing guiding prompts P l+1 for the next transformer layer. The process can be described as follows:\n\nFirstly, H l and A l are reshaped and projected to a lowerdimensional latent space:\n\nwhere g 1 and\n\nN are projected embeddings. Here, D ′ is the output channel number of the convolutional layer. Subsequently, the spatial attentionlike operation is performed on M H to emphasize details related to facial expressions:\n\nwhere\n\nand λ is a learnable weighted parameter per block  [49] . Finally, the learned guiding prompts are obtained as follows:\n\nwhere g 3 is a 1×1 convolutional layer, and P l+1 ∈ R T ×N ×D .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Expanding The Image Model To Efficient Video Model",
      "text": "The model has developed a strong image-level representational ability through training on the SFER dataset, but it can only model spatial dimension frame by frame. To expand its ability for efficiently capturing the dynamic changes of facial expressions in videos, we propose a Temporal-Modeling Adapter (TMA) module. TMA is placed between transformer layers and comprises three components: (1) a Temporal Adapter (T-Adapter), (2) a LayerNorm layer, and (3) a Vanilla Adapter  [42] , as illustrated in Fig.  3 .\n\nThe T-Adapter focuses on temporal modeling through the incorporation of Temporal Multi-Head Self-Attention (T-MSA)  [30] . Initially, the video embeddings\n\nFig.  3 . Temporal-Modeling Adapter (TMA) for temporal adaptation. The input H l ∈ R T ×N ×D is fed into a Temporal Adapter to capture temporal information, then it is fed into a LayerNorm and a Vanilla Adapter to reduce the domain gap between SFER and DFER. T l+1 ∈ R T ×N ×D is learned temporal information.\n\nH l ′ T ∈ R N ×T ×γD by a simple Linear layer and passed through a GELU activation layer, where γ is downsampling rate.\n\nwhere\n\nT is fed into the T-MSA to capture the relationships between frames as follows:\n\nwhose processes are described by equations 9, 10, and 11.\n\nwhere Q, K, V are the duplicates of H l ′ T and the projections are parameter matrices:\n\nSA represents the Self-Attention mechanism, head i corresponds to the i-th attention head, and Cat denotes the concatenation operation. Finally, the H l ′′ T is upsampled to the original dimension as below:\n\nwhere\n\nare learnable parameters of upsampling linear layer in T-Adapter.\n\nAfter that, H l ′′′ T is reshaped back to H l ′′ with the shape of T × N × D and fed into LayerNorm Layer. To enhance the TMA's capacity to capture temporal information and reduce the domain gap between SFER and DFER, we equip it with a Vanilla Adapter  [42] . The Vanilla Adapter has two simple Linear layers with a GELU activation function. The process can be described by the following equations:\n\nwhere\n\nare learnable parameters and T l+1 ∈ R T ×N ×D is the temporal information tokens learned from (l + 1)-th TMA.\n\nWith the integration of TMA modules, the image model can be efficiently extended to a video model, with only 9M learnable parameters. As shown in Fig.  2  These reference samples for each emotion category in emotion anchors provide an auxiliary supervision signal to prevent ambiguous expression labels from deteriorating the performance of the FER model. We assume that the majority of the manual annotations are reliable and FER model can learn to recognize the correct samples gradually. Thus, we can select a bag of reference samples for each expression category to serve as emotion anchors. These anchors can be used to estimate a probability distribution of emotion and guide the model learning. Specifically, during the training process, we maintain two queues for each emotion, denoted as\n\nis the number of emotion category, and both P and Q have S reference samples randomly selected from the training set. Queue Q stores the model's output feature vectors v from the last transformer layer, while queue P stores the output probabilities p (normalized by softmax), where ∥p∥ = 1. It's important to note that queues P and Q are not static, they are dynamically updated throughout the training process. Specifically, new reference samples are added to P and Q based on their ground truth labels from the current mini-batch, and the earlier samples are removed to maintain a fixed size. This dynamic updating mechanism ensures that the model is continually exposed to the most recent and relevant data, which helps in refining its understanding of each emotion category gradually.\n\nDuring training, for each sample\n\ni=1 , the corresponding output features and probabilities are denoted as v i and p i , respectively. As shown in Fig.  4 , we first calculate the cosine similarity between v i and each vector v c j in Q, yielding similarity scores between v i and the samples in C groups of emotion anchors,\n\n}. The similarity score β c i,j between v i and v c j is computed using the formula:\n\nNext, we select the top K samples in each emotion-anchor, resulting in the final scores",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion Anchors",
      "text": "Compare Afterwords, we obtain the soft label Y sof t i for the current input sample X i :\n\nFinally, we supervise the model using the labeled Y i and the pseudo label Y sof t i , described as follows:\n\nwhere CE and BCE represent the cross-entropy loss and binary cross-entropy loss, respectively. The total supervision loss is a combination of these two terms weighted by a hyperparameter η:\n\nIV. EXPERIMENTS",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "Our proposed model is evaluated on a diverse range of datasets: SFER datasets include RAF-DB  [15] , AffectNet-7/8  [16] , and FERPlus  [17] , and DFER datasets encompass DFEW  [4] , FERV39K  [18] , and MAFW  [19] . RAF-DB contains 29,672 real-world facial images annotated with basic expressions by  40",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Evaluation Metrics",
      "text": "We evaluate the performance on SFER datasets with Accuracy, which has been widely used for SFER tasks. As for DFER datasets, we follow the approach of previous studies  [8] ,  [11] ,  [14] ,  [53]  and report both unweighted average recall (UAR), which represents the averaged accuracy across classes, and weighted average recall (WAR), which indicates the overall accuracy (the same as the metric of SFER).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Implementation Details",
      "text": "In all experiments, we extract facial landmark-aware features with MobileFaceNet  [43] , a widely used facial landmark detection tool, by default. ViT-B/16  [52]  is used as the backbone of S2D. We follow the same training strategy as Li et al.  [54]  to train our SFER model. For the DFER model, we initialized it with the weights pre-trained on the AffectNet-7 dataset and fine-tuned it on the DFER dataset. Following  [8] ,  [11] ,  [19] ,  [24] , we sample 16 frames from each video clip and resize them to 224×224. Incidentally, we initially perform crop and alignment on the MAFW dataset using the RetinaFace  [55]  model from the deepface toolkit 2    [56] . Regarding training strategy, we mainly follow MAE-DFER  [14] . Specifically, we employ an AdamW optimizer with β 1 = 0.9 and β 2 = 0.95, an overall batch size of 64, a base learning rate lr base of 1e -5, and a weight decay of 0.05. We linearly scale the base learning rate according to the overall batch size N bs , using the formula: lr = lr base × N bs 8 . Furthermore, we use cosine annealing to decay the learning rate with the model undergoing training for a total of 100 epochs. The downsampling rate γ in Eq. 7 is set to 0.25 and the hyperparameter η in Eq. 21 is gradually increased from 0 to 1. For all DFER datasets, the queue size S for each emotion category is set to 16, and the K is set to 2. During training, only the parameters of MCPs, TMAs, and the Classifier are tunable, while all other parameters are frozen. All experiments are conducted on 8 Nvidia 3090Ti GPUs using the PyTorch framework.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D. Ablation Studies",
      "text": "In this section, we conduct ablation experiments on AffectNet-7/8  [16] , RAF-DB  [15] , FERPlus  [17] , DFEW  [4]  and FERV39K  [18]  datasets to demonstrate the effectiveness of several key components in S2D. For simplicity, we only report the results of fold 1 (fd1) for DFEW, following the practice of prior work  [14] .\n\n1) Image-Level Representation Enhancement: Selection of static facial expression features. Cross-dataset evaluations were initially conducted on the RAF-DB and AffectNet-7 datasets to assess their respective robustness.  results (highlighted in bold), which revealed a significant performance decline of the SFER model pre-trained on RAF-DB when tested on the AffectNet-7 dataset, culminating in a 44.64% drop in accuracy. Conversely, the model pre-trained on AffectNet-7, when evaluated on the RAF-DB dataset, demonstrated an accuracy improvement of 9.74%. It is well known that the AffectNet dataset has a higher noise level than the RAF-DB dataset. To explore whether this difference arises from data size or sample diversity, a further experiment about AffectNet was undertaken using an equivalent amount of data as RAF-DB. The subset sampled from AffectNet-7 is named AffectNet-Small. Despite AffectNet-Small having a much smaller data size than AffectNet-7, it only experienced a 5.02% accuracy drop on AffectNet-7 and still performs well on RAF-DB. For objectivity and fairness, a third-party evaluation was also introduced using the FERPlus testing set, consisting of 3,589 gray-scale images, each sized to 48x48 pixels. As shown in Table  I , testing on FERPlus reveals a pronounced 14.86% accuracy drop for the SFER model pre-trained on RAF-DB, but only a slight drop of 2.1% accuracy for the model pre-trained on AffectNet-7. Interestingly, the model trained on RAF-DB outperforms AffectNet-7 when tested on the FERPlus dataset. We hypothesize that this performance disparity is attributable to the greater consistency in annotation methods between RAF-DB and FERPlus, compared to AffectNet. To eliminate the influence of initial weights, we conducted additional experiments using MAE-ImageNet1K  [57]  weights for initialization. The experimental results, as highlighted in Table  I , are consistent with our previous observations. Specifically, the model trained on AffectNet-7 shows a lesser performance drop when tested on other datasets compared to the model trained on RAF-DB, indicating that the model trained on AffectNet-7 seems to be more robust than RAF-DB. Subsequent experiments were also conducted on DFER datasets by initializing the DFER model using various SFER models. These SFER models were initialized with the weights pre-trained on ImageNet-1K  [58]  or AffectNet with MAE  [57]  pre-training method and fine-tuned on RAF-DB, FERPlus, and AffectNet-7, respectively. This was done to evaluate the generalization ability of the static appearance features obtained through pre-training on various SFER datasets. As shown in Table  II , the DFER model pre-trained on the AffectNet-7 dataset exhibits superior performance on both DFEW and  FERV39K datasets, particularly when the SFER model is initialized with general domain weights. Given that the model pre-trained on AffectNet-7 outperforms others on DFEW and FERV39K, and considering the larger data size and greater diversity of samples in the AffectNet-7 dataset, we recommend utilizing the SFER model pre-trained on AffectNet-7 for enhanced downstream generalization performance.\n\nAblation study on the landmark-aware features. We compare the performance of three lightweight facial landmark detection models from PyTorch Face Landmark toolkit 3  on extracting landmark-aware features for in-the-wild DFER tasks. These models are PFLD  [60] , MobileNetV2@56  [59] , Mo-bileNetV2@224  [59] , MobileNetV2 ED  [59] , and Mobile-FaceNet  [43] , which have different localization precision and complexity levels. Table  III  shows that the features extracted by these models perform well on both DFEW and FERV39K datasets. However, as the localization precision of the facial landmark detection model decreases, the performance of the model declines to varying degrees on both DFEW and FERV39K, with the decline being particularly noticeable on DFEW. Considering that the MobileFaceNet  [43]  model has the lowest inter-ocular normalization error (ION) and achieves the highest performance on the more challenging and representative FERV39K dataset, we choose it as the facial landmark model for extracting landmark-aware features in the following experiments.\n\n2) Ablation study on the Multi-View Complementary Prompter: As presented in Table  IV , we evaluated the impact  of different fusion methods on SFER and DFER testing sets. Compared to the baseline (line 1), incorporating facial landmark-aware features via MCP yielded notable improvements (+1.14% on RAF-DB, +0.33% on AffectNet-7, +0.83% on FERPlus, +0.68% WAR on DFEW and +1.12% WAR on FERV39K) in performance across both SFER and DFER datasets. This emphasizes the significance of facial landmarkaware features in both SFER and DFER tasks. Furthermore, replacing MCP with the common fusion module of concatenation and projection (CAP, line 3) resulted in marked performance declines across all datasets. This suggests that MCP is more effective for integrating static features and landmarkaware features, whereas CAP may potentially destroy the structure of the original static features and degrade model performance.\n\n3) Ablation study on the Temporal-Modeling Adapter: We evaluate the model's performance with various adapters and report the results in Table  V . The results reveal that the Vanilla Adapter  [42]  brings 4.67% UAR with 3.80% WAR and 5.44% UAR with 4.88% WAR improvements on DFEW and FERV39K testing sets, respectively. Although this signifies a positive impact on the model's performance, it remains somewhat limited due to its lack of temporal information modeling. In contrast, the Temporal Adapter exhibits significant improvements over the Vanilla Adapter, surpassing it by 2.65% UAR with 3.59% WAR on DFEW, and 0.39% UAR with 0.24% WAR on the FERV39K dataset. This enhancement can be attributed to the robust temporal modeling capabilities brought by T-MSA in the Temporal Adapter. Consequently, we  combine the Vanilla Adapter with the Temporal Adapter, and introduce the Temporal-Modeling Adapter (TMA), leading to further performance improvements, particularly on FERV39K (+2.28% UAR and +1.41% WAR).\n\n4) The Effectiveness of SDL: To validate the effectiveness of the auxiliary supervised signal provided by our proposed SDL in reducing the interference caused by ambiguous annotations, we compare it with the original One-Hot supervision signal and the commonly used Label Smoothing method. As Table  VI  shows, Label Smoothing slightly decreases the performance of WAR (-0.13% WAR on DFEW and -0.20% WAR on FERV39K). In contrast, our SDL considerably improves the WAR on DFEW and FERV39K by 0.04% and 0.43%, respectively. We further visualize the output probability distribution of the model with SDL in Fig.  8 , which demonstrates that SDL significantly enhances the model's discriminability for ambiguous samples. The observed decline in UAR on FERV39K is attributable to data imbalance. Based on experimental experience, the model exhibits a stronger emphasis on learning expressions from specific classes with larger data volumes. While WAR is sensitive to the accuracy of these categories, UAR is primarily influenced by the minor categories. Therefore, an increase in WAR often comes at the expense of a decrease in UAR.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "5) Contribution Of Proposed Components:",
      "text": "In Table  VII , we conducted ablation experiments on the DFEW and FERV39K testing sets to quantify the contributions of the proposed components. Compared to the baseline (line 1), incorporating the MCP (line 2) improved the performance by 3.22% UAR and 2.95% WAR on DFEW, and 1.21% UAR and 1.22% WAR on FERV39K. This indicates the efficacy of facial landmarkaware features in enhancing the model's image-level representation capabilities. Moreover, integrating the Temporal-Modeling Adapter (TMA) module (line 3) further enhanced performance (+7.23% UAR and +7.39% WAR on DFEW, +9.36% UAR and +6.10% WAR on FERV39K), attributed   VII ). to its powerful ability to capture the temporal information of facial expressions. The SDL (line 4) still improved the performance by 0.04% WAR on DFEW and 0.43% WAR on FERV39K, even after S2D achieved significant improvement over the baseline. This highlights the significance of SDL, particularly in mitigating the interference caused by ambiguous annotations. Overall, the results in Table  VII  demonstrate the effectiveness of each component and their collective contribution to S2D. Furthermore, to specifically highlight the comparative advantage of our proposed method over the baseline, we have visualized the overall accuracy (WAR) and the detailed breakdown of class accuracy for each emotion category on the DFEW and FERV39K datasets in Fig.  5 . This visualization clearly demonstrates the method's enhanced efficacy at the class level when compared directly with the baseline.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "E. Comparison With State-Of-The-Art Methods",
      "text": "We compared our method with existing state-of-the-art methods on two sets of datasets: SFER datasets (RAF-DB   [15] , AffectNet  [16] , and FERPlus  [17] ) and DFER datasets (DFEW  [4] , FERV39K  [18] , and MAFW  [19] ). In this section, we report the average results across all 5 folds for DFEW and MAFW datasets. 1) Results on SFER Datasets: Table VIII provides a comparison with several SOTA methods on popular SFER datasets. Our method achieved the best results on RAF-DB, AffectNet-7, and FERPlus. Particularly noteworthy is our achievement of 92.57% accuracy on RAF-DB, surpassing existing methods by 0.50%. Despite the limited quantity of data in FERPlus and the small gray image size of 48x48 (meaning an obvious domain gap), our method still achieved the highest accuracy.\n\n2) Results on DFER Datasets: Firstly, we compare S2D with previous state-of-the-art supervised learning models on DFEW, FERV39K, and MAFW. Table  IX  shows that S2D significantly outperformed the previous best methods (i.e., IAL  [11] , M3DFEL  [53] ) and achieved a notable improvement of 6.47% UAR with 6.73% WAR and 5.05% UAR with 4.16% WAR on DFEW and FERV39K datasets, respectively. Regarding MAFW, S2D achieves a new state-of-the-art performance, surpassing the previous best methods  [19]  by a significant margin of 6.59% in UAR and 9.19% in WAR. These significant improvements indicate that our method can learn powerful representations for DFER by pre-training on a large-scale SFER dataset.\n\nSecondly, we compare S2D with the self-supervised method MAE-DFER  [14] , which was pre-trained on a large-scale video dataset (over 1 million video clips) with a selfsupervised approach  [71] . To ensure fairness, we adopted the same testing strategy as MAE-DFER. Experimental results demonstrate that our method outperformed MAE-DFER by 1.60%, 0.49%, and 3.06% in WAR on DFEW, FERV39K, and MAFW respectively, with fewer tunable parameters and training costs (refer to Fig.  1 ). Furthermore, our method, trained with an oversampling strategy, also outperformed MAE-DFER in UAR, achieving improvements of 2.04% on DFEW, 0.85% on FERV39K, and 1.54% on MAFW.\n\nWe also compare S2D with two vision-language models CLIPER  [12]  and DFER-CLIP  [13] , which rely on both prior language knowledge and general vision knowledge from CLIP  [27] . The results in Table  IX  indicate that our method demonstrated significant improvements over these vision-language models (+5.84% UAR and +4.78% WAR on DFEW, +2.70% UAR and +0.91% WAR on FERV39K, +4.51% UAR and +4.82% WAR on MAFW).\n\nAdditionally, we present the detailed performance of each emotion on DFEW in Table  X . The results show that S2D outperforms the previous best methods across most facial expressions, regardless of whether they are supervised (e.g., IAL  [11] , M3DFEL  [53] ) or self-supervised methods (e.g., MAE-DFER  [14] ). These improvements are particularly evident in happy, sad, neutral, angry, and surprise. For instance, S2D surpasses M3DFEL by 14.87%, 7.43%, 9.95% on sad, neutral, and angry, respectively. In contrast to MAE-DFER, which achieved an unbiased representation through massivescale self-supervised learning and performed well on disgust, supervised models exhibit bias in modeling disgust due to the lack of related training samples (occupying only 1.2% of the entire dataset). To mitigate this, we trained the S2D model with an oversampling strategy additionally, which greatly improves the performance on the minor classes, disgust and fear (25.52%-1.38%=24.14%, 50.22%-34.71%=15.51%), while keeping the highest UAR and WAR metrics.\n\nIn summary, the promising results on three in-the-wild  datasets demonstrate the strong generalization ability of S2D. We emphasize that our proposed method does not require retraining all model parameters on the DFER dataset. Instead, we only fine-tune a small number of parameters (less than 10% of tunable parameters), making it more parameter-efficient and practical.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "F. Visualization Analysis 1) Attention Visualization:",
      "text": "To confirm the critical role of the facial landmark-aware features in the S2D model, we visualize the attention scores of the last transformer block in Fig.  6 . Our observations reveal that the attention of the S2D model with facial landmark knowledge is highly concentrated on the key regions of the face, which are more informative for FER. In contrast, the attention of the model without facial landmark knowledge is more dispersed, suggesting a tendency to focus on the background and other irrelevant regions. For example, in the first sequence, frame 11 of row 3 demonstrates that the model's attention is primarily directed towards the eyes and mouth, rather than other irrelevant regions as shown in frame 11 of row 2. This distinction is even more pronounced in the second sequence, particularly in frame 2 of row 3 compared to frame 2 of row 2. These demonstrate that the facial landmarkaware feature directs the model's attention towards emotion- Furthermore, by examining the changes in attention over time, we observed that the model without landmark knowledge consistently focuses on specific regions (e.g., eyes in the first sequence row 2, and both eyes and faces in the second sequence row 2) and remains relatively unchanged. In contrast, the model with landmark knowledge exhibits greater sensitivity to changes in muscle movements, with its attention shifting accordingly in row 3 of the two sequences. For example, in the first sequence, when the woman opens her mouth, the model's attention is primarily on her lips, gradually diminishing as her mouth closes (from frame 6 to frame 13). A similar phenomenon also can be observed in the second sequence, where the model's attention initially focuses on the eyes and mouth from frame 1 to frame 11 in row 3, then disappears on the mouth in frame 4, gradually weakens, and finally completely disappears on the mouth from frame 5 to frame 11. These observations strongly demonstrate that our model accurately captures the dynamic changes of facial emotions in the temporal dimension, with muscle movements implicitly encoded in facial landmark-aware features.\n\n2) Visualization of Feature Distribution: We gradually added MCP, TMA and SDL to the baseline model and visualized the high-level features of the DFEW (fd 1) testing set using t-SNE  [72] . Fig.  7  illustrates that our proposed modules significantly enhance the discriminative and separable qualities of high-level features compared to the baseline model. The inclusion of the TMA module notably improves the discriminability of high-level features, indicating its effectiveness in capturing dynamic facial expression changes for DFER. Compared to column 3, each category's feature distribution in column 4 is more concentrated, particularly for happy, which exhibits a more distinct boundary than other expressions. This is attributable to the proposed SDL. However, it is notable that the features corresponding to surprise and fear are intermixed and challenging to distinguish. Additionally, disgust features are dispersed, lacking a clear clustering center. This aligns with the emotion-specific accuracies in Table X and is attributed to the data shortage of disgust in the dataset available.\n\n3) Visualization of Output Probability Distribution: To evaluate the effectiveness of our proposed SDL, a comparison was conducted between the output probability distributions  of the S2D model with and without SDL. As illustrated in Fig.  8 , the woman in the top left corner displays a subtle smile, challenging to discern due to minimal changes in her facial muscles. The model without SDL assigns high scores to both happy and neutral (depicted in the left bar chart) with a marginal difference of 0.02, suggesting the prediction is ambiguous. Conversely, the SDL-enhanced S2D model assigns a higher score to happy than neutral by 0.23 (right bar chart). This phenomenon is similarly observed in the bottom-left example. In the right column of Fig.  8 , although the angry and sad are clearly expressed on the faces of the two men, the model without SDL provides similar scores for the angry and sad (0.43 vs. 0.46, 0.37 vs. 0.35), resulting in wrong predictions. In contrast, the S2D equipped with SDL effectively distinguishes angry and sad with high discriminability.\n\nThe soft labels provided by SDL have fewer ambiguities and help reduce the model's uncertainty. Consequently, the SDL-incorporated model demonstrates higher discriminability for ambiguous expressions (e.g., smile and neutral) and the model without SDL struggles to distinguish between certain ambiguous samples (e.g., right examples in Fig.  8 ).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we propose a simple yet powerful framework, S2D, which adapts a landmark-aware image model for facial expression recognition in videos. This study indicates that the prior knowledge from SFER data and facial landmark detections can be leveraged to enhance DFER performance. The Multi-View Complementary Prompters (MCPs) employed in this work effectively utilize static facial expression features learned on AffectNet  [16]  dataset and facial landmark-aware features from MobileFaceNet  [43] . Furthermore, the S2D is extended from the static model efficiently with Temporal-Modeling Adapters (TMAs) and significantly enhanced by our Emotion-Anchors based Self-Distillation Loss (SDL). Experimental results on widely used benchmarks consistently reveal that S2D achieves performance comparable to previous state-of-the-art methods, demonstrating our model's ability to learn robust image-level representations for SFER and powerful dynamic facial representations for DFER. Lastly, compared to previous self-supervised methods, S2D is much more parameter-efficient and practical for DFER, with only 9M tunable parameters. We believe it will serve as a solid baseline and contribute to relevant research. In the future, we will explore more effective ways to leverage prior facial knowledge and other potential knowledge in the SFER model for improving DFER performance.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Performance comparison of dynamic facial expression recognition",
      "page": 1
    },
    {
      "caption": "Figure 1: Specifically, S2D is a standard Vision",
      "page": 2
    },
    {
      "caption": "Figure 2: Overall architecture of the proposed method. Our S2D accepts as input a facial expression image (or facial expression image sequence) XF",
      "page": 4
    },
    {
      "caption": "Figure 2: , our devised",
      "page": 4
    },
    {
      "caption": "Figure 3: The T-Adapter focuses on temporal modeling through",
      "page": 5
    },
    {
      "caption": "Figure 3: Temporal-Modeling Adapter (TMA) for temporal adaptation. The",
      "page": 5
    },
    {
      "caption": "Figure 2: , only the TMA, MCP,",
      "page": 6
    },
    {
      "caption": "Figure 4: These reference samples for each emotion category in emotion",
      "page": 6
    },
    {
      "caption": "Figure 4: , we first calculate the cosine similarity",
      "page": 6
    },
    {
      "caption": "Figure 4: Emotion-Anchors based Self-Distillation Loss. p is output proba-",
      "page": 6
    },
    {
      "caption": "Figure 5: The comparison of our proposed model with baseline at class",
      "page": 9
    },
    {
      "caption": "Figure 5: This visualization clearly demonstrates the method’s",
      "page": 9
    },
    {
      "caption": "Figure 1: ). Furthermore, our method, trained",
      "page": 10
    },
    {
      "caption": "Figure 6: Visualization of input facial image sequence with landmarks (row 1), and attention scores (only positive values) of the last transformer block without",
      "page": 11
    },
    {
      "caption": "Figure 6: Our observations reveal that the attention of the S2D model",
      "page": 11
    },
    {
      "caption": "Figure 7: High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model",
      "page": 12
    },
    {
      "caption": "Figure 7: illustrates that our proposed modules",
      "page": 12
    },
    {
      "caption": "Figure 8: Visualization of the output probability distribution. Here, An, Di,",
      "page": 12
    },
    {
      "caption": "Figure 8: , the woman in the top left corner displays a subtle",
      "page": 12
    },
    {
      "caption": "Figure 8: , although the angry",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Meng Wang, Fellow,": "Abstract—Dynamic\nfacial\nexpression recognition (DFER)\nin",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "80"
        },
        {
          "Meng Wang, Fellow,": "the wild is\nstill hindered by data\nlimitations,\ne.g.,\ninsufficient",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "S2D (Ours)"
        },
        {
          "Meng Wang, Fellow,": "quantity\nand diversity\nof pose,\nocclusion and illumination,\nas",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "MAE-DFER"
        },
        {
          "Meng Wang, Fellow,": "well as the inherent ambiguity of\nfacial expressions. In contrast,",
          "IEEE\nIEEE, and Richang Hong, Member,": "75"
        },
        {
          "Meng Wang, Fellow,": "static facial expression recognition (SFER) currently shows much",
          "IEEE\nIEEE, and Richang Hong, Member,": "DFER-CLIP"
        },
        {
          "Meng Wang, Fellow,": "higher performance and can benefit\nfrom more abundant high-",
          "IEEE\nIEEE, and Richang Hong, Member,": "IAL"
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "70\nCLIPER"
        },
        {
          "Meng Wang, Fellow,": "quality\ntraining data. Moreover,\nthe\nappearance\nfeatures\nand",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "EST"
        },
        {
          "Meng Wang, Fellow,": "dynamic\ndependencies\nof DFER remain\nlargely\nunexplored.",
          "IEEE\nIEEE, and Richang Hong, Member,": "WAR on DFEW (%)\nFormer-DFER"
        },
        {
          "Meng Wang, Fellow,": "Recognizing\nthe\npotential\nin\nleveraging\nSFER knowledge\nfor",
          "IEEE\nIEEE, and Richang Hong, Member,": "65"
        },
        {
          "Meng Wang, Fellow,": "DFER, we introduce a novel Static-to-Dynamic model (S2D) that",
          "IEEE\nIEEE, and Richang Hong, Member,": "CEFLNet"
        },
        {
          "Meng Wang, Fellow,": "leverages\nexisting\nSFER knowledge\nand\ndynamic\ninformation",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "#Model Size"
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "3D ResNet-18\n60"
        },
        {
          "Meng Wang, Fellow,": "implicitly encoded in extracted facial\nlandmark-aware features,",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "25M"
        },
        {
          "Meng Wang, Fellow,": "thereby significantly improving DFER performance. Firstly, we",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "75M"
        },
        {
          "Meng Wang, Fellow,": "build and train an image model\nfor SFER, which incorporates",
          "IEEE\nIEEE, and Richang Hong, Member,": "C3D\nR(2+1)D-18\n55"
        },
        {
          "Meng Wang, Fellow,": "a standard Vision Transformer (ViT) and Multi-View Comple-",
          "IEEE\nIEEE, and Richang Hong, Member,": "150M"
        },
        {
          "Meng Wang, Fellow,": "mentary Prompters\n(MCPs)\nonly. Then, we\nobtain our\nvideo",
          "IEEE\nIEEE, and Richang Hong, Member,": "(cid:215)\n20 \n30 \n40 \n50 \n80"
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "0\n10\n90\n100"
        },
        {
          "Meng Wang, Fellow,": "model\n(i.e., S2D),\nfor DFER, by\ninserting Temporal-Modeling",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "Number of Tunable Parameters (M)"
        },
        {
          "Meng Wang, Fellow,": "Adapters\n(TMAs)\ninto the\nimage model. MCPs\nenhance\nfacial",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "expression features with landmark-aware features inferred by an",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "Fig. 1.\nPerformance\ncomparison of dynamic\nfacial\nexpression recognition"
        },
        {
          "Meng Wang, Fellow,": "off-the-shelf facial landmark detector. And the TMAs capture and",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "on\nthe DFEW [4]\ntesting\nset. Bubble\nsize\nindicates\nthe model\nsize. Our"
        },
        {
          "Meng Wang, Fellow,": "model the relationships of dynamic changes in facial expressions,",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "proposed S2D achieves\nthe highest weighted average\nrecall\n(WAR) while"
        },
        {
          "Meng Wang, Fellow,": "effectively\nextending\nthe\npre-trained\nimage model\nfor\nvideos.",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "enjoying significantly less number of\ntunable parameters\n(< 10% tunable"
        },
        {
          "Meng Wang, Fellow,": "Notably, MCPs and TMAs only increase a fraction of\ntrainable",
          "IEEE\nIEEE, and Richang Hong, Member,": "parameters of\nthe whole model). Here, we compare our S2D with C3D [5],"
        },
        {
          "Meng Wang, Fellow,": "parameters\n(less\nthan\n+10%)\nto\nthe\noriginal\nimage model.",
          "IEEE\nIEEE, and Richang Hong, Member,": "R(2+1)D-18 [6], 3D ResNet-18 [7], Former-DFER [8], CEFLNet\n[9], EST"
        },
        {
          "Meng Wang, Fellow,": "Moreover, we present a novel Emotion-Anchors\n(i.e.,\nreference",
          "IEEE\nIEEE, and Richang Hong, Member,": "[10],\nIAL [11], CLIPER [12], DFER-CLIP [13] and MAE-DFER [14]."
        },
        {
          "Meng Wang, Fellow,": "samples\nfor each emotion category) based Self-Distillation Loss",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "to reduce the detrimental\ninfluence of ambiguous emotion labels,",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "further enhancing our S2D. Experiments conducted on popular",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "reflect\na\nperson’s\nemotional\nstate"
        },
        {
          "Meng Wang, Fellow,": "SFER and DFER datasets\nshow that we have achieved a new",
          "IEEE\nIEEE, and Richang Hong, Member,": "F ACIAL expressions"
        },
        {
          "Meng Wang, Fellow,": "state of\nthe art.",
          "IEEE\nIEEE, and Richang Hong, Member,": "role\nin\ninterpersonal\ninteractions."
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "Understanding\nthe\nemotional\nstates\nfrom facial\nexpressions"
        },
        {
          "Meng Wang, Fellow,": "Index Terms—Dynamic\nfacial\nexpression recognition, model",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "is\nincreasingly\nsignificant\ndue\nto\nits\napplications,\nsuch\nas"
        },
        {
          "Meng Wang, Fellow,": "adaptation,\ntransfer learning, emotion ambiguity.",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "human-computer interaction [1], healthcare aids [2], and driv-"
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "ing safety [3]. Currently,\nfacial expression recognition (FER)"
        },
        {
          "Meng Wang, Fellow,": "I.\nINTRODUCTION",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "can\nbe\nroughly\ndivided\ninto\ntwo\ntypes:\nStatic\nFacial Ex-"
        },
        {
          "Meng Wang, Fellow,": "† Equal contribution. ∗ Corresponding author.",
          "IEEE\nIEEE, and Richang Hong, Member,": "pression Recognition (SFER) and Dynamic Facial Expression"
        },
        {
          "Meng Wang, Fellow,": "This work was\nsupported\nin\npart\nby\nthe National Key Research\nand",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "Recognition (DFER). SFER mainly focuses on recognizing"
        },
        {
          "Meng Wang, Fellow,": "Development\nProgram of China\nunder Grant\n2019YFA0706203\nand\nalso",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "in part by the National Natural Science Foundation of China under Grant",
          "IEEE\nIEEE, and Richang Hong, Member,": "expressions\nfrom static images, whereas DFER concentrates"
        },
        {
          "Meng Wang, Fellow,": "62202139, and also in part by The University Synergy Innovation Program",
          "IEEE\nIEEE, and Richang Hong, Member,": "on recognizing expressions\nfrom dynamic\nimage\nsequences"
        },
        {
          "Meng Wang, Fellow,": "of Anhui Province (GXXT-2022-038).",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "(or videos)."
        },
        {
          "Meng Wang, Fellow,": "Yin\nChen,\nJia\nLi, Meng Wang\nand\nRichang\nHong\nare\nwith\nthe",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "School\nof Computer Science\nand\nInformation Engineering, Hefei Univer-",
          "IEEE\nIEEE, and Richang Hong, Member,": "With the advent of deep learning, FER has made consid-"
        },
        {
          "Meng Wang, Fellow,": "sity of Technology, Hefei 230601, China (e-mail: chenyin@mail.hfut.edu.cn;",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "erable progress on real-world SFER datasets\n(e.g., RAF-DB"
        },
        {
          "Meng Wang, Fellow,": "jiali@hfut.edu.cn; eric.mengwang@gmail.com; hongrc.hfut@gmail.com).",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "[15], AffectNet\n[16], FERPlus [17]), but\nthe performance on"
        },
        {
          "Meng Wang, Fellow,": "Shiguang Shan is with the Key Laboratory of\nIntelligent\nInformation Pro-",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "cessing,\nInstitute of Computing Technology, Chinese Academy of Sciences,",
          "IEEE\nIEEE, and Richang Hong, Member,": "in-the-wild DFER datasets (e.g., DFEW [4], FERV39K [18],"
        },
        {
          "Meng Wang, Fellow,": "Beijing 100190, China, and also with the University of Chinese Academy of",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "MAFW [19])\nis\nstill\nfar\nfrom satisfactory. This\nis primarily"
        },
        {
          "Meng Wang, Fellow,": "Sciences, Beijing, 100049, China (e-mail: sgshan@ict.ac.cn).",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "due to the difficulty in collecting DFER data,\nthe limitations"
        },
        {
          "Meng Wang, Fellow,": "© 2024 IEEE. Personal use of\nthis material\nis permitted. Permission from",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "IEEE must be obtained for\nall other uses,\nin any current or\nfuture media,",
          "IEEE\nIEEE, and Richang Hong, Member,": "of dataset representativeness (e.g., poses, occlusions, and illu-"
        },
        {
          "Meng Wang, Fellow,": "including reprinting/republishing this material\nfor advertising or promotional",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "minations),\nthe inherent ambiguity of\nfacial expressions, and"
        },
        {
          "Meng Wang, Fellow,": "purposes, creating new collective works, for resale or redistribution to servers",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "",
          "IEEE\nIEEE, and Richang Hong, Member,": "the insufficient mining of\ntemporal\ninformation."
        },
        {
          "Meng Wang, Fellow,": "or\nlists, or\nreuse of any copyrighted component of\nthis work in other works.",
          "IEEE\nIEEE, and Richang Hong, Member,": ""
        },
        {
          "Meng Wang, Fellow,": "Digital Object\nIdentifier: 10.1109/TAFFC.2024.3453443",
          "IEEE\nIEEE, and Richang Hong, Member,": "How to reduce the negative impact of data deficiencies"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "To mitigate the above main challenges, novel methods that"
        },
        {
          "2": "can harness prior knowledge, efficiently capture dynamic emo-"
        },
        {
          "2": "tional changes in videos, and provide more reliable supervision"
        },
        {
          "2": "signals\nshould be\nexplored to enhance DFER performance."
        },
        {
          "2": "CLIPER [12]\nand DFER-CLIP\n[13]\nhave\nleveraged\nprior"
        },
        {
          "2": "knowledge\nfrom the\nvery\npowerful\nvision-language model"
        },
        {
          "2": "CLIP [27]\nto\nenhance DFER,\nachieving\npromising\nresults."
        },
        {
          "2": "However,\nthese\nvision-language models\nstill\nunderperform"
        },
        {
          "2": "the\ncurrent\nstate-of-the-art\nvisual-only method MAE-DFER"
        },
        {
          "2": "[14], despite leveraging vision and language knowledge from"
        },
        {
          "2": "CLIP simultaneously. The I3D model [28] can transfer vision"
        },
        {
          "2": "knowledge\nlearned from static\nimages\nto videos,\nimproving"
        },
        {
          "2": "performance in downstream tasks. Nevertheless, models\nlike"
        },
        {
          "2": "I3D require retraining their all parameters to capture more fine-"
        },
        {
          "2": "grained spatiotemporal features, leading to high computational"
        },
        {
          "2": "costs. In contrast,\nthe adapter-based learning approaches [29]–"
        },
        {
          "2": "[32]\ncan\ntransfer\nan\nimage model\nto\nvideos\nefficiently\nby"
        },
        {
          "2": "fine-tuning\nonly\na\nsubset\nof\nparameters. However,\ndue\nto"
        },
        {
          "2": "the absence of prior\nfacial knowledge,\nthere exists a notable"
        },
        {
          "2": "domain gap between the general\nfeatures of\nthese upstream"
        },
        {
          "2": "models and the specific FER features, making direct applica-"
        },
        {
          "2": "tion to FER unfeasible. To bridge this gap,\nresearchers have"
        },
        {
          "2": "integrated Action Units (AU)\n[33]–[35] and facial\nlandmarks"
        },
        {
          "2": "[36]–[39] as prior facial knowledge into FER. Facial landmark"
        },
        {
          "2": "detection technology has attained a notable level of maturity,"
        },
        {
          "2": "particularly\nin\nchallenging\nreal-world\nscenarios, making\nit"
        },
        {
          "2": "more\nsuitable\nfor\nin-the-wild FER than AU. Moreover, we"
        },
        {
          "2": "have observed and validated that\nstatic\nfeatures\nlearned on"
        },
        {
          "2": "AffectNet dataset [16] exhibit strong robustness and are more"
        },
        {
          "2": "appropriate for\nrelated downstream tasks [40], which signifi-"
        },
        {
          "2": "cantly contributed to our victory in winning the championship"
        },
        {
          "2": "of Emotional Reaction Intensity Estimation Challenge held by"
        },
        {
          "2": "the CVPR2023-ABAW5 [41] competition. Additionally, exist-"
        },
        {
          "2": "ing facial\nlandmark-based methods\nstill\nlack simplicity. For"
        },
        {
          "2": "example, Poster\n[39] employs multi-level\nlandmark features,"
        },
        {
          "2": "making the method progressively complex. This motivates us"
        },
        {
          "2": "to seek a simple and effective structure."
        },
        {
          "2": "Building upon the aforementioned insights, we propose a"
        },
        {
          "2": "simple yet powerful model, named S2D, which is designed to"
        },
        {
          "2": "expand the Static FER model to the Dynamic FER task without"
        },
        {
          "2": "retraining all model parameters. Notably, S2D achieves a new"
        },
        {
          "2": "state-of-the-art performance with only a\nfraction of\ntunable"
        },
        {
          "2": "parameters\n(less\nthan 10% of\nthe whole model parameters),"
        },
        {
          "2": "as\nshown in Fig. 1. Specifically, S2D is\na\nstandard Vision"
        },
        {
          "2": "Transformer\n(ViT)\nthat\nincorporates Multi-View Complemen-"
        },
        {
          "2": "tary Prompter (MCP) and Temporal-Modeling Adapter (TMA)"
        },
        {
          "2": "modules. MCP is a simple module that combines static facial"
        },
        {
          "2": "expression features and landmark-aware features,\nthereby en-"
        },
        {
          "2": "hancing the image-level representational ability for both SFER"
        },
        {
          "2": "and DFER tasks. TMA consists of\na\ntemporal\nadapter with"
        },
        {
          "2": "Temporal Multi-Headed Self-Attention (T-MSA)\n[30]\nand a"
        },
        {
          "2": "Vanilla Adapter\n[42]. This configuration empowers TMA to"
        },
        {
          "2": "efficiently capture the dynamic changes of facial expressions in"
        },
        {
          "2": "the temporal dimension and transfer the learned representation"
        },
        {
          "2": "knowledge\nfrom SFER to DFER. Furthermore, we propose"
        },
        {
          "2": "an Emotion-Anchors\n(i.e., base reference expressions) based"
        },
        {
          "2": "Self-Distillation Loss\n(SDL)\nto provide\na\nreliable\nauxiliary"
        },
        {
          "2": "supervision\nsignal\nand\nhelp model\nlearning. We\nperformed"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and transferring facial\nexpression representations. The\ncode": "and model are publicly available here1.",
          "to deeper architectures. The second combines 2D CNN with": "RNN [4], [22], [23]. It first extracts features from each frame"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "Overall,\nthe main contributions of this work can be summa-",
          "to deeper architectures. The second combines 2D CNN with": "using 2D CNN and then models\ntemporal\ninformation using"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "rized as follows:",
          "to deeper architectures. The second combines 2D CNN with": "RNN.\nThe\nthird\nemerging\ntrend\nin\nresearch\ninvolves\nthe"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "•",
          "to deeper architectures. The second combines 2D CNN with": "utilization of Transformer methods. The\nresearch conducted"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "Image-level\nrepresentation\nenhancement. We\nintro-",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "by Former-DFER [8] utilizes convolution-based spatial Trans-"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "duce facial landmark-aware features extracted by Mobile-",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "former\nand temporal Transformer\nto achieve\nspatiotemporal"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "FaceNet\n[43]\nas\nanother view of\nthe\nraw input, which",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "fusion. STT [24] employs ResNet18 to extract\nfeatures and,"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "are sensitive to key facial\nregions,\nto guide the model\nto",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "in combination with Transformer, jointly learns spatiotemporal"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "focus more on the emotion-related facial areas. Then we",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "features.\nIAL [11]\nintroduces\na global\nconvolution-attention"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "utilize both the static features learned on AffectNet\n[16]",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "block\nand\nan\nintensity-aware\nloss\nto\ndifferentiate\nsamples"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "and facial\nlandmark-aware\nfeatures\nas prior knowledge",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "based\non\nvarying\nexpression\nintensities.\nIn\ncontrast\nto\nthe"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "for SFER and DFER tasks. By fusing these two features",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "aforementioned approaches, our method leverages\nthe prior"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "with the Multi-View Complementary Prompter\n(MCP),",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "knowledge of facial\nlandmark detections [43] and SFER data."
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "the image-level\nrepresentation is\nsignificantly enhanced.",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "It\nis\nalso\nhelpful\nfor\nthe DFER model\nto\neffectively",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "capture\nthe\ndynamic\nchanges\nof\nexpressions,\nfor\nthe",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "facial\ndynamic\ninformation\n(e.g., muscle movements)",
          "to deeper architectures. The second combines 2D CNN with": "B. Parameter-Efficient Transfer Learning"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "is\nimplicitly encoded in the\nsequential\nlandmark-aware",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "Parameter-efficient\ntransfer\nlearning techniques [42],\n[44]–"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "features.",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "[47] are first\nintroduced in natural\nlanguage processing (NLP)"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "• Expanding the static FER model to the dynamic FER",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "due\nto the\nrising computation costs of\nfully fine-tuning the"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "model\nefficiently. We\npropose\na\nTemporal-Modeling",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "expanding\nlanguage models\nfor\nvarious\ndownstream tasks."
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "Adapter\n(TMA) module to efficiently expand the static",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "The goal of\nthese\ntechniques\nis\nto decrease\nthe number of"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "FER model\nto the dynamic FER model by separate tem-",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "trainable parameters\nthus\nreducing the\ncomputational\ncosts,"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "poral modeling. With this ability, our FER model achieves",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "while\nachieving\na\ncomparable\nor\nbetter\nperformance\ncom-"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "SOTA performance on various DFER benchmarks, while",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "pared to full fine-tuning. Recently, parameter-efficient\ntransfer"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "maintaining tremendously parameter-efficient\n(only tun-",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "learning is also introduced in computer vision, and it can be"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "ing < 10% parameters of\nthe whole model). By offering",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "divided into prompt [48]–[51] learning and adapting [29]–[32]"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "a simple yet powerful baseline, our method provides a",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "learning techniques. The\nformer usually focuses on transfer"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "user-friendly solution for the research community, which",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "learning in the same domain (e.g.,\nimage-to-image or video-"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "is both easy to implement and follow.",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "to-video), whereas\nthe latter often targets adapting an image"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "• Emotion-Anchors based Self-Distillation Loss. Within",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "model\nfor video-based tasks. The work most\nrelated to ours"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "this loss, we utilize a group of reference samples to gener-",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "is AIM [30].\nIt utilizes\nlightweight\nadapters\nto modify the"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "ate more reliable soft\nlabels with probability distributions",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "vision\ntransformer\nblocks\nand\nadapt\na\nfrozen,\npre-trained"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "on all emotion classes. Such auxiliary supervision pro-",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "image model\nfor performing video action recognition. How-"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "vided by this loss prevents the ambiguous emotion labels",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "ever,\nthere are several major differences. Firstly, AIM adapts"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "from deteriorating the FER model, further improving our",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "to downstream tasks by modifying the original\ntransformer"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "DFER model.",
          "to deeper architectures. The second combines 2D CNN with": ""
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "layer directly, while our TMA is placed between the adjacent"
        },
        {
          "and transferring facial\nexpression representations. The\ncode": "",
          "to deeper architectures. The second combines 2D CNN with": "transformer blocks in the form of residual. Secondly, AIM uses"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3": "researchers started employing data-driven techniques to tackle"
        },
        {
          "3": "in-the-wild DFER challenges. At\npresent, DFER research"
        },
        {
          "3": "methods fall\ninto three main categories. The first category in-"
        },
        {
          "3": "volves using 3D CNN [21] to simultaneously model\ntemporal"
        },
        {
          "3": "and spatial\ninformation. However,\nthis method is\ncomputa-"
        },
        {
          "3": "tionally expensive, and the models may not be easily scalable"
        },
        {
          "3": "to deeper architectures. The second combines 2D CNN with"
        },
        {
          "3": "RNN [4], [22], [23]. It first extracts features from each frame"
        },
        {
          "3": "using 2D CNN and then models\ntemporal\ninformation using"
        },
        {
          "3": "RNN.\nThe\nthird\nemerging\ntrend\nin\nresearch\ninvolves\nthe"
        },
        {
          "3": "utilization of Transformer methods. The\nresearch conducted"
        },
        {
          "3": ""
        },
        {
          "3": "by Former-DFER [8] utilizes convolution-based spatial Trans-"
        },
        {
          "3": ""
        },
        {
          "3": "former\nand temporal Transformer\nto achieve\nspatiotemporal"
        },
        {
          "3": ""
        },
        {
          "3": "fusion. STT [24] employs ResNet18 to extract\nfeatures and,"
        },
        {
          "3": ""
        },
        {
          "3": "in combination with Transformer, jointly learns spatiotemporal"
        },
        {
          "3": ""
        },
        {
          "3": "features.\nIAL [11]\nintroduces\na global\nconvolution-attention"
        },
        {
          "3": ""
        },
        {
          "3": "block\nand\nan\nintensity-aware\nloss\nto\ndifferentiate\nsamples"
        },
        {
          "3": ""
        },
        {
          "3": "based\non\nvarying\nexpression\nintensities.\nIn\ncontrast\nto\nthe"
        },
        {
          "3": ""
        },
        {
          "3": "aforementioned approaches, our method leverages\nthe prior"
        },
        {
          "3": ""
        },
        {
          "3": "knowledge of facial\nlandmark detections [43] and SFER data."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "B. Parameter-Efficient Transfer Learning"
        },
        {
          "3": ""
        },
        {
          "3": "Parameter-efficient\ntransfer\nlearning techniques [42],\n[44]–"
        },
        {
          "3": ""
        },
        {
          "3": "[47] are first\nintroduced in natural\nlanguage processing (NLP)"
        },
        {
          "3": ""
        },
        {
          "3": "due\nto the\nrising computation costs of\nfully fine-tuning the"
        },
        {
          "3": ""
        },
        {
          "3": "expanding\nlanguage models\nfor\nvarious\ndownstream tasks."
        },
        {
          "3": ""
        },
        {
          "3": "The goal of\nthese\ntechniques\nis\nto decrease\nthe number of"
        },
        {
          "3": ""
        },
        {
          "3": "trainable parameters\nthus\nreducing the\ncomputational\ncosts,"
        },
        {
          "3": ""
        },
        {
          "3": "while\nachieving\na\ncomparable\nor\nbetter\nperformance\ncom-"
        },
        {
          "3": ""
        },
        {
          "3": "pared to full fine-tuning. Recently, parameter-efficient\ntransfer"
        },
        {
          "3": ""
        },
        {
          "3": "learning is also introduced in computer vision, and it can be"
        },
        {
          "3": ""
        },
        {
          "3": "divided into prompt [48]–[51] learning and adapting [29]–[32]"
        },
        {
          "3": ""
        },
        {
          "3": "learning techniques. The\nformer usually focuses on transfer"
        },
        {
          "3": ""
        },
        {
          "3": "learning in the same domain (e.g.,\nimage-to-image or video-"
        },
        {
          "3": ""
        },
        {
          "3": "to-video), whereas\nthe latter often targets adapting an image"
        },
        {
          "3": ""
        },
        {
          "3": "model\nfor video-based tasks. The work most\nrelated to ours"
        },
        {
          "3": ""
        },
        {
          "3": "is AIM [30].\nIt utilizes\nlightweight\nadapters\nto modify the"
        },
        {
          "3": ""
        },
        {
          "3": "vision\ntransformer\nblocks\nand\nadapt\na\nfrozen,\npre-trained"
        },
        {
          "3": ""
        },
        {
          "3": "image model\nfor performing video action recognition. How-"
        },
        {
          "3": ""
        },
        {
          "3": "ever,\nthere are several major differences. Firstly, AIM adapts"
        },
        {
          "3": ""
        },
        {
          "3": "to downstream tasks by modifying the original\ntransformer"
        },
        {
          "3": ""
        },
        {
          "3": "layer directly, while our TMA is placed between the adjacent"
        },
        {
          "3": "transformer blocks in the form of residual. Secondly, AIM uses"
        },
        {
          "3": ""
        },
        {
          "3": "an unlearnable self-attention for temporal modeling while our"
        },
        {
          "3": "TMA employs a learnable one."
        },
        {
          "3": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "//\nor": "Sg"
        },
        {
          "//\nor": "Landmark-Aware\nPosition"
        },
        {
          "//\nor": "Embedding\nFeature (s)"
        },
        {
          "//\nor": "Transformer Layers from ViT\nTuned Parameters\nComputation Flow For SFER Task\nDynamic Input"
        },
        {
          "//\nor": "Frozen Parameters\nFacial Landmark Detector\nComputation Flow For DFER Task\nStatic Input"
        },
        {
          "//\nor": "Overall architecture of\nFig. 2.\nthe proposed method. Our S2D accepts as\ninput a facial expression image (or\nfacial expression image sequence) XF"
        },
        {
          "//\nor": "and a\nlandmark-aware\nfeature\n(or\nlandmark-aware\nfeature\nfacial\nexpression image\nand landmark-aware\nfeature\nare\nembedded with\nsequence) XL. The"
        },
        {
          "//\nor": "patch embedding layers and fed into the transformer\nlayers {El}L−1\nborrowed from ViT. Temporal-Modeling Adapter\n(TMA)\nis used to capture temporal"
        },
        {
          "//\nor": "l=0"
        },
        {
          "//\nor": "information T l while Multi-View Complementary Prompter (MCP) uses landmark-aware features to generate guiding prompts P l\nto enhance the image-level"
        },
        {
          "//\nor": "representational ability for both SFER and DFER tasks. Note that\nthe position embedding is added to P 0 after\nthe first MCP block, and TMA is only used"
        },
        {
          "//\nor": "for\nthe DFER task. Sg means stop gradient."
        },
        {
          "//\nor": "yields\npromising\nperformance\non DFER benchmarks while\nto generate guiding prompts, while H0 is sent\nto the TMA to"
        },
        {
          "//\nor": "simultaneously ensuring parameter efficiency.\ncapture the temporal\ninformation. Finally,\nthe learned guiding"
        },
        {
          "//\nor": "prompts P ∈ RT ×N ×D and\ntemporal\ninformation\ntokens\nAn overall of our model\nis presented in Fig. 2,\nshowing"
        },
        {
          "//\nor": "T\n∈ RT ×N ×D are\nadded to the original\nfacial\nexpression\nour model architecture, computation flow (detailed in section"
        },
        {
          "//\nor": "tokens in a form of\nresidual:\nIII-A), multi-view input\nand interaction (detailed in section"
        },
        {
          "//\nor": "III-B),\nand training loss\n(detailed in section III-D). Section"
        },
        {
          "//\nor": "Hl′\n= Hl + P l+1 + T l+1,\nl = 0, 1, · · ·\n, L − 1,\n(1)"
        },
        {
          "//\nor": "III-C introduces how to expand the image model to an efficient"
        },
        {
          "//\nor": "where Hl′\nis\nthe\nprompted\ntokens,\nand P l+1, T l+1\nare\nvideo model\nin detail."
        },
        {
          "//\nor": "guiding prompts and temporal\ninformation from the (l + 1)-"
        },
        {
          "//\nor": "th MCP module and TMA module,\nrespectively. Next, Hl′\nis"
        },
        {
          "//\nor": "A. Preliminary"
        },
        {
          "//\nor": "fed into the transformer\nlayer El\nto extract a more powerful"
        },
        {
          "//\nor": "1) Problem Formulation: Given a video clip or\na\nfacial"
        },
        {
          "//\nor": "image-level\nrepresentation:"
        },
        {
          "//\nor": "image\nthe\ntask of\nsequence XF with an emotion label Y ,"
        },
        {
          "//\nor": ").\n(2)\nHl+1 = El(xclass, Hl′"
        },
        {
          "//\nor": "DFER is to learn a mapping function Fθ(XF ) → Y , where"
        },
        {
          "//\nor": "Fθ denotes a model and θ represents its learnable parameters."
        },
        {
          "//\nor": "B.\nImage-Level Representation Enhancement"
        },
        {
          "//\nor": "In this work, we employ the facial\nlandmark-aware features"
        },
        {
          "//\nor": "landmark\nXL extracted from XF using an off-the-shelf facial\nIn this\nsection, we will\nintroduce how to enhance image-"
        },
        {
          "//\nor": "detector as an auxiliary view. Thus, the input of Fθ is extended\nlevel representational ability through the selection of static fa-"
        },
        {
          "//\nor": "the mapping goal has changed to\nto (XF , XL). Accordingly,\ncial expression features,\nthe incorporation of facial\nlandmark-"
        },
        {
          "//\nor": "Fθ(XF , XL) → Y .\naware features, and the generation of guiding prompts."
        },
        {
          "//\nor": "2)\nS2D Model:\nAs\nillustrated\nin\nFig.\n2,\nour\ndevised\n1)\nSelection of Static Facial Expression Feature: To harness"
        },
        {
          "//\nor": "∈\nS2D accepts\nas\ninput\nthe\nfacial\nimage\nthe image-level\nrepresentational capacity of\nthe SFER model\nsequence XF"
        },
        {
          "//\nor": "RT ×C×H×W"
        },
        {
          "//\nor": "and\nfacial\nlandmark-aware\nfeature\nsequence\nand\ngain more\nrobust\nstatic\nfacial\nexpression\nfeatures, we"
        },
        {
          "//\nor": ". Here, T , C, H, and W represent\nthe\nemploy AffectNet\n[16] as our pre-training dataset. AffectNet\nXL ∈ RT ×C′×H ′×W ′"
        },
        {
          "//\nor": "number of frames, channels, width, and height of the facial im-\nis the existing largest SFER dataset, which contains more than"
        },
        {
          "//\nor": "1M face images from the Internet and about 450,000 manually\nage sequence, respectively. Firstly, we feed the two flows, XF"
        },
        {
          "//\nor": "into individual patch embedding layers\nseparately.\nannotated images. We first\ntrain the model on the AffectNet\nand XL,"
        },
        {
          "//\nor": "Each input frame is divided into N patches,\nthen mapped and\ndataset\nto get a robust expression appearance representation,"
        },
        {
          "//\nor": "flattened into a D-dimensional\nlatent space. We refer to these\nthen fine-tune it on other FER datasets."
        },
        {
          "//\nor": "embeddings as\nthe facial expression tokens H0 ∈ RT ×N ×D\n2) Facial Landmark-Aware Feature: To bolster image-level"
        },
        {
          "//\nor": "and facial\nlandmark tokens A0 ∈ RT ×N ×D. Then, H0 and a\nrepresentation for both SFER and DFER, we introduce facial"
        },
        {
          "//\nor": "[class]\nlandmark-aware\nfeatures\nextracted from facial\nimages\nas\nan\ntoken xclass with position embeddings are fed into the"
        },
        {
          "//\nor": "transformer layers {El}L−1\nof Vision Transformer (ViT) [52].\nauxiliary view to guide model learning. These facial landmark-"
        },
        {
          "//\nor": "l=0"
        },
        {
          "//\nor": "Note that H0\nand A0\nare firstly sent\ninto the MCP module\naware\nfeatures\nare\nthe\nfeatures\nin the penultimate\nstage of"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5": ""
        },
        {
          "5": "𝑇×𝑁×𝐷\n𝑁×𝑇×𝐷"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "FC\nGELU\nT-MSA\nFC\nFC\nGELU\nFC"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "Hl\nT l+1"
        },
        {
          "5": "Adapter\nTemporal\nLayer Norm\nAdapter\nVanilla"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "TMA"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "Fig. 3. Temporal-Modeling Adapter (TMA) for temporal adaptation. The"
        },
        {
          "5": "input Hl ∈ RT ×N ×D is fed into a Temporal Adapter\nto capture temporal"
        },
        {
          "5": "information,\nthen it\nis fed into a LayerNorm and a Vanilla Adapter to reduce"
        },
        {
          "5": ""
        },
        {
          "5": "the domain gap between SFER and DFER. T l+1 ∈ RT ×N ×D is\nlearned"
        },
        {
          "5": ""
        },
        {
          "5": "temporal\ninformation."
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "Hl′"
        },
        {
          "5": "T ∈ RN ×T ×γD by a simple Linear layer and passed through"
        },
        {
          "5": "a GELU activation layer, where γ is downsampling rate."
        },
        {
          "5": ""
        },
        {
          "5": "Hl′\n(7)\nT = GELU (Hl\nT W 1\nd + b1\nd),"
        },
        {
          "5": ""
        },
        {
          "5": "where W 1\nd ∈ RD×γD, b1\nd ∈ R1×γD are learnable parameters"
        },
        {
          "5": ""
        },
        {
          "5": "is\nfed into the\nof\nthe downsampling Linear\nlayer. Next, Hl′\nT"
        },
        {
          "5": "T-MSA to capture the relationships between frames as follows:"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "Hl′′\n(8)\nT = T -M SA(Hl′\nT ),"
        },
        {
          "5": ""
        },
        {
          "5": "whose processes are described by equations 9, 10, and 11."
        },
        {
          "5": ""
        },
        {
          "5": "(cid:18) QK⊤"
        },
        {
          "5": "√\nV ,\nSA(Q, K, V ) = softmax"
        },
        {
          "5": "(9)"
        },
        {
          "5": "DK"
        },
        {
          "5": ""
        },
        {
          "5": "(cid:16)\n(cid:17)"
        },
        {
          "5": ""
        },
        {
          "5": "QW Q\n, KW K\n, V W V\n,\n(10)\nheadi = SA"
        },
        {
          "5": "i\ni\ni"
        },
        {
          "5": "Hl′′"
        },
        {
          "5": "(11)\nT = Cat (head1, . . . , headh) W o,"
        },
        {
          "5": ""
        },
        {
          "5": "where Q, K, V are the duplicates of Hl′"
        },
        {
          "5": "T and the projections"
        },
        {
          "5": ", W K\n, W V\nare parameter matrices: {W Q\n} ∈ RD×DK ,\nand"
        },
        {
          "5": "i\ni\ni"
        },
        {
          "5": "W o ∈ RhDV ×D, DK = DV = D/h, SA represents the Self-"
        },
        {
          "5": "Attention mechanism, headi corresponds to the i-th attention"
        },
        {
          "5": ""
        },
        {
          "5": "head, and Cat denotes\nthe concatenation operation. Finally,"
        },
        {
          "5": "the Hl′′\nis upsampled to the original dimension as below:"
        },
        {
          "5": "T"
        },
        {
          "5": "= Hl′′\nHl′′′\n(12)\nT\nT W 1\nu + b1\nu,"
        },
        {
          "5": ""
        },
        {
          "5": "where W 1"
        },
        {
          "5": "d ∈ R1×D are learnable parameters"
        },
        {
          "5": "of upsampling linear\nlayer\nin T-Adapter."
        },
        {
          "5": "with the shape of\nAfter that, Hl′′′\nis reshaped back to Hl′′"
        },
        {
          "5": "T"
        },
        {
          "5": "T × N × D and fed into LayerNorm Layer. To enhance the"
        },
        {
          "5": "TMA’s capacity to capture temporal\ninformation and reduce"
        },
        {
          "5": "the domain gap between SFER and DFER, we equip it with"
        },
        {
          "5": "a Vanilla Adapter\n[42]. The Vanilla Adapter has\ntwo simple"
        },
        {
          "5": "Linear\nlayers with a GELU activation function. The process"
        },
        {
          "5": "can be described by the following equations:"
        },
        {
          "5": ""
        },
        {
          "5": "Hl′′\n= LayerN orm(Hl′′\n),\n(13)"
        },
        {
          "5": ""
        },
        {
          "5": "′"
        },
        {
          "5": "T\n= GELU (Hl′′\nW 2\n(14)\nd + b2\nd),"
        },
        {
          "5": ""
        },
        {
          "5": "′"
        },
        {
          "5": "T l+1 = T\nW 2\n(15)\nu + b2\nu,"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "Dataset"
        },
        {
          "6": "∑ -.\n𝑌\n$%&\n()*+="
        },
        {
          "6": "’\n∑ -\t\nℒ!\"#"
        },
        {
          "6": ""
        },
        {
          "6": "𝒑𝒓𝒆\nTop-K\n𝑵𝒃𝒔\nUpdate\n𝒀𝒊"
        },
        {
          "6": ""
        },
        {
          "6": "0.2\n0.7"
        },
        {
          "6": "0.1\n0.4"
        },
        {
          "6": "Model"
        },
        {
          "6": "✓"
        },
        {
          "6": "✓"
        },
        {
          "6": "…\n0.5\nCompare"
        },
        {
          "6": "0.2\n…\n…"
        },
        {
          "6": "Angry"
        },
        {
          "6": "✓\nHappy"
        },
        {
          "6": "✓"
        },
        {
          "6": "(𝑋’, 𝑌’)\nEmotion Anchors"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "Fig. 4.\nEmotion-Anchors based Self-Distillation Loss. p is output proba-"
        },
        {
          "6": ""
        },
        {
          "6": "bilities of S2D about emotion anchors. σ is the similarity score between input"
        },
        {
          "6": "and emotion anchors.\nis a sample from current batch and Y pre\nis\n(Xi, Yi)"
        },
        {
          "6": "i"
        },
        {
          "6": "the corresponding predicted probability. Y sof t\nis produced soft\nlabel for Xi."
        },
        {
          "6": "i"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "Afterwords, we obtain the\nsoft\nlabel Y sof t\nfor\nthe\ncurrent"
        },
        {
          "6": "i"
        },
        {
          "6": "input sample Xi:"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "1"
        },
        {
          "6": ""
        },
        {
          "6": "C(cid:88) c\nK(cid:88) k\nY sof t\n=\nσc\n(18)\nkpc\nk."
        },
        {
          "6": "i"
        },
        {
          "6": "(cid:80)C\n(cid:80)K"
        },
        {
          "6": "=1\nc=1\nk=1 σc\n=1"
        },
        {
          "6": ""
        },
        {
          "6": "Finally, we supervise the model using the labeled Yi and the"
        },
        {
          "6": ""
        },
        {
          "6": "pseudo label Y sof t\n, described as follows:\ni"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "1 N\nN(cid:88) i\nLtarget\n=\n(19)\nCE(pi, Yi),"
        },
        {
          "6": "ce"
        },
        {
          "6": ""
        },
        {
          "6": "=1"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "1 N\nN(cid:88) i\n),\nLaux\n(20)\nBCE(pi, Y sof t\nbce ="
        },
        {
          "6": "=1"
        },
        {
          "6": ""
        },
        {
          "6": "where CE and BCE represent the cross-entropy loss and binary"
        },
        {
          "6": ""
        },
        {
          "6": "cross-entropy loss, respectively. The total supervision loss is a"
        },
        {
          "6": ""
        },
        {
          "6": "combination of these two terms weighted by a hyperparameter"
        },
        {
          "6": ""
        },
        {
          "6": "η:"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "L = Ltarget\n+ ηLaux\n(21)\nce\nbce ."
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "IV. EXPERIMENTS"
        },
        {
          "6": ""
        },
        {
          "6": "A. Datasets"
        },
        {
          "6": ""
        },
        {
          "6": "Our\nproposed model\nis\nevaluated\non\na\ndiverse\nrange\nof"
        },
        {
          "6": ""
        },
        {
          "6": "datasets:\nSFER datasets\ninclude RAF-DB [15], AffectNet-"
        },
        {
          "6": ""
        },
        {
          "6": "7/8 [16], and FERPlus\n[17], and DFER datasets encompass"
        },
        {
          "6": ""
        },
        {
          "6": "DFEW [4], FERV39K [18], and MAFW [19]. RAF-DB con-"
        },
        {
          "6": ""
        },
        {
          "6": "tains\n29,672\nreal-world\nfacial\nimages\nannotated with\nbasic"
        },
        {
          "6": ""
        },
        {
          "6": "expressions by 40 trained annotators, using 12,271 for training"
        },
        {
          "6": "and 3,068 for testing. AffectNet-7/8 comprises over 1 million"
        },
        {
          "6": ""
        },
        {
          "6": "images, manually annotating around 450,000. AffectNet-7 has"
        },
        {
          "6": ""
        },
        {
          "6": "7 basic expressions with 283,901 training and 3,500 testing"
        },
        {
          "6": ""
        },
        {
          "6": "contempt\nimages,\nand AffectNet-8\nadds\nthe\ncategory with"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "287,568 training and 4,000 testing images. FERPlus extends"
        },
        {
          "6": ""
        },
        {
          "6": "FER2013 with 28,709 training, 3,589 validation,\nand 3,589"
        },
        {
          "6": "testing images. DFEW involves 16,000 video clips categorized"
        },
        {
          "6": ""
        },
        {
          "6": "into\n7\nexpressions,\nusing\n12,059\nclips\nfor\nexperimentation"
        },
        {
          "6": ""
        },
        {
          "6": "through 5-fold cross-validation. FERV39K has 38,935 video"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "sequences\nfrom 4\nscenarios,\n31,088\nclips\nfor\ntraining,\nand"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": "TABLE I"
        },
        {
          "7": "COMPARISON OF CROSS-DATASET EVALUATION. AFFECTNET-SMALL IS"
        },
        {
          "7": ""
        },
        {
          "7": "SAMPLED FROM AFFECTNET-7 AND HAS THE SAME DATA SIZE AS"
        },
        {
          "7": ""
        },
        {
          "7": "RAF-DB. UNDERLINED RESULTS ARE PRODUCED BY THE MODEL"
        },
        {
          "7": "INITIALIZED WITH MAE-IMAGENET1K [57], WHILE BOLDED RESULTS"
        },
        {
          "7": "ARE FROM THE MODEL INITIALIZED WITH MAE-AFFECTNET. ALL"
        },
        {
          "7": "RESULTS ARE PRODUCED BY SFER MODELS (THE S2D WITHOUT TMA"
        },
        {
          "7": "AND SDL)."
        },
        {
          "7": ""
        },
        {
          "7": "Training\nPre-training\nTesting dataset"
        },
        {
          "7": "images\ndata\nRAF-DB\nAfectNet-7\nFERPlus"
        },
        {
          "7": "92.21\n47.57\n77.35\n12,271\nRAF-DB\n85.07\n46.83\n62.82"
        },
        {
          "7": "76.16\n66.42\n64.32\n283,901\nAffectNet-7\n70.73\n63.67\n55.89"
        },
        {
          "7": "69.13\n61.40\n62.08\n12,271\nAffectNet-7-Small\n60.40\n53.44\n48.21"
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "results\n(highlighted\nin\nbold), which\nrevealed\na\nsignificant"
        },
        {
          "7": ""
        },
        {
          "7": "performance decline of the SFER model pre-trained on RAF-"
        },
        {
          "7": ""
        },
        {
          "7": "DB when tested on the AffectNet-7 dataset, culminating in a"
        },
        {
          "7": ""
        },
        {
          "7": "44.64% drop in accuracy. Conversely,\nthe model pre-trained"
        },
        {
          "7": ""
        },
        {
          "7": "on AffectNet-7, when\nevaluated\non\nthe RAF-DB dataset,"
        },
        {
          "7": ""
        },
        {
          "7": "demonstrated an accuracy improvement of 9.74%.\nIt\nis well"
        },
        {
          "7": ""
        },
        {
          "7": "known\nthat\nthe AffectNet\ndataset\nhas\na\nhigher\nnoise\nlevel"
        },
        {
          "7": ""
        },
        {
          "7": "than the RAF-DB dataset. To explore whether\nthis difference"
        },
        {
          "7": ""
        },
        {
          "7": "arises from data size or sample diversity, a further experiment"
        },
        {
          "7": ""
        },
        {
          "7": "about AffectNet was undertaken using an equivalent amount"
        },
        {
          "7": ""
        },
        {
          "7": "of data\nas RAF-DB. The\nsubset\nsampled from AffectNet-7"
        },
        {
          "7": ""
        },
        {
          "7": "is named AffectNet-Small. Despite AffectNet-Small having a"
        },
        {
          "7": ""
        },
        {
          "7": "much smaller data size than AffectNet-7, it only experienced a"
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "5.02% accuracy drop on AffectNet-7 and still performs well on"
        },
        {
          "7": ""
        },
        {
          "7": "RAF-DB. For objectivity and fairness, a third-party evaluation"
        },
        {
          "7": ""
        },
        {
          "7": "was also introduced using the FERPlus testing set, consisting"
        },
        {
          "7": ""
        },
        {
          "7": "of 3,589 gray-scale\nimages,\neach sized to 48x48 pixels. As"
        },
        {
          "7": ""
        },
        {
          "7": "shown in Table I,\ntesting on FERPlus\nreveals a pronounced"
        },
        {
          "7": ""
        },
        {
          "7": "14.86% accuracy\ndrop\nfor\nthe SFER model\npre-trained\non"
        },
        {
          "7": ""
        },
        {
          "7": "RAF-DB, but only a\nslight drop of 2.1% accuracy for\nthe"
        },
        {
          "7": ""
        },
        {
          "7": "model\npre-trained\non AffectNet-7.\nInterestingly,\nthe model"
        },
        {
          "7": ""
        },
        {
          "7": "trained on RAF-DB outperforms AffectNet-7 when tested on"
        },
        {
          "7": ""
        },
        {
          "7": "the FERPlus dataset. We hypothesize\nthat\nthis performance"
        },
        {
          "7": ""
        },
        {
          "7": "disparity\nis\nattributable\nto\nthe\ngreater\nconsistency\nin\nanno-"
        },
        {
          "7": ""
        },
        {
          "7": "tation methods\nbetween RAF-DB and\nFERPlus,\ncompared"
        },
        {
          "7": ""
        },
        {
          "7": "to AffectNet. To\neliminate\nthe\ninfluence\nof\ninitial weights,"
        },
        {
          "7": ""
        },
        {
          "7": "we conducted additional experiments using MAE-ImageNet1K"
        },
        {
          "7": ""
        },
        {
          "7": "[57] weights\nfor\ninitialization. The\nexperimental\nresults,\nas"
        },
        {
          "7": ""
        },
        {
          "7": "highlighted in Table I, are consistent with our previous obser-"
        },
        {
          "7": "vations. Specifically,\nthe model\ntrained on AffectNet-7 shows"
        },
        {
          "7": "a\nlesser\nperformance\ndrop when\ntested\non\nother\ndatasets"
        },
        {
          "7": "compared to the model\ntrained on RAF-DB,\nindicating that"
        },
        {
          "7": ""
        },
        {
          "7": "the model\ntrained on AffectNet-7 seems\nto be more\nrobust"
        },
        {
          "7": ""
        },
        {
          "7": "than RAF-DB."
        },
        {
          "7": ""
        },
        {
          "7": "Subsequent\nexperiments were\nalso\nconducted\non DFER"
        },
        {
          "7": ""
        },
        {
          "7": "datasets by initializing the DFER model using various SFER"
        },
        {
          "7": ""
        },
        {
          "7": "models. These SFER models were initialized with the weights"
        },
        {
          "7": ""
        },
        {
          "7": "pre-trained on ImageNet-1K [58] or AffectNet with MAE [57]"
        },
        {
          "7": ""
        },
        {
          "7": "pre-training method\nand fine-tuned\non RAF-DB, FERPlus,"
        },
        {
          "7": ""
        },
        {
          "7": "and AffectNet-7,\nrespectively. This was done to evaluate the"
        },
        {
          "7": ""
        },
        {
          "7": "generalization ability of the static appearance features obtained"
        },
        {
          "7": ""
        },
        {
          "7": "through\npre-training\non\nvarious\nSFER datasets. As\nshown"
        },
        {
          "7": ""
        },
        {
          "7": "in Table\nII,\nthe DFER model pre-trained on the AffectNet-"
        },
        {
          "7": "7 dataset exhibits\nsuperior performance on both DFEW and"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "BETTER GENERALIZATION PERFORMANCE.",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "WITHOUT FACIAL LANDMARK-AWARE FEATURES. MCP: FUSE STATIC"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "FEATURES AND LANDMARK-AWARE FEATURES WITH THE MULTI-VIEW"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "COMPLEMENTARY PROMPTER. CAP: FUSE STATIC FEATURES AND FACIAL"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "Pre-training\nDFEW\nFERV39K",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": ""
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "Init weights",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "LANDMARK-AWARE FEATURES WITH THE COMMONLY USED"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "data",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": ""
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "WAR\nUAR\nUAR\nWAR",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "CONCATENATION+PROJECTION MODULE."
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "RAF-DB\n55.41\n69.50\n34.25\n46.77",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": ""
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "Fusion Method\nRAF-DB\nAffectNet-7\nFERPlus"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "MAE-ImageNet1K [57]\nFERPlus\n56.10\n70.10\n33.88\n46.71",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": ""
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "AffectNet-7\n56.69\n70.91\n34.83\n47.36",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "None\n91.07\n66.09\n90.18"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "SFER"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "MCP\n92.21\n66.42\n91.01"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "RAF-DB\n61.06\n75.22\n38.97\n51.50",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "CAP\n88.88\n64.22\n86.15"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "MAE-AffectNet\n[57]\nFERPlus\n61.62\n76.08\n39.15\n51.56",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": ""
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "DFEW\nFERV39K"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "AffectNet-7\n61.56\n76.16\n41.28\n52.56",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": ""
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "Fusion Method"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "UAR\nWAR\nUAR\nWAR"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "DFER\nNone\n61.06\n75.48\n38.74\n51.44"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "TABLE III",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "MCP\n61.56\n76.16\n41.28\n52.56"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "ABLATION ON DIFFERENT FACIAL LANDMARK DETECTION MODELS WITH",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": "CAP\n59.94\n73.56\n38.31\n50.76"
        },
        {
          "USING THE SFER MODEL PRE-TRAINED ON AFFECTNET-7 DATASET FOR": "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR",
          "DFER DATASETS ARE PRODUCED BY OUR S2D WITH SDL. NONE:": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": "NORMALIZATION) IS AN EVALUATION METRIC FOR LANDMARK"
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": "DETECTION. THE RESULTS ARE PRODUCED BY S2D WITH SDL."
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": ""
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": ""
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": "Params"
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": "ION ↓"
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": "(M)"
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": "UAR"
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": "5.29\n3.74\n60.74"
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": ""
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": "4.39\n3.74\n61.45"
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": ""
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": "3.97\n0.73\n61.29"
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": "3.96\n3.74\n62.05"
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": ""
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": "3.76\n1.01\n61.56"
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": ""
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": ""
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": ""
        },
        {
          "VARYING LEVELS OF LOCALIZATION PRECISION. ION (INTER-OCULAR": "particularly when"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fear\nFear": "Baseline\nS2D (Ours)"
        },
        {
          "Fear\nFear": ""
        },
        {
          "Fear\nFear": ""
        },
        {
          "Fear\nFear": "The\ncomparison of our proposed model with baseline at\nclass\nFig. 5."
        },
        {
          "Fear\nFear": "level. We visualize the overall accuracy (WAR) and class accuracy of each"
        },
        {
          "Fear\nFear": "emotion on DFEW (fd1) and FERV39K datasets. The baseline method is our"
        },
        {
          "Fear\nFear": "model without TMA, MCP, and SDL.\n(i.e.,\nline 1 in Table VII)."
        },
        {
          "Fear\nFear": ""
        },
        {
          "Fear\nFear": ""
        },
        {
          "Fear\nFear": "TABLE VIII"
        },
        {
          "Fear\nFear": ""
        },
        {
          "Fear\nFear": "COMPARE WITH STATE-OF-THE-ART SFER METHODS ON RAF-DB,"
        },
        {
          "Fear\nFear": ""
        },
        {
          "Fear\nFear": "AFFECTNET-7, AFFECTNET-8 AND FERPLUS TESTING SETS. WE"
        },
        {
          "Fear\nFear": ""
        },
        {
          "Fear\nFear": "HIGHLIGHT THE BEST RESULT IN BOLD AND UNDERLINE THE"
        },
        {
          "Fear\nFear": ""
        },
        {
          "Fear\nFear": "SECOND-BEST RESULT. ‡ MEANS S2D PRE-TRAINED ON THE"
        },
        {
          "Fear\nFear": "AFFECTNET-8 DATASET. THE RESULTS ARE PRODUCED BY THE SFER"
        },
        {
          "Fear\nFear": "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD"
        },
        {
          "Fear\nFear": "ACHIEVED COMPARABLE OR EVEN BETTER PERFORMANCE THAN"
        },
        {
          "Fear\nFear": "PREVIOUS STATE-OF-THE-ART SFER METHODS."
        },
        {
          "Fear\nFear": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "ACHIEVED COMPARABLE OR EVEN BETTER PERFORMANCE THAN"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "PREVIOUS STATE-OF-THE-ART SFER METHODS."
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": ""
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": ""
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "RAF-DB"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": ""
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "88.14"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "88.99"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "88.42"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "90.51"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "88.62"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "88.14"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "90.91"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "91.61"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "91.98"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "90.87"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "92.07"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": ""
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "92.21"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": "92.57"
        },
        {
          "MODEL (THE S2D MODEL WITHOUT TMA AND SDL). OUR METHOD": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": ""
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": ""
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "Method"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": ""
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": ""
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "Supervised learning models"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "C3D [5]"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "R(2+1)D-18 [6]"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "3D ResNet-18 [7]"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "ResNet-18+LSTM [8]"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "Former-DFER [8]\n[MM’21]"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "CEFLNet\n[9]\n[IS’2022]"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "T-ESFL [19]\n[MM’22]"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "EST [10]\n[PR’23]"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "STT [24]\n[arXiv’22]"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "NR-DFERNet\n[70]\n[arXiv’22]"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "IAL [11]\n[AAAI’23]"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "M3DFEL [53]\n[CVPR’23]"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "S2D (ours)"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "S2D (ours) *"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "S2D (ours) *†"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "Self-supervised learning models"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "MAE-DFER [14] * [MM’23]"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "Vision-language models"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "CLIPER [12]\n[arXiv’23]"
        },
        {
          "AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH": "DFER-CLIP [13]\n[BMVC’23]"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "[15], AffectNet\n[16], and FERPlus\n[17]) and DFER datasets",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "MAFW respectively, with fewer tunable parameters and train-"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "(DFEW [4], FERV39K [18], and MAFW [19]). In this section,",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "ing costs\n(refer\nto Fig. 1). Furthermore, our method,\ntrained"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "we report\nthe average results across all 5 folds for DFEW and",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "with an oversampling strategy, also outperformed MAE-DFER"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "MAFW datasets.",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "in UAR, achieving improvements of 2.04% on DFEW, 0.85%"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "1) Results on SFER Datasets: Table VIII provides a com-",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "on FERV39K, and 1.54% on MAFW."
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "parison with several SOTA methods on popular SFER datasets.",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "We\nalso\ncompare S2D with\ntwo\nvision-language models"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "Our method achieved the best results on RAF-DB, AffectNet-",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "CLIPER [12] and DFER-CLIP [13], which rely on both prior"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "7, and FERPlus. Particularly noteworthy is our achievement",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "language knowledge and general vision knowledge from CLIP"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "of 92.57% accuracy on RAF-DB, surpassing existing methods",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "[27]. The results in Table IX indicate that our method demon-"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "by 0.50%. Despite\nthe\nlimited quantity of data\nin FERPlus",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "strated significant\nimprovements over\nthese vision-language"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "and the small gray image size of 48x48 (meaning an obvious",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "models (+5.84% UAR and +4.78% WAR on DFEW, +2.70%"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "domain gap), our method still achieved the highest accuracy.",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "UAR and +0.91% WAR on FERV39K, +4.51% UAR and"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "2) Results on DFER Datasets:\nFirstly, we\ncompare S2D",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "+4.82% WAR on MAFW)."
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "with previous\nstate-of-the-art\nsupervised learning models on",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "Additionally, we present\nthe detailed performance of each"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "DFEW, FERV39K,\nand MAFW. Table\nIX shows\nthat S2D",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "emotion on DFEW in Table X. The\nresults\nshow that S2D"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "significantly outperformed the previous best methods (i.e., IAL",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "outperforms\nthe\nprevious\nbest methods\nacross most\nfacial"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "[11], M3DFEL [53])\nand\nachieved\na\nnotable\nimprovement",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "expressions,\nregardless of whether\nthey are supervised (e.g.,"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "of\n6.47% UAR with\n6.73% WAR and\n5.05% UAR with",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "IAL [11], M3DFEL [53]) or\nself-supervised methods\n(e.g.,"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "4.16% WAR on DFEW and FERV39K datasets,\nrespectively.",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "MAE-DFER [14]). These improvements are particularly evi-"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "Regarding MAFW, S2D achieves a new state-of-the-art per-",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "dent\nin happy, sad, neutral, angry, and surprise. For instance,"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "formance,\nsurpassing\nthe\nprevious\nbest methods\n[19]\nby\na",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "S2D surpasses M3DFEL by 14.87%, 7.43%, 9.95% on sad,"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "significant margin\nof\n6.59% in UAR and\n9.19% in WAR.",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "neutral, and angry,\nrespectively.\nIn contrast\nto MAE-DFER,"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "These significant\nimprovements indicate that our method can",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "which achieved an unbiased representation through massive-"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "learn powerful\nrepresentations for DFER by pre-training on a",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "scale self-supervised learning and performed well on disgust,"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "large-scale SFER dataset.",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "supervised models exhibit bias in modeling disgust due to the"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "Secondly, we compare S2D with the self-supervised method",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "lack of\nrelated training samples (occupying only 1.2% of\nthe"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "MAE-DFER [14], which was\npre-trained\non\na\nlarge-scale",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "entire dataset). To mitigate\nthis, we\ntrained the S2D model"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "video\ndataset\n(over\n1 million\nvideo\nclips) with\na\nself-",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "with an oversampling strategy additionally, which greatly im-"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "supervised approach [71]. To ensure fairness, we adopted the",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "proves the performance on the minor classes, disgust and fear"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "same\ntesting\nstrategy\nas MAE-DFER. Experimental\nresults",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "(25.52%−1.38%=24.14%,\n50.22%−34.71%=15.51%), while"
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "demonstrate\nthat our method outperformed MAE-DFER by",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "keeping the highest UAR and WAR metrics."
        },
        {
          "DFER-CLIP [13]\n[BMVC’23]\n90": "1.60%, 0.49%, and 3.06% in WAR on DFEW, FERV39K, and",
          "59.61\n71.25\n41.27\n51.65\n38.89\n52.55": "In\nsummary,\nthe\npromising\nresults\non\nthree\nin-the-wild"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": "FINAL PREDICTION DURING TESTING TIME. †: TRAINING WITH OVERSAMPLING STRATEGY. UAR: UNWEIGHTED AVERAGE RECALL; WAR: WEIGHTED"
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": ""
        },
        {
          "SAMPLING TWO CLIPS UNIFORMLY ALONG THE TEMPORAL AXIS FOR EACH VIDEO SAMPLE AND THEN CALCULATING THE AVERAGE SCORE AS THE": "4"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "w/": "landmarks"
        },
        {
          "w/": "Fig. 6. Visualization of input facial"
        },
        {
          "w/": "and with facial\nlandmark-aware features (row 2 and row 3 ). The darker\nthe color,"
        },
        {
          "w/": "surprise and angry,\nrespectively. The S2D model with facial"
        },
        {
          "w/": "without\nfacial\nlandmarks is more likely to focus on the background and other"
        },
        {
          "w/": "to changes in muscle movements while the latter consistently focuses on specific regions and remains relatively unchanged."
        },
        {
          "w/": "datasets demonstrate the strong generalization ability of S2D."
        },
        {
          "w/": "We\nemphasize\nthat\nour\nproposed method\ndoes\nnot\nrequire"
        },
        {
          "w/": "retraining all model parameters on the DFER dataset. Instead,"
        },
        {
          "w/": "we only fine-tune a small number of parameters (less than 10%"
        },
        {
          "w/": "of tunable parameters), making it more parameter-efficient and"
        },
        {
          "w/": "practical."
        },
        {
          "w/": ""
        },
        {
          "w/": ""
        },
        {
          "w/": "F\n. Visualization Analysis"
        },
        {
          "w/": ""
        },
        {
          "w/": "1) Attention Visualization: To confirm the critical\nrole of"
        },
        {
          "w/": ""
        },
        {
          "w/": "the facial landmark-aware features in the S2D model, we visu-"
        },
        {
          "w/": ""
        },
        {
          "w/": "alize the attention scores of\nthe last\ntransformer block in Fig."
        },
        {
          "w/": ""
        },
        {
          "w/": "6. Our observations reveal\nthat\nthe attention of the S2D model"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": ""
        },
        {
          "12": "Happy"
        },
        {
          "12": "Sad"
        },
        {
          "12": "Neutral"
        },
        {
          "12": "Angry"
        },
        {
          "12": "Surprise"
        },
        {
          "12": "Disgust"
        },
        {
          "12": "Fear"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "and baseline+MCP+TMA model."
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "related facial\nregions successfully,\nthus benefiting FER."
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "Furthermore, by examining the\nchanges\nin attention over"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "time, we observed that\nthe model without\nlandmark knowl-"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "edge\nconsistently focuses on specific\nregions\n(e.g.,\neyes\nin"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "the first\nsequence\nrow 2,\nand\nboth\neyes\nand\nfaces\nin\nthe"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "second sequence\nrow 2)\nand remains\nrelatively unchanged."
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "In\ncontrast,\nthe model with\nlandmark\nknowledge\nexhibits"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "greater\nsensitivity to changes\nin muscle movements, with its"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "attention shifting accordingly in row 3 of\nthe two sequences."
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "For\nexample,\nin the first\nsequence, when the woman opens"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "her mouth,\nthe model’s\nattention\nis\nprimarily\non\nher\nlips,"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "gradually diminishing as her mouth closes\n(from frame 6 to"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "frame 13). A similar phenomenon also can be observed in the"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "second sequence, where the model’s attention initially focuses"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "on the eyes and mouth from frame 1 to frame 11 in row 3,"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "then disappears on the mouth in frame 4, gradually weakens,"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "and finally completely disappears on the mouth from frame"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "5 to frame 11. These observations\nstrongly demonstrate that"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "our model accurately captures the dynamic changes of\nfacial"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "emotions in the temporal dimension, with muscle movements"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "implicitly encoded in facial\nlandmark-aware features."
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "2) Visualization\nof\nFeature Distribution: We\ngradually"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "added MCP, TMA and SDL to the baseline model and visu-"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "alized the high-level\nfeatures of\nthe DFEW (fd 1)\ntesting set"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "using t-SNE [72]. Fig. 7 illustrates that our proposed modules"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "significantly enhance the discriminative and separable qualities"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "of high-level\nfeatures\ncompared to the baseline model. The"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "inclusion of\nthe TMA module notably improves the discrim-"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "inability\nof\nhigh-level\nfeatures,\nindicating\nits\neffectiveness"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "in\ncapturing\ndynamic\nfacial\nexpression\nchanges\nfor DFER."
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "Compared to column 3, each category’s feature distribution in"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "column 4 is more concentrated, particularly for happy, which"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "exhibits a more distinct boundary than other expressions. This"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "is attributable to the proposed SDL. However, it is notable that"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "the features corresponding to surprise and fear are intermixed"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "and challenging to distinguish. Additionally, disgust\nfeatures"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "are dispersed, lacking a clear clustering center. This aligns with"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": ""
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "the emotion-specific accuracies in Table X and is attributed to"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "the data shortage of disgust\nin the dataset available."
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "3) Visualization\nof Output\nProbability Distribution:\nTo"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "evaluate the effectiveness of our proposed SDL, a comparison"
        },
        {
          "Fig. 7. High-level feature visualization on DFEW (fd1) using t-SNE [72]. Here, we compare our final S2D model with baseline model, baseline+MCP model": "was\nconducted\nbetween\nthe\noutput\nprobability\ndistributions"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "13": "[11] H. Li, H. Niu, Z. Zhu, and F. Zhao, “Intensity-aware loss for dynamic"
        },
        {
          "13": "the AAAI\nfacial expression recognition in the wild,” in Proceedings of"
        },
        {
          "13": ""
        },
        {
          "13": "Conference on Artificial\nIntelligence, vol. 37, no. 1, 2023, pp. 67–75."
        },
        {
          "13": "[12] H. Li, H. Niu, Z. Zhu, and F. a. Zhao, “Cliper: A unified vision-language"
        },
        {
          "13": "framework for in-the-wild facial expression recognition,” arXiv preprint"
        },
        {
          "13": ""
        },
        {
          "13": "arXiv:2303.00193, 2023."
        },
        {
          "13": "[13]\nZ. Zhao and I. Patras, “Prompting visual-language models for dynamic"
        },
        {
          "13": "facial expression recognition,” arXiv preprint arXiv:2308.13382, 2023."
        },
        {
          "13": ""
        },
        {
          "13": "[14]\nL.\nSun, Z. Lian, B. Liu,\nand\nJ. Tao,\n“Mae-dfer: Efficient masked"
        },
        {
          "13": ""
        },
        {
          "13": "autoencoder\nfor self-supervised dynamic facial expression recognition,”"
        },
        {
          "13": "pp. 6110–6121, 2023."
        },
        {
          "13": "[15]\nS. Li, W. Deng, and J. Du, “Reliable crowdsourcing and deep locality-"
        },
        {
          "13": ""
        },
        {
          "13": "preserving learning for expression recognition in the wild,” 2017 IEEE"
        },
        {
          "13": ""
        },
        {
          "13": "Conference on Computer Vision and Pattern Recognition (CVPR), pp."
        },
        {
          "13": "2584–2593, 2017."
        },
        {
          "13": "[16] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “Affectnet: A database"
        },
        {
          "13": "for facial expression, valence, and arousal computing in the wild,” IEEE"
        },
        {
          "13": ""
        },
        {
          "13": "Transactions on Affective Computing, vol. 10, pp. 18–31, 2017."
        },
        {
          "13": ""
        },
        {
          "13": "[17]\nE. Barsoum, C. Zhang, C. Canton-Ferrer, and Z. Zhang, “Training deep"
        },
        {
          "13": "networks\nfor\nfacial\nexpression\nrecognition with\ncrowd-sourced\nlabel"
        },
        {
          "13": "the 18th ACM International Conference\ndistribution,” Proceedings of"
        },
        {
          "13": ""
        },
        {
          "13": "on Multimodal\nInteraction, 2016."
        },
        {
          "13": ""
        },
        {
          "13": "[18] Y. Wang, Y.\nSun, Y. Huang, Z. Liu,\nS. Gao, W. Zhang, W. Ge,"
        },
        {
          "13": "and W. Zhang, “Ferv39k: A large-scale multi-scene dataset\nfor\nfacial"
        },
        {
          "13": "of\nthe\nIEEE/CVF\nexpression\nrecognition\nin\nvideos,”\nin Proceedings"
        },
        {
          "13": ""
        },
        {
          "13": "Conference\non Computer Vision\nand Pattern Recognition,\n2022,\npp."
        },
        {
          "13": ""
        },
        {
          "13": "20 922–20 931."
        },
        {
          "13": "[19] Y. Liu, W. Dai, C. Feng, W. Wang, G. Yin, J. Zeng, and S. Shan, “Mafw:"
        },
        {
          "13": "A large-scale, multi-modal, compound affective database for dynamic"
        },
        {
          "13": "the 30th ACM\nfacial expression recognition in the wild,” Proceedings of"
        },
        {
          "13": ""
        },
        {
          "13": "International Conference on Multimedia, 2022."
        },
        {
          "13": ""
        },
        {
          "13": "[20]\nJ. L. Fleiss, “Measuring nominal scale agreement among many raters.”"
        },
        {
          "13": "Psychological bulletin, vol. 76, no. 5, p. 378, 1971."
        },
        {
          "13": "[21] Y. Fan, X. Lu, D. Li,\nand Y. Liu,\n“Video-based emotion recognition"
        },
        {
          "13": ""
        },
        {
          "13": "the 18th ACM\nusing cnn-rnn and c3d hybrid networks,” Proceedings of"
        },
        {
          "13": ""
        },
        {
          "13": "International Conference on Multimodal\nInteraction, 2016."
        },
        {
          "13": "[22]\nJ. Kossaifi, A. Toisoul, A. Bulat, Y. Panagakis, T. M. Hospedales, and"
        },
        {
          "13": "M. Pantic, “Factorized higher-order cnns with an application to spatio-"
        },
        {
          "13": "temporal emotion estimation,” 2020 IEEE/CVF Conference on Computer"
        },
        {
          "13": "Vision and Pattern Recognition (CVPR), pp. 6059–6068, 2019."
        },
        {
          "13": "[23]\nL. Sun, Z. Lian, J. Tao, B. Liu, and M. Niu, “Multi-modal continuous"
        },
        {
          "13": ""
        },
        {
          "13": "dimensional\nemotion\nrecognition\nusing\nrecurrent\nneural\nnetwork\nand"
        },
        {
          "13": ""
        },
        {
          "13": "the 1st International on Mul-\nself-attention mechanism,” Proceedings of"
        },
        {
          "13": ""
        },
        {
          "13": "timodal Sentiment Analysis in Real-life Media Challenge and Workshop,"
        },
        {
          "13": ""
        },
        {
          "13": "2020."
        },
        {
          "13": ""
        },
        {
          "13": "[24]\nF. Ma, B. Sun,\nand S. Li,\n“Spatio-temporal\ntransformer\nfor dynamic"
        },
        {
          "13": ""
        },
        {
          "13": "facial expression recognition in the wild,” ArXiv, vol. abs/2205.04749,"
        },
        {
          "13": ""
        },
        {
          "13": "2022."
        },
        {
          "13": ""
        },
        {
          "13": "[25]\nJ. Li, Y. Chen, X. Zhang,\nJ. Nie, Z. Li, Y. Yu, Y. Zhang, R. Hong,"
        },
        {
          "13": ""
        },
        {
          "13": "and M. Wang, “Multimodal\nfeature extraction and fusion for emotional"
        },
        {
          "13": ""
        },
        {
          "13": "reaction intensity estimation and expression classification in videos with"
        },
        {
          "13": ""
        },
        {
          "13": "transformers,” in Proceedings of the IEEE/CVF Conference on Computer"
        },
        {
          "13": ""
        },
        {
          "13": "Vision and Pattern Recognition, 2023, pp. 5837–5843."
        },
        {
          "13": ""
        },
        {
          "13": "[26]\nZ. Cai,\nS. Ghosh, K.\nStefanov, A. Dhall,\nJ. Cai, H. Rezatofighi,"
        },
        {
          "13": ""
        },
        {
          "13": "R. Haffari, and M. Hayat, “Marlin: Masked autoencoder for facial video"
        },
        {
          "13": ""
        },
        {
          "13": "the IEEE/CVF Conference\nrepresentation learning,” in Proceedings of"
        },
        {
          "13": ""
        },
        {
          "13": "on Computer Vision and Pattern Recognition (CVPR),\nJune 2023, pp."
        },
        {
          "13": ""
        },
        {
          "13": "1493–1504."
        },
        {
          "13": ""
        },
        {
          "13": "[27] A. Radford,\nJ. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,"
        },
        {
          "13": ""
        },
        {
          "13": "G. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable"
        },
        {
          "13": ""
        },
        {
          "13": "International\nvisual models\nfrom natural\nlanguage\nsupervision,”\nin"
        },
        {
          "13": ""
        },
        {
          "13": "conference on machine learning.\nPMLR, 2021, pp. 8748–8763."
        },
        {
          "13": ""
        },
        {
          "13": "[28]\nJ. Carreira and A. Zisserman, “Quo vadis, action recognition? a new"
        },
        {
          "13": ""
        },
        {
          "13": "the IEEE Conference\nmodel and the kinetics dataset,” in proceedings of"
        },
        {
          "13": ""
        },
        {
          "13": "on Computer Vision and Pattern Recognition, 2017, pp. 6299–6308."
        },
        {
          "13": ""
        },
        {
          "13": "[29]\nS. Chen, C. Ge, Z. Tong,\nJ. Wang, Y. Song,\nJ. Wang,\nand P. Luo,"
        },
        {
          "13": "“Adaptformer: Adapting vision transformers for scalable visual recogni-"
        },
        {
          "13": "tion,” Advances in Neural\nInformation Processing Systems, vol. 35, pp."
        },
        {
          "13": "16 664–16 678, 2022."
        },
        {
          "13": "[30]\nT. Yang, Y. Zhu, Y. Xie, A. Zhang, C. Chen, and M. Li, “Aim: Adapting"
        },
        {
          "13": "International\nimage models\nfor\nefficient\nvideo\naction\nrecognition,”"
        },
        {
          "13": "Conference on Learning Representations, 2022."
        },
        {
          "13": "[31]\nJ. Pan, Z. Lin, X. Zhu,\nJ. Shao,\nand H. Li,\n“St-adapter: Parameter-"
        },
        {
          "13": "in Neural\nInfor-\nefficient\nimage-to-video transfer\nlearning,” Advances"
        },
        {
          "13": "mation Processing Systems, vol. 35, pp. 26 462–26 477, 2022."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": "the IEEE/CVF Conference on Computer Vision and\nin Proceedings of"
        },
        {
          "14": "Pattern Recognition, 2023, pp. 17 958–17 968."
        },
        {
          "14": "[54]\nJ. Li, J. Nie, D. Guo, R. Hong, and M. Wang, “Emotion separation and"
        },
        {
          "14": "recognition from a facial expression by generating the poker\nface with"
        },
        {
          "14": "vision transformers,” arXiv preprint arXiv:2207.11081, 2023."
        },
        {
          "14": "[55]\nJ. Deng,\nJ. Guo, E. Ververas,\nI. Kotsia, and S. Zafeiriou, “Retinaface:"
        },
        {
          "14": "Single-shot multi-level\nface localisation in the wild,” in Proceedings of"
        },
        {
          "14": "the IEEE/CVF conference on computer vision and pattern recognition,"
        },
        {
          "14": "2020, pp. 5203–5212."
        },
        {
          "14": "[56]\nS. I. Serengil and A. Ozpinar, “Lightface: A hybrid deep face recognition"
        },
        {
          "14": "framework,” in 2020 Innovations in Intelligent Systems and Applications"
        },
        {
          "14": "Conference (ASYU).\nIEEE, 2020, pp. 23–27."
        },
        {
          "14": "[57] K. He, X. Chen, S. Xie, Y. Li, P. Doll´ar, and R. Girshick, “Masked au-"
        },
        {
          "14": "toencoders are scalable vision learners,” in Proceedings of the IEEE/CVF"
        },
        {
          "14": "conference\non\ncomputer\nvision\nand\npattern\nrecognition,\n2022,\npp."
        },
        {
          "14": "16 000–16 009."
        },
        {
          "14": "[58] O. Russakovsky,\nJ. Deng, H.\nSu,\nJ. Krause,\nS.\nSatheesh,\nS. Ma,"
        },
        {
          "14": "Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet\nlarge"
        },
        {
          "14": "journal of computer\nscale visual\nrecognition challenge,” International"
        },
        {
          "14": ""
        },
        {
          "14": "vision, vol. 115, pp. 211–252, 2015."
        },
        {
          "14": ""
        },
        {
          "14": "[59] M.\nSandler, A. G. Howard, M.\nZhu, A.\nZhmoginov,\nand\nL.-C."
        },
        {
          "14": ""
        },
        {
          "14": "Chen,\n“Mobilenetv2:\nInverted residuals\nand linear bottlenecks,” 2018"
        },
        {
          "14": ""
        },
        {
          "14": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,"
        },
        {
          "14": ""
        },
        {
          "14": "pp. 4510–4520, 2018."
        },
        {
          "14": ""
        },
        {
          "14": "[60] X. Guo, S. Li, J. Zhang, J. Ma, L. Ma, W. Liu, and H. Ling, “Pfld: A"
        },
        {
          "14": ""
        },
        {
          "14": "practical\nfacial\nlandmark detector,” ArXiv, vol. abs/1902.10859, 2019."
        },
        {
          "14": ""
        },
        {
          "14": "[61] K. Wang, X. Peng,\nJ. Yang, S. Lu, and Y. Qiao, “Suppressing uncer-"
        },
        {
          "14": ""
        },
        {
          "14": "tainties for\nlarge-scale facial expression recognition,” in Proceedings of"
        },
        {
          "14": ""
        },
        {
          "14": "the IEEE/CVF conference on computer vision and pattern recognition,"
        },
        {
          "14": ""
        },
        {
          "14": "2020, pp. 6897–6906."
        },
        {
          "14": ""
        },
        {
          "14": "[62] Y. Zhang, C. Wang, X. Ling, and W. Deng, “Learn from all: Erasing"
        },
        {
          "14": ""
        },
        {
          "14": "attention consistency for noisy label\nfacial expression recognition,” in"
        },
        {
          "14": ""
        },
        {
          "14": "Computer Vision–ECCV 2022:\n17th European Conference, Tel Aviv,"
        },
        {
          "14": ""
        },
        {
          "14": "Israel, October 23–27, 2022, Proceedings, Part XXVI.\nSpringer, 2022,"
        },
        {
          "14": ""
        },
        {
          "14": "pp. 418–434."
        },
        {
          "14": ""
        },
        {
          "14": "[63]\nJ. She, Y. Hu, H. Shi,\nJ. Wang, Q. Shen,\nand T. Mei,\n“Dive\ninto"
        },
        {
          "14": ""
        },
        {
          "14": "ambiguity: Latent distribution mining and pairwise uncertainty estima-"
        },
        {
          "14": ""
        },
        {
          "14": "the IEEE/CVF\ntion for facial expression recognition,” in Proceedings of"
        },
        {
          "14": ""
        },
        {
          "14": "conference on computer vision and pattern recognition, 2021, pp. 6248–"
        },
        {
          "14": ""
        },
        {
          "14": "6257."
        },
        {
          "14": ""
        },
        {
          "14": "[64] N. Le, K. Nguyen, Q. Tran, E. Tjiputra, B. Le,\nand A. Nguyen,"
        },
        {
          "14": ""
        },
        {
          "14": "“Uncertainty-aware\nlabel\ndistribution\nlearning\nfor\nfacial\nexpression"
        },
        {
          "14": ""
        },
        {
          "14": "the\nIEEE/CVF Winter Conference on\nrecognition,”\nin Proceedings of"
        },
        {
          "14": ""
        },
        {
          "14": "Applications of Computer Vision, 2023, pp. 6088–6097."
        },
        {
          "14": ""
        },
        {
          "14": "[65] H. Li, M.\nSui,\nF. Zhao, Z. Zha,\nand\nF. Wu,\n“Mvt: mask\nvision"
        },
        {
          "14": ""
        },
        {
          "14": "transformer for facial expression recognition in the wild,” arXiv preprint"
        },
        {
          "14": ""
        },
        {
          "14": "arXiv:2106.04520, 2021."
        },
        {
          "14": ""
        },
        {
          "14": "[66]\nF. Ma, B. Sun,\nand S. Li,\n“Facial\nexpression recognition with visual"
        },
        {
          "14": ""
        },
        {
          "14": "IEEE Transactions\non\ntransformers\nand\nattentional\nselective\nfusion,”"
        },
        {
          "14": ""
        },
        {
          "14": "Affective Computing, 2021."
        },
        {
          "14": ""
        },
        {
          "14": "[67]\nF. Xue, Q. Wang,\nand G. Guo,\n“Transfer: Learning\nrelation-aware"
        },
        {
          "14": ""
        },
        {
          "14": "facial\nexpression representations with transformers,” 2021 IEEE/CVF"
        },
        {
          "14": ""
        },
        {
          "14": "International Conference on Computer Vision (ICCV), pp. 3581–3590,"
        },
        {
          "14": ""
        },
        {
          "14": "2021."
        },
        {
          "14": ""
        },
        {
          "14": "[68]\nF. Xue, Q. Wang, Z. Tan, Z. Ma,\nand G. Guo,\n“Vision transformer"
        },
        {
          "14": ""
        },
        {
          "14": "with attentive pooling for\nrobust\nfacial expression recognition,” IEEE"
        },
        {
          "14": ""
        },
        {
          "14": "Transactions on Affective Computing, 2022."
        },
        {
          "14": ""
        },
        {
          "14": "[69]\nF. Ma, B. Sun, and S. Li, “Transformer-augmented network with online"
        },
        {
          "14": ""
        },
        {
          "14": "label correction for\nfacial expression recognition,” IEEE Transactions"
        },
        {
          "14": ""
        },
        {
          "14": "on Affective Computing, 2023."
        },
        {
          "14": ""
        },
        {
          "14": "[70] H. Li, M. Sui, Z. Zhu et al., “Nr-dfernet: Noise-robust network for dy-"
        },
        {
          "14": "namic facial expression recognition,” arXiv preprint arXiv:2206.04975,"
        },
        {
          "14": "2022."
        },
        {
          "14": "[71]\nZ. Tong, Y. Song, J. Wang, and L. Wang, “Videomae: Masked autoen-"
        },
        {
          "14": "coders are data-efficient\nlearners for self-supervised video pre-training,”"
        },
        {
          "14": "Advances in neural\ninformation processing systems, vol. 35, pp. 10 078–"
        },
        {
          "14": "10 093, 2022."
        },
        {
          "14": "[72]\nL. Van der Maaten and G. Hinton, “Visualizing data using t-sne.” Journal"
        },
        {
          "14": "of machine learning research, vol. 9, no. 11, 2008."
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A facial expression emotion recognition based human-robot interaction system",
      "authors": [
        "Z Liu",
        "M Wu",
        "W Cao",
        "L Chen",
        "J.-P Xu",
        "R Zhang",
        "M Zhou",
        "J.-W Mao"
      ],
      "year": "2017",
      "venue": "IEEE/CAA Journal of Automatica Sinica"
    },
    {
      "citation_id": "2",
      "title": "Impact of deep learning approaches on facial expression recognition in healthcare industries",
      "authors": [
        "C Bisogni",
        "A Castiglione",
        "S Hossain",
        "F Narducci",
        "S Umer"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "3",
      "title": "Towards facial expression analysis in a driver assistance system",
      "authors": [
        "T Wilhelm"
      ],
      "year": "2019",
      "venue": "Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "4",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "X Jiang",
        "Y Zong",
        "W Zheng",
        "C Tang",
        "W Xia",
        "C Lu",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "5",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "D Tran",
        "L Bourdev",
        "R Fergus",
        "L Torresani",
        "M Paluri"
      ],
      "year": "2014",
      "venue": "2015 IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "6",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "D Tran",
        "H Wang",
        "L Torresani",
        "J Ray",
        "Y Lecun",
        "M Paluri"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "8",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Z Zhao",
        "Q Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Clip-aware expressive feature learning for video-based facial expression recognition",
      "authors": [
        "Y Liu",
        "C Feng",
        "X Yuan",
        "L Zhou",
        "W Wang",
        "J Qin",
        "Z Luo"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "10",
      "title": "Expression snippet transformer for robust video-based facial expression recognition",
      "authors": [
        "Y Liu",
        "W Wang",
        "C Feng",
        "H Zhang",
        "Z Chen",
        "Y Zhan"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Intensity-aware loss for dynamic facial expression recognition in the wild",
      "authors": [
        "H Li",
        "H Niu",
        "Z Zhu",
        "F Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Cliper: A unified vision-language framework for in-the-wild facial expression recognition",
      "authors": [
        "H Li",
        "H Niu",
        "Z Zhu",
        "F Zhao"
      ],
      "year": "2023",
      "venue": "Cliper: A unified vision-language framework for in-the-wild facial expression recognition",
      "arxiv": "arXiv:2303.00193"
    },
    {
      "citation_id": "13",
      "title": "Prompting visual-language models for dynamic facial expression recognition",
      "authors": [
        "Z Zhao",
        "I Patras"
      ],
      "year": "2023",
      "venue": "Prompting visual-language models for dynamic facial expression recognition",
      "arxiv": "arXiv:2308.13382"
    },
    {
      "citation_id": "14",
      "title": "Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "L Sun",
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition"
    },
    {
      "citation_id": "15",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "16",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Canton-Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "18",
      "title": "Ferv39k: A large-scale multi-scene dataset for facial expression recognition in videos",
      "authors": [
        "Y Wang",
        "Y Sun",
        "Y Huang",
        "Z Liu",
        "S Gao",
        "W Zhang",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Mafw: A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild",
      "authors": [
        "Y Liu",
        "W Dai",
        "C Feng",
        "W Wang",
        "G Yin",
        "J Zeng",
        "S Shan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "Measuring nominal scale agreement among many raters",
      "authors": [
        "J Fleiss"
      ],
      "year": "1971",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "21",
      "title": "Video-based emotion recognition using cnn-rnn and c3d hybrid networks",
      "authors": [
        "Y Fan",
        "X Lu",
        "D Li",
        "Y Liu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "22",
      "title": "Factorized higher-order cnns with an application to spatiotemporal emotion estimation",
      "authors": [
        "J Kossaifi",
        "A Toisoul",
        "A Bulat",
        "Y Panagakis",
        "T Hospedales",
        "M Pantic"
      ],
      "year": "2019",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "23",
      "title": "Multi-modal continuous dimensional emotion recognition using recurrent neural network and self-attention mechanism",
      "authors": [
        "L Sun",
        "Z Lian",
        "J Tao",
        "B Liu",
        "M Niu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop"
    },
    {
      "citation_id": "24",
      "title": "Spatio-temporal transformer for dynamic facial expression recognition in the wild",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2022",
      "venue": "ArXiv"
    },
    {
      "citation_id": "25",
      "title": "Multimodal feature extraction and fusion for emotional reaction intensity estimation and expression classification in videos with transformers",
      "authors": [
        "J Li",
        "Y Chen",
        "X Zhang",
        "J Nie",
        "Z Li",
        "Y Yu",
        "Y Zhang",
        "R Hong",
        "M Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Marlin: Masked autoencoder for facial video representation learning",
      "authors": [
        "Z Cai",
        "S Ghosh",
        "K Stefanov",
        "A Dhall",
        "J Cai",
        "H Rezatofighi",
        "R Haffari",
        "M Hayat"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "27",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "28",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "29",
      "title": "Adaptformer: Adapting vision transformers for scalable visual recognition",
      "authors": [
        "S Chen",
        "C Ge",
        "Z Tong",
        "J Wang",
        "Y Song",
        "J Wang",
        "P Luo"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "30",
      "title": "Aim: Adapting image models for efficient video action recognition",
      "authors": [
        "T Yang",
        "Y Zhu",
        "Y Xie",
        "A Zhang",
        "C Chen",
        "M Li"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "31",
      "title": "St-adapter: Parameterefficient image-to-video transfer learning",
      "authors": [
        "J Pan",
        "Z Lin",
        "X Zhu",
        "J Shao",
        "H Li"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "Tem-adapter: Adapting image-text pretraining for video question answer",
      "authors": [
        "G Chen",
        "X Liu",
        "G Wang",
        "K Zhang",
        "P Torr",
        "X.-P Zhang",
        "Y Tang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "33",
      "title": "Au-aware deep networks for facial expression recognition",
      "authors": [
        "M Liu",
        "S Li",
        "S Shan",
        "X Chen"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "34",
      "title": "Understanding and mitigating annotation bias in facial expression recognition",
      "authors": [
        "Y Chen",
        "J Joo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "35",
      "title": "Fine-grained facial expression recognition in the wild",
      "authors": [
        "L Liang",
        "C Lang",
        "Y Li",
        "S Feng",
        "J Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "36",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "H Jung",
        "S Lee",
        "J Yim",
        "S Park",
        "J Kim"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "37",
      "title": "Impact of facial landmark localization on facial expression recognition",
      "authors": [
        "R Belmonte",
        "B Allaert",
        "P Tirilly",
        "I Bilasco",
        "C Djeraba",
        "N Sebe"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Facial expression recognition in video with multiple feature fusion",
      "authors": [
        "J Chen",
        "Z Chen",
        "Z Chi",
        "H Fu"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Poster: A pyramid cross-fusion transformer network for facial expression recognition",
      "authors": [
        "C Zheng",
        "M Mendieta",
        "C Chen"
      ],
      "year": "2023",
      "venue": "Poster: A pyramid cross-fusion transformer network for facial expression recognition"
    },
    {
      "citation_id": "40",
      "title": "Multimodal feature extraction and fusion for emotional reaction intensity estimation and expression classification in videos with transformers",
      "authors": [
        "J Li",
        "Y Chen",
        "X Zhang",
        "J Nie",
        "Z Li",
        "Y Yu",
        "Y Zhang",
        "R Hong",
        "M Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "41",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Baird",
        "A Cowen",
        "S Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "42",
      "title": "Parameter-efficient transfer learning for nlp",
      "authors": [
        "N Houlsby",
        "A Giurgiu",
        "S Jastrzebski",
        "B Morrone",
        "Q De Laroussilhe",
        "A Gesmundo",
        "M Attariyan",
        "S Gelly"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "43",
      "title": "Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices",
      "authors": [
        "S Chen",
        "Y Liu",
        "X Gao",
        "Z Han"
      ],
      "year": "2018",
      "venue": "ArXiv"
    },
    {
      "citation_id": "44",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "arxiv": "arXiv:2106.09685"
    },
    {
      "citation_id": "45",
      "title": "The power of scale for parameter-efficient prompt tuning",
      "authors": [
        "B Lester",
        "R Al-Rfou",
        "N Constant"
      ],
      "year": "2021",
      "venue": "The power of scale for parameter-efficient prompt tuning",
      "arxiv": "arXiv:2104.08691"
    },
    {
      "citation_id": "46",
      "title": "Towards a unified view of parameter-efficient transfer learning",
      "authors": [
        "J He",
        "C Zhou",
        "X Ma",
        "T Berg-Kirkpatrick",
        "G Neubig"
      ],
      "year": "2021",
      "venue": "Towards a unified view of parameter-efficient transfer learning",
      "arxiv": "arXiv:2110.04366"
    },
    {
      "citation_id": "47",
      "title": "Mar: Masked autoencoders for efficient action recognition",
      "authors": [
        "Z Qing",
        "S Zhang",
        "Z Huang",
        "X Wang",
        "Y Wang",
        "Y Lv",
        "C Gao",
        "N Sang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "48",
      "title": "Visual prompt tuning",
      "authors": [
        "M Jia",
        "L Tang",
        "B.-C Chen",
        "C Cardie",
        "S Belongie",
        "B Hariharan",
        "S.-N Lim"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "49",
      "title": "Visual prompt multimodal tracking",
      "authors": [
        "J Zhu",
        "S Lai",
        "X Chen",
        "D Wang",
        "H Lu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "50",
      "title": "Visual prompt tuning for generative transfer learning",
      "authors": [
        "K Sohn",
        "H Chang",
        "J Lezama",
        "L Polania",
        "H Zhang",
        "Y Hao",
        "I Essa",
        "L Jiang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "51",
      "title": "Visual prompting via image inpainting",
      "authors": [
        "A Bar",
        "Y Gandelsman",
        "T Darrell",
        "A Globerson",
        "A Efros"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "52",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "53",
      "title": "Rethinking the learning paradigm for dynamic facial expression recognition",
      "authors": [
        "H Wang",
        "B Li",
        "S Wu",
        "S Shen",
        "F Liu",
        "S Ding",
        "A Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "54",
      "title": "Emotion separation and recognition from a facial expression by generating the poker face with vision transformers",
      "authors": [
        "J Li",
        "J Nie",
        "D Guo",
        "R Hong",
        "M Wang"
      ],
      "year": "2023",
      "venue": "Emotion separation and recognition from a facial expression by generating the poker face with vision transformers",
      "arxiv": "arXiv:2207.11081"
    },
    {
      "citation_id": "55",
      "title": "Retinaface: Single-shot multi-level face localisation in the wild",
      "authors": [
        "J Deng",
        "J Guo",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "56",
      "title": "Lightface: A hybrid deep face recognition framework",
      "authors": [
        "S Serengil",
        "A Ozpinar"
      ],
      "year": "2020",
      "venue": "2020 Innovations in Intelligent Systems and Applications Conference (ASYU)"
    },
    {
      "citation_id": "57",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "58",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "O Russakovsky",
        "J Deng",
        "H Su",
        "J Krause",
        "S Satheesh",
        "S Ma",
        "Z Huang",
        "A Karpathy",
        "A Khosla",
        "M Bernstein"
      ],
      "year": "2015",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "59",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "M Sandler",
        "A Howard",
        "M Zhu",
        "A Zhmoginov",
        "L.-C Chen"
      ],
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "60",
      "title": "Pfld: A practical facial landmark detector",
      "authors": [
        "X Guo",
        "S Li",
        "J Zhang",
        "J Ma",
        "L Ma",
        "W Liu",
        "H Ling"
      ],
      "year": "1902",
      "venue": "ArXiv"
    },
    {
      "citation_id": "61",
      "title": "Suppressing uncertainties for large-scale facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "S Lu",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "62",
      "title": "Learn from all: Erasing attention consistency for noisy label facial expression recognition",
      "authors": [
        "Y Zhang",
        "C Wang",
        "X Ling",
        "W Deng"
      ],
      "year": "2022",
      "venue": "Computer Vision-ECCV 2022: 17th European Conference"
    },
    {
      "citation_id": "63",
      "title": "Dive into ambiguity: Latent distribution mining and pairwise uncertainty estimation for facial expression recognition",
      "authors": [
        "J She",
        "Y Hu",
        "H Shi",
        "J Wang",
        "Q Shen",
        "T Mei"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "64",
      "title": "Uncertainty-aware label distribution learning for facial expression recognition",
      "authors": [
        "N Le",
        "K Nguyen",
        "Q Tran",
        "E Tjiputra",
        "B Le",
        "A Nguyen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "65",
      "title": "Mvt: mask vision transformer for facial expression recognition in the wild",
      "authors": [
        "H Li",
        "M Sui",
        "F Zhao",
        "Z Zha",
        "F Wu"
      ],
      "year": "2021",
      "venue": "Mvt: mask vision transformer for facial expression recognition in the wild",
      "arxiv": "arXiv:2106.04520"
    },
    {
      "citation_id": "66",
      "title": "Facial expression recognition with visual transformers and attentional selective fusion",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "67",
      "title": "Transfer: Learning relation-aware facial expression representations with transformers",
      "authors": [
        "F Xue",
        "Q Wang",
        "G Guo"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "68",
      "title": "Vision transformer with attentive pooling for robust facial expression recognition",
      "authors": [
        "F Xue",
        "Q Wang",
        "Z Tan",
        "Z Ma",
        "G Guo"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "69",
      "title": "Transformer-augmented network with online label correction for facial expression recognition",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "70",
      "title": "Nr-dfernet: Noise-robust network for dynamic facial expression recognition",
      "authors": [
        "H Li",
        "M Sui",
        "Z Zhu"
      ],
      "year": "2022",
      "venue": "Nr-dfernet: Noise-robust network for dynamic facial expression recognition",
      "arxiv": "arXiv:2206.04975"
    },
    {
      "citation_id": "71",
      "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "authors": [
        "Z Tong",
        "Y Song",
        "J Wang",
        "L Wang"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "72",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}