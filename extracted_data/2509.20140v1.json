{
  "paper_id": "2509.20140v1",
  "title": "Inconvad: A Two-Stage Dual-Tower Framework For Multimodal Emotion Inconsistency Detection",
  "published": "2025-09-24T14:03:59Z",
  "authors": [
    "Zongyi Li",
    "Junchuan Zhao",
    "Francis Bu Sung Lee",
    "Andrew Zi Han Yee"
  ],
  "keywords": [
    "Multimodal emotion inconsistency detection",
    "Affective computing",
    "Cross-modal representation learning",
    "Multimodal emotion analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Detecting emotional inconsistency across modalities is a key challenge in affective computing, as speech and text often convey conflicting cues. Existing approaches generally rely on incomplete emotion representations and employ unconditional fusion, which weakens performance when modalities are inconsistent. Moreover, little prior work explicitly addresses inconsistency detection itself. We propose InconVAD, a two-stage framework grounded in the Valence-Arousal-Dominance (VAD) space. In the first stage, independent uncertainty-aware models yield robust unimodal predictions. In the second stage, a classifier identifies cross-modal inconsistency and selectively integrates consistent signals. Extensive experiments show that InconVAD surpasses existing methods in both multimodal emotion inconsistency detection and modeling, offering a more reliable and interpretable solution for emotion analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the rapid development of human-computer interaction systems, accurately modeling emotions across multiple modalities has become a central challenge in affective computing  [1] . A key difficulty lies in the fact that speech and text do not always convey consistent affective states. Such inconsistencies may reflect complex psychological mechanisms, social strategies, or even clinical conditions  [2] . Detecting and quantifying multimodal emotion inconsistency therefore requires a reliable framework that directly compares emotional representations across modalities, rather than relying solely on unimodal cues or generic fusion strategies.\n\nCurrent approaches to cross-modal emotion analysis face two fundamental challenges. First, they often rely on inadequate emotional representation models, such as discrete emotion categories, which fail to capture the nuance and continuity of real-world affective expressions. Existing studies on emotion inconsistency at the categorical level typically reduce the task to comparing emotion labels across modalities. For example,  [3]  introduced the Multimodal Cross-Attention Bayesian Network (MCABN), which employed attention mechanisms to identify inconsistencies between images and text on social media, using category-based labels with pseudo-label guidance to resolve conflicts. Similarly,  [2]  proposed a framework for detecting Acoustic-Text Emotion Inconsistency (ATEI) in depression diagnosis by categorizing emotions into three classes (pos-itive, negative, neutral). While such label-based methods are computationally efficient and offer clear interpretability, they overlook variations in emotional intensity. As a result, they provide only a coarse quantification of inconsistency, leading to the loss of finegrained affective information in practical applications that require detailed emotional analysis.\n\nSecond, most multimodal emotion recognition methods are built upon the implicit assumption that modalities such as speech and text are emotionally congruent  [4] . When this assumption is violated, their fusion strategies-typically averaging or concatenation-produce vague intermediate representations that dilute the literal sentiment expressed in text and obscure the authentic emotional cues in speech  [5, 6] . Moreover, these models generally lack mechanisms for uncertainty awareness, treating all inputs as equally reliable and failing to assign greater weight to the clearer or more trustworthy modality when discrepancies arise. The absence of explicit modeling for emotion inconsistency thus remains a critical deficiency in current multimodal emotion recognition research, underscoring the need for frameworks that directly address inconsistency rather than treating it as a byproduct of fusion  [7] .\n\nTo address these issues, we propose InconVAD, a two-stage framework grounded in the Valence-Arousal-Dominance (VAD) space. In the first stage, modality-specific towers independently predict VAD values with uncertainty-aware estimation, providing robust and comparable unimodal representations. In the second stage, a classifier explicitly detects cross-modal inconsistency, while a gated fusion module selectively integrates predictions only for consistent pairs. This design prevents representation collapse in cases such as sarcasm and preserves modality-specific cues that would otherwise be lost. Extensive experiments demonstrate that InconVAD surpasses existing methods in both multimodal emotion inconsistency detection and modeling, while offering greater interpretability through modality-specific VAD predictions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "As illustrated in Fig.  1 , the proposed InconVAD framework operates in two stages. Stage 1 employs two unimodal towers for VAD pretraining: the speech tower and the text tower process raw speech and text inputs, respectively, and output modality-specific representations hs and ht, together with VAD means µ s , µ t and log-variances log σ 2 s , log σ 2 t . Stage 2 comprises an inconsistency detection head and a gated fusion tower. It takes the outputs of Stage 1, where the detection head predicts an inconsistency score pinc, and the fusion tower, activated only for consistent pairs, generates a fused repre-Fig.  1 . Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation, while Phase B employs an inconsistency detection head and a fusion module for selective integration. sentation h f and the final VAD prediction y f . The following subsections describe each component in detail.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Phase A: Unimodal Vad Pretraining",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Tower",
      "text": "The speech tower extracts both acoustic and prosodic cues for reliable VAD estimation. We employ a pre-trained Wav2Vec2-base model fSE(•)  [8]  to generate frame-level acoustic embeddings Hs ∈ R Ts×D , and a prosody extractor fP E (•) to compute pitch and energy features p ∈ R Ts×2  [9] . These complementary features are concatenated and projected through a linear layer to form the input Hin ∈ R Ts×D ′ , ensuring that prosodic variation is explicitly incorporated into the acoustic space. The sequence is then processed by two Conformer blocks  [10] , which integrate multi-head self-attention, convolutional modules, and Macaron-style feed-forward layers to capture both local dynamics and long-range dependencies. The resulting contextualized features H ′ s are aggregated by an ASPool module  [11] , producing a fixed-dimensional utterance-level embedding hs ∈ R D ′ . Finally, prediction heads output per-dimension means µ s ∈ R 3 and log-variances log σ 2 s ∈ R 3 , yielding uncertaintyaware unimodal estimates:\n\n(1)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Text Tower",
      "text": "The text tower captures semantic and lexical-level affective cues, complementing the speech tower. A RoBERTa-base encoder fT E (•)  [12]  maps tokenized inputs x ∈ V L text to contextual embeddings Ht ∈ R T t ×D , which encode rich semantic and syntactic information. To explicitly inject affective knowledge, we employ the NRC VAD Lexicon v2 fPrior(•)  [13]  to derive token-level prior vectors pt ∈ R T t ×3 , representing valence, arousal, and dominance values. These priors are integrated with the encoder outputs using a FiLM layer  [14] , producing gated representations H ′ t that remain dimensionally consistent while incorporating explicit affective supervision. An ASPool module then aggregates H ′ t into an utterancelevel embedding ht ∈ R D ′ , which is passed to prediction heads to produce µ t ∈ R 3 and log σ 2 t ∈ R 3 . This process yields modalityspecific textual predictions that align with the speech tower outputs in the shared VAD space:\n\n(2)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training Strategy",
      "text": "To optimize the unimodal towers, we adopt a heteroscedastic regression framework, where the prediction uncertainty is explicitly modeled through variance estimation. The training objective is the Gaussian Negative Log-Likelihood (NLL) of the ground-truth labels  [15] . For a given modality m ∈ {s, t} (speech or text) and dimension k ∈ {V, A, D}, the loss is defined as:\n\nwhere y k m denotes the ground-truth label, and µ k m and σ k m 2 are the predicted mean and variance, respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Phase B: Inconsistency Detection With Gated Fusion",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Inconsistency Detection Classifier",
      "text": "The inconsistency classifier is designed to assess cross-modal inconsistency without modifying the unimodal feature extractors trained in Phase A. It operates on speech and text representations, H ′ s and H ′ t , and determines whether they convey consistent affective states.\n\nTo align the modalities, the representations are first projected into a shared latent space through lightweight projectors fSP (•) and fT P (•), producing Hs, Ht ∈ R T ×D ′ . These projections normalize the features and reduce domain gaps between modalities. The two projected sequences are then concatenated to form a joint representation H ∈ R T ×2D ′ , which is passed to a binary classifier fC (•). The classifier consists of two linear layers with GELU activation, followed by LayerNorm and a Sigmoid output, yielding the predicted inconsistency score pinc ∈ [0, 1]. The overall classification process can be expressed as:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fusion Module",
      "text": "Our fusion module is an end-to-end architecture that integrates speech and text features to predict VAD emotional dimensions.\n\nBuilding on the outputs of the speech and text towers, the projected sequences are passed into a cross-modal fusion module. Inspired by  [16]  and  [17] , we design a Transformer block that jointly models intra-and inter-modal dependencies. Specifically, multi-head selfattention (MHSA) is applied to capture intra-modal relationships (speech → speech and text → text), while multi-head cross-attention (MHCA) enables cross-modal interactions (speech → text and text → speech). The outputs are subsequently processed with Lay-erNorm (LN) and feed-forward networks (FFN) to obtain modalityspecific contextual representations fs, ft ∈ R L×D ′ . To dynamically integrate information across modalities, we utilize the gated multimodal fusion mechanism. Each modality is first projected through a learnable weight matrix and then normalized via a softmax function applied along the modality axis. This produces element-wise gates gs, gt ∈ R T ×1 , which adaptively control the contribution of each modality at every time step. The final fused representation h f ∈ R T ×D ′ is obtained as the weighted combination of modality-specific features. The entire process can be formulated as:\n\nwhere Ws, Wt ∈ R D ′ ×D ′ are learnable projections, [gs, gt] are learned gates for speech and text, and ⊙ denotes the element-wise product with broadcasting along the feature dimension.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Strategy",
      "text": "The inconsistency classifier is trained with a joint objective that combines classification accuracy and geometric regularization of the latent space. The primary term is the Binary Cross-Entropy loss, LBCE, which compares the predicted inconsistency score pinc against the ground-truth label  [18] . To further structure the latent space, we introduce a margin-based auxiliary loss, Lmargin, applied to the projected representations Hs and Ht:\n\nwhere d = ∥ Hs -Ht∥2 denotes the Euclidean distance. This term pulls consistent pairs (y = 1) closer in the latent space, while enforcing a minimum margin m between inconsistent pairs (y = 0)  [19] . The final training objective combines the two losses with a weighting factor λmargin:\n\nFor the fusion module, training is guided by two complementary objectives. On labeled data with available VAD annotations, we apply the same Gaussian NLL loss as in Stage 1. For unlabeled pairs without VAD ground truth, we introduce a selective agreement loss, which encourages the fused prediction (µ f , log σ 2 f ) to agree with a Gaussian target derived from the unimodal outputs, defined as\n\nwhere\n\nThe overall training objective of the fusion tower combines the supervised and selective terms:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setups",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "For Phase A, we use the IEMOCAP  [20]  dataset for the speech tower and the EmoBank  [21]  dataset for the text tower, both with dimensional Valence-Arousal-Dominance (VAD) annotations. To ensure label comparability across datasets, we apply a parametric Beta Cumulative Distribution Function (CDF) transform that maps each original label v into an aligned value v ′ in a shared target distribution. A source value v is first normalized to [0, 1], then mapped to its quantile u = Fsrc(v) using the source CDF, and finally aligned by applying the target inverse CDF, v ′ = F -1 tgt (u). The aligned labels v ′ are used as training targets, while during evaluation the model predictions v′ are mapped back through the inverse transform to obtain v for comparison against the original labels v. The Beta-CDF process can be formulated as:\n\nFor Phase B, to train the inconsistency classifier, we use IEMO-CAP  [20]  dataset and the EmoV-DB  [22]  dataset to construct binarylabeled data pairs. Consistency pairs (label y = 1) include speechtext pairs from IEMOCAP and neutral emotion speech-text pairs from EmoV-DB. Inconsistent pairs (y = 0) are generated from EmoV-DB by pairing neutral transcripts with non-neutral speech recordings of the same utterance ID. In addition, to train the fusion tower, we use only consistency pairs (y = 1), as cross-modal fusion is meaningful only when the two modalities are emotionally aligned. The tower is trained to integrate the unimodal predictions into a single fused VAD output y f , providing a unified estimate rather than separate outputs for each modality.   [24]  0.595 0.601 0.499 0.565 RL-MT  [25]  0.648 0.668 0.537 0.618 MFCNN14  [26]  0.714 0.639 0.575 0.642 W2v2-b + BERT-b + L  [27]  0.625 0.661 0.570 0.618\n\nOurs (Fusion Tower) 0.741 0.644 0.586 0.657",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "In Phase A, both towers use pretrained backbones, Wav2Vec2-base for speech and RoBERTa-base for text (hidden size 768), followed by projection layers of dimension 256. Training is performed with a batch size of 16 for up to 50 epochs with early stopping (patience = 5). Optimization uses AdamW with learning rates of 2×10 -5 for the backbone and 1×10 -4 for the heads, combined with a cosine schedule and 10% warm-up. Weight decay is set to 0.01. The minimum variance of 2 × 10 -3 is used by the heteroscedastic Gaussian NLL loss function. We use the concordance correlation coefficient (CCC) as the evaluation metric. To avoid data leakage, we use a speakerindependent split for IEMOCAP. All utterances from each of the ten speakers go to a single partition with an 8/1/1 train/validation/test ratio via group-based splitting. For EmoBank, we follow the official train/validation/test split annotated in the corpus. For Phase B inconsistency detection, both the speech and text towers are kept frozen, and optimization is performed solely on the classifier head. Each pair of data forms the input after modalityspecific linear projections to a 256-dimensional space. We use a batch size of 32 and train for up to 50 epochs with early stopping (patience = 5). Optimization uses AdamW on classifier parameters, with a learning rate of 1 × 10 -3 and weight decay 0.01. The loss combines binary cross-entropy and a margin term (margin m = 0.9, λ = 0.15). For fusion tower, we keep the batch size at 16 and train for up to 50 epochs. Optimization uses AdamW with learning rate 1 × 10 -4 , weight decay 0.01, and a cosine schedule with 10% warm-up. We use CCC as the evaluation metric. For data split, we use the same speaker-independent split as in Phase A for IEMO-CAP dataset, while EmoV-DB utterances are partitioned into 8/1/1 train/validation/test sets.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results",
      "text": "In Phase A, our unimodal speech and text towers obtain average CCCs of 0.616 on IMEOCAP dataset and 0.549 on Emobank dataset, respectively. The fusion tower achieves 0.657, surpassing the existing state-of-the-art fusion models, including Dimensional MTL  [23] , Two-stage SVM  [24] , RL-MT  [25] , MFCNN14  [26] , and W2v2-b + BERT-b + L  [27] , as shown in Table  1 . The consistent gains across Valence, Arousal, and Dominance highlight the complementary strengths of speech and text tower, and the effectiveness of our transformer blocks and gated fusion mechanism.\n\nFor the inconsistency detection task, our classifier achieves the best performance across reported metrics. As shown in Table  2 , On the test set, it attains an accuracy of 92.3% and an F1-score of 92.2%, surpassing prior methods (SVM  [28] , ATEI  [2] ). Precision and re- call are likewise competitive, confirming that the leakage-free training protocol and composite loss design enable clear separation between consistent and inconsistent pairs. The decision threshold τ * was fixed based on validation by maximizing Youden's J, ensuring fair evaluation. We conduct a series of ablation studies to validate the necessity our architectural design as shown in Table  3 . For the speech tower, removing the Conformer blocks or prosody injection substantially reduces average CCC, highlighting the importance of temporal modeling and prosodic cues. Eliminating ASPool module also leads to a consistent drop, confirming its role in emphasizing salient acoustic features. For text tower, removing the affect prior gating decreases average CCC from 0.549 to 0.543, validating the benefit of injecting lexical affective knowledge. Similarly, discarding ASPool module lowers overall performance. For the Fusion Tower, removing Transformer block or the gated multimodal fusion mechanism degrades the average CCC from 0.657 to 0.641 and 0.625, respectively. These results confirm that modeling mutual interactions and dimension-specific modality weighting are both critical for robust cross-modal integration.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this study, we propose InconVAD, a cross-modal emotion inconsistency detection framework grounded in a shared threedimensional VAD emotion space. The framework produces interpretable and comparable VAD predictions from both speech and text, enabling effective inconsistency detection across modalities. This work establishes a foundation for building more emotionally intelligent and trustworthy human-computer interaction systems in real-world applications. Furthermore, our study highlights the importance of explicitly modeling cross-modal inconsistencies rather than assuming unimodal agreement, paving the way for more reliable multimodal affective computing systems.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the proposed InconVAD framework operates",
      "page": 1
    },
    {
      "caption": "Figure 1: Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "Singapore"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "itive, negative, neutral). While such label-based methods are com-"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "putationally efficient and offer clear\ninterpretability,\nthey overlook"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "variations in emotional\nintensity. As a result,\nthey provide only a"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "coarse quantification of\ninconsistency,\nleading to the loss of fine-"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "grained affective information in practical applications that\nrequire"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "detailed emotional analysis."
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "Second, most multimodal\nemotion\nrecognition methods\nare"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "built upon the implicit assumption that modalities such as speech"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "and text are emotionally congruent\n[4]. When this assumption is"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "violated,\ntheir\nfusion strategies—typically averaging or\nconcate-"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "nation—produce vague intermediate representations that dilute the"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "literal sentiment expressed in text and obscure the authentic emo-"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "tional\ncues\nin speech [5, 6].\nMoreover,\nthese models generally"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "lack mechanisms\nfor uncertainty awareness,\ntreating all\ninputs as"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "equally reliable and failing to assign greater weight\nto the clearer"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "or more trustworthy modality when discrepancies arise.\nThe ab-"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "sence of explicit modeling for emotion inconsistency thus remains"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "a critical deficiency in current multimodal emotion recognition re-"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "search, underscoring the need for frameworks that directly address"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "inconsistency rather than treating it as a byproduct of fusion [7]."
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "To address\nthese issues, we propose InconVAD, a two-stage"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "framework grounded in the Valence–Arousal–Dominance\n(VAD)"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "space.\nIn the first\nstage, modality-specific towers\nindependently"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "predict VAD values with uncertainty-aware estimation, providing"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "robust\nand comparable unimodal\nrepresentations.\nIn the\nsecond"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "stage, a classifier explicitly detects cross-modal inconsistency, while"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "a gated fusion module\nselectively integrates predictions only for"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "consistent pairs.\nThis design prevents\nrepresentation collapse in"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "cases\nsuch as\nsarcasm and preserves modality-specific\ncues\nthat"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "would otherwise be lost.\nExtensive experiments demonstrate that"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "InconVAD surpasses\nexisting methods\nin both multimodal\nemo-"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "tion inconsistency detection and modeling, while offering greater"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "interpretability through modality-specific VAD predictions."
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "2. METHODOLOGY"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "As illustrated in Fig. 1, the proposed InconVAD framework operates"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "in two stages. Stage 1 employs two unimodal\ntowers for VAD pre-"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "training:\nthe speech tower and the text tower process raw speech and"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "text\ninputs,\nrespectively,\nand output modality-specific representa-"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "tions hs and ht, together with VAD means µs, µt and log-variances"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "log σ2\ns, log σ2\nt . Stage 2 comprises an inconsistency detection head"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "and a gated fusion tower.\nIt\ntakes the outputs of Stage 1, where the"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "detection head predicts an inconsistency score pinc, and the fusion"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": "tower, activated only for consistent pairs, generates a fused repre-"
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        },
        {
          "4 Wee Kim Wee School of Communication and Information, Nanyang Technological University,": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "while Phase B employs an inconsistency detection head and a fusion module for selective integration."
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "sentation hf\nand the final VAD prediction yf . The following sub-"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "sections describe each component in detail."
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ""
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ""
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "2.1. Phase A: Unimodal VAD Pretraining"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ""
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "2.1.1.\nSpeech Tower"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ""
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "The speech tower extracts both acoustic and prosodic cues for\nre-"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ""
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "liable VAD estimation. We employ a pre-trained Wav2Vec2-base"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ""
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "model fSE(·) [8] to generate frame-level acoustic embeddings Hs ∈"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ""
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "RTs×D, and a prosody extractor fP E(·) to compute pitch and energy"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ""
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "features p ∈ RTs×2 [9]. These complementary features are concate-"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ""
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "nated and projected through a linear layer to form the input Hin ∈"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "RTs×D′"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ", ensuring that prosodic variation is explicitly incorporated"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "into the acoustic space. The sequence is then processed by two Con-"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ""
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "former blocks [10], which integrate multi-head self-attention, con-"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "volutional modules, and Macaron-style feed-forward layers to cap-"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "ture both local dynamics and long-range dependencies. The result-"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "ing contextualized features H′\ns are aggregated by an ASPool mod-"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "ule [11], producing a fixed-dimensional utterance-level embedding"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ""
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ".\nFinally, prediction heads output per-dimension means\nhs ∈ RD′"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "log σ2\nand log-variances\ns ∈ R3, yielding uncertainty-\nµs ∈ R3"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ""
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "aware unimodal estimates:"
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ""
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": ""
        },
        {
          "Fig. 1. Overview of the proposed InconVAD framework with two phases: Phase A builds speech and text towers for unimodal VAD estimation,": "Hin = Linear([fSE(a), fP E(a)]),"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "introduce a margin-based auxiliary loss, Lmargin, applied to the pro-"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "2.2.1.\nInconsistency Detection Classifier",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "jected representations ˜Hs and ˜Ht:"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "The inconsistency classifier is designed to assess cross-modal incon-",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "(6)\nLmargin = y · d2 + (1 − y) · max(0, m − d)2,"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "sistency without modifying the unimodal feature extractors trained",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "in Phase A. It operates on speech and text representations, H′\ns and",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "where d = ∥ ˜Hs − ˜Ht∥2 denotes the Euclidean distance. This term"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "H′\nt, and determines whether they convey consistent affective states.",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "pulls consistent pairs (y = 1) closer in the latent space, while en-"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "To align the modalities,\nthe representations are first projected",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "forcing a minimum margin m between inconsistent pairs (y = 0)"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "into a shared latent space through lightweight projectors fSP (·) and",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "[19].\nThe final\ntraining objective combines the two losses with a"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": ". These projections normalize\nfT P (·), producing ˜Hs, ˜Ht ∈ RT ×D′",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "weighting factor λmargin:"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "the features and reduce domain gaps between modalities. The two",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "projected sequences are then concatenated to form a joint representa-",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "(7)\nLCLS = LBCE + λmargin · Lmargin."
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "tion ˜H ∈ RT ×2D′\n, which is passed to a binary classifier fC (·). The",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "For the fusion module, training is guided by two complementary"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "classifier consists of\ntwo linear\nlayers with GELU activation,\nfol-",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "objectives. On labeled data with available VAD annotations, we ap-"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "lowed by LayerNorm and a Sigmoid output, yielding the predicted",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "ply the same Gaussian NLL loss as in Stage 1. For unlabeled pairs"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "inconsistency score pinc ∈ [0, 1]. The overall classification process",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "without VAD ground truth, we introduce a selective agreement loss,"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "can be expressed as:",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "f ) to agree with a\nwhich encourages the fused prediction (µf , log σ2"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "(4)\n(cid:0)[fSP (H′\npinc = fC\ns), fT P (H′\nt)](cid:1) .",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "Gaussian target derived from the unimodal outputs, defined as"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "(cid:34)\n(cid:35)"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "(µk\n2\nagree)2\nf − µk\n(cid:88)"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "2.2.2. Fusion Module",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "+ 1"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ",\n(8)\nLagree =\nagree\n2\n2 log σk"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "2σk\nagree"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "k∈{V,A,D}"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "Our\nfusion module\nis\nan\nend-to-end\narchitecture\nthat\nintegrates",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "speech and text\nfeatures\nto predict VAD emotional dimensions.",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "2\n2"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "2"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "1\n=\nwhere µk\nand σk\n2 . The\nagree = µk"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "Building on the outputs of the speech and text towers, the projected",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "2\nagree\n1/σk\n2+1/σk\n1/σk\n2+1/σk"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "s\ns\nt\nt"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "sequences are passed into a cross-modal fusion module. Inspired by",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "overall\ntraining objective of\nthe fusion tower combines the super-"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "[16] and [17], we design a Transformer block that\njointly models",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "vised and selective terms:"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "intra- and inter-modal dependencies.\nSpecifically, multi-head self-",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "(9)\nLFusion = LNLL + λagree · Lagree."
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "attention (MHSA)\nis applied to capture intra-modal\nrelationships",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "(speech → speech and text → text), while multi-head cross-attention",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "(MHCA)\nenables\ncross-modal\ninteractions\n(speech → text\nand",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "3. EXPERIMENTAL SETUPS"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "text → speech). The outputs are subsequently processed with Lay-",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "erNorm (LN) and feed-forward networks (FFN) to obtain modality-",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "3.1. Datasets"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": ".\nspecific contextual representations fs, ft ∈ RL×D′",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "To dynamically integrate information across modalities, we uti-",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "For Phase A, we use the IEMOCAP [20] dataset\nfor\nthe speech"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "lize the gated multimodal fusion mechanism. Each modality is first",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "tower and the EmoBank [21] dataset\nfor\nthe text\ntower, both with"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "projected through a learnable weight matrix and then normalized via",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "dimensional Valence–Arousal–Dominance (VAD) annotations.\nTo"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "a softmax function applied along the modality axis. This produces",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "ensure label comparability across datasets, we apply a parametric"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "the\nelement-wise gates gs, gt ∈ RT ×1, which adaptively control",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "Beta Cumulative Distribution Function (CDF) transform that maps"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "contribution of each modality at every time step. The final fused rep-",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "each original label v into an aligned value v′ in a shared target distri-"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "is obtained as the weighted combination\nresentation hf ∈ RT ×D′",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "bution. A source value v is first normalized to [0, 1], then mapped to"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "of modality-specific features. The entire process can be formulated",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "its quantile u = Fsrc(v) using the source CDF, and finally aligned"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "as:",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "by applying the target\ninverse CDF, v′ = F −1\ntgt (u). The aligned la-"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": "bels v′ are used as training targets, while during evaluation the model"
        },
        {
          "2.2. Phase B: Inconsistency Detection with Gated Fusion": "f ′\ns = LN(cid:0) ˜Hs + MHSA( ˜Hs) + MHCA( ˜Hs, ˜Ht)(cid:1),",
          "ground-truth label\n[18].\nTo further\nstructure the latent\nspace, we": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Comparison with SOTA methods on emotional inconsis-",
      "data": [
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "dance Correlation Coefficient (CCC).",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": ""
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "Method",
          "Table 2. Comparison with SOTA methods on emotional": "Accuracy",
          "inconsis-": "Recall"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "Ours (Speech Tower)",
          "Table 2. Comparison with SOTA methods on emotional": "85.7",
          "inconsis-": "93.6"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "Ours (Text Tower)",
          "Table 2. Comparison with SOTA methods on emotional": "83.4",
          "inconsis-": "85.0"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "",
          "Table 2. Comparison with SOTA methods on emotional": "92.3",
          "inconsis-": "90.9"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "Dimensional MTL [23]",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": ""
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "Two-stage SVM [24]",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": ""
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "RL-MT [25]",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": ""
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "MFCNN14 [26]",
          "Table 2. Comparison with SOTA methods on emotional": "Table 3. Ablation results for",
          "inconsis-": ""
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "W2v2-b + BERT-b + L [27]",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": ""
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "",
          "Table 2. Comparison with SOTA methods on emotional": "with all metrics reported in CCC.",
          "inconsis-": ""
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "Ours (Fusion Tower)",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": ""
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": "Avg"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": "0.585"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": "0.584"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "3.2.\nImplementation Details",
          "Table 2. Comparison with SOTA methods on emotional": "w/o Attentive Statistics Pooling",
          "inconsis-": "0.612"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "In Phase A, both towers use pretrained backbones, Wav2Vec2-base",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": "0.616"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "for speech and RoBERTa-base for text (hidden size 768), followed",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": ""
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": "0.543"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "by projection layers of dimension 256. Training is performed with a",
          "Table 2. Comparison with SOTA methods on emotional": "w/o Attentive Statistics Pooling",
          "inconsis-": "0.546"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "batch size of 16 for up to 50 epochs with early stopping (patience =",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": ""
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": "0.549"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "5). Optimization uses AdamW with learning rates of 2×10−5 for the",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": ""
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": "0.641"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": "0.625"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": ""
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": "0.657"
        },
        {
          "Table 1. Comparison of unimodal and fusion towers using Concor-": "loss function. We use the concordance correlation coefficient (CCC)",
          "Table 2. Comparison with SOTA methods on emotional": "",
          "inconsis-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Comparison with SOTA methods on emotional inconsis-",
      "data": [
        {
          "MFCNN14 [26]": "W2v2-b + BERT-b + L [27]",
          "0.714": "0.625",
          "0.639": "0.661",
          "0.575": "0.570",
          "0.642": "0.618",
          "Table 3. Ablation results for": "",
          "the speech,": "",
          "text, and fusion towers,": ""
        },
        {
          "MFCNN14 [26]": "",
          "0.714": "",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "with all metrics reported in CCC.",
          "the speech,": "",
          "text, and fusion towers,": ""
        },
        {
          "MFCNN14 [26]": "Ours (Fusion Tower)",
          "0.714": "0.741",
          "0.639": "0.644",
          "0.575": "0.586",
          "0.642": "0.657",
          "Table 3. Ablation results for": "",
          "the speech,": "",
          "text, and fusion towers,": ""
        },
        {
          "MFCNN14 [26]": "",
          "0.714": "",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "Method",
          "the speech,": "V",
          "text, and fusion towers,": "D"
        },
        {
          "MFCNN14 [26]": "",
          "0.714": "",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "w/o Prosody Injection",
          "the speech,": "0.608",
          "text, and fusion towers,": "0.514"
        },
        {
          "MFCNN14 [26]": "",
          "0.714": "",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "w/o Conformer Blocks",
          "the speech,": "0.592",
          "text, and fusion towers,": "0.499"
        },
        {
          "MFCNN14 [26]": "3.2.\nImplementation Details",
          "0.714": "",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "w/o Attentive Statistics Pooling",
          "the speech,": "0.627",
          "text, and fusion towers,": "0.556"
        },
        {
          "MFCNN14 [26]": "In Phase A, both towers use pretrained backbones, Wav2Vec2-base",
          "0.714": "",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "Ours (Speech Tower)",
          "the speech,": "0.639",
          "text, and fusion towers,": "0.541"
        },
        {
          "MFCNN14 [26]": "for speech and RoBERTa-base for text (hidden size 768), followed",
          "0.714": "",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "",
          "the speech,": "",
          "text, and fusion towers,": ""
        },
        {
          "MFCNN14 [26]": "",
          "0.714": "",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "w/o Affect Prior Gating",
          "the speech,": "0.776",
          "text, and fusion towers,": "0.406"
        },
        {
          "MFCNN14 [26]": "by projection layers of dimension 256. Training is performed with a",
          "0.714": "",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "w/o Attentive Statistics Pooling",
          "the speech,": "0.778",
          "text, and fusion towers,": "0.435"
        },
        {
          "MFCNN14 [26]": "batch size of 16 for up to 50 epochs with early stopping (patience =",
          "0.714": "",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "",
          "the speech,": "",
          "text, and fusion towers,": ""
        },
        {
          "MFCNN14 [26]": "",
          "0.714": "",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "Ours (Text Tower)",
          "the speech,": "0.784",
          "text, and fusion towers,": "0.443"
        },
        {
          "MFCNN14 [26]": "5). Optimization uses AdamW with learning rates of 2×10−5 for the",
          "0.714": "",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "",
          "the speech,": "",
          "text, and fusion towers,": ""
        },
        {
          "MFCNN14 [26]": "",
          "0.714": "backbone and 1×10−4 for the heads, combined with a cosine sched-",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "w/o Transformer Block",
          "the speech,": "0.706",
          "text, and fusion towers,": "0.554"
        },
        {
          "MFCNN14 [26]": "",
          "0.714": "ule and 10% warm-up. Weight decay is set",
          "0.639": "",
          "0.575": "to 0.01. The minimum",
          "0.642": "",
          "Table 3. Ablation results for": "w/o Gated Multimodal Fusion",
          "the speech,": "0.720",
          "text, and fusion towers,": "0.534"
        },
        {
          "MFCNN14 [26]": "",
          "0.714": "variance of 2 × 10−3 is used by the heteroscedastic Gaussian NLL",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "",
          "the speech,": "",
          "text, and fusion towers,": ""
        },
        {
          "MFCNN14 [26]": "",
          "0.714": "",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "Ours (Fusion Tower)",
          "the speech,": "0.741",
          "text, and fusion towers,": "0.586"
        },
        {
          "MFCNN14 [26]": "loss function. We use the concordance correlation coefficient (CCC)",
          "0.714": "",
          "0.639": "",
          "0.575": "",
          "0.642": "",
          "Table 3. Ablation results for": "",
          "the speech,": "",
          "text, and fusion towers,": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Comparison with SOTA methods on emotional inconsis-",
      "data": [
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "variance of 2 × 10−3 is used by the heteroscedastic Gaussian NLL"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "loss function. We use the concordance correlation coefficient (CCC)"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "as the evaluation metric. To avoid data leakage, we use a speaker-"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "independent split for IEMOCAP. All utterances from each of the ten"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "speakers go to a single partition with an 8/1/1 train/validation/test"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "ratio via group-based splitting. For EmoBank, we follow the official"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "train/validation/test split annotated in the corpus."
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "For Phase B inconsistency detection, both the speech and text"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "towers are kept frozen, and optimization is performed solely on the"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "classifier head.\nEach pair of data forms the input after modality-"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "specific linear projections\nto a 256-dimensional\nspace. We use a"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "batch size of 32 and train for up to 50 epochs with early stopping"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "(patience = 5). Optimization uses AdamW on classifier parameters,"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "with a learning rate of 1 × 10−3 and weight decay 0.01. The loss"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "combines binary cross-entropy and a margin term (margin m = 0.9,"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "λ = 0.15).\nFor\nfusion tower, we keep the batch size at 16 and"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "train for up to 50 epochs. Optimization uses AdamW with learn-"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "ing rate 1 × 10−4, weight decay 0.01, and a cosine schedule with"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "10% warm-up. We use CCC as the evaluation metric. For data split,"
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": ""
        },
        {
          "ule and 10% warm-up. Weight decay is set\nto 0.01. The minimum": "we use the same speaker-independent split as in Phase A for IEMO-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "automatic lyric transcription system,”\nin Proc. 30th ACM In-"
        },
        {
          "6. REFERENCES": "[1] Y. Xu, X. Jiang, and D. Wu,\n“Cross-task inconsistency based",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "ternational Conference on Multimedia, 2022, pp. 3328–3337."
        },
        {
          "6. REFERENCES": "active learning (CTIAL) for emotion recognition,” IEEE Trans.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[17] H. Ma, J. Wang, H. Lin, B. Zhang, Y. Zhang, and B. Xu,\n“A"
        },
        {
          "6. REFERENCES": "Affective Comput., vol. 15, no. 3, pp. 1659–1668, 2024.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "transformer-based model with self-distillation for multimodal"
        },
        {
          "6. REFERENCES": "[2] R. Su, C. Xu, X. Wu, F. Xu, X. Chen, L. Wang, and N. Yan,",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "IEEE Transactions on\nemotion recognition in conversations,”"
        },
        {
          "6. REFERENCES": "“Investigating acoustic-textual emotional\ninconsistency infor-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Multimedia, vol. 26, pp. 776–788, Feb. 2024."
        },
        {
          "6. REFERENCES": "arXiv preprint\nmation for automatic depression detection,”",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[18]\nJ. J. Hopfield,\n“Neural networks and physical systems with"
        },
        {
          "6. REFERENCES": "arXiv:2412.18614, 2024.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Proceedings of\nemergent collective computational abilities,”"
        },
        {
          "6. REFERENCES": "[3] X. Wang, M. Li, Y. Chang, X. Luo, Y. Yao, and Z. Li,\n“Mul-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "the National Academy of Sciences, vol. 79, no. 8, pp. 2554–"
        },
        {
          "6. REFERENCES": "timodal cross-attention bayesian network for social news emo-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "2558, 1982."
        },
        {
          "6. REFERENCES": "tion recognition,” in Proc. 2023 International Joint Conference",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[19] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduc-"
        },
        {
          "6. REFERENCES": "on Neural Networks (IJCNN), 2023, pp. 1–9.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "tion by learning an invariant mapping,”\nin Proc. 2006 IEEE"
        },
        {
          "6. REFERENCES": "[4] Y. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.P. Morency, and",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Computer Society Conference on Computer Vision and Pattern"
        },
        {
          "6. REFERENCES": "R. Salakhutdinov, “Multimodal transformer for unaligned mul-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Recognition (CVPR’06), 2006, vol. 2, pp. 1735–1742."
        },
        {
          "6. REFERENCES": "timodal\nlanguage sequences,”\nin Proc. 57th Annual Meeting",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[20] C. Busso, M. Bulut, C.C. Lee, A. Kazemzadeh, E. Mower,"
        },
        {
          "6. REFERENCES": "of\nthe Association for Computational Linguistics (ACL), Flo-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan,\n“IEMO-"
        },
        {
          "6. REFERENCES": "rence, Italy, Jul. 2019, pp. 6558–6569.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "CAP:\nInteractive emotional dyadic motion capture database,”"
        },
        {
          "6. REFERENCES": "[5]\nS. Pramanick, A. Roy, and V. M. Patel,\n“Multimodal\nlearn-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Language Resources and Evaluation, vol. 42, no. 4, pp. 335–"
        },
        {
          "6. REFERENCES": "ing using optimal transport for sarcasm and humor detection,”",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "359, Dec. 2008."
        },
        {
          "6. REFERENCES": "IEEE/CVF Winter Conference on Applications of\nin In Proc.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[21]\nS. Buechel and U. Hahn,\n“EmoBank:\nStudying the impact"
        },
        {
          "6. REFERENCES": "Computer Vision (WACV), 2022, pp. 3930–3940.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "of annotation perspective and representation format on dimen-"
        },
        {
          "6. REFERENCES": "[6] Y. Wang, Y. Li, P. P. Liang, L.P. Morency, P. Bell, and C. Lai,",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "sional emotion analysis,” in Proc. 15th Conference of the Euro-"
        },
        {
          "6. REFERENCES": "“Cross-attention is not enough: Incongruity-aware dynamic hi-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "pean Chapter of the Association for Computational Linguistics"
        },
        {
          "6. REFERENCES": "arXiv\nerarchical\nfusion for multimodal affect\nrecognition,”",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "(EACL 2017), Volume 2: Short Papers, Valencia, Spain, Apr."
        },
        {
          "6. REFERENCES": "preprint arXiv:2305.13583, 2023.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "2017, pp. 578–585."
        },
        {
          "6. REFERENCES": "[7] H. Pan, Z. Lin, P. Fu, Y. Qi, and W. Wang,\n“Modeling intra",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[22] A. Adigwe, N. Tits, K. El Haddad, S. Ostadabbas, and T. Du-"
        },
        {
          "6. REFERENCES": "and inter-modality incongruity for multi-modal sarcasm detec-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "toit,\n“The emotional voices database:\nTowards controlling"
        },
        {
          "6. REFERENCES": "the Association for Computational\ntion,”\nin Proc. Findings of",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "arXiv\nthe emotion dimension in voice generation systems,”"
        },
        {
          "6. REFERENCES": "Linguistics: EMNLP 2020, Nov. 2020, pp. 1383–1392.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "preprint arXiv:1806.09514, 2018."
        },
        {
          "6. REFERENCES": "[8] A. Baevski, H. Zhou, A. Mohamed, and M. Auli,\n“wav2vec",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[23] B. T. Atmaja and M. Akagi,\n“Dimensional speech emotion"
        },
        {
          "6. REFERENCES": "2.0: A framework for self-supervised learning of speech repre-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "recognition from speech features and word embeddings by us-"
        },
        {
          "6. REFERENCES": "sentations,” in Proc. Advances in Neural Information Process-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "APSIPA Transactions on Signal and\ning multitask learning,”"
        },
        {
          "6. REFERENCES": "ing Systems (NeurIPS), 2020, vol. 33, pp. 12449–12460.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Information Processing, vol. 9, no. 1, pp. e17, Jul. 2020."
        },
        {
          "6. REFERENCES": "[9]\nJ. Zhao, X. Wang, and Y. Wang,\n“Prosody-Adaptable Audio",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[24] B. T. Atmaja and M. Akagi, “Two stage dimensional emotion"
        },
        {
          "6. REFERENCES": "Codecs for Zero-Shot Voice Conversion via In-Context Learn-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "recognition by fusing predictions of acoustic and text networks"
        },
        {
          "6. REFERENCES": "ing,” in Interspeech 2025, 2025, pp. 4893–4897.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "using svm,” Speech Communication, vol. 126, pp. 9–21, 2021."
        },
        {
          "6. REFERENCES": "[10] A. Gulati,\nJ. Qin, C.C. Chiu, N. Parmar, Y. Zhang,\nJ. Yu,",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[25]\nS. Srinivasan, Z. Huang,\nand K. Kirchhoff,\n“Representa-"
        },
        {
          "6. REFERENCES": "W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, “Conformer:",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "tion learning through cross-modal conditional\nteacher-student"
        },
        {
          "6. REFERENCES": "Convolution-augmented transformer\nfor\nspeech recognition,”",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "training for speech emotion recognition,”\nin Proc. 2022 IEEE"
        },
        {
          "6. REFERENCES": "in Interspeech 2020, 2020, pp. 5036–5040.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "International Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "6. REFERENCES": "[11] K. Okabe, T. Koshinaka, and K. Shinoda,\n“Attentive statistics",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "cessing (ICASSP), May 2022, pp. 6442–6446."
        },
        {
          "6. REFERENCES": "Interspeech\npooling for deep speaker embedding,”\nin Proc.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[26] A. Triantafyllopoulos, U. Reichel, S. Liu, S. Huber, F. Eyben,"
        },
        {
          "6. REFERENCES": "2018, 2018, pp. 2252–2256.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "and B. W. Schuller, “Multistage linguistic conditioning of con-"
        },
        {
          "6. REFERENCES": "[12] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "volutional layers for speech emotion recognition,” Frontiers in"
        },
        {
          "6. REFERENCES": "M. Lewis, L. Zettlemoyer, and V. Stoyanov, “RoBERTa: A ro-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Computer Science, vol. 5, pp. 1072479, 2023."
        },
        {
          "6. REFERENCES": "bustly optimized BERT pretraining approach,” arXiv preprint",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[27] E. Zhang, R. Trujillo,\nand C. Poellabauer,\n“The MERSA"
        },
        {
          "6. REFERENCES": "arXiv:1907.11692, 2019.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "dataset and a transformer-based approach for speech emotion"
        },
        {
          "6. REFERENCES": "[13]\nS. M. Mohammad, “NRC VAD lexicon v2: Norms for valence,",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "the Associa-\nrecognition,”\nin Proc. 62nd Annual Meeting of"
        },
        {
          "6. REFERENCES": "arXiv\narousal, and dominance for over 55k English terms,”",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "tion for Computational Linguistics (Volume 1: Long Papers),"
        },
        {
          "6. REFERENCES": "preprint arXiv:2503.23547, 2025.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Bangkok, Thailand, Aug. 2024, pp. 13960–13970."
        },
        {
          "6. REFERENCES": "[14] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville,",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[28]\nS. Castro, D. Hazarika, V. P´erez-Rosas, R. Zimmermann,"
        },
        {
          "6. REFERENCES": "“FiLM: Visual reasoning with a general conditioning layer,” in",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "R. Mihalcea,\nand S. Poria,\n“Towards multimodal\nsarcasm"
        },
        {
          "6. REFERENCES": "Proc. AAAI Conf. Artificial Intelligence, 2018, vol. 32.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "detection (an obviously perfect paper),”\nin Proc. 57th An-"
        },
        {
          "6. REFERENCES": "[15] A. Kendall and Y. Gal,\n“What uncertainties do we need in",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "nual Meeting of the Association for Computational Linguistics"
        },
        {
          "6. REFERENCES": "bayesian deep learning for computer vision?,”\nin Proc. Ad-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "(ACL), Florence, Italy, 2019, pp. 4619–4629."
        },
        {
          "6. REFERENCES": "vances in Neural\nInformation Processing Systems (NeurIPS),",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "automatic lyric transcription system,”\nin Proc. 30th ACM In-"
        },
        {
          "6. REFERENCES": "[1] Y. Xu, X. Jiang, and D. Wu,\n“Cross-task inconsistency based",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "ternational Conference on Multimedia, 2022, pp. 3328–3337."
        },
        {
          "6. REFERENCES": "active learning (CTIAL) for emotion recognition,” IEEE Trans.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[17] H. Ma, J. Wang, H. Lin, B. Zhang, Y. Zhang, and B. Xu,\n“A"
        },
        {
          "6. REFERENCES": "Affective Comput., vol. 15, no. 3, pp. 1659–1668, 2024.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "transformer-based model with self-distillation for multimodal"
        },
        {
          "6. REFERENCES": "[2] R. Su, C. Xu, X. Wu, F. Xu, X. Chen, L. Wang, and N. Yan,",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "IEEE Transactions on\nemotion recognition in conversations,”"
        },
        {
          "6. REFERENCES": "“Investigating acoustic-textual emotional\ninconsistency infor-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Multimedia, vol. 26, pp. 776–788, Feb. 2024."
        },
        {
          "6. REFERENCES": "arXiv preprint\nmation for automatic depression detection,”",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[18]\nJ. J. Hopfield,\n“Neural networks and physical systems with"
        },
        {
          "6. REFERENCES": "arXiv:2412.18614, 2024.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Proceedings of\nemergent collective computational abilities,”"
        },
        {
          "6. REFERENCES": "[3] X. Wang, M. Li, Y. Chang, X. Luo, Y. Yao, and Z. Li,\n“Mul-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "the National Academy of Sciences, vol. 79, no. 8, pp. 2554–"
        },
        {
          "6. REFERENCES": "timodal cross-attention bayesian network for social news emo-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "2558, 1982."
        },
        {
          "6. REFERENCES": "tion recognition,” in Proc. 2023 International Joint Conference",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[19] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduc-"
        },
        {
          "6. REFERENCES": "on Neural Networks (IJCNN), 2023, pp. 1–9.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "tion by learning an invariant mapping,”\nin Proc. 2006 IEEE"
        },
        {
          "6. REFERENCES": "[4] Y. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.P. Morency, and",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Computer Society Conference on Computer Vision and Pattern"
        },
        {
          "6. REFERENCES": "R. Salakhutdinov, “Multimodal transformer for unaligned mul-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Recognition (CVPR’06), 2006, vol. 2, pp. 1735–1742."
        },
        {
          "6. REFERENCES": "timodal\nlanguage sequences,”\nin Proc. 57th Annual Meeting",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[20] C. Busso, M. Bulut, C.C. Lee, A. Kazemzadeh, E. Mower,"
        },
        {
          "6. REFERENCES": "of\nthe Association for Computational Linguistics (ACL), Flo-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan,\n“IEMO-"
        },
        {
          "6. REFERENCES": "rence, Italy, Jul. 2019, pp. 6558–6569.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "CAP:\nInteractive emotional dyadic motion capture database,”"
        },
        {
          "6. REFERENCES": "[5]\nS. Pramanick, A. Roy, and V. M. Patel,\n“Multimodal\nlearn-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Language Resources and Evaluation, vol. 42, no. 4, pp. 335–"
        },
        {
          "6. REFERENCES": "ing using optimal transport for sarcasm and humor detection,”",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "359, Dec. 2008."
        },
        {
          "6. REFERENCES": "IEEE/CVF Winter Conference on Applications of\nin In Proc.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[21]\nS. Buechel and U. Hahn,\n“EmoBank:\nStudying the impact"
        },
        {
          "6. REFERENCES": "Computer Vision (WACV), 2022, pp. 3930–3940.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "of annotation perspective and representation format on dimen-"
        },
        {
          "6. REFERENCES": "[6] Y. Wang, Y. Li, P. P. Liang, L.P. Morency, P. Bell, and C. Lai,",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "sional emotion analysis,” in Proc. 15th Conference of the Euro-"
        },
        {
          "6. REFERENCES": "“Cross-attention is not enough: Incongruity-aware dynamic hi-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "pean Chapter of the Association for Computational Linguistics"
        },
        {
          "6. REFERENCES": "arXiv\nerarchical\nfusion for multimodal affect\nrecognition,”",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "(EACL 2017), Volume 2: Short Papers, Valencia, Spain, Apr."
        },
        {
          "6. REFERENCES": "preprint arXiv:2305.13583, 2023.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "2017, pp. 578–585."
        },
        {
          "6. REFERENCES": "[7] H. Pan, Z. Lin, P. Fu, Y. Qi, and W. Wang,\n“Modeling intra",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[22] A. Adigwe, N. Tits, K. El Haddad, S. Ostadabbas, and T. Du-"
        },
        {
          "6. REFERENCES": "and inter-modality incongruity for multi-modal sarcasm detec-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "toit,\n“The emotional voices database:\nTowards controlling"
        },
        {
          "6. REFERENCES": "the Association for Computational\ntion,”\nin Proc. Findings of",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "arXiv\nthe emotion dimension in voice generation systems,”"
        },
        {
          "6. REFERENCES": "Linguistics: EMNLP 2020, Nov. 2020, pp. 1383–1392.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "preprint arXiv:1806.09514, 2018."
        },
        {
          "6. REFERENCES": "[8] A. Baevski, H. Zhou, A. Mohamed, and M. Auli,\n“wav2vec",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[23] B. T. Atmaja and M. Akagi,\n“Dimensional speech emotion"
        },
        {
          "6. REFERENCES": "2.0: A framework for self-supervised learning of speech repre-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "recognition from speech features and word embeddings by us-"
        },
        {
          "6. REFERENCES": "sentations,” in Proc. Advances in Neural Information Process-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "APSIPA Transactions on Signal and\ning multitask learning,”"
        },
        {
          "6. REFERENCES": "ing Systems (NeurIPS), 2020, vol. 33, pp. 12449–12460.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Information Processing, vol. 9, no. 1, pp. e17, Jul. 2020."
        },
        {
          "6. REFERENCES": "[9]\nJ. Zhao, X. Wang, and Y. Wang,\n“Prosody-Adaptable Audio",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[24] B. T. Atmaja and M. Akagi, “Two stage dimensional emotion"
        },
        {
          "6. REFERENCES": "Codecs for Zero-Shot Voice Conversion via In-Context Learn-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "recognition by fusing predictions of acoustic and text networks"
        },
        {
          "6. REFERENCES": "ing,” in Interspeech 2025, 2025, pp. 4893–4897.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "using svm,” Speech Communication, vol. 126, pp. 9–21, 2021."
        },
        {
          "6. REFERENCES": "[10] A. Gulati,\nJ. Qin, C.C. Chiu, N. Parmar, Y. Zhang,\nJ. Yu,",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[25]\nS. Srinivasan, Z. Huang,\nand K. Kirchhoff,\n“Representa-"
        },
        {
          "6. REFERENCES": "W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, “Conformer:",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "tion learning through cross-modal conditional\nteacher-student"
        },
        {
          "6. REFERENCES": "Convolution-augmented transformer\nfor\nspeech recognition,”",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "training for speech emotion recognition,”\nin Proc. 2022 IEEE"
        },
        {
          "6. REFERENCES": "in Interspeech 2020, 2020, pp. 5036–5040.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "International Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "6. REFERENCES": "[11] K. Okabe, T. Koshinaka, and K. Shinoda,\n“Attentive statistics",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "cessing (ICASSP), May 2022, pp. 6442–6446."
        },
        {
          "6. REFERENCES": "Interspeech\npooling for deep speaker embedding,”\nin Proc.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[26] A. Triantafyllopoulos, U. Reichel, S. Liu, S. Huber, F. Eyben,"
        },
        {
          "6. REFERENCES": "2018, 2018, pp. 2252–2256.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "and B. W. Schuller, “Multistage linguistic conditioning of con-"
        },
        {
          "6. REFERENCES": "[12] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "volutional layers for speech emotion recognition,” Frontiers in"
        },
        {
          "6. REFERENCES": "M. Lewis, L. Zettlemoyer, and V. Stoyanov, “RoBERTa: A ro-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Computer Science, vol. 5, pp. 1072479, 2023."
        },
        {
          "6. REFERENCES": "bustly optimized BERT pretraining approach,” arXiv preprint",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[27] E. Zhang, R. Trujillo,\nand C. Poellabauer,\n“The MERSA"
        },
        {
          "6. REFERENCES": "arXiv:1907.11692, 2019.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "dataset and a transformer-based approach for speech emotion"
        },
        {
          "6. REFERENCES": "[13]\nS. M. Mohammad, “NRC VAD lexicon v2: Norms for valence,",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "the Associa-\nrecognition,”\nin Proc. 62nd Annual Meeting of"
        },
        {
          "6. REFERENCES": "arXiv\narousal, and dominance for over 55k English terms,”",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "tion for Computational Linguistics (Volume 1: Long Papers),"
        },
        {
          "6. REFERENCES": "preprint arXiv:2503.23547, 2025.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "Bangkok, Thailand, Aug. 2024, pp. 13960–13970."
        },
        {
          "6. REFERENCES": "[14] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville,",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "[28]\nS. Castro, D. Hazarika, V. P´erez-Rosas, R. Zimmermann,"
        },
        {
          "6. REFERENCES": "“FiLM: Visual reasoning with a general conditioning layer,” in",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "R. Mihalcea,\nand S. Poria,\n“Towards multimodal\nsarcasm"
        },
        {
          "6. REFERENCES": "Proc. AAAI Conf. Artificial Intelligence, 2018, vol. 32.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "detection (an obviously perfect paper),”\nin Proc. 57th An-"
        },
        {
          "6. REFERENCES": "[15] A. Kendall and Y. Gal,\n“What uncertainties do we need in",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "nual Meeting of the Association for Computational Linguistics"
        },
        {
          "6. REFERENCES": "bayesian deep learning for computer vision?,”\nin Proc. Ad-",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": "(ACL), Florence, Italy, 2019, pp. 4619–4629."
        },
        {
          "6. REFERENCES": "vances in Neural\nInformation Processing Systems (NeurIPS),",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        },
        {
          "6. REFERENCES": "2017, vol. 30, pp. 5574–5584.",
          "[16] X. Gu, L. Ou, D. Ong, and Y. Wang, “MM-ALT: A multimodal": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Cross-task inconsistency based active learning (CTIAL) for emotion recognition",
      "authors": [
        "Y Xu",
        "X Jiang",
        "D Wu"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "3",
      "title": "Investigating acoustic-textual emotional inconsistency information for automatic depression detection",
      "authors": [
        "R Su",
        "C Xu",
        "X Wu",
        "F Xu",
        "X Chen",
        "L Wang",
        "N Yan"
      ],
      "year": "2024",
      "venue": "Investigating acoustic-textual emotional inconsistency information for automatic depression detection",
      "arxiv": "arXiv:2412.18614"
    },
    {
      "citation_id": "4",
      "title": "Multimodal cross-attention bayesian network for social news emotion recognition",
      "authors": [
        "X Wang",
        "M Li",
        "Y Chang",
        "X Luo",
        "Y Yao",
        "Z Li"
      ],
      "year": "2023",
      "venue": "Proc. 2023 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "5",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proc. 57th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "6",
      "title": "Multimodal learning using optimal transport for sarcasm and humor detection",
      "authors": [
        "S Pramanick",
        "A Roy",
        "V Patel"
      ],
      "year": "2022",
      "venue": "Proc. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "7",
      "title": "Cross-attention is not enough: Incongruity-aware dynamic hierarchical fusion for multimodal affect recognition",
      "authors": [
        "Y Wang",
        "Y Li",
        "P Liang",
        "L Morency",
        "P Bell",
        "C Lai"
      ],
      "year": "2023",
      "venue": "Cross-attention is not enough: Incongruity-aware dynamic hierarchical fusion for multimodal affect recognition",
      "arxiv": "arXiv:2305.13583"
    },
    {
      "citation_id": "8",
      "title": "Modeling intra and inter-modality incongruity for multi-modal sarcasm detection",
      "authors": [
        "H Pan",
        "Z Lin",
        "P Fu",
        "Y Qi",
        "W Wang"
      ],
      "year": "2020",
      "venue": "Proc. Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "9",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proc. Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "10",
      "title": "Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via In-Context Learning",
      "authors": [
        "J Zhao",
        "X Wang",
        "Y Wang"
      ],
      "year": "2025",
      "venue": "Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via In-Context Learning"
    },
    {
      "citation_id": "11",
      "title": "Conformer: Convolution-augmented transformer for speech recognition",
      "authors": [
        "A Gulati",
        "J Qin",
        "C Chiu",
        "N Parmar",
        "Y Zhang",
        "J Yu",
        "W Han",
        "S Wang",
        "Z Zhang",
        "Y Wu",
        "R Pang"
      ],
      "venue": "Conformer: Convolution-augmented transformer for speech recognition"
    },
    {
      "citation_id": "12",
      "title": "Attentive statistics pooling for deep speaker embedding",
      "authors": [
        "K Okabe",
        "T Koshinaka",
        "K Shinoda"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "13",
      "title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A robustly optimized BERT pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "14",
      "title": "NRC VAD lexicon v2: Norms for valence, arousal, and dominance for over 55k English terms",
      "authors": [
        "S Mohammad"
      ],
      "year": "2025",
      "venue": "NRC VAD lexicon v2: Norms for valence, arousal, and dominance for over 55k English terms",
      "arxiv": "arXiv:2503.23547"
    },
    {
      "citation_id": "15",
      "title": "FiLM: Visual reasoning with a general conditioning layer",
      "authors": [
        "E Perez",
        "F Strub",
        "H Vries",
        "V Dumoulin",
        "A Courville"
      ],
      "year": "2018",
      "venue": "Proc. AAAI Conf. Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "What uncertainties do we need in bayesian deep learning for computer vision?",
      "authors": [
        "A Kendall",
        "Y Gal"
      ],
      "year": "2017",
      "venue": "Proc. Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "17",
      "title": "MM-ALT: A multimodal automatic lyric transcription system",
      "authors": [
        "X Gu",
        "L Ou",
        "D Ong",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Proc. 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "B Zhang",
        "Y Zhang",
        "B Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Neural networks and physical systems with emergent collective computational abilities",
      "authors": [
        "J Hopfield"
      ],
      "year": "1982",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "20",
      "title": "Dimensionality reduction by learning an invariant mapping",
      "authors": [
        "R Hadsell",
        "S Chopra",
        "Y Lecun"
      ],
      "year": "2006",
      "venue": "Proc. 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
    },
    {
      "citation_id": "21",
      "title": "IEMO-CAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "22",
      "title": "EmoBank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis",
      "authors": [
        "S Buechel",
        "U Hahn"
      ],
      "year": "2017",
      "venue": "Proc. 15th Conference of the European Chapter"
    },
    {
      "citation_id": "23",
      "title": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "authors": [
        "A Adigwe",
        "N Tits",
        "K Haddad",
        "S Ostadabbas",
        "T Dutoit"
      ],
      "year": "2018",
      "venue": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "arxiv": "arXiv:1806.09514"
    },
    {
      "citation_id": "24",
      "title": "Dimensional speech emotion recognition from speech features and word embeddings by using multitask learning",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "APSIPA Transactions on Signal and Information Processing"
    },
    {
      "citation_id": "25",
      "title": "Two stage dimensional emotion recognition by fusing predictions of acoustic and text networks using svm",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2021",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "26",
      "title": "Representation learning through cross-modal conditional teacher-student training for speech emotion recognition",
      "authors": [
        "S Srinivasan",
        "Z Huang",
        "K Kirchhoff"
      ],
      "year": "2022",
      "venue": "Proc. 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "27",
      "title": "Multistage linguistic conditioning of convolutional layers for speech emotion recognition",
      "authors": [
        "A Triantafyllopoulos",
        "U Reichel",
        "S Liu",
        "S Huber",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "28",
      "title": "The MERSA dataset and a transformer-based approach for speech emotion recognition",
      "authors": [
        "E Zhang",
        "R Trujillo",
        "C Poellabauer"
      ],
      "year": "2024",
      "venue": "Proc. 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "29",
      "title": "Towards multimodal sarcasm detection (an obviously perfect paper)",
      "authors": [
        "S Castro",
        "D Hazarika",
        "V Pérez-Rosas",
        "R Zimmermann",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2019",
      "venue": "Proc. 57th Annual Meeting of the Association for Computational Linguistics (ACL)"
    }
  ]
}