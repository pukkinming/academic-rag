{
  "paper_id": "2003.08026v2",
  "title": "A Unified Multi-Scale And Multi-Task Learning Framework For Driver Behaviors Reasoning",
  "published": "2020-03-18T03:13:04Z",
  "authors": [
    "Yang Xing",
    "Chen Lv",
    "Dongpu Cao",
    "Efstathios Velenis"
  ],
  "keywords": [
    "Intelligent vehicle",
    "multi-scale driver behaviors",
    "mutual understanding",
    "deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Mutual understanding between driver and vehicle is critically important to the design of intelligent vehicles and customized interaction interface. In this study, a unified driver behavior reasoning system toward multi-scale and multi-tasks behavior recognition is proposed. Specifically, a multi-scale driver behavior recognition system is designed to recognize both the driver's physical and mental states based on a deep encoder-decoder framework. This system can jointly recognize three driver behaviors with different time scales based on the shared encoder network. Driver body postures and mental behaviors include intention and emotion are studied and identified. The encoder network is designed based on a deep convolutional neural network (CNN), and several decoders for different driver states estimation are proposed with fully connected (FC) and long short-term memory (LSTM) based recurrent neural networks (RNN). The joint feature learning with the CNN encoder increases the computational efficiency and feature diversity, while the customized decoders enable an efficient multi-tasks inference. The proposed framework can be used as a solution to exploit the relationship between different driver states, and it is found that when drivers generate lane change intentions, their emotions usually keep neutral state and more focus on the task. Two naturalistic datasets are used to investigate the model performance, which is a local highway dataset, namely, CranData and one public dataset from Brain4Cars. The testing results on these two datasets show accurate performance and outperform existing methods on driver postures, intention, and emotion recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Motivation",
      "text": "riving is a complex task for the human driver, which usually require various adjustment in the physical, emotional, cognitive, and psychological aspects. Drivers should efficiently interact with other road entities based on their context perception, decision-making, and control action to the vehicle. With the development of intelligent and autonomous vehicles, a common agreement has been made that human drivers can be a good teacher to the intelligent agents in general  [1] . Hence, a comprehensive analysis of human drivers and learning driving patterns and styles from drivers can benefit the human-centered intelligent system design  [2] . As drivers are sharing their vehicle control authorities with the intelligent units, conflict can occur if the driver and vehicle cannot well understand with each other. Therefore, it is critical to design efficient driver-vehicle collaboration and shared control strategies based on driving behavior prediction and driver mental state inference  [3, 4] . Moreover, mutual-understanding enabled intelligent vehicles are much easier to be accepted by the public as human drivers/passengers can feel they are well-considered and understood so that to be confident with the intelligent vehicles  [5] .\n\nDriver behaviors recognition is a wide scope of research and has been widely studied in the past two decades. Driver behaviors can be divided into multiple scales, from seconds-level activities and cognitive process to hours or days-level of driving styles, skills, and habits. Among these, a large number of studies focus on the in-vehicle driver states estimation, such as driver attention, driver intention, emotion, and fatigue, etc., which are essential to driving safety issues  [6, 7] . In the past, it is not well studied how to understand multi-scales driver behaviors uniformly and how these behaviors influence each other. Existing studies mainly focus on a specific topic to understand and model the driver from a single aspect. As a result, it would be challenging to integrate so many different functional modules with varying techniques into a mutual understanding system on intelligent vehicles.\n\nTherefore, in this study, a driver behavior reasoning system is proposed based on a unified framework to increase the scalability, accuracy, and efficiency of the system. The unified driver behavior recognition system can estimate multi-scale driver behaviors at both a physical-level and mental-levels. The definition of multi-scale driver behaviors can be found in  [8] , where three different behaviors were identified, namely, strategical behavior, tactical behavior, and operational behavior. The time scales for the three kinds of behaviors are in descending order. The driver's physical behaviors, such as mirror checking and facial expressions, are at",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Literature Review",
      "text": "Driver intention inference is a process to anticipate the near-future driving maneuvers based on the reasoning of driver physical behaviors, traffic context, and vehicle states. To infer the driver's intention, vision-based methods are the most efficient categories as the driver's intention is closely related to driver behavior recognition, such as mirror checking detection before the maneuver  [9] . In  [10] , the authors developed a hidden Markov model (HMM) to model the probabilistic transitions of the mental states and predict the lane change intent on a driving simulator. Then, in  [11] , a sparse Bayesian learning network was developed to predict the lane change intent with naturalistic data. In  [12] , the impact of head movement and gaze movement on the intention inference was analyzed. It was found that the head movement before the maneuver was the most important clue for the intention estimation. At the same time, eye gaze signals did not significantly improve the model performance. In  [13] , a lane change intention prediction system considering different driving styles was proposed based on the Bayesian network (BN) and Gaussian mixture model (GMM). It showed that by integrating driving styles and traffic context, the lane change intent could be predicted 4.5 s ahead of the maneuver with 78.2% accuracy. As driver intention inference requires the modeling of temporal driver behavior dynamics; recently, some researchers applied deep learning models to solve the task. In  [14] , a driver intention and path prediction system for urban intersection driving behavior modeling was proposed based on the LSTM-RNN. Similarly, in  [15] , an LSTM-RNN framework was applied to anticipate the lane change and turn maneuvers based on the multi-modal data. The model can predict the lane change maneuver 3.42 s before it happens with a precision and recall of 88% and 86%, respectively. Although reasonable results have achieved in the past, most of the existing methods rely on a hand-craft feature engineering to extract features, which lacks objective evaluation and normally requires extra modules and functions such as head pose and eye gaze estimation to extract the features.\n\nRegarding driver emotion analysis, despite the neutral state, seven universal human emotions can be reflected by the facial movements, which are anger, fear, disgust, sadness, surprise, contempt, and happiness  [16] . It was shown that both the positive emotional stimuli (happy) and negative emotional stimuli (fear, anger, etc.) could worsen the driving performance on lane-keeping, traffic role violations, and aggressive driving  [17] . Therefore, it is necessary to recognize the driver's emotions to provide proper alarms, assistant, and shared controls for driving safety issues. Driver emotion recognition usually can be detected with the multi-modal signal, which can be grouped into three primary information, which are facial expression, speech, and electroencephalogram (EEG) and other physiological signals  [18] -  [21] . For example, in  [22] , an emotion recognition system with the inner cabin voice signal was proposed based on the prosodic and spectral features extraction. The authors in  [23]  argue that the visual and audio signals can be less effective in the advanced driver assistance system (ADAS) due to the facial expression can be faked, and the audio signals may not always available. Hence, a subject independent emotion recognition system based on the electrodermal activity (EDA), skin temperature (ST), an electrocardiogram (ECG) signals were proposed. In fact, the facial expression is the most informative interpersonal communication channel that carries about two-thirds of the total communication information  [24] . Methodologies for facial emotion recognition (FER) can be roughly divided into two categorical, which are conventional methods based on feature extraction  [25, 26]  and end-to-end deep learning-based methods  [27] -  [31] . The deep learning-based FER has achieved state-of-are results on many public datasets. However, one of the drawbacks of these systems is the lack of temporal dependency analysis during a long-term driving process. Specifically, the emotional states are not transient and should not be classified based on a single image during the driving task. Within a certain period, the variation of the driver's emotion is not that fast, and sometimes emotion recognition needs to be made based on the previous context and driver states.\n\nIn this study, a combined CNN-RNN model is developed to recognize these driver state uniformly. The CNN-RNN network has become a powerful model for sequential image processing  [32, 33] . In  [34] , A long-term recurrent convolutional network (LRCN) was proposed. The LRCN can map variable-length input to variable-length output and capture complex temporal dynamics. In  [35] , a driver hand gesture prediction model was proposed based on the combined LSTM and CNN network to detect the low latency gestures. In  [36] , a convolutional LSTM (ConvLSTM) was built for precipitation nowcasting by extending the fully connected LSTM to a convolutional structure. In this study, a multi-task framework is integrated into the CNN-RNN network to process the multi-scale driver behavior recognition task. Multi-task learning aims to leverage common representation and useful information in several related tasks to enhance the generalization performance on multiple tasks  [37]   [38] . Currently, many studies show the multi-task learning paradigm can achieve compatible and even better results compared with single-task learning. In  [39] , an encoder-decoder multi-task learning network for road segmentation, object detection, and classification was designed. In  [40] , a partially shared CNN architecture was developed for simultaneous gaze point and gaze direction estimation. The multi-task learning paradigm has also been utilized in many perceptions, translation, and classification tasks  [41, 42] . However, very few studies have analyzed the effectiveness of utilizing such a framework for the improvement of driver behavior reasoning, especially focus on the learning of multi-scale driver behaviors towards a unified and intelligent mutual-understanding scheme. To our best knowledge, this study is the first one to exploit a unified multi-task and multi-scale driver understanding model that can recognize both physical behaviors and mental activities for human drivers.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Contribution",
      "text": "As aforementioned, driver behavior recognition has been widely studied. However, very few studies exploit the common patterns and representations between different behaviors. In this study, a unified driver behavior reasoning framework is designed to estimate the multi-scale driver behaviors. The contribution of this study can be summarized as follows.\n\nFirst, an encoder-decoder based unified network structure is proposed for multi-scale driver behavior learning and reasoning. The network is flexible and scalable that can be expanded to enriching the representation in the encoder part and designing a proper state estimation module in the decoder part. Second, based on the proposed network, it is proved that the higher-level unobservable driver mental states such as intention and emotion can be jointly learned and inferred based on the lower-level observable driver pose and facial representations. The unified network is evaluated based on the different datasets and achieved state-of-the-art results compared with existing methods. Last, different driver states are jointly analyzed based on the proposed network. This analysis can benefit the intelligent and autonomous vehicles towards a better mutual understanding system.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Paper Organization",
      "text": "The remainder of this paper is organized as follows. Section ‚Ö° introduces the high-level framework of the unified driver behavior reasoning and the experimental setup. In section ‚Ö¢, the model structures are highlighted, and the training process is clarified. The experiment results and model evaluation for the different tasks with the different datasets are represented in Section ‚Ö£. In Section ‚Ö§, the discussion on the proposed model and future works is proposed. Last, the study is concluded in Section ‚Ö•.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ii. System Overview",
      "text": "In this section, the high-level structure of the proposed unified driver behavior reasoning system will be described. Specifically, three key aspects are introduced, which are naturalistic data collection and processing, CNN based driver physical behavior model construction, and temporal sequence processing for driver mental state inference based on LSTM-RNN.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. High-Level System Architecture",
      "text": "The overall framework is shown in Fig.  1  below. Three major components are designed, namely, data collection and processing, driver physical behavior recognition, and driver mental state inference. First, the naturalistic driving behavior on the highway is collected. A data collection and synchronization system were designed to capture both driver behaviors information and traffic context. Three cameras are implemented inside the vehicle cabin to collect driver front face images, hand movements, and traffic context. A detailed experiment testbed setup is described in the next part. Once the naturalistic driving data are collected and synchronized, these data need further processing to train and evaluate the system. The road information from the front-looking cameras will be used to extract the lane positions, lane styles, and road curvature. The road context information enables a more reasonable driver lane change intention inference as a solid lane means the driver cannot make a lane change in that direction. The second module of the proposed system is a CNN based encoder for spatial feature learning and extraction. In this part, two different pre-trained CNN models, namely, MobileNet V2 and Inception-Resnet V2 will be adopted and compared  [43, 44] . Based on the CNN encoder, high-level representation for driver physical behavior related features can be learned and extracted. Each frame of the sequence will be fed into the CNN encoder, and the output of each frame will be concatenated into a sequential feature tensor for further time-series model construction.\n\nThe driver's mental state inference module contains two separate parts, which are driver emotion recognition and driver intention inference. Driver mental states differ in physical behaviors from two aspects. First, mental states like driver intention are non-observable and can only be inferred based on driver physical behaviors such as head pose, and body movement. Second, these cognitive processes are usually depending on many aspects, such as self-motivation and traffic context stimuli; these processes are dynamic and require time-series modeling to capture the temporal pattern between previous and current behaviors. The decoder for driver physical behavior recognition part is simply fully-connected layers, while the LSTM-RNN based models are adopted for driver mental states inference. Driver emotion variation is viewed as a cognitive process in this study as driver emotion can be hardly changed very quickly within a short period. The encoder-decoder multi-task learning represents a simplified version of the human cognitive process. The CNN encoder plays a similar role to the human visual system, which mainly focuses on perception and feature extraction. On the other hand, the multiple RNN decoders mimic the brain reasoning parts to generate different inference results for different tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Experimental Data Processing",
      "text": "In this study, two naturalistic datasets are used to evaluate the proposed system. The first one is the collected highway dataset around the Cranfield area in the UK (known as CranData). The second one is a public dataset, namely, Brian4Cars dataset, which is available at http://www.brain4cars.com  [45] . The CranData was collected based on three cameras and one VBOX GPS logger for vehicle speed and heading measurement. The three cameras are mounted inside the vehicle for the driver's head and upper body monitoring, hand movement recording, and traffic context recording, respectively. The three video streams are synchronized and recorded at a frequency of 25 fps with 640 480 resolution. The default sampling rate of the VBOX data logger is 20 HZ. Three male drivers with varying ages and experiences were involved in the data collection. All of them were asked to drive as usual without telling them the real objective of the experiment. Each driver drove the vehicle on the highway for about one hour. The Brain4Cars dataset contains both inside and outside videos, which were sampled at 25 fps and 30 fps, respectively. The data consisted of 1180 miles of freeway and city driving and was collected from 10 drivers. The data also annotated the number of lanes on the road and the current lane. Five driving maneuvers were recorded, which are driving straight, lane change left/right, and turn left/right. For comparison reason, only the first three maneuvers are used, and the turn maneuvers are not studied in this study.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodologies",
      "text": "In this section, the multi-task learning model is described. The encoder part of the model is a CNN model that extracts the temporal-spatial abstract features from the driver behavior image sequence. Then, different decoder networks are trained separately to personalized estimate the specific task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Model Architecture",
      "text": "The overall model architecture is shown in Fig.  2  below. The model follows an encoder-decoder CNN-RNN structure so that the shared abstract spatial driver behavior features can be used for multi-task learning and prediction. A CNN model is used as an encoder for sequential feature extraction. Then, in the middle layer, a feature fusion module is developed before feeding the sequential features into the decoding part. The feature fusion layer makes the model even flexible and enables scalable implementation as the model can be easily integrated with other existing modules. The modularity design method is an efficient fashion to integrate different features together for comprehensive driver states estimation. For instance, the road context information can be integrated into the driver behavior feature tensor through the middle feature fusion layer to contribute a more precise driver lane change intention inference. Also, the onboard speech recording system can extract the speech and audio information between the driver and passengers so that the speech features can be involved in the driver's emotion recognition process. Once the features from different modules are fused, the temporal feature sequence can be fed into the multi-task decoders. The encoder-decoder multi-task learning framework represents a simplified version of the human cognitive process. The CNN encoder plays a similar role to the human visual system. The task of the encoder part is to extract the representative features that can reflect driver outer physical behaviors and states.\n\nThe encoder part consists of several convolutional layers and pooling layers of a deep classification network to construct a strong vision-based system to extract important spatial features from the video sequence. Specifically, two state-of-the-art pre-trained deep CNN networks, namely, MobileNet V2 and Inception-Restnet V2 are adopted. The structure of the two pre-trained networks is very different from each other. Although both of them achieved state-of-the-art results on image classification, object detection, and segmentation, etc., MobileNet is a light but an efficient network that is designed for embedded and mobile computing, which only contains 155 layers. On the contrary, the Inception-Resnet V2 is a much deeper network that contains 825 layers, and the feature representation capability can be more powerful than the MobileNet. The MobileNet solution can be used to investigate the real-time performance of the proposed framework on the embedded computational platform. In contrast, the Inception-Resnet V2 can be used to exploit the maximum model capability. The last fully connected and softmax layers of the two encoder networks are discarded and replaced with two extra fully connected layers for driver physical behavior recognition and feature extraction. As shown in Fig.  2 , the decoding part will be responsible for three different tasks, which is driver physical behavior recognition, driver intention inference, and driver emotion recognition, respectively. The driver's physical behavior recognition task jointly estimates the driver's mirror checking behaviors and the driver's facial expression. There are four mirror checking behaviors used in this study, which are front-facing, rear mirror checking, left mirror checking, and right mirror checking. Besides, there are two facial expressions for each image, which are the normal and emotional expression. Each mirror checking behaviors map two possible facial expressions so that eight different physical behaviors are identified in total. The final output of the FC decoder is the estimated behavior for the single image at each step, and no temporal patterns are required.\n\nThe two driver mental states inference tasks are similar to each other and require the temporal patterns for real-time inference. The driver intention inference module is designed to infer the lane change intention based on the sequential input tensor from the encoder. Three intentions, namely, lane change right, lane change left, and lane-keeping, are classified. The emotion recognition task focuses on the detection of neutral and emotional states based on the sequential inputs. The bidirectional LSTM-RNN (BiRNN) architecture is applied as BiRNN can capture more forward and backward dependency patterns of the sequential feature tensor. The detailed BiRNN model structure for these two models is shown in Fig.  2 . Bidirectional LSTM layers with 150 hidden units in each layer are implemented.\n\nThe BiRNN can be represented as follows, the forward and backward hidden states of the current time are the function of previous states, and the input tensor  [46] . The final output is a function of the forward states and backward states. ùë† ùëú * ùë°ùëéùëõ‚Ñé ùê∂ (9) Detailed explanation for LSTM cell can be found in  [47] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Model Training And Testing",
      "text": "The training strategy for the multi-task driver behavior reasoning model follows a fine-tuning process. The encoder CNN model is initialized with the weights trained on the ImageNet  [48] . Then, the last FC layers are discarded and replaced with new FC layers and classification layers for driver physical behavior classification. Driver physical behaviors and facial expressions are low-level driver activities, which are the outer reflection of the mental states. Hence, the encoder trained with physical behavior data can be used as an informative representation of the mental states. A 10 initial learning rate is applied for the convolutional layers of the CNN encoder to decrease the learning rate and maintain the feature extraction power. The weight learning rate and bias learning rate of the new FC layers are selected as 20. The mini-batch size for the MobileNet is 32, and 15 for the Inception-ResNet dues to the memory capability. There are 47111 samples in the CranData and 36760 samples in the Brain4Cars dataset. 80% of the data are split into the training set, while the rest are used for testing. Each model is trained with three epochs.\n\nThe lane change intention inference and emotion recognition decoders are trained with sequential inputs. The input tensors are generated by the encoder and the feature fusion layer. The activation from the second last FC layers (with 512 neurons) is used for feature extraction. The sequence length is varied according to the different length of the input sequence in the CranData. Each record in the Brain4Cars dataset is a sequence with 150 images for six seconds driving data. An initial learning rate of 0.1 with a 0.5 decay rate for every 50 epochs is applied. The mini-batch size is 32, and the max training epoch is 500. There are 201 samples within the CranData, and 244 samples are collected from the Brain4Cars dataset. The cross-entropy loss function is used, and the models are optimized with Adam optimizer  [49] . The overall testing rate achieved around 25 fps on a low-cost Nvidia GPU (MX150), which can satisfy the real-world application requirement.\n\nUnlike some multi-task learning methods which jointly training the encoder and decoder part with a summed or weighted summed cost function  [38, 39] , in this study, the decoder and encoder parts are trained separately. The in-vehicle driver monitoring systems usually need modularized design. The vision-based system is not the only on-board system for driver states reasoning on intelligent and autonomous vehicles. For example, there can be other driver assistance systems such as radio and human-machine-interaction (HMI). Separately training the encoder and decoders is more flexible, as more sensors and features can be easily integrated into the feature fusion layer, and only the decoders need to be updated if the input tensor changed. Hence, the CNN encoder in this study is merely used as a task-oriented feature extraction module.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experimental Results",
      "text": "In this section, the experimental results and model evaluation will be illustrated for the three different tasks. The classification results for the eight basic physical behaviors are first proposed to show the feature representation capability of the encoder CNN. Then, the time-series classification for intention inference and emotion recognition will be evaluated. Each of the tasks will be evaluated with two datasets, respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Driver Physical Behaviors Classification",
      "text": "The driver physical behavior classification task is first evaluated as precise driver observable behavior representation is the fundamental part of mental state modeling. The two pre-trained model performance on the CranData and Brain4Cars dataset are assessed separately. This part focusses on the evaluation of the two selected models based on the two datasets. The visualization of the learned feature representation of the two models is shown in Fig.  3 . Two behaviors, namely, normal emotional (happy in this case) driving and neutral right mirror checking behaviors are investigated. The occlusion sensitivity method  [50]  is used to represents the model representation capability. It is shown in Fig.  3  that all of the two models learn a good representation of the combined states of driver behaviors and particular focus on the facial area to estimate the head pose and facial expression jointly.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Driver Intention Inference",
      "text": "In this part, driver intention inference results are compared between multiple baseline algorithms on the two datasets. The baselines methods are first introduced as follows. As emotional recognition follows a similar time-series modeling process, the same baseline methods will be used in the next part. The CNN-RNN based driver intention model will be compared with the existing methods, which use head pose and eye gaze tracking for driver feature extraction following with machine learning methods for classification. For the CranData, the in-vehicle driver head poses and out-vehicle traffic context information will be concatenated. Specifically, the driver head pose and eye gaze feature will be detected based on a Conditional Local Neural Fields (CLNF) approach  [51] . The inside feature vector at each time constant can be formed as follows.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "ùêº",
      "text": "ùê∫ ùê∫ ùê∫ ùêª ùêª  (10)  where ùê∫ , is the 2D gaze angle in ùë• and ùë¶ coordinate, ùê∫ , ùê∫ , ùêª , and ùêª are the 3D gaze direction for each eye, head pose translation vector, and head pose direction vector, respectively. This gives a 14-dimensional vector inside the vehicle feature vector. As the CranData was collected on a highway, the line styles of the ego-lane are used. Two vehicle dynamic signals, namely the vehicle speed (ùëâ), and heading angle (ùêª) are collected using the VOBX. The total feature vector for the outside traffic context and vehicular dynamics can be formed as a four-dimensional vector.  where ùêø and ùêø are the lane style for right and left lane and ùêø ‚àà 0, 1 represent the two different lane styles (solid and dash). The in-vehicle feature ùêº and out-vehicle feature ùëÇ will be concatenated to form an 18-dimensional feature vector.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "ùëÇ ùêø ùêø ùëâ ùêª",
      "text": "According to the developed feature extraction method, several baselines are designed as follows. 1. Support Vector Machine (SVM). SVM is a discriminative classifier, which was used in the past for driver intention detection  [11, 12] . As SVM cannot process the sequential data directly, a statistical feature vector will be used to calculate the maximum, minimum, mean, and standard deviation (STD) of the head pose features. Hence, the 18D feature vector for each step will be expanded to a 72D feature vector for the whole sequence.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Hidden Markov Model (Hmm).",
      "text": "HMM is a generative classifier that assigns a probabilistic graph for each class  [10, 15] . A fixed-size vector with a length of 150 was used, which carries six seconds inside and outside-vehicle information. 3. LSTM based RNN. The LSTM-RNN based method is adopted for intention inference, which is based on the hand-crafted feature vector (HF-LSTM)  [45] . 4. CNN-RNN encoder-decoder models (CRNN). The CNN-RNN model is the model shown in Fig.  2 , which use CNN as encoder and Bi-directional LSTM-RNN for the decoder. Also, two different encoders, which are noted as CNN-RNN-M (MobileNetv2) and CNN-RNN-IR (InceptionResnetV2), are evaluated respectively. To quantitively evaluate the model performance, four performance indexes, namely, precision, recall, F1 scores, and the general average precision, are adopted to assess the model performance on each maneuver (lane change right, lane change left, and lane-keeping). Four metrics are statistically calculated for each maneuver, which is the true positive (ùëá , the model correct detects this maneuver), true negative (ùëá , the mode correct detects the other maneuvers), false positive (ùêπ , the model detects the other maneuvers as target one), and false negative (ùêπ , the mode detects the target maneuver as other maneuvers).\n\nAccording to the four-performance index, the Precision (ùëÉ ) can be calculated as:\n\nThe Recall (ùëÖ ) is calculated as:\n\nThe F1-score considers both the ùëÉ and ùëÖ , and is the harmonic mean of these two values. ùêπ1 2 (14) Last, the general average precision is calculated as: ùê∫\n\nThe model performances are shown in Table  1  and Table  2  below for CranData and Brain4Cars dataset. The models are evaluated based on the six seconds sequential data, which is collected 3.5 seconds before the maneuvers. The models are trained and tested for five times with randomly collected 80% data for training and 20% for testing at each time to generate the mean and STD of the proposed methods. The proposed encoder-decoder CRNN models achieved the state of are results on the two datasets that the lane change intent can be predicted 3.5s before the maneuver with around 90% accuracy. Moreover, it shows that by integrating extra features from a different functional module (lane detection module (L) in this case), the model performance can be increased (CRNN-M-L and CRNN-IR-L). If extra features are available for the task, the encoder part does not need to be modified, and only the decoder for the specific task has to be updated. Hence, the framework is extendable and allows fast learning and adjustment. The model classification performance on the three intents on the two datasets is shown in Fig.  5 . It shows that lane-keeping intent achieved the most accurate results on the CranData. In contrast, lane change right intent is the most accurate one on the Brain4Cars dataset, which shows the advantage of the proposed method over the results given in  [45] . The deeper Inception-ResNet generates slightly better results than the MobileNet. The model training performances are shown in Fig.  6 . The intention inference decoder with the high-level features from the two models can be converged after 200 epochs on both of the two datasets.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "C. Driver Emotion Evaluation",
      "text": "In this part, the model performance of the driver emotion recognition task is evaluated. The mental emotion is viewed as a cognitive process that can last for a few seconds and not varied rapidly  [52] . Dues to the naturalistic data limitation, only two states, namely, the emotional state and neutral state are classified. Most of the emotional states in this study are happy states with a small number of surprising cases. The temporal emotional process is similar to the intention process in the last part and can be recognized with similar methods. The HMM and LSTM models are also used as the baseline methods. As the hand-craft head pose and eye gaze features in the last part cannot be used to precisely estimate the facial expression. Two popular features for facial expression and emotion representation are adopted, which are hybrid Histogram of Oriented Gradients (HOG) and the 68 points of facial landmarks  [53]   [54] . A cell size of 32 is chosen for the HOG extractor, and the coordinates of the facial landmarks points in the image plane are used as the features vector. These two feature extraction methods will be combined with the HMM and LSTM to generate four baseline methods. Results comparison between different models on the two datasets is shown in Table  3 , and Table  4 . respectively.  As shown in Table  3  and Table  4 , the proposed encoder-decoder method achieved the most accurate recognition results on the two datasets. The MobileNet achieved slightly better accuracy than the Inception-ResNet in this case. The facial landmarks features are less representative and efficient than the other feature. Regarding the two emotion states, a neutral driving emotion process can be accurately detected. In contrast, the recall (sensitivity) performance on the emotional driving tasks is less precise, which may due to the imbalanced dataset. This can be found in the confusion matrix comparison that is shown in Fig.  7 . As given in Fig.  7 , the amount of the labeled emotional process is much fewer than the neutral driving samples. The confusion matrix is calculated based on the summation of the five-testing process. The MobileNet shows more precise results than the Inception-ResNet on the two datasets. The model training performance of two different models on the two datasets is shown in Fig.  8 . It shows that the binary emotional and neutral states classification encoder is easier to be trained than the intention task and can be converged after 200 epochs on both of two datasets.",
      "page_start": 9,
      "page_end": 11
    },
    {
      "section_name": "D. Analysis Of Driver Behaviors, Intention, And Emotion",
      "text": "The relationship between driver intent and emotion are analyzed in this part to exploit the connection between these two behaviors. The two behaviors are analyzed from two aspects. First, the proportion of emotional process with respect to the three driving intent are statistically analyzed. Then, based on the driver behaviors and facial expression labels, the relative time intervals between the mirror checking behaviors and the facial expression duration are studied.\n\nFirst, the proportion of the emotional process for the three driving intent is represented in Fig.  9 . In sum, the proportion of the emotional process in the lane-keeping (LK) cases is dramatically larger than that in the lane changing cases. For the CranData dataset, the emotional process accounts for 24.6% in the lane-keeping cases, while only about 16% of the lane changing preparation processes show the emotional facial expression. For the Brain4Cars dataset, the emotional states during the lane changing preparation process are even smaller, which are only 7.8% for the lane change left, and 10.9% for the lane change right. These results and comparisons are made based on the on-hand datasets. However, both of the two datasets indicates that the emotional states are more easily occur during easy driving tasks, such as lane-keeping maneuver. While for the lane change scenarios, the drivers may need to pay more attention to the driving task, which makes them more focus on the situation-aware and vehicle control, and usually show less emotional expression. Next, the relative time intervals between the facial expression and mirror checking behaviors are analyzed. The facial expression represents the driver's emotion, while the mirror checking behaviors show a strong indication for the future driving intention. Hence, analysis of the time intervals between these two states is useful to exploit the dynamics of different mental states. In this step, only the emotional lane change preparation (intended) process is studied. To jointly analyze the emotion and intention state, two critical time intervals are calculated, namely, the time interval between the first facial expression and first mirror checking behavior in the sequence, and the time interval of the last facial expression and las mirror checking moments. The initial time interval and the end time interval are represented as follows.\n\nùêºùëõùëñùë°ùëñùëéùëô ùêºùëõùë°ùëíùëüùë£ùëéùëô ùë° 1 ùë° 1 (16) ùê∏ùëõùëë ùêºùëõùë°ùëíùëüùë£ùëéùëô ùë° ùëíùëõùëë ùë° ùëíùëõùëë (17) where ùë° 1 and ùë° 1 are the first moments of the facial expression and mirror checking, while ùë° ùëíùëõùëë and ùë° ùëíùëõùëë are the last moments of the two behaviors.\n\nStatistical results on the two datasets are illustrated in Fig.  10  below. As shown in Fig.  10 , the first facial expression generally occurs about one second earlier than the first mirror checking moment on the two datasets (1.194 s and 1.08 s, respectively). Although, the results of the finish time intervals of the two behaviors are not similar with each, the average finish time intervals are within one second, which means once the mirror checking is finished and the driver is ready to make a lane change maneuver, their obvious facial expressions will not last very long so that the driver can focus on the lane change maneuver.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "V. Discussion And Future Works",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A. Advantages And Limitation",
      "text": "In this study, a unified multi-scale driver behavior reasoning framework is proposed and evaluated with three different tasks, which are driver physical behavior recognition, driver intention inference, and driver emotion recognition. Regarding physical behavior recognition, the encoder CNN which was trained with the joint driver behaviors, indicates an efficient representation for driver activity and facial expression. The classification results for the combined driver behaviors are consistent and more accurate than some existing methods  [9, 15] . Moreover, the model can learn more features and patterns based on jointly labeled driver behavioral data. The visualization for model representation performance shows the CNN encoder can capture the important features for driver behavior classification that particular focus on the facial area. This step is the foundation of multi-task driver behavior reasoning as the following tasks require a proper representation to make driver mental state inference.\n\nSecond, two driver mental states are studied, which are driver intention and driver emotion. Driver intention based on the encoder-decoder architecture is compared with multiple existing methods on two datasets. The performance shows the accuracy and efficiency of the proposed methods. In terms of emotion recognition, most of the existing studies focus on the identification of the emotion using statistic images or short image sequences. However, in this study, the emotional state is viewed as a cognitive process like driver intention since the emotional states cannot change rapidly. Based on this assumption, driver emotion is jointly studied with driver intention. It is found that the driver tends to exhibit an emotional driving behavior when the driving tasks are relatively straightforward, such as during normal driving and lane-keeping maneuver. On the contrary, the driver usually keeps a neutral emotion state and focus on the driving task when they generate a lane change intent and prepare for the maneuver.\n\nIn sum, the advantages of this study can be summarized as follows. ÔÉò First, the encoder-decoder framework enables a multi-scale and multi-task driver behavior reasoning. Unlike end-to-end training for multi-tasks learning  [38, 39] , the feature representation process and task inference process are separately trained so that the model is more flexible and easier to be extended. ÔÉò Second, the feature fusion layer can be used to integrate other features from different modules, and only the decoder part needs to be updated if more function modules are involved. The feature presentation capability of the CNN encoder can be enhanced Brain4Car Results",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Crandata Results",
      "text": "by introducing more driver state data. This structure is an effective manner for driver behavior reasoning on the intelligent and automated vehicles as driver behaviors data of standard, and single-task are easy to be collected and labeled. ÔÉò It is hard to estimate the drivers' intention on highly automated vehicles as drivers are usually not driving the vehicle by themselves. However, based on the investigation between driver emotion and driver intention, it is found that driver's emotional states can be used as a clue to estimate whether a driver is concentrating on the road context or not. This can be evidence to determine if the driver is holding a specific driving intention before they take-over the vehicle control authority. However, there are also limitations exist in this study. One of the primary limitations of this study is limited driver behaviors are collected due to the naturalistic driving task. The emotional states are not very sufficient as it is dangerous to disrupt the driver by sending them too many negative emotional messages or performing secondary tasks such as answering the phone and texting. Hence, driver attention and distraction states are not studied in the current stage, which we think is also essential to the understanding of driver behaviors for both manual driving and autonomous driving tasks.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B. Future Works",
      "text": "A straightforward task in the future is to exploit and involve more driver behaviors such as driver attention and driver drowsiness, etc. A more comprehensive analysis of the relationship between different driver mental states and cognitive processes is expected to enhance the human understanding intelligence of vehicle automation. In this study, driver activity and facial expression are jointly labeled due to the limitation of data variance. However, in the future, more data can be collected in a distributed manner to improve the generalization ability of the CNN encoder. For example, more challenge and dagerous behavioral data such as more emotions (sad, anger, surprise, tec.), secondary tasks, and fatigue behavior can be collected on the driving simulators or from open public datasets so that the CNN encoder can distinguish more basic driver states  [55] . However, increase the data variance and volume with a distributed manner would generate a new consideration, which is the domain adaptation  [56] . Introducing more data from other environments should not decrease the original performance on behavior classification. Therefore, domain adaptation needs to be analyzed and evaluated before more data can be adopted.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this study, a unified multi-scale driver behavior reasoning framework is proposed. Three different driver behaviors are studied, which are driver physical behaviors (normal driving activities and facial expression), driver intention, and driver emotion. An encoder-decoder multi-task model is proposed based on the integration of CNN and RNN models. By fine-tuning the CNN encoder with the driver's physical behavioral classification task, CNN can learn a precise representation for different driver states. Two pre-trained CNN models, namely MobilyNet V2 and Inception-ResNet V2 are used for the base structure of CNN encoder. The models are evaluated on two different datasets (CranData and Brian4Cars) for highway and urban road driving behavior recognition. Driver behavior classification achieved 95% on the eight defined driver states. The model performance on driver intention inference achieved state-of-the-art results on the two datasets (around 90% in general). Last, the emotional driving process recognition achieved over 95% accuracy. The MobileNet encoder can generate similar results with the deeper network, which makes the framework portable to the on-board embedded systems. The framework is flexible and extendable to be implemented on the intelligent and autonomous vehicles for comprehensive driver/passenger understanding.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: below. Three major components are designed, namely, data collection and processing,",
      "page": 3
    },
    {
      "caption": "Figure 1: Illustration of the high-level architecture of the unified multi-task driver behavior reasoning system.",
      "page": 3
    },
    {
      "caption": "Figure 2: Multi-task learning model framework for driver behavior reasoning. A pre-trained deep CNN is fine-tuned based on driver behavioral data and used as the",
      "page": 5
    },
    {
      "caption": "Figure 2: , the decoding part will be responsible for three different tasks, which is driver physical behavior recognition,",
      "page": 5
    },
    {
      "caption": "Figure 2: Bidirectional LSTM layers with 150 hidden units in each",
      "page": 5
    },
    {
      "caption": "Figure 3: Two behaviors, namely, normal emotional (happy in this",
      "page": 6
    },
    {
      "caption": "Figure 3: that all of the two models learn a good representation of the",
      "page": 6
    },
    {
      "caption": "Figure 3: Model representation visualization results in the normal emotional driving, and neutral right mirror checking behaviors, respectively. The left four results are",
      "page": 6
    },
    {
      "caption": "Figure 4: below. It should be mentioned that in the Brain4Cars",
      "page": 6
    },
    {
      "caption": "Figure 4: , both of the two models can achieve more than 90%",
      "page": 7
    },
    {
      "caption": "Figure 4: Confusion matrix of the behavior classification based on MobileNetV2 and Inception-ResNetV2 on the CranData and Brain4Car datasets.",
      "page": 7
    },
    {
      "caption": "Figure 2: , which use CNN as",
      "page": 8
    },
    {
      "caption": "Figure 5: It shows that lane-keeping intent",
      "page": 8
    },
    {
      "caption": "Figure 6: The intention inference decoder",
      "page": 9
    },
    {
      "caption": "Figure 5: Confusion matrix and model performance for the intention inference task. The upper two graphs indicate the classification results of MobileNet and",
      "page": 9
    },
    {
      "caption": "Figure 6: Intention inference model learning process on the two datasets. The upper two graphs are the learning accuracy and training loss on CranData, and the bottom",
      "page": 9
    },
    {
      "caption": "Figure 7: Confusion matrix and model performance for the emotion recognition task. The upper two graphs indicate the classification results of MobileNet and",
      "page": 10
    },
    {
      "caption": "Figure 7: As given",
      "page": 11
    },
    {
      "caption": "Figure 7: , the amount of the labeled emotional process is much fewer than the neutral driving samples. The confusion matrix is",
      "page": 11
    },
    {
      "caption": "Figure 8: It shows that the binary emotional and neutral states classification encoder is easier to be trained than the intention task and can",
      "page": 11
    },
    {
      "caption": "Figure 8: Emotion inference model learning process on the two datasets. The upper two graphs are the learning accuracy and training loss on CranData, and the bottom",
      "page": 11
    },
    {
      "caption": "Figure 9: In sum, the proportion of the",
      "page": 11
    },
    {
      "caption": "Figure 9: The proportion of the emotional and neutral process within the three different intent. The left part indicates the statistics on CranData dataset, and the right",
      "page": 11
    },
    {
      "caption": "Figure 10: below. As shown in Fig. 10, the first facial expression generally",
      "page": 12
    },
    {
      "caption": "Figure 10: Statistical analysis of the relative time intervals between the facial expression duration and mirror checking behaviors. Negative values mean emotion start",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6439\n68.3%": "30\n0.3%",
          "33\n0.4%": "598\n6.3%",
          "53\n0.6%": "1\n0.0%",
          "10\n0.1%": "2\n0.0%",
          "24\n0.3%": "0\n0.0%",
          "0\n0.0%": "0\n0.0%",
          "98.2%\n1.8%": "94.0%\n6.0%"
        },
        {
          "6439\n68.3%": "59\n0.6%",
          "33\n0.4%": "0\n0.0%",
          "53\n0.6%": "878\n9.3%",
          "10\n0.1%": "0\n0.0%",
          "24\n0.3%": "1\n0.0%",
          "0\n0.0%": "0\n0.0%",
          "98.2%\n1.8%": "93.2%\n6.8%"
        },
        {
          "6439\n68.3%": "11\n0.1%",
          "33\n0.4%": "19\n0.2%",
          "53\n0.6%": "0\n0.0%",
          "10\n0.1%": "48\n0.5%",
          "24\n0.3%": "0\n0.0%",
          "0\n0.0%": "1\n0.0%",
          "98.2%\n1.8%": "60.8%\n39.2%"
        },
        {
          "6439\n68.3%": "140\n1.5%",
          "33\n0.4%": "0\n0.0%",
          "53\n0.6%": "1\n0.0%",
          "10\n0.1%": "0\n0.0%",
          "24\n0.3%": "830\n8.8%",
          "0\n0.0%": "0\n0.0%",
          "98.2%\n1.8%": "84.1%\n15.9%"
        },
        {
          "6439\n68.3%": "1\n0.0%",
          "33\n0.4%": "6\n0.1%",
          "53\n0.6%": "0\n0.0%",
          "10\n0.1%": "2\n0.0%",
          "24\n0.3%": "3\n0.0%",
          "0\n0.0%": "0\n0.0%",
          "98.2%\n1.8%": "84.0%\n16.0%"
        },
        {
          "6439\n68.3%": "0\n0.0%",
          "33\n0.4%": "0\n0.0%",
          "53\n0.6%": "3\n0.0%",
          "10\n0.1%": "0\n0.0%",
          "24\n0.3%": "10\n0.1%",
          "0\n0.0%": "0\n0.0%",
          "98.2%\n1.8%": "91.0%\n9.0%"
        },
        {
          "6439\n68.3%": "0\n0.0%",
          "33\n0.4%": "0\n0.0%",
          "53\n0.6%": "0\n0.0%",
          "10\n0.1%": "0\n0.0%",
          "24\n0.3%": "0\n0.0%",
          "0\n0.0%": "1\n0.0%",
          "98.2%\n1.8%": "100%\n0.0%"
        },
        {
          "6439\n68.3%": "96.4%\n3.6%",
          "33\n0.4%": "91.2%\n8.8%",
          "53\n0.6%": "93.8%\n6.2%",
          "10\n0.1%": "77.4%\n22.6%",
          "24\n0.3%": "95.6%\n4.4%",
          "0\n0.0%": "50.0%\n50.0%",
          "98.2%\n1.8%": "95.4%\n4.6%"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5403\n73.5%": "49\n0.7%",
          "31\n0.4%": "723\n9.8%",
          "27\n0.4%": "1\n0.0%",
          "0\n0.0%": "0\n0.0%",
          "58\n0.8%": "1\n0.0%",
          "3\n0.0%": "14\n0.2%",
          "97.8%\n2.2%": "91.8%\n8.2%"
        },
        {
          "5403\n73.5%": "45\n0.6%",
          "31\n0.4%": "0\n0.0%",
          "27\n0.4%": "415\n5.6%",
          "0\n0.0%": "1\n0.0%",
          "58\n0.8%": "1\n0.0%",
          "3\n0.0%": "0\n0.0%",
          "97.8%\n2.2%": "89.2%\n10.8%"
        },
        {
          "5403\n73.5%": "0\n0.0%",
          "31\n0.4%": "0\n0.0%",
          "27\n0.4%": "0\n0.0%",
          "0\n0.0%": "0\n0.0%",
          "58\n0.8%": "0\n0.0%",
          "3\n0.0%": "0\n0.0%",
          "97.8%\n2.2%": "100%\n0.0%"
        },
        {
          "5403\n73.5%": "19\n0.3%",
          "31\n0.4%": "0\n0.0%",
          "27\n0.4%": "1\n0.0%",
          "0\n0.0%": "0\n0.0%",
          "58\n0.8%": "518\n7.0%",
          "3\n0.0%": "7\n0.1%",
          "97.8%\n2.2%": "95.0%\n5.0%"
        },
        {
          "5403\n73.5%": "0\n0.0%",
          "31\n0.4%": "2\n0.0%",
          "27\n0.4%": "0\n0.0%",
          "0\n0.0%": "0\n0.0%",
          "58\n0.8%": "0\n0.0%",
          "3\n0.0%": "16\n0.2%",
          "97.8%\n2.2%": "88.9%\n11.1%"
        },
        {
          "5403\n73.5%": "2\n0.0%",
          "31\n0.4%": "0\n0.0%",
          "27\n0.4%": "0\n0.0%",
          "0\n0.0%": "6\n0.1%",
          "58\n0.8%": "0\n0.0%",
          "3\n0.0%": "0\n0.0%",
          "97.8%\n2.2%": "75.0%\n25.0%"
        },
        {
          "5403\n73.5%": "97.9%\n2.1%",
          "31\n0.4%": "95.6%\n4.4%",
          "27\n0.4%": "93.5%\n6.5%",
          "0\n0.0%": "85.7%\n14.3%",
          "58\n0.8%": "89.6%\n10.4%",
          "3\n0.0%": "40.0%\n60.0%",
          "97.8%\n2.2%": "96.4%\n3.6%"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4981\n67.8%": "447\n6.1%",
          "13\n0.2%": "733\n10.0%",
          "19\n0.3%": "11\n0.1%",
          "0\n0.0%": "0\n0.0%",
          "7\n0.1%": "8\n0.1%",
          "99.2%\n0.8%": "60.8%\n39.2%"
        },
        {
          "4981\n67.8%": "56\n0.8%",
          "13\n0.2%": "0\n0.0%",
          "19\n0.3%": "414\n5.6%",
          "0\n0.0%": "1\n0.0%",
          "7\n0.1%": "0\n0.0%",
          "99.2%\n0.8%": "87.3%\n12.7%"
        },
        {
          "4981\n67.8%": "0\n0.0%",
          "13\n0.2%": "0\n0.0%",
          "19\n0.3%": "0\n0.0%",
          "0\n0.0%": "0\n0.0%",
          "7\n0.1%": "0\n0.0%",
          "99.2%\n0.8%": "100%\n0.0%"
        },
        {
          "4981\n67.8%": "34\n0.5%",
          "13\n0.2%": "1\n0.0%",
          "19\n0.3%": "0\n0.0%",
          "0\n0.0%": "1\n0.0%",
          "7\n0.1%": "558\n7.6%",
          "99.2%\n0.8%": "93.9%\n6.1%"
        },
        {
          "4981\n67.8%": "0\n0.0%",
          "13\n0.2%": "9\n0.1%",
          "19\n0.3%": "0\n0.0%",
          "0\n0.0%": "0\n0.0%",
          "7\n0.1%": "4\n0.1%",
          "99.2%\n0.8%": "72.3%\n27.7%"
        },
        {
          "4981\n67.8%": "0\n0.0%",
          "13\n0.2%": "0\n0.0%",
          "19\n0.3%": "0\n0.0%",
          "0\n0.0%": "5\n0.1%",
          "7\n0.1%": "1\n0.0%",
          "99.2%\n0.8%": "83.3%\n16.7%"
        },
        {
          "4981\n67.8%": "90.3%\n9.7%",
          "13\n0.2%": "97.0%\n3.0%",
          "19\n0.3%": "93.2%\n6.8%",
          "0\n0.0%": "71.4%\n28.6%",
          "7\n0.1%": "96.5%\n3.5%",
          "99.2%\n0.8%": "91.6%\n8.4%"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: and Table 2 below for CranData and Brain4Cars dataset. The models are",
      "data": [
        {
          "Algorithms": "",
          "Left Lane Change": "Pr (%) \nRe (%) \nF1 Score",
          "Right Lane Change": "Pr (%) \nRe (%) \nF1 Score",
          "Keep Straight": "Pr (%) \nRe (%) \nF1 Score",
          "General \nAve (%)": ""
        },
        {
          "Algorithms": "SVM \nHMM \nHF-LSTM \nCRNN-M \nCRNN-IR \nCRNN-M-L \nCRNN-IR-L",
          "Left Lane Change": "86.2(cid:3399)5.7 \n91.8(cid:3399)4.0 \n88.8(cid:3399)3.5 \n67.4(cid:3399)5.5 \n81.9(cid:3399)10.9 \n73.7(cid:3399)6.1 \n88.5(cid:3399)13.2 \n92.6(cid:3399)6.0 \n89.9(cid:3399)7.2 \n87.6(cid:3399)8.1 \n89.6(cid:3399)12.6 \n88.1(cid:3399)7.7 \n87.1(cid:3399)10.2 \n92.8(cid:3399)4.5 \n89.7(cid:3399)6.7 \n89.0(cid:3399)8.1 \n89.1(cid:3399)3.5 \n88.8(cid:3399)3.9 \n94.2(cid:3399)6.1 \n88.2(cid:3399)13.9 \n90.8(cid:3399)9.6",
          "Right Lane Change": "75.5(cid:3399)1.3 \n76.2(cid:3399)3.7 \n75.2(cid:3399)7.5 \n70.9(cid:3399)7.2 \n86.4(cid:3399)5.6 \n77.7(cid:3399)4.8 \n82.9(cid:3399)8.6 \n90.5(cid:3399)8.2 \n86.1(cid:3399)5.7 \n89.8(cid:3399)11.2 \n92.2(cid:3399)9.3 \n90.7(cid:3399)9.4 \n92.5(cid:3399)5.3 \n93.5(cid:3399)4.3 \n92.9(cid:3399)2.4 \n92.6(cid:3399)2.9 \n92.2(cid:3399)8.4 \n92.2(cid:3399)3.8 \n94.4(cid:3399)ùüì. ùüï \n92.5(cid:3399)6.1 \n93.3(cid:3399)4.1",
          "Keep Straight": "90.3(cid:3399)6.3 \n82.6(cid:3399)8.5 \n86.0(cid:3399)5.3 \n72.0(cid:3399)8.6 \n39.0(cid:3399)9.2 \n50.3(cid:3399)9.1 \n87.0(cid:3399)13.6 \n73.9(cid:3399)14.2 \n78.4(cid:3399)8.0 \n92.4(cid:3399)4.4 \n85.1(cid:3399)13.6 \n88.0(cid:3399)7.7 \n98.2(cid:3399)ùüí. ùüè \n92.2(cid:3399)7.8 \n94.8(cid:3399)3.6 \n93.4(cid:3399)7.1 \n94.5(cid:3399)8.7 \n93.5(cid:3399)3.4 \n92.7(cid:3399)7.7 \n98.9(cid:3399)2.3 \n95.6(cid:3399)5.1",
          "General \nAve (%)": "83.6(cid:3399)4.2 \n69.4(cid:3399)5.3 \n85.6(cid:3399)5.7 \n89.7(cid:3399)6.7 \n92.7(cid:3399)3.4 \n91.7(cid:3399)2.2 \n93.7(cid:3399)5.1"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: and Table 2 below for CranData and Brain4Cars dataset. The models are",
      "data": [
        {
          "Algorithms": "",
          "Left Lane Change": "Pr (%) \nRe (%) \nF1 Score",
          "Right Lane Change": "Pr (%) \nRe (%) \nF1 Score",
          "Keep Straight": "Pr (%) \nRe (%) \nF1 Score",
          "General \nAve (%)": ""
        },
        {
          "Algorithms": "SVM  \nHMM  \nHF-LSTM  \nCRNN-M \nCRNN-IR \nCRNN-M-L \nCRNN-IR-L",
          "Left Lane Change": "71.8(cid:3399)11.6 \n77.6(cid:3399)5.8 \n74.1(cid:3399)7.3 \n76.9(cid:3399)10.6 \n92.8(cid:3399)5.0 \n83.8(cid:3399)7.3 \n84.4(cid:3399)8.9 \n90.1(cid:3399)7.3 \n86.7(cid:3399)2.9 \n87.2(cid:3399)9.2 \n90.8(cid:3399)3.2 \n88.7(cid:3399)4.7 \n92.8(cid:3399)ùüí. ùüï \n93.2(cid:3399)8.9 \n84.3(cid:3399).84 \n87.4(cid:3399)7.5 \n89.0(cid:3399)3.7 \n88.1(cid:3399)4.6 \n86.3(cid:3399)8.0 \n92.9(cid:3399)9.1 \n88.9(cid:3399)2.3",
          "Right Lane Change": "83.7(cid:3399)5.0 \n76.1(cid:3399)11.8 \n79.1(cid:3399)5.4 \n79.7(cid:3399)10.0 \n91.6(cid:3399)6.0 \n85.1(cid:3399)8.2 \n86.9(cid:3399)2.0 \n85.4(cid:3399)10.3 \n85.8(cid:3399)4.8 \n87.8(cid:3399)10.8 \n93.5(cid:3399)6.6 \n89.9(cid:3399)3.7 \n90.5 (cid:3399) 7.4 \n95.7(cid:3399)ùüí. ùüé \n90.2(cid:3399)2.7 \n88.4(cid:3399)10.2 \n94.9(cid:3399)4.9 \n91.3(cid:3399)7.2 \n91.5(cid:3399)6.5 \n94.2(cid:3399)4.1 \n92.7(cid:3399)3.4",
          "Keep Straight": "63.9(cid:3399)7.4 \n64.3 (cid:3399)15.5 \n63.1(cid:3399)8.4 \n87.7(cid:3399)7.3 \n61(cid:3399)17.8 \n71(cid:3399)14.9 \n84.4(cid:3399)8.3 \n81.3(cid:3399)4.8 \n82.8(cid:3399)5.9 \n89.4(cid:3399)10.3 \n81.1(cid:3399)9.8 \n84.3(cid:3399)4.0 \n85.6(cid:3399)12.8 \n87.6(cid:3399)ùüî. ùüñ \n86.2(cid:3399)8.2 \n88.4(cid:3399)6.4 \n81.5(cid:3399)7.2 \n84.6(cid:3399)5.4 \n91.9(cid:3399)ùüî. ùüí \n82.1(cid:3399)7.5 \n86.4(cid:3399)3.6",
          "General \nAve (%)": "73.8(cid:3399)5.9 \n80.4(cid:3399)8.3 \n84.9(cid:3399)3.7 \n88.2(cid:3399)1.7 \n89.4(cid:3399)4.4 \n88.6(cid:3399)2.7 \n89.4(cid:3399)2.2"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "56\n27.3%": "4\n2.0%",
          "2\n1.0%": "76\n37.1%",
          "1\n0.5%": "0\n0.0%",
          "94.9%\n5.1%": "95.0%\n5.0%"
        },
        {
          "56\n27.3%": "2\n1.0%",
          "2\n1.0%": "4\n2.0%",
          "1\n0.5%": "60\n29.3%",
          "94.9%\n5.1%": "90.9%\n9.1%"
        },
        {
          "56\n27.3%": "90.3%\n9.7%",
          "2\n1.0%": "92.7%\n7.3%",
          "1\n0.5%": "98.4%\n1.6%",
          "94.9%\n5.1%": "93.7%\n6.3%"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "58\n28.3%": "6\n2.9%",
          "4\n2.0%": "76\n37.1%",
          "3\n1.5%": "0\n0.0%",
          "89.2%\n10.8%": "92.7%\n7.3%"
        },
        {
          "58\n28.3%": "1\n0.5%",
          "4\n2.0%": "3\n1.5%",
          "3\n1.5%": "54\n26.3%",
          "89.2%\n10.8%": "93.1%\n6.9%"
        },
        {
          "58\n28.3%": "89.2%\n10.8%",
          "4\n2.0%": "91.6%\n8.4%",
          "3\n1.5%": "94.7%\n5.3%",
          "89.2%\n10.8%": "91.7%\n8.3%"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "67\n27.3%": "2\n0.8%",
          "1\n0.4%": "77\n31.4%",
          "8\n3.3%": "8\n3.3%",
          "88.2%\n11.8%": "88.5%\n11.5%"
        },
        {
          "67\n27.3%": "6\n2.4%",
          "1\n0.4%": "3\n1.2%",
          "8\n3.3%": "73\n29.8%",
          "88.2%\n11.8%": "89.0%\n11.0%"
        },
        {
          "67\n27.3%": "89.3%\n10.7%",
          "1\n0.4%": "95.1%\n4.9%",
          "8\n3.3%": "82.0%\n18.0%",
          "88.2%\n11.8%": "88.6%\n11.4%"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "60\n24.5%": "1\n0.4%",
          "1\n0.4%": "82\n33.5%",
          "10\n4.1%": "7\n2.9%",
          "84.5%\n15.5%": "91.1%\n8.9%"
        },
        {
          "60\n24.5%": "3\n1.2%",
          "1\n0.4%": "4\n1.6%",
          "10\n4.1%": "77\n31.4%",
          "84.5%\n15.5%": "91.7%\n8.3%"
        },
        {
          "60\n24.5%": "93.8%\n6.3%",
          "1\n0.4%": "94.3%\n5.7%",
          "10\n4.1%": "81.9%\n18.1%",
          "84.5%\n15.5%": "89.4%\n10.6%"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: , and Table 4.",
      "data": [
        {
          "Algorithms": "",
          "Neutral Driving": "Pr (%) \nRe (%) \nF1 Score",
          "Emotional Driving": "Pr (%) \nRe (%) \nF1 Score",
          "General \nAve (%)": ""
        },
        {
          "Algorithms": "HOG-HMM \nFacial-HMM \nHOG -LSTM \nFacial -LSTM \nCRNN-M \nCRNN-IR",
          "Neutral Driving": "80.8(cid:3399)1.7 \n87.3(cid:3399)7.8 \n83.8(cid:3399)4.4 \n71.3(cid:3399)13.8 \n72.7(cid:3399)19.3 \n70.1(cid:3399)12.7 \n79.0(cid:3399)9.8 \n85.3(cid:3399)3.8 \n81.7(cid:3399)5.6 \n55.3(cid:3399)2.2 \n92.3(cid:3399)13.3 \n66.1(cid:3399)14.2 \n96.9(cid:3399)2.0 \n97.4(cid:3399)4.2 \n97.1(cid:3399)ùüê. ùüê \n94.6(cid:3399)4.2 \n97.5(cid:3399)2.5 \n96.0(cid:3399)3.2",
          "Emotional Driving": "47.9(cid:3399)17.3 \n32.0(cid:3399)4.5 \n37.3(cid:3399)5.8 \n72.7(cid:3399)19.5 \n66.0(cid:3399)20.7 \n66.5(cid:3399)14.0 \n34.5(cid:3399)12.0 \n29.7(cid:3399)22.2 \n30.4(cid:3399)17.0 \n83.3(cid:3399)28.8 \n22.9(cid:3399)13.5 \n28.1(cid:3399)19.2 \n93.3(cid:3399)10.9 \n88.7(cid:3399)ùüó. ùüê \n90.4(cid:3399)6.4 \n88.9(cid:3399)13.6 \n79.8(cid:3399)18.5 \n83.3(cid:3399)16.0",
          "General \nAve (%)": "74.4(cid:3399)5.9 \n69.5(cid:3399)12.4 \n71.2(cid:3399)8.2 \n51.6(cid:3399)14.1 \n95.6(cid:3399)5.8 \n93.7(cid:3399)9.7"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: , and Table 4.",
      "data": [
        {
          "Algorithms": "",
          "Neutral Driving": "Pr (%) \nRe (%) \nF1 Score",
          "Emotional Driving": "Pr (%) \nRe (%) \nF1 Score",
          "General \nAve (%)": ""
        },
        {
          "Algorithms": "HOG-HMM \nFacial-HMM \nHOG -LSTM \nFacial -LSTM \nCRNN-M \nCRNN-IR",
          "Neutral Driving": "91.7(cid:3399)2.9 \n96.3(cid:3399)2.7 \n93.9(cid:3399)1.5 \n71.9(cid:3399)5.7 \n4.9(cid:3399)12.7 \n57.2(cid:3399)8.0 \n91.9(cid:3399)4.3 \n92.8(cid:3399)3.3 \n92.3(cid:3399)2.6 \n70.1(cid:3399)14.1 \n54.7(cid:3399)13.4 \n60.8(cid:3399)11.6 \n99.1(cid:3399)ùüê. ùüé \n98.6(cid:3399)1.3 \n98.8(cid:3399)0.8 \n98.2(cid:3399)2.9 \n99.0(cid:3399)1.3 \n98.6(cid:3399)1.5",
          "Emotional Driving": "78.2(cid:3399)12.9 \n57.8(cid:3399)16.5 \n64.9(cid:3399)11.9 \n61.5(cid:3399)3.9 \n80.0(cid:3399)9.3 \n69.1(cid:3399)2.6 \n58.6(cid:3399)11.4 \n56.2(cid:3399)18.0 \n56.0(cid:3399)10.6 \n56.4(cid:3399)15.7 \n72.0(cid:3399)14.1 \n62.5(cid:3399)13.2 \n92.1(cid:3399)7.4 \n93.3(cid:3399)14.9 \n91.8(cid:3399)7.3 \n94.9(cid:3399)7.0 \n88.6(cid:3399)18.6 \n90.5(cid:3399)11.5",
          "General \nAve (%)": "89.6(cid:3399)2.6 \n64.4(cid:3399)3.0 \n86.9(cid:3399)4.2 \n62.5(cid:3399)10.8 \n98.0(cid:3399)5.6 \n97.6(cid:3399)7.2"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: , and Table 4.",
      "data": [
        {
          "156\n76.1%": "4\n2.0%",
          "9\n4.4%": "36\n17.6%",
          "94.5%\n5.5%": "90.0%\n10.0%"
        },
        {
          "156\n76.1%": "97.5%\n2.5%",
          "9\n4.4%": "80.0%\n20.0%",
          "94.5%\n5.5%": "93.7%\n6.3%"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: , and Table 4.",
      "data": [
        {
          "151\n73.7%": "4\n2.0%",
          "5\n2.4%": "45\n22.0%",
          "96.8%\n3.2%": "91.8%\n8.2%"
        },
        {
          "151\n73.7%": "97.4%\n2.6%",
          "5\n2.4%": "90.0%\n10.0%",
          "96.8%\n3.2%": "95.6%\n4.4%"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: , and Table 4.",
      "data": [
        {
          "212\n86.5%": "2\n0.8%",
          "4\n1.6%": "27\n11.0%",
          "98.1%\n1.9%": "93.1%\n6.9%"
        },
        {
          "212\n86.5%": "99.1%\n0.9%",
          "4\n1.6%": "87.1%\n12.9%",
          "98.1%\n1.9%": "97.6%\n2.4%"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: , and Table 4.",
      "data": [
        {
          "208\n84.9%": "3\n1.2%",
          "2\n0.8%": "32\n13.1%",
          "99.0%\n1.0%": "91.4%\n8.6%"
        },
        {
          "208\n84.9%": "98.6%\n1.4%",
          "2\n0.8%": "94.1%\n5.9%",
          "99.0%\n1.0%": "98.0%\n2.0%"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Human-centered intelligent vehicles: Toward multimodal interface integration",
      "authors": [
        "Massimo Cellario"
      ],
      "year": "2001",
      "venue": "IEEE intelligent systems"
    },
    {
      "citation_id": "2",
      "title": "Human-centered autonomous vehicle systems: Principles of effective shared autonomy",
      "authors": [
        "Lex Fridman"
      ],
      "year": "2018",
      "venue": "Human-centered autonomous vehicle systems: Principles of effective shared autonomy",
      "arxiv": "arXiv:1810.01835"
    },
    {
      "citation_id": "3",
      "title": "Driver-automation cooperative approach for shared steering control under multiple system constraints: Design and experiments",
      "authors": [
        "Anh Nguyen",
        "Chouki Tu",
        "Jean-Christophe Sentouh",
        "Popieul"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Industrial Electronics"
    },
    {
      "citation_id": "4",
      "title": "Analysis of driver behavior for joint human-machine systems design of intelligent driving system",
      "authors": [
        "Tsukasa Shimizu",
        "Masayuki Okuwa",
        "Pongsathorn Raksincharoensak"
      ],
      "year": "2015",
      "venue": "FAST-zero'15: 3rd International Symposium on Future Active Safety Technology Toward zero traffic accidents"
    },
    {
      "citation_id": "5",
      "title": "Societal and individual acceptance of autonomous driving",
      "authors": [
        "Eva Fraedrich",
        "Barbara Lenz"
      ],
      "year": "2016",
      "venue": "Societal and individual acceptance of autonomous driving"
    },
    {
      "citation_id": "6",
      "title": "Field experiments on longitudinal characteristics of human driver behavior following an autonomous vehicle",
      "authors": [
        "Xiangmo Zhao"
      ],
      "year": "2020",
      "venue": "Transportation Research Part C: Emerging Technologies"
    },
    {
      "citation_id": "7",
      "title": "An Ensemble Deep Learning Approach for Driver Lane Change Intention Inference",
      "authors": [
        "Yang Xing"
      ],
      "year": "2020",
      "venue": "Transportation Research Part C: Emerging Technologies"
    },
    {
      "citation_id": "8",
      "title": "A critical view of driver behavior models: what do we know, what should we do?",
      "authors": [
        "John Michon"
      ],
      "year": "1985",
      "venue": "A critical view of driver behavior models: what do we know, what should we do?"
    },
    {
      "citation_id": "9",
      "title": "Driver Lane Change Intention Inference for Intelligent Vehicles: Framework, Survey, and Challenges",
      "authors": [
        "Yang Xing"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Vehicular Technology"
    },
    {
      "citation_id": "10",
      "title": "Towards real-time recognition of driver intentions",
      "authors": [
        "Andrew Liu",
        "Alex Pentland"
      ],
      "year": "1997",
      "venue": "Proceedings of Conference on Intelligent Transportation Systems"
    },
    {
      "citation_id": "11",
      "title": "Lane change intent analysis using robust operators and sparse bayesian learning",
      "authors": [
        "Joel Mccall"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "12",
      "title": "On the roles of eye gaze and head dynamics in predicting driver's intent to change lanes",
      "authors": [
        "Anup Doshi",
        "Mohan Trivedi"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "13",
      "title": "Estimating Driver's Lane-Change Intent Considering Driving Style and Contextual Traffic",
      "authors": [
        "Xiaohan Li",
        "Wenshuo Wang",
        "Matthias Roetting"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "14",
      "title": "Naturalistic driver intention and path prediction using recurrent neural networks",
      "authors": [
        "Alex Zyner",
        "Stewart Worrall",
        "Eduardo Nebot"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "15",
      "title": "Car that knows before you do: Anticipating maneuvers via learning temporal driving models",
      "authors": [
        "Ashesh Jain"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "16",
      "title": "Handbook of emotions",
      "authors": [
        "Lewis",
        "Jeannette Michael",
        "Lisa Haviland-Jones",
        "Barrett"
      ],
      "year": "2010",
      "venue": "Handbook of emotions"
    },
    {
      "citation_id": "17",
      "title": "Are Happy Drivers Better Drivers? The Impact of Emotion, Life Stress and Mental Health Issues on Driving Performance and Safety",
      "authors": [
        "Mitchell Cunningham",
        "Michael Regan"
      ],
      "year": "2017",
      "venue": "Australasian Road Safety Conference"
    },
    {
      "citation_id": "18",
      "title": "A wearable system for the affective monitoring of car racing drivers during simulated conditions",
      "authors": [
        "C Katsis"
      ],
      "year": "2011",
      "venue": "Transportation research part C: emerging technologies"
    },
    {
      "citation_id": "19",
      "title": "Multimodal and multiresolution depression detection from speech and facial landmark features",
      "authors": [
        "Md Nasir"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "20",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "Tengfei Song"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Analysis of EEG signals and facial expressions for continuous emotion detection",
      "authors": [
        "Mohammad Soleymani"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion analysis: Exploring the role of context",
      "authors": [
        "Ashish Tawari",
        "Mohan Trivedi"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on multimedia"
    },
    {
      "citation_id": "23",
      "title": "Cnn based subject-independent driver emotion recognition system involving physiological signals for adas",
      "authors": [
        "Mouhannad Ali"
      ],
      "year": "2016",
      "venue": "Cnn based subject-independent driver emotion recognition system involving physiological signals for adas"
    },
    {
      "citation_id": "24",
      "title": "The MPI facial expression database-a validated database of emotional and conversational facial expressions",
      "authors": [
        "Kathrin Kaulard"
      ],
      "year": "2012",
      "venue": "PloS one"
    },
    {
      "citation_id": "25",
      "title": "Framework for reliable, real-time facial expression recognition for low resolution images",
      "authors": [
        "Rizwan Khan",
        "Ahmed"
      ],
      "year": "2013",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "26",
      "title": "Real-time mobile facial expression recognition system-a case study",
      "authors": [
        "Myunghoon Suk",
        "Balakrishnan Prabhakaran"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "27",
      "title": "Deep structured learning for facial action unit intensity estimation",
      "authors": [
        "Robert Walecki"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Deep region and multi-label learning for facial action unit detection",
      "authors": [
        "Kaili Zhao",
        "Wen-Sheng Chu",
        "Honggang Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "29",
      "title": "Facial expression recognition using enhanced deep 3D convolutional neural networks",
      "authors": [
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "30",
      "title": "Spatio-Temporal Encoder-Decoder Fully Convolutional Network for Video-based Dimensional Emotion Recognition",
      "authors": [
        "Zhengyin Du"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Label-less learning for emotion cognition",
      "authors": [
        "Min Chen",
        "Yixue Hao"
      ],
      "year": "2019",
      "venue": "Label-less learning for emotion cognition"
    },
    {
      "citation_id": "32",
      "title": "A deep learning approach to real-time parking occupancy prediction in transportation networks incorporating multiple spatio-temporal data sources",
      "authors": [
        "Shuguan Yang"
      ],
      "year": "2019",
      "venue": "Transportation Research Part C: Emerging Technologies"
    },
    {
      "citation_id": "33",
      "title": "Cnn-rnn: A unified framework for multi-label image classification",
      "authors": [
        "Jiang Wang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "34",
      "title": "Long-term recurrent convolutional networks for visual recognition and description",
      "authors": [
        "Jeffrey Donahue"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "35",
      "title": "[POSTER] A Probabilistic Combination of CNN and RNN Estimates for Hand Gesture Based Interaction in Car",
      "authors": [
        "Aditya Tewari"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Symposium on Mixed and Augmented Reality"
    },
    {
      "citation_id": "36",
      "title": "Convolutional LSTM network: A machine learning approach for precipitation nowcasting",
      "authors": [
        "S Xingjian"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "37",
      "title": "A hybrid deep learning based traffic flow prediction method and its understanding",
      "authors": [
        "Yuankai Wu"
      ],
      "year": "2018",
      "venue": "Transportation Research Part C: Emerging Technologies"
    },
    {
      "citation_id": "38",
      "title": "A survey on multi-task learning",
      "authors": [
        "Yu Zhang",
        "Qiang Yang"
      ],
      "year": "2017",
      "venue": "A survey on multi-task learning",
      "arxiv": "arXiv:1707.08114"
    },
    {
      "citation_id": "39",
      "title": "Multinet: Real-time joint semantic reasoning for autonomous driving",
      "authors": [
        "Marvin Teichmann"
      ],
      "year": "2018",
      "venue": "IEEE Intelligent Vehicles Symposium (IV)"
    },
    {
      "citation_id": "40",
      "title": "Multiview Multitask Gaze Estimation With Deep Convolutional Neural Networks",
      "authors": [
        "Dongze Lian"
      ],
      "year": "2018",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "41",
      "title": "Multi-task, multi-label and multi-domain learning with residual convolutional networks for emotion recognition",
      "authors": [
        "Gerard Pons",
        "David Masip"
      ],
      "year": "2018",
      "venue": "Multi-task, multi-label and multi-domain learning with residual convolutional networks for emotion recognition",
      "arxiv": "arXiv:1802.06664"
    },
    {
      "citation_id": "42",
      "title": "Multi-task sequence to sequence learning",
      "authors": [
        "Minh Luong",
        "Thang"
      ],
      "year": "2015",
      "venue": "Multi-task sequence to sequence learning",
      "arxiv": "arXiv:1511.06114"
    },
    {
      "citation_id": "43",
      "title": "MobileNetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "Mark Sandler"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "44",
      "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
      "authors": [
        "Christian Szegedy"
      ],
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "45",
      "title": "Brain4cars: Car that knows before you do via sensory-fusion deep learning architecture",
      "authors": [
        "Ashesh Jain"
      ],
      "year": "2016",
      "venue": "Brain4cars: Car that knows before you do via sensory-fusion deep learning architecture",
      "arxiv": "arXiv:1601.00740"
    },
    {
      "citation_id": "46",
      "title": "Bidirectional recurrent neural networks",
      "authors": [
        "Mike Schuster",
        "Kuldip Paliwal"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "J√ºrgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "48",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng"
      ],
      "year": "2009",
      "venue": "Imagenet: A large-scale hierarchical image database"
    },
    {
      "citation_id": "49",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "Diederik Kingma",
        "Jimmy Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "50",
      "title": "Visualizing and understanding convolutional networks",
      "authors": [
        "Matthew Zeiler",
        "Rob Fergus"
      ],
      "year": "2014",
      "venue": "Visualizing and understanding convolutional networks"
    },
    {
      "citation_id": "51",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrusaitis"
      ],
      "year": "2018",
      "venue": "13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "52",
      "title": "Relationships between cognitive emotion regulation strategies and depressive symptoms: A comparative study of five specific samples",
      "authors": [
        "Nadia Garnefski",
        "Vivian Kraaij"
      ],
      "year": "2006",
      "venue": "Personality and Individual differences"
    },
    {
      "citation_id": "53",
      "title": "Emotion Detection and Characterization using Facial Features",
      "authors": [
        "Charvi Jain"
      ],
      "year": "2018",
      "venue": "2018 3rd International Conference and Workshops on Recent Advances and Innovations in Engineering (ICRAIE)"
    },
    {
      "citation_id": "54",
      "title": "A Multimodal Emotion Recognition System Using Facial Landmark Analysis",
      "authors": [
        "Farhad Rahdari",
        "Esmat Rashedi",
        "Mahdi Eftekhari"
      ],
      "year": "2019",
      "venue": "Iranian Journal of Science and Technology"
    },
    {
      "citation_id": "55",
      "title": "Modelling lane changing behaviour in approaches to roadworks: Contrasting and combining driving simulator data with stated choice data",
      "authors": [
        "Stephane Hess"
      ],
      "year": "2020",
      "venue": "Transportation Research Part C: Emerging Technologies"
    },
    {
      "citation_id": "56",
      "title": "Action and event recognition in videos by learning from heterogeneous web sources",
      "authors": [
        "Li Niu"
      ],
      "year": "2016",
      "venue": "Action and event recognition in videos by learning from heterogeneous web sources"
    }
  ]
}