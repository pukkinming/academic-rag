{
  "paper_id": "2307.06090v3",
  "title": "Can Large Language Models Aid In Annotating Speech Emotional Data? Uncovering New Frontiers",
  "published": "2023-07-12T11:27:40Z",
  "authors": [
    "Siddique Latif",
    "Muhammad Usama",
    "Mohammad Ibrahim Malik",
    "Björn W. Schuller"
  ],
  "keywords": [
    "Speech emotion recognition",
    "data annotation",
    "data augmentation",
    "large language models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Despite recent advancements in speech emotion recognition (SER) models, state-of-the-art deep learning (DL) approaches face the challenge of the limited availability of annotated data. Large language models (LLMs) have revolutionised our understanding of natural language, introducing emergent properties that broaden comprehension in language, speech, and vision. This paper examines the potential of LLMs to annotate abundant speech data, aiming to enhance the state-of-the-art in SER. We evaluate this capability across various settings using publicly available speech emotion classification datasets. Leveraging ChatGPT, we experimentally demonstrate the promising role of LLMs in speech emotion data annotation. Our evaluation encompasses single-shot and few-shots scenarios, revealing performance variability in SER. Notably, we achieve improved results through data augmentation, incorporating ChatGPTannotated samples into existing datasets. Our work uncovers new frontiers in speech emotion classification, highlighting the increasing significance of LLMs in this field moving forward.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The rapid growth in Natural Language Processing (NLP) has led to the development of advanced conversational tools, often called large language models (LLM)  [1] . These tools are capable of assisting users with various language-related tasks, such as question answering, semantic parsing, proverbs and grammar correction, arithmetic, code completion, general knowledge, reading comprehensions, summarisation, logical inferencing, common sense reasoning, pattern recognition, translation, dialogues, joke explanation, educational content, and language understanding  [1] . LLMs are trained on an enormous amount of general-purpose data and human-feedbackenabled reinforcement learning. A new field of study called \"Foundational Models\" has emerged from these LLMs, highlighting the interest of the academic community and computing industry  [2] . The foundational models have demonstrated the ability to perform tasks for which they were not explicitly trained. This ability, known as emergence, is considered an early spark of artificial general intelligence (AGI)  [3] . The emergence properties of the foundational models have sparked a wide range of testing of these models for various tasks, such as sentiment analysis, critical thinking skills, lowresource language learning and translation, sarcasm and joke understanding, classification, and other affective computing challenges.\n\nSpeech emotion recognition (SER) is a fundamental problem in affective computing. The need for SER has evolved rapidly with the rapid integration of modern technologies in every aspect of our lives. SER systems are designed to understand the wide range of human emotions from the given input data (audio, video, text, or physiological signal) using traditional and modern machine learning (ML) techniques  [4] ,  [5] . However, the availability of larger annotated data remains a challenging aspect for speech emotion recognition (SER) systems, which prompts the need for further investigation and exploration of new methods.\n\nThe use of crowd-sourced and expert intelligence for data annotation is a common practice. The annotated data serves as the ground truth for ML models to learn and generate predictions. This annotation policy is mostly opted in computational social science (sentiment analysis, bot detection, stance detection, emotion classification, etc.), human emotion understanding, and image classification  [6] ,  [7] . However, these strategies are prone to a variety of biases, ranging from human biases to situational biases  [8] ,  [9] . These annotation techniques also necessitate a big pool of human annotators, clear and straightforward annotator instructions, and a verification rationale that is not always available or dependable  [10] . Although there are a few unsupervised techniques for data annotations, these techniques necessitate a high sample size of the data; unfortunately, the generated annotations do not embed the context  [11] .\n\nAnnotating speech emotion data is a doubly challenging process. The annotators listen to a speech recording and assign an annotation to a data sample using the pre-defined criteria. Human emotions are highly context-dependent, and annotating emotions based on a brief recording in a specific controlled situation might restrict the annotations' accuracy. Though the state-of-the-art on human-annotated emotion classification is strong, the generalisability of the learning for unseen data with slightly different circumstances might stymie the SER system's effectiveness. The recent availability of several LLMs (ChatGPT, Google Bard, etc.) has unearthed the possibility of replacing or assisting human annotators. LLMs are trained on enormous text corpora, allowing them to learn and grasp complicated language patterns. Their emergence property  [12]  makes them well-suited for data annotations and various studies (e. g.,  [13] ,  [14] ) explored LLMs for annotations of various natural language processing (NLP) tasks. However, none of the studies explores them to annotate speech emotion data based on the transcripts.\n\nIn this paper, we present an evaluation of the effectiveness This work has been accepted to the IEEE Computational Intelligence Magazine for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This is the initial version. The updated version can be downloaded from IEEE.\n\nof large language models (LLMs) in annotating speech data for SER. We performed a series of experiments to show the effectiveness of ChatGPT for data annotation. However, we observed that annotations solely based on text lacked generalisation to speech emotion data due to the absence of audio context. To address this limitation, we propose a novel pipeline that incorporates audio features such as average energy, pitch, and gender information to provide essential audio context for accurate sample annotation. Furthermore, we introduce a method for encoding speech into a fixedlength discrete feature representation using a Vector Quantised Variational Autoencoder (VQ-VAE)  [15] , which serves as the audio context in the annotation prompt. To the best of our knowledge, this is the first endeavour to leverage LLMs for annotating speech emotion data, specifically for classification purposes, and evaluating their performance. We conduct a comparative analysis between LLM-based data annotations and human data annotations using publicly available datasets, including IEMOCAP and MSP-IMPROV.\n\nIn the following section, we provide a brief literature review on the use of LLMs for data annotation. We highlight the gap between conventional annotations and annotations made with LLMs. Section III covers the methodology used in this study, Section IV presents the initial results and compares the performance of various LLMs for speech emotion data annotation, Section V provides a detailed discussion of the results and limitations, and Section VI concludes the paper with the potential to extend this work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "This section provides an overview of the research on leveraging fundamental models such as LLMs for data annotation  [16] . Data annotations are critical for developing ML models capable of uncovering complex patterns in large datasets and pushing the state-of-the-art in a particular domain. Human expert annotators, bulk annotations, semi-supervised annotations, and crowdsourced annotations are all widely used approaches in practice  [17] . These strategies have their pros and cons. Human annotators, for example, can provide high-quality data annotations but are susceptible to challenges such as fairness, bias, subjectivity, high cost and time, label drifting, annotation fatigue and inconsistency, dealing with data ambiguity, and scalability. Bulk annotations are a faster and less expensive technique to create data annotations, but they might result in lower-quality annotations. Semi-supervised annotations combine the benefits of human-expert annotations with bulk annotations for data annotation, but they are complex to implement and have generalisability and robustness difficulties. Although crowdsourcing human intelligence to annotate large datasets is the quickest and most cost-effective option, it can create lower-quality annotations and is more challenging to manage the quality of the annotations.\n\nRecently, a few studies have investigated the efficacy of LLMs (i. e., ChatGPT) for data annotations. The goal of these experiments was to explore the potential of ChatGPT for data annotation and to find out whether ChatGPT can achieve full emergence in downstream tasks such as classification. Zhu et al.  [13]  tested the ability of ChatGPT to reproduce the human-generated annotations for five seminal computational social science datasets. The datasets include stance detection (two datasets), hate speech detection, sentiment analysis, and bot detection. Their results indicate that ChatGPT is capable of annotating the data, but its performance varies depending on the nature of the tasks, the version of ChatGPT, and the prompts. The average re-annotation performance is 60.9% across all five datasets. For the sentiment analysis task, the accuracy of ChatGPT re-annotating the tweets is reported at 64.9%, and for the hate speech task, the ChatGPT performance has gone down to 57.1%. The authors also provided a prompt template that was used for re-annotating the data.\n\nFact-checking is a well-known way to deal with the misinformation epidemic in computational social science. Hose et al.  [18]  evaluated the ability of LLMs, specifically ChatGPT, to assist fact-checkers in expediting misinformation detection. They used ChatGPT as a zero-shot classifier to re-annotate 12,784 human-annotated (\"true claim\", \"false claim\") factchecked statements. ChatGPT was able to correctly re-annotate 72.0% of the statements. The study further suggests that Chat-GPT performs well on recent fact-checked statements with \"true claim\" annotations. Despite the reasonable performance of ChatGPT on fact-checking, it is hard to suggest that it will replace human fact-checkers anytime soon. Yang et al.  [19]  explored the rating of news outlet credibility by formulating the problem as a binary re-annotation task for ChatGPT. Chat-GPT achieved a reasonable performance in re-annotating 7,523 domains with a Spearman correlation coefficient of ρ = 0.54. Tornberg  [20]  also used ChatGPT-4 as a zero-shot classifier for re-annotating 500 political tweets. He found that ChatGPT-4 outperformed experts and crowd annotators in terms of accuracy, reliability, and bias. Gilardi et al.  [21]  reported that ChatGPT used as a zero-shot classifier, outperformed the crowd-works-based text annotations for five text-annotation tasks around content moderation. We have also observed studies using LLMs (ChatGPT) for annotating/re-annotating data for various computational social science tasks such as election opinion mining tasks  [22] , intent classification  [23] , genre identification  [24] , stance detection  [25] , and sentiment analysis  [26] . Several other prominent works that evaluate the application of LLMs in the annotation of computational social science datasets for various applications include  [27] ,  [28] ,  [29] ,  [30] .\n\nAmin et al.  [31]  evaluated the capabilities of ChatGPT in three famous NLP classification tasks in affective computing: personality recognition, suicide tendency prediction, and sentiment analysis. Their results indicated that ChatGPT shows far better performance (in the presence of the noisy data) than Word2Vec models  [32] ; ChatGPT further produces comparable performance with Bag-of-Words (BoW) and Word2Vec models (without noisy data) and was outperformed by a RoBERTa model  [33]  trained for a specific affective computing task. ChatGPT scored an unweighted average recall of 85.5% on the sentiment analysis, outperforming BoW and Word2Vec models by nearly 20.0%. RoBERTa also scored an unweighted average recall of 85.0% on this task. For the suicide tendency prediction task, ChatGPT's performance was This work has been accepted to the IEEE Computational Intelligence Magazine for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This is the initial version. The updated version can be downloaded from IEEE.\n\nthe same as Word2Vec and BoW, with all three models achieving an unweighted average recall of nearly 91.0%. RoBERTa outperformed ChatGPT on this task, achieving an unweighted average recall of 97.4%. For the personality recognition task, RoBERTa performed best, scoring an unweighted average recall of 62.3%. ChatGPT performed the worst on this task, getting an unweighted average recall of 54.0%. Interestingly, Word2Vec and BoW models also performed marginally well when compared to ChatGPT for this task. Wang et al.  [34]  argued that GPT-3 can be a low-cost solution for the data annotations for downstream natural language understanding and generation tasks. This research evaluated the efficacy of augmenting human-annotated data with GPT-3 annotated data for improving the performance (language understanding and generation) in a constrained annotation budget. They tested their method on various language understanding and generation tasks, ranging from sentiment analysis, question answering, summarisation, text retrieval to textual entailment. They found that GPT-3 based annotations policy saved 50.0% to 96.0% cost in annotation tasks. However, they also noted that GPT-3 is not yet as reliable as human annotators in annotating high-stakes sensitive cases. More details on the evaluation of the comparison of ChatGPT with human experts on various NLP tasks are compared and evaluated in  [35] . Huang et al.  [14]  explored the ability of ChatGPT to reproduce annotations and their corresponding natural language explanation. Their results indicate that lay people agreed with the results more when they were provided with the ChatGPT-generated natural language explanation of the annotations than just the considered post itself along with the annotation. ChatGPT agreed with the human-annotated data points 80.0% of the time.\n\nIn contrast to the aforementioned studies, our research explores the untapped potential of LLMs in annotating emotions in speech data. We present a novel approach that incorporates audio context into LLMs to improve the precision of annotations. To our knowledge, no prior research has investigated the utilisation of LLMs for annotating speech emotion data.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In our exploration of emotional data annotation, we conduct a series of experiments. Firstly, we annotate samples using only text, and then we incorporate audio features and gender information alongside textual data for improved annotation. To incorporate audio context, we utilise the average energy and pitch of each utterance and pass it to ChatGPT. Additionally, we propose the use of VQ-VAE to generate a 64-dimensional discrete representation of audio, which is also provided to ChatGPT as the audio context. For speech-emotion classification, we train a bi-directional Long-Short Term Memory (BLSTM)-based classifier. The following section provides further details on our proposed method.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Vq-Vae For Speech Code Generation",
      "text": "We propose to use a Vector-Quantised Variational Autoencoder (VQ-VAE)  [36]  to learn a discrete representation from the speech data. Unlike traditional VAEs where the discrete space is continuous, VQ-VAEs express the latent space as a set of discrete latent codes and the prior is learnt rather than being fixed. As illustrated in Figure  1 . the model is comprised of three main parts: the encoder, the vector quantiser, and the decoder.\n\nThe encoder takes in the input in the form of Melspectrograms and passes it through a series of convolutional layers having a shape of (n, h, w, d) where n is the batch size, h is the height, w is the width and d represents the total number of filters after convolutions. Let us denote the output from the encoder as z e . The vector quantiser component contains an embedding space with k total vectors each with dimension d. The main goal of this component is to output a series of embedding vectors that we call z q . To accomplish this, we first reshape z e in the form of (n * h * w, d) and calculate the distance for each of these vectors with the vectors in the embedding dictionary. For each of the n * h * w vectors, we find the closest of the k vectors from the embedding space and index the closest vector from the embedding space for each n * h * w vector. The discrete indices of each of the vectors in the embedding space are called codes, and we get a unique series of codes for each input to the model. The selected vectors are then reshaped back to match the shape of z e . Finally, the reshaped vector embeddings are passed through a series of transpose convolutions to reconstruct the original input Mel-spectrogram. One problem with this approach is that the process of selecting vectors is not differentiable. To tackle this problem, the authors simply copy the gradients from z q to z e .\n\nThe total loss is composed of three loss elements: the reconstruction loss, the code book loss, and the commitment loss. The reconstruction loss is responsible for optimising the encoder and decoder and is represented by: Reconstruction Loss = -log(p(x|z q )).\n\n(\n\nWe use a code book loss which forces the vector embeddings to move closer to the encoder output z e .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Code Book Loss",
      "text": "where sg is the stop gradient operator, this essentially freezes all gradient flows. e are the vector embeddings and x is the input to the encoder. And finally, for making sure that the encoder commits to an embedding we add a commitment loss.\n\nhere β is a hyperparameter that controls the weight we want to assign to the commitment loss. Overall, we train the VQ-VAE model to represent the audio representation in the form of a discrete list of integers or \"codes\". These audio representations can be used in addition to the transcriptions and fed to ChatGPT for annotation. In the following section, we will delve into the details of the annotation procedure.\n\nThis work has been accepted to the IEEE Computational Intelligence Magazine for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This is the initial version. The updated version can be downloaded from IEEE.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Emotion Label Annotation Using Llms",
      "text": "We evaluated the data annotation ability of ChatGPT with different experiments. We start our experiments by annotating the training data of IEMOCAP by passing the textual transcripts to ChatGPT and annotating the data both in zeroshot and few-shot settings. For a few shots, we randomly selected 10 samples from the training data and passed them to ChatGPT as context. We trained the classifier using the training samples annotated with ChatGPT and unweighted average recall (UAR) is computed. We repeat this procedure of annotation by passing the audio features along with the textual information. First of all, we use average pitch and energy for a given utterance and re-annotated the data both in a zero-shot and a few-shots setting, and classification UAR is measured using a BLSTM based classifier. As the female voice usually has a high pitch and energy, therefore, we also annotated the data by providing the gender information. Finally, we propose to use an audio representation by VQ-VAE (Section III-A) and pass it to ChatGPT as audio context. We then used the OpenAI API with the \"ChatGPT pro\" version to annotate the data. In our approach, we meticulously designed and curated multiple prompts for annotating the data, leveraging ChatGPT for the annotation process. We trained the classifier on the annotated dataset and computed the UAR, considering it as a benchmark for evaluating the classification performance. To improve upon this benchmark, we conducted additional experiments, exploring various prompts to enhance the classification results beyond the established performance level.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Speech Emotion Classifier",
      "text": "In this work, we implement convolutional neural network (CNN)-BLSTM-based classifiers due to their popularity in SER research  [37] . It has been found that the performance of BLSTM can be improved by feeding it with a good emotional representation  [38] . Therefore, we use CNN as emotional feature extractor from the given input data  [39] . A CNN layer acts like data-driven filter banks and can model emotionally salient features. We pass these emotional features to the BLSTM layer to learn contextual information. Emotions in speech are in the temporal dimension, therefore, the BLSTM layer helps model these temporal relationships  [40] . We pass the outputs of BLSTM to an attention layer to aggregate the emotional salient attributes distributed over the given utterance. For a given output sequence h i , utterance level salient attributes are aggregated as follows:\n\nwhere α i represents the attention weights that can be computed as follows:\n\nwhere W is a trainable parameter. The attentive representation R attentive computed by the attention layer is passed to the fully connected layer for emotion classification. Overall, our classifier is jointly empowered by the CNN layers to capture an abstract representation, the BLSTM layer for context capturing, the attention layer for emotional salient attributes aggregation, and the fully connected layer emotion classification.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experimental Setup",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Datasets",
      "text": "To evaluate the effectiveness of annotations by ChatGPT, we use three datasets: IEMOCAP, MSP-IMPROV, and MELD which are commonly used for speech emotion classification research  [41] ,  [42] . Both, the IEMOCAP and the MSP-IMPROV datasets are collected by simulating naturalistic dyadic interactions among professional actors and have similar labelling schemes. MELD contains utterances from the Friends TV series.\n\n1) IEMOCAP: The Interactive Emotional Dyadic Motion Capture (IEMOCAP) database is a multimodal database that contains 12 hours of recorded data  [43] . The recordings were captured during dyadic interactions between five male and five female speakers. The Dyadic interactions enabled the speakers to converse in unrehearsed emotions as opposed to reading from a text. The interactions are almost five minutes long and are segregated into smaller utterances based on sentences, where each utterance is then assigned a label according to the emotion. Overall, the dataset contains nine different emotions. To be consistent with previous studies, we use four emotions including sad (1084), happy (1636), angry (1103), and neutral (1708).\n\n2) MSP-IMPROV: This corpus is a multimodal emotional database recorded from 12 actors performing dyadic interactions  [44] , similar to IEMOCAP  [43] . The utterances in MSP-IMPROV are grouped into six sessions, and each session has recordings of one male and one female actor. The scenarios were carefully designed to promote naturalness while maintaining control over lexical and emotional contents. The emotional labels were collected through perceptual evaluations using crowdsourcing  [45] . The utterances in this corpus are annotated in four categorical emotions: angry, happy, neutral, and sad. To be consistent with previous studies  [39] ,  [46] , we use all utterances with four emotions: anger (792), sad (885), neutral (3477), and happy (2644).\n\nThis work has been accepted to the IEEE Computational Intelligence Magazine for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This is the initial version. The updated version can be downloaded from IEEE.\n\n3) MELD: Multimodal EmotionLines Dataset  [47]  or MELD contains over 1400 dialogues and 13000 utterances and multiple speakers from the popular TV series Friends. The utterances have been labelled from a total of seven emotions: Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear. Furthermore, MELD also contains sentiment annotations for each utterance. To stay consistent with the other datasets we choose four emotions including sadness (1002 samples), neutral (6436 samples), joy and anger (1607 samples). With this configuration, we get a total of 11353 utterances from the dataset.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Speech Features",
      "text": "For utterances across all datasets, we use a consistent sampling rate of 16 kHz. For extracting the audio features we then convert the audio into Mel spectrograms. The Melspectrograms are computed with a short-time Fourier transform of size 1024, a hop size of 256, and a window size of 1024. We specify a total of 80 Mel-bands for the output and cutoff frequency of 8 kHz. We set a cutoff length of 256 for each Mel spectrogram to have a final shape of 80x256, where smaller samples are zero-padded. Finally, the Mel spectrograms are normalised in the range of [-1, 1].",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Hyperparameters",
      "text": "The VQ-VAE was trained using the following parameters: We chose a batch size of 256 and trained for a total of 1000 epochs with a learning rate of 1e -4 . The convolution layers each had a stride and kernel size of 2 and 3, respectively. A total of 8192 token embeddings were selected, where each had a dimensionality of 512. With our particular configuration, we got a total of 64 codes for each given utterance. We pass these codes to ChatGPT along with textual data for annotation. Based on these annotations, we trained over the classifier.\n\nOur classifier consists of convolutional layers and a Bidirectional LSTM (BLSTM)-based classification network. To generate high-level abstract feature representations, we employ two CNN layers. In line with previous studies  [48] ,  [49] , we utilise a larger kernel size for the first convolutional layer and a smaller kernel size for the second layer. The CNN layers learn feature representations, which are then passed to the BLSTM layer with 128 LSTM units for contextual representation learning. Following the BLSTM layer, an attention layer is applied to aggregate the emotional content spread across different parts of the given utterance. The resulting attentive features are then fed into a dense layer with 128 hidden units to extract emotionally discriminative features for a softmax layer. The softmax layer employs the cross-entropy loss function to calculate posterior class probabilities, enabling the network to learn distinct features and perform accurate emotion classification.\n\nIn our experiments, we utilise the Adam optimiser with its default parameters. The training of our models starts with a learning rate of 0.0001, and at the end of each epoch, we assess the validation accuracy. If the validation accuracy fails to improve for five consecutive epochs, we decrease the learning rate by half and revert the model to the best-performing previous epoch. This process continues until the learning rate drops below 0.00001. As for the choice of non-linear activation function, we use the rectified linear unit (ReLU) due to its superior performance compared to leaky ReLU and hyperbolic tangent during the validation phase.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Experiments And Results",
      "text": "All experiments are conducted in a speaker-independent manner to ensure the generalisability of our findings. Specifically, we adopt an easily reproducible and widely used leave-one-speaker-out cross-validation scheme, as commonly employed in related literature  [50] ,  [51] ,  [52] . For crosscorpus SER, we follow  [52] ,  [53]  and use IEMOCAP for training and MSP-IMPROV is used for validation and testing.\n\nFor the experiments, we repeat each experiment ten times and calculate the mean and standard deviation of the results. The performance is presented in terms of the unweighted average recall rate (UAR), a widely accepted metric in the field that more accurately reflects the classification accuracy across multiple emotion categories when the data is in imbalance across these.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Within Corpus Experiments",
      "text": "For the within-corpus experiments, we select the IEMO-CAP data and compare the results with the baseline UAR achieved using actual true labels. We trained the classifier for different settings: (1) true label settings, (2) zero-shot ChatGPT labels, and (3) few-shots ChatGPT labels. In the first experiment, we trained the CNN-BSTM-based classifier on true labels using the well-known above mentioned leaveone-speaker-out scheme  [54] ,  [7] . In the second and third experiments, the classifier is trained in the same leave-onespeaker-out scheme, however, we annotated samples using ChatGPT with our proposed approach. We repeat the second and third experiments using text only and text plus audio context. Results are presented in Figure  2 . Overall, results on data annotated using few shots achieve improved results compared to the zero-shot scenario. It is important to note that the emotion classification performance using training data annotated with only text is poor compared to the baseline. Here, baseline results represent when the classifier is trained This work has been accepted to the IEEE Computational Intelligence Magazine for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This is the initial version. The updated version can be downloaded from IEEE.\n\nusing the original annotations of IEMOCAP. This observation underscores the insufficiency of textual information alone to provide the necessary context for accurate annotation by ChatGPT. Consequently, additional context becomes essential to enable ChatGPT in effectively annotating the data. As previously found, for example, happy and angry voice samples often have high energy and pitch compared to a sad and neutral voice  [55] . Building upon this insight, we incorporated the average energy and pitch values of a given utterance as additional contextual information for ChatGPT during the re-annotation process, both in zero-shot and few-shot settings. However, the performance improvement was not considerable, primarily due to the confounding factor of gender, as female voices typically exhibit higher pitch and energy compared to male voices  [56] .\n\nTo address this limitation, we extended the experiment by providing gender labels to ChatGPT, resulting in improved classification accuracy as illustrated in 2. In addition to average energy, pitch, and gender information, we further proposed the utilisation of audio patterns to provide enhanced audio context for annotation. To achieve this, we employed a VQ-VAE model to encode the given utterance into discrete representations. These representations, along with the textual and other feature inputs, were employed in various experiments for annotation (refer to Figure  2 ). Notably, in the zero-shot scenario, no substantial improvements were observed. However, significant advancements were achieved by incorporating the discrete codes generated by VQ-VAE, in conjunction with average energy, pitch, and gender information.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Cross-Corpus Evaluations",
      "text": "In this experiment, we perform a cross-corpus analysis to assess the generalisability of annotations performed using our proposed approach. Here, we trained models on IEMO-CAP, and testing is performed on the MSP-IMPROV data. IEMOCAP is more blanched data, therefore, we select it for training by following previous studies  [51] ,  [57] ,  [58] . We randomly select 30.0 % of the MSP-IMPROV data for parameter tuning and 70.0 % of data as testing data. We report results using the few-shots annotation by ChatGPT as it consistently demonstrated superior performance compared to the zero-shot setting. We compare our results with different studies in Table  I . In  [53] , the authors use the CNN-LSTM model for cross-corpus evaluation. They show that CNN-LSTM can learn emotional contexts and help achieve improved results for cross-corpus SER. In  [57] , the authors utilise the representations learnt from unlabelled data and feed it to an attention-based CNN classifier. They show that the classifier's performance can be improved by augmenting the classifier with information from unlabelled data. We compare our results using the CNN-BLSTM-based classifier by using the IEMOCAP annotated by the ChatGPT model. This experiment demonstrates the generalisability of annotations performed by ChatGPT in crosscorpus settings. However, it is worth noting that our results did not surpass those of previous studies. In the subsequent experiment, we aim to showcase the potential for enhancing the performance of SER using data annotations generated by ChatGPT, both within-corpus and cross-corpus settings.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Augmentating The Training Data",
      "text": "In the previous two experiments, we showed, how we can annotate new speech-emotional data using a large language model like ChatGPT. However, the performance does not surpass the UAR achieved using actual labels. In this experiment, we aim to address this limitation by showcasing the potential of improving SER performance through data augmentation using our proposed approach. For this, we can utilise abundantly available audio data by annotating with our proposed approach. For instance, data from YouTube can be annotated and used to augment the SER system. To validate this concept, we select the MELD dataset, which consists of dialogue samples from the Friends TV series. We employ the few-shot approach, using samples from the IEMOCAP dataset for few-shots, and annotate the MELD data with four emotions: happy, anger, neutral, and sad. We used samples from IEMOCAP data for the few-shots and annotated MELD data in four emotions including happy, anger, neutral, and sad. Results are presented in Figure  3 , where we compare the results with the CNN-BLSTM classifier using the actual IECMOAP labels and when data is augmented using the samples with ChatGPT labels. This analysis provides insights into the effectiveness of data augmentation for enhancing the performance of the SER system. Furthermore, we provide a comprehensive comparison of our results with previous studies in both within-corpus and cross-corpus settings, as presented in Table  II . In  [59] ,  [60] , the authors utilise DialogueRNN for speech emotion recognition using IEMOCAP data. Peng et al.  [60]  use an attention-based CNN network for emotion classification. We achieve better This work has been accepted to the IEEE Computational Intelligence Magazine for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This is the initial version. The updated version can be downloaded from IEEE. results compared to these studies by augmenting the classifier with additional data annotated by ChatGPT. One possible reason can be that these studies did not train the models with augmentation. However, we also compared the results with  [53] , where the authors use different data augmentation techniques to augment the classifier and achieve improved results. In contrast, we use ChatGPT to annotate the publicly available data and use it for augmentation of the training set. We are achieving considerably improved results compared to  [53] . One possible reason is that we are adding new data in the classifiers' training set, however, authors in  [53]  employed perturbed versions of the same data, which can potentially lead to overfitting of the system. Similarly, we achieve considerably improved results for cross-corpus settings compared to the precious studies  [53] ,  [52] , where the authors augmented their classification models with either synthetic data or perturbed samples using audio-based data augmentation techniques like speed perturbation, SpecAugmet, and mixup. Overall, our results showcase the effectiveness of our approach in achieving superior performance compared to previous studies, both in within-corpus and cross-corpus settings. The utilisation of ChatGPT for data annotation and augmentation proves to be a promising strategy for enhancing SER systems.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "D. Limitations",
      "text": "In this section, we highlight the potential limitations of our work and in general the limitations of LLMs for data annotation. During our experiments, we observed the following limitations:\n\n• We obtained promising results by augmenting the training data with samples annotated using ChatGPT. However, this approach proved ineffective when applied to corpora such as LibriSpeech  [61] , where the recordings lack emotional variations. Although we attempted to utilise LibriSpeech data (results are not shown here), the results were not as promising as those achieved with MELD. As a result, it is a trade-off situation. Therefore, it becomes a trade-off between cost and accuracy. Striking the right balance is crucial when utilising ChatGPT for data annotation to avoid potential inaccuracies in classification performance. Despite the mentioned limitations, we have found ChatGPT to be an invaluable tool for speech-emotion data annotation. We believe that its capabilities will continue to evolve. Currently, generating annotations using ChatGPT and incorporating them to augment human-annotated data has demonstrated improved performance in speech emotion classification. This highlights the potential of ChatGPT as a valuable asset in advancing research in this field.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vi. Conclusions And Outlook",
      "text": "In this paper, we conducted a comprehensive evaluation of ChatGPT's effectiveness in annotating speech emotion data. To the best of our knowledge, this study is the first of its kind to explore the capabilities of ChatGPT in the domain of speech emotion recognition. The results of our investigation have been encouraging, and we have discovered promising outcomes. Below are the key findings of our study:\n\n• Based on our findings, we observed that text-based emotional annotations do not generalise effectively to speech data. To address this limitation, we introduced a novel approach that harnesses the audio context in annotating speech data, leveraging the capabilities of a large language model. By incorporating the audio context, we successfully enhanced the performance of SER, yielding improved results compared to the text-based approach. • We observed that the quality of annotations by ChatGPT considerably improved when using a few-shot approach compared to a zero-shot one. By incorporating a small number of annotated samples, we were able to achieve improved results in our evaluation. • We introduced an effective technique to utilise large language models (LLMs) to augment the speech emotion\n\nThis work has been accepted to the IEEE Computational Intelligence Magazine for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This is the initial version. The updated version can be downloaded from IEEE.\n\nrecognition (SER) system with the annotated data by ChatGPT. The augmented system yielded improved results compared to the current state-of-the-art SER systems that utilise conventional augmentation techniques.\n\nIn our future work, we aim to expand our experimentation by applying our approach to new datasets and diverse contexts. This will allow us to further validate the effectiveness and generalisability of our proposed technique. Additionally, we plan to explore and compare the annotation abilities of different LLMs for speech emotion data, enabling us to gain insights into their respective strengths and weaknesses. We also intend to use LLMs in the training pipeline of the SER system.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: the model is comprised",
      "page": 3
    },
    {
      "caption": "Figure 1: Model Diagram of the VQ-VAE",
      "page": 4
    },
    {
      "caption": "Figure 2: Overall, results",
      "page": 5
    },
    {
      "caption": "Figure 2: Comparing the classification performance (UAR %) using",
      "page": 5
    },
    {
      "caption": "Figure 2: ). Notably, in the zero-shot scenario, no",
      "page": 6
    },
    {
      "caption": "Figure 3: , where we compare",
      "page": 6
    },
    {
      "caption": "Figure 3: Comparing the classier performance (UAR %) with data",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model\nUAR (%)": "within corpus"
        },
        {
          "Model\nUAR (%)": "DialogueRNN [59]\n(2019)\n63.40"
        },
        {
          "Model\nUAR (%)": "CNN-attention [60]\n(2021)\n65.4"
        },
        {
          "Model\nUAR (%)": "CNN-BLSTM (+ augmentation)\n(2022)\n[53]\n65.1±1.8"
        },
        {
          "Model\nUAR (%)": "68.0± 1.4\nOur work (+ augmentations)\n(2023)"
        },
        {
          "Model\nUAR (%)": "cross-corpus"
        },
        {
          "Model\nUAR (%)": "Cyclegan-DNN [52]\n(+ augmentations)\n(2019)\n46.52±0.43"
        },
        {
          "Model\nUAR (%)": "CNN-BLSTM (+ augmentations)\n[53]\n(2022)\n46.2 ± 1.3"
        },
        {
          "Model\nUAR (%)": "48.1± 0.9\nOur work (+ augmentations)\n(2023)"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "2",
      "title": "On the opportunities and risks of foundation models",
      "authors": [
        "R Bommasani",
        "D Hudson",
        "E Adeli",
        "R Altman",
        "S Arora",
        "S Arx",
        "M Bernstein",
        "J Bohg",
        "A Bosselut",
        "E Brunskill"
      ],
      "year": "2021",
      "venue": "On the opportunities and risks of foundation models",
      "arxiv": "arXiv:2108.07258"
    },
    {
      "citation_id": "3",
      "title": "Emergent abilities of large language models",
      "authors": [
        "J Wei",
        "Y Tay",
        "R Bommasani",
        "C Raffel",
        "B Zoph",
        "S Borgeaud",
        "D Yogatama",
        "M Bosma",
        "D Zhou",
        "D Metzler"
      ],
      "year": "2022",
      "venue": "Emergent abilities of large language models",
      "arxiv": "arXiv:2206.07682"
    },
    {
      "citation_id": "4",
      "title": "Transformers in speech processing: A survey",
      "authors": [
        "S Latif",
        "A Zaidi",
        "H Cuayahuitl",
        "F Shamshad",
        "M Shoukat",
        "J Qadir"
      ],
      "year": "2023",
      "venue": "Transformers in speech processing: A survey",
      "arxiv": "arXiv:2303.11607"
    },
    {
      "citation_id": "5",
      "title": "Deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif"
      ],
      "year": "2022",
      "venue": "Deep representation learning for speech emotion recognition"
    },
    {
      "citation_id": "6",
      "title": "Computation and social science",
      "authors": [
        "C Cioffi-Revilla",
        "C Cioffi-Revilla"
      ],
      "year": "2017",
      "venue": "Introduction to computational social science: Principles and applications"
    },
    {
      "citation_id": "7",
      "title": "Deep representation learning in speech processing: Challenges, recent advances, and future trends",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Deep representation learning in speech processing: Challenges, recent advances, and future trends",
      "arxiv": "arXiv:2001.00378"
    },
    {
      "citation_id": "8",
      "title": "Ai-based emotion recognition: Promise, peril, and prescriptions for prosocial path",
      "authors": [
        "S Latif",
        "H Ali",
        "M Usama",
        "R Rana",
        "B Schuller",
        "J Qadir"
      ],
      "year": "2022",
      "venue": "Ai-based emotion recognition: Promise, peril, and prescriptions for prosocial path",
      "arxiv": "arXiv:2211.07290"
    },
    {
      "citation_id": "9",
      "title": "Caveat emptor: the risks of using big data for human development",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usama",
        "J Qadir",
        "A Zwitter",
        "M Shahzad"
      ],
      "year": "2019",
      "venue": "Ieee technology and society magazine"
    },
    {
      "citation_id": "10",
      "title": "Two contrasting data annotation paradigms for subjective nlp tasks",
      "authors": [
        "P Röttger",
        "B Vidgen",
        "D Hovy",
        "J Pierrehumbert"
      ],
      "year": "2021",
      "venue": "Two contrasting data annotation paradigms for subjective nlp tasks",
      "arxiv": "arXiv:2112.07475"
    },
    {
      "citation_id": "11",
      "title": "Unsupervised approaches for textual semantic annotation, a survey",
      "authors": [
        "X Liao",
        "Z Zhao"
      ],
      "year": "2019",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "12",
      "title": "Discovering latent knowledge in language models without supervision",
      "authors": [
        "C Burns",
        "H Ye",
        "D Klein",
        "J Steinhardt"
      ],
      "year": "2022",
      "venue": "Discovering latent knowledge in language models without supervision",
      "arxiv": "arXiv:2212.03827"
    },
    {
      "citation_id": "13",
      "title": "Can chatgpt reproduce human-generated labels? a study of social computing tasks",
      "authors": [
        "Y Zhu",
        "P Zhang",
        "E.-U Haq",
        "P Hui",
        "G Tyson"
      ],
      "year": "2023",
      "venue": "Can chatgpt reproduce human-generated labels? a study of social computing tasks",
      "arxiv": "arXiv:2304.10145"
    },
    {
      "citation_id": "14",
      "title": "Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech",
      "authors": [
        "F Huang",
        "H Kwak",
        "J An"
      ],
      "year": "2023",
      "venue": "Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech",
      "arxiv": "arXiv:2302.07736"
    },
    {
      "citation_id": "15",
      "title": "Group latent embedding for vector quantized variational autoencoder in non-parallel voice conversion",
      "authors": [
        "S Ding",
        "R Gutierrez-Osuna"
      ],
      "year": "2019",
      "venue": "Group latent embedding for vector quantized variational autoencoder in non-parallel voice conversion"
    },
    {
      "citation_id": "16",
      "title": "Harnessing the power of llms in practice: A survey on chatgpt and beyond",
      "authors": [
        "J Yang",
        "H Jin",
        "R Tang",
        "X Han",
        "Q Feng",
        "H Jiang",
        "B Yin",
        "X Hu"
      ],
      "year": "2023",
      "venue": "Harnessing the power of llms in practice: A survey on chatgpt and beyond",
      "arxiv": "arXiv:2304.13712"
    },
    {
      "citation_id": "17",
      "title": "Natural Language Annotation for Machine Learning: A guide to corpus-building for applications",
      "authors": [
        "J Pustejovsky",
        "A Stubbs"
      ],
      "year": "2012",
      "venue": "Natural Language Annotation for Machine Learning: A guide to corpus-building for applications"
    },
    {
      "citation_id": "18",
      "title": "Using chatgpt to fight misinformation: Chatgpt nails 72% of 12,000 verified claims",
      "authors": [
        "E Hoes",
        "S Altay",
        "J Bermeo"
      ],
      "year": "2023",
      "venue": "Using chatgpt to fight misinformation: Chatgpt nails 72% of 12,000 verified claims"
    },
    {
      "citation_id": "19",
      "title": "Large language models can rate news outlet credibility",
      "authors": [
        "K.-C Yang",
        "F Menczer"
      ],
      "year": "2023",
      "venue": "Large language models can rate news outlet credibility",
      "arxiv": "arXiv:2304.00228"
    },
    {
      "citation_id": "20",
      "title": "Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning",
      "authors": [
        "P Törnberg"
      ],
      "year": "2023",
      "venue": "Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning",
      "arxiv": "arXiv:2304.06588"
    },
    {
      "citation_id": "21",
      "title": "Chatgpt outperforms crowdworkers for text-annotation tasks",
      "authors": [
        "F Gilardi",
        "M Alizadeh",
        "M Kubli"
      ],
      "year": "2023",
      "venue": "Chatgpt outperforms crowdworkers for text-annotation tasks",
      "arxiv": "arXiv:2303.15056"
    },
    {
      "citation_id": "22",
      "title": "Opinion mining from youtube captions using chatgpt: A case study of street interviews polling the 2023 turkish elections",
      "authors": [
        "T Elmas",
        "İ Gül"
      ],
      "year": "2023",
      "venue": "Opinion mining from youtube captions using chatgpt: A case study of street interviews polling the 2023 turkish elections",
      "arxiv": "arXiv:2304.03434"
    },
    {
      "citation_id": "23",
      "title": "Chatgpt to replace crowdsourcing of paraphrases for intent classification: Higher diversity and comparable model robustness",
      "authors": [
        "J Cegin",
        "J Simko",
        "P Brusilovsky"
      ],
      "year": "2023",
      "venue": "Chatgpt to replace crowdsourcing of paraphrases for intent classification: Higher diversity and comparable model robustness",
      "arxiv": "arXiv:2305.12947"
    },
    {
      "citation_id": "24",
      "title": "Chatgpt: Beginning of an end of manual linguistic data annotation? use case of automatic genre identification",
      "authors": [
        "T Kuzman",
        "I Mozetic",
        "N Ljubešic"
      ],
      "year": "2023",
      "venue": "Chatgpt: Beginning of an end of manual linguistic data annotation? use case of automatic genre identification"
    },
    {
      "citation_id": "25",
      "title": "Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media",
      "authors": [
        "M Mets",
        "A Karjus",
        "I Ibrus",
        "M Schich"
      ],
      "year": "2023",
      "venue": "Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media",
      "arxiv": "arXiv:2305.13047"
    },
    {
      "citation_id": "26",
      "title": "Is chatgpt a good sentiment analyzer? a preliminary study",
      "authors": [
        "Z Wang",
        "Q Xie",
        "Z Ding",
        "Y Feng",
        "R Xia"
      ],
      "year": "2023",
      "venue": "Is chatgpt a good sentiment analyzer? a preliminary study",
      "arxiv": "arXiv:2304.04339"
    },
    {
      "citation_id": "27",
      "title": "Can large language models transform computational social science?",
      "authors": [
        "C Ziems",
        "W Held",
        "O Shaikh",
        "J Chen",
        "Z Zhang",
        "D Yang"
      ],
      "year": "2023",
      "venue": "Can large language models transform computational social science?",
      "arxiv": "arXiv:2305.03514"
    },
    {
      "citation_id": "28",
      "title": "Generating faithful synthetic data with large language models: A case study in computational social science",
      "authors": [
        "V Veselovsky",
        "M Ribeiro",
        "A Arora",
        "M Josifoski",
        "A Anderson",
        "R West"
      ],
      "year": "2023",
      "venue": "Generating faithful synthetic data with large language models: A case study in computational social science",
      "arxiv": "arXiv:2305.15041"
    },
    {
      "citation_id": "29",
      "title": "Navigating prompt complexity for zeroshot classification: A study of large language models in computational social science",
      "authors": [
        "Y Mu",
        "B Wu",
        "W Thorne",
        "A Robinson",
        "N Aletras",
        "C Scarton",
        "K Bontcheva",
        "X Song"
      ],
      "year": "2023",
      "venue": "Navigating prompt complexity for zeroshot classification: A study of large language models in computational social science",
      "arxiv": "arXiv:2305.14310"
    },
    {
      "citation_id": "30",
      "title": "Towards coding social science datasets with language models",
      "authors": [
        "C Rytting",
        "T Sorensen",
        "L Argyle",
        "E Busby",
        "N Fulda",
        "J Gubler",
        "D Wingate"
      ],
      "year": "2023",
      "venue": "Towards coding social science datasets with language models",
      "arxiv": "arXiv:2306.02177"
    },
    {
      "citation_id": "31",
      "title": "Will affective computing emerge from foundation models and general ai? a first evaluation on chatgpt",
      "authors": [
        "M Amin",
        "E Cambria",
        "B Schuller"
      ],
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "32",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "33",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "34",
      "title": "Want to reduce labeling cost? gpt-3 can help",
      "authors": [
        "S Wang",
        "Y Liu",
        "Y Xu",
        "C Zhu",
        "M Zeng"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "35",
      "title": "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
      "authors": [
        "B Guo",
        "X Zhang",
        "Z Wang",
        "M Jiang",
        "J Nie",
        "Y Ding",
        "J Yue",
        "Y Wu"
      ],
      "year": "2023",
      "venue": "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
      "arxiv": "arXiv:2301.07597"
    },
    {
      "citation_id": "36",
      "title": "Neural discrete representation learning",
      "authors": [
        "A Van Den Oord",
        "O Vinyals",
        "K Kavukcuoglu"
      ],
      "year": "2018",
      "venue": "Neural discrete representation learning"
    },
    {
      "citation_id": "37",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "39",
      "title": "Direct Modelling of Speech Emotion from Raw Speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "40",
      "title": "Quran reciter identification: A deep learning approach",
      "authors": [
        "A Qayyum",
        "S Latif",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "2018 7th International Conference on Computer and Communication Engineering (ICCCE)"
    },
    {
      "citation_id": "41",
      "title": "Retrieving categorical emotions using a probabilistic framework to define preference learning samples",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2016",
      "venue": "Retrieving categorical emotions using a probabilistic framework to define preference learning samples"
    },
    {
      "citation_id": "42",
      "title": "Emotion spotting: Discovering regions of evidence in audio-visual emotion expressions",
      "authors": [
        "Y Kim",
        "E Provost"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "43",
      "title": "Copyright may be transferred without notice, after which this version may no longer be accessible. This is the initial version. The updated version can be downloaded from IEEE. dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "44",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Increasing the reliability of crowdsourcing evaluations using online quality assessment",
      "authors": [
        "A Burmania",
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Progressive neural networks for transfer learning in emotion recognition",
      "authors": [
        "J Gideon",
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Progressive neural networks for transfer learning in emotion recognition",
      "arxiv": "arXiv:1706.03256"
    },
    {
      "citation_id": "47",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "48",
      "title": "Learning discriminative features from spectrograms using center loss for speech emotion recognition",
      "authors": [
        "D Dai",
        "Z Wu",
        "R Li",
        "X Wu",
        "J Jia",
        "H Meng"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "49",
      "title": "Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (addog)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "51",
      "title": "Multi-task semi-supervised adversarial autoencoding for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "52",
      "title": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition",
      "authors": [
        "F Bao",
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition"
    },
    {
      "citation_id": "53",
      "title": "Multitask learning from augmented auxiliary data for improving speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "A preliminary study on augmenting speech emotion recognition using a diffusion model",
      "authors": [
        "I Malik",
        "S Latif",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Proceedings of Interspeech"
    },
    {
      "citation_id": "55",
      "title": "An acoustic study of emotions expressed in speech",
      "authors": [
        "S Yildirim",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "Z Deng",
        "S Lee",
        "S Narayanan",
        "C Busso"
      ],
      "year": "2004",
      "venue": "Eighth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "56",
      "title": "Experimental evidence that women speak in a higher voice pitch to men they find attractive",
      "authors": [
        "P Fraccaro",
        "B Jones",
        "J Vukovic",
        "F Smith",
        "C Watkins",
        "D Feinberg",
        "A Little",
        "L Debruine"
      ],
      "year": "2011",
      "venue": "Journal of Evolutionary Psychology"
    },
    {
      "citation_id": "57",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "On enhancing speech emotion recognition using generative adversarial networks",
      "authors": [
        "S Sahu",
        "R Gupta",
        "C Espy-Wilson"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "59",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "60",
      "title": "Efficient speech emotion recognition using multi-scale cnn and attention",
      "authors": [
        "Z Peng",
        "Y Lu",
        "S Pan",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "61",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}