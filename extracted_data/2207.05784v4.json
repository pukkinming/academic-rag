{
  "paper_id": "2207.05784v4",
  "title": "Distilled Non-Semantic Speech Embeddings With Binary Neural Networks For Low-Resource Devices",
  "published": "2022-07-12T18:32:53Z",
  "authors": [
    "Harlin Lee",
    "Aaqib Saeed"
  ],
  "keywords": [
    "speech representations",
    "knowledge distillation",
    "paralinguistic tasks",
    "binary neural networks",
    "digital health",
    "internet-of-things"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This work introduces BRILLsson, a novel binary neural network-based representation learning model for a broad range of non-semantic speech tasks. We train the model with knowledge distillation from a large and real-valued TRILLsson model with only a fraction of the dataset used to train TRILLsson. The resulting BRILLsson models are only 2MB in size with a latency less than 8ms, making them suitable for deployment in low-resource devices such as wearables. We evaluate BRILLsson on eight benchmark tasks (including but not limited to spoken language identification, emotion recognition, human vocal sounds, and keyword spotting), and demonstrate that our proposed ultra-light and low-latency models perform as well as largescale models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Representation learning takes advantage of large amounts of unlabeled data to learn features that can be used for a variety of downstream signal processing and machine learning tasks. This has demonstrated especially impressive performance in speech and audio processing  [17, 20, 21] , as the ubiquity of smart phones, watches, and home appliances has made it easy and inexpensive to collect a wealth of unlabeled audio signals. In particular, there is growing interest in using representation learning to build general-purpose models for non-semantic speech tasks, which are problems related to human speech other than its meaning, such as spoken language identification  [12] , emotion recognition  [3, 18] , human vocal sounds  [7] , keyword spotting  [24]  and more.\n\nHowever, the large sizes of the trained models and the amount of computational resources that are required to run them on newly acquired data have stalled the real-world deployment of existing models for non-semantic speech applications. These assumptions are critical in mobile computing, edge computing, internet-of-things (IoT), and tiny machine learning (tinyML) settings, which are where many paralinguistic speech applications actually take place, e.g., in small wearable for healthcare or with voice-controlled artificial intelligence (AI) assistants in smart devices. Although models such as FRILL  [14]  and TRILLsson  [21]  were recently proposed to reduce the complexity and size of the deep learning models, a large gap still remains between highly effective (i.e., accurate) and highly efficient (i.e., light enough to be run on devices as small as smartwatches) representation learning models for paralinguistic speech tasks.\n\nTo this end, we design and evaluate BRILLsson, binary neural networks (BNNs)  [5]  that are small and fast enough to be deployed in devices with limited memory and computational resources. BNNs have weights of only +1 or -1, which make them ideal compact architectures, especially in conjunction with co-designed machine learning hardware that one may see in modern IoT applications. Furthermore, we employ knowledge distillation  [8, 16, 25]  from TRILLsson to BRILLsson, and show that distillation can be achieved using data that is slightly unrelated or smaller than the one used to train the original model, which is beneficial when original data is not available or so large that it requires extensive computing power. Finally, we illustrate that BRILLsson achieves performance on many non-semantic speech benchmark and other tasks that is comparable to that of much larger models.\n\nIn summary, our main contributions are:\n\n• We propose BRILLsson, ultra-light and fast models for representation learning that are suitable for low-resource devices. BRILLsson's size is only 2MB, and its latency is less than 8ms.\n\n• Perform successful knowledge transfer via embedding distillation from a large-scale real-valued model to binary neural networks. While similar approaches have been explored in image classification  [11]  and speech separation  [4] , ours is the first in the context of general-purpose representation learning for non-semantic speech.\n\n• Demonstrate that despite their compressed size, our BNNs perform comparably to TRILL, FRILL, and TRILLsson on eight different benchmark datasets.\n\n• Our models are ideal for continuous on-device inference for privacy-preserving health monitoring (e.g., coughing, sneezing) due to its low-computational footprint.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "Our objective is to create extremely compact general-purpose audio models that 1) generate informative embeddings (or representations) for a broad range of audio recognition tasks, and 2) can run efficiently on-device with low latency for low-resource devices, e.g., wearables without constant connection to the cloud. We use knowledge distillation to transfer learned representations of a large-scale pre-trained teacher model to smaller BNN-based student models that are otherwise difficult to pre-train with self-supervised learning (or a similar strategy) due to their limited capacity. We would like to highlight that while distillation has been successfully leveraged for knowledge transfer for non-semantic speech before, to the best of our knowledge, this work is the first attempt at utilizing it for ultra-compact binary neural networks. Figure  1  provides high-level illustration of our approach, and the following subsections describe its essential building blocks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Embedding Distillation",
      "text": "Distillation is a technique to create a model with smaller size and less computational load without sacrificing its effectiveness  [8] . It transfers information from a large supervising model -the teacher (F t )-to a relatively small model -the student (F s )-with the goal of compression for efficient inference. The teacher is generally a fixed pre-trained network learned with a massive amount of high-quality data, in other words a privileged model. In contrast, the student is a low capacity network that is guided to imitate the output of the teacher. The information-rich signal from the teacher enables a compact student network to learn important aspects of the data that would otherwise be missed when solely minimizing a task-specific objective.\n\nIn the seminal paper, Hinton et al.  [8]  proposed to use softened class probabilities from the teacher to provide supervision, which acts as targets for the student model to optimize for. Here, as our teacher model provides 1024-dimensional embeddings, we instead leverage mean-squared-error loss and a linear layer (F r ) of the same dimensionality on top of student model for distillation purpose  [16] . We use one-second audio clips as inputs to teacher-student models to get outputs of 1024 dimensions, on top of which the following loss function is computed:\n\nx is the training data, θ represents the respective model parameters, and F r is the linear layer model representing a regressor function to match teacher's dimensionality that is discarded after distillation. We use a batch size of 512 and a fixed learning rate of 0.001 with an Adam optimizer  [10]  to train for approximately 234K steps with a single NVIDIA RTX3090 GPU.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Teacher: Large Efficientnet-V2 Model",
      "text": "For our teacher model F t , we use EfficientNet-v2  [23]  from TRILLsson  [21] , which has achieved exceptional performance on several Non-Semantic Speech Benchmark  [20]  (NOSS) and other related tasks. This model was trained using a combination of two large-scale speech datasets (Speech Au-dioSet 4.9K and Libri-light 53K) via the teacher-student distillation framework. We choose version three of the EfficientNet-v2 model with 21.5M parameters and access it directly from TensorFlow Hub (TFHub)  1  during training. This model has a front-end based on log-magnitude Mel spectrograms with 80 bins ranging from 125Hz to 7500Hz, uses window length of 25ms and hop length of 10ms, and is initially trained with frame width of 2s of audio.\n\nThere are larger models available within the TRILLsson family, but we choose the EfficientNet variant as it is both high-performing and less computationally demanding to accommodate a modest compute budget (e.g., hardware with a single GPU system). This allows us to train, or distill, longer in a short period of time with a large batch size, and demonstrate our central point that we can extract compact binary models purely with knowledge distillation. Further, EfficientNet is a mobile friendly architecture discovered by neural architecture search for image classification tasks with a large capacity. However, we do note that having access to more computing resources may allow one to leverage even bigger models with better supervision, which we leave for future work.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Student: Light-Weight Binary Neural Networks",
      "text": "Our student models F s are binary neural network (BNN) architectures with single-bit weights and activations. Because neurons in BNNs can have only two possible states, BNNs provide extreme compression and speed-up gains compared to real-valued artificial neural networks. These fast and energy efficient BNNs are well-suited for deployment on low-resource devices with limited memory and battery power. Specifically, we use binary convolutional networks: a binary DenseNet-28  [2]  and MeliusNet  [1]  with around 4.5M and 6.4M parameters, respectively. Their sizes are less than 2MB in floating-point format, and 1.03MB for DenseNet-28 and 1.25MB for MeliusNet in quantized form, which are several folds smaller compared to models utilized in  [20, 21, 14] . We use the Larq  [6]  framework for the implementation of BNNs.\n\nOur student models are paired with an audio processing front-end based on log-magnitude Mel spectrograms, and can directly consume raw audio waveform. Our front-end uses window size of 25ms, hop size of 10 ms, and 64 Mel-spaced frequency bins in the range of 60Hz to 7800Hz for 98 frames, corresponding to 980ms. These inputs to the BNNs are then mapped to latent vectors of size 576 for DenseNet-28 and 512 for MeliusNet. A GlobalMaxPooling layer on top reduces the size of the bottleneck layer by computing the maximum of all values in each output feature map. Finally, these compact output embeddings are provided to classifiers for downstream tasks. We note that to match TRILLsson's embedding dimensions, we add an additional binary fully-connected layer with 1024 hidden units F r , which is discarded after the training phase.   [15]  Human Sounds 320 80 10 Voxforge  [12]  Spoken Language 117,942 30,370 6 SpeechCommands  [24]  Commands Recognition 85,511 4,890 12 CREMA-D  [3]  Emotion Detection 5,144 1,556 6 MSWC-(Micro EN)  [13]  Keyword Spotting 69,868 13,117 31 MSWC-(Micro ES)  [13]  21,254 3,398 20 Vocalsound  [7]  Human Vocal Sounds 15,570 3,594 6\n\n3 Experimental Setup",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Distillation Dataset",
      "text": "We perform knowledge distillation with open-source Libri-light dataset  [9] , which is derived from public audio books in the LibriVox project. It is the largest publicly available, unlabeled semisupervised audio dataset to date. From this, we use a medium subset of the dataset with around 5193 hours of speech (approximately 321GB in size) due to our modest compute budget. We split each audio clip into non-overlapping one-second segments for training, resulting in around 12M examples. It is important to note that TRILLsson models are trained with 58K hours of speech data originating from both Audioset and Libri-light. Also, TRILLsson is trained with a teacher of massive scale, i.e., CAP12  [19] , that is in turn trained on 900K hours of audio from YT-U data  [26] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Downstream Speech Sensing Tasks And Evaluation",
      "text": "We evaluate the effectiveness of our method on a broad range of tasks varying from spoken language identification, keyword spotting, accent recognition, identifying emotion, to human vocal sounds monitoring. Table  1  provides an overview and key characteristics of the 8 datasets. We use MUSAN  [22]  to evaluate the detection of music, speech and noise in audio clips. Voxforge  [12]  is used for identifying spoken English, Spanish, French, German, Russian, and Italian. We use SpeechCommands  [24]  with 12 classes for spoken commands and CREMA-D with 6 classes (anger, disgust, fear, happy/joy, neutral, sad) for emotion recognition. For human sounds task, we utilize 10 classes subset from ESC-50  [15] , same as FRILL  [14] . We also use microsets from multilingual word corpus (MSWC) for keyword spotting in English (EN) and Spanish (ES), each task with 31 and 20 classes, respectively. Lastly, Vocalsound  [7]  contains audio recording for detection of laughter, sighs, coughs, throat clearing, sneezes, and sniffs. We follow the published train and test splits of the datasets except for MUSAN, where we randomly split the data into training (80%) and test (20%) sets. In case of ESC-50 (HS), following FRILL, we use first four folds as training set and the last fold as a test set. We evaluate the quality of learned representations with a linear classifier trained on top frozen feature extractor or encoder in a similar manner as prior work  [20, 21] . The classifier is trained with a batch size of 64 (except for CREMA-D, where we use batch size of 32 due to relatively small size of the datasets) with learning rate of 0.001 with Adam optimizer  [10]  for 100 epochs. We use a randomly selected one-second segment from each audio clip in the training set, and evaluate the performance on the entire audio clip during testing. We did not use dev (i.e. validation set) for model selection as the DenseNet and MeliusNet models are already of extremely small size.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Latency Benchmarking Of Binary Neural Networks",
      "text": "We use Larq Compute Engine  [6]  for latency benchmarking of our BNN models. To align with prior work, we create a float-32 TFLite format model and run it for 150 runs in a single thread to get an averaged inference time on a device equipped with Snapdragon 855.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baseline Models For Comparison",
      "text": "We compare our approach with five methods: TRILL, TRILL-Distilled, FRILL, teacher model TRILLsson3 (EfficientNet-V2), and TRILLsson1 (ResNet-50). TRILL  [20]  is a TRIpLet-Loss Network that is pre-trained with large amount of speech data from Audioset. It has shown to learn powerful representations for non-semantic speech tasks and achieved state-of-the-art performance on some of them when it was published in 2020. It uses a ResNet-50 network architecture, and its layer 19 has shown to provide the most useful features with dimensionality of 12288. TRILL-Distilled  [20]  is a smaller MobileNet-based model with 2048-dimensional embeddings that is trained with distillation to predict TRILL's embeddings. Along a similar line, FRILL  [14]  uses a MobileNetV3 model that is designed to be a fast variant of TRILL specifically focusing on mobile devices. It is trained with distillation to mimic the TRILL layer 19 representations. In our work, we use Small 2.0 GAP model with 2048-dimensional embeddings due to its best performance. Finally, we compare against the teacher model F t , as well as a smaller model in TRILLsson family, i.e., a ResNet-50 model number one with 5M parameters.\n\nWe access all the baseline models from TFHub and use them as frozen feature extractors. We use default audio front-end that comes along with the model from TFHub. Similar to our method, we only add a linear classification layer for evaluating performance on downstream tasks as explained in Section 3.2. In cases where baseline models provide predictions per-time step, we average them to compute final prediction. Furthermore, we train and evaluate baseline models for tasks and datasets that were not presented in the prior work to establish fair comparison; for the rest, we use accuracy score as reported in  [21, 14] . The baseline latency values when available are taken from FRILL  [14] . Because the TRILLsson models are released as TFHub modules, we were unable to convert them into the TFLite format and compute on-device latency ourselves in a manner consistent with others. Importantly, unlike previous works that trained multiple linear classifiers with different techniques, we only train and evaluate a logistic regressor implemented with a linear dense layer. An exhaustive search over classification methods may yield further improvement.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Discussion",
      "text": "We evaluate the performance of our BRILLsson approach, and contextualize how well binary models generalize on a broad range of speech sensing tasks as compared to large-scale models. Table  2  Table  2 : Generalization performance of BRILLsson on a range of non-semantic speech representations tasks. We train a linear classifier on top of distilled frozen models to assess the quality of learned embeddings. In all cases embeddings are from the last layer of the corresponding BRILLsson models. RN-50 is ResNet-50, EN-v2 is EfficientNet-v2, DN is DenseNet-28, and MN is MeliusNet. T denotes a tiny model based on a intermediate layer of DenseNet-28. presents results of BRILLsson along with five large-scale baselines models. Since most of the datasets are published with fixed train-test-validation splits, the convention for audio recognition is to use the validation set for model selection and report a single classifier's performance on the test set. Since these models are trained and tested on the same samples, the accuracies in Table  2  can be directly compared to each other without error bars. First, we notice that the teacher model TRILLsson (EN-v2) overall performs well in comparison to other floating-point based models, which highlights the usefulness of this model as teacher for distillation. Our DenseNet (DN) based binary model demonstrate excellent performance on all considered tasks even with its small size. Note that DN is only 2MB including the audio front-end, whereas the teacher model has size of around 99MB. Similarly, our MeliusNet (MN) has similar or slightly better performance than DN, in particular on Vocalsound where it achieves accuracy of 83.2%. This is only 2% less than the acuracy of TRILL-Distilled. Interestingly, BRILLsson has superior generalization on keyword spotting tasks, achieving 89.2% accuracy on SpeechCommands and 88.5% on MSWC-EN. However, BRILLsson saw performance downgrade in CREMA-D and Voxforge. For CREMA-D, we believe the limited capacity of our model, challenging nature of the problem (i.e. emotion detection) and small dataset are the main reasons that BRILLsson does not perform as well. We hypothesize that scaling the dataset may improve the performance of classifier trained on top of BRILLsson. On the other hand, Voxforge has more samples per class than CREMA-D, but spoken language identification is still a very difficult task. In particular, the pre-training dataset we used contains only English speech, whereas Voxforge contains other languages like German. We believe the main difficulty lies in the network trying to extrapolate to these audio inputs that it did not see during pre-training. We once again emphasize that our BRILLsson models have latency of less than 8ms with size of merely around 2MB.\n\nFurthermore, we add a linear classifier after batch-normalization-12 intermediate layer in DN model to experiment with creating an even smaller model, labeled BRILLsson (T) in Table  2 . The resulting model has size of 0.65MB and latency of 6.1ms. Interestingly, on Voxforge the tiny model achieves 73.0% while having only 1.4M parameters. These results demonstrate the usefulness of representations learned with BNNs and that a single BNN model can be used as a feature extractor on low-resource devices for multiple downstream tasks.\n\nAlong similar lines, we explore the quality of representations from intermediate layers of the distilled model using MN on SpeechCommands. For each intermediate layer, a classification head is added on top and trained in the same manner as previous experiments. The rest of the model is fixed during this phase. Then, we convert each model to TFLite format, evaluate its accuracy and latency, and report the results in Figure  2 . We see that conversion to TFLite format does not result in a significant performance degradation. Also, we observe a trade-off between accuracy and speed, as expected; for instance, the model built on the layer section-2-transition-pw has the highest accuracy of 86.1%, but has relatively high latency of 7.3ms. A more comprehensive investigation into the optimal BNN architecture and intermediate layer embeddings is deferred to future work.\n\n5.75 6.00 6.25 6.50 6.75 7.00 7.  25    For example, a model with classifier after section-1transition-pw layer achieves 81% accuracy with latency of 7ms. stpw is an abbreviation of sectiontransition-pw. head bn is the final layer of MN.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusions",
      "text": "We have designed, developed, and publicly released BRILLsson: an extremely compact, fast, and flexible model for non-semantic speech representation learning. We used embedding distillation to transfer knowledge from an existing TRILLsson model to small binary neural networks. Our approach significantly reduced the model size while keeping the performance on par with large-scale real-valued counterparts, which is valuable for low-resource devices. While this work focused on utilizing existing neural architectures, we would like to explore neural architecture search methods in the future to design even more light-weight BNN models that are suitable for micro controllers.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: provides high-level illustration of",
      "page": 2
    },
    {
      "caption": "Figure 1: Illustration of distilling binary neural models for non-semantic speech.",
      "page": 3
    },
    {
      "caption": "Figure 2: We see that conversion to TFLite format does not",
      "page": 8
    },
    {
      "caption": "Figure 2: Performance of intermediate layers’ representations on SpeechCommands for MeliusNet",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "Laye\nstp",
          "Column_3": "r\nw_0",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": ""
        },
        {
          "Column_1": "",
          "Column_2": "stp\nstp",
          "Column_3": "w_1\nw_2",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": ""
        },
        {
          "Column_1": "",
          "Column_2": "he",
          "Column_3": "ad_bn",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Meliusnet: An improved network architecture for binary neural networks",
      "authors": [
        "Joseph Bethge",
        "Christian Bartz",
        "Haojin Yang",
        "Ying Chen",
        "Christoph Meinel"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "2",
      "title": "Back to simplicity: How to train accurate bnns from scratch? arXiv preprint",
      "authors": [
        "Joseph Bethge",
        "Haojin Yang",
        "Marvin Bornstein",
        "Christoph Meinel"
      ],
      "year": "2019",
      "venue": "Back to simplicity: How to train accurate bnns from scratch? arXiv preprint",
      "arxiv": "arXiv:1906.08637"
    },
    {
      "citation_id": "3",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Ruben Michael K Keutmann",
        "Ani Gur",
        "Ragini Nenkova",
        "Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "4",
      "title": "Distilled binary neural network for monaural speech separation",
      "authors": [
        "Xiuyi Chen",
        "Guangcan Liu",
        "Jing Shi",
        "Jiaming Xu",
        "Bo Xu"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "5",
      "title": "Binaryconnect: Training deep neural networks with binary weights during propagations",
      "authors": [
        "Matthieu Courbariaux",
        "Yoshua Bengio",
        "Jean-Pierre David"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "Larq: An open-source library for training binarized neural networks",
      "authors": [
        "Lukas Geiger",
        "Plumerai Team"
      ],
      "year": "2020",
      "venue": "Journal of Open Source Software"
    },
    {
      "citation_id": "7",
      "title": "Vocalsound: A dataset for improving human vocal sounds recognition",
      "authors": [
        "Yuan Gong",
        "Jin Yu",
        "James Glass"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "8",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeffrey Dean"
      ],
      "year": "2015",
      "venue": "NIPS Deep Learning and Representation Learning Workshop"
    },
    {
      "citation_id": "9",
      "title": "Libri-light: A benchmark for asr with limited or no supervision",
      "authors": [
        "J Kahn",
        "M Rivière",
        "W Zheng",
        "E Kharitonov",
        "Q Xu",
        "P Mazaré",
        "J Karadayi",
        "V Liptchinsky",
        "R Collobert",
        "C Fuegen",
        "T Likhomanenko",
        "G Synnaeve",
        "A Joulin",
        "A Mohamed",
        "E Dupoux"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "11",
      "title": "Training binary neural networks with knowledge transfer",
      "authors": [
        "Sam Leroux",
        "Bert Vankeirsbilck",
        "Tim Verbelen",
        "Pieter Simoens",
        "Bart Dhoedt"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "12",
      "title": "",
      "authors": [
        "Ken Maclean",
        "Voxforge",
        "Ken Maclean"
      ],
      "year": "2018",
      "venue": ""
    },
    {
      "citation_id": "13",
      "title": "Multilingual spoken words corpus",
      "authors": [
        "Mark Mazumder",
        "Sharad Chitlangia",
        "Colby Banbury",
        "Yiping Kang",
        "Juan Manuel Ciro",
        "Keith Achorn",
        "Daniel Galvez",
        "Mark Sabini",
        "Peter Mattson",
        "David Kanter"
      ],
      "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "14",
      "title": "FRILL: A Non-Semantic Speech Embedding for Mobile Devices",
      "authors": [
        "Jacob Peplinski",
        "Joel Shor",
        "Sachin Joglekar",
        "Jake Garrison",
        "Shwetak Patel"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "15",
      "title": "ESC: Dataset for Environmental Sound Classification",
      "authors": [
        "J Karol",
        "Piczak"
      ],
      "year": "2015",
      "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "Fitnets: Hints for thin deep nets",
      "authors": [
        "Adriana Romero",
        "Nicolas Ballas",
        "Samira Kahou",
        "Antoine Chassang",
        "Carlo Gatta",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "Proceedings of ICLR"
    },
    {
      "citation_id": "17",
      "title": "Contrastive learning of general-purpose audio representations",
      "authors": [
        "Aaqib Saeed",
        "David Grangier",
        "Neil Zeghidour"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "18",
      "title": "Leveraging recent advances in deep learning for audio-visual emotion recognition",
      "authors": [
        "Liam Schoneveld",
        "Alice Othmani",
        "Hazem Abdelkawy"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "19",
      "title": "Universal paralinguistic speech representations using self-supervised conformers",
      "authors": [
        "Joel Shor",
        "Aren Jansen",
        "Wei Han",
        "Daniel Park",
        "Yu Zhang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "20",
      "title": "Towards Learning a Universal Non-Semantic Representation of Speech",
      "authors": [
        "Joel Shor",
        "Aren Jansen",
        "Ronnie Maor",
        "Oran Lang",
        "Omry Tuval",
        "Félix De Chaumont",
        "Marco Quitry",
        "Ira Tagliasacchi",
        "Dotan Shavitt",
        "Yinnon Emanuel",
        "Haviv"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "21",
      "title": "TRILLsson: Distilled Universal Paralinguistic Speech Representations",
      "authors": [
        "Joel Shor",
        "Subhashini Venugopalan"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "22",
      "title": "A music, speech, and noise corpus",
      "authors": [
        "David Snyder",
        "Guoguo Chen",
        "Daniel Povey",
        "Musan"
      ],
      "year": "2015",
      "venue": "A music, speech, and noise corpus",
      "arxiv": "arXiv:1510.08484"
    },
    {
      "citation_id": "23",
      "title": "Efficientnetv2: Smaller models and faster training",
      "authors": [
        "Mingxing Tan",
        "Quoc Le"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "24",
      "title": "Speech commands: A dataset for limited-vocabulary speech recognition",
      "authors": [
        "Pete Warden"
      ],
      "year": "2018",
      "venue": "Speech commands: A dataset for limited-vocabulary speech recognition",
      "arxiv": "arXiv:1804.03209"
    },
    {
      "citation_id": "25",
      "title": "Improving knowledge distillation using unified ensembles of specialized teachers",
      "authors": [
        "Adamantios Zaras",
        "Nikolaos Passalis",
        "Anastasios Tefas"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "26",
      "title": "Bigssl: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition",
      "authors": [
        "Yu Zhang",
        "Daniel Park",
        "Wei Han",
        "James Qin",
        "Anmol Gulati",
        "Joel Shor",
        "Aren Jansen",
        "Yuanzhong Xu",
        "Yanping Huang",
        "Shibo Wang",
        "Zongwei Zhou",
        "Bo Li",
        "Min Ma",
        "William Chan",
        "Jiahui Yu",
        "Yongqiang Wang",
        "Liangliang Cao",
        "Khe Sim",
        "Bhuvana Ramabhadran",
        "Tara Sainath",
        "Françoise Beaufays",
        "Zhifeng Chen",
        "Quoc Le",
        "Chung-Cheng Chiu",
        "Ruoming Pang",
        "Yonghui Wu"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    }
  ]
}