{
  "paper_id": "2312.10579v2",
  "title": "Der-Gcn: Dialogue And Event Relation-Aware Graph Convolutional Neural Network For Multimodal Dialogue Emotion Recognition",
  "published": "2023-12-17T01:49:40Z",
  "authors": [
    "Wei Ai",
    "Yuntao Shou",
    "Tao Meng",
    "Nan Yin",
    "Keqin Li"
  ],
  "keywords": [
    "Contrastive learning",
    "Event extraction",
    "Masked graph autoencoders",
    "Multiple information Transformer",
    "Multimodal dialogue emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With the continuous development of deep learning (DL), the task of multimodal dialogue emotion recognition (MDER) has recently received extensive research attention, which is also an essential branch of DL. The MDER aims to identify the emotional information contained in different modalities, e.g., text, video, and audio, in different dialogue scenes. However, existing research has focused on modeling contextual semantic information and dialogue relations between speakers while ignoring the impact of event relations on emotion. To tackle the above issues, we propose a novel Dialogue and Event Relation-Aware Graph Convolutional Neural Network for Multimodal Emotion Recognition (DER-GCN) method. It models dialogue relations between speakers and captures latent event relations information. Specifically, we construct a weighted multi-relationship graph to simultaneously capture the dependencies between speakers and event relations in a dialogue. Moreover, we also introduce a Self-Supervised Masked Graph Autoencoder (SMGAE) to improve the fusion representation ability of features and structures. Next, we design a new Multiple Information Transformer (MIT) to capture the correlation between different relations, which can provide a better fuse of the multivariate information between relations. Finally, we propose a loss optimization strategy based on contrastive learning to enhance the representation learning ability of minority class features. We conduct extensive experiments on the IEMOCAP and MELD benchmark datasets, which verify the effectiveness of the DER-GCN model. The results demonstrate that our model significantly improves both the average accuracy and the f1 value of emotion recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction 1.Motivation",
      "text": "T HE task of Multimodal Dialogue Emotion Recognition (MDER) is to identify the emotional changes of speakers in different modalities, such as text, video, and audio. In recent decades, due to the application of MDER in some emerging application scenarios, for instance, the recognition of negative emotions has attracted research attention in social media such as Meta and Weibo  [1] ,  [2] ,  [3] , the intelligent recommendation system for online shopping  [4] , and chat robots  [5] , etc. Furthermore, when shopping online, the APP will recommend the most interesting products according to the user's preferences  [6] ,  [7] .\n\nHowever, MDER is more challenging than sentence-level emotion recognition or unimodal emotion recognition tasks because sentiment changes are generally determined by a series of meaningful internal and external factors  [8] ,  [9] . Specifically, in the dialogue process, the speaker's emotion is affected not only by internal factors composed of con- textual information but also by external factors composed of dialogue and event relationships (e.g., entity, location, keywords, etc.). For example, when speakers talk about a sensitive topic on social media, they often express their emotions more implicitly and suggestively  [10] . While the use of events to strengthen the dialogue between speakers can compensate for the lack of noticeable semantic features. Therefore, how to comprehensively consider the influence of internal and external factors on emotion recognition is still a problem to be solved. In addition, in MDER, due to the high cost of labeling, the data distribution exhibits a long-tailed state  [11] ,  [10] ,  [12] . It leads to the model being less effective at identifying the minority class emotion.\n\nThe current mainstream MDER methods are used Recurrent Neural Networks (RNNs),  [13] , Transformers  [14] , and Graph Neural Networks (GNNs)  [15]  to model the semantic information of context and dialogue relationship between speakers, respectively. Although RNN-based methods have achieved good results in emotion recognition tasks based on contextual semantic modeling  [16] ,  [17] , it ignores the influence of external factors (e.g., dialogue relations and event relations). To better integrate contextual semantic information, Transformer-based methods are applied, but it still ignores the influence of external factors on emotion recognition. To consider the influence of internal and external factors on emotion recognition, many researchers have begun to adopt GNN to model MDER  [18] . Although the GNN-based method considers the dialogue relationship, it still ignores the influence of the event relationship on MDER. However, the event relationship also greatly influences the speaker's emotion, and the speaker usually shows the same emotion when discussing the same event. Therefore, modeling the event relations in the dialogue is beneficial to obtain a better spatial distribution of emotion categories. As shown in Fig.  1  as an example, Fig.  1(b ) is a graph that only considers the dialogue relationship between speakers, and its emotion categories have overlapping areas in the spatial distribution, as shown in Fig.  1 (d) . Fig.  1(c ) is a graph that comprehensively considers the interaction relationship and event relationship between speakers. Its spatial distribution of emotion categories has better discrimination, as shown in Fig.  1(e ). Hence, it is necessary to take the event relationship as the starting point of the MDER architecture design.\n\nTo tackle the above problem, we propose a novel Dialogue and Event Relation-Aware Graph Convolutional Neural Network for Multimodal Emotion Recognition architecture, namely DER-GCN. DER-GCN mainly includes six modules: data preprocessing, feature extraction and fusion, masked graph representation learning, multi-relational information aggregation, balanced sampling strategy, and emotion classification. Firstly, we use RoBERTa  [19] , 3D-CNN  [20] , and Bi-LSTM-based Encoder  [21]  to obtain embedding representations for three modalities: text, video, and audio. Secondly, we use a bidirectional gated recurrent unit (Bi-GRU) for feature extraction and Doc2EDAG  [22]  for event extraction to strengthen the dialogue relationship between speakers. Then, we design a novel cross-modal feature fusion method to learn complementary semantic information between different modalities. Specifically, we use cross-modal attention to learn the differences between the semantic information of different modes. The average pooling operation is used to learn the global information of each mode to guide the inter-modality and intra-modality information aggregation, respectively. Thirdly, we design a self-supervised mask graph autoencoder (SMGAE) to model the correlation between dialogues and events. Unlike the previous works  [23] , which only perform mask reconstruction on nodes in the graph, SMGAE performs mask reconstruction on some nodes and edges simultaneously. Fourthly, we design the Multiple Information Transformer (MIT) to better fuse the multivariate information between relations and capture the correlation between different relations. MIT is paid attention mechanism to filter unimportant relational information, which fuses to obtain better embedding representations. Fifthly, we propose a loss optimization function based on contrastive learning to alleviate the longtail effect in MDER, which balances the proportion of each emotion category during model training. Finally, we have used an emotion classifier constructed from a multilayer perceptron (MLP) to output the final sentiment category.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Our Contributions",
      "text": "Hence, MDER should consider the dialogue between speakers and the event relationship in the dialogue as the starting point of model design. Inspired by the analysis above, we present a novel Dialogue and Event Relation-Aware Graph Convolutional Neural Network for Multimodal Emotion Recognition (DER-GCN) to learn better emotion feature embedding. The main contributions of this paper are summarized as follows:\n\n• A novel dialogue and event relation-aware emotion representation learning architecture is present and named DER-GCN. DER-GCN can achieve crossmodal feature fusion, solve the imbalanced data distribution problem, and learn more discriminative emotion class boundaries.\n\n• A novel self-supervised graph representation learning framework, named SMGAE, is presented. SM-GAE enhances the feature representation capability of nodes and optimizes the structural representation of graphs, which has a stronger anti-noise ability.\n\n• A new Weighted Relation-aware Multi-subgraph Information Aggregation method is implemented and named MIT. MIT is used to learn the importance of different relations in information aggregation to fuse to obtain more discriminative feature embeddings.\n\n• Finally, extensive experiments are performed on two popular benchmark datasets, MELD and IEMOCAP, which demonstrate that DER-GCN outperforms existing comparative algorithms in weight accuracy and f1-value for multimodal emotion recognition.\n\nThe remainder of this paper is organized as follows. Section 2 reviews the related work of the predecessors. Section 3 defines the problem and briefly describes how the data is preprocessed. Section 4 illustrates the structural design of DER-GCN. The datasets, evaluation metrics, and comparison algorithms are described in Section 5. Section 6 shows the experimental results. Finally, we present experimental conclusions and future work in Section 7.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Recognition In Conversation",
      "text": "Multimodal Dialogue Emotion Recognition (MDER) is an interdisciplinary research field that has attracted extensive attention from researchers in cognitive science, psychology, etc. Existing MDER research mainly includes emotion recognition based on Recurrent Neural Network (RNN)  [24] , emotion recognition based on Graph Neural Network (GNN)  [25] , and emotion recognition based on Transformer  [14] . RNNs mainly extract contextual semantic information by modeling long-range contextual dependencies. GNNs model, the dynamic interaction process of dialogue, mainly relies on the graph structure's inherent properties to model the dependencies between speakers. The Transformer mainly uses the attention mechanism to achieve cross-modal feature fusion to capture the different semantic information between modalities.\n\nIn the RNN-based multimodal emotion recognition research, Wang et al.  [26]  conducted Dual-Sequence LSTM (DS-LSTM), which uses a dual-stream LSTM to extract contextual features in the Mei-Frequency map simultaneously. DS-LSTM comprehensively considers the context features of different times and frequencies and achieves a better emotion recognition effect. However, DS-LSTM cannot achieve feature fusion between modalities. Li et al.  [27]  created attention-based bidirectional LSTM RNNs (A-BiLSTM RNNs). This method combines the self-attention mechanism and LSTM to learn multimodal features with a time dimension. It carries out decision-level information fusion on the obtained multimodal features to realize emotion recognition. Although RNN-based methods have achieved good results in emotion recognition tasks based on contextual semantic modeling, it ignores the influence of external factors (e.g., dialogue relations and event relations).\n\nIn Transformer-based multimodal emotion recognition research, Huang et al.  [28]  employed Multimodal Transformer Fusion (MTF), which uses a multi-head attention mechanism to obtain intermediate feature representations of multimodal emotions. Then, a self-attention mechanism is utilized to capture long-lived dependencies in context. MTF significantly outperforms other feature fusion methods. Transformer-based methods can extract richer contextual semantic information, but it still ignores the influence of external factors on emotion recognition.\n\nIn GNN-based multimodal emotion recognition research, Sheng et al.  [29]  performed Summarization and Aggregation Graph Inference Network (SumAggGIN), which captures distinguishable fine-grained features between phrases by building a heterogeneous graph neural network. SumAggGIN achieves sentiment prediction related to dialogue topics. Although the GNN-based method considers the dialogue relationship, it still ignores the influence of the event relationship on MDER.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Transformers For Dialogue Generation",
      "text": "In recent years, the task of dialogue generation has also begun to receive extensive attention. For example, dialogue generation technology can be used in healthcare to help patients access health information.  Huang",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Masked Self-Supervised Graph Learning",
      "text": "Masked self-supervised graph representation learning, which can automatically learn deeper feature representations from raw data without using a large amount of labeled data, has been used by more and more researchers. The current mainstream research on mask self-supervised graph representation learning focuses on mask and data reconstruction at the node and edge levels.\n\nIn node-level mask-based self-supervised learning, Liu et al.  [30]  performed a spatiotemporal graph neural network (STG-Net), which masks graph nodes based on an edge weighting strategy. GCN is used to reconstruct contextual features to obtain a better data representation. Wang et al.  [31]  created HeCo, which learns high-level embedding representations of nodes by using a view masking mechanism. In addition, HeCo introduces a contrastive learning strategy, which can further improve the model's ability to learn feature representations. However, HeCo needs to design meta-paths manually. The above methods only consider feature mask reconstruction and ignore structure mask reconstruction.\n\nIn edge-level mask-based self-supervised learning, Pan et al.  [31]  conducted adversarial graph embedding (AGE), which reconstructs the topology of a graph by using an adversarial regularized graph auto-encoder (ARGA) and an adversarial regularized variational graph auto-encoder (ARVGA). AGE is trained in a self-supervised manner to learn the underlying distribution law of the data. However, AGE may lose some useful semantic information. The above methods only consider structure mask reconstruction and ignore feature mask reconstruction.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Balanced Optimization Based On Contrastive Learning",
      "text": "The datasets in multimodal dialogue emotion recognition suffer from data imbalance, which makes the cross-entropy loss function widely used for classification no longer applicable. However, contrastive learning can learn distinguishable class boundary information between different classes by continuously narrowing the gap between positive samples. It continuously widens the gap between positive and negative samples  [32] . Therefore, contrastive learning is often used to solve the data imbalance problem in practical problems.\n\nCai et al.  [33]  applied a Heterogeneous Graph Contrastive Learning Network (HGCL), which obtains the embedded representation of each node by maximizing the interaction information between local graph nodes and the global representation of the full graph nodes. HGCL can learn better class boundary information from multivariate heterogeneous data. Peng et al.  [34]  proposed supervised contrastive learning (SCL) to compare the input samples with other instances and input samples with negative samples, which were generated by the Soft Brownnian Offset sampling method to enhance feature representation capability. SCL can effectively alleviate the problem of imbalanced data distribution by continuously expanding the difference between positive and negative samples. However, SCL may suffer from overfitting because the feature representation is too strong.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preliminary Information",
      "text": "In this section, we will define the multimodal emotion recognition task and briefly introduce the preprocessing methods for the three modalities of text, audio and video in the multimodal emotion dataset. Their processing procedures are as follows: (1) Word Embedding: To obtain word vectors with rich semantic information, we will use the RoBERTa model  [19]  to obtain the vector representation of each word. (2) Visual Feature Extraction: To capture the features of the speaker's facial expression changes and gesture changes in each frame of the video, we will use the 3D-CNN model  [35]  for feature extraction. (3) Audio Feature Extraction: To capture the speech features that can distinguish different speakers, we would use the structure of the Encoder to extract the feature of the sound signal. In addition, to capture the semantic information of capturing the topic events discussed by the speaker during the dialogue, we also perform event extraction on the text.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Problem Definition",
      "text": "For the task of MDER, since the number of speakers N (N ≥ 2) participating in the dialogue is not fixed, we assume that N speakers involved in a conversation and are represented as P = {P 1 , P 2 , . . . , P N }, respectively. During the dialogue, a series of utterances from the speaker are arranged in chronological order, which can be expressed as U = {u 1 , u 2 , . . . , u T }. Where T represents the total number of utterances, each of utterance has three modalities, i.e., text (t), audio (a), and visual (v). The task of this paper is to predict the speaker's emotion category at the current moment q based on the speaker's words, voice, and his expressions. The emotion prediction task is defined as follows:\n\nwhere e i represents the emotion of the i-th sentence, and K represents the window size of the historical context.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Design Of The Der-Gcn Structure",
      "text": "In this section, we illustrate the six components that makeup DER-GCN, as shown in Fig.  2 . The structure of DER-GCN is as follows: (1) Sequence modeling and cross-modal feature fusion: For the input text, video, and audio modal features, DER-GCN inputs them into the bidirectional gated recurrent unit (Bi-GRU) to extract contextual semantic information. Furthermore, to capture the regions with the strongest emotional features among the three modalities, we design a cross-modal attention mechanism for feature extraction and fusion of complementary semantic information. (  2 ) Multi-relational emotional interaction graph: Unlike current mainstream algorithms that only use graph convolutional neural networks (GCN) to model the interaction between speakers, we construct a multi-relational graph neural network that includes events and speakers, thereby enhancing the feature representation capability of the model. (3) Intra-relational Masked Graph Autoencoder: To improve the fusion representation ability of node features and edge structures in GCN, we designed a Masked Graph Autoencoder (MGAE). MGAE improves the representation ability of GCN by random masking and reconstruction of nodes and edges and alleviates the problem of class distribution imbalance. 4) Information aggregation between relations: To guide DER-GCN better to perform information aggregation of multi-relational graph neural networks, we design a multi-relational information fusion Transformer, which can effectively fuse the semantic information in the subgraphs composed of different relationships and learn better-embedded representation. (  5 ) Contrastive Learning: The commonly used benchmark datasets in the field of multimodal emotion recognition have the problem of unbalanced class distribution. We introduce a contrastive learning mechanism to learn more discriminative class boundary information. (  6 ) Emotion classifier: To make DER-GCN provide more gradient information in the backpropagation process and promote the model to be fully trained during emotion classification, we construct a linear layer with residual connections as the emotional classifier of DER-GCN.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Sequence Modeling And Cross-Modal Feature Fusion",
      "text": "The emotional change of the speaker at the current time t is not only related to the utterance at the t-th time but also to the contextual utterances before the t -1 time and after the t+1 time. How capture the contextual semantic information contained in the three modalities of video, audio and text is a challenging task. In this paper, we use Bi-GRU to model the long-term dependencies of the three modalities so that the model can more accurately understand the emotional changes of the speaker at the current moment t. The formula for GRU is defined as follows:\n\nwhere, z t represents the update gate, which is used to select the context information that needs to be retained at the current time t to update the state of the hidden layer at the t -1-th time. r t represents the reset gate, which is used to forget the unimportant contextual information in the conversation at the current moment t. x t , h t represent the input unimodal feature vector and the hidden layer for storing contextual information. ht represents the candidate's hidden layer state. W z , W r , W ht are parameters that can be learned in GRU. γ represents text, video or audio. ⊙ means Hadamard product. Bi-GRU contains contextual semantic information extracted from forward and reverse. The formula is defined as follows:\n\nwhere, → h γ t is the contextual semantic information extracted in the forward direction, ← h γ t is the contextual information extracted in the reverse direction, and ψ is composed of all the contextual information at the previous T moments.\n\nTo realize the information interaction and fusion among the three modalities, we propose a cross-modal attention mechanism, which is used to exploit the interaction between modalities in a more fine-grained manner to improve the semantic understanding ability of the model.\n\nFirst, we normalize the hidden layer feature vectors of the three modalities obtained after Bi-GRU processing. The formula is defined as follows:\n\nwhere, ε γ = 1 √ d γ is the scaling factors of the three modalities. n represents the dimension of the modality.\n\nThen, to better preserve the semantic information of the three modalities, we perform an average pooling operation on H γ ij , and the formula is defined as follows:\n\nwhere f pooling (•) represents the average pooling operation.\n\nNext, we perform a fusion operation on the three modal features and use the tanh activation function to obtain their weights. The formula is defined as follows:\n\nwhere,\n\nare the network parameters that can be learned in the model. According to the above formula, we can get the normalized attention weight\n\nThe formula is defined as follows:\n\nFinally, we obtain the feature representation ξ after the fusion of the three modalities according to the attention weight. The formula is defined as follows:",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Weighted Multi-Relational Affective Interaction Graph",
      "text": "As shown in Fig.  3 , we build a multi-relational affective interaction graph that includes the relationships between speakers and heterogeneous elements extracted from events. To capture the heterogeneous information contained in different relations, we construct a weighted multi-relational affective interaction graph\n\nto associate the relationship between\n\n(1)  nodes edges. The node set V in the multi-relational emotional interaction graph is a series of fused multimodal feature vectors. The edge e ij ∈ ℵ is composed of speaker relation or event relation between v i and v j . ω ij ∈ W is the weight of the edge e ij . r ∈ ℜ is an edge relation.\n\nThe formula for the edge e r E ij of different relations composed of events is defined as follows:\n\nwhere, A r E represents the adjacency matrix of the multirelationship graph, where its rows represent all event nodes, and its columns represent event nodes belonging to relation r E . A T r E is the transposition of the matrix A r E . To capture the difference between different edges under the same relationship, we define the weight of the edge e r E ij as follows:\n\nFor the edge e r S ij composed of the relationship between the speakers, if there is a dialogue between the speakers, we connect an edge for them. Otherwise, no edge is established. For the edge weight ω r S ij of edge e r S ij , we use the similarity attention mechanism to assign weights to it. First, we use two linear layers to compute the similarity between nodes in the graph. The formula is defined as follows: Then, we use the attention mechanism to get the weight of each edge, and the formula is defined as:\n\nwhere M i is the set of neighbor nodes of node i. The larger ω ij , the higher the correlation between nodes.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Self-Supervised Masked Graph Autoencoder",
      "text": "To improve the joint representation ability of features and structures of graph neural networks, we propose a selfsupervised masked graph autoencoder (SMGAE), which learns better feature embedding representation by randomly masking and reconstructing the nodes and edges in the graph. Unlike recent studies that only reconstruct features or structures, we reconstruct both features and structures to improve the generalization performance of the model. First, we sample some nodes and edges in the graph and use the mask token to mask the node's feature vector and edge weights. The node feature formula after masking is defined as follows:\n\nwhere, V M represents the masked node-set, and ξ [M ] is the masked multimodal feature vector.\n\nThe formula for the masked edge is defined as follows:\n\nwhere, φ M represents the masked edge set, and e\n\n[M ] ij represents the masked edge.\n\nThe goal of SMGAE is to reconstruct the masked node features and adjacency matrix A by using a small number of node features and edge weights. This paper is adopted a graph convolutional neural network (GCN) as our Encoder to aggregate information. The formula is defined as:\n\n, ξi , êi is the expected value of the generated node feature, and p ϑ e\n\n, ξi , êi is the expected value of the generated edge. e\n\n[M ] i is the unmasked edge. ξi , êi represent the node features and edges generated by encoding, respectively.\n\nIn this paper, we will use a graph convolutional neural network (GCN) as our encoder to aggregate information, the formula is defined as:\n\nwhere, I (t) i\n\nis the feature vector representation of node i at time t. ℵ r i represents the edge set of node i under the edge relation\n\nAfter getting the encoded feature vector, we need to use the decoder to map the latent feature distribution to the input ξ. The design of the Encoder determines the ability of feature recovery, while simple decoders (such as multilayer perceptrons) are less capable and cannot recover high-level semantic information. In this paper, we choose the Graph Attention Network (GAT) with stronger decoding ability as the decoder of SMGAE, which can utilize the surrounding neighbor information to recover the input features instead of just relying on the nodes themselves.\n\nIn the process of coding and decoding, we do not use the mean square error (MSE). Because it is easily affected by the vector dimension and norm, but uses the cosine similarity error, which is more stable in the training process and guide the optimization direction of the model gradient. The formula is defined as follows:\n\n2 ξ i W is the feature vector decoded by the graph neural network. λ is a hyperparameter, and ∥W ∥ 2 F is the weight decay coefficient of the model, which is used to improve the robustness of the model.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "In This Paper, We Define",
      "text": ", and the loss function becomes:\n\nWhere Atr(•) is the trace of the matrix. Then we can get the first-order partial derivative of L to W , and set the value of the first-order partial derivative to 0 to obtain the optimal network parameter W . The formula is defined as follows:\n\nFor the reconstruction of the edge structure, we will use the contrastive loss of positive and negative samples to optimize, the formula is defined as follows:\n\nwhere, κ + represents the masked edge, and D edge i represents the probability of the edge belonging to the i-th node.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Weighted Relation-Aware Multi-Subgraph Information Aggregation",
      "text": "To better fuse the multiple information between relations and capture the correlation between different relations, we design a Multiple Information Transformer (MIT) to aggregate the interactive information between different relations through multiple information fusion. After modeling the information aggregation of multiple subgraphs, the sentiment classification effect of DER-GCN will be more credible.\n\nAs shown in the Fig.  4 , MIT is composed of Transformers with multiple cross branches, and the interactive information between different relations will be bidirectionally transmitted in MIT. Specifically, we first input the feature vectors obtained after masked graph autoencoder learning into three fully connected layers and 1D convolutional layers, respectively, to obtain vectors Q, K, V . The formula is defined as follows:\n\nwhere\n\nare the learnable network parameters in the fully connected layer, and Conv is a onedimensional convolution operation. Next, we use the softmax function to obtain the attention scores for feature vectors composed of different relations as follows:\n\nWhere ε is the dimension of the feature vector Q. T represents the transposition of the matrix. Finally, we perform information fusion across relations by the following formula: Where, ϑ represents the ϑ-th relation. After cross-relational information fusion, we can obtain multi-relational fusion vectors containing rich semantic information.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Balanced Sampling Strategy-Based Contrastive Learning Mechanism",
      "text": "The number of emotions in each category in the MERC task is quite different. If the cross-entropy loss function is used to guide the learning process of the model, it will cause the model to have a serious overfitting effect on the minority category of emotions. Inspired by contrastive learning, it can learn discriminative boundary information for instances between classes. Therefore, it effectively alleviates the longtail problem in MERC.\n\nBased on the above research, we introduce a triplet loss function in the process of model training to solve the problem of class distribution imbalance. In addition, we also add a global cross-entropy loss to preserve as much graph structure information as possible.\n\nFor each utterance m i , we sample its positive samples m + i and negative samples m - i to get the triplet loss value of the model, which narrows the gap between positive samples and actual samples. It can widen the gap between negative samples and actual samples. The formula is defined as follows:\n\nWhere, E(, ) is used to calculate the Euclidean distance between two feature vectors. b is a hyperparameter of the model that measures the distance between samples.\n\nWe also construct a global cross-entropy loss to preserve the information of similar structures better. The formula is defined as follows:\n\nwhere θ is the total number of dialogues in the benchmark dataset, γ n represents the number of utterances in the n-th dialogue, and λ is the total number of sentiment categories.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion Classification",
      "text": "The emotional features E f obtained after going through the graph convolutional neural network is sent to a linear layer with residual connections, and then goes through a layer of softmax layer to obtain the probability distribution P of emotional labels: the formula is defined as follows:\n\nwhere,\n\nparameters that can be learned in the model. We get the sentiment label with the maximum probability through the argmax function:\n\nwhere ŷ represents the sentiment label predicted by the model.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Benchmark Dataset Used",
      "text": "The IEMOCAP  [36]  and MELD  [37]  benchmark datasets are two multimodal dialogue sentiment datasets that researchers widely use to evaluate the effectiveness of their models.\n\nThe Interactive Emotional Dyadic Motion Capture Database (IEMOCAP) is a multimodal emotion recognition dataset. The IEMOCAP dataset contains three modalities of the speaker's video, audio, and dialogue text. The dataset contains 5 actors and 5 actresses, and each dialogue scene has a dialogue between an actor and an actress. The labels of these conversations are all manually annotated, and at least three experts in the emotion domain are assigned to each conversation.\n\nThe Multi-modal EmotionLines Dataset (MELD) is a popular multi-modal benchmark dataset in the MDER domain, consisting of multiple dialogue clips from the TV series friends. The total video and audio duration of MELD is approximately 13.7 hours, and each video clip contains multiple speakers. The labels of these conversations are all manually annotated, and at least five experts in the emotion domain are assigned to each conversation.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "In this section, we illustrate the evaluation metrics used to verify the effectiveness of the model proposed in this paper. This paper uses the following four evaluation metrics: 1) accuracy; 2) f1; 3) weight accuracy (WA); 4) weight f1 (WF1). Due to the serious data imbalance problem in the IEMOCAP and MELD benchmark datasets, we will mainly use WA and WF1 as our main evaluation metrics.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Baseline Models",
      "text": "To verify the effectiveness of our model on the IEMO-CAP and MELD benchmark datasets, we conduct comparative experiments with twelve state-of-the-art deep learningbased algorithms, including one traditional CNN algorithm (i.e., TextCNN  [38] ), four RNN algorithms (i.e., bc-LSTM  [39] , DialogueRNN  [40] , CMN  [41] , and A-DMN  [42] ), three GNN algorithms (i.e., DialogueGCN  [43] , RGAT  [44] , LR-GCN  [45] ), one feature fusion algorithm (i.e., LFM  [46] ), and three pre-trained algorithms (CoMPM  [47] , EmoBERTa  [48] , and COGMEN  [49] ).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Implementation Details",
      "text": "To effectively evaluate the experimental effect of the algorithm in this paper, we divide the dataset into the training set, validation set, and test set, and the ratio of the test set to validation set is 4:1. The test set is used to evaluate the training effect of the model. The validation set is used to fine-tune the model parameters. All experiments in this paper are carried out on a server with Ubuntu 18.04 operating system, hardware model Nvidia RTX 3090, and a video memory capacity of 24G. Our Python version is 3.7, and the Pytorch version is 1.8.1. We choose the Adam optimization algorithm  [50]  for gradient updates in the updating process of model parameters. A total of 60 iterations of training are performed in this experiments, and the batch size during each iteration is 32. We set the learning rate to 0.0003, dropout to 0.25, and L2 regularization term to 0.0001.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Comparison With Baselines",
      "text": "To verify the effectiveness of the DER-GCN model proposed in this paper, we have done extensive experiments to compare it with other comparison algorithms. Tables  1  and 2  present the emotion recognition effects of DER-GCN and other comparative algorithms on two popular datasets, respectively.\n\nIEMOCAP: As shown in Table  1 , compared with other comparison algorithms, our proposed multimodal dialogue emotion recognition method DER-GCN has the best emotion recognition effect on the IEMOCAP dataset, and the WA and WF1 values are 69.7% and 69.4%, respectively. DER-GCN proposes a method for dialogue emotion recognition that comprehensively considers sequential context information, dialogue relations between speakers, and event relations. Among other benchmark models, LR-GCN performs slightly worse than DER-GCN, with WA and WF1 values of 68.5% and 68.3%, respectively. We speculate that LR-GCN outperforms other baseline models because it considers both the interaction between speakers and the latent semantic relationship of the dialogue context. However, LR-GCN ignores the event relations in the dialogue, so its emotion recognition effect is lower than that of the model proposed in this paper, DER-GCN. The emotion prediction effect of A-DMN and LFM is much lower than that of DER-GCN, with WA values of 64.6% and 63.4%, and WF1 values of 64.3% and 62.7%, respectively. It is because they do not model speaker relations and event relations in dialogue although they design a fusion mechanism to obtain complementary multimodal semantic features. The emotion prediction performance of other baseline methods, such as TextCNN, is much worse than that of DER-GCN, because they only model sequential context information, which results in limited semantic information learned by the model.\n\nMELD: As shown in Table  2 , the emotion prediction effect of the DER-GCN model on the MELD dataset is better than other comparison algorithms, and the WA and WF1 values are 66.8% and 66.1%, respectively. The effect of LR-GCN is second, with WA and WF1 values of 65.7% and 65.6%, respectively. The prediction performance of A-DMN is lower than that of DER-GCN and LR-GCN, with WA and WF1 values of 61.5% and 60.4%, respectively. Other comparison algorithms perform poorly because they all ignore modeling the relationships between speakers. In addition, compared with other comparison algorithms, DER-GCN has significantly improved the prediction accuracy on the minority class sentiment labels \"fear\" and \"disgust\". Specifically, the WA and WF1 values of DER-GCN on the \"fear\" label are 14.8% and 10.4%, respectively, and the prediction effect is improved by about 10%. The WA and WF1 values of DER-GCN on the \"disgust\" label are 17.2% and 10.3%, respectively, and the prediction effect is improved by about 10%. We have guessed that DER-GCN can improve the prediction effect of minority class sentiment. The model adopts a loss optimization strategy based on the contrastive learning mechanism, which can better represent minority class features.\n\nThe experimental results show that the event relationship in the dialogue significantly strengthens the model's understanding of the speaker's emotion. In addition, crossmodal feature fusion and loss optimization strategy based on contrastive learning can also enhance the model's emotion classification ability.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Analysis Of The Experimental Results",
      "text": "To clarify the feature representation ability of the model on each emotion category, we analyze the distribution of emotion classification of DER-GCN on the test set. Fig.  5  presents the confusion matrix for emotion classification by DER-GCN on the IEMOCAP and MELD datasets.\n\nOn the IEMOCAP dataset, we observe the confusion matrix and find that DER-GCN easily misclassifies \"neutral\" sentiment into \"frustrated\" and \"sad\" sentiment. We believe that this is because there are semantically similar parts between \"neutral\" sentiment and \"frustrated\" or \"sad\" sentiment, which leads to fuzzy class boundaries in the representation of emotional features among different categories learned by DER-GCN. At the same time, we also find that the model incorrectly classified \"frustrated\" or \"sad\" sentiment as \"neutral\" sentiment. In addition, DER-GCN also has mutual misclassification between \"sad\" sentiment and \"frustrated\" sentiment. There is also overlapping semantic information between \"happy\" sentiment and \"excited\" sentiment, which makes it difficult for DER-GCN to distinguish these emotions. The classification effect of the model in \"sad\" or \"angry\" sentiment is relatively good. Most of the tested utterances can be correctly classified. For the \"excited\" sentiment, we find that DER-GCN misclassifies it as the \"sad\" sentiment. We think this is because    speakers usually express their emotions more implicitly and sarcastically when they talk about sensitive topics, and the model cannot capture this semantic information.\n\nThe MELD dataset shows a specific semantic correlation between the \"neutral\" sentiment and other types of emotion. Therefore, DER-GCN is prone to misclassify the \"neutral\" sentiment as other emotions. The opposite is also true. For the \"surprise\" sentiment, DER-GCN incorrectly classifies it into \"joy\" and \"anger\" sentiment. We guess this is because speakers with \"surprise\" sentiments are usually accompanied by \"joy\" or \"anger\" sentiments.\n\nOn the one hand, the speaker is stimulated by something wrong to produce surprise-like emotions, which will cause the speaker to feel angry. On the other hand, the speaker is surprised by surprise prepared by others, which will cause the speaker to feel joy. For the \"fear\" sentiment, the model is prone to misclassify it as the \"neutral\" sentiment. For the \"disgust\" sentiment, the number of test utterances correctly classified by DER-GCN is minimal, and the classification results are unreliable. This is because the number of \"disgust\" sentiments in the MELD dataset is very small. DER-GCN cannot learn effective semantic information from such a small amount of data. At the same time, this problem also exists in the \"fear\" category of emotions. For the \"angry\" sentiment, DER-GCN not only misclassifies it as the \"surprise\" sentiment but also misclassifies it as the \"sadness\" or the \"joy\" sentiment. On the one hand, speakers with \"angry\" emotions are usually accompanied by \"sadness\" sentiments. On the other hand, speakers with an \"angry\" sentiment may be more implicit in expressing their emotions. The above two reasons may lead to biases in DER-GCN in understanding the semantics of test utterances.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Importance Of The Modalities",
      "text": "To verify the importance of the three modal features of text, video, and audio, we conduct experiments on the IEMOCAP and MELD datasets to compare the performance of unimodal, bimodal, and multimodal features. The experimental results are shown in the Table  3 . Due to the problem of data imbalance in the dataset, WF1 comprehensively considers the precision rate and recall rate. So we choose WF1 as our main evaluation metric and WA as our secondary evaluation metric. For the experimental results of the single modality, the values of WA and WF1 of the text modality are higher than the audio and video modality. The values of WA are 63.2% and 62.8% on the IEMOCAP and MELD datasets, respectively, and the values of WF1 are 63.8% and 61.9% on the IEMOCAP and MELD datasets, respectively, which indicates that the text modal features play the most important role in the emotion recognition of the model. The effect of the audio modality is second, the values of WA are 61.4% and 62.1%, respectively, and the values of WF1 are 61.6% and 61.3%, respectively. The video modality performs the worst, with values of 57.8% and 60.5% for WA, respectively, and with values of 57.1% and 60.6% for WF1, respectively, indicating that it is difficult for the model to extract useful emotional features from video features. The experimental results show that the noise introduced by the text features is the least, which will benefit the model in learning the embedded representation of the emotional features. The experimental results of bi-modality are better than single-modality. The WA value is improved by 0.2% to 8%, and the WF1 value is improved by 0.7% to 7%. It indicates that emotional features are not only related to contextual information but also changes in sound signals in audio and facial expressions in video. The bimodal feature combines two different unimodal features, which can effectively improve the emotion prediction effect of the model. Furthermore, the bimodal features fused with text and audio performed the best emotion prediction, with values of 65.8% and 63.8% for WA, respectively, and with values of 64.7% and 62.6% for WF1, respectively. The emotion prediction effect of bimodal features fused by text and video is second, with the values of 64.4% and 63.1% for WA, respectively, and with the values of 64.0% and 63.4% for WA, respectively. The bimodal features fused with audio and video have the worst emotion prediction performance, with values of 61.2% and 60.3% for WA, respectively, and with values of 60.9% and 59.8% for WF1, respectively.\n\nAfter the fusion of three modal features of text, video, and audio, the multi-modal features have the best emotion prediction performance. It is better than the performance of single-modal and bi-modal features, which indicates that the model not only utilizes the semantic information of the dialogue context. It also utilizes video and audio features to enhance the representation ability of the emotional feature vectors.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Effectiveness Of Cross-Modal Feature Fusion",
      "text": "In this section, to verify the effectiveness of the cross-modal feature fusion method proposed in this paper, we compare it with other three fusion methods,. i.e., add and concatenation operation, and tensor fusion network (TFN).\n\nThe experimental results are shown in Table  4 . Compared with other multi-modal feature fusion methods, the cross-modal feature fusion method proposed in this paper has achieved the best experimental results, and the values of WA are 69.7% and 66.8%, respectively, and the values of WF1 are 69.4% and 66.1%, respectively. Specifically, compared with the Add method, the WA value of the crossmodal feature fusion method is improved by 3.9% to 4.5%, and the WF1 value is improved by 3.7% to 4.6%. We think the Add method cannot capture the complementary semantic information between different modalities. The crossmodal feature fusion method can extract the most relevant semantic information with emotional features through the attention mechanism, thereby improving the emotion recognition effect of the model. At the same time, compared with the Concatenate method, the WA value of the cross-modal feature fusion method is improved by 4.3% to 5.1%, and the WF1 value is improved by 4.5% to 5.3%. The reason is that the feature vector dimensions of the text, video, and audio modalities are high, leading to the combinational explosion of multimodal embedding representations generated by feature concatenation. Different from the Concatenate method, the cross-modal feature fusion method can achieve efficient feature dimensionality reduction while capturing rich semantic information. In addition, compared with the Tensor Fusion method, the WA value of the cross-modal feature fusion method is improved by 3% to 3.1%, and the WF1 value is improved by 2.6% to 3.8%. It is because the Tensor Fusion method needs to use tensors for feature representation, which introduces much computational consumption and reduces emotion recognition accuracy. The above experimental results demonstrate the effectiveness of the cross-modal feature fusion method proposed in this paper.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Effectiveness Of Bi-Gru",
      "text": "To verify the effectiveness of Bi-GRU for contextual semantic information extraction, we use three methods for comparative experiments. The experimental results are shown in Table  5 .\n\n• Without contextual modeling: This method does not use any contextual information modeling method for emotion recognition. Specifically, we replace the GRU layers with linear layers.  • Unidirectional GRU (Uni-GRU): Instead of modeling context information, we use a unidirectional GRU to extract contextual semantic information, which can memorize utterance information before the current moment.\n\n• Bidirectional GRU (Bi-GRU): Different from the above methods, we use Bi-GRU to model two opposite contextual utterances, which contain richer contextual information.\n\nAmong the three contrasting methods, we find that the emotion recognition method that does not model contextual semantic information works the worst, with WA values of 62.3% and 60.1% on IEMOCAP and MELD datasets, respectively, and with WF1 values of 61.7% and 61.6%, indicating the necessity of contextual semantic information modeling. The Uni-GRU method outperforms methods that do not model contextual semantic information, with values of 67.1% and 63.4% for WA , respectively, with values of 66.2% and 63.0% for WF1 , respectively. Bi-GRU performs the best for emotion recognition, with WA values of 69.7% and 66.8%, respectively, and with WF1 values of 69.4% and 66.1%. Compared with the other two methods, the WA value is increased by 2.6% to 7.4%, and the WF1 value is increased by 3.1% to 7.7%. Therefore, the experimental results show that the emotional information of the current moment is related to both historical discourse and future discourse.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Discussion On Contextual Distances",
      "text": "We analyze test utterances correctly classified by DER-GCN on the IEMOCAP and MELD datasets. As shown in Fig.  6 , we present the distribution of distances between the current test utterance, and the second-highest attended utterance in the context. The utterances with correct sentiment classification have the strongest context-dependence on the current moment. Furthermore, as the distance increases, the dependence of the current utterance on the distant context decreases. However, we found that a significant portion of utterances will focus on contextual utterances with distances between 20 and 30, which indicates the need for contextual semantic information modeling. Therefore, the modeling dialogue texts' long-distance dependencies can improve the model's emotion recognition accuracy.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Visualization",
      "text": "To more intuitively display the emotional feature vectors in high-dimensional space, we use the t-SNE  [51]  method to reduce the dimensionality of the original emotional features in the IEMOCAP dataset and the emotional features obtained after model learning and obtain a two-dimensional spatial distribution.\n\nAs shown in Fig.  7 (a), we can see that the original emotional feature distribution of IEMOCAP is very scattered, and there are many overlapping parts between each emotion category, which will cause the model to fail to classify each emotion type correctly. As shown in Fig.  7 (b), we find that the feature representations learned by bc-LSTM have ambiguous class boundary information, and the overlap between each emotion category is greatly reduced, which leads to a better feature distribution than the original sentiment feature distribution. It is because bc-LSTM can utilize the contextual information of the utterance to enhance the feature representation ability of each emotion. Therefore, it makes the feature vector more discriminative in the spatial distribution. However, bc-LSTM does not consider the interaction between speakers, and the semantic information learned by the model is insufficient, which leads to limited sentiment classification ability of the learned class boundaries. As shown in Fig.  7(c ), the feature representations learned by DialogueGCN have clearer class boundary information than bc-LSTM, which leads to the stronger sentiment classification ability of the model. We believe this is because DialogueGCN considers the semantic information of the extracted sequential context and models the dialogue relationship between speakers. Embedding representations learned by DialogueGCN have richer semantic information, which leads to different emotion categories being more discriminative in spatial distribution. However, DialogueGCN can still not distinguish on some test utterances because the speaker expresses his emotions implicitly and vaguely when talking about specific events, while the model cannot capture this semantic information. As shown in Fig.  7(d",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "This paper proposes the Dialogue and Event Relation-Aware Graph Convolutional Neural Networks (DER-GCN) model, which enables multimodal emotion recognition for multiple dialogue relations. In order to capture the potential semantic information related to the dialogue topic during the dialogue process, we use an event extraction method to extract the main events in the dialogue. In order to obtain better node embedding representation, we design a graph autoencoder based on node and edge masking mechanism, which reconstructs the original graph's topological structure and features vectors through self-supervised learning. We introduce a sampling strategy based on contrastive learning to alleviate the data imbalance problem. DER-GCN is used to learn optimal network parameters in the multimodal emotion recognition task. On the IEMOCAP and MELD benchmark datasets, DER-GCN has greatly improved the effect of emotion recognition compared with other comparison algorithms.\n\nIn future research work, we will consider and adopt self-supervised learning algorithms for multimodal emotion recognition, thereby adding large-scale unlabeled dialogue data to our model to improve the generalization ability of the model. Furthermore, we will also consider how to extend our model to other multimodal recognition tasks.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An illustrative example of the impact of event rela-",
      "page": 1
    },
    {
      "caption": "Figure 1: as an example, Fig. 1(b) is a graph that only",
      "page": 2
    },
    {
      "caption": "Figure 1: (d). Fig. 1(c) is a graph that",
      "page": 2
    },
    {
      "caption": "Figure 1: (e). Hence, it is necessary to take the event relationship",
      "page": 2
    },
    {
      "caption": "Figure 2: The structure of DER-GCN",
      "page": 4
    },
    {
      "caption": "Figure 2: (a) The overall process framework of DER-GCN: It first preprocesses multimodal data to obtain encoded feature",
      "page": 5
    },
    {
      "caption": "Figure 3: , we build a multi-relational af-",
      "page": 5
    },
    {
      "caption": "Figure 3: (a) Heterogeneous dialogue graph composed of dialogue relations and event relations. (b) We split the heterogeneous",
      "page": 6
    },
    {
      "caption": "Figure 4: , MIT is composed of Transformers",
      "page": 7
    },
    {
      "caption": "Figure 4: The Multiple Information Transformer (MIT) consists",
      "page": 8
    },
    {
      "caption": "Figure 5: presents the confusion matrix for emotion classification by",
      "page": 9
    },
    {
      "caption": "Figure 5: The classification of DER-GCN and LR-GCN on the IEMOCAP and MELD dataset.",
      "page": 10
    },
    {
      "caption": "Figure 6: Histograms of attention scores for IEMOCAP and",
      "page": 12
    },
    {
      "caption": "Figure 6: , we present the distribution of distances between the",
      "page": 12
    },
    {
      "caption": "Figure 7: (a), we can see that the original emo-",
      "page": 12
    },
    {
      "caption": "Figure 7: (b), we find",
      "page": 12
    },
    {
      "caption": "Figure 7: (c), the feature representa-",
      "page": 12
    },
    {
      "caption": "Figure 7: Visualizing feature embeddings for the multimodal sentiment on the IEMOCAP benchmark dataset. Each dot",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison with other baseline models on the IEMOCAP dataset, Acc.=Accuracy, Average(w) = Weighted",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "Happy\nSad\nNeutral\nAngry\nExcited\nFrustrated\nAverage(w)"
        },
        {
          "Methods": "",
          "IEMOCAP": "Acc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1"
        },
        {
          "Methods": "TextCNN\nbc-LSTM\nCMN\nLFM\nA-DMN\nLR-GCN\nDER-GCN",
          "IEMOCAP": "27.7 29.8\n57.1 53.8\n34.3 40.1\n61.1 52.4\n46.1 50.0\n62.9 55.7\n48.9 48.1\n29.1 34.4\n57.1 60.8\n54.1 51.8\n57.0 56.7\n51.1 57.9\n67.1 58.9\n55.2 54.9\n25.0 30.3\n55.9 62.4\n52.8 52.3\n61.7 59.8\n55.5 60.2\n71.1 60.6\n56.5 56.1\n25.6 33.1\n75.1 78.8\n58.5 59.2\n64.7 65.2\n80.2 71.8\n61.1 58.9\n63.4 62.7\n88.3 77.9\n43.1 50.6\n69.4 76.8\n63.0 62.9\n63.5 56.5\n53.3 55.7\n64.6 64.3\n54.2 55.5\n81.6 79.1\n59.1 63.8\n69.4 69.0\n76.3 74.0\n68.2 68.9\n68.5 68.3\n60.7 58.8\n71.3 72.1\n69.7 69.4\n75.9 79.8\n66.5 61.5\n71.1 73.3\n66.1 67.8"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 1: Comparison with other baseline models on the IEMOCAP dataset, Acc.=Accuracy, Average(w) = Weighted",
      "data": [
        {
          "Methods": "",
          "MELD": "Neutral\nSurprise\nFear\nSadness\nJoy\nDisgust\nAnger\nAverage(w)"
        },
        {
          "Methods": "",
          "MELD": "Acc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1"
        },
        {
          "Methods": "TextCNN\nbc-LSTM\nDialogueRNN\nA-DMN\nLR-GCN\nDER-GCN",
          "MELD": "76.2 74.9\n43.3 45.5\n4.6 3.7\n18.2 21.1\n46.1 49.4\n8.9 8.3\n35.3 34.5\n56.3 55.0\n78.4 73.8\n46.8 47.7\n3.8 5.4\n22.4 25.1\n51.6 51.3\n4.3 5.2\n36.7 38.4\n57.5 55.9\n72.1 73.5\n54.4 49.4\n1.6 1.2\n23.9 23.8\n52.0 50.7\n1.5 1.7\n41.0 41.5\n56.1 55.9\n76.5 78.9\n56.2 55.3\n8.2 8.6\n22.1 24.9\n59.8 57.4\n1.2 3.4\n41.3 40.9\n61.5 60.4\n81.5 80.8\n55.4 57.1\n0.0 0.0\n36.3 36.9\n62.2 65.8\n7.3 11.0\n52.6 54.7\n65.7 65.6\n14.8 10.4\n56.7 41.5\n17.2 10.3\n66.8 66.1\n76.8 80.6\n50.5 51.0\n69.3 64.3\n52.5 57.4"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Time-frequency representation and convolutional neural network-based emotion recognition",
      "authors": [
        "S Khare",
        "V Bajaj"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "2",
      "title": "Omg: Towards effective graph classification against label noise",
      "authors": [
        "N Yin",
        "L Shen",
        "M Wang",
        "X Luo",
        "Z Luo",
        "D Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "3",
      "title": "Messages are never propagated alone: Collaborative hypergraph neural network for time-series forecasting",
      "authors": [
        "N Yin",
        "L Shen",
        "H Xiong",
        "B Gu",
        "C Chen",
        "X Hua",
        "S Liu",
        "X Luo"
      ],
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Improving speech emotion recognition with adversarial data augmentation network",
      "authors": [
        "L Yi",
        "M.-W Mak"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "5",
      "title": "Pirnet: Personality-enhanced iterative refinement network for emotion recognition in conversation",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "6",
      "title": "Coco: A coupled contrastive framework for unsupervised domain adaptive graph classification",
      "authors": [
        "N Yin",
        "L Shen",
        "M Wang",
        "L Lan",
        "Z Ma",
        "C Chen",
        "X.-S Hua",
        "X Luo"
      ],
      "year": "2023",
      "venue": "Coco: A coupled contrastive framework for unsupervised domain adaptive graph classification",
      "arxiv": "arXiv:2306.04979"
    },
    {
      "citation_id": "7",
      "title": "Deal: An unsupervised domain adaptive framework for graph-level classification",
      "authors": [
        "N Yin",
        "L Shen",
        "B Li",
        "M Wang",
        "X Luo",
        "C Chen",
        "Z Luo",
        "X.-S Hua"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia, ser. MM '22"
    },
    {
      "citation_id": "8",
      "title": "Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "S Yang",
        "K Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "9",
      "title": "Object detection in medical images based on hierarchical transformer and mask mechanism",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "C Xie",
        "H Liu",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "10",
      "title": "A multimessage passing framework based on heterogeneous graphs in conversational emotion recognition",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "J Du",
        "H Liu",
        "K Li"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "11",
      "title": "Prediction model of dow jones index based on lstm-adaboost",
      "authors": [
        "R Ying",
        "Y Shou",
        "C Liu"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Communications, Information System and Computer Engineering (CISCE)"
    },
    {
      "citation_id": "12",
      "title": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2023",
      "venue": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "arxiv": "arXiv:2312.06337"
    },
    {
      "citation_id": "13",
      "title": "Mode variational lstm robust to unseen modes of variation: Application to facial expression recognition",
      "authors": [
        "W Baddar",
        "Y Ro"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Eigen-gnn: a graph structure preserving plug-in for gnns",
      "authors": [
        "Z Zhang",
        "P Cui",
        "J Pei",
        "X Wang",
        "W Zhu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "16",
      "title": "Graph information bottleneck for remote sensing segmentation",
      "authors": [
        "Y Shou",
        "W Ai",
        "T Meng"
      ],
      "year": "2023",
      "venue": "Graph information bottleneck for remote sensing segmentation",
      "arxiv": "arXiv:2312.02545"
    },
    {
      "citation_id": "17",
      "title": "Czl-ciae: Clip-driven zeroshot learning for correcting inverse age estimation",
      "authors": [
        "Y Shou",
        "W Ai",
        "T Meng",
        "K Li"
      ],
      "year": "2023",
      "venue": "Czl-ciae: Clip-driven zeroshot learning for correcting inverse age estimation",
      "arxiv": "arXiv:2312.01758"
    },
    {
      "citation_id": "18",
      "title": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2023",
      "venue": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "arxiv": "arXiv:2312.05735"
    },
    {
      "citation_id": "19",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "1907",
      "venue": "ArXiv"
    },
    {
      "citation_id": "20",
      "title": "A 3d cnn-lstm-based image-to-image foreground segmentation",
      "authors": [
        "T Akilan",
        "Q Wu",
        "A Safaei",
        "J Huo",
        "Y Yang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "21",
      "title": "Transfer learning from speaker verification to multispeaker text-to-speech synthesis",
      "authors": [
        "Y Jia",
        "Y Zhang",
        "R Weiss",
        "Q Wang",
        "J Shen",
        "F Ren",
        "P Chen",
        "R Nguyen",
        "I Pang",
        "Y Moreno",
        "Wu"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "22",
      "title": "Revisiting the evaluation of end-to-end event extraction",
      "authors": [
        "S Zheng",
        "W Cao",
        "W Xu",
        "J Bian"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
    },
    {
      "citation_id": "23",
      "title": "Graphmae: Self-supervised masked graph autoencoders",
      "authors": [
        "Z Hou",
        "X Liu",
        "Y Cen",
        "Y Dong",
        "H Yang",
        "C Wang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "24",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Causal graph convolutional neural network for emotion recognition",
      "authors": [
        "W Kong",
        "M Qiu",
        "M Li",
        "X Jin",
        "L Zhu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "J Ding",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Exploring temporal representations by leveraging attention-based bidirectional lstm-rnns for multi-modal emotion recognition",
      "authors": [
        "C Li",
        "Z Bao",
        "L Li",
        "Z Zhao"
      ],
      "year": "2020",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "28",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "J Huang",
        "J Tao",
        "B Liu",
        "Z Lian",
        "M Niu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Summarize before aggregate: A global-to-local heterogeneous graph inference network for conversational emotion recognition",
      "authors": [
        "D Sheng",
        "D Wang",
        "Y Shen",
        "H Zheng",
        "H Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "30",
      "title": "Spatiotemporal graph neural network based mask reconstruction for video object segmentation",
      "authors": [
        "D Liu",
        "S Xu",
        "X.-Y Liu",
        "Z Xu",
        "W Wei",
        "P Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Self-supervised heterogeneous graph neural network with co-contrastive learning",
      "authors": [
        "X Wang",
        "N Liu",
        "H Han",
        "C Shi"
      ],
      "year": "2021",
      "venue": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining"
    },
    {
      "citation_id": "32",
      "title": "Hard negative mixing for contrastive learning",
      "authors": [
        "Y Kalantidis",
        "M Sariyildiz",
        "N Pion",
        "P Weinzaepfel",
        "D Larlus"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "33",
      "title": "Heterogeneous graph contrastive learning network for personalized microvideo recommendation",
      "authors": [
        "D Cai",
        "S Qian",
        "Q Fang",
        "J Hu",
        "W Ding",
        "C Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "34",
      "title": "Open-set fault diagnosis via supervised contrastive learning with negative out-of-distribution data augmentation",
      "authors": [
        "P Peng",
        "J Lu",
        "T Xie",
        "S Tao",
        "H Wang",
        "H Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "35",
      "title": "Merastc: Micro-expression recognition using effective feature encodings and 2d convolutional neural network",
      "authors": [
        "P Gupta"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "37",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "38",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL"
    },
    {
      "citation_id": "39",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "40",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "41",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "42",
      "title": "Adapted dynamic memory network for emotion recognition in conversation",
      "authors": [
        "S Xing",
        "S Mai",
        "H Hu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "44",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "T Ishiwatari",
        "Y Yasuda",
        "T Miyazaki",
        "J Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "45",
      "title": "Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "W Li",
        "D Song",
        "W Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Bagher",
        "L.-P Zadeh",
        "Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "47",
      "title": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "authors": [
        "J Lee",
        "W Lee"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "48",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "T Kim",
        "P Vossen"
      ],
      "year": "2021",
      "venue": "Computing Research Repository"
    },
    {
      "citation_id": "49",
      "title": "Cogmen: Contextualized gnn based multimodal emotion recognition",
      "authors": [
        "A Joshi",
        "A Bhat",
        "A Jain",
        "A Singh",
        "A Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "50",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "51",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    }
  ]
}