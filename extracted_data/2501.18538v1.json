{
  "paper_id": "2501.18538v1",
  "title": "Mini-Resemotenet: Leveraging Knowledge Distillation For Human-Centered Design",
  "published": "2025-01-30T18:06:44Z",
  "authors": [
    "Amna Murtada",
    "Omnia Abdelrhman",
    "Tahani Abdalla Attia"
  ],
  "keywords": [
    "Facial Emotion Recognition",
    "Human-Computer Interaction",
    "User Experience",
    "Knowledge Distillation",
    "Human Centered Design"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial Emotion Recognition has emerged as increasingly pivotal in the domain of User Experience, notably within modern usability testing, as it facilitates a deeper comprehension of user satisfaction and engagement. This study aims to extend the ResEmoteNet model by employing a knowledge distillation framework to develop Mini-ResEmoteNet models-lightweight student models-tailored for usability testing. Experiments were conducted on the FER2013 and RAF-DB datasets to assess the efficacy of three student model architectures: Student Model A, Student Model B, and Student Model C, their development involves reducing the number of feature channels in each layer of the teacher model by approximately 50%, 75%, and 87.5% respectively. Besides demonstrating exceptional performance on the FER2013 dataset, Student Model A (E1) achieved a test accuracy of 76.33%, marking a 0.21% absolute improvement over EmoNeXt, Moreover, the results exhibit absolute improvements in terms of inference speed and memory usage during inference compared to the ResEmoteNet model, the findings indicate that the proposed methods surpass other stateof-the-art approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial expression recognition (FER) has become a crucial field of study in computer vision with important applications in human-computer interaction, emotion analysis, and usability testing. Subtle variations in facial components such as lips, teeth, skin, hair, cheekbones, nose, face shape, eyebrows, eyes, jawline, and mouth complicate the FER task. FER systems operate through three main stages: face acquisition, feature extraction, and emotion classification. These stages enable the detection and interpretation of human facial expressions across seven predefined emotional categories. In the face acquisition stage, methods like Faster R-CNN and YOLO  [1]  are employed for efficient face detection achieving high accuracy by localizing facial regions in images. In feature extraction, CNN-based models like ResNet capture hierarchical patterns from facial data, while SE networks  [2]  recalibrate channel importance to focus on emotionally relevant information. Vision transformers  [3]  offer the ability to capture long-range dependencies across facial features. In the emotion classification stage, extracted features are assigned to one of the seven emotion classes. This process is typically performed by specialized deep learning models, with multimodal approaches integrating visual and contextual data for improved accuracy  [4] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "FER has advanced significantly due to deep learning, but high computational costs remain challenging for real-time deployment. Computational costs refer to resources like memory usage, inference time, and parameter count. Large models like Vision Transformers and SE-enhanced ResNets require substantial resources which limits scalability in constrained environments, TABLE  I . illustrates how different FER models compare in size and computational complexity.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Table I. Model Size And Parameters Comparison Of Resemotenet With Existing State-Of-The-Art Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Fer Models Model Size (Mb)",
      "text": "Total Parameters  POSTER++ [5]  233.27 58,316,594 QCS  [6]  331.50 82,874,119 FMAE  [7]  1217.31 304,326,632\n\nSegmentation VGG-19  [8]  676.88 169,220,807\n\nEmoNeXt  [9]  122.26 30,564,331\n\nEnsemble ResMaskingNet  [10]  85.15 21,288,263\n\nResEmoteNet  [11]  320.95 80,238,599\n\nPOSTER++  [5]  and QCS  [6]  balance model size and performance, but their parameter counts (58.3M and 82.8M, respectively) still indicate significant computational costs, particularly for real-time systems. FMAE  [7]  represents a highly complex model, with a size exceeding 1.2 GB, while its performance is likely superior. Segmentation VGG-19  [8] is another large model, with a size of 676.88 MB. While slightly more efficient than FMAE, it remains computationally prohibitive for many applications. EmoNeXt  [9]  and Ensemble ResMaskingNet  [10]  represent lightweight models that prioritize efficiency. Ensemble ResMaskingNet, in particular, demonstrates an optimal design for low-resource environments, with only 21.3 million parameters and a size of 85.15 MB.\n\nResEmoteNet  [11] sits between these extremes, providing a balance between high performance and computational efficiency. With a size of 320.95 MB and 80.2 million parameters, it is more resource-efficient than models like FMAE and Segmentation VGG-19 but not as lightweight as Ensemble ResMaskingNet. ResEmoteNet achieves this balance by integrating advanced features such as squeeze-andexcitation (SE) blocks and residual connections, which enhance its classification performance but add to its computational requirements as illustrated in Fig.  1 .\n\nThe increasing demand for real-time FER systems has prompted the development of lightweight architectures that reduce computational complexity while maintaining accuracy. MobileNet  [11]  significantly reduced the number of parameters and floating-point operations by implementing depthwise separable convolutions. EfficientNet  [13]  advanced lightweight architectures through the use of a compound scaling strategy that methodically modified network dimensions to strike a balance between accuracy and efficiency. Despite significant advancements, lightweight architectures frequently struggle to handle imbalanced datasets and subtle emotional expressions. These challenges highlight the importance of hybrid approaches that combine lightweight design principles with optimization techniques such as knowledge distillation.\n\nKnowledge distillation, introduced by  [14] , reduces the computational demands of deep learning models by transferring knowledge from a large, high-performing teacher model to a smaller student model. This process enables the student model to mimic the teacher's behavior while operating with significantly reduced resources, making it particularly valuable for real-time FER applications. Loss functions, such as distillation loss and cross-entropy loss, balance imitating the teacher, and aligning with ground truth labels. T and Alpha hyper-parameter fine-tune the distillation process, ensuring accuracy and efficiency in the student model. Knowledge distillation has optimized resource-intensive models like vision transformers and SE-enhanced CNNs for real-time use  [15] . It enables smaller models to inherit the strengths of their larger counterparts while improving generalizability to diverse real-world conditions, such as varying lighting and occlusions.\n\nTo close these gaps and enhance ResEmoteNet for usability testing applications, In this paper, ResEmoteNet is used as a teacher model and applies knowledge distillation to create lightweight adaptations that will improve computational performance in student models. ResEmoteNet has been chosen as it outperforms state-of-the-art models across four FER databases  [11] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Student Models Architecture:",
      "text": "In the construction of the student models, we opted to reduce the parameters of the teacher model by varying proportions to examine the of parameter reduction without compromising model accuracy. This investigation led to the development of three distinct student model architectures: Student Model A involves reducing the number of feature channels (filters) in each layer of the teacher model by approximately 50%; Student Model B reduces the feature channels by approximately 75%; and Student Model C further reduces them by approximately 87.5%, as illustrated in TABLE  II .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Our Knowledge Distillation Method:",
      "text": "Employing ResEmoteNet as a teacher model within the framework of knowledge distillation, the methodology prioritizes the extraction of hard labels and soft predictions from the teacher model, culminating in a composite of distillation loss functions. The input data is subsequently propagated through the student model for processing. Fig.  2  illustrates the proposed method.\n\nIn the mechanism of Soft Target Distillation, temperature scaling is implemented via the temperature hyper-parameter T to smooth the probability distributions, thereby enhancing the efficacy of knowledge distillation. This smoothing is achieved by dividing the logits by T before the application of the softmax function. The teacher model's softened logits convey comprehensive information regarding class relationships, thereby simplifying the learning process to a certain degree. The use of the KL divergence loss facilitates the transfer of knowledge between the teacher and student soft predictions, seemingly enabling accelerated training. Where ùëÉ ùëñ,ùëê is the \"softened\" probability distribution of the teacher model and ùëÑ ùëñ,ùëê is the \"softened\" probability distribution of the student model. In the context of Hard Label Learning, the Cross-entropy loss is employed to guarantee that the student model sustains its accuracy concerning the primary task while preserving direct supervision through ground truth labels; this loss can be calculated between the predictions of the student model and the ground truth labels.\n\nWhere C is the number of classes, ùë¶ ùëñ,ùëê is the ground truth label for class c (1 for the correct class, 0 otherwise), ùë¶`ùëñ ,ùëê is the predicted probability (softmax output) for class c for sample i. Then the total loss function is a weighted combination of two components: ùêøùëúùë†ùë† = ùê∂ùëüùëúùë†ùë†ùê∏ùëõùë°ùëüùëúùëùùë¶ùêøùëúùë†ùë† + ùõΩ √ó ùëá 2 √ó ùêøùëúùë†ùë†ùêæùê∑ (6) Where Œ± is the weight for hard label loss, Œ≤ is the weight for distillation loss, T is the temperature parameter, CrossEntropyLoss is Cross-entropy loss with hard labels and KLDivLoss is KL divergence loss between softened predictions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Datasets:",
      "text": "This subsection presents an overview of the datasets utilized in this study. Our experimental investigations were conducted using two datasets: FER2013  [16]  and RAF-DB  [17] . Our facial emotion recognition task aims to identify seven fundamental emotions: Angry, Disgust, Fear, Happy, Neutral, Sad, and Surprise.  The FER2013 dataset is Ideal for benchmarking and developing models for controlled conditions. Its lower resolution and noise make it a good dataset for testing noise Through evaluation of temperature values 1, 2, 3, 4, and 5, it was determined that a temperature of 3 consistently resulted in superior performance. This is likely attributable to the fact that a temperature of 3 offered an ideal balance between capturing intricate class relationships and preserving a robust training signal. Furthermore, the softer target probabilities achieved at T=3 may have enabled the student model to generalize more effectively by assimilating broader class representations, thereby avoiding overfitting the teacher's most confident predictions. Additionally, experimentation with alpha values ranging from 0.10, 0.15, and 0.20 demonstrated that alphas of 0.15 and 0.20 were optimal, as the model's performance reached saturation at these points, indicating an ideal equilibrium between learning from the teacher's soft labels and the ground truth labels. This equilibrium allowed the student models to adeptly capture nuanced inter-class relationships while maintaining adherence to the hard-label classification task.\n\nOur experimental evaluations were executed utilizing the PyTorch framework on an NVIDIA Tesla P100 GPU infrastructure provided by Kaggle. In assessing the performance of our facial emotion recognition system, we employed Accuracy (%) as the principal metric, defined as:  [11]  where TP is True Positive, TN is True Negative, FP is False Positive, and FN is False Negative.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Results And Discussion",
      "text": "This section delineates the outcomes of our experimental investigations conducted on the benchmark datasets FER2013 and RAF-DB. We appraised the efficacy of our proposed approach, the ResEmoteNet student models, with the testing results systematically organized in Table  III  for FER2013 and Table  IV  for RAF-DB.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Resemotenet Student Models Performance Across",
      "text": "Datasets:\n\nWe commenced our study with the FER2013 dataset, primarily due to its challenging characteristics arising from issues such as inaccurate labeling, the absence of faces in certain images, and an imbalanced data distribution. TABLE  IV . presents the experimental results of FER2013, the initial experimentation involved student model A, configured with a temperature of 3 and an alpha value of 0.20. This model appeared to exhibit the optimal performance with a test accuracy of 76.33%, which is a reduction of 3.46% compared to the teacher model.\n\nThe next best performance was observed during the fourth experiment with student model B, which was also configured with a temperature of 3 but with an alpha of 0.15, resulting in a test accuracy of 70.20%, indicating a decrease of 9.59% relative to the teacher model. Further experimentation in the sixth trial explored the potential of student model C, comprising merely 1,259,911 parameters, which achieved an accuracy of 58.58%, thereby exhibiting a decline of 21.21% from the teacher model. Additionally, we provide the confusion matrices of the ResEmoteNet student model for the Happy Surprise Sad Disgust Angry Fear Neutral   IV . depicted the experimental results on RAF-DB, which is known for presenting real-world challenges including variations in pose, lighting, and occlusion, was deemed suitable for evaluating the extensive capabilities of student models A and B. Under identical hyper-parameter settings, student model A (E1) achieved a test accuracy of 84.21%, which is 10.55% lower than that of the teacher model. In comparison, student model B (E4) attained a test accuracy of 81.41%, showing a reduction of 13.35% relative to the teacher model.  In addition to demonstrating exceptional performance on the FER2013 dataset, the student model A (E1) achieved a test accuracy of 76.42% while utilizing 5088.46 MB of memory during inference. In contrast, the student model B (E4) obtained a test accuracy of 70.20% with a memory usage of 1814.97 MB during inference. These models exhibited absolute improvements in memory usage during inference by 49.63% and 82.05%, respectively, compared to the ResEmoteNet model, which consumes 10102.94 MB of memory. Furthermore, regarding inference speed, student models A (E1) and B (E2) recorded average inference times of 0.14 ms and 0.15 ms, respectively, representing absolute improvements of 90.00% and 89.29% over the ResEmoteNet teacher model, which has an inference time of 1.4 ms.\n\nIn the context of working with RAF-DB and evaluating student model performance on the Fer2013 dataset, student model A (E1) achieved a test accuracy of 76.42% with a memory usage of 827.96 MB during inference. In contrast, the student model B (E4) recorded a test accuracy of 70.20% while employing 748.72 MB of memory during inference. The student models A (E1) and B (E4) demonstrated absolute improvements over the ResEmoteNet model in terms of memory usage during inference, showing reductions of 1.22% and 10.68% respectively, compared to ResEmoteNet's 838.19 MB. Furthermore, regarding inference speed, student models A (E1) and B (E2) exhibited average inference times of 0.29 ms and 0.24 ms, respectively, representing absolute improvements of 90.94% and 92.50% over the ResEmoteNet teacher model, which has an inference time of 3.2 ms.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Student Resemotenet Models Performance Comparison With Prior Studies:",
      "text": "In TABLE VI. the efficacy of the proposed methods is evaluated against various state-of-the-art techniques, with the results indicating superior performance of our methodologies compared to existing approaches. The FER2013 dataset presents notable challenges due to issues such as inaccurate labeling, the presence of images devoid of facial features, and an imbalanced distribution of data. Despite these challenges, the student models achieved a classification accuracy of 76.33% with 20,069,383 parameters, representing a 0.21% absolute improvement over EmoNeXt  [9] , which operates with 30,564,331 parameters. In the context of RAF-DB, the ResEmoteNet student model A(E1) achieved a classification accuracy of 85.00%, marking a 0.2% enhancement over CMT VGGFACE  [18] . Additionally, student model B(E4) attained an accuracy of 82.45%, signifying a 1.05% absolute improvement over C MT PSR  [18] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusion",
      "text": "This study emphasizes how crucial transfer learning is to improving neural networks' ability to recognize facial emotions. We demonstrated that the student models Mini-ResEmoteNet greatly increase the overall resilience of neural networks by using the knowledge distillation approach. Our method's effectiveness is confirmed by the notable gains seen on the RAFDB and FER2013 benchmarks. Notably, Student Model A (E1) outperformed EmoNeXt by 0.21%, achieving a test accuracy of 76.33%. Additionally, Student Models A (E1) and B (E4) showed average inference durations of 0.14 and 0.15 ms, respectively, which are absolute gains over the instructor model's 1.4 ms operating time. In addition, the results demonstrate that the memory consumption during inference is increased by 49.63% and 82.05%, respectively, in comparison to the ResEmoteNet model, which uses 10,102.94 MB of memory. According to the results, the suggested techniques outperform other cutting-edge techniques. These outcomes demonstrate the efficiency of our method in precisely identifying facial emotions, providing noteworthy progress in the area of facial emotion detection. Additional improvements to Mini-ResEmoteNet models and integration of the Mini-ResEmoteNet to develop a usability testing framework will be investigated in future research.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Showcasing The Components of ResEmoteNet  [11]",
      "page": 2
    },
    {
      "caption": "Figure 2: The Proposed Knowledge Distillation Method",
      "page": 2
    },
    {
      "caption": "Figure 3: Showcasing Images of The 7 Emotion Classes",
      "page": 3
    },
    {
      "caption": "Figure 4: The Confusion Matrices of The ResEmoteNet Student Models",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FER Models": "POSTER++ [5]",
          "Model Size \n(MB)": "233.27",
          "Total \nParameters": "58,316,594"
        },
        {
          "FER Models": "QCS [6]",
          "Model Size \n(MB)": "331.50",
          "Total \nParameters": "82,874,119"
        },
        {
          "FER Models": "FMAE [7]",
          "Model Size \n(MB)": "1217.31",
          "Total \nParameters": "304,326,632"
        },
        {
          "FER Models": "Segmentation VGG-19 [8]",
          "Model Size \n(MB)": "676.88",
          "Total \nParameters": "169,220,807"
        },
        {
          "FER Models": "EmoNeXt [9]",
          "Model Size \n(MB)": "122.26",
          "Total \nParameters": "30,564,331"
        },
        {
          "FER Models": "Ensemble ResMaskingNet [10]",
          "Model Size \n(MB)": "85.15",
          "Total \nParameters": "21,288,263"
        },
        {
          "FER Models": "ResEmoteNet [11]",
          "Model Size \n(MB)": "320.95",
          "Total \nParameters": "80,238,599"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "",
          "Models Architecture": "CNN",
          "Total \nParameters": ""
        },
        {
          "Models": "ResEmoteNet",
          "Models Architecture": "64,128,256",
          "Total \nParameters": "80,238,599"
        },
        {
          "Models": "Student Model \n(A)",
          "Models Architecture": "32, 64, 128",
          "Total \nParameters": "20,069,383"
        },
        {
          "Models": "Student Model \n(B)",
          "Models Architecture": "16,32, 64",
          "Total \nParameters": "5,022,215"
        },
        {
          "Models": "Student Model \n(C)",
          "Models Architecture": "8,16,32",
          "Total \nParameters": "1,259,911"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classes \nDistribution": "",
          "FER2013": "Training \nSet",
          "RAF-DB": "Training \nSet"
        },
        {
          "Classes \nDistribution": "Happy",
          "FER2013": "7215",
          "RAF-DB": "4772"
        },
        {
          "Classes \nDistribution": "Surprise",
          "FER2013": "3171",
          "RAF-DB": "1982"
        },
        {
          "Classes \nDistribution": "Sad",
          "FER2013": "4830",
          "RAF-DB": "2524"
        },
        {
          "Classes \nDistribution": "Angry",
          "FER2013": "3995",
          "RAF-DB": "705"
        },
        {
          "Classes \nDistribution": "Disgust",
          "FER2013": "436",
          "RAF-DB": "717"
        },
        {
          "Classes \nDistribution": "Fear",
          "FER2013": "4097",
          "RAF-DB": "281"
        },
        {
          "Classes \nDistribution": "Neutral",
          "FER2013": "4965",
          "RAF-DB": "1290"
        },
        {
          "Classes \nDistribution": "Total per set",
          "FER2013": "28709",
          "RAF-DB": "12271"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Characteristics \nStudent \nStudent \nCharacteristics \nResEmoteNet \nModel \nModel \n(E1A) \n(E4B)": "Model Size (MB) \n306.09 \n76.56 \n19.16"
        },
        {
          "Characteristics \nStudent \nStudent \nCharacteristics \nResEmoteNet \nModel \nModel \n(E1A) \n(E4B)": "FER2013"
        },
        {
          "Characteristics \nStudent \nStudent \nCharacteristics \nResEmoteNet \nModel \nModel \n(E1A) \n(E4B)": "Accuracy (%) \n79.79 \n76.42 \n70.20"
        },
        {
          "Characteristics \nStudent \nStudent \nCharacteristics \nResEmoteNet \nModel \nModel \n(E1A) \n(E4B)": "Memory Usage (MB) \n10102.94 \n5088.46 \n1814.97"
        },
        {
          "Characteristics \nStudent \nStudent \nCharacteristics \nResEmoteNet \nModel \nModel \n(E1A) \n(E4B)": "Average Inference \n1.4 \n0.14 \n0.15 \nTime (ms)"
        },
        {
          "Characteristics \nStudent \nStudent \nCharacteristics \nResEmoteNet \nModel \nModel \n(E1A) \n(E4B)": "RAF-DB"
        },
        {
          "Characteristics \nStudent \nStudent \nCharacteristics \nResEmoteNet \nModel \nModel \n(E1A) \n(E4B)": "Accuracy (%) \n94.76 \n85.00 \n82.45"
        },
        {
          "Characteristics \nStudent \nStudent \nCharacteristics \nResEmoteNet \nModel \nModel \n(E1A) \n(E4B)": "Memory Usage (MB) \n838.19 \n827.96 \n748.72"
        },
        {
          "Characteristics \nStudent \nStudent \nCharacteristics \nResEmoteNet \nModel \nModel \n(E1A) \n(E4B)": "Average Inference \n3.2 \n0.29 \n0.24 \nTime (ms)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy(%) \nAccuracy \nTable Head \nLoss \nT \nAlfa \n(%)": "FER2013"
        },
        {
          "Accuracy(%) \nAccuracy \nTable Head \nLoss \nT \nAlfa \n(%)": "Teacher Model \n(ResEmoteNet)"
        },
        {
          "Accuracy(%) \nAccuracy \nTable Head \nLoss \nT \nAlfa \n(%)": "E1: Student Model A"
        },
        {
          "Accuracy(%) \nAccuracy \nTable Head \nLoss \nT \nAlfa \n(%)": "E2: Student Model A"
        },
        {
          "Accuracy(%) \nAccuracy \nTable Head \nLoss \nT \nAlfa \n(%)": "E3: Student Model B"
        },
        {
          "Accuracy(%) \nAccuracy \nTable Head \nLoss \nT \nAlfa \n(%)": "E4: Student Model B"
        },
        {
          "Accuracy(%) \nAccuracy \nTable Head \nLoss \nT \nAlfa \n(%)": "E5: Student Model B"
        },
        {
          "Accuracy(%) \nAccuracy \nTable Head \nLoss \nT \nAlfa \n(%)": "E6: Student Model C"
        },
        {
          "Accuracy(%) \nAccuracy \nTable Head \nLoss \nT \nAlfa \n(%)": "RAF-DB"
        },
        {
          "Accuracy(%) \nAccuracy \nTable Head \nLoss \nT \nAlfa \n(%)": "Teacher Model \n- \n- \n94.76 \n0.1802 \n(ResEmoteNet)"
        },
        {
          "Accuracy(%) \nAccuracy \nTable Head \nLoss \nT \nAlfa \n(%)": "E1: Student Model A \n3  \n0.20 \n85.00 \n0.5213"
        },
        {
          "Accuracy(%) \nAccuracy \nTable Head \nLoss \nT \nAlfa \n(%)": "E4: Student Model B \n3  \n0.15 \n82.45 \n0.6225"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table Head": "",
          "Accuracy(%)": "FER2013",
          "Parameters": ""
        },
        {
          "Table Head": "C MT PSR [18]",
          "Accuracy(%)": "-",
          "Parameters": "-"
        },
        {
          "Table Head": "C MT VGGFACE [18]",
          "Accuracy(%)": "-",
          "Parameters": "-"
        },
        {
          "Table Head": "POSTER++ [5]",
          "Accuracy(%)": "-",
          "Parameters": "58,316,594"
        },
        {
          "Table Head": "QCS[6]",
          "Accuracy(%)": "-",
          "Parameters": "82,874,119"
        },
        {
          "Table Head": "FMAE [7]",
          "Accuracy(%)": "-",
          "Parameters": "304,326,632"
        },
        {
          "Table Head": "SegmentationVGG-19 \n[8]",
          "Accuracy(%)": "75.97",
          "Parameters": "169,220,807"
        },
        {
          "Table Head": "EmoNeXt[9]",
          "Accuracy(%)": "76.12",
          "Parameters": "30,564,331"
        },
        {
          "Table Head": "Ensemble \nResMaskingNet [10]",
          "Accuracy(%)": "76.82",
          "Parameters": "21,288,263"
        },
        {
          "Table Head": "ResEmoteNet[11]",
          "Accuracy(%)": "79.79",
          "Parameters": "80,238,599"
        },
        {
          "Table Head": "Student Model A(E1)",
          "Accuracy(%)": "76.33",
          "Parameters": "20,069,383"
        },
        {
          "Table Head": "Student Model B(E4)",
          "Accuracy(%)": "70.20",
          "Parameters": "5,022,215"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2016.90"
    },
    {
      "citation_id": "2",
      "title": "Squeeze-and-Excitation Networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00745"
    },
    {
      "citation_id": "3",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "4",
      "title": "Transformer-Based Multimodal Emotional Perception for Dynamic Facial Expression Recognition in the Wild",
      "authors": [
        "X Zhang",
        "M Li",
        "S Lin",
        "H Xu",
        "G Xiao"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology",
      "doi": "10.1109/TCSVT.2023.3312858"
    },
    {
      "citation_id": "5",
      "title": "POSTER++: A simpler and stronger facial expression recognition network",
      "authors": [
        "J Mao",
        "R Xu",
        "X Yin",
        "Y Chang",
        "B Nie",
        "A Huang"
      ],
      "year": "2023",
      "venue": "POSTER++: A simpler and stronger facial expression recognition network",
      "arxiv": "arXiv:2301.12148"
    },
    {
      "citation_id": "6",
      "title": "QCS: Feature refining from quadruplet cross similarity for facial expression recognition",
      "authors": [
        "C Wang",
        "L Chen",
        "L Wang",
        "Z Li",
        "X Lv"
      ],
      "year": "2024",
      "venue": "QCS: Feature refining from quadruplet cross similarity for facial expression recognition",
      "arxiv": "arXiv:2311.01234"
    },
    {
      "citation_id": "7",
      "title": "Representation learning and identity adversarial training for facial behavior understanding",
      "authors": [
        "M Ning",
        "A Salah",
        "I Ertugrul"
      ],
      "year": "2024",
      "venue": "Representation learning and identity adversarial training for facial behavior understanding",
      "arxiv": "arXiv:2407.12345"
    },
    {
      "citation_id": "8",
      "title": "A novel facial emotion recognition model using segmentation VGG-19 architecture",
      "authors": [
        "S Vignesh",
        "M Savithadevi",
        "M Sridevi",
        "R Sridhar"
      ],
      "year": "2023",
      "venue": "International Journal of Information Technology",
      "doi": "10.1007/s41870-023-01184-z"
    },
    {
      "citation_id": "9",
      "title": "EmoNeXt: an Adapted ConvNeXt for Facial Emotion Recognition",
      "authors": [
        "Y Boudouri",
        "A Bohi"
      ],
      "year": "2023",
      "venue": "2023 IEEE 25th International Workshop on Multimedia Signal Processing (MMSP)",
      "doi": "10.1109/MMSP59012.2023.10337732"
    },
    {
      "citation_id": "10",
      "title": "Facial Expression Recognition Using Residual Masking Network",
      "authors": [
        "L Pham",
        "T Vu",
        "T Tran"
      ],
      "year": "2021",
      "venue": "2020 25th International Conference on Pattern Recognition (ICPR)",
      "doi": "10.1109/ICPR48806.2021.9411919"
    },
    {
      "citation_id": "11",
      "title": "ResEmoteNet: Bridging accuracy and loss reduction in facial emotion recognition",
      "authors": [
        "A Roy",
        "H Kathania",
        "A Sharma",
        "A Dey",
        "M Ansari"
      ],
      "year": "2024",
      "venue": "ResEmoteNet: Bridging accuracy and loss reduction in facial emotion recognition",
      "arxiv": "arXiv:2409.10545v2"
    },
    {
      "citation_id": "12",
      "title": "MobileNets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "A Howard",
        "M Zhu",
        "B Chen",
        "D Kalenichenko",
        "W Wang",
        "T Weyand",
        "M Andreetto",
        "H Adam"
      ],
      "year": "2017",
      "venue": "MobileNets: Efficient convolutional neural networks for mobile vision applications",
      "arxiv": "arXiv:1704.04861"
    },
    {
      "citation_id": "13",
      "title": "EfficientNet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "14",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "15",
      "title": "Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection",
      "authors": [
        "C Liu",
        "S He",
        "Q Zhou",
        "S Li",
        "W Meng"
      ],
      "venue": "Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Neural Information Processing: 20th International Conference"
    },
    {
      "citation_id": "17",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "18",
      "title": "Distribution matching for multi-task learning of classification tasks: A large-scale study on faces & beyond",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2024",
      "venue": "Distribution matching for multi-task learning of classification tasks: A large-scale study on faces & beyond",
      "arxiv": "arXiv:2401.01219v2"
    }
  ]
}