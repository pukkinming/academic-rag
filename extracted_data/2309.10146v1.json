{
  "paper_id": "2309.10146v1",
  "title": "Comparing An Android Head With Its Digital Twin Regarding The Dynamic Expression Of Emotions",
  "published": "2023-09-18T20:56:18Z",
  "authors": [
    "Amelie Kassner",
    "Christian Becker-Asano"
  ],
  "keywords": [
    "facial expression",
    "emotion",
    "empirical study",
    "android robot",
    "social robotics"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotions, which are an important component of social interaction, can be studied with the help of android robots and their appearance, which is as similar to humans as possible. The production and customization of android robots is expensive and time-consuming, so it may be practical to use a digital replica. In order to investigate whether there are any perceptual differences in terms of emotions based on the difference in appearance, a robot head was digitally replicated. In an experiment, the basic emotions evaluated in a preliminary study were compared in three conditions and then statistically analyzed. It was found that apart from fear, all emotions were recognized on the real robot head. The digital head with \"ideal\" emotions performed better than the real head apart from the anger representation, which offers optimization potential for the real head. Contrary to expectations, significant differences between the real and the replicated head with the same emotions could only be found in the representation of surprise.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction And Motivation",
      "text": "Robots are expected to support us as social partners in the future. For natural interaction, it is necessary that their appearance and behavior are adapted to their environment  [1] . Emotions serve as a nonverbal tool that can enhance such interactions  [2] . Android robots can be used for human-human and human-robot interaction research due to their very humanlike appearance. However, their expressiveness is limited by their hardware and their production is still quite expensive. Virtual robot heads, on the other hand, can be produced comparatively easy without high cost. A virtual robot head can be used to mimic the human face and better understand its functionality  [3] . Furthermore, virtual robot heads have more freedom of movement in their animation since they are not bound by physical constraints.\n\nIn order to investigate possible differences regarding emotion perception between a real and virtual robot head, a physical android robot head was digitally recreated in Unreal Engine 5 (UE5). This replica serves as a basis for further research and provides information about possible adaptations of the real robot head to better represent emotions on it. Six basic emotions  [9]  were modelled with the robot head and validated in a pre-experiment. Afterwards, the real and the virtual version of the robot head were compared against each other using a 3D visualization inside a head-mounted display for the virtual version.\n\nThe remainder of this paper is structured as follow. In the following section related work will be presented and discussed. In Section III the experimental hypotheses will be stated and the hardware setup will be introduced, before a pre-study is explained in Section IV. The main study is described in Section V with its results presented and analyzed in Section VI. A general discussion in Section VII concludes our presentation.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Emotions",
      "text": "There are many different approaches trying to define emotions  [4] ,  [5] , but a unified definition has not been found yet. Emotion theories and models such as basic emotion theory (BET)  [6] , main emotion systems  [7]  or prototypical approaches  [8]  try to look at emotions from different perspectives. Ekman's research has defined the basic emotions of anger, disgust, fear, happiness, sadness, and surprise, and assumes that they are universal and culturally independent  [9] . Even though the validity of this research has been doubted in some cases (cf.  [6] ,  [10] ), these basic emotions serve as a basis for research in many cases, including in the field of humanrobot interaction  [11] . Ekman's results could be investigated and replicated in further studies  [12] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Social Robots And Androids",
      "text": "Social robots are able to interact naturally with humans via verbal and nonverbal signals. Emotions are a part of this and can help represent a robot's internal state and allow viewing individuals to interpret and respond to it  [13] . Androids have an appearance as similar to humans as possible and are intended to advance research regarding human-human and human-robot interaction. Human movements and facial expressions are of great importance here for natural interaction  [14] . Geminoid HI-1 and Geminoid F have already been used to study crosscultural differences in terms of emotion perception, where fear was more difficult to detect and confusion varied depending on nationality  [2] . Replicating human faces is difficult due to their high complexity  [3] , yet this has been attempted several times. Robot heads are often able to represent emotions, which has been validated in experiments over images or the real robots  [15] ,  [16] . Since representation via two-dimensional displays can be problematic, a hybrid approach, for example, relies on a robot head with facial expressions controlled via computer animation  [3] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "C. Virtual Agents",
      "text": "Virtual agents with an embodied representation need emotions as they can be an important part of their credibility, higher satisfaction, better performance, and better perception of the agent  [17] ,  [18] . Virtual agents have already been used to explore whether age can have an impact on emotion recognition  [19] . Creating a virtual agent is complex and requires a multidisciplinary approach for correct processing of signals, appropriate animations, and underlying knowledge, for example on emotion representation  [20] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Differences Between Real And Virtual Representations",
      "text": "When interacting with real and virtual robots, presence and embodiment play an important role and must be considered when making comparisons  [3] ,  [21] . When represented via screens, virtual robots are limited in their interaction capabilities  [22] . Interactions with physical robots are also often perceived more positively  [13] ,  [23] -  [25] . Instead of using a representation on a screen, it is possible to use virtual reality (VR) and resort to representations via CAVE  [26]  or head-mounted displays (HMD). The depicted robot and the person viewing it are then co-present  [27] . Using the same experimental setup in real and virtual allows a direct comparison as long as differences of the media are taken into account  [28] . Contrary to previous results, however, no significant differences between the presentation in real and in VR could be found in  [29] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Experiment Foundations",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Hypotheses",
      "text": "The research is based on the following hypotheses: 1) There are general differences in emotion perception between the real robot head, the virtual version of the robot head with recreated emotions, and the virtual version of the robot head with emotions that are not artificially limited to recreate the physical constraints of the real robot head. 2) On the virtual robot head, the emotions without constraints will be evaluated better than the emotions modeled on the real robot head.\n\n3) The real robot head will be rated better than the digital robot head with the recreated emotions. 4) Similar to the second and third hypothesis, the virtual robot head with unrestricted emotion representation will be evaluated better than the real robot head. The last hypothesis relies on the assumption that the better emotion representation achieved by more freedom of animation has a greater influence than the different types of embodiment (real/virtual).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Hardware And Software Used",
      "text": "An android robot head of the Stuttgart Media University was used for both the main and pre-study. This head is equipped with 14 actuators powered by compressed air for controlling facial expressions and head movement.\n\nFor the digital replica, the robot head was scanned with the EinScan HX handheld 3D scanner, post-processed in Blender, and rigged and animated via the UE5's MetaHuman framework. The virtual scene was rendered using the HTC Vive connected to a laptop equipped with Intel i7-10870H CPU, 32 GB RAM and NVIDIA GeForce RTX 3080 GPU.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Pre-Study",
      "text": "In order to validate the emotions, a pre-study was conducted in the form of an online survey. The experimental setup of  [2]  was used as a reference. The emotions anger, sadness, surprise, fear, happiness, disgust and a neutral facial expression were implemented on the real robot head (condition 1). Those emotions were manually recreated on the virtual robot head using the MetaHuman facial rig (condition 2). Additionally, another set of emotional expressions were taken from the MetaHuman pose library (condition 3), thus, ignoring the hardware limitations of the real robot head.\n\nParticipants were randomly assigned to one of the three conditions. At the beginning, participants had to read through and confirm a privacy statement. Age, gender, highest level of education, affinity for technology, and previous experience with android robots were queried. Afterwards, it was explained that labels should be assigned to each of the images of emotional facial expressions. It was pointed out that the option \"None of these labels\" can be selected should none of the listed labels fit, and that it is possible to assign labels more than once. On the next page, images representing neutral, anger, sadness, surprise, fear, happiness, and disgust were displayed below each other in random order. All labels available for selection were introduced at the beginning of the page. Participants were asked to view all images first, before assigning labels.\n\nFrom March 4 to March 10, 2023, participants were invited to take part in the online survey through the University's mailing list and social networks. A total of 127 people participated, 114 of whom completed the survey and were included. 70 individuals identified themselves as female (M = 31.06 years; SD = 15.35), 41 as male (M = 29.95 years; SD = 12.81), and 3 as non-binary (M = 26 years; SD = 3.61). Of these, 37 subjects had been automatically assigned to the first condition (real robot head), 30 to the second condition (virtual head with non-constraint emotions), and 47 to the third condition (virtual head with replicated emotions). The results are presented in Tables I, II and III.\n\nIn the first condition, happiness (91.9%), sadness (89.2%), and neutral facial expression (89.2%) were well recognized. Surprise was frequently mistaken for fear (21.6%), and anger for disgust (27%). Fear (10.8%, below chance level) and disgust (18.9%) were both poorly recognized, with disgust most often classified as none of the emotions (51.4%) and fear as surprise (73%). In the second condition, very good recognition rates were achieved for sadness (96.7%), surprise (96.7%), fear (90%), and happiness (100%). Anger, on the other hand, was more often classified as disgust (46.7%) and vice versa for disgust (anger 43.3%). Regarding the third condition, sadness (91.5%), surprise, and happiness (87.2% each) performed best. It is interesting that neutral facial expression, although corresponding to the same facial expression as in condition two, performed worse here (68.1%). As in condition one, anger was mistaken for disgust (12.8%). Fear was also below chance (8.5%) and was classified significantly more often than surprise (57.4%). Disgust was more likely to be classified as \"None of these labels\" (38.3%).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "V. Main Study",
      "text": "Based on the results of the pre-study, disgust was excluded for the main study due to the poor recognition rate in all three conditions. Since fear was more often classified as surprise, the representation of fear was modified to be more distinguishable from surprise. The main experiment followed a within-subject design. The order of the conditions was determined via the Balanced Latin Square Design and the sequence of emotional expressions shown within each condition was randomized.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Displayed Emotions And Their Evaluation",
      "text": "In the main study, the same three conditions as in the prestudy were used, with the difference of showing live transitions of the emotions from the neutral expression into the emotion. The timings for the animations of the different expressions were taken from  [11]  with, e.g., surprise being presented much faster than happiness. The virtual environment was built to look as similar as possible to the actual environment.\n\nThe basic emotions of anger, sadness, surprise, fear, and happiness (cf.  [9] ) were shown both on the real and virtual robot head, with each emotional expression presented twice per condition, cp. Fig.  1 .\n\nTo keep the emotion ratings consistent across all three conditions, participants were asked over a speaker using the synthetically generated words \"anger,\" \"sadness,\" \"surprise,\" \"fear,\" and \"happiness.\" The participants were then to give a verbal rating on a scale from 0 (not at all) to 4 (completely) for each of the emotions just heard, which were always asked in this order for each facial expression. The procedure was based on  [16] . The same five-point scale was presented in both, the real and virtual conditions, as a piece of (real or virtual) paper placed in front of the participants, see S in Fig.  3 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Experimental Setup",
      "text": "The experiment took place from May 13 to May 18, 2023 at the Humanoid Lab of the Stuttgart Media University. The participants sat face-to-face with the physical robot head and were able to view the virtual robot head via the HTC Vive VRheadset. Using laptop L1 (cp. Fig.  2  and Fig.  3 ), the real head was controlled via a GUI and the virtual head via the Unreal scene from an operator behind the participant. On a second laptop L2 the corresponding questionnaire was first filled out by the participant himself and for the emotion rating by the experimenter.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Participants",
      "text": "Thirty individuals participated in the main study, 16 of whom identified as female (M = 24.19, SD = 4.85) and 14 as male (M = 25.29, SD = 4.61). Recruitment was done Fig.  1 : Emotions used in the main study, on the real robot head (top), recreated on the digital robot head (middle), and \"ideal\" emotions taken from Unreal's Pose Library (bottom); From left to right: neutral, anger, sadness, surprise, fear (as changed after pre-study), and happiness. Six persons had participated in the pre-study, but since it was not resolved which emotion was assessed, this was not seen as an exclusion criterion. Only one person (3.33%) had repeatedly worked with an android robot in the last six months, four others (13.33%) rarely. For 12 people (40%) VR was a new experience, 16 people (53.33%) had rarely done anything with it. Two persons (6.66%) had repeatedly dealt with it.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Experimental Procedure",
      "text": "First, participants were shown the robot head and the evaluation scale. The oral evaluation procedure was explained to them. They were told that the robot head starts from the neutral Fig.  3 : Experiment setup for the main experiment (top view); L1 main laptop for control, L2 laptop with questionnaire, LA loudspeaker for playing asked emotions, K robot head, S scale, V R VR goggles, B1 and B2 base stations. position, moves into an expression, and stays in that expression until all scores for it have been given. In the questionnaire, participants were informed about the use and processing of their data with a privacy statement. Subsequently, participation had to answer questions regarding their participation in the prestudy, age, gender, experience with android robots, and VR in the last six months. Using a habituation condition, participants were supposed to internalize the scale and practice responding verbally. They were presented with an printed emoji and asked to rate it according to the scale. Any ambiguities could be clarified afterwards, and questions were also possible at any time during the conditions.\n\nEach of the 30 participants was shown three conditions with 10 facial expressions each (two each of anger, sadness, surprise, fear, happiness). For each of the emotional expressions, they were asked to rate on a scale of 0-4 how strongly it corresponds to each of the five emotion labels.\n\nThe emotional expressions were triggered in random but pre-defined order by the experimenter as described above and the ratings were queried and entered into the questionnaire. After all three conditions had been completed, the participants were asked whether there were any problems, and if so, this was noted. If desired, the background of the work was explained. Otherwise, thanks were given for participation and participants could pick some sweets.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Vi. Results Of The Main Study",
      "text": "The answers were analyzed by grouping them according to the emotions shown and forming a mean and a difference of the same ratings in each case. According to the results of a Shapiro-Wilk test, the Friedman test had to be used to test for significant differences between ratings. A second Friedman test was used for comparing the ratings between conditions. In the case of significant differences, a pairwise comparison with the Durbin-Conover post hoc test was also performed. The condition with the real robot head is abbreviated as \"E\", the condition with the virtual robot head with the replicated emotions is abbreviated as \"VN\", and the condition with the emotions adopted from Unreal is abbreviated as \"V\". Statistical analyses were performed using jamovi software and a significance level of p = 0.05 was used.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Intended Expression Of Anger",
      "text": "Anger was most likely to be identified as anger in all three conditions. The value was highest for the \"ideal\" virtual representation V (M = 3.00; SD = 1.05). The replicated virtual representation VN achieved a mean value of 2.68 (SD = 0.887) for anger, closely followed by the representation on the real robot head E (M = 2.68; SD = 0.965). The representation on the real and replicated robot head was partially classified as surprise (real M = 0.433; SD = 0.691, replicated M = 0.333; SD = 0.531), in condition V more as sadness (M = 0.750; SD = 0.716).\n\nIn conditions V and VE, there were occasional differences of 3 or 4 points between the first and second ratings. All other differences, also for the other emotions, had a maximum delta of 2. The second Friedman test reveals that there are no significant differences between the three conditions for the rating of anger (χ 2 = 4.53; p = 0.104).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Intended Expression Of Sadness",
      "text": "Sadness has a very good detection rate, as the mean values for sadness are highest for E (M = 3.52; SD = 0.533), VN (M = 3.38; SD = 0.739), and V (M = 3.92; SD = 0.265), and the median is 3.5 for E and VN, and as high as 4 for V. Fear was most likely to be selected in second place, where the mean values are 0.85 (SD = 0.811) for E, 0.95 (SD = 0.913) for VN, and 1.15 (SD = 1.07) for V.\n\nOnly in the evaluation of fear there were isolated differences of 3 or 4 points, otherwise they deviated only rarely and always by a maximum of 2 points in the other evaluations. Here, the second Friedman test yields significant differences for the three conditions (χ 2 = 15.0; p < 0.001), which is why a pairwise comparison could be performed. While the presentation of sadness and its evaluation show significant differences for V compared to E and VN (p < 0.001), there is no significant difference between E and VN (p = 0.698).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Intended Expression Of Surprise",
      "text": "Surprise shows the best recognition rate of all emotions shown. The mean value is 3.93 (SD = 0.173) in condition V, 3.77 (SD = 0.487) in E, and 3.53 (SD = 0.642) in VN. The medians for surprise are all at the maximum value of 4. Fear was rated second highest in condition E (M = 1.20; SD = 1.02) and in condition VN (M = 0.983; SD = 1.13), and happiness in condition V (M = 1.27; SD = 1.17). The delta of the ratings differs only slightly in most cases. In the case of surprise, a difference of 3 occurred once each in conditions E and VN, and for fear a difference of 3 for condition V. Only in the case of happiness did a difference of 3 occur once for condition V and a difference of 4 for conditions VN and V.\n\nComparing the display of surprise across the three conditions with the second Friedman test, significant differences emerge (χ 2 = 11.8; p = 0.003). However, according to the Durbin-Conover post hoc test, only the differences between the simulated virtual robot head and the real (p = 0.017) and \"ideal\" robot head (p < 0.001) are significant. No significant differences were found between the rating of surprise on the real robot head and the rating of surprise on the \"ideal\" virtual robot head (p = 0.223).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Intended Expression Of Fear",
      "text": "Fear was recognized worst compared to the other emotion representations. The mean values are only 1.07 (SD = 0.691) for condition E and 1.18 (SD = 0.933) for condition VN. Only the mean of 2.82 (SD = 0.771) for Condition V is acceptable. For Condition E, ratings were most likely for surprise (M = 2.50; SD = 0.881) and sadness (M = 1.10; SD = 0.759). Likewise for condition VN (surprise M = 2.13; SD = 1.06, sadness M = 1.27; SD = 0.971). For condition V, the rating for surprise (M = 2.45; SD = 1.12) comes second. The ambiguous ratings of the representation of fear also become apparent in the deltas between the two ratings given. For anger, there is a difference of 3 in condition V. Differences of more than three points occurred for sadness, surprise as well as fear. Conditions E and V were affected for sadness and surprise, and only condition V was affected for fear. The second Friedman test for anxiety, the comparison across conditions, reveals significant differences (χ 2 = 41.9; p < 0.001). The Durbin-Conover post hoc test continues to show significant differences for ratings of anxiety between conditions E -V as well as conditions V -VN (p < 0.001). Only the ratings between conditions E -VN show no significant differences with p = 0.4. However, there were no significant differences for sadness (χ 2 = 0.804; p = 0.669) and surprise (χ 2 = 0.804; p = 0.669).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Intended Expression Of Happiness",
      "text": "Happiness was not consistently rated with maximum values in all three conditions, but it was most likely to be rated as happiness. Condition V performed best here (M = 3.47; SD = 0.692), followed by Condition VN (M = 3.02; SD = 0.7484) and Condition E (M = 2.95; SD = 0.834). Other emotions tended to be less reported here, with a bit surprise for Condition E and VN (for Condition E with M = 0.500; SD = 0.639, for Condition VN with M = 0.483; SD = 0.663) and very little anger for Condition V (M = 0.200; SD = 0.407). There are no major differences in the deltas for happiness. For anger and surprise, there is a difference of 3 points once each in condition E, otherwise the ratings differ by a maximum of 2 points.\n\nTo investigate this further, a Friedman test was used to check whether there were differences in terms of ratings across conditions. Based on the previous results, only the rating for happiness was compared here. The differences were indeed significant (χ 2 = 13.4; p < 0.001). The Durbin-Conover pairwise comparison specifically indicates that there are significant differences between conditions E and V (p < 0.001) and V and VN (p = 0.002). Only between condition E, the real robot head, and condition VN, the replicated virtual robot head, there are no significant differences with p = 0.618.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vii. Discussion And Conclusion",
      "text": "The virtual recreation of an android robot head for comparison of emotion perception was presented. A virtual model was created using a 3D scanner and animated using the MetaHuman framework in Unreal Engine 5. The aim was to investigate whether the emotions displayed on the heads are generally perceived differently and whether more expressive emotional faces on the virtual robot head lead to better recognition rates.\n\nThree conditions were designed, first, the emotions represented on the real head (E), second, the same emotions replicated on the virtual head (VN), and, third, the virtual head with \"ideal\" emotions that are not bound by the limitations of the physical hardware (V). The basic emotions anger, sadness, surprise, fear and happiness were chosen as emotional facial expressions and were validated in a pre-study.\n\nExcept for anger, significant differences across conditions were found for all emotions, supporting the first hypothesis.\n\nThe assumption that the \"ideal\" virtual head performs better due to more display options (hypothesis 2) was confirmed, since there were always significant differences between the conditions V and VN and the mean value was higher for V in each case. Therefore, the emotional expressions of the Unreal pose library can serve as an orientation for adjustments of the real robot head's emotional expressions. However, the assumption of the real robot head performing better than the replicated robot head cannot be confirmed (hypothesis 3). Except for the rating of surprise, for which the real robot head was rated better, no significant differences were found. This suggests that the type of presentation (real/virtual) does not always influence the perception of dynamic emotional expressions. The fourth hypothesis, namely that the \"ideal\" virtual robot head is rated better than the real robot head, can be confirmed for the emotions sadness, fear, and happiness. For surprise, no such significant differences in the ratings were found.\n\nThe finding that surprise, sadness, and happiness can be recognized well while fear seems more difficult to be expressible by facial expression alone is consistent with previous findings (cf.  [15] ,  [24] ). For fear, it should be noted that the confusions already occurred in the pre-study and the adaptation of the presentation did not lead to better recognition. The physical head might be limited by its hardware and movement capabilities, so any revision of the head should most likely focus on representation capabilities for fear. Additional modalities such as speech (high pitch) or sounds (gasping, shouting) could also support the portrayal of fear.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "All participants provided written consent to provide their data for the sole purpose of the empirical study. They were informed that the data will be kept securely according to the university's privacy rules and guidelines. As described above, we are aware that the results presented here are of limited generalizability, because of the participants' limited diversity. Also, a robot portraying emotional facial expressions might impact a future society in several, unforeseen ways and needs to be investigated as well.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: To keep the emotion ratings consistent across all three",
      "page": 3
    },
    {
      "caption": "Figure 3: B. Experimental setup",
      "page": 3
    },
    {
      "caption": "Figure 2: and Fig. 3), the real head",
      "page": 3
    },
    {
      "caption": "Figure 1: Emotions used in the main study, on the real robot head (top), recreated on the digital robot head (middle), and “ideal”",
      "page": 4
    },
    {
      "caption": "Figure 2: Experiment setup for the main experiment (side view);",
      "page": 4
    },
    {
      "caption": "Figure 3: Experiment setup for the main experiment (top view);",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Condition 1 (N=37)": ""
        },
        {
          "Condition 1 (N=37)": "Label"
        },
        {
          "Condition 1 (N=37)": "neutral"
        },
        {
          "Condition 1 (N=37)": "anger"
        },
        {
          "Condition 1 (N=37)": "sadness"
        },
        {
          "Condition 1 (N=37)": "surprise"
        },
        {
          "Condition 1 (N=37)": "fear"
        },
        {
          "Condition 1 (N=37)": "happiness"
        },
        {
          "Condition 1 (N=37)": "disgust"
        },
        {
          "Condition 1 (N=37)": "none"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Condition 3 (N=47)": ""
        },
        {
          "Condition 3 (N=47)": "Label"
        },
        {
          "Condition 3 (N=47)": "neutral"
        },
        {
          "Condition 3 (N=47)": "anger"
        },
        {
          "Condition 3 (N=47)": "sadness"
        },
        {
          "Condition 3 (N=47)": "surprise"
        },
        {
          "Condition 3 (N=47)": "fear"
        },
        {
          "Condition 3 (N=47)": "happiness"
        },
        {
          "Condition 3 (N=47)": "disgust"
        },
        {
          "Condition 3 (N=47)": "none"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Condition 2 (N=30)": ""
        },
        {
          "Condition 2 (N=30)": "Label"
        },
        {
          "Condition 2 (N=30)": "neutral"
        },
        {
          "Condition 2 (N=30)": "anger"
        },
        {
          "Condition 2 (N=30)": "sadness"
        },
        {
          "Condition 2 (N=30)": "surprise"
        },
        {
          "Condition 2 (N=30)": "fear"
        },
        {
          "Condition 2 (N=30)": "happiness"
        },
        {
          "Condition 2 (N=30)": "disgust"
        },
        {
          "Condition 2 (N=30)": "none"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "analyzed emotion": "anger\nsadness\nsurprise\nfear\nhappiness"
        },
        {
          "analyzed emotion": "p < 0.007\np < 0.002\np < 0.0017\np < 0.05∗\np < 0.02"
        },
        {
          "analyzed emotion": "χ2 = 87.4; p < 0.001\nχ2 = 98.4; p < 0.001\nχ2 = 84.1; p < 0.001\nχ2 = 91.8; p < 0.001\nχ2 = 93.0; p < 0.001"
        },
        {
          "analyzed emotion": "χ2 = 79.9; p < 0.001\nχ2 = 99.3; p < 0.001\nχ2 = 75.7; p < 0.001\nχ2 = 67.2; p < 0.001\nχ2 = 94.0; p < 0.001"
        },
        {
          "analyzed emotion": "χ2 = 77.4; p < 0.001\nχ2 = 99.1; p < 0.001\nχ2 = 90.5; p < 0.001\nχ2 = 84.6; p < 0.001\nχ2 = 93.6; p < 0.001"
        },
        {
          "analyzed emotion": "p < 0.001\np < 0.001\np < 0.001\np < 0.002∗∗\np < 0.001"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Empathy in Virtual Agents and Robots",
      "authors": [
        "A Paiva",
        "I Leite",
        "H Boukricha",
        "I Wachsmuth"
      ],
      "year": "2017",
      "venue": "ACM Trans. Interact. Intell. Syst",
      "doi": "10.1145/2912150"
    },
    {
      "citation_id": "2",
      "title": "Evaluating facial displays of emotion for the android robot Geminoid F",
      "authors": [
        "C Becker-Asano",
        "H Ishiguro"
      ],
      "year": "2011",
      "venue": "2011 IEEE Workshop on Affective Computational Intelligence (WACI 2011"
    },
    {
      "citation_id": "3",
      "title": "Furhat: A Back-Projected Human-Like Robot Head for Multiparty Human-Machine Interaction",
      "authors": [
        "S Al Moubayed",
        "J Beskow",
        "G Skantze",
        "B Granström"
      ],
      "year": "2011",
      "venue": "Cognitive behavioural systems: COST 2102 International Training School"
    },
    {
      "citation_id": "4",
      "title": "Toward a Working Definition of Emotion",
      "authors": [
        "K Mulligan",
        "K Scherer"
      ],
      "year": "2012",
      "venue": "Emotion Review",
      "doi": "10.1177/1754073912445818"
    },
    {
      "citation_id": "5",
      "title": "What is emotion?",
      "authors": [
        "M Cabanac"
      ],
      "year": "2002",
      "venue": "Behavioural Processes",
      "doi": "10.1016/S0376-6357(02)00078-5"
    },
    {
      "citation_id": "6",
      "title": "Emotional Expression: Advances in Basic Emotion Theory",
      "authors": [
        "D Keltner",
        "D Sauter",
        "J Tracy",
        "A Cowen"
      ],
      "year": "2019",
      "venue": "Journal of nonverbal behavior",
      "doi": "10.1007/s10919-019-00293-3"
    },
    {
      "citation_id": "7",
      "title": "Affective neuroscience: The foundations of human and animal emotions",
      "authors": [
        "J Panksepp"
      ],
      "year": "1998",
      "venue": "Affective neuroscience: The foundations of human and animal emotions"
    },
    {
      "citation_id": "8",
      "title": "Concept of emotion viewed from a prototype perspective",
      "authors": [
        "B Fehr",
        "J Russell"
      ],
      "year": "1984",
      "venue": "Journal of Experimental Psychology: General",
      "doi": "10.1037/0096-3445.113.3.464"
    },
    {
      "citation_id": "9",
      "title": "Universals and cultural differences in facial expressions of emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1971",
      "venue": "Nebraska Symposium on Motivation"
    },
    {
      "citation_id": "10",
      "title": "Is there universal recognition of emotion from facial expression? A review of the cross-cultural studies",
      "authors": [
        "J Russell"
      ],
      "year": "1994",
      "venue": "Psychological Bulletin",
      "doi": "10.1037/0033-2909.115.1.102"
    },
    {
      "citation_id": "11",
      "title": "An Android for Emotional Interaction: Spatiotemporal Validation of Its Facial Expressions",
      "authors": [
        "W Sato",
        "S Namba",
        "D Yang",
        "S Nishida",
        "C Ishi",
        "T Minato"
      ],
      "venue": "Frontiers in psychology",
      "doi": "10.3389/fpsyg.2021.800657"
    },
    {
      "citation_id": "12",
      "title": "Facial expressions of emotion",
      "authors": [
        "D Matsumoto",
        "D Keltner",
        "M Shiota",
        "M O'sullivan",
        "M Frank"
      ],
      "year": "2008",
      "venue": "Facial expressions of emotion"
    },
    {
      "citation_id": "13",
      "title": "Social Robotics",
      "authors": [
        "C Breazeal",
        "K Dautenhahn",
        "T Kanda"
      ],
      "year": "2016",
      "venue": "Springer Handbooks, Springer handbook of robotics"
    },
    {
      "citation_id": "14",
      "title": "Interactive humanoids and androids as ideal interfaces for humans",
      "authors": [
        "H Ishiguro"
      ],
      "year": "2006",
      "venue": "Proceedings of the 11th international conference on Intelligent user interfaces"
    },
    {
      "citation_id": "15",
      "title": "Appropriate emotions for facial expressions of 33-DOFs android head EveR-4 H33",
      "authors": [
        "H Ahn",
        "D.-W Lee",
        "D Choi",
        "D.-Y Lee",
        "M Hur",
        "H Lee"
      ],
      "year": "2012",
      "venue": "IEEE Ro-Man"
    },
    {
      "citation_id": "16",
      "title": "Control of facial expressions of the humanoid robot head ROMAN",
      "authors": [
        "K Berns",
        "J Hirth"
      ],
      "year": "2006",
      "venue": "2006 IEEE/RSJ International Conference on Intelligent Robots and Systems: Beijing, China"
    },
    {
      "citation_id": "17",
      "title": "An Empathic Virtual Dialog Agent to Improve Human-Machine Interaction",
      "authors": [
        "M Ochs",
        "C Pelachaud",
        "D Sadek"
      ],
      "year": "2008",
      "venue": "Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems"
    },
    {
      "citation_id": "18",
      "title": "Modelling character emotion in an interactive virtual environment",
      "authors": [
        "E Mehdi",
        "P Nico",
        "J Dugdale",
        "B Pavard"
      ],
      "year": "2004",
      "venue": "AISB Convention, Symposium on Language, Speech and Gesture for Expressive Characters"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition of virtual agents facial expressions: the effects of age and emotion intensity",
      "authors": [
        "J Beer",
        "A Fisk",
        "W Rogers"
      ],
      "year": "2009",
      "venue": "Annual Meeting. Human Factors and Ergonomics Society. Annual meeting",
      "doi": "10.1177/154193120905300205"
    },
    {
      "citation_id": "20",
      "title": "Creating interactive virtual humans: some assembly required",
      "authors": [
        "J Gratch",
        "J Rickel",
        "E Andre",
        "J Cassell",
        "E Petajan",
        "N Badler"
      ],
      "year": "2002",
      "venue": "IEEE Intell. Syst",
      "doi": "10.1109/MIS.2002.1024753"
    },
    {
      "citation_id": "21",
      "title": "The benefit of being physically present: A survey of experimental works comparing copresent robots, telepresent robots and virtual agents",
      "authors": [
        "J Li"
      ],
      "year": "2015",
      "venue": "International Journal of Human-Computer Studies",
      "doi": "10.1016/j.ijhcs.2015.01.001"
    },
    {
      "citation_id": "22",
      "title": "Where Robots and Virtual Agents Meet",
      "authors": [
        "T Holz",
        "M Dragone",
        "G O'hare"
      ],
      "year": "2009",
      "venue": "Int J of Soc Robotics",
      "doi": "10.1007/s12369-008-0002-2"
    },
    {
      "citation_id": "23",
      "title": "Android Robots vs Virtual Agents: which system differently aged users prefer?",
      "authors": [
        "C Greco",
        "T Amorese",
        "M Cuciniello",
        "G Cordasco",
        "A Esposito"
      ],
      "year": "2022",
      "venue": "31st IEEE International Conference on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "24",
      "title": "The Perception of Emotion in Artificial Agents",
      "authors": [
        "R Hortensius",
        "F Hekele",
        "E Cross"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Cogn. Dev. Syst",
      "doi": "10.1109/TCDS.2018.2826921"
    },
    {
      "citation_id": "25",
      "title": "The Benefits of Interactions with Physically Present Robots over Video-Displayed Agents",
      "authors": [
        "W Bainbridge",
        "J Hart",
        "E Kim",
        "B Scassellati"
      ],
      "year": "2011",
      "venue": "Int J of Soc Robotics",
      "doi": "10.1007/s12369-010-0082-7"
    },
    {
      "citation_id": "26",
      "title": "Scientists in wonderland: A report on visualization applications in the CAVE virtual reality environment",
      "authors": [
        "C Cruz-Neira"
      ],
      "year": "1993",
      "venue": "Proceedings of 1993 IEEE Research Properties in Virtual Reality Symposium"
    },
    {
      "citation_id": "27",
      "title": "Evaluation of human sense of security for coexisting robots using virtual reality. 1st report: evaluation of pick and place motion of humanoid robots",
      "authors": [
        "S Nonaka",
        "K Inoue",
        "T Arai",
        "Y Mae"
      ],
      "year": "2004",
      "venue": "Proceedings / 2004 IEEE International Conference on Robotics and Automation"
    },
    {
      "citation_id": "28",
      "title": "Comparative evaluation of virtual and real humanoid with robotoriented psychology scale",
      "authors": [
        "H Kamide",
        "M Yasumoto",
        "Y Mae",
        "T Takubo",
        "K Ohara",
        "T Arai"
      ],
      "venue": "2011 IEEE International Conference on Robotics and Automation"
    },
    {
      "citation_id": "29",
      "title": "",
      "authors": [
        "China Shanghai"
      ],
      "year": "2011",
      "venue": ""
    },
    {
      "citation_id": "30",
      "title": "User Responses to a Humanoid Robot Observed in Real Life, Virtual Reality, 3D and 2D",
      "authors": [
        "M Mara"
      ],
      "venue": "Frontiers in psychology",
      "doi": "10.3389/fpsyg.2021.633178"
    }
  ]
}