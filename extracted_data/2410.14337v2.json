{
  "paper_id": "2410.14337v2",
  "title": "Perception Of Emotions In Human And Robot Faces: Is The Eye Region Enough?",
  "published": "2024-10-18T09:50:05Z",
  "authors": [
    "Chinmaya Mishra",
    "Gabriel Skantze",
    "Peter Hagoort",
    "Rinus Verdonschot"
  ],
  "keywords": [
    "Human-Robot Interaction",
    "Emotional Robotics",
    "Posture and Facial Expressions",
    "Design and Human Factors",
    "Emotion Recognition",
    "Affective Robots"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The increased interest in developing next-gen social robots has raised questions about the factors affecting the perception of robot emotions. This study investigates the impact of robot appearances (humanlike, mechanical) and face regions (full-face, eye-region) on human perception of robot emotions. A between-subjects user study (N = 305) was conducted where participants were asked to identify the emotions being displayed in videos of robot faces, as well as a human baseline. Our findings reveal three important insights for effective social robot face design in Human-Robot Interaction (HRI): Firstly, robots equipped with a back-projected, fully animated face -regardless of whether they are more human-like or more mechanical-looking -demonstrate a capacity for emotional expression comparable to that of humans. Secondly, the recognition accuracy of emotional expressions in both humans and robots declines when only the eye region is visible. Lastly, within the constraint of only the eye region being visible, robots with more human-like features significantly enhance emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "There has been a surge in the development of next-generation social robots. Numerous commercial entities have proposed their versions of general-purpose robots, such as Optimus 6  , GR-1  7  , and Ameca 8 . While many new robots maintain a humanoid body design akin to NAO 9 and Pepper 10 , the robot faces exhibit significant diversity, ranging from a highly human-like face in Ameca to a blank face design in Optimus. This calls for more research investigating how the design of the face affects the perception of social robots, and consequently, the interactions humans will have with them.\n\nSocial robots, by definition, are designed to conduct human-like interactions  [21] . A key component of human communication is facial expressions which are used to convey meaning  [17] , build relationships  [24] , and help in decision making  [31] . Prior studies suggest that our brains perceive robot facial expressions similarly to human expressions  [7, 13] . Thus, social robots must not only recognize human emotions but also be able to convey them. Additionally, a robot can use its facial expressions to signal its intentions and internal state to a human interlocutor, which would make it easier for them to understand the robot and have a more seamless interaction with it. Modeling appropriate robot emotions is an active field of research. It has been found that robots expressing emotions are perceived as more intelligent  [18]  and trustworthy  [12] . However, it is equally important to investigate the factors influencing the perception of robot emotions. Identifying these factors would help design social robots that are easier to understand and interact with.\n\nResearchers have investigated how robot facial expressions are perceived by humans based on robot form and appearance. An early study on emotion recognition with the Feelix robot found that adults recognize emotions in still images of the robot similarly to human faces  [6] .  [5]  obtained similar results, indicating that individuals could interpret the robot's facial expressions from both images and videos. Literature indicates two branches of research into robots' emotion recognition by individuals depending on the robot form-factor; the first involves robots with a human-like face  [3, 14, 25] , and the second involves non-humanoid robot faces  [4, 11] . This leads to the first research question:\n\nRQ1: Does having a human-like face improve the recognition of a robot's emotions?\n\nThe answer to this question is not clear from the literature.  [4]  investigated this query in virtual agents, comparing recognition rates for human faces, synthetic human faces, and a non-human-like virtual agent (iCat). Their results indicated a higher recognition for the human face, followed by the synthetic human face, and lastly, the virtual agent.  [8]  also reported that emotions in a female humanoid virtual agent (Mary) were better recognized than in Nao and Zeno.  [25]  and  [3]  assessed humanoid android faces against human faces.  [25]  found robot facial expressions were on par with human expressions, while  [3]  noted human emotions surpassed those of the Geminoid F robot. While these trends suggest that greater human-likeness enhances emotion recognition, this remains uncertain for human-like robot faces. Moreover, studies involving robots do not compare recognition between human-like and mechanical-looking robot faces, hindering clarity on human-likeness impact. Thus, we propose our first hypothesis:\n\nH1: Human-like robot faces yield better emotion recognition compared to mechanical-looking robot faces Another aspect to consider is the role of specific face regions in emotion recognition. This stems from the broad variation in robot face designs, resulting in diverse implementations of facial regions. For instance, robots like Nao and Pepper feature static faces devoid of human-like movements, while others, such as Fuahat  [27]  and Ameca, possess full-face designs with human-like movements across all facial regions. This leads to our second research question:\n\nRQ2: Is it necessary to model the entire robot face with intricate human-like movements, or could we focus solely on certain regions, like the eyes?\n\nThis question not only sheds light on the significance of distinct facial regions in emotion recognition but also offers a chance to simplify robot emotion generation by reducing complexities. Previous studies in psychology show the significance of seeing full-face over specific facial regions in emotion recognition  [1, 33] , however, they also point to the fact that information for emotion recognition is not distributed evenly across the entire face. For example, studies suggest that the eye region alone provides sufficient information for emotion recognition  [1, 34] .  [1]  compared the emotion recognition from pictures of the eyeregion, mouth region and the full face. Their results indicated that the eye-region was as informative as the full face for complex emotions. Real-world examples include animated characters like those in the movie WALL-E (e.g., WALL-E, EVE, MO), which use minimalistic eye expressions to convey emotions and meanings  11  .\n\nInsights from human emotion recognition studies form the basis to investigate modeling specific face regions (like eye-region) instead of the full face for social robot design. However, this aspect remains less explored in the literature, possibly due to limited platforms with capabilities for human-like facial and eye movements. For instance, social robots like Pepper and Nao feature static eyes-only designs, precluding comparisons of emotion recognition between eye-only and full-face expressions. Some studies have tried to evaluate emotion recognition from robots' eye expressions  [2, 23]  and find the best ways to model them  [2, 9, 20, 29] . In a study  [14]  on \"animated faces\" for the MASHI robot, researchers compared emotion recognition rates between full-face and eye-region expressions, finding that while the full-face yielded better recognition, eye-region expressions remained acceptable. However, these studies have been limited to either virtual characters or robots with limited expressive capabilities, such as Nao or Pepper. This leads us to our second hypothesis: H2: Full face expressions will lead to better emotion recognition compared to eye-region only To explore the impact of robot appearance and facial regions on emotion recognition, we conducted a between-subjects user study, comprising two online experiments. One experiment centered on full-face emotion recognition, while the other focused solely on the eye region. In both studies, participants were tasked with identifying emotions conveyed in video recordings featuring a human, a human-like robot, and a mechanical-looking robot.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Materials And Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Robot Platform",
      "text": "We used the Furhat robot  [27]  for this study, a humanoid robot head featuring a 3D animated face projected onto a translucent mask via back projection. This setup enables the robot to adopt diverse appearances, spanning from realistic human-like to mechanical characters. Furthermore, Furhat can perform nuanced facial movements, resulting in human-like expressions. Furhat comes with a pack of pre-installed characters with a diverse range of gender, age, and human-likeness. We considered three main criteria when selecting the characters on the robot for the experiment. Firstly, we wanted to choose one robot character with a human-like appearance and another with a more mechanical look. Secondly, we wanted similar facial expressiveness in both characters to eliminate expressiveness as an influencing factor. Finally, we needed to ensure that the robot's face was adequately visible in the video recordings shown to the participants (see Section 2.3). Since the robot's face is back-projected onto a 3D mask, many factors such as the contrast and brightness of the robot's face (character), the lighting in the room, and the camera being used to record the video become important.\n\nWe chose two pre-installed characters: Hayden with a realistic human-like appearance, and Titan with a mechanical look (see Fig.  1 ). Titan gets its mechanical look from the square pupils, lack of eyebrows, white face color, and lines on the face that give the impression of its face comprising of different modular parts. The choice of mechanical look was partly due to the robotic character that was available on Furhat. Another aspect was the lack of eyebrows which is quite common in the robotic platforms being widely used in research so far, such as NAO, Pepper, Cozmo. A recent example of a highly expressive and humanlike robot without any clear eyebrows is Ameca. This made the choice relevant to currently used robotic systems. Apart from these differences, Titan is able to express emotions similarly to the human-like face Hayden, as both of them share the same face model. This is in contrast to the mechanical faces that have been used in prior studies which had static eyes and mouths like Nao and Pepper. Thus, it is possible to directly compare recognition of the emotions expressed by the human-like and mechanical-looking face using the Furhat robot.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Robot Emotions",
      "text": "Facial Action Coding System (FACS) is a comprehensive and widely used system for describing and categorizing facial expressions based on the movement of individual facial muscles  [15] . The muscle movements, called Action Units (AUs), are assigned numerical codes to represent different facial expressions and emotions. FACS has been widely used in modelling robots' expressions in HRI  [2, 4, 25, 31, 32] .\n\nFor this study, we modeled the six basic emotions  [16]  on Furhat. Since Furhat employs Apple's ARKit parameters for its face model, we mapped the FACS AUs to their corresponding ARKit parameters to generate facial expressions. Each ARKit parameter can be assigned a value between 0 (no movement of the AU) and 1 (maximum movement of the AU). For this experiment, we assigned the maximum value (i.e., 1) to the ARKit parameters to exaggerate the robot's facial expressions to make them easily recognizable in the videos. This was in line with the findings from  [26] , where the authors concluded that a robot's facial expressions should be exaggerated to communicate emotional content instead of just mimicking human facial expressions. Table  1  shows the AUs used to generate the basic emotions (adopted from  [10] ).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "To evaluate our hypotheses (refer to Section 1), we conducted a between-subjects user study with 3 variables (2 face regions × 3 appearances × 7 emotions). The two face regions studied were full-face expressions and eye-region-only expressions. Each group was shown either of the face regions: the full-face or eye-regiononly expressions. Three appearance conditions were defined: one with expressions by a human confederate (referred to as H) serving as the control, and the other two featuring emotions expressed by the robot characters Hayden (Ha) and Titan (Ti). These robot characters represented varying degrees of human likeness, allowing us to examine their impact on emotion recognition. Instead of images, short videos were used as stimuli for the user study. This was because still images capture only a snapshot of the emotion being expressed  [4]  and contain very little information about expressive posturing  [5] . We recorded videos of 6 emotions, Happy, Sad, Anger, Surprise, Disgust, and Fear, for each of the face types (H, Ha, and Ti), with a baseline Neutral expression (42 videos).\n\nFor H1, which pertained to the influence of the human-likeness of the robot's face on emotion recognition, we compared the recognition of emotions expressed by a human confederate, the human-looking robot character Hayden, and the mechanical-looking robot character Titan. The expressions of the confederate and robots were recorded in high resolution using the Canon HF-G30 video camera at the university lab. The confederate was shown examples of images and videos of facial expressions using FACS before the recording. A total of 21 videos were recorded for all the emotions and appearance conditions (7 emotions × 3 appearance types).\n\nH2 aimed to compare the recognition rates between full-face and eye regiononly expressions. The video recordings of the full-face expressions for all three appearance types; human, Hayden, and Titan were cropped to the eye region only. The cropped region and the proportion of the visible eye-region were kept consistent for all the videos. A total of 21 eye-region videos were extracted from the original full-face recordings.\n\nTwo online experiments were designed using the survey software Qualtrics. In the first experiment (full face condition), participants were shown short videos of the robots and the confederate on the screen and asked to select the matching emotion from the options provided on the screen. The robots and the confederate did not speak (no lip sync) and the videos had no audio. The videos first showed the robots or the confederate showing a neutral facial expression, followed by expressing an emotion and then returning to a neutral facial expression. The experiment adopted a forced-choice paradigm, requiring participants to choose one of the 7 emotions displayed as radio buttons below the video. The exact question asked was: \"What emotion is being expressed in the video below?\". Video presentation order was randomized, with a constraint to ensure that consecutive videos of the same appearance type did not occur more than twice in a row. The second experiment (eyes-only condition) followed a similar design but used the cropped eye-region videos as the stimuli. All the video durations were kept between 6-8 seconds for both experiments. Each of the experiments took roughly 7-8 minutes to finish.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Participants And Procedure",
      "text": "We recruited a total of 305 participants via the online survey platform Prolific (https://www.prolific.com/). The participants provided their informed consent before participating in the experiments. No personally identifiable data such as IP addresses or names were collected from the participants and the data was anonymized as soon as the participants submitted their responses. In both experiments, each participant was asked to watch 21 video clips and choose the emotion displayed in them.\n\nThe first experiment involved 153 (77 males, 74 females, 2 nonbinary, and 1 undisclosed), aged 18 to 59 (M = 30.05, SD = ±8.23) where they viewed videos showing the full face of the robots and the confederate. The second experiment collected data from 152 participants (76 males, 74 females, 2 non-binary) aged 19 to 54 (M = 27.70, SD = ±6.66) where they viewed videos showing only the eye region of the robots and the confederate. There was no overlap between participants in both experiments. We implemented two manipulation check questions; failing either resulted in automatic discarding of the survey response. Participants received 1 GBP upon successful experiment completion. The study has received the approval by the ethics committee of the university.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "A response was counted as correct if it matched the intended emotion expressed in the video. RStudio (version 2024.04.2) and R software version 4.4.1 was used for the statistical analysis. A GLMM (Generalized Linear Mixed Model) with Gaussian distribution (lme4 package) was fit with the responses as the dependent variable. The face regions, appearances, and emotions were set as the fixed effect variables, and the participants as the random effect grouping factor. The model indicated a significant main effect of the face regions (χ 2 = 69.139, df = 1, p < 0.001), appearances (χ 2 = 55.931, df = 2, p < 0.001), and emotions (χ 2 = 1953.848, df = 6, p < 0.001) on the responses. We also observed significant interaction effects between face regions and appearance (χ 2 = 20.978, df = 2, p < 0.001), face regions and emotions (χ 2 = 300.544, df = 6, p < 0.001), and, appearances and emotions (χ 2 = 791.019, df = 12, p < 0.001) on the responses.\n\nAge is known to influence the recognition of both human emotions  [19]  and emotions expressed by virtual agents  [28] . We split the age of the participants into 3 groups: Young Adulthood (18 -25 years), Adulthood  (26 -44 years)  and Middle Adulthood (45-59 years). A GLMM was fit with responses as the dependent variable, age group as the fixed effect variable and participants as the random effect grouping factor. The model did not show any significant effect of age groups on the responses by the participants (i.e., the recognition rates of the emotions).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Effect Of Appearances",
      "text": "Estimated marginal effects were computed using \"emmeans\" with the Tukey method to test pairwise comparisons. It was found that participants recognized the emotions significantly better in the human face compared to the mechanicallooking Titan (t = 5.750, SE = ±0.012, p < 0.001). Emotion recognition was significantly better in Hayden than in the Titan face (t = 7.036, SE = ±0.012, p < 0.001). However, there were no significant differences between the recognition rates in the human face vs. Hayden (t = 1.286, SE = ±0.012, p = 0.4030). However, on further analyzing the results for the interaction between appearances and face regions, it was found that these differences only held for the eye-region conditions. When looking specifically at this condition, there was a significant decrease in the recognition rate from human face to Titan (t = 7.020, SE = ±0.017, p < 0.001) and Hayden to Titan (t = 7.573, SE = ±0.017, p < 0.001), again in line with H1. However, we did not find any significant differences between the recognition rates for the full-face data based on the appearances (see Fig.  2 ). This indicates that the significant main effect of appearance is driven by the eye-region condition.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Effect Of Facial Regions",
      "text": "Marginal estimates with \"emmeans\" revealed that participants recognized the emotions significantly better when they were shown the full-face videos as compared to the eye-region videos (t = 8.315, SE = ±0.013, p < 0.001). Overall, participants were able to recognize 64.4% of the emotions correctly when shown the full face of the robots. Recognition was 51.3% when only the eye-region videos of the robot were shown. Additionally, the eye-region recognition was higher than in a previous study with similar stimuli (49.1% in  [2] ). This supports H2, which predicted that emotion recognition from a full face should be better than just the eye region. Further pair-wise comparisons between face regions and emotions were conducted. Figure  3  shows the confusion matrix with recognition accuracy for both face region types. It was observed that the recognition rates for Fear, Anger, Surprise, and Disgust were similar for both full-face and eye-region videos. Significant differences were found for Happy (t = 15.986, SE = ±0.027, p < 0.001), Sad (t = 6.743, SE = ±0.027, p < 0.001), and Neutral (t = 8.314, SE = ±0.027, p < 0.001), with higher recognition rates in full-face videos.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "This study aimed to explore how robot appearances and facial regions affect robot emotion recognition. GLMM results highlighted the significant effects of appearances and facial regions on emotion recognition. Analysis of estimated margins indicated that greater human-likeness correlated with improved emotion recognition. Notably, emotion recognition rates showed no significant differences between the human and human-like robot faces (Hayden) in full-face videos, which further strengthens the advantage of a human-like face design. However, this difference only holds when the perception is limited to the eye region condition which means that H1 is only supported when the full face is not visible. This suggests that emotions expressed by a robot having a full-face (animated) design, irrespective of its appearance (human-like or mechanical), are recognized similarly to those expressed by a human signifying the advantage of having a fully-animated robot face. One reason for the lack of significant difference between recognition rates in the full-face condition could be because, even though Titan is mechanical-looking, it still has a very expressive mouth region. In comparison with mechanical-looking robots like iCub, Furhat's Titan character appears more human-like when the full face is viewed. As mentioned in Section 2.1, we acknowledge that the specific design of the mechanical-looking robot used here is quite arbitrary (in our case, having an expressive mouth but no eyebrows), so the results might not generalize to all mechanical-looking robots however it aligns with the robotic platforms being widely used in research so far, such as NAO, Pepper, Cozmo.\n\nOn the other hand, a significant difference was found in emotion recognition favoring human-likeness for eye-region videos. This can be attributed to the video cropping, which obscured human-like features and emphasized mechanical aspects (see Section 2.1). The significant decrease in emotion recognition between full-face and eye-region for Titan's emotions is indicative of the same. A comparison of Hayden and Titan's eye-region videos sheds light on how mechanical appearance impacts emotion recognition. This raises questions about the role of eyebrows or pupil shape in the difficulty in recognizing Titan's emotions and whether having a mouth mitigates these effects, as full-face emotion recognition did not differ significantly. These questions warrant further exploration in broader studies. Our findings underscore the potential of human-like robot appearance for enhancing emotion recognition. Future research should explore diverse robot embodiments to address appearance variability among robot designs. In line with H2, the recognition rate for full-face was significantly higher than for the eye-region videos. Pair-wise analysis of estimated margins also supported this hypothesis, with significantly higher recognition for 3 emotions (Happy, Sad, Neutral ) in the full-face videos. This is in line with previous findings which reported better recognition with full-face stimuli  [14] . However, it is worth noting that the difference in recognition for four emotions between eye-region and fullface responses was not statistically significant. This could point to the capability of the eye-region to express emotions sufficiently depending on the emotions that need to be expressed. Nonetheless, omitting a full face may result in the loss of valuable additional cues that could greatly help in emotion recognition. For example, we observe a significant decrease in the recognition of Happy when moving from full-face to eye-region. This could be attributed to the fact that the major cues for happiness lie in the mouth region  [34] . This needs to be kept in mind when deciding whether or not to model the full face when designing a social robot's face.\n\nIt was found that participants struggled to recognize Fear and Disgust for both facial region types (see Fig.  2 ), consistent with findings from a study using a virtual eye region model  [2] . Additionally, participants often confused Fear with Surprise and Disgust with Anger. This could be explained by the Perceptual-Attentional Limitation Hypothesis which posits that the confusion between these emotions arises due to their shared muscle movements and visual similarities  [22, 30] .\n\nThis study is aimed as a first step at investigating the overall influence of appearance and facial regions (if any) on emotion recognition of robot expressions. Focusing on the specifics such as the influence of the features such as eyebrows or pupil size will be looked into in future studies and was beyond the scope of the present paper.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we investigate the influence of appearance and facial region on robot emotion recognition, with specific attention to the human-likeness of their appearance and the role of the eye-region. A comprehensive between-subjects user study was conducted with 305 participants. Results indicated that humanlikeness improved participants' ability to recognize emotions in robots. Additionally, recognition rates from the eye-region, while not as effective as full-face, were found to be within a comparable range. However, it is essential to acknowledge that foregoing the modeling of the full face may result in the loss of crucial cues for certain emotions, as exemplified by the significance of mouth cues in recognizing happiness. Based on the findings of our study we propose the following overall recommendations when designing robot faces and modelling emotions on them:\n\n-A back-projected fully animated face (regardless of whether it is more humanlike or more mechanical-looking) has similar capabilities to express emotions as a human face. -If possible, the whole face should be modeled instead of just the eye region to avoid losing emotional expressiveness. -If a robot face is designed with only the eye region, having more human-like features is important for better emotion recognition.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "This study involved measuring the perception of the facial expressions exhibited by robots by the participants and trying to see if the human-likeness and facial regions of a robot's face had any influence. Two key considerations need to be taken while generalizing results from this work. Firstly, the work focuses on the recognition rates of only the 6 basic emotions whereas humans exhibit far more than just these emotions. Moreover, human emotion is multi-modal and context-dependent. This study only looked at the recognition rates of emotions when expressed on the face without any context or associated cues from other modalities such as speech. This restricts the interpretation of the results to only one modality -visual and also just the 6 basic emotions. Secondly, the study only used a Furhat robot which ties the results to the capabilities and implementation of expressiveness on a single robot platform. To generalize the findings, further studies with a wider selection of robots and appearances need to be carried out.\n\nNonetheless, this study provides a first step toward assessing the influence of the appearance of a robot on emotion recognition by humans.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). Titan gets its me-",
      "page": 4
    },
    {
      "caption": "Figure 1: Emotional expressions displayed by the three characters. The first row depicts",
      "page": 5
    },
    {
      "caption": "Figure 2: Emotion recognition score for each of the three appearances and the combined",
      "page": 8
    },
    {
      "caption": "Figure 2: ). This indicates that the significant main effect of appearance",
      "page": 8
    },
    {
      "caption": "Figure 3: Normalized confusion matrix between actual and selected emotions by the",
      "page": 9
    },
    {
      "caption": "Figure 3: shows the confusion matrix with recognition accuracy for both",
      "page": 9
    },
    {
      "caption": "Figure 2: ), consistent with findings from a study using a",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 1: shows the AUs used to generate",
      "page": 5
    },
    {
      "caption": "Table 1: Mapping of FACS AUs to emotion categories used in the study",
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Is there a \"language of the eyes\"? evidence from normal adults, and adults with autism or asperger syndrome",
      "authors": [
        "S Baron-Cohen",
        "S Wheelwright",
        "T Jolliffe"
      ],
      "year": "1997",
      "venue": "Visual cognition"
    },
    {
      "citation_id": "2",
      "title": "Virtual eye region: development of a realistic model to convey emotion",
      "authors": [
        "S Barrett",
        "F Weimer",
        "J Cosmas"
      ],
      "year": "2019",
      "venue": "Heliyon"
    },
    {
      "citation_id": "3",
      "title": "Evaluating facial displays of emotion for the android robot geminoid f",
      "authors": [
        "C Becker-Asano",
        "H Ishiguro"
      ],
      "year": "2011",
      "venue": "2011 IEEE workshop on affective computational intelligence (WACI)"
    },
    {
      "citation_id": "4",
      "title": "Recognizing emotion in virtual agent, synthetic human, and human facial expressions",
      "authors": [
        "J Beer",
        "A Fisk",
        "W Rogers"
      ],
      "year": "2010",
      "venue": "Proceedings of the Human Factors and Ergonomics Society Annual Meeting"
    },
    {
      "citation_id": "5",
      "title": "Emotion and sociable humanoid robots",
      "authors": [
        "C Breazeal"
      ],
      "year": "2003",
      "venue": "International journal of human-computer studies"
    },
    {
      "citation_id": "6",
      "title": "I show you how i like you-can you read it in my face?[robotics",
      "authors": [
        "L Cañamero",
        "J Fredslund"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on systems, man, and cybernetics-Part A: Systems and humans"
    },
    {
      "citation_id": "7",
      "title": "Reading sadness beyond human faces",
      "authors": [
        "M Chammat",
        "A Foucher",
        "J Nadel",
        "S Dubal"
      ],
      "year": "2010",
      "venue": "Brain Research"
    },
    {
      "citation_id": "8",
      "title": "Impact of personality on the recognition of emotion expressed via human, virtual, and robotic embodiments",
      "authors": [
        "P Chevalier",
        "J Martin",
        "B Isableu",
        "A Tapus"
      ],
      "year": "2015",
      "venue": "24th IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "9",
      "title": "The robot's eye expression for imitating human facial expression",
      "authors": [
        "S Chumkamon",
        "K Masato",
        "E Hayashi"
      ],
      "year": "2014",
      "venue": "11th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology"
    },
    {
      "citation_id": "10",
      "title": "The facial action coding system for characterization of human affective response to consumer product-based stimuli: a systematic review",
      "authors": [
        "E Clark",
        "J Kessinger",
        "S Duncan",
        "M Bell",
        "J Lahne",
        "D Gallagher",
        "S O'keefe"
      ],
      "year": "2020",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "11",
      "title": "Child's recognition of emotions in robot's face and body",
      "authors": [
        "I Cohen",
        "R Looije",
        "M Neerincx"
      ],
      "year": "2011",
      "venue": "Proceedings of the 6th international conference on Human-robot interaction"
    },
    {
      "citation_id": "12",
      "title": "Promises and trust in human-robot interaction",
      "authors": [
        "L Cominelli",
        "F Feri",
        "R Garofalo",
        "C Giannetti",
        "M Meléndez-Jiménez",
        "A Greco",
        "M Nardelli",
        "E Scilingo",
        "O Kirchkamp"
      ],
      "year": "2021",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "13",
      "title": "Assessment of human response to robot facial expressions through visual evoked potentials",
      "authors": [
        "R Craig",
        "R Vaidyanathan",
        "C James",
        "C Melhuish"
      ],
      "year": "2010",
      "venue": "2010 10th IEEE-RAS International Conference on Humanoid Robots"
    },
    {
      "citation_id": "14",
      "title": "Development of animated facial expressions to express emotions in a robot: Roboticon",
      "authors": [
        "L Danev",
        "M Hamann",
        "N Fricke",
        "T Hollarek",
        "D Paillacho"
      ],
      "year": "2017",
      "venue": "2017 IEEE Second Ecuador Technical Chapters Meeting (ETCM)"
    },
    {
      "citation_id": "15",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "16",
      "title": "Pan-cultural elements in facial displays of emotion",
      "authors": [
        "P Ekman",
        "E Sorenson",
        "W Friesen"
      ],
      "year": "1969",
      "venue": "Science"
    },
    {
      "citation_id": "17",
      "title": "Facial expressions, emotions, and sign languages",
      "authors": [
        "E Elliott",
        "A Jacobs"
      ],
      "year": "2013",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "18",
      "title": "Improving aspects of empathy and subjective performance for hri through mirroring facial expressions",
      "authors": [
        "B Gonsior",
        "S Sosnowski",
        "C Mayer",
        "J Blume",
        "B Radig",
        "D Wollherr",
        "K Kühnlenz"
      ],
      "year": "2011",
      "venue": "Improving aspects of empathy and subjective performance for hri through mirroring facial expressions"
    },
    {
      "citation_id": "19",
      "title": "Age deficits in facial affect recognition: The influence of dynamic cues",
      "authors": [
        "S Grainger",
        "J Henry",
        "L Phillips",
        "E Vanman",
        "R Allen"
      ],
      "year": "2017",
      "venue": "Journals of Gerontology Series B: Psychological Sciences and Social Sciences"
    },
    {
      "citation_id": "20",
      "title": "Using eye shape to improve affect recognition on a humanoid robot with limited expression: University of southern california",
      "authors": [
        "J Greczek",
        "K Swift-Spong",
        "M Mataric"
      ],
      "year": "2011",
      "venue": "Comp. Sci. Department"
    },
    {
      "citation_id": "21",
      "title": "Understanding social robots",
      "authors": [
        "F Hegel",
        "C Muhl",
        "B Wrede",
        "M Hielscher-Fastabend",
        "G Sagerer"
      ],
      "year": "2009",
      "venue": "Second International Conferences on Advances in Computer-Human Interactions"
    },
    {
      "citation_id": "22",
      "title": "Exploration of visual factors in the disgust-anger confusion: the importance of the mouth",
      "authors": [
        "E Hendel",
        "A Gallant",
        "M Mazerolle",
        "S Cyr",
        "A Roy-Charland"
      ],
      "year": "2023",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "23",
      "title": "A preliminary study on reading the mind in the eyes of the robot",
      "authors": [
        "J Kang",
        "Y Park"
      ],
      "year": "2021",
      "venue": "2021 30th IEEE International Conference on Robot & Human Interactive Communication"
    },
    {
      "citation_id": "24",
      "title": "Emotions and interpersonal relationships: Toward a person-centered conceptualization of emotions and coping",
      "authors": [
        "R Lazarus"
      ],
      "year": "2006",
      "venue": "Journal of personality"
    },
    {
      "citation_id": "25",
      "title": "Can a humanoid face be expressive? a psychophysiological investigation",
      "authors": [
        "N Lazzeri",
        "D Mazzei",
        "A Greco",
        "A Rotesi",
        "A Lanatà",
        "D De Rossi"
      ],
      "year": "2015",
      "venue": "Frontiers in bioengineering and biotechnology"
    },
    {
      "citation_id": "26",
      "title": "Exaggerating facial expressions: A way to intensify emotion or a way to the uncanny valley?",
      "authors": [
        "M Mäkäräinen",
        "J Kätsyri",
        "T Takala"
      ],
      "year": "2014",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "27",
      "title": "The furhat back-projected humanoid head-lip reading, gaze and multi-party interaction",
      "authors": [
        "S Moubayed",
        "G Skantze",
        "J Beskow"
      ],
      "year": "2013",
      "venue": "International Journal of Humanoid Robotics"
    },
    {
      "citation_id": "28",
      "title": "Age-related changes in gaze behaviour during social interaction: An eye-tracking study with an embodied conversational agent",
      "authors": [
        "K Pavic",
        "A Oker",
        "M Chetouani",
        "L Chaby"
      ],
      "year": "2021",
      "venue": "Quarterly Journal of Experimental Psychology"
    },
    {
      "citation_id": "29",
      "title": "It's in your eyes: Which facial design is best suited to let a robot express emotions?",
      "authors": [
        "K Pollmann",
        "N Tagalidou",
        "N Fronemann"
      ],
      "year": "2019",
      "venue": "Proceedings of Mensch und Computer"
    },
    {
      "citation_id": "30",
      "title": "The confusion of fear and surprise: a developmental study of the perceptual-attentional limitation hypothesis using eye movements",
      "authors": [
        "A Roy-Charland",
        "M Perron",
        "C Young",
        "J Boulard",
        "J Chamberland"
      ],
      "year": "2015",
      "venue": "The Journal of Genetic Psychology"
    },
    {
      "citation_id": "31",
      "title": "The psychology of appraisal: Specific emotions and decision-making",
      "authors": [
        "J So",
        "C Achar",
        "D Han",
        "N Agrawal",
        "A Duhachek",
        "D Maheswaran"
      ],
      "year": "2015",
      "venue": "Journal of Consumer Psychology"
    },
    {
      "citation_id": "32",
      "title": "Survey of emotions in human-robot interactions: Perspectives from robotic psychology on 20 years of research",
      "authors": [
        "R Stock-Homburg"
      ],
      "year": "2022",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "33",
      "title": "Age differences in emotion recognition skills and the visual scanning of emotion faces",
      "authors": [
        "S Sullivan",
        "T Ruffman",
        "S Hutton"
      ],
      "year": "2007",
      "venue": "The Journals of Gerontology Series B: Psychological Sciences and Social Sciences"
    },
    {
      "citation_id": "34",
      "title": "Mapping the emotional face. how individual face parts contribute to successful emotion recognition",
      "authors": [
        "M Wegrzyn",
        "M Vogt",
        "B Kireclioglu",
        "J Schneider",
        "J Kissler"
      ],
      "year": "2017",
      "venue": "PloS one"
    }
  ]
}