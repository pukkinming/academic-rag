{
  "paper_id": "2310.17383v1",
  "title": "On The Recognition Of The Game Type Based On Physiological Signals And Eye Tracking",
  "published": "2023-10-26T13:27:23Z",
  "authors": [
    "Łukasz Czekaj",
    "Łukasz Radzinski",
    "Mateusz Kolimaga",
    "Jakub Domaszewicz",
    "Robert Kitłowski",
    "Mariusz Szwoch",
    "Włodzisław Duch"
  ],
  "keywords": [
    "behavioural science",
    "physiology",
    "man-machine interfaces",
    "statistical analysis",
    "signal processing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automated interpretation of signals yields many impressive applications from the area of affective computing and human activity recognition (HAR). In this paper we ask the question about possibility of cognitive activity recognition on the base of particular set of signals. We use recognition of the game played by the participant as a playground for exploration of the problem. We build classifier of three different games (Space Invaders, Tetris, Tower Defence) and inter-game pause. We validate classifier in the player-independent and playerdependent scenario. We discuss the improvement in the player-dependent scenario in the context of biometric person recognition. On the base of the results obtained in game classification, we consider potential applications in smart surveillance and quantified self.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Easy access to the wide spectrum of wearable and mobile sensors drives interest of researchers toward automated and continuous recognition of various human activities. Among the motivations are: activity tracking in such areas as healthcare and elder care, smart surveillance, intelligent environment, education, entertainment and virtual reality, biofeedback and wellness (emotional strain and stress assessment). Implementations of research results together with Big Data technologies and individual engaged in the self-tracking manifest as cultural phenomenon of quantifies self movement  Swan (2013)  Affective computing is fast developing multidisciplinary research area bringing together artificial intelligence, cognitive and social sciences, with the aim of detection, recognition and interpretation of human affects  Tao and Tan (2005) ,  Poria et al. (2017) ,  Wang et al. (2022) . Affective computing utilize various machine learning techniques in processing data acquired from wearable and mobile sensors, cameras and other dedicated input devices. Among its interest are: automated emotion recognition  Dzedzickis et al. (2020) , affective gaming  Hudlicka (2008)  (smart games and player engagement), stress detection and mental workload  Giannakakis et al. (2022) . Closely related is so called mind-reading with impressive results in recognition of cognitive states from fMRI data  Norman et al. (2006)  and generating images from EEG signals  Bai et al. (2023) . We were inspired by various results in quantified self  Swan (2012) ,  HAR Bouchabou et al. (2021) , mental workload and time pressure detection  Nickel and Nachreiner (2003) , games as affective stimulation  Chen et al. (2019) .\n\nIn this paper we focus on the problem of the recognition of cognitive activity, i.e. classification of what type of cognitive task from a given set is performed by the person at a given moment. The input to the classifier are physiological signals along with eye tracking data (for brevity we call them signals). We simplify this general problem and study it as a task of distinguishing between different game types and inter-game pause (for short we refer to this pause extended game type classification as game classification). Our work aligns with the framework of sensor-based, single-user activity recognition and the results may be viewed as extension of classical HAR from physical to mental activities. The intuitions behind presented research are as follows. Despite the fact that the participant is sitting in front of the computer screen throughout the game, and the physical conditions do not change much between different games, the virtual environment changes a lot. Different games engage participant in different ways. Theirs dynamics, speed, turns design and time pressure, and overall concentration level required by the game reflect in signals recorded during the game. Game specific screen layout is correlated with eye-movement patterns. The mentioned factors enable identification of the type of game. In this paper we present XG-Boost  Chen and Guestrin (2016)  based detector (multilabel classifier) fuelled by the features obtained from During the session, signals (ECG, RESP, GSR, EYETR) are recorded along with information about game type or a pause. Sliding window of the width 15 s and step 1 s is applied on the signals and is used for feature extraction. Window is labelled according to its centre. Subject normalized features vector is passed to the multi-class model. In the prediction mode, for every data window, models provide 4 probabilities: 1 for each game type/pause. Then the label corresponding to the class with the highest probability value is taken.\n\nthe following signals: electrocardiography (ECG), respiration (RESP), galvanic skin response (GSR) and eye tracking (EYETR). All those signals can be recorded in non-invasive way and the applications using such signals do not require professional training. Moreover, the sensors may be implemented in non-distributive way without need of wearables (e.g. ECG recording from the steering  Warnecke et al. (2022) ). We build two types of game detectors: playerindependent and player-dependent. Detectors are evaluated using leave-one-out cross-validation (LOOCV)one test player is selected in each iteration. For playerindependent detector, recordings for test player are completely excluded from the train dataset. For playerdependent detectors, recordings from the test player are split between train and test datasets. The model also learns features specific for the test player (see FIG.  2  for details). Since we observed better performance of the player-dependent detector, we asked if that difference should be attributed to the player's related clusterisation of recorded signals (e.g. player related shift of mean HR) or player's preferences of the specific game playing style (i.e. clustering of signals recorded for given game which may be interpreted as playing style). To answer the question we build a biometric player recognition model  Odinaka et al. (2012)  that predicted if a given part of signals belongs to the test user. Our results suggest that player's related clustering is important factor in model performance improvement.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data Collection",
      "text": "The data collection process is depicted in FIG.  1 . Each player participated in one recording session. The recording session was preceded with eye tracker calibration and a short relaxation (for the purpose of heart rate and breath rate stabilisation). The recording session consisted of game rounds interwoven with pauses. Game round took ∼ 5 mins and pause took ∼ 1 min. There were 4 consecutive rounds for each game. Participants played in 3 games: Space Invaders, Tetris and Tower Defense. Games differ in graphical layout and dynamics: (i) Space Invaders is characterized by spawning waves of enemies; (ii) in Tetris the gameplay speeds up and the time pressure increases at the end of each round; (iii) Tower Defense is a subgenre of turn-based strategy game. Data collection during recording session was performed with following devices: ECG and respiration (bioimpedance method) were recorded with the Aidmed One recorder  (Czekaj et al. (2020) ), GSR was recorded with Shimmer device (https://shimmersensing.com/), eye positions were collected with Talon (https://talonvoice.com/) and Tobii Eye Tracker (https://gaming.tobii.com/). Data was collected from 20 volunteers in age 20-40 years, 5 female. All the data was collected in the same laboratory station. The participants were instructed about data collection protocol. Desktop application developed in Python was used to control the protocol. Data were collected in a separate room in a quiet, isolated environment.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Detector",
      "text": "In this paper we present results for 3 classification tasks: game classification for i) player-independent and ii) player-dependent scenario, iii) biometric player recognition. For each task we built XGBoost based model fed by features extracted from the signals (see FIG.  1 ). For tasks i) and ii) we developed multiclass models (multi:softmax) with game type including pause as a label. For task iii) we used binary loss (binary:logistic) and label denotes selected player. Biometric player recognition mimics player-dependent scenario in the way of data split, however the goal is to recognize player on the base of signals sample. We applied class balancing with example weighting in all models. For details of train and test datasets preparation see FIG.  2 . For all tasks we build separate models for 3 sets of features derived from increasing number of signals: I) SIG-1: ECG, RESP; II) SIG-2: ECG, RESP, GSR; III) SIG-3: ECG, RESP, GSR, EYETR. Our motivation here was to evaluate a hierarchy of signals/sensors from less to more intrusive: Aidmed One recorder is chest belt wearable, GSR requires fingers attached electrodes and eye tracking requires special camera placed in front of player or the use of special glasses. Signal processing and feature extraction pipeline was the same in all models. Based on ECG, we calculated heart rate variability (HRV) features using (https://github.com/Aurahealthcare)  Pham et al. (2021) . GSR signal was decomposed into phasic and tonic components and analysed with Neurokit  Makowski et al. (2021)  toolkit. A detailed list of features is presented below. ECG based features:\n\n• AVG_HR_15s -average HR from 15 s window • AVG_HR_5s -average HR from central 5 s range • AVG_HR_RATIO -ratio of AVG_HR_5s-to-AVG_HR_15s\n\n• AVG_HR_DIFF -difference between average HR of left and right half of the signal window\n\n• SDNN -standard deviation of inter-beat intervals\n\n• RMSSD -root-mean-square of successive differences\n\n• LF_POWER -relative power of the low-frequency band (≤ 0.15 Hz)\n\n• HF_POWER -relative power of the low-frequency band (0.15 -0.4 Hz)\n\n• LF_HF_RATIO -ratio of LF-to-HF power\n\n• TOTAL_POWER -total power of the heart rate signal\n\nRespiration based features:\n\n• RESP_AMP -respiration amplitude\n\n• RESP_DOM_FREQ -dominant frequency of respiration signal (estimation of respiration rate)\n\n• RESP_PEAKS -peaks number in respiration signal (estimation of respiration rate)\n\nGSR based features (features are calculated separately for the tonic and phasic components):\n\n• AVG_GSR_15s -average value of GSR component from 15 s window • AVG_GSR_5s -average value of GSR component from central 5 s range\n\n• AVG_GSR_RATIO -ratio of AVG_GSR_5s-to-AVG_GSR_15s\n\n• AVG_MEAN_GSR_DIFF -difference between average GSR component of left and right half of the signal window\n\n• STD_GSR -standard deviation of GSR signal component\n\n• GSR_PEAKS -number of peaks in GSR (raw signal)\n\nEye tracking based features (gaze position in the screen coordinate, gaze outside the screen was filtered out):\n\n• BLINKS -blinks number\n\n• GAZE_OUTSIDE -pc. of time when gaze was not located on screen\n\n• AVG_POSITION -average of eye position\n\n• STD_POSITION -standard deviation of gaze position\n\n• KURT_POSITION -kurtosis of gaze position",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "Results for the player-independent classification, playerdependent classification and biometric player recognition are summarized in TAB. 1, TAB. 2 and TAB. 3 respectively. Raw output of the models is presented in FIG.  3 . For each experiment we present results of random model (random) as a baseline and then results for selected sets of signals: SIG-1, SIG-2, SIG-3. For each experiment we reported accuracy, precision, recall and F1 score. Data from different games and cross validation iterations were aggregated using multiclass macro averaging and balanced accuracy score  Mosley (2013) . All game classification models significantly outperform All sets of signals carry information about player identity. Our interpretation of the results is that the ECG, respiration and GSR features are more specific to the user and the technologies for cognitive activity recognition based on that signals will require user specific calibration of the model. On the other hand eye tracking carry information about game specific layout and the reaction on visual signals generalize between users.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "Using the example of game type/pause classification during play we have demonstrated the possibility of cognitive activity recognition. We have discovered the user specific characteristic of the heart rate, respiration and GSR signal. Our results may find applications in tasks related with smart surveillance (e.g. in profession like flight control where mental overload/fatigue may be crucial factor for safety and performance), affect-aware video games and smart games (e.g. feedback provided to student about focus) and e-learning software, and new methods for human-machine interactions. Cognitive activity recognition implemented as a part of quantified self application may be used as qualitative feedback loops for behavior change.\n\nSince each player participated in only one recording ses-",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Data collection and signal processing. The top",
      "page": 2
    },
    {
      "caption": "Figure 2: for details). Since we observed better performance of",
      "page": 2
    },
    {
      "caption": "Figure 2: Models and Evaluation Scenarios. The top",
      "page": 2
    },
    {
      "caption": "Figure 1: Each player participated in one recording session. The",
      "page": 2
    },
    {
      "caption": "Figure 2: For all tasks we build separate models for 3 sets of fea-",
      "page": 3
    },
    {
      "caption": "Figure 3: Detector output for player-dependent scenario",
      "page": 4
    },
    {
      "caption": "Figure 3: For each experiment we present results of ran-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Player-independent classification.",
      "data": [
        {
          "signals": "random",
          "acc.": "0.25",
          "prec.": "0.25",
          "rec.": "0.25",
          "F1": "0.25"
        },
        {
          "signals": "SIG-1\nSIG-2\nSIG-3",
          "acc.": "0.50\n0.52\n0.71",
          "prec.": "0.46\n0.48\n0.68",
          "rec.": "0.48\n0.49\n0.70",
          "F1": "0.47\n0.48\n0.69"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Player-independent classification.",
      "data": [
        {
          "signals": "random",
          "acc.": "0.25",
          "prec.": "0.25",
          "rec.": "0.25",
          "F1": "0.25"
        },
        {
          "signals": "SIG-1\nSIG-2\nSIG-3",
          "acc.": "0.62\n0.76\n0.89",
          "prec.": "0.58\n0.75\n0.88",
          "rec.": "0.58\n0.74\n0.89",
          "F1": "0.58\n0.75\n0.89"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Player-independent classification.",
      "data": [
        {
          "signals": "random",
          "acc.": "0.5",
          "prec.": "0.5",
          "rec.": "0.5",
          "F1": "0.5"
        },
        {
          "signals": "SIG-1\nSIG-2\nSIG-3",
          "acc.": "0.78\n0.84\n0.87",
          "prec.": "0.82\n0.88\n0.90",
          "rec.": "0.78\n0.84\n0.87",
          "F1": "0.78\n0.84\n0.87"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "DreamDiffusion: Generating High-Quality Images from Brain EEG Signals",
      "authors": [
        "Y Bai",
        "X Wang",
        "Y Cao",
        "Y Ge",
        "C Yuan"
      ],
      "year": "2023",
      "venue": "DreamDiffusion: Generating High-Quality Images from Brain EEG Signals"
    },
    {
      "citation_id": "2",
      "title": "A Survey of Human Activity Recognition in Smart Homes Based on IoT Sensors Algorithms: Taxonomies, Challenges, and Opportunities with Deep Learning",
      "authors": [
        "D Bouchabou",
        "S Nguyen",
        "C Lohr",
        "B Leduc",
        "I Kanellos"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "3",
      "title": "XGBoost: A Scalable Tree Boosting System",
      "authors": [
        "T Chen",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proc. of the 22nd ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "4",
      "title": "Artificial neural networks-based classification of emotions using wristband heart rate monitor data",
      "authors": [
        "Y Chen",
        "C Hsiao",
        "W Zheng",
        "R Lee"
      ],
      "year": "2019",
      "venue": "Medicine"
    },
    {
      "citation_id": "5",
      "title": "Validation and usability of AIDMED-telemedical system for cardiological and pulmonary diseases",
      "authors": [
        "L Czekaj",
        "J Domaszewicz",
        "L Radzinski",
        "A Jarynowski",
        "R Kitlowski",
        "A Doboszynska"
      ],
      "year": "2020",
      "venue": "E-methodology"
    },
    {
      "citation_id": "6",
      "title": "Human Emotion Recognition: Review of Sensors and Methods. Sensors",
      "authors": [
        "A Dzedzickis",
        "A Kaklauskas",
        "V Bucinskas"
      ],
      "year": "2020",
      "venue": "Human Emotion Recognition: Review of Sensors and Methods. Sensors"
    },
    {
      "citation_id": "7",
      "title": "Review on Psychological Stress Detection Using Biosignals",
      "authors": [
        "G Giannakakis",
        "D Grigoriadis",
        "K Giannakaki",
        "O Simantiraki",
        "A Roniotis",
        "M Tsiknakis"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Affective computing for game design",
      "authors": [
        "E Hudlicka"
      ],
      "year": "2008",
      "venue": "Proceedings of the 4th Intl. North American Conference on Intelligent Games and Simulation"
    },
    {
      "citation_id": "9",
      "title": "NeuroKit2: A Python toolbox for neurophysiological signal processing",
      "authors": [
        "D Makowski",
        "T Pham",
        "Z Lau",
        "J Brammer",
        "F Lespinasse",
        "H Pham",
        "C Schölzel"
      ],
      "year": "2021",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "10",
      "title": "A balanced approach to the multiclass imbalance problem",
      "authors": [
        "L Mosley"
      ],
      "year": "2013",
      "venue": "A balanced approach to the multiclass imbalance problem"
    },
    {
      "citation_id": "11",
      "title": "Sensitivity and Diagnosticity of the 0.1-Hz Component of Heart Rate Variability as an Indicator of Mental Workload",
      "authors": [
        "P Nickel",
        "F Nachreiner"
      ],
      "year": "2003",
      "venue": "Human Factors"
    },
    {
      "citation_id": "12",
      "title": "Beyond mind-reading: multi-voxel pattern analysis of fMRI data",
      "authors": [
        "K Norman",
        "S Polyn",
        "G Detre",
        "J Haxby"
      ],
      "year": "2006",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "13",
      "title": "ECG biometric recognition: A comparative analysis",
      "authors": [
        "I Odinaka",
        "P Lai",
        "A Kaplan",
        "J O'sullivan",
        "E Sirevaag",
        "J Rohrbaugh"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "14",
      "title": "Heart Rate Variability in Psychology: A Review of HRV Indices and an Analysis Tutorial",
      "authors": [
        "T Pham",
        "Z Lau",
        "S Chen",
        "D Makowski"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "15",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information fusion"
    },
    {
      "citation_id": "16",
      "title": "Sensor mania! the internet of things, wearable computing, objective metrics, and the quantified self 2.0",
      "authors": [
        "M Swan"
      ],
      "year": "2012",
      "venue": "Journal of Sensor and Actuator networks"
    },
    {
      "citation_id": "17",
      "title": "The quantified self: Fundamental disruption in big data science and biological discovery",
      "authors": [
        "M Swan"
      ],
      "year": "2013",
      "venue": "Big data"
    },
    {
      "citation_id": "18",
      "title": "Affective computing: A review",
      "authors": [
        "J Tao",
        "T Tan"
      ],
      "year": "2005",
      "venue": "International Conference on Affective computing and intelligent interaction"
    },
    {
      "citation_id": "19",
      "title": "Python 3 Reference Manual",
      "authors": [
        "G Van Rossum",
        "F Drake"
      ],
      "year": "2009",
      "venue": "Python 3 Reference Manual"
    },
    {
      "citation_id": "20",
      "title": "A systematic review on affective computing: emotion models, databases, and recent advances",
      "authors": [
        "Y Wang",
        "W Song",
        "W Tao",
        "A Liotta",
        "D Yang",
        "X Li",
        "S Gao",
        "Y Sun",
        "W Ge",
        "W Zhang",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "21",
      "title": "Printed and Flexible ECG Electrodes Attached to the Steering Wheel for Continuous Health Monitoring during Driving",
      "authors": [
        "J Warnecke",
        "N Ganapathy",
        "E Koch",
        "A Dietzel",
        "M Flormann",
        "R Henze",
        "T Deserno"
      ],
      "year": "2022",
      "venue": "Sensors"
    }
  ]
}