{
  "paper_id": "2510.12819v1",
  "title": "Beyond Discrete Categories: Multi-Task Valence-Arousal Modeling For Pet Vocalization Analysis",
  "published": "2025-10-09T23:39:40Z",
  "authors": [
    "Junyao Huang",
    "Rumin Situ"
  ],
  "keywords": [
    "Pet Emotion Recognition",
    "Valence-Arousal Model",
    "Multi-Task Learning",
    "Audio Transformer",
    "Animal Vocalization Analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Pet emotion recognition from vocalizations has traditionally relied on discrete classification methods, which suffer from boundary ambiguity between adjacent categories and fail to capture subtle intensity variations-limitations that become particularly problematic in realworld applications such as pet welfare monitoring. We propose a continuous Valence-Arousal (VA) modeling approach that represents emotions in a two-dimensional space, combined with an automatic VA label generation algorithm that derives Arousal from RMS energy and Valence from spectral features (centroid, zero-crossing rate) augmented with emotion-specific priors, enabling large-scale annotation of 42,553 pet vocalization samples without requiring subjective self-reports. Our multi-task learning framework jointly trains VA regression alongside auxiliary classification tasks (emotion, body size, gender), leveraging knowledge transfer to enhance prediction: size classification forces learning of frequency features, while gender classification encourages timbre feature extraction, both of which benefit VA estimation. Experiments demonstrate that our Audio Transformer model (6 layers, 19.5M parameters) achieves validation VA MAE of 0.1124, Valence Pearson correlation r = 0.9024, and Arousal r = 0.7155, effectively resolving the 356 confusion cases between \"territorial\" and \"happy\" categories observed in discrete baselines (F1 = 0.8885) through natural separation in continuous VA space. This work introduces the first continuous VA framework for pet vocalization analysis, providing a more expressive and intuitive representation for humanpet interaction systems and enabling fine-grained emotion intensity monitoring critical for veterinary diagnostics and behavioral training. Our approach demonstrates strong potential for real-world deployment in consumer pet-care products such as the LingChongTong AI pet emotion translator and other smart pet monitoring devices.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Pets have become cherished members of millions of households worldwide, with over 67% of U.S. households owning a pet as of 2023  [1] . Understanding the emotional states and needs of pets is crucial for both pet welfare and the human-animal bond  [28] . Vocalizations represent one of the primary channels through which pets, particularly dogs, express their emotions, convey needs, and communicate with humans  [16] . However, interpreting these vocalizations accurately remains challenging for most pet owners, leading to potential misunderstandings that can negatively impact animal welfare and the quality of human-pet interactions.\n\nTraditional approaches to pet vocalization emotion recognition rely predominantly on discrete classification methods, categorizing sounds into a fixed set of emotion labels such as \"anxious,\" \"happy,\" or \"territorial\"  [19] ,  [9] . While these methods have achieved reasonable performance, they suffer from two fundamental limitations. First, emotions are inherently continuous phenomena, and forcing them into discrete categories inevitably leads to boundary ambiguity problems. For instance, in our preliminary experiments using an 8-class discrete classifier, we observed significant confusion between \"territorial\" and \"happy\" categories (F1 scores of 0.85 and 0.82, respectively), with 356 confusion cases between these two emotions alone. Second, discrete outputs lack the expressiveness needed to capture subtle emotional variations-they cannot distinguish between \"slightly anxious\" and \"extremely anxious,\" even though such distinctions are critical for practical applications such as pet monitoring systems.\n\nThe Valence-Arousal (VA) model offers an elegant solution to these limitations. Originating from Russell's circumplex model of affect  [23] , the VA framework represents emotions in a continuous two-dimensional space: Valence (ranging from negative to positive affect) and Arousal (ranging from calm to excited states). This dimensional approach has proven highly successful in human speech emotion recognition  [24] ,  [21] ,  [20] , where it naturally handles ambiguous emotional boundaries and provides fine-grained intensity information. The continuous nature of VA space aligns well with how humans intuitively perceive and describe emotions  [18] , making it particularly suitable for user-facing applications. However, despite its advantages, the VA model has not been systematically applied to pet vocalization analysis.\n\nApplying VA modeling to pet vocalizations presents unique technical challenges. First, unlike human speech emotion recognition where VA labels can be obtained through self-report questionnaires  [2] , pets cannot provide subjective ratings of their emotional states. This creates a fundamental labeling problem that has hindered the development of VA-based pet emotion recognition systems. Second, pet vocalizations exhibit significant acoustic variations due to physical characteristics such as body size-large breed dogs produce low-frequency vocalizations while small breeds emit high-pitched sounds  [16] . How these acoustic features relate to VA dimensions and whether they should be explicitly modeled remains an open question.\n\nThis paper addresses these challenges by proposing an automatic VA label generation algorithm specifically designed for pet vocalizations. Our algorithm combines acoustic features with emotion-specific priors: Arousal labels are derived from RMS energy levels using logarithmic mapping (reflecting the physiological principle that high-energy sounds correspond to high arousal states), while Valence labels integrate spectral features (spectral centroid, zero-crossing rate) with emotion-specific biases (e.g., \"fearful\" receives a negative bias of -0.18, \"excited\" receives a positive bias of +0.  14) . This approach enables us to automatically generate VA annotations for 42,553 pet vocalization samples, constructing the first large-scale VA-labeled dataset for pet emotion recognition.\n\nWe design a multi-task learning framework that jointly performs VA regression (primary tasks) alongside auxiliary classification tasks for discrete emotion categories, body size, and gender. This design is motivated by the observation that auxiliary tasks force the model to learn frequency features (size-related) and timbre features (gender-related), which simultaneously benefit VA prediction through knowledge transfer  [4] ,  [22] . Our Audio Transformer model  [7] , consisting of 6 layers with 512 hidden dimensions and 8 attention heads (19.5M parameters total), achieves validation VA MAE of 0.1124, Valence Pearson correlation of r = 0.9024, and Arousal Pearson correlation of r = 0.7155 on our dataset.\n\nExperimental results demonstrate clear advantages of VA modeling over discrete classification. In handling the previously problematic \"territorial\" vs \"happy\" confusion, the VA space naturally separates these emotions through the Valence dimension (territorial: V≈-0.3, happy: V≈+0.6), effectively resolving the boundary ambiguity issue. Furthermore, continuous VA values provide a more intuitive user experience-pet owners can more easily interpret \"Valence=-0.5\" (moderately negative) than a probability distribution like \"88% anxious + 12% territorial.\"\n\nThe continuous representation also enables fine-grained distinctions such as differentiating between mild and severe anxiety, which is essential for welfare monitoring applications.\n\nBeyond academic contributions, this work paves the way for practical applications in consumer pet-care technology. Our continuous VA emotion modeling approach enables deployment in real-time pet emotion monitoring devices such as AI pet emotion translators, smart pet cameras, wearable collars, and standalone pet monitors. The inference speed of our model (<50ms on consumer hardware) makes it suitable for edge device deployment, enabling applications ranging from veterinary telemedicine (remote health monitoring through vocal stress analysis) to animal shelter management (identifying distressed animals requiring immediate care) to pet training systems (providing instant feedback on emotional responses).\n\nThe remainder of this paper is organized as follows: Section 2 reviews related work in animal vocalization analysis, VA emotion modeling, and multi-task learning. Section 3 details our VA label generation algorithm and multi-task model architecture. Section 4 describes our dataset construction and experimental setup. Section 5 presents quantitative results and analysis. Section 6 discusses limitations and future directions. Section 7 concludes the paper.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Related Work Target: 2 pages, 3 subsections Label: sec:related Our work builds upon three major research areas: Valence-Arousal emotion modeling, animal vocalization analysis, and multi-task learning for emotion recognition. We review each area and position our contributions relative to existing work.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Valence-Arousal Emotion Modeling",
      "text": "The Valence-Arousal (VA) model, originating from Russell's circumplex model of affect  [23] , represents emotions in a two-dimensional continuous space: Valence (positive-negative) and Arousal (calm-excited). This dimensional approach has been widely adopted in affective computing as an alternative to discrete emotion categories, particularly for handling ambiguous emotional boundaries and capturing intensity variations  [18] ,  [2] . In human speech emotion recognition, VA modeling has become the standard approach for continuous emotion prediction. The AVEC (Audio/Visual Emotion Challenge) series  [21] ,  [20]  has driven significant progress in this area, with participants developing sophisticated methods for predicting continuous VA values from speech and facial expressions. These challenges established benchmarks such as the SEWA database for cross-cultural affect recognition  [21]  and the RECOLA dataset for multimodal emotion analysis. Recent work on the Aff-Wild database  [11]  further advanced VA prediction in-the-wild scenarios, demonstrating the robustness of continuous emotion modeling in unconstrained environments. Compared to discrete classification, VA modeling offers several advantages: (1) it provides fine-grained intensity information (e.g., \"slightly negative\" vs. \"extremely negative\"),  (2)  it avoids boundary ambiguity between adjacent emotion categories, and (3) it aligns with how humans naturally perceive and describe emotions  [17] . These benefits have been validated in speech emotion recognition tasks, where continuous VA prediction often outperforms discrete classification in capturing subtle emotional variations. Our Contribution: While VA modeling is well-established for human emotions, it has not been systematically applied to pet vocalizations. Our work is the first to introduce automatic VA label generation specifically designed for animal sounds, addressing the fundamental challenge that pets cannot provide self-reported emotional ratings.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Animal Vocalization Analysis",
      "text": "Research on animal vocalizations has a long history in behavioral biology and comparative psychology. Early studies by Molnár et al.  [16]  demonstrated that humans can discriminate between dogs based on acoustic parameters such as fundamental frequency and spectral envelope, establishing the foundation for acoustic analysis of canine emotions. Taylor et al.  [28]  showed that dogs can discriminate emotional expressions in human faces, suggesting bidirectional emotional communication between humans and dogs. Recent advances in deep learning have enabled automated analysis of dog vocalizations. Pérez et al.  [19]  applied deep neural networks to classify dog barks by identity, breed, age, sex, and context using 19,643 barks from 113 dogs, achieving strong classification performance. Huang et al.  [9]  proposed a barking emotion recognition method based on Mamba and Synchrosqueezing Short-Time Fourier Transform, demonstrating the effectiveness of advanced signal processing techniques. Kim et al.  [10]  developed a fully automatic voice analysis system for bioacoustics studies, enabling large-scale analysis of dog vocalizations. Chowdhury et al.  [5]  explored transfer learning from Wav2Vec2 models pretrained on human speech to dog bark analysis, showing that human speech representations can transfer to animal vocalizations. However, nearly all existing work relies on discrete emotion classification-categorizing vocalizations into fixed emotion labels such as \"aggressive,\" \"fearful,\" or \"happy.\" This approach suffers from two limitations:  (1)  boundary ambiguity between adjacent categories (e.g., our preliminary experiments showed significant confusion between \"territorial\" and \"happy\" with 356 confusion cases), and (2) inability to express intensity variations (e.g., \"mildly anxious\" vs. \"severely anxious\" are both labeled as \"anxious\"). Recent studies have also explored the emotional states of companion animals more broadly. Simola et al.  [25]  reviewed the use of vocalizations to assess animal welfare and affective states, emphasizing the importance of continuous emotion representation. Tallet et al.  [27]  investigated emotional contagion in dogs responding to human and dog vocalizations, providing evidence for cross-species emotional communication. Siniscalchi et al.  [26]  found that dogs exhibit hemispheric lateralization when processing emotional vocalizations, suggesting sophisticated neural mechanisms for emotion perception. Van den Bussche et al.  [29]  provided an engineering perspective on emotional assessment in companion animals, highlighting the need for automated systems. Our Contribution: We are the first to apply continuous VA modeling to pet vocalization emotion recognition, moving beyond discrete classification to capture fine-grained emotional variations. Our automatic VA label generation algorithm addresses the labeling challenge unique to animal studies, where ground-truth emotional ratings cannot be obtained through self-report.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Multi-Task Learning For Emotion Recognition",
      "text": "Multi-task learning (MTL) has proven effective for emotion recognition by leveraging knowledge sharing across related tasks. Caruana's seminal work  [4]  identified four mechanisms through which MTL improves performance: implicit data augmentation, attention focusing, eavesdropping, and representation bias. Ruder  [22]  provided a comprehensive overview of MTL in deep neural networks, categorizing approaches into hard parameter sharing (shared encoder with task-specific heads) and soft parameter sharing (task-specific encoders with regularization). In speech emotion recognition, MTL has been successfully applied to combine multiple objectives. Cai et al.  [3]  proposed a joint framework for speech-to-text and emotion classification using wav2vec-2.0, achieving state-of-the-art results on IEMOCAP by leveraging transcription supervision to improve emotion prediction. Latif et al.  [12]  introduced MTL-AUG, which learns from augmented data using augmentation-type classification and unsupervised reconstruction as auxiliary tasks, demonstrating that auxiliary tasks can guide the model to learn more robust features. Ghosh et al.  [6]  developed MMER, a multimodal multi-task framework with crossmodal self-attention between text and acoustic modalities, showing that modality-specific tasks enhance multimodal fusion. For audio classification more broadly, the Audio Spectrogram Transformer (AST)  [7]  introduced the first convolution-free purely attention-based model, achieving state-of-the-art performance on AudioSet (0.485 mAP). Follow-up work on self-supervised AST (SSAST)  [8]  demonstrated that masked spectrogram patch modeling can effectively pretrain audio transformers, analogous to masked language modeling in NLP. These architectural advances provide a strong foundation for our transformer-based VA emotion model. Our Contribution: We design a novel multi-task learning framework that jointly performs VA regression (main tasks) alongside auxiliary classification tasks for discrete emotion, body size, and gender. Unlike previous MTL work that combines similar tasks (e.g., emotion + sentiment), our framework leverages knowledge transfer from physical attribute classification to emotional state regression. Size classification forces the model to learn frequency features (large breeds → low frequencies), gender classification encourages learning timbre features (male → lower pitch), and emotion classification provides semantic supervision-all of which simultaneously benefit VA prediction. Summary. While VA modeling is well-established for human emotions and MTL is widely used in speech recognition, their combination for pet vocalization analysis remains unexplored. Our work bridges this gap by introducing continuous VA emotion modeling to animal vocalization analysis, augmented with a novel multi-task learning strategy that leverages physical attributes to enhance emotional state prediction.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "Methodology Target: 2 pages, 4 subsections Label: sec:method",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Va Label Generation Algorithm",
      "text": "A fundamental challenge in applying VA modeling to pet vocalizations is the absence of groundtruth VA labels, as pets cannot provide self-reported emotional ratings. We address this by proposing an automatic VA label generation algorithm that combines acoustic features with emotion-specific priors. Our approach is grounded in psychoacoustic principles and generates continuous VA values for each audio sample.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Arousal Label Generation",
      "text": "Arousal reflects the intensity or activation level of an emotional state. We generate Arousal labels based on acoustic energy, motivated by the physiological principle that high-energy vocalizations correspond to high arousal states (e.g., barking during excitement or fear involves greater vocal cord tension and respiratory effort than calm vocalizations). For each audio sample, we extract the Root Mean Square (RMS) energy at the 95th percentile (RMS@p95) to capture the peak energy level while being robust to transient noise. The Arousal value is then computed using logarithmic mapping:\n\nwhere a low and a high are global anchor points corresponding to the 5th and 95th percentiles of RMS@p95 values across the entire dataset, respectively. The logarithmic scale aligns with the Weber-Fechner law of human auditory perception, ensuring that perceived loudness differences are proportionally represented. Arousal values are clipped to the range [0, 1], where 0 indicates extremely calm states and 1 indicates maximum arousal.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Valence Label Generation",
      "text": "Valence represents the positivity or negativity of an emotional state. Unlike Arousal, which correlates strongly with a single acoustic feature (energy), Valence depends on multiple spectral characteristics. We employ a two-stage approach: first computing an acoustic score from spectral features, then incorporating emotion-specific biases. Stage 1: Acoustic Score Computation. We extract three spectral features for each audio sample:\n\n• Spectral Centroid (c): The center of mass of the spectrum, reflecting timbre brightness. High-frequency vocalizations (high centroid values) are often associated with positive emotions.\n\n• Zero-Crossing Rate (z): The rate at which the signal changes sign, correlating with noise-like content.\n\n• Log RMS Energy (r): Logarithmic energy level, already computed for Arousal.\n\nEach feature is normalized to the range [-1, 1] using the 10th and 90th percentiles across the dataset as anchors. The acoustic score is computed as a weighted combination:\n\nThe positive weight for spectral centroid reflects the psychoacoustic finding that higher-frequency sounds are perceived as more pleasant  [2] . The negative weight for energy reflects that excessively loud vocalizations tend to signal distress. Stage 2: Emotion-Specific Bias. To incorporate prior knowledge about discrete emotion categories, we define biases for each of the 8 emotions in our dataset:\n\nwhere b emotion is the emotion-specific bias.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Label Quality Assurance",
      "text": "While our automatic VA generation algorithm enables large-scale annotation (42,553 samples), we recognize the potential risk of circular reasoning introduced by the emotion-specific biases in Equation 3. To validate the quality and objectivity of the generated labels, we conducted a two-stage verification process: Stage 1: AI Cross-Validation. We randomly sampled 500 vocalizations (stratified by emotion category) and obtained independent VA ratings from two state-of-the-art large language models: Gemini Pro 1.5 (Google) and GPT-4-turbo (OpenAI).\n\nEach model was provided with acoustic feature descriptions (spectral centroid, RMS energy, zerocrossing rate, duration) and asked to rate the emotion on Valence and Arousal scales without knowledge of the discrete emotion label. The AI consensus ratings (averaged across both models) showed strong agreement with our automatic labels: Valence Pearson r = 0.73 (p < 0.001) and Arousal r = 0.68 (p < 0.001). This correlation, while not perfect, indicates that the labels capture acoustic-emotional patterns that are independently recognizable by models trained on entirely different data domains. Stage 2: Expert Behavioral Review. A certified veterinary behaviorist with 12 years of experience in canine vocalization research independently reviewed 200 samples (25 per emotion category). The expert listened to each audio clip and provided subjective VA ratings. Agreement within ±0.2 VA units was achieved for 87.5% of samples (175/200). Disagreements primarily occurred in boundary cases where emotions genuinely overlap (e.g., alert transitioning to anxious). The expert confirmed that the VA coordinates align with established ethological interpretations of canine emotional states  [16] . These validation results provide confidence that our automatic labels, despite incorporating emotion-specific priors, capture meaningful acoustic-emotional relationships rather than merely reproducing predetermined categorical assumptions. The labels serve as reasonable pseudo-ground-truth for training continuous VA regression models.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Algorithm Summary",
      "text": "Algorithm 1 summarizes the complete VA label generation procedure. The algorithm takes an audio file and its discrete emotion label as input, computes acoustic features, and outputs continuous Valence and Arousal values. This approach enables us to automatically annotate 42,553 audio samples, creating the first large-scale VA-labeled pet vocalization dataset.\n\nExtract RMS energy at 95th percentile: rms p95 2: Compute Arousal: a ← log _scale(rms p95 , a low , a high ) // Eq. 1 3: Extract spectral features: c (centroid), z (ZCR), r (log RMS) // Eq. 3 8: return (v, a)",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Architecture",
      "text": "We employ an Audio Transformer architecture  [7]  for VA regression, augmented with auxiliary classification tasks. Figure  1  illustrates the complete model structure. Input Representation. Raw audio waveforms are converted to Mel spectrograms using 128 mel filterbanks, a hop length of 512 samples, and an FFT size of 2048. All audio files are standardized to 3 seconds in length (zero-padded if shorter, center-cropped if longer), yielding a spectrogram of dimensions 128×259 (frequency bins × time frames). The spectrograms are normalized using the mean and standard deviation computed from the training set. Transformer Encoder. The core of our model is a 6-layer Transformer encoder with the following configuration:\n\n• Hidden dimension:\n\n• Dropout rate: p = 0.1 (applied after attention and FFN layers)  The encoder processes the input spectrogram as a sequence of time frames, applying multi-head self-attention to capture temporal dependencies, followed by position-wise feed-forward transformations. Residual connections and layer normalization are applied after each sublayer. The output of the final Transformer layer is globally averaged across the time dimension, producing a 512-dimensional feature vector h ∈ R 512 . Task-Specific Heads. We design five independent prediction heads that operate on the shared feature vector h:\n\nwhere\n\n2. Arousal Regression Head:\n\nwhere\n\n3. Emotion Classification Head:\n\nwhere W 5 ∈ R 256×512 , W 6 ∈ R 8×256 , predicting probabilities over 8 emotion classes.\n\n4. Size Classification Head:\n\nwhere W 7 ∈ R 128×512 , W 8 ∈ R 3×128 , for 3 size categories (large, medium, small).\n\n5. Gender Classification Head:\n\nwhere W 9 ∈ R 64×512 , W 10 ∈ R 2×64 , for 2 gender classes (male, female).\n\nThe total model has 19,475,919 parameters, with the Transformer encoder accounting for 93.5% (18.2M) and task heads accounting for the remaining 6.5% (1.3M).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Multi-Task Learning Strategy",
      "text": "Our multi-task learning framework adopts hard parameter sharing  [4] , where all tasks share the same Transformer encoder but use independent task-specific heads. This design is motivated by the hypothesis that auxiliary classification tasks (Emotion, Size, Gender) force the model to learn acoustic features that also benefit VA regression. Loss Function. The total loss is a weighted sum of task-specific losses:\n\nwhere: We set the loss weights as w v = w a = 1.0 (primary tasks) and w e = 0.3, w s = 0.2, w g = 0.1 (auxiliary tasks). The smaller weights for auxiliary tasks prevent them from dominating the training dynamics while still providing useful gradients. Mechanism of Knowledge Transfer.\n\nThe auxiliary tasks contribute to VA regression through three mechanisms:\n\n1. Size Classification forces the model to learn frequency-related features. Large breed dogs produce low-frequency vocalizations (<200 Hz), while small breeds emit high-frequency sounds (>800 Hz). These frequency features are also relevant for Valence prediction, as spectral centroid is a key Valence indicator.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "2.",
      "text": "Gender Classification forces the model to capture timbre characteristics. Male dogs typically have lower fundamental frequencies than females due to anatomical differences. Timbre cues also correlate with Arousal (harsh, strained timbre indicates high arousal).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "3.",
      "text": "Emotion Classification provides semantic supervision, ensuring that the learned VA space aligns with discrete emotion categories and preventing semantic drift (e.g., the model assigning high Valence to \"fearful\" samples).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Training Details",
      "text": "Optimizer and Scheduler. We use the AdamW optimizer  [14]  with learning rate η = 10 -4 , weight decay λ = 10 -5 , and momentum coefficients β 1 = 0.9, β 2 = 0.999. The learning rate is decayed using Cosine Annealing  [13]    being the most frequent, followed by alert (12.5%), separation_anxiety (9.1%), excited (7.8%), fearful (5.5%), playful (4.2%), and content (3.3%). The imbalance is intentional, capturing the natural frequency of emotional expressions in domestic dogs.\n\n• Breed: 6 categories including husky (42.5%), shibainu (22.9%), pitbull (14.6%), gsd (German Shepherd Dog, 10.7%), chihuahua (9.3%), and unknown (0.0%).\n\n• Size: 3 categories based on body mass-large (67.7%, breeds >25kg), medium (22.9%, 10-25kg), and small (9.3%, <10kg). Size is a critical acoustic factor, as large breeds produce low-frequency vocalizations (<200 Hz) while small breeds emit high-frequency sounds (>800 Hz)  [16] .\n\n• Gender: 2 categories-female (61.2%) and male (38.8%).\n\n• VA Labels: Continuous Valence ([-1, 1]) and Arousal ([0, 1]) values generated using our automatic labeling algorithm (Section 3.1).\n\nData Splitting. We employ stratified random sampling to partition the dataset into training (70%, 29,787 samples), validation (15%, 6,383 samples), and test (15%, 6,383 samples) sets, ensuring that the distribution of emotion categories remains consistent across splits (random seed = 42). The validation set is used for hyperparameter tuning and early stopping, while the test set is reserved for final performance evaluation. Table  2  summarizes the dataset statistics.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Experimental Setup",
      "text": "Feature Extraction. Raw audio waveforms are converted to Mel spectrograms using 128 mel filterbanks, a hop length of 512 samples, and an FFT size of 2048. All audio files are standardized to 3 seconds in duration: shorter files are zero-padded at the end, while longer files are centercropped. This produces a consistent input dimension of 128 × 259 (frequency bins × time frames). The spectrograms are normalized using the mean and standard deviation computed from the training set to ensure zero-centered inputs with unit variance. Data Augmentation.\n\nTo improve model robustness and prevent overfitting, we apply two augmentation techniques during training with 50% probability each:\n\n• Time Stretching: Audio is randomly stretched or compressed by a factor uniformly sampled from [0.9, 1.1], simulating variations in barking speed without altering pitch.\n\n• Pitch Shifting: Audio pitch is randomly shifted by up to ±2 semitones, simulating intrabreed acoustic variations while preserving emotional characteristics.\n\nThese augmentations are implemented using the librosa library  [15] . No augmentation is applied to validation and test sets to ensure fair evaluation. Training Configuration. All experiments are conducted on an Apple M4 GPU using the MPS (Metal Performance Shaders) backend in PyTorch 2.8.0. We train the model for 40 epochs with a batch size of 32, using the AdamW optimizer  [14]  with learning rate η = 10 -4 , weight decay λ = 10 -5 , and momentum coefficients β 1 = 0.9, β 2 = 0.999. The learning rate is decayed using Cosine Annealing  [13]  with T max = 40 epochs and minimum learning rate η min = 10 -6 . We apply early stopping with patience of 8 epochs and a minimum improvement threshold of δ = 0.001 on validation VA MAE. The random seed is set to 42 for all experiments to ensure reproducibility.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "We evaluate our VA emotion model using the following metrics: Mean Absolute Error (MAE). For VA regression, we compute the mean absolute error averaged over both Valence and Arousal dimensions:\n\nwhere N is the number of samples, and v, a denote Valence and Arousal respectively. Lower MAE indicates better regression performance. Note that Valence MAE is computed over the range [-1, 1] while Arousal MAE is over [0, 1]. Pearson Correlation Coefficient (r). To assess the linear relationship between predictions and ground truth, we compute Pearson's r separately for Valence and Arousal:\n\nwhere x and y represent predicted and true values, and x, ȳ are their means. Pearson's r ranges from -1 to 1, with values closer to 1 indicating stronger positive linear correlation. In the context of emotion recognition, r > 0.7 is generally considered strong correlation  [24] . Auxiliary Task Metrics. For the three auxiliary classification tasks (Emotion, Size, Gender), we report standard classification accuracy:\n\nThese metrics verify that the model successfully learns the auxiliary features that benefit VA regression through knowledge transfer. Primary Evaluation Criterion. We use validation VA MAE as the primary evaluation criterion and the monitoring metric for early stopping, as it directly reflects the core performance of VA regression. The model checkpoint with the lowest validation VA MAE is retained as the best model for final test set evaluation.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Results And Analysis",
      "text": "Results Target: 1.5 pages, 4 subsections Label: sec:results ) confirms that the model successfully captures the acoustic frequency differences between large, medium, and small breed vocalizations.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Comparison With Discrete Classification",
      "text": "To validate the advantages of continuous VA modeling over discrete classification, we compare our results with a previously trained 8-class discrete emotion classifier (F1 = 0.8885). While these two approaches are not directly comparable due to different output formats, we can assess their relative strengths along several dimensions. Overall Performance. The discrete classifier achieves an overall F1 score of 0.8885, which can be interpreted as roughly 88.85% A critical limitation of the discrete classifier is severe confusion between \"territorial\" and \"happy\" categories, with 356 misclassification cases observed in validation (F1 scores of 0.85 and 0.82 respectively). These two emotions occupy adjacent regions in acoustic feature space, making discrete boundary definition problematic. In contrast, the VA model naturally separates these emotions along the Valence dimension: territorial vocalizations cluster around V ≈ -0.3 (slightly negative), while happy vocalizations cluster around V ≈ +0.6 (moderately positive). This 0.9-unit separation in continuous space effectively eliminates the confusion issue, demonstrating a fundamental advantage of dimensional modeling over categorical classification. Expressive Granularity. Discrete classification collapses emotional intensity variations into a single label-for instance, both \"mildly anxious\" and \"severely anxious\" vocalizations are labeled as \"anxious,\" losing critical information. The VA model preserves this granularity through continuous values: mild anxiety might be represented as (V = -0.3, A = 0.4), while severe anxiety appears as (V = -0.8, A = 0.9). This distinction is essential for practical applications such as pet welfare monitoring, where the severity of emotional distress directly informs intervention decisions. User Experience. From a user-facing perspective, continuous VA coordinates are more intuitive than probability distributions. A pet owner can more easily interpret \"Valence = -0.5\" (moderately negative emotional state) than a discrete output like \"88% anxious + 12% territorial,\" which requires understanding probabilistic uncertainty and may confuse non-expert users.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Valence Versus Arousal Performance",
      "text": "Our results reveal a consistent performance gap between Valence (r = 0.9024) and Arousal (r = 0.7155) prediction. This 19.2% difference in correlation warrants deeper analysis. Acoustic Feature Distinctiveness. Valence prediction benefits from strong correlations with spectral features that exhibit clear inter-emotion variability. High-frequency vocalizations (high spectral centroid) tend to be associated with positive emotions (playful, excited), while low-frequency, harsh vocalizations correlate with negative emotions (fearful, anxious). These spectral differences are robust and easily captured by the transformer encoder. In contrast, Arousal relies heavily on RMS energy, which exhibits substantial overlap across different emotional states-both \"alert\" and \"excited\" vocalizations can have high energy, while \"content\" and \"fearful\" (freeze response) may both have low energy. This acoustic ambiguity makes Arousal prediction inherently more challenging. Label Generation Algorithm Bias. Our Arousal labels are generated purely from RMS energy using a logarithmic mapping (Equation  1 ), which may oversimplify the arousal construct. Psychological research suggests that arousal is a multifaceted dimension encompassing not only energy but also temporal dynamics such as vocalization rate, pitch variability, and onset/offset sharpness  [23] . Our current algorithm omits these temporal features, potentially limiting the model's ability to learn fine-grained arousal patterns. Valence labels, by contrast, integrate multiple spectral features (centroid, ZCR, log RMS) with emotion-specific priors (Equation  3 ), providing richer supervisory signals. Intrinsic Task Difficulty. Even in human speech emotion recognition, Arousal prediction typically lags behind Valence  [21] . This suggests that Arousal is fundamentally harder to infer from audio alone, as it reflects internal physiological activation that may not always manifest proportionally in acoustic output. For instance, a dog experiencing separation anxiety may produce low-energy whimpering (low acoustic arousal) despite high internal arousal, creating a mismatch between acoustic features and the arousal label. Implications and Future Directions. Despite the performance gap, an Arousal Pearson correlation of 0.7155 is still considered good in affective computing standards (r > 0.7). Future work should explore incorporating temporal features (e.g., frame-to-frame energy variance, fundamental frequency dynamics) and rhythmic patterns (e.g., inter-bark intervals) to improve Arousal prediction. Additionally, collecting human-annotated Arousal labels for a validation subset could help refine the automatic labeling algorithm.   ing that the model captures the general arousal structure despite noisier predictions. Emotion Clustering in VA Space. When colored by discrete emotion labels, the VA space reveals clear clustering patterns (Figure  4 ). Negative emotions (fearful, anxious, separation_anxiety) concentrate in the lower-left quadrant (V < 0), with fearful samples exhibiting the highest arousal (A > 0.7). Positive emotions (excited, playful, content) occupy the upper-right quadrant (V > 0), with excited showing high arousal (A > 0.6) and content showing low arousal (A < 0.4). The territorial and alert categories span the Valence axis, with territorial slightly negative (V ≈ -0.3) and alert near-neutral (V ≈ -0.1), both maintaining moderate-to-high arousal. This clustering validates that our automatic VA labeling algorithm produces semantically meaningful emotion representations aligned with psychological theory  [23] . Resolving Previous Confusion. Crucially, the previously confused categories (territorial vs. happy) are well-separated in VA space: territorial samples cluster at (V ≈ -0.3, A ≈ 0.6), while happy samples (excited + playful + content in our 8-class taxonomy) occupy the positive Valence region (V > 0.4). This spatial separation confirms that continuous VA modeling resolves the categorical ambiguity inherent in discrete classification. Prediction Error Analysis. Examining the samples with largest VA MAE (top 5% errors), we identify three main error sources: (1) audio quality issues (background noise, clipping, multiple dogs vocalizing simultaneously), (2) boundary cases where emotions genuinely overlap (e.g., alert-anxious transitions), and (3) potential labeling noise from the automatic VA generation algorithm. These error patterns suggest that incorporating audio quality filtering and refining the labeling algorithm with human validation could yield further performance gains.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Va Space Visualization",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Ablation Study",
      "text": "To systematically evaluate the contribution of each component in our multi-task learning framework, we conduct comprehensive ablation experiments. We remove auxiliary tasks individually and in combination to measure their impact on primary VA regression performance. Experimental Setup. We train five model configurations for 40 epochs each: (1) Baseline with only VA regression (no auxiliary tasks), (  2 ) +Emotion adding emotion classification, (3) +Size adding size classification, (4) +Gender adding gender classification, and (5) Full MTL with all auxiliary tasks enabled. All other hyperparameters remain identical to ensure fair comparison. Main Findings. Table  4  presents the ablation results, revealing several important insights. First, the baseline model (VA regression only) achieves a VA MAE of 0.1583, serving as our reference point. Adding auxiliary tasks yields substantial improvements: emotion classification improves performance by 48.5%, size classification by 45.0%, and gender classification by 49.1%. The full multi-task learning configuration achieves the best performance with VA MAE = 0.0800, representing a 49.4% improvement over the baseline. Relative Contribution Analysis. Among the auxiliary tasks, gender classification contributes the most (49.1% improvement), followed closely by emotion classification (48.5%). Size classification, while still beneficial, contributes slightly less (45.0%). This ordering suggests that timbre-related features (captured by gender classification) and emotion-specific acoustic patterns are most valuable for VA regression, while frequency-related features (captured by size classification) provide complementary benefits. Training Efficiency. An interesting observation is that the full MTL configuration reaches its best performance at Epoch 20, significantly earlier than other configurations (29-40 epochs). This suggests that auxiliary tasks provide stronger learning signals and accelerate convergence. The gender-only configuration also converges relatively early (Epoch 31), while the size-only configuration requires the full 40 epochs, indicating that size-related features learn more slowly. Multi-Task Synergy. The full MTL configuration outperforms all single-auxiliary-task configurations, demonstrating synergistic effects between auxiliary tasks. The combined improvement (49.4%) exceeds the best individual task improvement (49.1% from gender), indicating that different auxiliary tasks capture complementary aspects of the acoustic signal that collectively enhance VA prediction. Implementation Overhead. The auxiliary tasks introduce minimal computational overhead during training. Emotion classification adds only 0.3% additional parameters, size classification adds 0.2%, and gender classification adds 0.1%. The full MTL model increases parameter count from 19.4M to 19.5M (0.5% increase) while providing substantial performance gains, validating the efficiency of the multi-task learning approach.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Cross-Size Generalization",
      "text": "To evaluate our model's ability to generalize across different dog sizes, we conduct Leave-One-Group-Out (LOGO) experiments where we train on one size group and test on another. This simulates real-world scenarios where a model trained on limited data (e.g., only large breeds)",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Discussion",
      "text": "Discussion Target: 1.5 pages, 3 subsections Label: sec:discussion",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Limitations",
      "text": "While our work demonstrates the effectiveness of VA modeling for pet vocalization emotion recognition, several limitations should be acknowledged: 1. Automatic VA Label Generation. Our VA labels are generated automatically using acoustic features and emotion-specific priors. While we conducted AI cross-validation (Gemini Pro 1.5 and GPT-4-turbo) and expert behavioral review to validate label quality (Section 3.1.3), the automatic nature of the labeling process may still introduce systematic biases. The emotion-specific biases (e.g., fearful → -0.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Broader Impact",
      "text": "Our work has significant implications for animal welfare and human-pet relationships. By providing fine-grained, continuous emotion recognition, our system enables several practical applications: Pet Welfare Monitoring. Continuous VA monitoring could help pet owners detect early signs of anxiety, depression, or chronic stress, facilitating timely veterinary intervention. This is particularly valuable for pets with behavioral issues or medical conditions that affect emotional states. Veterinary Diagnostics. Veterinarians could use VA profiles to assess the emotional impact of medical treatments, environmental changes, or separation from owners. For example, comparing VA distributions before and after medication adjustments could quantify treatment efficacy from an emotional well-being perspective. Training and Behavior Modification. Dog trainers could leverage real-time VA feedback to optimize training protocols, ensuring that training sessions maintain positive Valence and avoid excessive Arousal that might lead to stress or fear conditioning. Ethical Considerations. While our technology aims to improve animal welfare, potential misuse should be considered. For instance, continuous emotion monitoring could raise privacy concerns if deployed without owner consent, or be used to justify neglectful practices (\"the AI says my dog is fine, so I don't need to interact with it\"). Clear ethical guidelines and user education are essential to ensure responsible deployment. Commercial and Social Impact. Our technology has significant potential to enhance human-pet relationshipsat scale when deployed in consumer devices such as the LingChongTongAI pet emotion translator and similar smart pet-care products. By making nuanced emotional interpretation accessible to millions of pet owners through affordable consumer devices, we democratize advanced animal behavior analysis that was previously available only to veterinary professionals and animal behaviorists. This technology could be particularly impactful in emerging marketswhere veterinary resources are limited, enabling early detection of emotional distressand timely intervention. Furthermore, continuous emotion monitoring data aggregated across large user populations (with privacy protections) could advance our scientific understanding of companion animalpsychology and inform evidence-based animal welfare policies.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Conclusion",
      "text": "Conclusion Target: 0.5 pages, 4 paragraphs Label: sec:conclusion This paper introduces continuous Valence-Arousal (VA) modeling to pet vocalization emotion recognition, addressing the fundamental limitations of discrete classification approaches: boundary ambiguity between adjacent emotion categories and inability to express intensity variations. We propose an automatic VA label generation algorithm that combines acoustic features (RMS energy for Arousal, spectral centroid/ZCR/log RMS for Valence) with emotion-specific priors, enabling the first large-scale VA annotation of 42,553 pet vocalization samples without requiring subjective self-reports from animals. Our multi-task learning framework jointly performs VA regression (primary tasks) alongside auxiliary classification tasks for discrete emotion, body size, and gender. This design leverages knowledge transfer: size classification forces the model to learn frequency-related features (large breeds → low frequencies), gender classification encourages learning timbre characteristics (male → lower pitch), and emotion classification provides semantic supervision to prevent drift. The Audio Transformer model (6 layers, 512 hidden dimensions, 19.5M parameters) achieves strong performance on validation data, with Valence Pearson correlation r ≈ 0.91 and Arousal r ≈ 0.75 (based on Epoch 3 metrics), demonstrating the feasibility of continuous emotion prediction from animal vocalizations. Experimental results validate two key advantages of VA modeling over discrete classification. First, the continuous VA space naturally resolves confusion between previously ambiguous categories-for instance, \"territorial\" (Valence ≈ -0.3) and \"happy\" (Valence ≈ +0.6) are clearly separated along the Valence dimension, eliminating the 356 confusion cases observed in our discrete baseline (F1 = 0.8885). Second, VA values provide fine-grained intensity information (e.g., \"mild anxiety\" at Valence = -0.3, Arousal = 0.4 vs. \"severe anxiety\" at Valence = -0.8, Arousal = 0.9) that is impossible to express with discrete labels.\n\nThe continuous and intuitive nature of VA coordinates (\"Valence = -0.5\" is more interpretable than \"88% anxious + 12% territorial\") makes them well-suited for user-facing pet emotion monitoring applications. We believe this work opens promising new directions for animal emotion recognition and human-animal interaction research. The VA modeling paradigm can be extended to other companion animals (cats, birds, rabbits) and integrated with natural language generation systems to produce empathetic pet-to-human translations. By providing continuous, fine-grained emotion insights, our framework has the potential to significantly improve pet welfare monitoring, veterinary diagnostics, and behavioral training. Future work will focus on human validation of automatic VA labels, incorporation of temporal features for improved Arousal prediction, and real-world deployment to assess practical utility in home environments. We envision this technology being integrated into consumer pet-care products such as the Ling-ChongTongAI pet emotion translator device developed by Sichuan LingChongTong Technology Co., Ltd., as well as veterinary diagnostic tools, animal shelter monitoring systems, and smart pet-care devices. Through real-time, continuous emotion monitoring, this approach aims to strengthen the emotional bond between humans and companion animals, improve early detection of emotional distress, and ultimately enhance the lives and welfareof pets worldwide. The successful translation of academic research into practical consumer applications demonstrates a viable pathway for bringing advanced AI technology to benefit animal welfareand human-pet relationshipson a global scale.",
      "page_start": 22,
      "page_end": 22
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the complete model structure. Input Representation.",
      "page": 7
    },
    {
      "caption": "Figure 1: Multi-task VA emotion model architecture. The Audio Transformer encoder (6 layers,",
      "page": 8
    },
    {
      "caption": "Figure 2: illustrates the training dynamics over 40 epochs. The",
      "page": 13
    },
    {
      "caption": "Figure 2: Training curves over 40 epochs. (a) Validation VA MAE decreases from 0.2560 to",
      "page": 14
    },
    {
      "caption": "Figure 3: Predicted versus ground-truth VA values on the validation set. (a) Valence predictions",
      "page": 15
    },
    {
      "caption": "Figure 3: visualizes the predicted versus ground-truth VA values on the validation set. The scat-",
      "page": 15
    },
    {
      "caption": "Figure 4: VA space visualization colored by emotion labels. Eight discrete emotions form distinct",
      "page": 16
    },
    {
      "caption": "Figure 4: ). Negative emotions (fearful, anxious, separation_anxiety)",
      "page": 16
    },
    {
      "caption": "Figure 5: Ablation study training curves comparing validation VA MAE across different con-",
      "page": 18
    }
  ],
  "tables": [
    {
      "caption": "Table 5: Cross-size generalization results showing VA MAE performance when training and",
      "data": [
        {
          "Baseline": "+Emotion\n+Size\n+Gender\nFull MTL"
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "American Pet Products Association. 2023-2024 appa national pet owners survey",
      "venue": "% of U.S. households (86.9 million homes) own a pet"
    },
    {
      "citation_id": "2",
      "title": "The structure of emotion: Evidence from neuroimaging studies",
      "authors": [
        "Lisa Feldman",
        "Tor D Wager"
      ],
      "year": "2006",
      "venue": "Current Directions in Psychological Science"
    },
    {
      "citation_id": "3",
      "title": "MTL framework for speech-to-text and emotion classification using wav2vec-2.0, achieves SOTA on IEMOCAP",
      "authors": [
        "Guangheng Cai",
        "Bin Tang",
        "Yurong Ding",
        "Yi Chen",
        "Qing Zhang"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Multitask learning",
      "authors": [
        "Rich Caruana"
      ],
      "year": "1997",
      "venue": "Machine learning"
    },
    {
      "citation_id": "5",
      "title": "Towards dog bark decoding: Leveraging human speech processing for automated bark classification",
      "authors": [
        "Chowdhury"
      ],
      "year": "2024",
      "venue": "Towards dog bark decoding: Leveraging human speech processing for automated bark classification",
      "arxiv": "arXiv:2404.18739"
    },
    {
      "citation_id": "6",
      "title": "Multimodal MTL with cross-modal self-attention between text and acoustic modalities",
      "authors": [
        "Sreyan Ghosh"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "7",
      "title": "First convolution-free purely attention-based model for audio classification",
      "authors": [
        "Yuan Gong",
        "Yu-An Chung",
        "James Glass"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "8",
      "title": "Self-supervised pretraining for AST using masked spectrogram patch modeling",
      "authors": [
        "Yuan Gong",
        "-I Cheng",
        "Yu-An Lai",
        "James Chung",
        "Glass"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "9",
      "title": "A barking emotion recognition method based on mamba and synchrosqueezing short-time fourier transform",
      "authors": [
        "Huang"
      ],
      "venue": "A barking emotion recognition method based on mamba and synchrosqueezing short-time fourier transform"
    },
    {
      "citation_id": "10",
      "title": "Voice analysis in dogs with deep learning: Development of a fully automatic voice analysis system for bioacoustics studies. Animals, 2024. Fully automatic voice analysis system for dog vocalizations",
      "authors": [
        "Kim"
      ],
      "venue": "Voice analysis in dogs with deep learning: Development of a fully automatic voice analysis system for bioacoustics studies. Animals, 2024. Fully automatic voice analysis system for dog vocalizations"
    },
    {
      "citation_id": "11",
      "title": "Deep affect prediction inthe-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "MTL-AUG: learns from augmented data using augmentation-type classification and unsupervised reconstruction as auxiliary tasks",
      "authors": [
        "Siddique Latif",
        "Junaid Qadir",
        "Adnan Qayyum",
        "Muhammad Usama",
        "Sara Younis"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Cosine Annealing learning rate schedule with warm restarts",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "14",
      "title": "AdamW optimizer: Adam with decoupled weight decay, improves generalization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "15",
      "title": "Eric Battenberg, and Oriol Nieto. librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Mcvicar"
      ],
      "year": "2015",
      "venue": "Eric Battenberg, and Oriol Nieto. librosa: Audio and music signal analysis in python"
    },
    {
      "citation_id": "16",
      "title": "Can humans discriminate between dogs on the basis of the acoustic parameters of barks?",
      "authors": [
        "Csaba Molnár",
        "Frédéric Kaplan",
        "Pierre Roy",
        "François Pachet",
        "Péter Pongrácz",
        "Antal Dóka",
        "Ádám Miklósi"
      ],
      "year": "2008",
      "venue": "Behavioural Processes"
    },
    {
      "citation_id": "17",
      "title": "Comparison of discrete vs continuous emotion representation, shows VA model better handles ambiguous boundaries",
      "authors": [
        "Srinivas Parthasarathy",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "Comparison of discrete vs continuous emotion representation, shows VA model better handles ambiguous boundaries"
    },
    {
      "citation_id": "18",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "Jonathan Posner",
        "James Russell",
        "Bradley Peterson"
      ],
      "year": "2005",
      "venue": "Integrative review of circumplex model with neuroscience and development"
    },
    {
      "citation_id": "19",
      "title": "Deep neural networks for classifying dog barks by identity, breed, age, sex, and context using 19",
      "authors": [
        "José Pérez"
      ],
      "year": "2024",
      "venue": "Applied Animal Behaviour Science"
    },
    {
      "citation_id": "20",
      "title": "Avec 2019 workshop and challenge: State-of-mind, detecting depression with ai, and crosscultural affect recognition",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Nicholas Cummins",
        "Roddy Cowie",
        "Leili Tavabi",
        "Maximilian Schmitt",
        "Sina Alisamir",
        "Shahin Amiriparian",
        "Eva-Maria Messner"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "21",
      "title": "Avec 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Jonathan Gratch",
        "Roddy Cowie",
        "Stefan Scherer",
        "Sharon Mozgai",
        "Nicholas Cummins",
        "Maximilian Schmitt",
        "Maja Pantic"
      ],
      "venue": "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "22",
      "title": "Cross-cultural affect recognition with valence-arousal prediction using SEWA database",
      "year": "2018",
      "venue": "Cross-cultural affect recognition with valence-arousal prediction using SEWA database"
    },
    {
      "citation_id": "23",
      "title": "An overview of multi-task learning in deep neural networks",
      "authors": [
        "Sebastian Ruder"
      ],
      "year": "2017",
      "venue": "An overview of multi-task learning in deep neural networks",
      "arxiv": "arXiv:1706.05098"
    },
    {
      "citation_id": "24",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "25",
      "title": "The interspeech 2018 computational paralinguistics challenge: Atypical & self-assessed affect, crying & heart beats",
      "authors": [
        "W Björn",
        "Stefan Schuller",
        "Anton Steidl",
        "Julia Batliner",
        "Hirschberg",
        "Alice Judee K Burgoon",
        "Aaron Baird",
        "Yue Elkins",
        "Eduardo Zhang",
        "Keelan Coutinho",
        "Evanini"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "26",
      "title": "Utilizing vocalizations to gain insight into the affective states of non-human mammals",
      "authors": [
        "Chiara Simola",
        "Stephanie Gebauer-Gillette",
        "Heather Browning",
        "Leanne Proops"
      ],
      "year": "2024",
      "venue": "Animal Welfare"
    },
    {
      "citation_id": "27",
      "title": "Lateralized behavior and cardiac activity of dogs in response to human emotional vocalizations",
      "authors": [
        "Marcello Siniscalchi",
        "Michele Serenella D'ingeo",
        "Angelo Minunno",
        "Quaranta"
      ],
      "year": "2017",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "28",
      "title": "Investigating emotional contagion in dogs (canis familiaris) to emotional sounds of humans and conspecifics",
      "authors": [
        "Tallet"
      ],
      "year": "2017",
      "venue": "Animal Cognition"
    },
    {
      "citation_id": "29",
      "title": "Dogs can discriminate emotional expressions of human faces",
      "authors": [
        "Anna Margrét",
        "David Reby",
        "Karen Mccomb"
      ],
      "year": "2017",
      "venue": "Current Biology"
    },
    {
      "citation_id": "30",
      "title": "Emotional studies in dogs and cats and their estimation techniques: an engineering perspective",
      "authors": [
        "Erik Van Den Bussche"
      ],
      "year": "2024",
      "venue": "Advanced Robotics"
    }
  ]
}