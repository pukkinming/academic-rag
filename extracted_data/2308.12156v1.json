{
  "paper_id": "2308.12156v1",
  "title": "Multimodal Latent Emotion Recognition From Micro-Expression And Physiological Signals",
  "published": "2023-08-23T14:17:44Z",
  "authors": [
    "Liangfei Zhang",
    "Yifei Qian",
    "Ognjen Arandjelovic",
    "Anthony Zhu"
  ],
  "keywords": [
    "Emotion recognition",
    "multi-modal learning",
    "micro-expression recognition",
    "physiological signal analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper discusses the benefits of incorporating multimodal data for improving latent emotion recognition accuracy, focusing on micro-expression (ME) and physiological signals (PS). The proposed approach presents a novel multimodal learning framework that combines ME and PS, including a 1D separable and mixable depthwise inception network, a standardised normal distribution weighted feature fusion method, and depth/physiology guided attention modules for multimodal learning. Experimental results show that the proposed approach outperforms the benchmark method, with the weighted fusion method and guided attention modules both contributing to enhanced performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotional states have a significant impact on physical and psychological well-being, with recognition of emotions being crucial for effective communication and understanding of individuals' emotional states and mental well-being. The complex interplay between physiology and psychology in emotional responses has led to interdisciplinary research into accurate and rapid emotion recognition, which is increasingly important in multimedia and human-computer interaction. Real-time emotion recognition has potential applications in virtual and augmented reality, healthcare, education, and marketing. In interpersonal communication, facial expressions are a critical means of conveying emotions, with micro-expressions (MEs) offering valuable insights into an individual's emotional state, including potential deception. MEs were initially discovered by  Haggard and  Isaacs in 1966 while analysing motion picture films of psychotherapy sessions for nonverbal cues between patients and therapists  [1] . Ekman and Friesen subsequently incorporated ME recognition into their deception studies  [2] , and popularised it through the TV show \"Lie To Me\". Recognising MEs enables experts to identify even the most subtle changes in an individual's facial expressions, potentially indicating their latent emotion. While facial expressions are the most reliable and universally accepted way of recognising emotions, vocal cues, body language, and physiological responses can also provide valuable information about a person's emotional state.\n\nEnhancing emotion recognition accuracy entails exploring avenues beyond just improving the machine learning model, considering richer data types can also help achieve better performance. Human experience of the world is often multimodal, referring to how something happens or is experienced through multiple modalities. Incorporating multimodal signals can enable artificial intelligence to learn about the real world better. Relying solely on human physical signals, such as facial expression, speech, gesture, or posture, is not guaranteed as people can control these signals to hide their real emotions, especially during social communication. In contrast, physiological signals (PS), which are in response to the central nervous system (CNS) and peripheral nervous system (PNS) of the human body, can provide reliable information about emotions according to Cannon's theory  [3] . One significant advantage of using PS is that they are largely involuntarily activated and, therefore, difficult to control, which is a similar characteristic to ME. Researchers have attempted to establish standard relationships between emotional changes and various types of PS. This paper explores the benefits of incorporating multimodal data for improving latent emotion recognition accuracy, specifically with micro-expression and physiological signals. The main contributions of the present work are as follows: nolistsep",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "•",
      "text": "We introduce a novel multimodal learning framework that combines ME and PS to enhance latent emotion recognition performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "•",
      "text": "We design a 1D separable and mixable depthwise inception network that effectively extracts features from various PS.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "•",
      "text": "We propose a standardised normal distribution weighted feature fusion method that reconstructs informative maps from different frames of ME video.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "•",
      "text": "We develop a guided attention module is developed that achieves multimodal learning for both microexpression (colour and depth information) and latent emotion recognition (ME and PS).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Micro-Expression Recognition",
      "text": "Micro-expression recognition (MER) which refers to the recognition of emotions expressed in a sequence of faces known to be brief and subtle, is one significant challenge in affective computing to mining human's authentic emotions. In recent years, computer vision technology has been increasingly utilised for automatic MER, which has improved the feasibility of applications involving MEs  [4] . Since the publication of the open spantenous ME databases  [5] ,  [6]  in 2013, the volume of research on automatic MER has been increasing steadily over the years. From the handcrafted computer vision methods in the early years to the deep learning approaches more recently, the main ideas of ME feature extraction could be categorised as primarily pursuing either a spatial strategy or a temporal one.\n\nFacial Region Of Interest (ROI) segmentation is a common technique for spatial feature extraction in ME analysis  [7] ,  [8] ,  [9] , where the face sample is segmented into different regions based on the Facial Action Coding System (FACS)  [10] . These regions can correspond to independent facial muscle complexes and are subjected to individual appearance normalisation. Polikovsky et al. introduced a gradient feature that constructs histograms of local gradient projections across different regions  [11] , and the Local Binary Pattern (LBP) operator describes local appearance in a robust way using the relative brightness of neighbouring pixels  [12] . As for deep learning approaches, the convolution and pooling layers perform spatial feature extraction. Attention blocks have been introduced in recent years to improve the learning of spatial features within neural networks, by generating weight masks for feature maps to focus on significant regions. Graph Convolutional Network (GCN) are another optimisation measure used to capture spatial information, often by using Action Units (AUs) as corresponding graph nodes  [13] . These methods use a priori knowledge to enhance the extracted spatial features.\n\nThe sudden appearance of MEs makes their temporal features important. While some methods use only the apex frame of each ME sample  [14] ,  [15] ,  [16] , most use all frames between the onset and offset, treating all temporal changes equally. Some even use temporal frame interpolation to increase the number of frames  [8] ,  [9] ,  [12] ,  [17] ,  [18] . Many handcrafted feature-based approaches treat raw video data as a 3D spatio-temporal volume, applying the same operator used to extract spatial features to pseudo-images formed by a cut through the 3D volume comprising one spatial dimension and the temporal dimension. LBP-TOP and 3DHOG are examples of this approach. Similar in this regard are optical flow-based features, which combine local spatial and temporal elements  [9] ,  [16] ,  [19] . Instead of using raw appearance imagery, some authors propose using pre-processed data in the form of optic flow matrices to exploit proximal temporal information directly in deep learning methods  [20] ,  [21] ,  [22] ,  [23] . Longer-range temporal patterns are learned by treating video sequences as 3-dimensional matrices or by employing structures like RNN or LSTM  [18] ,  [24] .\n\nIn previous research, colour images or videos have been the primary data samples used due to the availability of ME databases. However, the recent release of 4DME  [25]  and CAS(ME) 3  [26]  has expanded the range of data available for emotion recognition related to micro-expressions. 4DME primarily focuses on 3D micro-expression data, while CAS(ME) 3 not only includes RGB-D micro-expression video clips but also includes physiological signals in one part of the database. This enables multimodal learning with microexpressions for emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "Numerous works in the literature show that multimodal emotion recognition systems outperform unimodal approaches  [27] ,  [28] . Emotions are intricate experiences that involve not only observable physical expressions but also internal feelings, thoughts, and other processes that may not be consciously perceived by the individual. For instance, a person may force a smile during a formal social occasion even if they are experiencing negative emotions. The other approach to detect emotions is through PS such as electroencephalogram (EEG), temperature, electrocardiogram (ECG), electromyogram (EMG), galvanic skin response (GSR), respiration (RSP), etc. ECG is a non-invasive method for measuring the electrical activity of the heart, which can be displayed as a waveform on a computer screen or chart recorder. This helps in identifying whether a heartbeat is normal or abnormal. While EEG signals are commonly used for emotional expression recognition due to their robust sensing of emotions in the brain  [29] ,  [30] ,  [31] , recent studies have explored the use of ECG-based recognition  [32] ,  [33] , though the number of such studies is limited. The high dimensionality of EEG signals makes it difficult to identify effective features for emotional expression recognition, and thus some techniques have been proposed to fuse several PS for emotion detection.\n\nThe fusion module is the most crucial aspect of multimodal learning. The techniques include early fusion, which involves combining extracted features from signals before sending them to the classifier, and late fusion, which involves taking the final result by voting the results produced by several classifiers. The most straightforward approach in early fusion is to concatenate the feature vectors from all modalities, also known as plain early fusion. In the work by Verma and Tiwary, plain early fusion was employed to fuse energy-based features extracted from 32channel EEG signals  [34] . Another way to encode feature dependencies is to use probabilistic inference models like Hidden Markov Models and Bayesian Networks (BNs), for example by building a BN to fuse features from both EEG and ECG signals  [35] . Late fusion, on the other hand, uses multiple classifiers that can be trained independently, and the final decision is made by combining the outputs of each classifier. A framework for emotion recognition based on the weighted fusion of basic classifiers was proposed  [36] , where three support vector machines using different features were developed and their results were combined using weighted fusion based on each classifier's confidence estimation for each class.\n\nSeveral studies have attempted to combine facial expressions with physiological data to create accurate emotion recognition systems  [37] ,  [38] . Both studies used late fusion techniques, such as voting and decision tree, to combine the decisions made from facial expressions and PS, though Cimtay et al. use early fusion for different signals. However, people may conceal their true emotions behind fake facial expressions, whereas MEs can reveal their genuine emotions, as can PS. Therefore, combining MEs and PS through multimodal learning could be a better strategy for authentic emotion recognition. Li et al. attempted to use voice, electrodermal activity (EDA) and depth information to assist MER. They converted the voice and EDA signals into 2D greyscale input channels and trained them with colour and depth information from the apex frame  [26] . However, their results of combining EDA or speech signals were not satisfactory due to not addressing the noise in the signals or designing a specific network for PS. Despite this, the database they provided is a valuable resource for researchers to optimize multimodal emotion recognition processing with MEs.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "Colour images are a crucial and widely used source for computer vision tasks  [39] ,  [40] ,  [41] , providing valuable information for deep learning models to analyse and interpret visual data. In addition, ME has been extensively studied and recognised as a valuable source for authentic emotion recognition, as discussed in Section 2. Therefore, in our proposed framework, we consider colour images from ME video clips as the primary source, with depth information used to guide the spatial features from each frame. Features are extracted from colour images and depth maps separately using backbone networks. To fuse the spatial features extracted from each frame, we designed a standard normal distribution guided fusion method that pays more attention to the middle, where facial movements usually reach their apex, and less attention to the ends. Apart from ME, PS are used in another branch of the proposed framework to recognise latent emotions by our designed 1D separable and mixable depthwise inception network and enhance spatiotemporal features extracted from the ME sample. By analysing both ME and PS, the network can gain a deeper understanding of the subject's emotional state and achieve more accurate latent emotion recognition results. Figure  1  shows the proposed framework.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "1D Separable & Mixable Depthwise Inception Cnn",
      "text": "The network is designed to extract PS features in our framework, whose structure can be seen as Figure  2 . The depthwise structure contains separate convolutions for each group of channels, allowing for more precise feature extraction and capturing spatial correlations. This enables the network to learn more complex and diverse representations of the input data. The Inception block uses a combination of convolutional filters of different sizes within a single layer. This allows the network to capture features at multiple scales without the need for multiple layers, which can be computationally expensive. The Inception block can be seen as an ensemble of smaller networks with different filter sizes, which provides a form of regularisation that helps prevent overfitting and improves generalisation performance.\n\nIn order to effectively extract features from multiple input channels, a method is proposed that involves the extraction of features from each channel individually, followed by their combination for further learning. To achieve this, a depthwise convolutional layer is initially employed to enhance the features from each input channel. Subsequently, a depthwise inception block with three groups is designed to extract features from each physiological source. The design of the depthwise inception block is illustrated in Figure  3 , and comprises four branches of depthwise convolutional layers with varying kernel sizes to extract features from different scales. The resulting features from each branch are then concatenated together and fed into the next depthwise inception block. Since the number of output channels in each branch of the inception block is different, the features from different sources are mixed together to form a new group of channels for further learning. Finally, the mixed features are  fed into the last two blocks as a whole group to extract the final features from the physiological sources.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Standardised Normal Distribution Weighted Feature Fusion",
      "text": "The sequence of extracted features is-mapped to the ME feature with a standard normal distribution function. Note that the standard normal distribution is the special case of the normal distribution with mean µ = 0 and variance σ 2 = 1. A technicality is that since the frames are discrete, the function is slightly altered mapping from a set of extracted features of all the frames to a set of values within the range (0, 1) representing the weight of features from the set.\n\nIt is generally believed that, during the instance of ME, the most prominent facial movement is roughly in the middle of the timeframe, namely, the apex frame of a ME sample is roughly in the middle of the clip. In addition, the frames closer to the apex frame contains more valuable features than those more distant from the apex frame. In contrast to extracting spatial features only in the apex frame, we fuse all the features that are laid in several adjacent frames, i.e. the whole duration. Instead of a uniformly weighted function where each feature is the same weight, in our approach, features extracted from frames which are closer to the middle of the clip are considered more representative and valuable and thus are mapped to higher weight. This is in line with the proposition stated above. The function is as follows:\n\n(1) where F denotes the number of frames in one ME sample. Note that roughly 99.7% of the probability density for a standard normal distribution lies within 3 standard deviations of the mean, and thus lies within the range (-3, 3). Only the weights within 3 standard deviations are considered and the remaining 0.3% is negligible.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "The Depth/Physiology Guided Attention Module",
      "text": "The module is designed for feature fusion of both colour and depth information for each frame of micro-expression, as well as the final fusion of ME and PS features. For attention modules, formally we have a query Q, a key K, and a value V to calculate attention. The depth and PS features could be considered as the input Q to guide attention learning. At the beginning of the module, the main features are copied as sources for both inputs K and V . After fully-connected layers, the scaled dot-product attention mechanism, denoted as SDP below, is run through several times in parallel. The scaled dot-product attention is an attention mechanism where the dot products are scaled down by\n\nd is the dimension of the queries and keys, and softmax denotes the softmax function. The dot product results of the attention mechanism are divided by √ d to maintain a variance of 1. The independent attention outputs are then concatenated and linearly transformed to the expected dimension. The multi-head attention mechanism is defined as follows:\n\nwhere W represents the learnable parameter matrices. The multi-head attention mechanism allows for different parts of the sequence to be attended to differently, such as longerterm dependencies versus shorter-term dependencies.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiments And Evaluation",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Preparation And Experiment Setting",
      "text": "We use Part C of CAS(ME) 3 corpus  [26]  developed to address the challenges of ME elicitation, collection, and annotation. CAS(ME) 3 is composed of three parts. Part A and B contain labelled and unlabelled long videos recorded in the same environment and labelled by the same labellers. CAS(ME) 3 uniquely introduces multimodality to ME analysis in Part C, which is a third-generation multimodal spontaneous ME database that goes beyond just RGB images and includes depth information, voice, and PS. Part C used a third-generation of ME eliciting paradigm, mock crime, with higher ecological validity. Participants were asked to steal a small amount of money from an envelope and were subsequently questioned about the theft.\n\nThe scenario was designed to create a stressful situation that would elicit spontaneous MEs associated with guilt or deception. Part C contains 166 MEs from 31 subjects and makes it possible to enrich multimodal ME analysis with PS, including electrodermal activity (EDA), heart rate/fingertip pulse -electrocardiogram (ECG), respiration (RSP), and pulse photoplethysmography (PPG). The colour and depth frames in Part C are captured at a frame rate of 30 fps. Due to the definition of the happening time of a micro-expression, we selected only the samples with less than 15 frames (500ms) from the database. We cropped the facial region based on the landmarks detected from the onset colour image, and this cropping was applied to all subsequent colour and depth images. To process the data, we utilised pretrained VGG-Face  [42]  and VGG-16  [43]  networks as backbone networks for colour and depth, respectively.\n\nAs for the PS branch, we utilised EDA, ECG, and PPG signals as the three-channel input. To process the source signal data, we employed wavelet denoising on the segmented signal clips. This method is highly effective in denoising 1D signals due to its ability to capture both local and global features of the signal accurately, whilst maintaining a good balance between time and frequency localisation. Daubechies wavelets are orthogonal and form a complete basis set, allowing the signal to be decomposed into its wavelet coefficients, which can then be thresholded to remove noise, as shown in Figure  5 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Results",
      "text": "The traditional evaluation approach for micro-expression recognition involves leave-one-subject-out (LOSO) crossvalidation, where a single subject's data is withheld and used as a validation data set, while all remaining subjects' data is used for training. The overall performance of a method is then assessed by aggregating the results of all different possible iterations of the process, i.e. of all subjects being withheld in turn. Li et al. also utilised LOSO in their experiments  [26] . To ensure fair comparison and more accurate evaluation, we applied the LOSO approach in our experiments as well. Accuracy, unweighted F1-score (UF1) and unweighted average recall (UAR), averaging the perclass recall and F1-score respectively, are used as metrics during evaluation.\n\nOur study aimed to investigate multimodal latent emotion recognition, and the main results can be seen in Table 1. To confirm the effectiveness of our proposed network structure and each designed module, we conducted ablation experiments. Table  2  presents the results related to the standardized normal distribution guided spatial feature fusion, while Table  3  displays the results of the depth/physiology guided attention module.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Analysis And Discussion",
      "text": "All results of our experiments will be discussed in this section. We start by investigating the impact of multimodal learning on latent emotion recognition performance. Then, we discuss the effectiveness of each design inside the proposed framework.   The results presented in Table  1  demonstrate a significant improvement in performance compared to the benchmark results of Li et al. who used RGB and depth information from the apex frame only as 4-channel input for AlexNet. While producing worse performance than the proposed approach their work demonstrated the value of depth information. In contrast to Li et al., herein we used all frames from a video clip and a standard normal distribution feature fusion module to merge the features extracted from all frames. We note that although the use of depth information facilitates the learning of more expressive features, it may introduce noise, which is particularly problematic in microexpression analysis wherein the signal corrupted by noise is weak; therefore, one of the potential avenues for further research in colour and depth micro-expression recognition could be finding a better approach to denoise the depth information. Regarding physiological signals, Li et al. used them as greyscale 2D input channels to the same backbone, without addressing their noise content or designing a specialized network to process them. In contrast, the proposed approach employed the Daubechies wavelet for denoising and introduced a 1D separable and mixable depthwise inception CNN for feature extraction. Our results suggest that this network structure contributes to the improved performance in recognising latent emotions.\n\nThe performance of the proposed standardised normal distribution weighted fusion method is compared with that of the uniform distributed fusion method in Table  2 . The results demonstrate that the proposed method can fuse the features extracted from each frame of micro-expressions more effectively than simply adding them together. The weighted fusion method assigns different weights to different features based on their importance, allowing more important features to have a greater influence on the overall learning process. This emphasises the significance of each feature's contribution to the final result and enhances the performance of the model in recognising micro-expressions. Furthermore, a comparison study was conducted to evaluate the impact of depth/physiology guided attention modules on the performance of the proposed model, a comparison study was conducted. We used concatenation as the baseline fusion method for multimodal learning and trained and tested four different configurations of the model. The results of the study, presented in Table  3 , revealed that the depth-guided attention module outperformed concatenation in incorporating colour and depth information. Additionally, the physiology-guided attention module used for emotion recognition demonstrated significantly better results, indicating that these guided attention modules are capable of effectively fusing extracted features from multiple modalities to learn more beneficial mixed features and contribute to the improved performance of the proposed model in latent emotion recognition.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "Emotional states have a significant impact on physical and psychological well-being, and the recognition of emotions is essential for effective communication and understanding of an individual's emotional and mental state. However, relying solely on facial expressions is not sufficient as people can control these signals to hide their real emotions, especially during social communication. Therefore, this paper proposes a multimodal learning framework that combines micro-expressions and physiological signals to enhance latent emotion recognition performance. The proposed approach denoises the signals and uses a 1D separable and mixable depthwise inception CNN for physiological feature extraction. Furthermore, the paper proposes a standardised normal distribution weighted feature fusion method and a guided attention module that achieves multimodal learning for both micro-expression and latent emotion recognition. The results show a significant improvement in performance compared to the benchmark results, demonstrating the potential benefits of incorporating multimodal data for improving latent emotion recognition.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of the proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the proposed framework.",
      "page": 3
    },
    {
      "caption": "Figure 2: The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the",
      "page": 4
    },
    {
      "caption": "Figure 3: Illustration of the depthwise inception block’s network design and",
      "page": 4
    },
    {
      "caption": "Figure 4: Structure of the guided attention module.",
      "page": 5
    },
    {
      "caption": "Figure 5: Examples of Daubechies wavelet denoising results for physiological signals.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Liangfei Zhang, Yifei Qian, Ognjen Arandjelovi´c, and Anthony Zhu": "incorporating multimodal data for improving latent emotion recognition accuracy,\nAbstract—This paper discusses the benefits of"
        },
        {
          "Liangfei Zhang, Yifei Qian, Ognjen Arandjelovi´c, and Anthony Zhu": "focusing on micro-expression (ME) and physiological signals (PS). The proposed approach presents a novel multimodal\nlearning"
        },
        {
          "Liangfei Zhang, Yifei Qian, Ognjen Arandjelovi´c, and Anthony Zhu": "framework that combines ME and PS,\nincluding a 1D separable and mixable depthwise inception network, a standardised normal"
        },
        {
          "Liangfei Zhang, Yifei Qian, Ognjen Arandjelovi´c, and Anthony Zhu": "distribution weighted feature fusion method, and depth/physiology guided attention modules for multimodal\nlearning. Experimental"
        },
        {
          "Liangfei Zhang, Yifei Qian, Ognjen Arandjelovi´c, and Anthony Zhu": "results show that\nthe proposed approach outperforms the benchmark method, with the weighted fusion method and guided attention"
        },
        {
          "Liangfei Zhang, Yifei Qian, Ognjen Arandjelovi´c, and Anthony Zhu": "modules both contributing to enhanced performance."
        },
        {
          "Liangfei Zhang, Yifei Qian, Ognjen Arandjelovi´c, and Anthony Zhu": "learning, micro-expression recognition, physiological signal analysis.\nIndex Terms—Emotion recognition, multi-modal"
        },
        {
          "Liangfei Zhang, Yifei Qian, Ognjen Arandjelovi´c, and Anthony Zhu": "✦"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "and guided attention fusion module. The final\nloss is (Lps + Lmm)/2, where Lps and Lmm are both cross-entropy losses calculated from the PS"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "branch and whole multimodal\nlearning, respectively."
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "2\nRELATED WORK\nThe sudden appearance of MEs makes\ntheir\ntemporal"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "features important. While some methods use only the apex"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "2.1\nMicro-expression recognition"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "frame of each ME sample [14], [15], [16], most use all frames"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "Micro-expression recognition (MER) which refers\nto\nthe"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "between the onset and offset, treating all temporal changes"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "recognition of emotions expressed in a sequence of\nfaces"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "equally.\nSome\neven use\ntemporal\nframe\ninterpolation to"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "known to be brief and subtle, is one significant challenge in"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "increase the number of frames [8], [9], [12], [17], [18]. Many"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "affective computing to mining human’s authentic emotions."
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "handcrafted feature-based approaches treat raw video data"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "In recent years,\ncomputer vision technology has been in-"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "as a 3D spatio-temporal volume, applying the same operator"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "creasingly utilised for automatic MER, which has improved"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "used to extract spatial features to pseudo-images formed by"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "the feasibility of applications involving MEs [4]. Since the"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "a cut through the 3D volume comprising one spatial dimen-"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "publication of the open spantenous ME databases [5], [6] in"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "sion and the temporal dimension. LBP-TOP and 3DHOG are"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "2013,\nthe volume of\nresearch on automatic MER has been"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "examples of this approach. Similar in this regard are optical"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "increasing steadily over\nthe years. From the handcrafted"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "flow-based features, which combine local spatial and tempo-"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "computer vision methods\nin the\nearly years\nto the deep"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "ral elements [9], [16], [19]. Instead of using raw appearance"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "learning approaches more recently,\nthe main ideas of ME"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "imagery, some authors propose using pre-processed data in"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "feature extraction could be categorised as primarily pursu-"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "the form of optic flow matrices to exploit proximal temporal"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "ing either a spatial strategy or a temporal one."
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "information directly in deep learning methods\n[20],\n[21],"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "Facial Region Of\nInterest\n(ROI) segmentation is a com-"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "[22],\n[23]. Longer-range temporal patterns are learned by"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "mon technique for spatial\nfeature extraction in ME analy-"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "treating video sequences as 3-dimensional matrices or by"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "sis\n[7],\n[8],\n[9], where\nthe\nface\nsample\nis\nsegmented into"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "employing structures like RNN or LSTM [18], [24]."
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "different\nregions based on the Facial Action Coding Sys-"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "In previous research, colour images or videos have been"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "tem (FACS)\n[10]. These\nregions\ncan correspond to inde-"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "the primary data samples used due to the availability of"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "pendent\nfacial muscle complexes and are subjected to in-"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "ME databases. However,\nthe recent\nrelease of 4DME [25]"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "dividual\nappearance normalisation. Polikovsky et\nal.\nin-"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "and CAS(ME)3\n[26] has expanded the range of data avail-"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "troduced a gradient\nfeature that\nconstructs histograms of"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "able for emotion recognition related to micro-expressions."
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "local gradient projections across different regions [11], and"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "4DME primarily focuses on 3D micro-expression data, while"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "the Local Binary Pattern\n(LBP)\noperator describes\nlocal"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "CAS(ME)3 not only includes RGB-D micro-expression video"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "appearance in a robust way using the relative brightness of"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "clips but also includes physiological signals in one part of"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "neighbouring pixels [12]. As for deep learning approaches,"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "the database. This enables multimodal learning with micro-"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "the convolution and pooling layers perform spatial\nfeature"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "expressions for emotion recognition."
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "extraction. Attention blocks have been introduced in recent"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "years to improve the learning of spatial features within neu-"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "2.2\nMultimodal emotion recognition"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "ral networks, by generating weight masks for feature maps"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "to focus on significant\nregions. Graph Convolutional Net-\nNumerous works\nin the\nliterature\nshow that multimodal"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "work (GCN) are another optimisation measure used to cap-\nemotion\nrecognition\nsystems\noutperform unimodal\nap-"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "ture spatial\ninformation, often by using Action Units (AUs)\nproaches [27],\n[28]. Emotions are intricate experiences that"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "as corresponding graph nodes\n[13]. These methods use a\ninvolve not only observable physical expressions but also"
        },
        {
          "Fig. 1. Architecture of\nthe proposed framework comprising three main components: ME feature extraction branch, PS feature extraction branch,": "priori knowledge to enhance the extracted spatial features.\ninternal feelings, thoughts, and other processes that may not"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3": "multimodal emotion recognition processing with MEs."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "3\nPROPOSED METHOD"
        },
        {
          "3": ""
        },
        {
          "3": "Colour\nimages are a crucial and widely used source\nfor"
        },
        {
          "3": "computer vision tasks [39], [40], [41], providing valuable in-"
        },
        {
          "3": "formation for deep learning models to analyse and interpret"
        },
        {
          "3": "visual data.\nIn addition, ME has been extensively studied"
        },
        {
          "3": "and recognised as a valuable source for authentic emotion"
        },
        {
          "3": "recognition,\nas discussed in Section 2. Therefore,\nin our"
        },
        {
          "3": "proposed framework, we consider colour images from ME"
        },
        {
          "3": "video clips as the primary source, with depth information"
        },
        {
          "3": "used to guide the spatial features from each frame. Features"
        },
        {
          "3": "are extracted from colour images and depth maps separately"
        },
        {
          "3": "using backbone networks. To fuse the spatial\nfeatures ex-"
        },
        {
          "3": "tracted from each frame, we designed a standard normal"
        },
        {
          "3": "distribution guided fusion method that pays more attention"
        },
        {
          "3": "to the middle, where facial movements usually reach their"
        },
        {
          "3": "apex, and less attention to the ends. Apart\nfrom ME, PS"
        },
        {
          "3": "are used in another branch of\nthe proposed framework to"
        },
        {
          "3": "recognise\nlatent\nemotions by our designed 1D separable"
        },
        {
          "3": "and mixable depthwise\ninception\nnetwork\nand enhance"
        },
        {
          "3": "spatiotemporal\nfeatures extracted from the ME sample. By"
        },
        {
          "3": "analysing both ME and PS,\nthe network can gain a deeper"
        },
        {
          "3": "understanding of\nthe subject’s emotional state and achieve"
        },
        {
          "3": "more accurate latent emotion recognition results. Figure 1"
        },
        {
          "3": "shows the proposed framework."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "3.1\n1D separable & mixable depthwise inception CNN"
        },
        {
          "3": ""
        },
        {
          "3": "The\nnetwork\nis designed to\nextract\nPS\nfeatures\nin\nour"
        },
        {
          "3": "framework, whose structure can be seen as Figure 2. The"
        },
        {
          "3": "depthwise structure contains separate convolutions for each"
        },
        {
          "3": "group of\nchannels, allowing for more precise\nfeature\nex-"
        },
        {
          "3": "traction and capturing spatial correlations. This enables the"
        },
        {
          "3": "network to learn more complex and diverse representations"
        },
        {
          "3": "of the input data. The Inception block uses a combination of"
        },
        {
          "3": "convolutional filters of different sizes within a single layer."
        },
        {
          "3": "This\nallows\nthe network to capture\nfeatures\nat multiple"
        },
        {
          "3": "scales without\nthe need for multiple layers, which can be"
        },
        {
          "3": "computationally expensive. The Inception block can be seen"
        },
        {
          "3": "as\nan ensemble of\nsmaller networks with different filter"
        },
        {
          "3": "sizes, which provides a form of regularisation that helps pre-"
        },
        {
          "3": "vent overfitting and improves generalisation performance."
        },
        {
          "3": "In order\nto\neffectively extract\nfeatures\nfrom multiple"
        },
        {
          "3": "input channels, a method is proposed that\ninvolves the ex-"
        },
        {
          "3": "traction of features from each channel individually, followed"
        },
        {
          "3": "by their combination for further learning. To achieve this, a"
        },
        {
          "3": "depthwise convolutional\nlayer is initially employed to en-"
        },
        {
          "3": "hance the features from each input channel. Subsequently, a"
        },
        {
          "3": "depthwise inception block with three groups is designed to"
        },
        {
          "3": "extract features from each physiological source. The design"
        },
        {
          "3": "of\nthe depthwise inception block is illustrated in Figure 3,"
        },
        {
          "3": "and comprises\nfour branches of depthwise\nconvolutional"
        },
        {
          "3": "layers with varying kernel\nsizes\nto extract\nfeatures\nfrom"
        },
        {
          "3": "different scales. The resulting features from each branch are"
        },
        {
          "3": "then concatenated together and fed into the next depthwise"
        },
        {
          "3": "inception block. Since the number of output channels in each"
        },
        {
          "3": "branch of the inception block is different, the features from"
        },
        {
          "3": "different sources are mixed together to form a new group of"
        },
        {
          "3": "channels for further learning. Finally, the mixed features are"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": "length of signals."
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        },
        {
          "Fig. 2. The separable and mixable network proposed for physiological signals, see Figure 3 for details of depthwise inception block, where L is the": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "follows:": "(cid:17)\n(cid:16)"
        },
        {
          "follows:": "exp\n− i2"
        },
        {
          "follows:": "2\n6σ"
        },
        {
          "follows:": ",\ni = −3σ + f ·"
        },
        {
          "follows:": "where\nWf =\n(cid:16)\n(cid:17) ,"
        },
        {
          "follows:": "(cid:80)F −1\nF − 1"
        },
        {
          "follows:": "− i2"
        },
        {
          "follows:": "f =0 exp\n2"
        },
        {
          "follows:": "(1)"
        },
        {
          "follows:": ""
        },
        {
          "follows:": "where F denotes the number of frames in one ME sample."
        },
        {
          "follows:": "Note that roughly 99.7% of the probability density for a stan-"
        },
        {
          "follows:": "dard normal distribution lies within 3 standard deviations"
        },
        {
          "follows:": ""
        },
        {
          "follows:": "of the mean, and thus lies within the range (−3, 3). Only the"
        },
        {
          "follows:": ""
        },
        {
          "follows:": "weights within 3 standard deviations are considered and the"
        },
        {
          "follows:": "remaining 0.3% is negligible."
        },
        {
          "follows:": ""
        },
        {
          "follows:": "3.3\nThe depth/physiology guided attention module"
        },
        {
          "follows:": "The module is designed for feature fusion of both colour and"
        },
        {
          "follows:": "depth information for each frame of micro-expression, as"
        },
        {
          "follows:": "well as the final fusion of ME and PS features. For attention"
        },
        {
          "follows:": "modules, formally we have a query Q, a key K, and a value"
        },
        {
          "follows:": "V\nto calculate attention. The depth and PS features could"
        },
        {
          "follows:": "be considered as the input Q to guide attention learning. At"
        },
        {
          "follows:": "the beginning of\nthe module,\nthe main features are copied"
        },
        {
          "follows:": "as sources for both inputs K and V . After fully-connected"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: displays the results of the depth/physiology",
      "data": [
        {
          "5": "The\nscenario was designed to create a stressful\nsituation"
        },
        {
          "5": "that would elicit spontaneous MEs associated with guilt or"
        },
        {
          "5": "deception. Part C contains 166 MEs\nfrom 31 subjects and"
        },
        {
          "5": "makes it possible to enrich multimodal ME analysis with PS,"
        },
        {
          "5": "including electrodermal activity (EDA), heart rate/fingertip"
        },
        {
          "5": "pulse\n–\nelectrocardiogram (ECG),\nrespiration (RSP),\nand"
        },
        {
          "5": "pulse photoplethysmography (PPG)."
        },
        {
          "5": "The colour and depth frames in Part C are captured at a"
        },
        {
          "5": "frame rate of 30 fps. Due to the definition of the happening"
        },
        {
          "5": "time of a micro-expression, we selected only the samples"
        },
        {
          "5": "with less\nthan 15 frames\n(500ms)\nfrom the database. We"
        },
        {
          "5": "cropped the facial region based on the landmarks detected"
        },
        {
          "5": "from the onset colour image, and this cropping was applied"
        },
        {
          "5": "to all subsequent colour and depth images. To process the"
        },
        {
          "5": "data, we utilised pretrained VGG-Face [42] and VGG-16 [43]"
        },
        {
          "5": "networks as backbone networks for colour and depth,\nre-"
        },
        {
          "5": "spectively."
        },
        {
          "5": "As for the PS branch, we utilised EDA, ECG, and PPG"
        },
        {
          "5": "signals as\nthe\nthree-channel\ninput. To process\nthe source"
        },
        {
          "5": ""
        },
        {
          "5": "signal data, we employed wavelet denoising on the seg-"
        },
        {
          "5": "mented signal\nclips. This method is\nhighly\neffective\nin"
        },
        {
          "5": "denoising\n1D signals due\nto\nits\nability\nto\ncapture\nboth"
        },
        {
          "5": ""
        },
        {
          "5": "local\nand global\nfeatures of\nthe\nsignal\naccurately, whilst"
        },
        {
          "5": ""
        },
        {
          "5": "maintaining a good balance between time and frequency"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "localisation. Daubechies wavelets are orthogonal and form"
        },
        {
          "5": ""
        },
        {
          "5": "a complete basis set, allowing the signal to be decomposed"
        },
        {
          "5": ""
        },
        {
          "5": "into its wavelet coefficients, which can then be thresholded"
        },
        {
          "5": ""
        },
        {
          "5": "to remove noise, as shown in Figure 5."
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "4.2\nExperimental results"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "The\ntraditional\nevaluation approach for micro-expression"
        },
        {
          "5": "recognition\ninvolves\nleave-one-subject-out\n(LOSO)\ncross-"
        },
        {
          "5": "validation, where a single subject’s data is withheld and"
        },
        {
          "5": "used as a validation data set, while all remaining subjects’"
        },
        {
          "5": "data\nis used for\ntraining. The\noverall performance\nof\na"
        },
        {
          "5": "method is\nthen assessed by aggregating the results of all"
        },
        {
          "5": ""
        },
        {
          "5": "different possible iterations of the process, i.e. of all subjects"
        },
        {
          "5": ""
        },
        {
          "5": "being withheld in turn. Li\net\nal.\nalso utilised LOSO in"
        },
        {
          "5": ""
        },
        {
          "5": "their experiments [26]. To ensure fair comparison and more"
        },
        {
          "5": ""
        },
        {
          "5": "accurate evaluation, we applied the LOSO approach in our"
        },
        {
          "5": ""
        },
        {
          "5": "experiments as well. Accuracy, unweighted F1-score (UF1)"
        },
        {
          "5": ""
        },
        {
          "5": "and unweighted average recall\n(UAR), averaging the per-"
        },
        {
          "5": ""
        },
        {
          "5": "class\nrecall and F1-score respectively, are used as metrics"
        },
        {
          "5": "during evaluation."
        },
        {
          "5": "Our study aimed to investigate multimodal\nlatent emo-"
        },
        {
          "5": ""
        },
        {
          "5": "tion recognition, and the main results\ncan be seen in Ta-"
        },
        {
          "5": ""
        },
        {
          "5": "ble 1. To confirm the effectiveness of our proposed network"
        },
        {
          "5": "structure and each designed module, we conducted ablation"
        },
        {
          "5": "experiments. Table 2 presents the results related to the stan-"
        },
        {
          "5": "dardized normal distribution guided spatial feature fusion,"
        },
        {
          "5": "while Table 3 displays the results of\nthe depth/physiology"
        },
        {
          "5": "guided attention module."
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "4.3\nAnalysis and discussion"
        },
        {
          "5": ""
        },
        {
          "5": "All\nresults of our\nexperiments will be discussed in this"
        },
        {
          "5": "section. We start by investigating the impact of multimodal"
        },
        {
          "5": "learning on latent emotion recognition performance. Then,"
        },
        {
          "5": "we discuss the effectiveness of each design inside the pro-"
        },
        {
          "5": "posed framework."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: , revealed that",
      "data": [
        {
          "Acc\nUF1\nUAR": "Concatenation\nUniform Distribution\n0.610\n0.258\n0.283",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "0.604\n0.262\n0.288\n0.701\n0.492\n0.468"
        },
        {
          "Acc\nUF1\nUAR": "0.640\n0.353\n0.345\nStandard Normal Distribution",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "0.640\n0.315\n0.318\n0.738\n0.586\n0.563\nGuided Attention module"
        },
        {
          "Acc\nUF1\nUAR": "The results presented in Table 1 demonstrate a significant",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "more\neffectively than simply adding them together. The"
        },
        {
          "Acc\nUF1\nUAR": "improvement\nin performance compared to the benchmark",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "weighted fusion method assigns different weights\nto dif-"
        },
        {
          "Acc\nUF1\nUAR": "results of Li et al. who used RGB and depth information",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "ferent\nfeatures based on their\nimportance, allowing more"
        },
        {
          "Acc\nUF1\nUAR": "from the apex frame only as 4-channel\ninput\nfor AlexNet.",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "important features to have a greater influence on the overall"
        },
        {
          "Acc\nUF1\nUAR": "While producing worse performance\nthan the proposed",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "learning process. This emphasises the significance of each"
        },
        {
          "Acc\nUF1\nUAR": "approach their work demonstrated the value of depth in-",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "feature’s contribution to the final\nresult and enhances the"
        },
        {
          "Acc\nUF1\nUAR": "formation. In contrast to Li et al., herein we used all frames",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "performance of the model in recognising micro-expressions."
        },
        {
          "Acc\nUF1\nUAR": "from a video clip and a standard normal distribution feature",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "Furthermore, a comparison study was conducted to evalu-"
        },
        {
          "Acc\nUF1\nUAR": "fusion module\nto merge\nthe\nfeatures\nextracted from all",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "ate the impact of depth/physiology guided attention mod-"
        },
        {
          "Acc\nUF1\nUAR": "frames. We note that although the use of depth information",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "ules on the performance of the proposed model, a compar-"
        },
        {
          "Acc\nUF1\nUAR": "facilitates the learning of more expressive features,\nit may",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "ison study was conducted. We used concatenation as\nthe"
        },
        {
          "Acc\nUF1\nUAR": "introduce noise, which is particularly problematic in micro-",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "baseline fusion method for multimodal learning and trained"
        },
        {
          "Acc\nUF1\nUAR": "expression analysis wherein the signal corrupted by noise",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "and tested four different configurations of\nthe model. The"
        },
        {
          "Acc\nUF1\nUAR": "is weak;\ntherefore, one of\nthe potential avenues for further",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "results of\nthe\nstudy, presented in Table\n3,\nrevealed that"
        },
        {
          "Acc\nUF1\nUAR": "research in colour and depth micro-expression recognition",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "the depth-guided attention module outperformed concate-"
        },
        {
          "Acc\nUF1\nUAR": "could be finding a better approach to denoise the depth",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "nation in incorporating colour and depth information. Ad-"
        },
        {
          "Acc\nUF1\nUAR": "information. Regarding physiological signals, Li et al. used",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "ditionally,\nthe physiology-guided attention module used"
        },
        {
          "Acc\nUF1\nUAR": "them as greyscale 2D input channels to the same backbone,",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "for\nemotion recognition demonstrated significantly better"
        },
        {
          "Acc\nUF1\nUAR": "without addressing their noise content or designing a spe-",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "results,\nindicating that\nthese guided attention modules are"
        },
        {
          "Acc\nUF1\nUAR": "cialized network to process them. In contrast, the proposed",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "capable of effectively fusing extracted features from multi-"
        },
        {
          "Acc\nUF1\nUAR": "approach employed the Daubechies wavelet\nfor denoising",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "ple modalities to learn more beneficial mixed features and"
        },
        {
          "Acc\nUF1\nUAR": "and introduced a\n1D separable\nand mixable depthwise",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "contribute to the improved performance of\nthe proposed"
        },
        {
          "Acc\nUF1\nUAR": "inception CNN for\nfeature extraction. Our\nresults suggest",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "model in latent emotion recognition."
        },
        {
          "Acc\nUF1\nUAR": "that\nthis network\nstructure\ncontributes\nto\nthe\nimproved",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": ""
        },
        {
          "Acc\nUF1\nUAR": "performance in recognising latent emotions.",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": ""
        },
        {
          "Acc\nUF1\nUAR": "",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "5\nCONCLUSION"
        },
        {
          "Acc\nUF1\nUAR": "The performance of\nthe proposed standardised normal",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": ""
        },
        {
          "Acc\nUF1\nUAR": "distribution weighted fusion method is compared with that",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "Emotional states have a significant\nimpact on physical and"
        },
        {
          "Acc\nUF1\nUAR": "of\nthe uniform distributed fusion method in Table 2. The",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "psychological well-being, and the recognition of emotions"
        },
        {
          "Acc\nUF1\nUAR": "results demonstrate that the proposed method can fuse the",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "is essential for effective communication and understanding"
        },
        {
          "Acc\nUF1\nUAR": "features\nextracted from each frame\nof micro-expressions",
          "Acc\nUF1\nUAR\nAcc\nUF1\nUAR": "of an individual’s\nemotional and mental\nstate. However,"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": "[17] X. Li, X. Hong, A. Moilanen, X. Huang, T. Pfister, G. Zhao, and"
        },
        {
          "7": "M. Pietikainen, “Towards reading hidden emotions: A compara-"
        },
        {
          "7": ""
        },
        {
          "7": "tive study of spontaneous micro-expression spotting and recogni-"
        },
        {
          "7": ""
        },
        {
          "7": "tion methods,” IEEE Transactions on Affective Computing, 2018."
        },
        {
          "7": "[18] H. Q. Khor,\nJ. See, R. C. W. Phan, and W. Lin, “Enriched long-"
        },
        {
          "7": "term recurrent convolutional network for facial micro-expression"
        },
        {
          "7": "recognition,” in Proceedings of 13th IEEE International Conference on"
        },
        {
          "7": ""
        },
        {
          "7": "Automatic Face and Gesture Recognition, FG, 2018."
        },
        {
          "7": ""
        },
        {
          "7": "[19]\nS. T. Liong, Y.\nS. Gan,\nJ.\nSee, H. Q. Khor,\nand Y. C. Huang,"
        },
        {
          "7": "“Shallow triple\nstream three-dimensional CNN (STSTNet)\nfor"
        },
        {
          "7": "micro-expression recognition,” in Proceedings of IEEE International"
        },
        {
          "7": "Conference on Automatic Face and Gesture Recognition, 2019."
        },
        {
          "7": ""
        },
        {
          "7": "[20] Z. Xia, X. Hong, X. Gao, X. Feng, and G. Zhao, “Spatiotemporal"
        },
        {
          "7": ""
        },
        {
          "7": "Recurrent Convolutional Networks for Recognizing Spontaneous"
        },
        {
          "7": "Micro-Expressions,” IEEE Transactions on Multimedia, vol. 22, no. 3,"
        },
        {
          "7": "2020."
        },
        {
          "7": "[21] Y.\nLiu, H. Du,\nL. Zheng,\nand\nT. Gedeon,\n“A neural micro-"
        },
        {
          "7": ""
        },
        {
          "7": "of 14th IEEE International\nexpression recognizer,” in Proceedings"
        },
        {
          "7": ""
        },
        {
          "7": "Conference on Automatic Face and Gesture Recognition, FG, 2019."
        },
        {
          "7": "[22] A.\nJ. R. Kumar and B. Bhanu, “Micro-Expression Classification"
        },
        {
          "7": "Based on Landmark Relations With Graph Attention Convolu-"
        },
        {
          "7": "of\nthe\nIEEE/CVF Conference\non\ntional Network,”\nin Proceedings"
        },
        {
          "7": "Computer Vision and Pattern Recognition (CVPR) Workshops, 2021,"
        },
        {
          "7": "pp. 1511–1520."
        },
        {
          "7": ""
        },
        {
          "7": "[23] L. Zhang, X. Hong, O. Arandjelovi´c, and G. Zhao, “Short and"
        },
        {
          "7": ""
        },
        {
          "7": "long range relation based spatio-temporal\ntransformer for micro-"
        },
        {
          "7": ""
        },
        {
          "7": "expression recognition,” IEEE Transactions on Affective Computing,"
        },
        {
          "7": ""
        },
        {
          "7": "vol. 13, no. 4, pp. 1973–1985, 2022."
        },
        {
          "7": ""
        },
        {
          "7": "[24] D. H. Kim, W.\nJ. Baddar, and Y. M. Ro, “Micro-expression recog-"
        },
        {
          "7": ""
        },
        {
          "7": "nition with expression-state constrained spatio-temporal\nfeature"
        },
        {
          "7": ""
        },
        {
          "7": "the 2016 ACM Multimedia Confer-\nrepresentations,” Proceedings of"
        },
        {
          "7": ""
        },
        {
          "7": "ence, pp. 382–386, 2016."
        },
        {
          "7": ""
        },
        {
          "7": "[25] X. Li, S. Cheng, Y. Li, M. Behzad,\nJ. Shen, S. Zafeiriou, M. Pan-"
        },
        {
          "7": "tic, and G. Zhao, “4DME: A Spontaneous 4D Micro-Expression"
        },
        {
          "7": "Dataset With Multimodalities,” IEEE Transactions on Affective Com-"
        },
        {
          "7": "puting, pp. 1–18, 2022."
        },
        {
          "7": "[26]\nJ. Li, Z. Dong, S. Lu, S.-J. Wang, W.-J. Yan, Y. Ma, Y. Liu, C. Huang,"
        },
        {
          "7": "and X. Fu, “CAS(ME)3: A Third Generation Facial Spontaneous"
        },
        {
          "7": "Micro-Expression Database With Depth Information and High"
        },
        {
          "7": "IEEE Transactions\non Pattern Analysis\nand\nEcological Validity,”"
        },
        {
          "7": "Machine Intelligence, vol. 45, no. 3, pp. 2782–2800, 2023."
        },
        {
          "7": "[27]\nS. K. D’mello and J. Kory, “A review and meta-analysis of multi-"
        },
        {
          "7": "modal affect detection systems,” ACM computing surveys (CSUR),"
        },
        {
          "7": "vol. 47, no. 3, pp. 1–36, 2015."
        },
        {
          "7": "[28] N. Nahid, A. Rahman, and M. A. R. Ahad, “Contactless human"
        },
        {
          "7": "emotion analysis across different modalities,” Contactless Human"
        },
        {
          "7": "Activity Analysis, pp. 237–269, 2021."
        },
        {
          "7": ""
        },
        {
          "7": "[29] R. Munoz, R. Olivares, C. Taramasco, R. Villarroel, R. Soto, T. S."
        },
        {
          "7": ""
        },
        {
          "7": "Barcelos, E. Merino, and M. F. Alonso-S´anchez, “Using black hole"
        },
        {
          "7": ""
        },
        {
          "7": "algorithm to improve eeg-based emotion recognition,” Computa-"
        },
        {
          "7": ""
        },
        {
          "7": "intelligence and neuroscience, vol. 2018, 2018."
        },
        {
          "7": ""
        },
        {
          "7": "[30] B. Nakisa, M. N. Rastgoo, D. Tjondronegoro, and V. Chandran,"
        },
        {
          "7": ""
        },
        {
          "7": "“Evolutionary computation algorithms for feature selection of eeg-"
        },
        {
          "7": ""
        },
        {
          "7": "based emotion recognition using mobile sensors,” Expert Systems"
        },
        {
          "7": ""
        },
        {
          "7": "with Applications, vol. 93, pp. 143–155, 2018."
        },
        {
          "7": ""
        },
        {
          "7": "[31] P. Pandey and K. Seeja, “Emotional\nstate\nrecognition with eeg"
        },
        {
          "7": ""
        },
        {
          "7": "signals using subject\nindependent approach,” in Data Science and"
        },
        {
          "7": ""
        },
        {
          "7": "Big Data Analytics: ACM-WIR 2018.\nSpringer, 2019, pp. 117–124."
        },
        {
          "7": ""
        },
        {
          "7": "[32]\nS. Br´as,\nJ. H. Ferreira, S. C. Soares, and A.\nJ. Pinho, “Biometric"
        },
        {
          "7": ""
        },
        {
          "7": "and emotion identification: An ecg compression based method,”"
        },
        {
          "7": ""
        },
        {
          "7": "Frontiers in psychology, vol. 9, p. 467, 2018."
        },
        {
          "7": ""
        },
        {
          "7": "[33] H. Kaji, H.\nIizuka,\nand M.\nSugiyama,\n“Ecg-based concentra-"
        },
        {
          "7": ""
        },
        {
          "7": "tion recognition with multi-task regression,” IEEE Transactions on"
        },
        {
          "7": ""
        },
        {
          "7": "Biomedical Engineering, vol. 66, no. 1, pp. 101–110, 2018."
        },
        {
          "7": ""
        },
        {
          "7": "[34] G. K. Verma and U. S. Tiwary, “Multimodal fusion framework: A"
        },
        {
          "7": ""
        },
        {
          "7": "multiresolution approach for emotion classification and recogni-"
        },
        {
          "7": ""
        },
        {
          "7": "tion from physiological signals,” NeuroImage, vol. 102, pp. 162–172,"
        },
        {
          "7": ""
        },
        {
          "7": "2014."
        },
        {
          "7": ""
        },
        {
          "7": "[35] D. Shin, D. Shin, and D. Shin, “Development of emotion recog-"
        },
        {
          "7": "nition interface using complex eeg/ecg bio-signal\nfor interactive"
        },
        {
          "7": "contents,” Multimedia Tools and Applications, vol. 76, pp. 11 449–"
        },
        {
          "7": "11 470, 2017."
        },
        {
          "7": "[36]\nS. Wang, J. Du, and R. Xu, “Decision fusion for eeg-based emotion"
        },
        {
          "7": "recognition,” in 2015 International Conference on Machine Learning"
        },
        {
          "7": "and Cybernetics (ICMLC), vol. 2.\nIEEE, 2015, pp. 883–889."
        },
        {
          "7": "[37] Y. Cimtay, E. Ekmekcioglu, and S. Caglar-Ozhan, “Cross-subject"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "Access, vol. 8, pp. 168 865–168 878, 2020."
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "[38] Y. Huang,\nJ. Yang, P. Liao, and J. Pan, “Fusion of\nfacial expres-"
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "sions and eeg for multimodal emotion recognition,” Computational"
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "intelligence and neuroscience, vol. 2017, 2017."
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "[39] Y. Qian,\nL.\nZhang,\nX. Hong,\nC. Donovan,\nand O. Arand-"
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "jelovic, “Segmentation assisted u-shaped multi-scale transformer"
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "for crowd counting,” in 33rd British Machine Vision Conference 2022,"
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "BMVC 2022, London, UK, November 21-24, 2022.\nBMVA Press, 2022."
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "[40] Y.\nShu, X. Gu, G.-Z. Yang,\nand B. P. L. Lo,\n“Revisiting\nself-"
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "supervised contrastive learning for facial expression recognition,”"
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "in 33rd British Machine Vision Conference 2022, BMVC 2022, London,"
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "UK, November 21-24, 2022.\nBMVA Press, 2022."
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "[41] A. Conti, P. Rota, Y. Wang, and E. Ricci, “Cluster-level pseudo-"
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "labelling for\nsource-free cross-domain facial expression recogni-"
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "tion,” in 33rd British Machine Vision Conference 2022, BMVC 2022,"
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "London, UK, November 21-24, 2022.\nBMVA Press, 2022."
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "[42] O. M. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face recogni-"
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "tion,” in British Machine Vision Conference, 2015."
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "[43] K.\nSimonyan\nand A.\nZisserman,\n“Very\ndeep\nconvolutional"
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "arXiv\npreprint\nnetworks\nfor\nlarge-scale\nimage\nrecognition,”"
        },
        {
          "multimodal emotion recognition based on hybrid fusion,” IEEE": "arXiv:1409.1556, 2014."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Micromomentary facial expressions as indicators of ego mechanisms in psychotherapy",
      "authors": [
        "E Haggard",
        "K Isaacs"
      ],
      "year": "1966",
      "venue": "Methods of research in psychotherapy"
    },
    {
      "citation_id": "2",
      "title": "Nonverbal Leakage and Clues to Deception",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1969",
      "venue": "Psychiatry"
    },
    {
      "citation_id": "3",
      "title": "The james-lange theory of emotions: A critical examination and an alternative theory",
      "authors": [
        "W Cannon"
      ],
      "year": "1927",
      "venue": "The American journal of psychology"
    },
    {
      "citation_id": "4",
      "title": "Review of Automatic Microexpression Recognition in the Past Decade",
      "authors": [
        "L Zhang",
        "O Arandjelović"
      ],
      "year": "2021",
      "venue": "Machine Learning and Knowledge Extraction"
    },
    {
      "citation_id": "5",
      "title": "A Spontaneous Micro-expression Database: Inducement, collection and baseline",
      "authors": [
        "X Li",
        "T Pfister",
        "X Huang",
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2013",
      "venue": "Proceedings of 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG"
    },
    {
      "citation_id": "6",
      "title": "CASME database: A dataset of spontaneous micro-expressions collected from neutralized faces",
      "authors": [
        "W Yan",
        "Q Wu",
        "Y Liu",
        "S Wang",
        "X Fu"
      ],
      "year": "2013",
      "venue": "Proceedings of IEEE International Conference and Workshops on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "7",
      "title": "Facial action unit detection with local key facial sub-region based multi-label classification for micro-expression analysis",
      "authors": [
        "L Zhang",
        "O Arandjelovic",
        "X Hong"
      ],
      "year": "2021",
      "venue": "Proceedings of the 1st Workshop on Facial Micro-Expression: Advanced Techniques for Facial Expressions Generation and Spotting"
    },
    {
      "citation_id": "8",
      "title": "Micro-Expression Recognition Using Robust Principal Component Analysis and Local Spatiotemporal Directional Features",
      "authors": [
        "S.-J Wang",
        "W.-J Yan",
        "G Zhao",
        "X Fu",
        "C.-G Zhou"
      ],
      "year": "2014",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "9",
      "title": "A Main Directional Mean Optical Flow Feature for Spontaneous Micro-Expression Recognition",
      "authors": [
        "Y Liu",
        "J Zhang",
        "W Yan",
        "S Wang",
        "G Zhao",
        "X Fu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Facial Action Coding System: Manual and Investigator's Guide",
      "authors": [
        "P Ekman",
        "W Friesen",
        "J Hager"
      ],
      "year": "2002",
      "venue": "Facial Action Coding System: Manual and Investigator's Guide"
    },
    {
      "citation_id": "11",
      "title": "Facial micro-expressions recognition using high speed camera and 3D-Gradient descriptor",
      "authors": [
        "S Polikovsky",
        "Y Kameda",
        "Y Ohta"
      ],
      "year": "2009",
      "venue": "IET Seminar Digest"
    },
    {
      "citation_id": "12",
      "title": "Recognising spontaneous facial micro-expressions",
      "authors": [
        "T Pfister",
        "X Li",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2011",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "13",
      "title": "AU-assisted Graph Attention Convolutional Network for Micro-Expression Recognition",
      "authors": [
        "H.-X Xie",
        "L Lo",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "A novel apex-time network for cross-dataset micro-expression recognition",
      "authors": [
        "M Peng",
        "C Wang",
        "T Bi",
        "Y Shi",
        "X Zhou",
        "T Chen"
      ],
      "year": "2019",
      "venue": "Proceedings of 8th International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "15",
      "title": "OFF-ApexNet on micro-expression recognition system",
      "authors": [
        "Y Gan",
        "S Liong",
        "W Yau",
        "Y Huang",
        "L Tan"
      ],
      "year": "2019",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "16",
      "title": "Less is more: Micro-expression recognition from video using apex frame",
      "authors": [
        "S Liong",
        "J See",
        "K Wong",
        "R Phan"
      ],
      "year": "2018",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "17",
      "title": "Towards reading hidden emotions: A comparative study of spontaneous micro-expression spotting and recognition methods",
      "authors": [
        "X Li",
        "X Hong",
        "A Moilanen",
        "X Huang",
        "T Pfister",
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Enriched longterm recurrent convolutional network for facial micro-expression recognition",
      "authors": [
        "H Khor",
        "J See",
        "R Phan",
        "W Lin"
      ],
      "year": "2018",
      "venue": "Proceedings of 13th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "19",
      "title": "Shallow triple stream three-dimensional CNN (STSTNet) for micro-expression recognition",
      "authors": [
        "S Liong",
        "Y Gan",
        "J See",
        "H Khor",
        "Y Huang"
      ],
      "year": "2019",
      "venue": "Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "20",
      "title": "Spatiotemporal Recurrent Convolutional Networks for Recognizing Spontaneous Micro-Expressions",
      "authors": [
        "Z Xia",
        "X Hong",
        "X Gao",
        "X Feng",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "A neural microexpression recognizer",
      "authors": [
        "Y Liu",
        "H Du",
        "L Zheng",
        "T Gedeon"
      ],
      "year": "2019",
      "venue": "Proceedings of 14th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "22",
      "title": "Micro-Expression Classification Based on Landmark Relations With Graph Attention Convolutional Network",
      "authors": [
        "A Kumar",
        "B Bhanu"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2021"
    },
    {
      "citation_id": "23",
      "title": "Short and long range relation based spatio-temporal transformer for microexpression recognition",
      "authors": [
        "L Zhang",
        "X Hong",
        "O Arandjelović",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Micro-expression recognition with expression-state constrained spatio-temporal feature representations",
      "authors": [
        "D Kim",
        "W Baddar",
        "Y Ro"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM Multimedia Conference"
    },
    {
      "citation_id": "25",
      "title": "4DME: A Spontaneous 4D Micro-Expression Dataset With Multimodalities",
      "authors": [
        "X Li",
        "S Cheng",
        "Y Li",
        "M Behzad",
        "J Shen",
        "S Zafeiriou",
        "M Pantic",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "CAS(ME)3: A Third Generation Facial Spontaneous Micro-Expression Database With Depth Information and High Ecological Validity",
      "authors": [
        "J Li",
        "Z Dong",
        "S Lu",
        "S.-J Wang",
        "W.-J Yan",
        "Y Ma",
        "Y Liu",
        "C Huang",
        "X Fu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "27",
      "title": "A review and meta-analysis of multimodal affect detection systems",
      "authors": [
        "J Kory"
      ],
      "year": "2015",
      "venue": "ACM computing surveys (CSUR)"
    },
    {
      "citation_id": "28",
      "title": "Contactless human emotion analysis across different modalities",
      "authors": [
        "N Nahid",
        "A Rahman",
        "M Ahad"
      ],
      "year": "2021",
      "venue": "Contactless Human Activity Analysis"
    },
    {
      "citation_id": "29",
      "title": "Using black hole algorithm to improve eeg-based emotion recognition",
      "authors": [
        "R Munoz",
        "R Olivares",
        "C Taramasco",
        "R Villarroel",
        "R Soto",
        "T Barcelos",
        "E Merino",
        "M Alonso-Sánchez"
      ],
      "year": "2018",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "30",
      "title": "Evolutionary computation algorithms for feature selection of eegbased emotion recognition using mobile sensors",
      "authors": [
        "B Nakisa",
        "M Rastgoo",
        "D Tjondronegoro",
        "V Chandran"
      ],
      "year": "2018",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "31",
      "title": "Emotional state recognition with eeg signals using subject independent approach",
      "authors": [
        "P Pandey",
        "K Seeja"
      ],
      "year": "2019",
      "venue": "Data Science and Big Data Analytics: ACM-WIR 2018"
    },
    {
      "citation_id": "32",
      "title": "Biometric and emotion identification: An ecg compression based method",
      "authors": [
        "S Brás",
        "J Ferreira",
        "S Soares",
        "A Pinho"
      ],
      "year": "2018",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "33",
      "title": "Ecg-based concentration recognition with multi-task regression",
      "authors": [
        "H Kaji",
        "H Iizuka",
        "M Sugiyama"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "34",
      "title": "Multimodal fusion framework: A multiresolution approach for emotion classification and recognition from physiological signals",
      "authors": [
        "G Verma",
        "U Tiwary"
      ],
      "year": "2014",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "35",
      "title": "Development of emotion recognition interface using complex eeg/ecg bio-signal for interactive contents",
      "authors": [
        "D Shin",
        "D Shin",
        "D Shin"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "36",
      "title": "Decision fusion for eeg-based emotion recognition",
      "authors": [
        "S Wang",
        "J Du",
        "R Xu"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Machine Learning and Cybernetics (ICMLC)"
    },
    {
      "citation_id": "37",
      "title": "Cross-subject multimodal emotion recognition based on hybrid fusion",
      "authors": [
        "Y Cimtay",
        "E Ekmekcioglu",
        "S Caglar-Ozhan"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "38",
      "title": "Fusion of facial expressions and eeg for multimodal emotion recognition",
      "authors": [
        "Y Huang",
        "J Yang",
        "P Liao",
        "J Pan"
      ],
      "year": "2017",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "39",
      "title": "Segmentation assisted u-shaped multi-scale transformer for crowd counting",
      "authors": [
        "Y Qian",
        "L Zhang",
        "X Hong",
        "C Donovan",
        "O Arandjelovic"
      ],
      "year": "2022",
      "venue": "33rd British Machine Vision Conference 2022, BMVC 2022"
    },
    {
      "citation_id": "40",
      "title": "Revisiting selfsupervised contrastive learning for facial expression recognition",
      "authors": [
        "Y Shu",
        "X Gu",
        "G.-Z Yang",
        "B Lo"
      ],
      "year": "2022",
      "venue": "33rd British Machine Vision Conference 2022, BMVC 2022"
    },
    {
      "citation_id": "41",
      "title": "Cluster-level pseudolabelling for source-free cross-domain facial expression recognition",
      "authors": [
        "A Conti",
        "P Rota",
        "Y Wang",
        "E Ricci"
      ],
      "year": "2022",
      "venue": "33rd British Machine Vision Conference 2022, BMVC 2022"
    },
    {
      "citation_id": "42",
      "title": "Deep face recognition",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "British Machine Vision Conference"
    },
    {
      "citation_id": "43",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    }
  ]
}