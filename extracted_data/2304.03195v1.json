{
  "paper_id": "2304.03195v1",
  "title": "Micron-Bert: Bert-Based Facial Micro-Expression Recognition",
  "published": "2023-04-06T16:19:09Z",
  "authors": [
    "Xuan-Bac Nguyen",
    "Chi Nhan Duong",
    "Xin Li",
    "Susan Gauch",
    "Han-Seok Seo",
    "Khoa Luu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Micro-expression recognition is one of the most challenging topics in affective computing. It aims to recognize tiny facial movements difficult for humans to perceive in a brief period, i.e., 0.25 to 0.5 seconds. Recent advances in pre-training deep Bidirectional Transformers (BERT) have significantly improved self-supervised learning tasks in computer vision. However, the standard BERT in vision problems is designed to learn only from full images or videos, and the architecture cannot accurately detect details of facial micro-expressions. This paper presents Micron-BERT (µ-BERT), a novel approach to facial micro-expression recognition. The proposed method can automatically capture these movements in an unsupervised manner based on two key ideas. First, we employ Diagonal Micro-Attention (DMA) to detect tiny differences between two frames. Second, we introduce a new Patch of Interest (PoI) module to localize and highlight micro-expression interest regions and simultaneously reduce noisy backgrounds and distractions. By incorporating these components into an end-to-end deep network, the proposed µ-BERT significantly outperforms all previous work in various micro-expression tasks. µ-BERT can be trained on a large-scale unlabeled dataset, i.e., up to 8 million images, and achieves high accuracy on new unseen facial micro-expression datasets. Empirical experiments show µ-BERT consistently outperforms state-of-theart performance on four micro-expression benchmarks, including SAMM, CASME II, SMIC, and CASME3, by significant margins. Code will be available at https:// github.com/uark-cviu/Micron-BERT",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Facial expressions are a complex mixture of conscious reactions directed toward given stimuli. They involve experiential, behavioral, and physiological elements. Because they are crucial to understanding human reactions, this topic has been widely studied in various application domains  [5] . In general, facial expression problems can be classified into two main categories, macro-expression, and micro-expression. The main differences between the two are facial expression intensities, and duration  [2] . In particular, macro-expressions happen spontaneously, cover large movement areas in a given face, e.g., mouth, eyes, cheeks, and typically last from 0.5 to 4 seconds. Humans can usually recognize these expressions. By contrast, microexpressions are involuntary occurrences, have low intensity, and last between 5 milliseconds and half a second. Indeed, micro-expressions are challenging to identify and are mostly detectable only by experts. Micro-expression understanding is essential in numerous applications, primarily lie detection, which is crucial in criminal analysis.\n\nMicro-expression identification requires both semantics and micro-movement analysis. Since they are difficult to observe through human eyes, a high-speed camera, usually with 200 frames per second (FPS)  [6, 15, 51] , is typically used to capture the required video frames. Previous work  [11]  tried to understand this micro information using MagNet  [29]  to amplify small motions between two frames, e.g., onset and apex frames. However, these methods still have limitations in terms of accuracy and robustness. In summary, the contributions of this work are four-fold:\n\n• A novel Facial Micro-expression Recognition (MER) via Pre-training of Deep Bidirectional Transformers approach (Micron-BERT or µ-BERT) is presented to tackle the problem in a self-supervised learning manner. The proposed method aims to identify and localize micro-movements in faces accurately.\n\n• As detecting the tiny moment changes in faces is an essential input to the MER module, a new Diagonal Micro Attention (DMA) mechanism is proposed to precisely identify small movements in faces between two consecutive video frames.\n\n• A new Patch of Interest (POI) module is introduced to efficiently spot facial regions containing the microexpressions. Far apart from prior methods, it is trained in an unsupervised manner without using any facial labels, such as facial bounding boxes or landmarks.\n\n• The proposed µ-BERT framework is designed in a self-supervised learning manner and trained in an endto-end deep network. Indeed, it consistently achieves State-of-the-Art (SOTA) results in various standard micro-expression benchmarks, including CASME II  [50] , CASME3  [14] , SAMM  [6]  and SMIC  [15] . It achieves high recognition accuracy on new unseen subjects of various gender, age, and ethnicity.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Generally, prior studies in micro-expression can be divided into two categories, including micro-expression spotting (MES) and micro-expression recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Micro-Expression Spotting (Mes).",
      "text": "The goal of MES is to determine the specific instant during which a microexpression occurs. Li et al.  [16]  adopted a spatial-channel attention network to detect micro-expression action units. Tran et al.  [39]  attempted to standardize with the SMIC-E database and an evaluation protocol. MESNet  [43]  introduced a CNN-based approach with a (2+1)D convolutional network, a clip proposal, and a classifier. Micro-Expression Recognition (MER). The goal of MER tasks is to classify the facial micro-expressions in a video. Ling et al.  [11]  present a new way of learning facial graph representations, allowing these small movements to be seen. Kumar and Bhanu  [31]  exploited connections between landmark points and their optical flow patch and achieved improvements over state-of-the-art (SOTA) methods for both the CASME II and SAMM. Liu et al.  [21]  presented a new method using transfer learning achieved an accuracy of 84.27% on a composite of three datasets. Wang et al.  [45]  presented an Eulerian-motion magnification-based approach that highlights these small movements. Other Work. Other research, while not necessarily on MES or MER, is relevant to our approach. An advance in video motion magnification is shown in  [29] , outperforming the SOTA methods in multiple areas. This learning-based model can extract filters from data directly rather than rely on ones designed by hand, like the state-of-the-art method.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Bert Revisited",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Bert In Vision Problems",
      "text": "Transformers and deep learning have significantly improved results for many tasks in computer vision  [1, 7, 9, 22, 23, 26, 27, 30, 40, 41] . Worth mentioning is Vision Transformer (ViT)  [7] , one of the first research efforts at the intersection of Transformers and computer vision. Unlike the traditional CNN network, ViT splits an image into a sequence of patches and applies the Transformers-based framework directly. Inspired by the success of BERT in Natural Language Processing (NLP), Bidirectional Encoder representation from Image Transformers (BEiT)  [1]  is presented as a self-supervised learning framework in computer vision. In particular, image patches are tokenized using DALL-E  [32]  to the visual tokens. These tokens are then randomly masked before feeding into the transformer backbone. The training objective is to recover the original visual tokens from the corrupted patches. These methods  [1, 38]  have marked a remarkable improvement compared to supervised learning methods by leveraging large-scale unlabelled datasets, e.g., ImageNet-1K, ImagNet-21K  [33] , to discover semantic information.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Limitations Of Bert In Vision Problems",
      "text": "One limitation of using BERT in vision problems is the tokenization step. In the NLP field, a token has precisely one word mapped into it. In vision problems, however, many possible images or patches can share the same token as long as they have the same content. Therefore, designing a BERT model to mask a token and train a prediction model in the missing contexts in computer vision is more challenging than NLP. In addition, the tokenizer, i.e., DALL-E  [32] , is not robust enough to map similar contexts to a token. It yields noise in the tokenization process and affects the overall training performance. He et al.,  [9]  presented a Masked Auto Encoder (MAE) that utilizes the BERT framework. Instead of tokenizing images, it eliminates patches of an image via a random masking strategy and reconstructs the context of these masked patches to the original content. Although this method can avoid using the tokenizer, it only considers the context inside an image. Thus, it does not apply to micro-expression, which requires understanding semantic information from consecutive video frames. In this paper, µ-BERT is presented to address these limitations.  of Interest (PoI), Blockwise Swapping, Diagonal Micro Attention (DMA), and a µ-Decoder. Given input images I t and I t+δ , the role of the µ-Encoder is to represent I t and I t+δ into latent vectors. Then, Patch of Interest (PoI) constrains µ-BERT to look into facial regions containing microexpressions rather than unrelated regions such as the background. Blockwise Swapping and Diagonal Micro Attention (DMA) allow the model to focus on facial regions that primarily consist of micro differences between frames. Finally, µ-Decoder reconstructs the output signal back to the determined one. Compared to prior works, µ-BERT can adaptively focus on changes in facial regions while ignoring the ones in the background and effectively recognizes micro-expressions even when face movements occur. Moreover, µ-BERT can also alleviate the dependency on the accuracy of alignment approaches in pre-processing step.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "The Proposed Μ-Bert Approach",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Non-Overlapping Patches Representation",
      "text": "In µ-BERT, an input frame I t ∈ R H×W ×C is divided into a set of several non-overlapping patches P t as Eqn.  (1) .\n\nwhere H, W, C are the height, width, and number of channels, respectively. Each patch p i t has a resolution of ps × ps. In our experiments, H = W = 224, C = 3, and ps = 8.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Μ-Encoder",
      "text": "Each patch p i ∈ P t is linearly projected into a latent vector of dimension d denoted as z i t ∈ R 1×d , with additive fixed positional encoding  [42] . Then, an image I t can be represented as in Eqn.  (2) .\n\nwhere α and e are the projection embedding network and positional embedding, respectively. Let µ-Encoder, denoted as E, be a stack of continuous blocks. Each block consists of alternating layers of Multi Head Attention (MHA) and Multi-Layer Perceptron (MLP), as illustrated in Figure  3 .\n\nThe Layer Norm (LN) is employed to the input signal before feeding to MHA and MLP layers, as in Eqn. (3).\n\nx ′ l = x l-1 + MHA(LN(x l-1 ))\n\nwhere L e is the number of blocks in E. Given Z t , The output latent vector P t is represented as in Eqn.  (4) .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Μ-Decoder",
      "text": "The proposed auto-encoder is designed symmetrically. It means that the decoder part denoted as D, has a similar architecture to the encoder E. Given a latent vector P t , the decoded signal Q t is represented as in Eqn.  (5) .\n\nWe add one more Linear layer to interpolate Q t to an intermediate signal y t before reshaping it into the image size.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Blockwise Swapping",
      "text": "Given two frames I t and I t+δ , we realize the fact that: where p i t is the i th -patch at frame t. s denotes a function to measure the similarity between p i t and p i t+δ where a higher score indicates higher similarity and 0 ≤ s(p i t , p i t+δ ) ≤ 1. Given a patch correlation as in Eqn. (  7 ), we propose a Blockwise Swapping mechanism to (1) firstly randomly swap two corresponding blocks p i t and p i t+δ between two frames to create a swapped image I t/s , and then (2) enforce the model to spot these changes and reconstruct I t from I t/s . By doing so, the model is further strengthened in recognizing and restoring the swapped patches. As a result, the learned model can be enhanced by the capability to notice small differences between frames. Moreover, as shown in Eqn.  (7) , shorter time δ causing larger similarity between I t from I t/s can further help to enhance the robustness on spotting these differences. The detail of this strategy is described in Algorithm 1 and Figure  4 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Diagonal Micro Attention (Dma)",
      "text": "As a result of Blockwise Swapping, the image patches P t/s from I t/s consists of two types, i.e. p j t/s from P t of I t and p i t/s from P t+δ of I t+δ . Then, the next stage is to learn how to reconstruct P t from P t/s . Since p i t/s includes all changes between I t and I t/s , more emphasis is placed on p i t/s during reconstruction process. Theoretically, the ground truth of the index of p i t/s in P t/s can be utilized to enforce the model focusing on these swapped patches. However, adopting this information may reduce the learning capability to spot these microchanges. Therefore, a novel attention mechanism named Diagonal Micro-Attention (DMA) is presented to enforce the network automatically focusing on swapped patches p i t/s and equip it with the ability to precisely spot and identify all changes between images. Notice that these changes may include patches in the background. The following section introduces a solution to constrain the learned network focusing on only meaningful facial regions. The details of DMA are presented in Figure  5 . Formally, we construct an attention map Â between P t+δ and P t/s where the diag( Â) illustrates correlations between two corresponding patches p i t+δ and p i t/s . From the observation that Â(i, i) > Â(j, j) for all p i t/s ∈ P t+δ and p j t/s ∈ P t , diag( Â) can be effectively adopted as weights indicating important features. Full operations of DMA are presented in Eqn.  (8)  and Eqn.  (9) .\n\nwhere × denotes the Element-wise multiplication operator.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Patch Of Interest (Poi)",
      "text": "In Section 4.5, Diagonal Micro-Attention has been introduced to weigh the importance of swapped patches automatically. These swapped patches are randomly produced via Blockwise Swapping, as in Algorithm 1. In theory, the ideal case is when all swapped patches are located within the facial region only so that the deep network can learn the micro-movements from the facial parts solely and not be distracted by the background. In practice, however, we Algorithm 1 Blockwise Swapping Input: Pt, P t+δ image patches (Np = h×w); rs: swapping ratio (default: 0.5); min bs: minimum block size (default: 16); min ar: minimum aspect ratio (default: 0.  The POI relies on the contextual agreement between the frame I t+δ and Crop(I t+δ ). Motivated by the BERT framework, we add a Contextual Token z CT to the beginning of the sequence of patches, as in Eqn. (  2 ), to learn the contextual information in the image. The deeper this token passes through the Transformer blocks, the more information is accumulated from z i t ∈ P t . As a result, z CT becomes a placeholder to store the information extracted from other patches in the sequence and present the contextual information of the image. Let p CT t+δ and p CT t+δ/c be the contextual features of frame I t+δ and its cropped version Crop(I t+δ ) respectively. The agreement loss is then defined as in Eqn.  (10) .\n\nwhere H is the function that enforces p CT t+δ to be similar to p CT t+δ/crop so that the model can discover the salient patches. The POI can be extracted from the attention map A at the last attention layer of encoder E. In particular, we measure:\n\nwhere Np-1 i=0 s i t+δ = 1. The higher the score s i t+δ , the richer the patch contains contextual information. Now, Eqn. (9) can be reformulated as in Eqn.  (12) .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Loss Functions",
      "text": "The proposed µ-BERT deep network is optimized using the proposed loss function as in Eqn.  (13) .\n\nwhere γ and β are the weights for each loss. Reconstruction Loss. The output of the decoder y ′ t is reconstructed to the original image I t using the Mean Square Error (MSE) function.\n\nContextual Agreement Loss. MSE is also used to enforce the similarity of contextual features of I t+δ/crop and I t+δ",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets And Protocols",
      "text": "CASME II  [50] . With a 200 fps sampling rate and a facial resolution of 280 × 340, CASME II provides 247 microexpression samples from 26 subjects of the same ethnicity. Labels include apex frames, action units, and emotions. SAMM  [6] . Also, using a 200 fps frame rate and a facial resolution of 400 × 400, SAMM consists of 159 samples from 32 participants and 13 ethnicities. The samples all have emotions, apex frames, and action unit labels. SMIC  [15] . SIMC is made up of 164 samples. Lacking apex frame and action unit labels, the samples span 16 participants of 3 ethnicities. The recordings are taken with a resolution of 640 × 480 at 100 fps. CASME3  [14] . Officially known as CAS(ME) 3 provides 1,109 labeled micro-expressions and 3,490 labeled macroexpressions. This dataset has roughly 80 hours of footage with a resolution of 1280 × 720.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Micro-Expression Self-Training",
      "text": "We use all raw frames from CASME3 for self-training except frames of test set. It is important to note that we do not use labels or meta information such as onset, offset, and apex index frames nor labeled emotions. In total, we construct an unlabelled dataset of 8M frames. The images are resized to 224 × 224. Then, each image is divided into patches of 8 × 8, yielding N p = 784 patches. The temporal index δ is selected randomly between a lower bound of 5 and an upper bound of 11, experimentally. The swapping ratio r s is selected as 50% of the number of patches being swapped from I t+δ to I t . Each patch is projected to a latent space of d = 512 dimensions before being fed into the encoder and decoder. For the encoder and decoder, we keep the same d for all vectors and similar configurations, i.e., L e = L d = 4. µ-BERT is implemented easily in Pytorch framework and trained by 32 × A100 GPUs (40G each). The learning rate is set to 0.0001 initially and then reduced to zero gradually under ConsineLinear  [24]  policy. The batch size is set to 64/GPU. The model is optimized by AdamW  [25]  for 100 epochs. The training is completed within three days.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Micro-Expression Recognition",
      "text": "We leverage the pretrained µ-BERT as an initial weight and take the encoder E and DMA module of µ-BERT as the MER backbone. The input of MER is the onset and apex frames which correspond to I t and I t+δ respectively. In Eqn. (  8 ), P dma are the features representing the micro changes and movements between onset and apex frames. They can be effectively adopted for recognizing microexpressions. We adopt the standard metrics and protocols of MER2019 challenge  [34]  with the unweighted F1 score\n\nNi , where C is the number of MEs, N i is the total number of i th ME in the dataset. Leave-one-out crossvalidation (LOOCV) scheme is used for evaluation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "Our proposed µ-BERT shows a significant improvement over prior methods and baselines, as shown in Table 1 on the CASME3. Tested using 3, 4, and 7 emotion classes, µ-BERT achieves double-digit gains over the compared methods in each category. In the case of 3 emotion classes, µ-BERT achieved a 56.04% UF1 score and 61.25% UAR, compared to RCN-A's  [49]  39.28% UF1 and 38.93% UAR. For 4 emotion classes, µ-BERT outperforms Baseline (+Depth)  [14]  47.18% to 30.01% for UF1 and 49.13% to 29.82% for UAR. Large gains over Baseline (+Depth)  [14]  are seen in the case of 7 emotion classes, where µ-BERT attains UF1 and UAR scores of 32.64% and 32.54% respectively, compared to 17.73% and 18.29% for the baseline.\n\nTable  2  details results for CASMEII. µ-BERT shows improvements over all other methods. For three categories, it achieves a UF1 of 90.34% and UAR of 89.14%, representing 3.37% and 0.86% increases over the prior leading method (OFF-ApexNet  [8] ), respectively. Similar improvement is seen in five categories: a 4.83% over TSCNN  [35]  in terms of UF1 and a 0.89% increase over SMA-STN  [19]  for UAR. Similarly, µ-BERT performs competitively with other methods on the SAMM as seen in Table  3 . Using 5 emotion classes, µ-BERT outperforms MinMaNet  [47]  by a large margin in terms of UF1 (83.86% vs 76.40%) and UAR (84.75% vs 76.70%), respectively. The performance of µ-BERT on SMIC is compared against several others in Table  4 . µ-BERT outperforms others with a 7.5% increase in UF1 to 85.5% and a 3.97% boost in UAR to 83.84%.\n\nOn the composite dataset, µ-BERT again outperforms  other methods (Table  5 ). Attaining a UF1 score of 89.03% and UAR of 88.42%, µ-BERT realizes 0.73%, and 0.82% gains over previous best MiMaNet  [47] , respectively. Table  6  shows the impact of DMA and POI on CASME3. Our",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "How Μ-Bert Perceives Micro-Movements",
      "text": "To understand the micro-movements between two frames, the onset and apex frames are inputs for µ-BERT. These frames represent the moments that the microexpression starts and is observed. We measure diag( Â) (Subsection 4.5) and S t+δ (Eqn (  11 )) values to identify which regions contain small movements between two frames. Comparisons of µ-BERT with RAFT  [37] , i.e., optical flow-based method and MagNet  [29]  are also conducted as in Fig.  7 . The third and fourth columns in Fig  7  show the results of RAFT  [37] , and MagNet  [29]  on spotting the micro-movements, respectively. While RAFT is an optical flow-based method, MagNet amplifies small differences between the two frames. These methods are sensitive to the environment (e.g., lighting, illuminations). Thus, noises in the background still exist in their outputs. In addition, neither RAFT nor MagNet understand semantic information in the frame and distinguish changes inside facial or background regions. Meanwhile, µ-BERT shows its advantages in perceiving micro-movements via distinguishing the facial regions and spotting the micro-expressions. In particular, the attention map in the fifth column, in Fig.  7  illustrates the micro-differences between onset and apex frames. The higher contrast represents the higher chance of small movements in these regions. With the POI module, µ-BERT can automatically figure out the informative patches and ignore the background ones. Then, with DMA module, µ-BERT, can detect and localize which corresponding patches/regions contain tiny movements. As shown in the seventh column, attention maps represent the most salient regions in the image. By empowering DMA and POI, µ-BERT effectively identifies micro-movements within facial regions, as demonstrated in the last column.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Studies",
      "text": "This section compares µ-BERT against other selfsupervised learning (SSL) methods on the MER task. CASME3 is used for experiments since it has many unlabelled images to demonstrate the power of SSL methods. We also analyze the essential contributions of Diagonal Micro-Attention (DMA) and Patch of Interest (POI) modules. Finally, we illustrate the robustness of µ-BERT pretrained on CASME3 on unseen datasets and domains.\n\nComparisons with self-supervised learning methods. We utilize the encoder and decoder parts of µ-BERT (without DMA and POI) to train previous SSL methods (MoCo V3  [4] , BEIT  [1] , MAE  [9] ) and then continue learning the MER task on the large-scale database CASME3  [14] .\n\nOverall results are shown in Table  6  The role of DMA. This module is the guide to tell the network where to look and which patches to focus. By doing so, the µ-BERT gets more robust knowledge of micro-",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusions And Discussions",
      "text": "Unlike a few concurrent research on micro-expression, we move forward and study how to explore BERT pretraining for this problem. In our proposed µ-BERT, we presented a novel Diagonal Micro Attention (DMA) to learn the micro-movements of the subject across frames. The Patch of Interest (POI) module is proposed to guide the network, focusing on the most salient parts, i.e., facial regions, and ignoring the noisy sensitivities from the background. Empowered by the simple design of µ-BERT, SOTA performance on micro-expression recognition tasks is achieved in four benchmark datasets. Our perspective will inspire more future study efforts in this direction. Limitations. We demonstrated the efficiency of the POI module in removing noise in the background, which is sensitive to lighting and illumination. However, suppose any facial parts, e.g., the forehead, are affected by lighting conditions while there are no movements. In that case, these noisy factors might also be included as micro-difference features. The robustness with different lighting conditions will be left as our future works.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Given two frames from a high-speed video, the proposed",
      "page": 1
    },
    {
      "caption": "Figure 2: An overview of the proposed µ-BERT approach to facial micro-expression recognition.",
      "page": 3
    },
    {
      "caption": "Figure 3: Builing block of Encoder and Decoder. Each block",
      "page": 3
    },
    {
      "caption": "Figure 3: The Layer Norm (LN) is employed to the input signal be-",
      "page": 3
    },
    {
      "caption": "Figure 4: Blockwise Swapping. For each triplet, we present the",
      "page": 4
    },
    {
      "caption": "Figure 4: 4.5. Diagonal Micro Attention (DMA)",
      "page": 4
    },
    {
      "caption": "Figure 5: Formally, we construct an attention",
      "page": 4
    },
    {
      "caption": "Figure 5: Diagonal Micro-Attention (DMA) module. Diagonal",
      "page": 4
    },
    {
      "caption": "Figure 6: Patch of Interest (POI) module. The Pt and Pt+δ/c",
      "page": 5
    },
    {
      "caption": "Figure 7: We demonstrate how µ-BERT perceives the tiny differences between two frames. The first two rows are onset and apex input",
      "page": 6
    },
    {
      "caption": "Figure 7: The third and fourth columns in Fig 7",
      "page": 7
    },
    {
      "caption": "Figure 7: illustrates the micro-differences between onset and apex",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 2: details results for CASMEII. µ-BERT shows im-",
      "page": 6
    },
    {
      "caption": "Table 4: µ-BERT outperforms others with a 7.5% increase",
      "page": 6
    },
    {
      "caption": "Table 1: MER on the CASME3 dataset.",
      "page": 7
    },
    {
      "caption": "Table 2: MER on CASME II dataset.",
      "page": 7
    },
    {
      "caption": "Table 3: MER on SAMM dataset.",
      "page": 7
    },
    {
      "caption": "Table 5: ). Attaining a UF1 score of 89.03%",
      "page": 7
    },
    {
      "caption": "Table 6: shows the impact of DMA and POI on CASME3. Our",
      "page": 7
    },
    {
      "caption": "Table 4: MER on SMIC dataset.",
      "page": 7
    },
    {
      "caption": "Table 5: MER on the Composite dataset (MECG2019).",
      "page": 8
    },
    {
      "caption": "Table 6: It is expected that",
      "page": 8
    },
    {
      "caption": "Table 6: MER performance on CASME3 by different self-",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "BEiT: BERT pretraining of image transformers",
      "authors": [
        "Hangbo Bao",
        "Li Dong",
        "Furu Wei"
      ],
      "year": "2008",
      "venue": "BEiT: BERT pretraining of image transformers"
    },
    {
      "citation_id": "2",
      "title": "Video-based facial micro-expression analysis: A survey of datasets, features and algorithms",
      "authors": [
        "Xianye Ben",
        "Yi Ren",
        "Junping Zhang",
        "Su-Jing Wang",
        "Kidiyo Kpalma",
        "Weixiao Meng",
        "Yong-Jin Liu"
      ],
      "venue": "Video-based facial micro-expression analysis: A survey of datasets, features and algorithms"
    },
    {
      "citation_id": "3",
      "title": "Qing-Qiang Wu, and Jun-Feng Yao. Block division convolutional network with implicit deep features augmentation for micro-expression recognition",
      "authors": [
        "Bin Chen",
        "Kun-Hong Liu",
        "Yong Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "4",
      "title": "An empirical study of training self-supervised vision transformers",
      "authors": [
        "Xinlei Chen",
        "Saining Xie",
        "Kaiming He"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "5",
      "title": "Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications",
      "authors": [
        "Adrian Ciprian",
        "Marc Corneanu",
        "Jeffrey Oliu Simón",
        "Sergio Cohn",
        "Guerrero"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Samm: A spontaneous micro-facial movement dataset",
      "authors": [
        "Adrian Davison",
        "Cliff Lansley",
        "Nicholas Costen",
        "Kevin Tan",
        "Moi Hoon"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "8",
      "title": "Off-apexnet on micro-expression recognition system",
      "authors": [
        "Y Gan",
        "S Liong",
        "W Yau",
        "Y Huang",
        "L Tan"
      ],
      "year": "2019",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "9",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr Dollár",
        "Ross Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Micro-expression classification based on landmark relations with graph attention convolutional network",
      "authors": [
        "Ankith Jain",
        "Rakesh Kumar",
        "Bir Bhanu"
      ],
      "year": "2007",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "11",
      "title": "Microexpression recognition based on facial graph representation learning and facial action unit fusion",
      "authors": [
        "Ling Lei",
        "Tong Chen",
        "Shigang Li",
        "Jianfeng Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "12",
      "title": "Microexpression recognition based on facial graph representation learning and facial action unit fusion",
      "authors": [
        "Ling Lei",
        "Tong Chen",
        "Shigang Li",
        "Jianfeng Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "13",
      "title": "A novel Graph-TCN with a graph structured representation for micro-expression recognition",
      "authors": [
        "Ling Lei",
        "Jianfeng Li",
        "Tong Chen",
        "Shigang Li"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Cas(me)¡sup¿3¡/sup¿: A third generation facial spontaneous micro-expression database with depth information and high ecological validity",
      "authors": [
        "Jingting Li",
        "Zizhao Dong",
        "Shaoyuan Lu",
        "Su-Jing Wang",
        "Wen-Jing Yan",
        "Yinhuan Ma",
        "Ye Liu",
        "Changbing Huang",
        "Xiaolan Fu"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "15",
      "title": "A spontaneous micro-expression database: Inducement, collection and baseline",
      "authors": [
        "Xiaobai Li",
        "Tomas Pfister",
        "Xiaohua Huang",
        "Guoying Zhao",
        "Matti Pietikäinen"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "16",
      "title": "Microexpression action unit detection with spatial and channel attention",
      "authors": [
        "Yante Li",
        "Xiaohua Huang",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "17",
      "title": "Shallow triple stream three-dimensional cnn (ststnet) for microexpression recognition",
      "authors": [
        "S Liong",
        "Y Gan",
        "J See",
        "H Khor",
        "Y Huang"
      ],
      "year": "2019",
      "venue": "Gesture Recognition (FG)"
    },
    {
      "citation_id": "18",
      "title": "Shallow triple stream threedimensional cnn (ststnet) for micro-expression recognition",
      "authors": [
        "Sze-Teng Liong",
        "Yee Gan",
        "John See",
        "Huai-Qian",
        "Yen-Chang Khor",
        "Huang"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE international conference on automatic face & gesture recognition (FG 2019)"
    },
    {
      "citation_id": "19",
      "title": "SMA-STN: Segmented movement-attending spatiotemporal network for micro-expression recognition",
      "authors": [
        "Jiateng Liu",
        "Wenming Zheng",
        "Yuan Zong"
      ],
      "year": "2020",
      "venue": "SMA-STN: Segmented movement-attending spatiotemporal network for micro-expression recognition",
      "arxiv": "arXiv:2010.09342"
    },
    {
      "citation_id": "20",
      "title": "A neural micro-expression recognizer",
      "authors": [
        "Yuchi Liu",
        "Heming Du",
        "Liang Zheng",
        "Tom Gedeon"
      ],
      "year": "2019",
      "venue": "Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "21",
      "title": "Lightweight vit model for microexpression recognition enhanced by transfer learning. Frontiers in Neurorobotics",
      "authors": [
        "Yanju Liu",
        "Yange Li",
        "Xinhai Yi",
        "Zuojin Hu",
        "Huiyu Zhang",
        "Yanzhong Liu"
      ],
      "year": "2022",
      "venue": "Lightweight vit model for microexpression recognition enhanced by transfer learning. Frontiers in Neurorobotics"
    },
    {
      "citation_id": "22",
      "title": "Swin transformer v2: Scaling up capacity and resolution",
      "authors": [
        "Ze Liu",
        "Han Hu",
        "Yutong Lin",
        "Zhuliang Yao",
        "Zhenda Xie",
        "Yixuan Wei",
        "Jia Ning",
        "Yue Cao",
        "Zheng Zhang",
        "Li Dong"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Ze Liu",
        "Yutong Lin",
        "Yue Cao",
        "Han Hu",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "Baining Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "24",
      "title": "Sgdr: Stochastic gradient descent with warm restarts",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2016",
      "venue": "Sgdr: Stochastic gradient descent with warm restarts",
      "arxiv": "arXiv:1608.03983"
    },
    {
      "citation_id": "25",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "26",
      "title": "Clusformer: A transformer based clustering approach to unsupervised large-scale face and visual landmark recognition",
      "authors": [
        "Xuan-Bac Nguyen",
        "Duc Toan Bui",
        "Chi Duong",
        "Tien Bui",
        "Khoa Luu"
      ],
      "venue": "Clusformer: A transformer based clustering approach to unsupervised large-scale face and visual landmark recognition"
    },
    {
      "citation_id": "27",
      "title": "Self-supervised learning based on spatial awareness for medical image analysis",
      "authors": [
        "Xuan-Bac Nguyen",
        "Sang Lee",
        "Soo Kim",
        "Hyung Jeong"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "28",
      "title": "GEME: Dual-stream multi-task gender-based micro-expression recognition",
      "authors": [
        "Xuan Nie",
        "A Madhumita",
        "Mengyang Takalkar",
        "Haimin Duan",
        "Min Zhang",
        "Xu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "29",
      "title": "Learning-based video motion magnification",
      "authors": [
        "Tae-Hyun Oh",
        "Ronnachai Jaroensri",
        "Changil Kim",
        "Mohamed Elgharib",
        "Frédo Durand",
        "William Freeman",
        "Wojciech Matusik"
      ],
      "year": "2018",
      "venue": "Learning-based video motion magnification"
    },
    {
      "citation_id": "30",
      "title": "Non-volume preserving-based fusion to group-level emotion recognition on crowd videos",
      "authors": [
        "Gia Kha",
        "Ngan Quach",
        "Chi Le",
        "Ibsa Duong",
        "Kaushik Jalata",
        "Khoa Roy",
        "Luu"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "Micro-expression classification based on landmark relations with graph attention convolutional network",
      "authors": [
        "Ankith Jain",
        "Rakesh Kumar",
        "Bir Bhanu"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "32",
      "title": "Zero-shot text-to-image generation",
      "authors": [
        "Aditya Ramesh",
        "Mikhail Pavlov",
        "Gabriel Goh",
        "Scott Gray",
        "Chelsea Voss",
        "Alec Radford",
        "Mark Chen",
        "Ilya Sutskever"
      ],
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "33",
      "title": "Imagenet-21k pretraining for the masses",
      "authors": [
        "Tal Ridnik",
        "Emanuel Ben-Baruch",
        "Asaf Noy",
        "Lihi Zelnik-Manor"
      ],
      "year": "2021",
      "venue": "Imagenet-21k pretraining for the masses",
      "arxiv": "arXiv:2104.10972"
    },
    {
      "citation_id": "34",
      "title": "Megc 2019 -the second facial microexpressions grand challenge",
      "authors": [
        "John See",
        "Hoon Moi",
        "Jingting Yap",
        "Xiaopeng Li",
        "Su-Jing Hong",
        "Wang"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "35",
      "title": "Recognizing spontaneous micro-expression using a three-stream convolutional neural network",
      "authors": [
        "B Song",
        "K Li",
        "Y Zong",
        "J Zhu",
        "W Zheng",
        "J Shi",
        "L Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "36",
      "title": "Dynamic micro-expression recognition using knowledge distillation",
      "authors": [
        "Bo Sun",
        "Siming Cao",
        "Dongliang Li",
        "Jun He",
        "Lejun Yu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Raft: Recurrent all-pairs field transforms for optical flow",
      "authors": [
        "Zachary Teed",
        "Jia Deng"
      ],
      "year": "2020",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "38",
      "title": "Training data-efficient image transformers and amp; distillation through attention",
      "authors": [
        "Hugo Touvron",
        "Matthieu Cord",
        "Matthijs Douze",
        "Francisco Massa",
        "Alexandre Sablayrolles",
        "Herve Jegou",
        "; Thuong-Khanh Tran",
        "Quang-Nhat Vo",
        "Xiaopeng Hong",
        "Xiaobai Li",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "39",
      "title": "Direcformer: A directed attention in transformer approach to robust action recognition",
      "authors": [
        "Thanh-Dat Truong",
        "Quoc-Huy Bui",
        "Chi Duong",
        "Han-Seok Seo",
        "Son Phung",
        "Xin Li",
        "Khoa Luu"
      ],
      "year": "2002",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "40",
      "title": "The right to talk: An audio-visual transformer approach",
      "authors": [
        "Thanh-Dat Truong",
        "Chi Duong",
        "The Vu",
        "Anh Hoang",
        "Bhiksha Pham",
        "Ngan Raj",
        "Khoa Le",
        "Luu"
      ],
      "year": "2002",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "41",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "42",
      "title": "Mesnet: A convolutional neural network for spotting multi-scale micro-expression intervals in long videos",
      "authors": [
        "Su-Jing Wang",
        "Ying He",
        "Jingting Li",
        "Xiaolan Fu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "43",
      "title": "Micro expression recognition via dual-stream spatiotemporal attention network",
      "authors": [
        "Yan Wang",
        "Yikun Huang",
        "Can Liu",
        "Xiaoying Gu",
        "Dandan Yang",
        "Shuopeng Wang",
        "Bo Zhang"
      ],
      "year": "2007",
      "venue": "Journal of Healthcare Engineering"
    },
    {
      "citation_id": "44",
      "title": "Effective recognition of facial microexpressions with video motion magnification",
      "authors": [
        "Yandan Wang",
        "John See",
        "Yee-Hui Oh",
        "C.-W Raphael",
        "Yogachandran Phan",
        "Huo-Chong Rahulamathavan",
        "Su-Wei Ling",
        "Xujie Tan",
        "Li"
      ],
      "year": "2016",
      "venue": "Effective recognition of facial microexpressions with video motion magnification"
    },
    {
      "citation_id": "45",
      "title": "A novel micro-expression recognition approach using attention-based magnificationadaptive networks",
      "authors": [
        "Mengting Wei",
        "Wenming Zheng",
        "Yuan Zong",
        "Xingxun Jiang",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "46",
      "title": "Micro-expression recognition enhanced by macro-expression from spatial-temporal domain",
      "authors": [
        "Bin Xia",
        "Shangfei Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "47",
      "title": "Learning from macro-expression: a micro-expression recognition framework",
      "authors": [
        "Bin Xia",
        "Weikang Wang",
        "Shangfei Wang",
        "Enhong Chen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "48",
      "title": "Revealing the invisible with model and data shrinking for composite-database micro-expression recognition",
      "authors": [
        "Zhaoqiang Xia",
        "Wei Peng",
        "Huai-Qian Khor",
        "Xiaoyi Feng",
        "Guoying Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "49",
      "title": "Casme ii: An improved spontaneous micro-expression database and the baseline evaluation",
      "authors": [
        "Wen-Jing Yan",
        "Xiaobai Li",
        "Su-Jing Wang",
        "Guoying Zhao",
        "Yong-Jin Liu",
        "Yu-Hsin Chen",
        "Xiaolan Fu"
      ],
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "50",
      "title": "Casme database: A dataset of spontaneous microexpressions collected from neutralized faces",
      "authors": [
        "Wen-Jing Yan",
        "Qi Wu",
        "Yong-Jin Liu",
        "Su-Jing Wang",
        "Xiaolan Fu"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "51",
      "title": "ICE-GAN: Identity-aware and capsule-enhanced gan for micro-expression recognition and synthesis",
      "authors": [
        "Jianhui Yu",
        "Chaoyi Zhang",
        "Yang Song",
        "Weidong Cai"
      ],
      "year": "2020",
      "venue": "ICE-GAN: Identity-aware and capsule-enhanced gan for micro-expression recognition and synthesis",
      "arxiv": "arXiv:2005.04370"
    },
    {
      "citation_id": "52",
      "title": "Feature refinement: An expression-specific feature learning and fusion method for micro-expression recognition",
      "authors": [
        "Ling Zhou",
        "Qirong Mao",
        "Xiaohua Huang",
        "Feifei Zhang",
        "Zhihong Zhang"
      ],
      "year": "2021",
      "venue": "Feature refinement: An expression-specific feature learning and fusion method for micro-expression recognition",
      "arxiv": "arXiv:2101.04838"
    },
    {
      "citation_id": "53",
      "title": "Feature refinement: An expression-specific feature learning and fusion method for micro-expression recognition",
      "authors": [
        "Ling Zhou",
        "Qirong Mao",
        "Xiaohua Huang",
        "Feifei Zhang",
        "Zhihong Zhang"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "54",
      "title": "Dual-inception network for cross-database micro-expression recognition",
      "authors": [
        "L Zhou",
        "Q Mao",
        "L Xue"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face and Gesture Recognition (FG)"
    }
  ]
}