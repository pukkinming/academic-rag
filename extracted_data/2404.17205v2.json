{
  "paper_id": "2404.17205v2",
  "title": "Two In One Go: Single-Stage Emotion Recognition With Decoupled Subject-Context Transformer",
  "published": "2024-04-26T07:30:32Z",
  "authors": [
    "Xinpeng Li",
    "Teng Wang",
    "Jian Zhao",
    "Shuyi Mao",
    "Jinbao Wang",
    "Feng Zheng",
    "Xiaojiang Peng",
    "Xuelong Li"
  ],
  "keywords": [
    "emotion recognition",
    "single-stage framework",
    "and decouple-fuse wild emotion recognition. The kid's emotion state: peace Two in One Go: Single-stage Emotion Recognition with Decoupled Subject-context Transformer ACM MM",
    "2024",
    "Melbourne",
    "Australia"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition aims to discern the emotional state of subjects within an image, relying on subject-centric and contextual visual cues. Current approaches typically follow a two-stage pipeline: first localize subjects by off-the-shelf detectors, then perform emotion classification through the late fusion of subject and context features. However, the complicated paradigm suffers from disjoint training stages and limited fine-grained interaction between subject-context elements. To address the challenge, we present a single-stage emotion recognition approach, employing a Decoupled Subject-Context Transformer (DSCT), for simultaneous subject localization and emotion classification. Rather than compartmentalizing training stages, we jointly leverage box and emotion signals as supervision to enrich subject-centric feature learning. Furthermore, we introduce DSCT to facilitate interactions between fine-grained subjectcontext cues in a \"decouple-then-fuse\" manner. The decoupled query tokens-subject queries and context queries-gradually intertwine across layers within DSCT, during which spatial and semantic relations are exploited and aggregated. We evaluate our single-stage framework on two widely used context-aware emotion recognition datasets, CAER-S and EMOTIC. Our approach surpasses two-stage alternatives with fewer parameter numbers, achieving a 3.39% accuracy improvement and a 6.46% average precision gain on CAER-S and EMOTIC datasets, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic human emotion recognition gets increasing research attention in the multimedia community, where studies include inferring emotions from speech  [67, 78] , image  [45, 74]  and multimodalities  [40, 42] . Its potential applications span across healthcare, driver surveillance, and diverse human-computer interaction systems  [8, 46, 47, 63] , reflecting the fundamental role of emotions  [11] .\n\nIn this paper, we focus on the problem of inferring the emotion of one person in a real-world image. Concretely, given an inthe-wild image, we aim to identify the subject's apparent discrete emotion categories (e.g. happy, sad, fearful, or neutral). Existing methods typically involve two stages: subject detection and emotion classification. Conventional approaches primarily emphasize facial cues  [7, 51, 60-62, 75, 82] , featuring a two-stage without fusion paradigm. As depicted in Fig.  1 (a), a standard off-the-shelf detector indicates a facial region, and a dedicated face encoder extracts facial features for subsequent classification into distinct emotional categories. Recent advances have increasingly recognized the importance of contextual cues in emotion recognition, like body language, scene semantics, and social interactions  [22, 24, 28, 37, 38, 45, 64, 65] . This system is characterized as a two-stage with late fusion paradigm. As illustrated in Fig.  1(b) , it first identifies subjects and contexts within the image, processes them through independent encoders, and fuses the resulting features for emotion prediction.\n\nWhile effective, existing approaches are hindered by two primary limitations. Firstly, the disjointed learning processes of emotion classifiers and subject detectors in a two-stage paradigm often result in inefficient computational efficiency. Illustrated in Fig.  2 , existing methods' effectiveness is limited with many parameters. Secondly, existing paradigms may exhibit a restricted capacity for subjectcontext fusion, thereby falling short in addressing real-world images that are susceptible to nuanced contextual influences  [36, 59] .\n\nSubtle cues: The kid is lying on the bed with his parents. Although the kid's facial expression is indistinguishable and the parents are excited, the nuanced subject-context cue, i.e. he is looking at the cloth, shows his emotional state is peace. Shown in Fig.  1 , the first paradigm focuses solely on facial expressions, neglecting essential contextual cues, and the second paradigm's late fusion scheme misses fine-grained subject-context interaction, leading to sub-optimal emotion recognition.\n\nTo alleviate the limitation, we introduce a single-stage framework, employing a Decoupled Subject-Context Transformer (DSCT) with early fusion, for simultaneous subject localization and emotion classification, characterized as a single-stage with early fusion paradigm. As illustrated in Fig.  1 (c), we adopt an encoder-decoder architecture with DSCT, where learnable queries are correlated with the global and multi-scale features for prediction. Rather than disjoint training stages, we jointly leverage box and emotion signals as supervision to enrich subject-centric feature learning, i.e., the framework is trained with a joint loss of classification and localization. Fig.  2  demonstrates that our method is effective and efficient, surpassing two-stage prior arts with fewer parameters.\n\nFurthermore, we introduce DSCT to facilitate interactions between fine-grained subjects and context in a decouple-then-fuse manner. As depicted in Fig.  1(c ), the queries are decomposed into subject and context queries to capture the subject's emotional signal, e.g., facial expression, and a wide range of contextual cues, e.g., body posture and gesture, agents, objects, and scene attributes. The decoupled query tokens-subject queries and context queries-gradually intertwine across layers within DSCT. For effective fusion, the spatial and semantic relations between context and subject information are exploited and aggregated. The spatial relation picks up contextual queries with short-range subject-context interaction, such as the subject between objects in hands and close agents. As complementary, the semantic relation chooses contextual queries with long-range subject-context interaction, like the subject between scene attributes and distant people. Fig.  1 (c) shows that the singlestage framework notices useful and subtle emotional cues between the subject and context, e.g. the kid is looking at the father's clothes.\n\nExtensive experiments are conducted on two standard contextaware emotion recognition benchmarks to validate the efficacy of our approach. The proposed framework attains impressive results, achieving 91.81% accuracy on the CAER-S dataset  [24]  and 37.81% mean average precision on EMOTIC  [22] . In the case of similar parameter numbers, the proposal surpasses counterparts by a substantial margin of 3.39% accuracy and 6.46% average precision on CAER-S and EMOTIC respectively. Furthermore, we provide valuable insights by visualizing network output, feature map activation, and query selection, underscoring the proposal can discern useful and nuanced emotional cues of subject and context.\n\nThe main contributions can be summarized as follows:\n\nâ€¢ We present a novel single-stage framework for simultaneous subject localization and emotion classification to address the limitations of disjoint training stages. â€¢ To facilitate fine-grained interactions between subjects and context, we introduce a new decoupled subject-context transformer to decouple and fuse queries across layers. â€¢ The spatial and semantic relations are exploited and aggregated to capture the short-range and long-range subjectcontext interaction complementarily. â€¢ Extensive experiments and visualization on two standard datasets show that the single-stage framework outperforms two-stage alternatives by a significant margin and excels in capturing useful and nuanced emotional cues.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Visual Emotion Recognition. The Visual Emotion Recognition (VER) task can be broadly categorized into two main paradigms. 1) Two-stage without fusion. Traditional methods focus on utilizing subject-centric regions while treating contextual areas as noise, as observed in various studies  [9, 27, 43, 50, 75] . The pipeline includes subject detection and emotion classification. These studies primarily address challenges associated with label uncertainty  [5, 6, 23, 34, 52, 55, 56, 66, 80, 81] , micro expressions  [41, 71] , and disentangled representations  [69, 79] . 2) Two-stage with late fusion. In recent years, the research has paid increasing attention to context-aware emotion recognition, which emphasizes the use of multiple contexts for more robust emotion classification  [22, 24, 28, 38, 39, 53, 64, 65, 77] . In addition to two-stage components, the pipeline includes multi-branch and late fusion characteristics. Typically, a multiplestream architecture, followed by a fusion network, is employed to independently encode the subject and context information. Despite the effectiveness of these methods, they suffer from disjoint training stages and limited interaction between fine-grained subject-context elements. In contrast, we present a single-stage approach with an early fusion, employing a Decoupled Subject-Context Transformer, for simultaneous subject localization and emotion classification.\n\nEnd-to-End Object Detection. The end-to-end framework with vision Transformers stirs up wind in the object detection task. DETR  [2]  streamlines object detection into one step by a set-based loss and a transformer encoder-decoder architecture. The following works have attempted to eliminate the issue of slow convergence by designing architecture  [10, 54] , query  [32, 58, 86] , and bipartite matching  [4, 25, 26, 72, 73] . The original DETR framework, along with its various adaptations, has not only brought forth a simple yet powerful end-to-end architecture for common object detection but has also been extended to other related tasks, including multipleobject tracking  [70] , action detection  [33] , human-object interaction  [19, 20] , person search  [1] , and instance segmentation  [17, 57] . We propose the adaptation and modification for VER: First, since we suggest a single-stage framework, we adopt deformable DETR for simultaneous subject localization and emotion classification; Second, as generic objects exhibit distinct and localized characteristics, but contexts are essential and nuanced-related for VER, we introduce a decoupled subject-context transformer to capture contextual interaction.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method 3.1 Single-Stage Framework",
      "text": "Current two-stage approaches for in-the-wild emotion recognition may suffer low efficiency from disjoint training stages and limited interaction between fine-grained subject-context elements. To address the limitation, we introduce a single-stage framework, employing a Decoupled Subject-Context Transformers with early fusion, for subject localization and emotion classification.\n\nArchitecture. As shown in Fig.  3 , the system handles an entire image through a CNN backbone, an encoder with deformable transformers, and a decoder with novel Decoupled Subject-Context Transformers (DSCT). Given an image, we extract multi-scale features through the backbone, flatten them in spatial dimensions, and supplement position encoding and level embeddings. The encoder subsequently encodes the global and multi-scale features through six deformable transformers. After that, the decoder correlates the given learnable queries with encoded features with six DSCTs. Finally, the Feed-Forward Networks (FFNs) transform a set of ğ‘ subject queries into ğ‘ final predictions, including emotion classes and bounding boxes. We defer to the supplementary material the detailed definition of the architecture, which follows deformable DETR  [86] . We jointly train classification and localization to enrich subject-centric feature learning. Furthermore, DSCTs facilitate fine-grained subject-context interactions by early fusion.\n\nQueries. Each learnable query is a concatenation of 256-dimension spatial and 256-dimension semantic embeddings. The spatial embedding is decoded into the 2-d normalized coordinate of the reference point and the semantic one into 1) the bounding box as relative offsets w.r.t. the reference point and 2) the corresponding emotion class of the subject. The semantic embeddings of queries adaptively integrate multi-scale image features by sampling locations around the reference points. We refer the reader to the supplementary material for detailed definitions, which follow deformable DETR  [86] . We adopt a set of ğ‘ queries for prediction, where ğ‘ is typically larger than the average subject number per image in a dataset.\n\nOptimization. We use set-level prediction  [2]  that encapsulates several predictions or ground truth within a set. For clarity, we During training, since the prediction number is larger than the actual number of subjects in an image, we first pad the set of ground truths with âˆ… to ensure a consistent size. We employ the bipartite matching  [2]  that computes one-to-one associations between the set of predictions Å· and the padded ground truths ğ‘¦:\n\nwhere Ïƒ represents the optimal assignment, ğœ âˆˆ ğ”– ğ‘ denotes a permutation of ğ‘ elements, L match ğ‘¦ ğ‘– , Å·ğœ (ğ‘– ) indicates a pair-wise matching cost between ground truth and a prediction with index ğœ (ğ‘–). L match encompasses a classification loss L cls and a box regression loss L box , expressed as:\n\nwhere ğœƒ cls , ğœƒ box âˆˆ R are hyperparameters. We efficiently compute the matching results using the Hungarian algorithm  [2] .\n\nGiven the optimal assignment Ïƒ, the training loss L is:\n\nwhere ğœ† cls , ğœ† box âˆˆ R are hyperparameters. For matching and training, we employ the focal loss  [30]  for L cls and set L box as the ğ‘™ 1 loss and generalized IoU loss  [49] .\n\nDuring inference, we set the mean of the class output logit as the score of each prediction. For multi-label tasks, subject emotions are determined with a threshold ğ‘¡:\n\nwhere ğ‘œ represents the index list of the emotion class. In the case of multi-class tasks, subject emotions are determined as:\n\nwhere ğ‘œ corresponds to the index of the emotion class. Discussion. Current emotion recognition approaches usually include two steps of localization and classification, which suffer from disjoint training stages. Therefore, we pursue a single-stage framework to simultaneously recognize the subject's bounding box and emotion class. The deformable DETR pipeline, including the above-mentioned one-stage processing and joint classification and localization loss, aligns well with our demand.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Decoupled Subject-Context Transformer",
      "text": "To facilitate interactions between fine-grained subject and contextual elements, we introduce a novel Decoupled Subject-Context Transformer (DSCT), which treats queries in a \"decouple-then-fuse\" manner and exploits spatial-semantic relational aggregation.\n\nDecouple then Fuse. Before the DSCT, the queries are decomposed into subject and context queries. As shown in Fig.  3 , we directly adopt ğ‘ subject queries and context queries as input queries of the decoder, where all queries have the same tensor size. In DSCT, both types of queries are correlated with multi-scale image features through the base deformable transformer, and then subject queries integrate context queries by spatial-sentimental relational aggregation before output. The subject and context queries are fused in an early way through all DSCT layers. As illustrated in the left section of Fig.  4 , the reference point of the subject query primarily attends to the subject area to capture the subject's emotional signal, e.g., facial expression, while the reference points of the context queries are distributed across the entire image to pick up extensive and subtle contextual cues, e.g., body posture and gesture, surrounding agents, and scene attributes like grass and sky.\n\nSpatial-Semantic Relational Aggregation. As shown in the right part of Fig.  4 , the DSCT fuses context queries based on their spatial-semantic relationships w.r.t. the subject query.\n\nThe DSCT first picks up queries with the short-range subjectcontext interaction, such as subject and objects in hands and close agents, based on the relative spatial distance. For each context query, the distance is calculated as the Euclidean distance between reference points of the context and subject queries. For the subject and context queries, we denote their coordinate vectors of the reference points as ğ‘ ğ‘› ğ‘† , where ğ‘› = 1, ..., ğ‘ and ğ‘ is the total number of the subject queries, and ğ‘ ğ‘š ğ¶ , where ğ‘š = 1, ..., ğ‘€ and ğ‘€ is the total number of the context queries. The relative spatial distance ğ‘‘ ğ‘› ğ‘š for a pair of subject and context query is computed as:\n\nThen we select ğ¾ ğ‘ ğ‘ queries of the shortest spatial distance from total ğ‘€ context queries for each subject query.\n\nAs complementary, the queries with long-range subject-context interaction, like subject and scene attributes and distant people, are chosen via semantic relevance. For each context query, the relevance is calculated as the similarity between semantic embeddings of context and subject queries. For the subject and context queries, we denote their semantic embeddings as ğ¸ ğ‘› ğ‘† , where ğ‘› = 1, ..., ğ‘ and ğ‘ is the total number of the subject queries, and ğ¸ ğ‘š ğ¶ , where ğ‘š = 1, ..., ğ‘€ and ğ‘€ is the total number of the context queries. The semantic relevance ğ‘Ÿ ğ‘› ğ‘š for is calculated as:\n\nSince the semantic embeddings are processed with image features in the same architecture, we can measure their similarity without the transforming matrices in previous methods  [28, 65] . Then we select ğ¾ ğ‘ ğ‘š queries of the smallest semantic relevance from total ğ‘€ context queries for each subject query. Finally, we adopt relevance re-weighting fusion to integrate the context queries into the subject query. We denote their semantic embeddings as ğ¸ ğ‘† and ğ¸ ğ‘˜ ğ¶ , where ğ‘˜ = 1, ..., ğ¾ ğ‘ ğ‘š + ğ¾ ğ‘ ğ‘ . The attention weight ğ‘¤ ğ‘˜ is computed by the dot product of ğ¸ ğ‘† and ğ¸ ğ‘˜ ğ¶ . Then the softmax function makes the sum of attention weights to be 1. After that, the fused contextual subject query ÃŠğ‘† is defined as:\n\nDiscussion. In the object detection task, queries can capture effective information from distinctive and localized areas. While in the task of emotion recognition, contexts have essential and nuanced influence, we introduce the novel DSCT to capture finegrained subject-context interaction, where spatial and semantic relationships are exploited and aggregated.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments 4.1 Implementations",
      "text": "We set the number of queries ğ‘ to 4 and 9 for CAER-S  [24]  and EMOTIC  [22] . We set ğ‘ to 4 and 9. To facilitate the training, we initialize the weights of architecture and borrow 300 context queries from Deformable DETR  [86] , which was pre-trained on COCO  [31] . Our batch size is 32, and we set hyperparameters ğœƒ box , ğœ† box , ğœƒ cls , and ğœ† cls to 5, 5, 2, and 5, respectively. For evaluation, we first use nonmax-suppression to remove the duplicate subjects and then select the subject that exhibits the highest bounding box overlap with the ground truth. The experiments were conducted using 8 GPUs of the NVIDIA Tesla A6000. We show details about architectural configurations, training strategies, and preprocessing steps, which follow those outlined in  [86] , in the supplementary material.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We conducted extensive experiments on two typical and popular context-aware emotion recognition datasets in real-world scenarios, namely the CAER-S  [24]  and EMOTIC  [22] .\n\nThe CAER-S dataset consists of 70,000 images, randomly divided into training (70%), validation (10%), and testing (20%) sets. Annotations include face bounding boxes and multi-class emotion labels. The dataset encompasses seven emotion categories: Surprise, Fear, Disgust, Happiness, Sadness, Anger, and Neutral. Performance on this dataset is measured using overall accuracy (acc)  [24] .\n\nThe EMOTIC dataset  [22]  contains a total number of 23,571 images and 34,320 annotated agents, which are randomly split into training (70%), validation (10%), and testing (20%) sets. Annotations include body and head bounding boxes, as well as multi-label emotion categories. EMOTIC encompasses 26 emotion categories: Affection, Anger, Annoyance, Anticipation, Aversion, Confidence, Disapproval, Disconnection, Disquietment, Doubt/Confusion, Embarrassment, Engagement, Esteem, Excitement, Fatigue, Fear, Happiness, Pain, Peace, Pleasure, Sadness, Sensitivity, Suffering, Surprise, Sympathy, Yearning. Performance on EMOTIC is evaluated based on the mean Average Precision (mAP) for all classes  [22] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Quantitative And Qualitative Results",
      "text": "The performance of various methods on CAER-S and EMOTIC datasets is presented in Table  1  and Table 2 . To facilitate a fair comparison, we categorize the methods into two groups based on the number of parameters: similar-parameter measures and larger-parameter ones, using a threshold of 100 Million (M) parameters. For the methods without released code, we count their parameters through their backbone configuration in paper and present the details in the rightmost column. Subscripts in Emoti-Con  [38]  and HECO  [65]  correspond to specific context modalities as mentioned in their respective papers. The performance of the methods is sourced from their original papers or re-implemented results of other papers. Our proposal framework outperforms similarparameter methods by a notable margin, achieving a significant 3.39% improvement on CAER-S and an impressive 6.46% boost on EMOTIC. Notably, the proposal even surpasses larger-parameter approaches, underscoring its effectiveness and efficiency for emotion recognition when compared to two-stage methods.\n\nWe present the qualitative results in Fig.  5  and Fig.  6 , depicting the bounding boxes and emotion classes output by our proposal framework, alongside those produced by the EMO-Net  [22] , a representative two-stage late-fusion method. To enhance the clarity of the proposal's output, we include visual indicators for subject queries' reference points and sampling locations. Outputs of different subjects are color-coded for differentiation. These visualizations illustrate that the proposal consistently yields high-quality results, showcasing superior classification accuracy when compared to EMO-Net. In EMOTIC, the EMO-Net might neglect subtle emotions like \"Pain\", or produce wrong even opposite emotions like \"Disapproval\". In CAER-S, when the facial expression is not distinguishing, the EMO-Net is confused with \"Anger\" and \"Neutral\", \"Happy\" and \"Surprise\", or \"Surprise\" and \"Fear\".",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Visualization And Analysis",
      "text": "Classification and localization. We conducted experiments to fine-tune ğœƒ cls and ğœ† cls on the EMOTIC dataset  [22]  and keep ğœƒ box = 5 and ğœ† box = 5. The results of these hyperparameter experiments are thoughtfully presented in Table  3 . Notably, the most compelling performance is achieved when ğœƒ cls : ğœƒ box is set to 2:5, and ğœ† cls : ğœ† box is set to 5:5. When the ratio of classification and localization coefficients raises to 3:1, the performance drops significantly by 1.25%.   We can see adding appropriate localization loss can boost classification performance and facilitate subject-centric feature learning. Besides, we noticed that ğœƒ cls has minimal impact on performance, whereas ğœ† cls significantly influences the results. Feature map activation. We visualize feature map activation of methods of different paradigms. We select EMO-Net  [22]  as a representative two-stage method. For fairness, we re-implement  EMO-Net using ResNet50 as the backbone to achieve a similar model complexity to the proposal (referred to as EMO-Net-R50). The selected feature maps originate from the final layer of the ResNet50 backbone. In Fig.  7 , a clear distinction emerges: EMO-Net-R50 exhibits a tendency to emphasize a few large regions, while the proposal consistently places importance on smaller, more intricate areas. This observation suggests that the proposal excels in the precise handling of fine-grained subject-context cues compared to conventional two-stage methods of coarse-grained cues.\n\nSampling positions of queries We visualize the reference point (blue star) and feature sampling positions (grey circle) of the normal subject queries and contextual subject queries of the DSCT in Fig.  8 . For the normal subject query, the sampling positions only rely on its own sampling points  [86] . For the contextual subject query, the sampling positions rely on both the sampling points of the subject query and context queries. We can see the sampling positions of normal subject queries only cover the subject area while the contextual ones are densely distributed across the image. The quantitative result shows the contextual query of DSCT outperforms the normal one with a 0.71% precision improvement. The gain can be attributed to aggregating extensive contextual cues, which are essential for in-the-wild emotion recognition. Multiple subjects. We conduct an evaluation on images with varying subject numbers. We select EMO-Net-R50 and EMO-Net-R50-M, which further masks subjects in the context  [24] , for comparison. Table  4  presents the performance on EMOTIC for images with different subject numbers. As the subject number in an image increases, the complexity of subject-context interaction also rises. Notably, the proposal maintains stable performance with increasing subject numbers, while EMO-Net-R50's performance deteriorates. This observation verifies our early fusion proposal can handle complex interactions better than late fusion two-stage methods.",
      "page_start": 5,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "Subject Query Number. We conduct experiments to investigate the impact of query number setting. Table  5  presents the performance of different query numbers. Fig.  9  offers a visualization of Query number visualization  bounding boxes (colorful rectangles), reference points (red circles), and sampling locations (colorful circles) corresponding to different subject query numbers. Table  5  shows the proposal achieves the best result on EMOTIC and CAER-S when the query number is 4 and 9 respectively. Notably, we observe a consistent performance stability trend as the query number increases, ranging from 1 to 6 on EMOTIC and from 1 to 10 on CAER-S. Fig.  9  also indicates that different subject queries attend to separate subject areas.\n\nComponents of DSCT. We assess the impact of components of DSCT on the EMOTIC dataset  [22] , and the results are displayed in Table  6 . The categories include \"baseline\" (only subject queries), \"Decouple-fuse\" (decouple queries and then fuse), \"Spatial\" (select ğ¾ ğ‘ ğ‘ context queries with shortest spatial distance), and \"Semantic\" (select ğ¾ ğ‘ ğ‘’ context queries with semantic relevance). As we can see, the DSCT enhances performance by 0.71%, highlighting the importance of sufficient contextual interaction and fusion. Specifically, decoupling and fusing context queries improve performance by 0.18%, and selecting context queries based on spatial and semantic relation boosts the result by an extra 0.56% and 0.12%. The results demonstrate the effectiveness of each proposed component.\n\nSelection of Spatial and Semantic Relation. We conduct experiments with varying values of ğ¾ ğ‘ ğ‘ and ğ¾ ğ‘ ğ‘’ on EMOTIC  [22]  to evaluate the sensitivity of Spatial and Semantic Relation parameters. As shown in Figure  11 , the optimal performance is achieved when ğ¾ ğ‘ ğ‘ is set to 100. The best performance is 0.41% higher than the one when ğ¾ ğ‘ ğ‘ is 300. This observation suggests that not all contextual information is valuable for effective emotion recognition. The selection of the 100 closest context queries w.r.t. the subject   Figure  11 : The evaluation of ğ¾ ğ‘ ğ‘ and ğ¾ ğ‘ ğ‘’ on EMOTIC  [22]  query also aligns with gradual interaction decay in  [65] . Figure  10  shows spatial relational selection keeps contextual cues such as objects in hands and close agents for short-range interactions. Besides, the optimal performance is achieved when ğ¾ ğ‘ ğ‘’ is set to 50, which is 0.39% higher than the one when ğ¾ ğ‘ ğ‘’ is 300. This suggests that some contextual information is a disturbance. Figure  10  shows semantic relational selection preserves contextual signals like scene attributes and distant people for long-range interactions. We can also see two selections are complementary from Figure  10 .\n\nRe-weighting Strategy. We conduct experiments of re-weighting strategies for context query fusion on the EMOTIC dataset. The results, shown in Table  7 , indicate the performance under different re-weighting strategies: Semantic (semantic relevance weights based on semantic relation w.r.t the subject query), Equal (equal weights), Spatial (spatial distance weights based on spatial relation w.r.t the subject query), and Attentive (attentive weights learned from the subject query through a linear layer). The findings reveal that the best performance is achieved when employing semantic reweighting, which performs better than equal re-weighting by 0.53%. The observation aligns well with previous studies that contexts have variant contributions to emotion recognition  [28, 65]  Fusion location. We conduct experiments of different locations for subject-context fusion on the EMOTIC dataset and show the results in Table  8 . The decoder has six layers, and we perform the query fusion in different layers. The best performance is achieved when fusing in the 1st to 6th layers, which exceeds fusing only in the 6th layer by 0.57%. It can be explained by that early fusion can capture fine-grained subject-context interaction to boost performance. The observation aligns with the psychological studies  [36, 59] . Feature Extractor. To evaluate the influence of feature extractors, we conducted experiments with various backbone architectures for the proposal on both EMOTIC and CAER-S datasets. The results, as depicted in Table  9 , include performance metrics and corresponding parameter counts. Specifically, \"R\" refers to ResNet  [15] , and \"WR\" designates Wide ResNet  [68] . For ResNet50, we utilized a pre-trained backbone from deformable DETR as the initialization. The optimal performance on EMOTIC is attained when using ResNet-101 as the backbone, while on CAER-S, ResNet-50 yields the best results. Interestingly, it's evident that the relationship between performance and parameter counts is not linear on both datasets. Moreover, the performance on CAER-S seems to be significantly influenced by the choice of pre-trained initialization. DETR-like Architecture. The family of DETR-like architectures has gained significant momentum in the object detection task. To assess the impact of incorporating different DETR-like architectures into proposal, we conducted experiments on the CAER-S dataset by leveraging the detrex platform  [48] . For equitable comparisons, all architectures utilize ResNet50 as the backbone. The results, summarized in Table  10 , reveal the performance of proposal with various DETR-like architectures. Notably, there is a performance gap of around 1% between DETR-based and deformable DETR-based architectures. However, performances remain comparable within DETR-based and deformable DETR-based architectures even though they adopt different techniques.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Architecture",
      "text": "Acc % Architecture Acc % DETR  [2]  89.74 DINO  [73]  89.27 Anchor-DETR  [58]  90.42 Deformable DETR  [86]  91.81 DAB-DETR  [10]  87.49 DAB-D-DETR  [10]  91.39 DN-DETR  [25]  89.79 H-D-DETR  [18]  91.65 Conditional-DETR  [35]  90.07 DETA  [44]  91.77 Table  10 : Ablation study on different DETR architectures.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces a single-stage visual emotion recognition framework with Decoupled Subject-Context Transformers (DSCT). The proposal predicts subjects' emotions and locations simultaneously and processes subject and context emotional cues by early fusion. We evaluate our single-stage framework on two widely used context-aware emotion recognition datasets, CAER-S and EMOTIC. Our approach surpasses two-stage alternatives with fewer parameter numbers, achieving a 3.39% accuracy improvement and a 6.46% average precision gain on CAER-S and EMOTIC datasets, respectively. We observe that the joint training of localization and classification can facilitate subject-centric feature learning. Besides, we find that early fusion improves handling the fine-grained subjectcontext interaction, e.g. multiple subjects in one scene. We also explore the spatial and semantic relationships between subject and contextual cues for more effective interaction and fusion.\n\nA PRELIMINARIES Sampling Locations. The core of deformable attention is to reduce computation cost by attending to a small set of key sampling points of spatial locations around a reference point. Given a multi-scale input feature map {ğ‘¥ ğ‘™ } ğ¿ ğ‘™=1 where ğ‘¥ ğ‘™ âˆˆ R ğ¶ Ã—ğ» ğ‘™ Ã—ğ‘Š ğ‘™ , the ğ¾ sampling locations for each attention head and each feature level are generated from the semantic embedding of each query element ğ‘§ ğ‘ âˆˆ R ğ¶ . Because the direct prediction of coordinates of sampling location is difficult to learn, it is formulated as a prediction of a reference point ğ‘Ÿ ğ‘ âˆˆ [0, 1] 2 along with ğ¾ sampling offsets Î”ğ‘Ÿ ğ‘ âˆˆ R ğ‘€ Ã—ğ¿Ã—ğ¾ Ã—2 . So, the ğ‘˜ th sampling location at ğ‘™ th feature level and ğ‘š th attention head for query element ğ‘§ ğ‘ is defined by ğ‘ ğ‘šğ‘™ğ‘ğ‘˜ = ğœ™ ğ‘™ (ğ‘Ÿ ğ‘ ) + Î”ğ‘Ÿ ğ‘šğ‘™ğ‘ğ‘˜ where ğœ™ ğ‘™ (â€¢) is a function for rescaling the coordinate of reference point to the input feature map of the ğ‘™ th level.\n\nDeformable Attention Module. Given a multi-scale input feature map {ğ‘¥ ğ‘™ } ğ¿ ğ‘™=1 , the multi-scale deformable attention ğ‘“ ğ‘šğ‘  ğ‘ = MSDeformAttn(ğ‘§ ğ‘ , ğ‘ ğ‘ , {ğ‘¥ ğ‘™ } ğ¿ ğ‘™=1 ) for query element ğ‘§ ğ‘ is calculated using a set of predicted sampling locations ğ‘ ğ‘ as follows:\n\nwhere ğ‘™, ğ‘˜ and ğ‘š index the input feature level, the sampling location and the attention head, respectively, while ğ´ ğ‘šğ‘™ğ‘ğ‘˜ indicates an attention weight for the ğ‘˜ ğ‘¡â„ sampling location at the ğ‘™ ğ‘¡â„ feature level and the ğ‘š ğ‘¡â„ attention head. Î¦ ğ‘šğ‘™ğ‘ğ‘˜ means the sampled ğ‘˜ th key element at ğ‘™ th feature level and ğ‘š th attention head using the sampling location, which is obtained by bilinear interpolation as Î¦ ğ‘šğ‘™ğ‘ğ‘˜ = ğ‘¥ ğ‘™ (ğ‘ ğ‘šğ‘™ğ‘ğ‘˜ ) = ğ‘¥ ğ‘™ (ğœ™ ğ‘™ (ğ‘Ÿ ğ‘ ) + Î”ğ‘Ÿ ğ‘šğ‘™ğ‘ğ‘˜ ). ğ‘Š ğ‘š and ğ‘Š â€² ğ‘š serve as learnable embedding parameters for the ğ‘š th attention head, and ğ´ğ‘šğ‘™ğ‘ğ‘˜ is normalized such that ğ‘˜,ğ‘™ ğ´ ğ‘šğ‘™ğ‘ğ‘˜ = 1.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B Detailed Architecture",
      "text": "Encoder. We employ the multi-scale deformable attention module in place of the standard encoder layer. In accordance with  [86] , the encoder both takes in and produces multi-scale feature maps with matching resolutions. Within the encoder, we derive multi-scale feature maps {ğ‘¥ ğ‘™ } ğ¿-1 ğ‘™=1 (ğ¿ = 4) from the output feature maps of stages ğ¶ 3 to ğ¶ 5 in ResNet  [15]  (modified by a 1 Ã— 1 convolution). Each ğ¶ ğ‘™ has a resolution 2 ğ‘™ lower than the original image. The lowest resolution feature map ğ‘¥ ğ¿ is acquired through a 3 Ã— 3 convolution with a stride of 2 on the final ğ¶ 5 stage, labeled as ğ¶ 6 . All multiscale feature maps consist of ğ¶ = 256 channels. To determine the feature level of each query pixel, we introduce a scale-level embedding, referred to as ğ‘’ ğ‘™ , to the feature representation, in addition to the positional embedding. Unlike the positional embedding with predetermined encodings, the scale-level embeddings {ğ‘’ ğ‘™ } ğ¿ ğ‘™=1 are initialized randomly and trained alongside the network.\n\nDecoder. In our approach, we employ the Decoupled Subject-Context Transformer (DSCT) across all decoder layers. Our methodology encompasses three key components: Deformable Attention, Self-Attention Modules, and Spatial-Semantic Relational Aggregation. Deformable attention facilitates the extraction of features from feature maps, self-attention modules enable queries to interact with each other, while spatial-semantic relational aggregation exploits spatial-semantic relationships for the fusion of subject and context.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C More Implementation Details",
      "text": "ImageNet  [12]  pre-trained ResNet-50  [15]  serves as the backbone for our ablation experiments. By default, deformable attentions utilize ğ‘€ = 8 and ğ¾ = 4. Parameters of the deformable Transformer encoder are shared across different feature levels. Training models last for 50 epochs by default, with a learning rate decay at the 40th epoch by a factor of 0.1. Similar to DETR  [2] , our models are trained using the Adam optimizer  [21] , with a base learning rate of 2Ã—10 -4 , ğ›½ 1 = 0.9, ğ›½ 2 = 0.999, and weight decay of 10 -4 . The learning rates of the linear projections, responsible for predicting query reference points and sampling offsets, undergo a 0.1 multiplication.\n\nWe incorporate scale augmentation, adjusting the size of input images so that the shortest side ranges from 480 to 800 pixels, while the longest side is at most 1333 pixels. To facilitate the learning of global relationships through encoder self-attention, we also introduce random crop augmentations during training. Specifically, there's a 0.5 probability of cropping a training image to a random rectangular patch, which is then resized to 800-1333 pixels.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "D Additional Results",
      "text": "Classification vs. Localization. We conducted experiments to fine-tune ğœ† box on the EMOTIC dataset  [22]  while maintaining ğœƒ cls , ğœƒ box , and ğœ† cls constant. The results are showcased in Table  11 , and the loss curves are depicted in Figure  12 . The optimal outcome is attained when ğœ† ğ‘ğ‘œğ‘¥ = 5, surpassing the performance achieved with ğœ† ğ‘ğ‘œğ‘¥ = 1 by 1.06% in average precision. This underscores the affirmative influence of integrating a localization loss on subject-centric feature acquisition. As illustrated in Figure  12 , the supplementary localization task enhances performance by mitigating the risk of classification over-fitting during model training. Comparison of Early Fusion and Late Fusion. We investigate the efficacy of early fusion and late fusion by conducting an evaluation on images featuring varying numbers of subjects. We employ DSCT for all layers and for the 6th layer, representing early fusion and late fusion, respectively. It is worth noting that in late fusion, the queries still incorporate multi-scale image features from the encoder. However, the impact of fusing more low-level features can be inferred by employing DSCTs for early layers of the decoder. Table  12  showcases the performance on EMOTIC (mAP",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "ğœ†",
      "text": "",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a), a standard off-the-shelf detector",
      "page": 1
    },
    {
      "caption": "Figure 1: (b), it first identifies subjects and contexts",
      "page": 1
    },
    {
      "caption": "Figure 2: , existing",
      "page": 1
    },
    {
      "caption": "Figure 1: Motivation of single-stage framework. Contexts",
      "page": 2
    },
    {
      "caption": "Figure 1: , the first paradigm focuses solely on facial ex-",
      "page": 2
    },
    {
      "caption": "Figure 1: (c), we adopt an encoder-decoder",
      "page": 2
    },
    {
      "caption": "Figure 2: demonstrates that our method is effective and efficient,",
      "page": 2
    },
    {
      "caption": "Figure 1: (c), the queries are decomposed into",
      "page": 2
    },
    {
      "caption": "Figure 2: Performance vs. model efficiency of different meth-",
      "page": 2
    },
    {
      "caption": "Figure 1: (c) shows that the single-",
      "page": 2
    },
    {
      "caption": "Figure 3: Overall architecture of our single-stage emotion recognition approach for simultaneous subject localization and",
      "page": 3
    },
    {
      "caption": "Figure 4: Illustration of the DSCT. The left figure shows the reference points of the subject (orange diamond) and context",
      "page": 4
    },
    {
      "caption": "Figure 4: , the reference point of the subject query primarily attends",
      "page": 4
    },
    {
      "caption": "Figure 4: , the DSCT fuses context queries based on their",
      "page": 4
    },
    {
      "caption": "Figure 5: and Fig. 6, depicting",
      "page": 5
    },
    {
      "caption": "Figure 5: The output visualization on EMOTIC.",
      "page": 6
    },
    {
      "caption": "Figure 6: The output visualization on CAER-S.",
      "page": 6
    },
    {
      "caption": "Figure 7: , a clear distinction emerges: EMO-Net-",
      "page": 6
    },
    {
      "caption": "Figure 8: For the normal subject query, the sampling positions only",
      "page": 6
    },
    {
      "caption": "Figure 7: The visualization of feature maps activation.",
      "page": 7
    },
    {
      "caption": "Figure 8: The visualization of the reference point (blue star)",
      "page": 7
    },
    {
      "caption": "Figure 9: offers a visualization of",
      "page": 7
    },
    {
      "caption": "Figure 9: The position visualization of subject queries.",
      "page": 7
    },
    {
      "caption": "Figure 9: also indicates that",
      "page": 7
    },
    {
      "caption": "Figure 11: , the optimal performance is achieved when",
      "page": 7
    },
    {
      "caption": "Figure 10: The position visualization of the subject query (blue star) and selected context queries (grey spots).",
      "page": 8
    },
    {
      "caption": "Figure 11: The evaluation of ğ¾ğ‘ ğ‘and ğ¾ğ‘ ğ‘’on EMOTIC [22]",
      "page": 8
    },
    {
      "caption": "Figure 10: shows spatial relational selection keeps contextual cues such",
      "page": 8
    },
    {
      "caption": "Figure 10: Re-weighting Strategy. We conduct experiments of re-weighting",
      "page": 8
    },
    {
      "caption": "Figure 12: The optimal outcome is",
      "page": 11
    },
    {
      "caption": "Figure 12: , the supplementary",
      "page": 11
    },
    {
      "caption": "Figure 12: The loss of classification and localization during training.",
      "page": 12
    },
    {
      "caption": "Figure 13: The output comparison of early and late fusion (incorrectly inferred emotions are marked in red).",
      "page": 12
    },
    {
      "caption": "Figure 12: Early fusion",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ours-R50": "Ours-R101"
        },
        {
          "Ours-R50": "MA-Net\nOurs-R18\nRRLA"
        },
        {
          "Ours-R50": "GRERN"
        },
        {
          "Ours-R50": "GNN-CNN\nEMOT-Net\nCAER-Net"
        },
        {
          "Ours-R50": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ğœƒcls\nğœ†cls": "mAP (%)",
          "5\n10\n15\n2\n2\n2\n2\n2\n2\n5\n10\n15": "35.41\n35.89\n35.41\n34.99\n34.64\n36.01"
        },
        {
          "ğœƒcls\nğœ†cls": "ğœƒcls\nğœ†cls",
          "5\n10\n15\n2\n2\n2\n2\n2\n2\n5\n10\n15": "2\n5\n10\n15\n1\n1\n2\n5\n10\n15\n10\n8"
        },
        {
          "ğœƒcls\nğœ†cls": "mAP (%)",
          "5\n10\n15\n2\n2\n2\n2\n2\n2\n5\n10\n15": "35.94\n35.00\n35.05\n34.75\n34.70\n35.45"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Set Number": "EMOTIC (mAP %)\nCAER-S (Acc %)",
          "1\n2\n3\n4\n5": "37.14\n36.71\n36.61\n36.70\n37.26\n91.78\n91.57\n91.39\n91.57\n91.47"
        },
        {
          "Set Number": "Set Number",
          "1\n2\n3\n4\n5": "6\n7\n8\n9\n10"
        },
        {
          "Set Number": "EMOTIC (mAP %)\nCAER-S (Acc %)",
          "1\n2\n3\n4\n5": "36.97\n36.03\n36.26\n35.92\n35.68\n91.52\n91.59\n91.42\n91.44\n91.81"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "K_sp\nK_se": "36.85\n35.92"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "PSTR: End-to-End One-Step Person Search With Transformers",
      "authors": [
        "Jiale Cao",
        "Yanwei Pang",
        "Muhammad Rao",
        "Hisham Anwer",
        "Jin Cholakkal",
        "Mubarak Xie",
        "Fahad Shah",
        "Shahbaz Khan"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "2",
      "title": "End-to-end object detection with transformers",
      "authors": [
        "Nicolas Carion",
        "Francisco Massa",
        "Gabriel Synnaeve",
        "Nicolas Usunier",
        "Alexander Kirillov",
        "Sergey Zagoruyko"
      ],
      "year": "2020",
      "venue": "End-to-end object detection with transformers"
    },
    {
      "citation_id": "3",
      "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
      "authors": [
        "Liang-Chieh Chen",
        "George Papandreou",
        "Iasonas Kokkinos",
        "Kevin Murphy",
        "Alan Yuille"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "4",
      "title": "Group detr: Fast training convergence with decoupled one-to-many label assignment",
      "authors": [
        "Qiang Chen",
        "Xiaokang Chen",
        "Gang Zeng",
        "Jingdong Wang"
      ],
      "year": "2022",
      "venue": "Group detr: Fast training convergence with decoupled one-to-many label assignment",
      "arxiv": "arXiv:2207.13085"
    },
    {
      "citation_id": "5",
      "title": "Label distribution learning on auxiliary label space graphs for facial expression recognition",
      "authors": [
        "Shikai Chen",
        "Jianfeng Wang",
        "Yuedong Chen",
        "Zhongchao Shi",
        "Xin Geng",
        "Yong Rui"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "6",
      "title": "Understanding and mitigating annotation bias in facial expression recognition",
      "authors": [
        "Yunliang Chen",
        "Jungseock Joo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "7",
      "title": "Towards unbiased visual emotion recognition via causal intervention",
      "authors": [
        "Yuedong Chen",
        "Xu Yang",
        "Tat-Jen Cham",
        "Jianfei Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "8",
      "title": "Fear-type emotion recognition for future audio-based surveillance systems",
      "authors": [
        "ChloÃ© Clavel",
        "Ioana Vasilescu",
        "Laurence Devillers",
        "GaÃ«l Richard",
        "Thibaut Ehrette"
      ],
      "year": "2008",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "9",
      "title": "Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications",
      "authors": [
        "Adrian Ciprian",
        "Marc Corneanu",
        "Jeffrey Oliu SimÃ³n",
        "Sergio Cohn",
        "Guerrero"
      ],
      "year": "2016",
      "venue": "TPAMI"
    },
    {
      "citation_id": "10",
      "title": "Dynamic detr: End-to-end object detection with dynamic attention",
      "authors": [
        "Xiyang Dai",
        "Yinpeng Chen",
        "Jianwei Yang",
        "Pengchuan Zhang",
        "Lu Yuan",
        "Lei Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "11",
      "title": "The expression of the emotions in man and animals",
      "authors": [
        "Charles Darwin",
        "Phillip Prodger"
      ],
      "year": "1998",
      "venue": "The expression of the emotions in man and animals"
    },
    {
      "citation_id": "12",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "Imagenet: A large-scale hierarchical image database"
    },
    {
      "citation_id": "13",
      "title": "Graph reasoning-based emotion recognition network",
      "authors": [
        "Qinquan Gao",
        "Hanxin Zeng",
        "Gen Li",
        "Tong Tong"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "14",
      "title": "Mask r-cnn",
      "authors": [
        "Kaiming He",
        "Georgia Gkioxari",
        "Piotr DollÃ¡r",
        "Ross Girshick"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "15",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "16",
      "title": "Context-aware emotion recognition based on visual relationship detection",
      "authors": [
        "Manh-Hung Hoang",
        "Soo-Hyung Kim",
        "Hyung-Jeong Yang",
        "Guee-Sang Lee"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "Istr: End-to-end instance segmentation with transformers",
      "authors": [
        "Jie Hu",
        "Liujuan Cao",
        "Yao Lu",
        "Shengchuan Zhang",
        "Yan Wang",
        "Ke Li",
        "Feiyue Huang",
        "Ling Shao",
        "Rongrong Ji"
      ],
      "year": "2021",
      "venue": "Istr: End-to-end instance segmentation with transformers",
      "arxiv": "arXiv:2105.00637"
    },
    {
      "citation_id": "18",
      "title": "Detrs with hybrid matching",
      "authors": [
        "Ding Jia",
        "Yuhui Yuan",
        "Haodi He",
        "Xiaopei Wu",
        "Haojun Yu",
        "Weihong Lin",
        "Lei Sun",
        "Chao Zhang",
        "Han Hu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Hotr: End-to-end human-object interaction detection with transformers",
      "authors": [
        "Bumsoo Kim",
        "Junhyun Lee",
        "Jaewoo Kang",
        "Eun-Sol Kim",
        "Hyunwoo J Kim"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "MSTR: Multi-Scale Transformer for End-to-End Human-Object Interaction Detection",
      "authors": [
        "Bumsoo Kim",
        "Jonghwan Mun",
        "Minchul Kyoung-Woon On",
        "Junhyun Shin",
        "Eun-Sol Lee",
        "Kim"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "21",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "22",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2019",
      "venue": "TPAMI"
    },
    {
      "citation_id": "23",
      "title": "Uncertainty-aware label distribution learning for facial expression recognition",
      "authors": [
        "Nhat Le",
        "Khanh Nguyen",
        "Quang Tran",
        "Erman Tjiputra",
        "Bac Le",
        "Anh Nguyen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "24",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "25",
      "title": "Dn-detr: Accelerate detr training by introducing query denoising",
      "authors": [
        "Feng Li",
        "Hao Zhang",
        "Shilong Liu",
        "Jian Guo",
        "Lionel Ni",
        "Lei Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Mask dino: Towards a unified transformer-based framework for object detection and segmentation",
      "authors": [
        "Feng Li",
        "Hao Zhang",
        "Huaizhe Xu",
        "Shilong Liu",
        "Lei Zhang",
        "Lionel Ni",
        "Heung-Yeung Shum"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Human emotion recognition with relational region-level analysis",
      "authors": [
        "Weixin Li",
        "Xuan Dong",
        "Yunhong Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Sequential interactive biased network for context-aware emotion recognition",
      "authors": [
        "Xinpeng Li",
        "Xiaojiang Peng",
        "Changxing Ding"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Joint Conference on Biometrics (IJCB)"
    },
    {
      "citation_id": "30",
      "title": "Focal loss for dense object detection",
      "authors": [
        "Tsung-Yi Lin",
        "Priya Goyal",
        "Ross Girshick",
        "Kaiming He",
        "Piotr DollÃ¡r"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "31",
      "title": "Microsoft coco: Common objects in context",
      "authors": [
        "Tsung-Yi Lin",
        "Michael Maire",
        "Serge Belongie",
        "James Hays",
        "Pietro Perona",
        "Deva Ramanan",
        "Piotr DollÃ¡r",
        "C Lawrence"
      ],
      "year": "2014",
      "venue": "Computer Vision-ECCV 2014: 13th European Conference"
    },
    {
      "citation_id": "32",
      "title": "Dab-detr: Dynamic anchor boxes are better queries for detr",
      "authors": [
        "Shilong Liu",
        "Feng Li",
        "Hao Zhang",
        "Xiao Yang",
        "Xianbiao Qi",
        "Hang Su",
        "Jun Zhu",
        "Lei Zhang"
      ],
      "year": "2022",
      "venue": "Dab-detr: Dynamic anchor boxes are better queries for detr",
      "arxiv": "arXiv:2201.12329"
    },
    {
      "citation_id": "33",
      "title": "End-to-end temporal action detection with transformer",
      "authors": [
        "Xiaolong Liu",
        "Qimeng Wang",
        "Yao Hu",
        "Xu Tang",
        "Song Bai",
        "Xiang Bai"
      ],
      "year": "2021",
      "venue": "End-to-end temporal action detection with transformer",
      "arxiv": "arXiv:2106.10271"
    },
    {
      "citation_id": "34",
      "title": "Teaching with soft label smoothing for mitigating noisy labels in facial expressions",
      "authors": [
        "Tohar Lukov",
        "Na Zhao",
        "Hee Lee",
        "Ser-Nam Lim"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "35",
      "title": "Conditional detr for fast training convergence",
      "authors": [
        "Depu Meng",
        "Xiaokang Chen",
        "Zejia Fan",
        "Gang Zeng",
        "Houqiang Li",
        "Yuhui Yuan",
        "Lei Sun",
        "Jingdong Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "36",
      "title": "Emotions in context: A sociodynamic model of emotions",
      "authors": [
        "Batja Mesquita",
        "Michael Boiger"
      ],
      "year": "2014",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "37",
      "title": "Multimodal and contextaware emotion perception model with multiplicative fusion",
      "authors": [
        "Trisha Mittal",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2021",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "38",
      "title": "EmotiCon: Context-Aware Multimodal Emotion Recognition Using Frege's Principle",
      "authors": [
        "Trisha Mittal",
        "Pooja Guhan",
        "Uttaran Bhattacharya",
        "Rohan Chandra"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "39",
      "title": "Affect2mm: Affective analysis of multimedia content using emotion causality",
      "authors": [
        "Trisha Mittal",
        "Puneet Mathur",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "Deep auto-encoders with sequential learning for multimodal dimensional emotion recognition",
      "authors": [
        "Dung Nguyen",
        "Thanh Duc",
        "Rui Nguyen",
        "Thanh Zeng",
        "Son Nguyen",
        "Thin Tran",
        "Sridha Nguyen",
        "Clinton Sridharan",
        "Fookes"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "41",
      "title": "Micron-BERT: BERT-based Facial Micro-Expression Recognition",
      "authors": [
        "Xuan-Bac Nguyen",
        "Chi Duong",
        "Xin Li",
        "Susan Gauch",
        "Han-Seok Seo",
        "Khoa Luu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "42",
      "title": "C-GCN: Correlation based graph convolutional network for audio-video emotion recognition",
      "authors": [
        "Weizhi Nie",
        "Minjie Ren",
        "Jie Nie",
        "Sicheng Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "43",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "Fatemeh Noroozi",
        "Dorota Kaminska",
        "Ciprian Corneanu",
        "Tomasz Sapinski",
        "Sergio Escalera",
        "Gholamreza Anbarjafari"
      ],
      "year": "2018",
      "venue": "Survey on emotional body gesture recognition"
    },
    {
      "citation_id": "44",
      "title": "NMS Strikes Back",
      "authors": [
        "Jeffrey Ouyang-Zhang",
        "Hyun Cho",
        "Xingyi Zhou",
        "Philipp KrÃ¤henbÃ¼hl"
      ],
      "year": "2022",
      "venue": "NMS Strikes Back",
      "arxiv": "arXiv:2212.06137"
    },
    {
      "citation_id": "45",
      "title": "EMERSK-Explainable Multimodal Emotion Recognition with Situational Knowledge",
      "authors": [
        "Mijanur Palash",
        "Bharat Bhargava"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "An android for enhancing social skills and emotion recognition in people with autism",
      "authors": [
        "Giovanni Pioggia",
        "Roberta Igliozzi",
        "Marcello Ferro",
        "Arti Ahluwalia",
        "Filippo Muratori",
        "Danilo Rossi"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "47",
      "title": "Linguistic-based emotion analysis and recognition for measuring consumer satisfaction: an application of affective computing",
      "authors": [
        "Fuji Ren",
        "Changqin Quan"
      ],
      "year": "2012",
      "venue": "Information Technology and Management"
    },
    {
      "citation_id": "48",
      "title": "detrex: Benchmarking Detection Transformers",
      "authors": [
        "Tianhe Ren",
        "Shilong Liu",
        "Feng Li",
        "Hao Zhang",
        "Ailing Zeng",
        "Jie Yang",
        "Xingyu Liao",
        "Ding Jia",
        "Hongyang Li",
        "He Cao",
        "Jianan Wang",
        "Zhaoyang Zeng",
        "Xianbiao Qi",
        "Yuhui Yuan",
        "Jianwei Yang",
        "Lei Zhang"
      ],
      "year": "2023",
      "venue": "detrex: Benchmarking Detection Transformers",
      "arxiv": "arXiv:2306.07265[cs.CV]"
    },
    {
      "citation_id": "49",
      "title": "Generalized intersection over union: A metric and a loss for bounding box regression",
      "authors": [
        "Hamid Rezatofighi",
        "Nathan Tsoi",
        "Junyoung Gwak",
        "Amir Sadeghian",
        "Ian Reid",
        "Silvio Savarese"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "50",
      "title": "Deep learning for human affect recognition: Insights and new developments",
      "authors": [
        "Marc Philipp V Rouast",
        "Raymond Adam",
        "Chiong"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "51",
      "title": "Deep disturbance-disentangled learning for facial expression recognition",
      "authors": [
        "Delian Ruan",
        "Yan Yan",
        "Si Chen",
        "Jing-Hao Xue",
        "Hanzi Wang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "52",
      "title": "Dive into ambiguity: Latent distribution mining and pairwise uncertainty estimation for facial expression recognition",
      "authors": [
        "Jiahui She",
        "Yibo Hu",
        "Hailin Shi",
        "Jun Wang",
        "Qiu Shen",
        "Tao Mei"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "53",
      "title": "How You Feelin'? Learning Emotions and Mental States in Movie Scenes",
      "authors": [
        "Dhruv Srivastava",
        "Aditya Singh",
        "Makarand Tapaswi"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "54",
      "title": "Rethinking transformer-based set prediction for object detection",
      "authors": [
        "Zhiqing Sun",
        "Shengcao Cao",
        "Yiming Yang",
        "Kris Kitani"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "55",
      "title": "Suppressing uncertainties for large-scale facial expression recognition",
      "authors": [
        "Kai Wang",
        "Xiaojiang Peng",
        "Jianfei Yang",
        "Shijian Lu",
        "Yu Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "56",
      "title": "Rethinking the learning paradigm for facial expression recognition",
      "authors": [
        "Weijie Wang",
        "Nicu Sebe",
        "Bruno Lepri"
      ],
      "year": "2022",
      "venue": "Rethinking the learning paradigm for facial expression recognition",
      "arxiv": "arXiv:2209.15402"
    },
    {
      "citation_id": "57",
      "title": "End-to-end video instance segmentation with transformers",
      "authors": [
        "Yuqing Wang",
        "Zhaoliang Xu",
        "Xinlong Wang",
        "Chunhua Shen",
        "Baoshan Cheng",
        "Hao Shen",
        "Huaxia Xia"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "58",
      "title": "Anchor detr: Query design for transformer-based detector",
      "authors": [
        "Yingming Wang",
        "Xiangyu Zhang",
        "Tong Yang",
        "Jian Sun"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "59",
      "title": "Faces in context: A review and systematization of contextual influences on affective face processing",
      "authors": [
        "J Matthias",
        "Tobias Wieser",
        "Brosch"
      ],
      "year": "2012",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "60",
      "title": "Patch-Aware Representation Learning for Facial Expression Recognition",
      "authors": [
        "Yi Wu",
        "Shangfei Wang",
        "Yanan Chang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "61",
      "title": "Generative Neutral Features-Disentangled Learning for Facial Expression Recognition",
      "authors": [
        "Zhenqian Wu",
        "Yazhou Ren",
        "Xiaorong Pu",
        "Zhifeng Hao",
        "Lifang He"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "62",
      "title": "Co-completion for occluded facial expression recognition",
      "authors": [
        "Zhen Xing",
        "Weimin Tan",
        "Ruian He",
        "Yangle Lin",
        "Bo Yan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "63",
      "title": "An emotion recognition model based on facial recognition in virtual learning environment",
      "authors": [
        "Dongri Yang",
        "Abeer Alsadoon",
        "Chandana Pw",
        "Ashutosh Prasad",
        "Amr Singh",
        "Elchouemi"
      ],
      "year": "2018",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "64",
      "title": "Context De-confounded Emotion Recognition",
      "authors": [
        "Dingkang Yang",
        "Zhaoyu Chen",
        "Yuzheng Wang",
        "Shunli Wang",
        "Mingcheng Li",
        "Siao Liu",
        "Xiao Zhao",
        "Shuai Huang",
        "Zhiyan Dong",
        "Peng Zhai"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "65",
      "title": "Emotion Recognition for Multiple Context Awareness",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Shunli Wang",
        "Yang Liu",
        "Peng Zhai",
        "Liuzhen Su",
        "Mingcheng Li",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "Computer Vision-ECCV 2022: 17th European Conference"
    },
    {
      "citation_id": "66",
      "title": "A circularstructured representation for visual emotion distribution learning",
      "authors": [
        "Jingyuan Yang",
        "Jie Li",
        "Leida Li",
        "Xiumei Wang",
        "Xinbo Gao"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "67",
      "title": "Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "Jiaxin Ye",
        "Yujie Wei",
        "Xin-Cheng Wen",
        "Chenglong Ma",
        "Zhizhong Huang",
        "Kunhong Liu",
        "Hongming Shan"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "68",
      "title": "Wide residual networks",
      "authors": [
        "Sergey Zagoruyko",
        "Nikos Komodakis"
      ],
      "year": "2016",
      "venue": "Wide residual networks",
      "arxiv": "arXiv:1605.07146"
    },
    {
      "citation_id": "69",
      "title": "Face2exp: Combating data biases for facial expression recognition",
      "authors": [
        "Dan Zeng",
        "Zhiyuan Lin",
        "Xiao Yan",
        "Yuting Liu",
        "Fei Wang",
        "Bo Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "70",
      "title": "Motr: End-to-end multiple-object tracking with transformer",
      "authors": [
        "Fangao Zeng",
        "Bin Dong",
        "Yuang Zhang",
        "Tiancai Wang",
        "Xiangyu Zhang",
        "Yichen Wei"
      ],
      "year": "2022",
      "venue": "Computer Vision-ECCV 2022: 17th European Conference"
    },
    {
      "citation_id": "71",
      "title": "Feature Representation Learning with Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition",
      "authors": [
        "Zhijun Zhai",
        "Jianhui Zhao",
        "Chengjiang Long",
        "Wenju Xu",
        "Shuangjiang He",
        "Huijuan Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "72",
      "title": "Semantic-aligned matching for enhanced DETR convergence and multi-scale feature fusion",
      "authors": [
        "Gongjie Zhang",
        "Zhipeng Luo",
        "Yingchen Yu",
        "Jiaxing Huang",
        "Kaiwen Cui",
        "Shijian Lu",
        "Eric Xing"
      ],
      "year": "2022",
      "venue": "Semantic-aligned matching for enhanced DETR convergence and multi-scale feature fusion",
      "arxiv": "arXiv:2207.14172"
    },
    {
      "citation_id": "73",
      "title": "Dino: Detr with improved denoising anchor boxes for end-to-end object detection",
      "authors": [
        "Hao Zhang",
        "Feng Li",
        "Shilong Liu",
        "Lei Zhang",
        "Hang Su",
        "Jun Zhu",
        "Lionel Ni",
        "Heung-Yeung Shum"
      ],
      "year": "2022",
      "venue": "Dino: Detr with improved denoising anchor boxes for end-to-end object detection",
      "arxiv": "arXiv:2203.03605"
    },
    {
      "citation_id": "74",
      "title": "Recognition of emotions in user-generated videos through frame-level adaptation and emotion intensity learning",
      "authors": [
        "Haimin Zhang",
        "Min Xu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "75",
      "title": "Facial expression analysis under partial occlusion: A survey",
      "authors": [
        "Ligang Zhang",
        "Brijesh Verma",
        "Dian Tjondronegoro",
        "Vinod Chandran"
      ],
      "year": "2018",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "76",
      "title": "Context-aware affective graph reasoning for emotion recognition",
      "authors": [
        "Minghui Zhang",
        "Yumeng Liang",
        "Huadong Ma"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "77",
      "title": "Learning emotion representations from verbal and nonverbal communication",
      "authors": [
        "Sitao Zhang",
        "Yimu Pan",
        "James Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "78",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "Shiqing Zhang",
        "Shiliang Zhang",
        "Tiejun Huang",
        "Wen Gao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "79",
      "title": "Learning a facial expression embedding disentangled from identity",
      "authors": [
        "Wei Zhang",
        "Xianpeng Ji",
        "Keyu Chen",
        "Yu Ding",
        "Changjie Fan"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "80",
      "title": "Relative uncertainty learning for facial expression recognition",
      "authors": [
        "Yuhang Zhang",
        "Chengrui Wang",
        "Weihong Deng"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "81",
      "title": "Learn from all: Erasing attention consistency for noisy label facial expression recognition",
      "authors": [
        "Yuhang Zhang",
        "Chengrui Wang",
        "Xu Ling",
        "Weihong Deng"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "82",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "83",
      "title": "Learning deep global multi-scale and local attention features for facial expression recognition in the Two in One Go: Single-stage Emotion Recognition with Decoupled Subject-context Transformer ACM MM",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu",
        "Shanmin Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "84",
      "title": "Robust lightweight facial expression recognition network with label distribution training",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu",
        "Feng Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "85",
      "title": "Places: A 10 million image database for scene recognition",
      "authors": [
        "Bolei Zhou",
        "Agata Lapedriza",
        "Aditya Khosla",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "86",
      "title": "Deformable detr: Deformable transformers for end-to-end object detection",
      "authors": [
        "Xizhou Zhu",
        "Weijie Su",
        "Lewei Lu",
        "Bin Li",
        "Xiaogang Wang",
        "Jifeng Dai"
      ],
      "year": "2020",
      "venue": "Deformable detr: Deformable transformers for end-to-end object detection",
      "arxiv": "arXiv:2010.04159"
    }
  ]
}