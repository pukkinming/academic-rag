{
  "paper_id": "2405.13049v3",
  "title": "Semeval-2024 Task 3: Multimodal Emotion Cause Analysis In Conversations",
  "published": "2024-05-19T09:59:00Z",
  "authors": [
    "Fanfan Wang",
    "Heqing Ma",
    "Jianfei Yu",
    "Rui Xia",
    "Erik Cambria"
  ],
  "keywords": [
    "Ohh",
    "get a room Yeah",
    "she couldn't live without the Chan Love"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The ability to understand emotions is an essential component of human-like artificial intelligence, as emotions greatly influence human cognition, decision making, and social interactions. In addition to emotion recognition in conversations, the task of identifying the potential causes behind an individual's emotional state in conversations, is of great importance in many application scenarios. We organize SemEval-2024 Task 3, named Multimodal Emotion Cause Analysis in Conversations, which aims at extracting all pairs of emotions and their corresponding causes from conversations. Under different modality settings, it consists of two subtasks: Textual Emotion-Cause Pair Extraction in Conversations (TECPE) and Multimodal Emotion-Cause Pair Extraction in Conversations (MECPE). The shared task has attracted 143 registrations and 216 successful submissions. In this paper, we introduce the task, dataset and evaluation settings, summarize the systems of the top teams, and discuss the findings of the participants.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Understanding emotions is crucial to achieve human-like artificial intelligence, as emotions are intrinsic to humans and significantly influence our cognition, decision-making, and social interactions. Conversation is an important form of human communication and contains a large number of emotions. Furthermore, given that conversation in its natural form is multimodal, many studies have explored multimodal emotion recognition in conversations (ERC), using language, audio and vision modalities  (Poria et al., 2019b; Mittal et al., 2020; Lian et al., 2021; Zhao et al., 2022; Zheng et al., 2023) .\n\nHowever, emotion recognition alone is not sufficient to fully understand the intricacies of hu-man emotions. Emotion cause analysis (ECA), the process of identifying the potential causes behind an individual's emotion state, has broad application scenarios such as human-computer interaction, commerce customer service, empathetic conversational agents, and automatic psychotherapy. For example, conversational agents equipped with emotion cause analysis can better understand the user's emotional state, offer empathetic responses, and provide more personalized services. By identifying the cause of the emotional state of a patient, a psychotherapy system can provide more accurate and customized treatments. ECA has gained increasing attention both in academic and practical fields  (Ding et al., 2019; Xia et al., 2019; Xia and Ding, 2019; Ding et al., 2020a,b; Poria et al., 2021; Li et al., 2022; An et al., 2023; Wang et al., 2023b) . However, to our knowledge, there has not been any evaluation competition conducted specifically for emotion cause analysis in conversations.\n\nTo promote research in this direction, we organize a shared task in SemEval-2024, named Multimodal Emotion Cause Analysis in Conversations. Our task consists of two subtasks: Subtask 1 (Textual Emotion-Cause Pair Extraction in Conversations, TECPE) focuses on extracting emotion and textual cause spans solely based on text; Subtask 2 (Multimodal Emotion-Cause Pair Extraction in Conversations, MECPE) involves extracting emotion-cause pairs at the utterance level considering three modalities.\n\nFor this shared task, we provide a multimodal emotion cause dataset ECF 2.0 sourced from the sitcom Friends. This dataset contains 1,715 conversations and 16,720 utterances, where 12,256 emotion-cause pairs are annotated at the utterance level, covering three modalities (language, audio, and vision). Specifically, in our preliminary work  (Wang et al., 2023a) , we have constructed a benchmark dataset, Emotion-Cause-in-Friends (ECF 1.0), which contains 1,374 conver- sations and 13,619 utterances. On this basis, we have furthermore annotated an extended test set as the evaluation data and provided the span-level annotations of emotion causes within the textual modality.\n\nOur task has attracted 143 registrations and a total of 216 successful submissions during the 16-day evaluation phase. Participants tended to decompose our task into emotion recognition and cause prediction, proposing numerous well-designed pipeline systems. Moreover, many teams applied advanced Large Language Models (LLMs) for emotion cause analysis and achieved promising results. After the evaluation, 18 teams finally submitted system description papers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Task",
      "text": "We clarify the definitions of emotion and cause before introducing the task and dataset. Emotion is a psychological state associated with thought, feeling, and behavioral response  (Ekman and Davidson, 1994) . In computer science, emotions are often described as discrete emotion categories, such as Ekman's six basic emotions, including Anger, Disgust, Fear, Joy, Sadness and Surprise  (Ekman, 1971) . In conversations, emotions were usually annotated at the utterance level  (Li et al., 2017; Hsu et al., 2018; Poria et al., 2019a) . Cause refers to the objective event or subjective argument that triggers the corresponding emotion  (Lee et al., 2010; Russo et al., 2011) .\n\nThe goal of our shared task, named Multimodal Emotion Cause Analysis in Conversations, is to extract potential pairs of emotions and their corre-sponding causes from a given conversation. Figure  1  illustrates a typical multimodal conversation scenario, which involves multiple emotions and their corresponding causes. Under different modality settings, we define the following two subtasks:\n\nSubtask 1: Textual Emotion-Cause Pair Extraction in Conversations (TECPE). Extracting all emotion-cause pairs from the given conversation solely based on text, where each pair contains an emotion utterance along with its emotion category and the textual cause span, e.g., (U3_Joy, U2_\"You made up!\") in Figure  1 .\n\nSubtask 2: Multimodal Emotion-Cause Pair Extraction in Conversations (MECPE). It should be noted that sometimes the cause cannot be reflected only in text. As shown in Figure  1 , the cause for Phoebe's Disgust in U5 is that Monica and Chandler were kissing in front of her, which is reflected in the visual modality of U5. Therefore, we accordingly define this multimodal subtask to extract all emotion-cause pairs in consideration of three modalities (language, audio, and vision). In this subtask, the cause is defined at the utterance level, and each pair contains an emotion utterance along with its emotion category and a cause utterance, e.g., (U5_Disgust, U5).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Source",
      "text": "Sitcoms come with real-world-inspired interhuman interactions and usually contain more emotions than other TV series or movies. Based on the famous American sitcom Friends,  Poria et al. (2019a)  constructed the multimodal conversational dataset MELD by extracting audiovisual clips corresponding to the scripts of the source episodes and annotating each utterance with one of six basic emotions (Anger, Disgust, Fear, Joy, Sadness and Surprise) or Neutral. MELD has recently become a widely used benchmark for ERC.\n\nIn our preliminary work  (Wang et al., 2023a) , we chose MELD as the data source and further annotated the causes given emotion annotations, thereby constructing the ECF 1.0 dataset. For this SemEval competition, we release the entire ECF 1.0 dataset as a training set and additionally create a test set as evaluation data, which is also sourced from Friends.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Collection",
      "text": "To construct the extended test set, we first crawl the subtitle files of all the episodes of Friends, which contains the utterance text and the corresponding timestamps. The subtitles are then separated by scene (scene descriptions are written in square brackets in the subtitle files), and each scene in every episode is viewed as a conversation. If the length of a conversation exceeds 40 utterances, we further divide it into several conversations of random lengths. Conversations included in the ECF 1.0 are removed. Next, we divide the collected conversations into several parts according to their lengths, with each part falling within the length ranges [1, 5], [6, 10],  [11, 15] ,  [16, 20] ,  [21, 25], and [26, 35] , respectively. Finally, we randomly sample conversations from each part according to the distribution probability of conversation lengths in ECF 1.0, and a total of 400 conversations are sampled for annotation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Annotation",
      "text": "We employ three graduate students involved in the annotation of the ECF 1.0 dataset to annotate the extended test set. Given a multimodal conversation, they first need to annotate the speaker and emotion category for each utterance, and then further annotate the utterances containing corresponding causes for each non-neutral emotion. If the causes are explicitly expressed in the text, they should also mark the textual cause spans. After annotation, we determine the emotion categories and cause utterances by majority voting, and take the largest boundary (i.e., the union of the spans) as the gold annotation of the textual cause span. If disagreements arise, another expert is invited for Dataset Modality Scene # Ins Emotion-Stimulus  (Ghazi et al., 2015)  T -2,414 s ECE Corpus  (Gui et al., 2016)  T News 2,105 d NTCIR-13-ECA  (Gao et al., 2017)  T Fiction 2,403 d Weibo-Emotion  (Cheng et al., 2017)  T Blog 7,000 p REMAN  (Kim and Klinger, 2018)  T Fiction 1,720 d GoodNewsEveryone  (Bostan et al., 2020)  T News 5,000 s RECCON-IE  (Poria et al., 2021)  T Conv 665 u RECCON-DD  (Poria et al., 2021)  T Conv 11,104 u ConvECPE  (Li et al., 2022)  T,A,V Conv 7,433 u ECF 1.0  (Wang et al., 2023a)  T,A,V Conv 13,619 u ECF 2.0 T,A,V Conv 16,720 u  the final decision.\n\nAnnotation Cost. The average duration of each conversation in our dataset is 31.6 seconds and it takes about 10 minutes to annotate a conversation. Each annotator would be paid CNY 300 when finishing every 50 conversations, which leads to the basic salary of CNY 36 (USD 5.2) per hour, which is higher than the current average salary in Jiangsu Province, China.\n\nData Post-processing. We conduct the following post-processing and cleaning of the data:\n\n• Correct the utterance text that does not match what the speaker said in the video; • Correct the timestamps that are not aligned with utterance text; • Separate the utterance whose segment of timestamps covers two speakers' utterances and modify their timestamps; • Separate the conversation which spans scenes; • Discard conversations if there is significant disagreement in annotations and the expert also finds it difficult to determine.\n\nAfter these steps, we store the text data in JSON files separately for each subtask. For Subtask 2, we use the FFmpeg 1  tool to extract video clips of each utterance from the source episodes based on the start and end timestamps.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset Statistic",
      "text": "In our preliminary work  (Wang et al., 2023a) , we have already constructed the ECF 1.0 dataset that contains 1,374 conversations and 13,619 utterances. Furthermore, we have annotated an extended test set specifically for this SemEval evaluation, which together with ECF 1.0 constitutes the ECF 2.0 dataset 2 that contains 1,715 conversations and 16,720 utterances.\n\nIn Table  1 , we compare our dataset with the related datasets for ECA, in terms of modality, scene, and size. It is evident that ECF 2.0 is currently the largest available emotion cause dataset.\n\nTable  2  presents the detailed statistics of our dataset for the two subtasks. It can be seen that, in the entire ECF 2.0 dataset, 56.88% of the utterances are labeled with one of the six basic emotions, 92.81% of the emotion utterances have corresponding cause utterances, and 88.18% of the emotion utterances are annotated with textual cause spans.\n\nIn addition, as shown in Figure  2  and Figure  3 , the newly annotated test set is basically consistent with the original ECF 1.0 dataset in terms of con-2 Our dataset is available on Google Drive.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation",
      "text": "Our SemEval task runs on CodaLab 3  . We released the training data in September 2023, and notified participants to commence model development. The evaluation phase began on January 16, 2024, and ended on January 31, 2024. We mixed the extended test set (consisting of 341 conversations with emotion and cause annotations; the labels are not publicly available) with some noise data (containing 324 conversations, not intended for evaluation) and released them together. Each team is allowed to submit their results up to three times a day.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "We evaluate the emotion-cause pairs of each emotion category with F 1 scores separately and further calculate a weighted average of F 1 scores across the six emotion categories, denoted as \"w-avg. F 1 \". Specifically, for Subtask 1, which involves the textual cause span, we adopt two strategies to determine whether the span is extracted correctly:\n\n• Strict Match: A predicted span is regarded as correct if it's the same as one of the annotated spans;\n\nRank User Name   • Proportional Match: Calculate the overlap proportion of the predicted span and the annotated one.\n\nThe evaluation metrics for the two strategies are \"w-avg. S. F 1 \" and \"w-avg. P. F 1 \", respectively. Taking into account the complexity of Subtask 1, we choose \"w-avg. P. F 1 \" as the main metric 4 for the ranking.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baselines",
      "text": "As mentioned in our previous work  (Wang et al., 2023a) , for Subtask 2 we also employed the BiLSTM-based ECPE-2steps model as our baseline system. Specifically, we maintain the validation set of the ECF 1.0 datset unchanged and merge the test set into the training set to train the 4 Specific calculation details can be found on GitHub.\n\nmodel. The evaluation of the model predictions on the extended test set achieves a weighted average F 1 of 0.1926.\n\nFor Subtask 1, based on the same model, we just convert the cause extraction module in Step 1 from the cause utterance prediction to the prediction of the start index and end index within the utterance, then simply match the indexes as candidate cause spans, followed by emotion-cause pairing and filtering in Step 2. The evaluation result for the weighted average proportional F 1 on the extended test set is 0.1801.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Participating Systems And Results",
      "text": "Our competition was created on Codalab in November 2023, and has attracted 143 registrations and a total of 216 submissions. After the evaluation, 18 teams have submitted system de-scription papers.\n\nTeam Samsung Research China-Beijing  (Zhang et al., 2024)  won first place in both subtasks, holding a significant lead over the second-place team. Teams petkaz  (Kazakov et al., 2024)  and UIC NLP GRADS  (Chandakacherla et al., 2024)  respectively captured the second and third places in Subtask 1. Teams NUS-Emo  (Luo et al., 2024)  and SZTU-MIPS  (Cheng et al., 2024)  attained second and third positions in Subtask 2. The official leaderboards for Subtask 1 and Subtask 2 are shown in Table  3  and Table 4 , respectively.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "System Overview",
      "text": "Almost all systems have implemented our task through a two-step framework, first performing the ERC task and then predicting the causes based on emotions. In the following, we briefly introduce the systems from the top teams and some other notable approaches.\n\nTeam Samsung Research China-Beijing  (Zhang et al., 2024)  achieved first place in both subtasks with a pipeline framework. They fine-tuned the LLaMA2-based InstructERC  (Lei et al., 2023)  to extract the emotion category of each utterance in a conversation. For further data augmentation, they added three additional auxiliary tasks based on the original training data strategy of In-structERC. Then, the MuTEC  (Bhat and Modi, 2023)  and TSAM  (Zhang et al., 2022)  models are used, respectively, to extract cause spans for Subtask 1 and cause utterances for Subtask 2. They also obtained different multimodal representations through openSMILE  (Eyben et al., 2010) , LLaVA  (Liu et al., 2024) , and a self-designed face module to explore the integration of audio-visual information. It should be noted that they used various models for ensemble learning to determine the final prediction.\n\nTeam petkaz  (Kazakov et al., 2024)  ranked second in Subtask 1. They fine-tuned GPT 3.5  (Ouyang et al., 2022)  for emotion classification and then used a BiLSTM-based neural network to detect cause utterances. The cause extractor model is initialized with BERT  (Devlin et al., 2019) , followed by three BiLSTM layers. They treat the entire cause utterance as a cause span.\n\nTeam NUS-Emo  (Luo et al., 2024)  achieved the second highest score in Subtask 2. First, they conducted zero-shot testing experiments to evaluate multiple LLMs, including OPT-IML3  (Iyer et al., 2022) , Instruct-GPT4  (Peng et al., 2023) ,  Flan-T5 (Chung et al., 2022) , and ChatGLM  (Du et al., 2022) . ChatGLM3-6B is ultimately selected as its backbone model based on its superior performance. They designed an emotion-cause-aware instruction-tuning mechanism to update the LLM and incorporated the visual representation from the ImageBind  (Girdhar et al., 2023)  encoder.\n\nTeam UIC NLP GRADS  (Chandakacherla et al., 2024)  achieved the third place in Subtask 1, and their system performed well in the strict metric, ranking second. They fine-tuned RoBERTa  (Liu et al., 2019)  for emotion classification, and then further fine-tuned a SpanBERT  (Joshi et al., 2019)  model that had been fine-tuned in SQuAD 2.0  (Rajpurkar et al., 2018) , to predict cause spans in QA format.\n\nTeam SZTU-MIPS  (Cheng et al., 2024)  ranked third in Subtask 2. They integrated text, audio, and image modalities for emotion recognition and adopted the MiniGPTv2 model  (Chen et al., 2023)  for multimodal cause extraction. Specifically, textual features are obtained from Instruc-tERC, while acoustic features are extracted using HuBERT  (Hsu et al., 2021) . For visual modality, faces are first extracted using OpenFace (Baltrusaitis et al., 2016) from video frames, followed by extraction of facial features using expMAE  (Cheng et al., 2023) .\n\nTeam nicolay-r (Rusnachenko and Liang, 2024) finetuned Flan-T5 by designing the chain of thoughts for emotion causes based on the Three-Hop Reasoning (THOR) framework  (Fei et al., 2023) , to predict the emotion of the current utterance and the emotion caused by the current utterance towards the target utterance. Their reasoning revision methodology and rule-based span correction technique bring further improvements.\n\nTeam JMI  (. et al., 2024)  implemented two different approaches. In their best system, they used in-context learning using GPT 3.5 for emotion prediction and cause prediction, respectively. Conversation-level video descriptions were extracted via GPT-4V  (Yang et al., 2023)  to provide more context to GPT 3.5. In addition, they also fine-tuned two separate Llama2  (Touvron et al., 2023)  models to recognize emotions and extract causes.\n\nTeam AIMA (Ghahramani Kure et al., 2024) fine-tuned EmoBERTa  (Kim and Vossen, 2021)  for emotion classification and then obtained the emotion-cause pairs via a Transformer-based encoder. After finding the pairs, they further fine-tuned the DeBERTa  (He et al., 2021)  that had been fine-tuned on SQuAD 2.0 to extract the cause spans for Subtask 1.\n\nTeam UWBA  (Baloun et al., 2024)  fused the features of three modalities at the utterance level and then used them for emotion classification and pair prediction. It is interesting that they summarized five span categories (Whole Utterance, First part, Last part, Middle part, Other) through observations of training data, and then trained a classifier to further predict textual cause spans in cause utterance.\n\nFurthermore, Team DeepPavlov (Belikova and Kosenko, 2024) investigated the performance of Video-LLaMA  (Zhang et al., 2023)  in several modes and found that model fine-tuning yields notable improvements in emotion and cause classification. Team PWEITINLP  (Levchenko et al., 2024)  utilized GPT-3 for emotion classification. Some other Teams, including UIR-ISC  (Guo et al., 2024) , LyS  (Ezquerro and Vilares, 2024) , QFNU_CS  (Wang et al., 2024)  and Hidetsune  (Takahashi, 2024) , all employed BERTbased models to address our task, among which LyS proposed an end-to-end model comprising a BERT encoder and a graph-based decoder to identify emotion cause relations. Team LastResort  (Mathur et al., 2024)  tackled our task as sequence labeling problems and used BiLSTM followed by a CRF layer to solve it. Team NCL  (Li et al., 2024)  solely utilized pre-trained models to extract features from three modalities. Team VerbaNexAI Lab  (Pacheco et al., 2024)  demonstrated the inadequacy of machine learning techniques alone for emotion cause analysis.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "Our task, Multimodal Emotion Cause Analysis in Conversations, involves informal real-life conversations and complex audio-visual scenes. Additionally, emotions exhibit strong subjectivity, and we have observed that even humans sometimes struggle to accurately identify emotions and their causes. This complexity underscores the intricate nature of human emotions and the nuanced contexts in which they occur, posing a substantial challenge for data annotation and subsequent model development.\n\nDataset Bias. Emotion category imbalance is an inherent problem in the ERC task  (Li et al., 2017; Hsu et al., 2018; Poria et al., 2019a) , aligning with real-world phenomena where people tend to express positive emotions like joy more frequently in their daily communications, while expressions of disgust and fear are less common. Our dataset is sourced from TV series that closely resemble the real world, naturally also exhibiting an imbalance in emotions, as illustrated in Figure  3 . However, such an imbalance may adversely affect a model's ability to learn and generalize across different emotions, potentially leading to biases towards frequently expressed emotions  (Kazakov et al., 2024; Chandakacherla et al., 2024) . Moreover, emotion cause datasets often have a noticeable pattern in the location of causes and emotions. Some systems rely on this position bias, either by using a fixed window size or by direct post-processing to add the emotion utterance as the cause  (Rusnachenko and Liang, 2024; . et al., 2024) , which overlooks the effective semantic connections between distant contexts and may lead to poor generalization capabilities for unseen data where the cause is not in proximity to the emotion. In the future, LLMs can be leveraged to assist with annotation to expand the diversity of datasets available for fine-tuning, which encompass a wider range of emotional expressions and cultural backgrounds. This can mitigate existing dataset biases and enhance the model's applicability and generalizability across various scenarios.\n\nUtilization of LLMs. Recently, LLMs have exhibited remarkable capabilities in a wide range of tasks and are rapidly advancing the field of natural language processing. Therefore, LLMs are allowed to be used in our competition. It is evident that about a third of the teams have used LLMs for emotion cause analysis, and most of them are ranked at the top. However, some participants have observed that LLMs perform poorly in zero-shot and few-shot settings on emotion and cause recognition tasks  (Kazakov et al., 2024; . et al., 2024; Belikova and Kosenko, 2024) , indicating a crucial need for task-specific fine-tuning. Furthermore, prompt engineering is essential, as LLMs often produce hallucinations or unstructured outputs. Due to resource and cost constraints, most researchers cannot take full advantage of the strongest capabilities of LLM. Future research is encouraged to explore ways to enhance lightweight models or to bridge the gap between pre-training and downstream tasks, thereby augmenting LLMs' ability to understand emotions.\n\nPotential of Multimodal Information. Multimodal information is important for discovering both emotions and their causes in conversations. In our daily communications, we depend not only on the speaker's voice intonation and facial expressions to perceive his emotions, but also on some auditory and visual scenes to speculate the potential causes that trigger the emotions of speakers beyond text. However, some participants found that the introduction of audio or visual modalities results in minimal improvements or even a decrease in system performance  (Zhang et al., 2024; Cheng et al., 2024; Baloun et al., 2024) . This issue arises partly due to the characteristics of our dataset, which involves a large number of complex visual scenes but few visual cause clues, leading to the introduction of noise. Another limiting factor might be that multimodal feature extraction methods are not advanced enough or fusion strategies are not effective enough. The challenges that require further exploration include the effective interaction and fusion of multimodal information, as well as the perception, understanding, and utilization of audiovisual scenes. Furthermore, there is a demand for more high-quality data sets on multimodal emotion cause analysis to support research in this area.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we describe the SemEval-2024 Task 3 named Multimodal Emotion Cause Analysis in Conversations, which aims to extract all potential pairs of emotions and their corresponding causes from a conversation. The shared task has attracted 143 registrations and 216 successful submissions. We provide detailed descriptions of task definition and data annotation, summarize participating systems, and discuss their findings.\n\nAs an important direction of affective computing, multimodal emotion cause analysis in conversation plays an important role in many real-world applications. We hope that our research and resources can contribute towards the design of future systems in this direction.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Ethics Statement",
      "text": "Our ECF 2.0 dataset is annotated on the basis of the MELD dataset 5  which is licensed under the GNU General Public License v3.0 and is used only for scientific research. We do not share personal information and do not release sensitive content that can be harmful to any individual or community. Conducting multimodal emotion cause analysis will help us better understand emotions in human conversations, build human-machine dialogue systems, and contribute to society and human well-being.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of our task and annotated dataset. Each arc points from the cause utterance to the emotion it",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates a typical multimodal conversation sce-",
      "page": 2
    },
    {
      "caption": "Figure 1: Subtask 2: Multimodal Emotion-Cause Pair",
      "page": 2
    },
    {
      "caption": "Figure 2: The distribution of conversation lengths. The",
      "page": 4
    },
    {
      "caption": "Figure 2: and Figure 3,",
      "page": 4
    },
    {
      "caption": "Figure 3: The distribution of emotions. The horizontal",
      "page": 4
    },
    {
      "caption": "Figure 3: However, such an imbalance may adversely affect",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison of existing ECA datasets. T, A,",
      "page": 3
    },
    {
      "caption": "Table 2: Statistics of our dataset.",
      "page": 3
    },
    {
      "caption": "Table 1: , we compare our dataset with the",
      "page": 4
    },
    {
      "caption": "Table 2: presents the detailed statistics of our",
      "page": 4
    },
    {
      "caption": "Table 3: The leaderboard for Subtask 1 (TECPE). “†” indicates that the team has submitted a system description",
      "page": 5
    },
    {
      "caption": "Table 4: The leaderboard for Subtask 2 (MECPE). “†” indicates that the team has submitted a system description",
      "page": 5
    },
    {
      "caption": "Table 3: and Table 4, respectively.",
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "JMI at SemEval 2024 task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned llama models",
      "authors": [
        "Arefa",
        "Chandni Mohammed Abbas Ansari",
        "Tanvir Saxena",
        "Ahmad"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "2",
      "title": "Global-view and speaker-aware emotion cause extraction in conversations",
      "authors": [
        "Jiaming An",
        "Zixiang Ding",
        "Ke Li",
        "Rui Xia"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/TASLP.2023.3319990"
    },
    {
      "citation_id": "3",
      "title": "UWBA at SemEval-2024 task 3: Dialogue representation and multimodal fusion for emotion cause analysis",
      "authors": [
        "Josef Baloun",
        "Jiri Martinek",
        "Ladislav Lenc",
        "Pavel Kral",
        "Matěj Zeman",
        "Lukáš Vlček"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "4",
      "title": "Openface: An open source facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrusaitis",
        "Peter Robinson",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "5",
      "title": "Deep-Pavlov at SemEval-2024 task 3: Multimodal large language models in emotion reasoning",
      "authors": [
        "Julia Belikova",
        "Dmitrii Kosenko"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "6",
      "title": "Multi-task learning framework for extracting emotion cause span and entailment in conversations",
      "authors": [
        "Ashwani Bhat",
        "Ashutosh Modi"
      ],
      "year": "2023",
      "venue": "Transfer Learning for Natural Language Processing Workshop"
    },
    {
      "citation_id": "7",
      "title": "Goodnewseveryone: A corpus of news headlines annotated with emotions, semantic roles, and reader perception",
      "authors": [
        "Laura Ana",
        "Maria Bostan",
        "Evgeny Kim",
        "Roman Klinger"
      ],
      "year": "2020",
      "venue": "Proceedings of The 12th Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "8",
      "title": "UIC NLP GRADS at SemEval-2024 task 3: Two-step disjoint modeling for emotion-cause pair extraction",
      "authors": [
        "Sharad Chandakacherla",
        "Vaibhav Bhargava",
        "Natalie Parde"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "9",
      "title": "Minigptv2: large language model as a unified interface for vision-language multi-task learning",
      "authors": [
        "Jun Chen",
        "Deyao Zhu",
        "Xiaoqian Shen",
        "Xiang Li",
        "Zechun Liu",
        "Pengchuan Zhang",
        "Raghuraman Krishnamoorthi",
        "Vikas Chandra",
        "Yunyang Xiong",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigptv2: large language model as a unified interface for vision-language multi-task learning"
    },
    {
      "citation_id": "10",
      "title": "An emotion cause corpus for chinese microblogs with multiple-user structures",
      "authors": [
        "Xiyao Cheng",
        "Ying Chen",
        "Bixiao Cheng",
        "Shoushan Li",
        "Guodong Zhou"
      ],
      "year": "2017",
      "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing (TAL-LIP)"
    },
    {
      "citation_id": "11",
      "title": "Semi-supervised multimodal emotion recognition with expression mae",
      "authors": [
        "Zebang Cheng",
        "Yuxiang Lin",
        "Zhaoru Chen",
        "Xiang Li",
        "Shuyi Mao",
        "Fan Zhang",
        "Daijun Ding",
        "Bowen Zhang",
        "Xiaojiang Peng"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "MIPS at SemEval-2024 task 3: Multimodal emotion-cause pair extraction in conversations with multimodal language models",
      "authors": [
        "Zebang Cheng",
        "Fuqiang Niu",
        "Yuxiang Lin",
        "Zhiqi Cheng",
        "Xiaojiang Peng",
        "Bowen Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "13",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "S Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Eric Li",
        "Xuezhi Wang",
        "Mostafa Dehghani",
        "Siddhartha Brahma",
        "Albert Webson",
        "Shane Shixiang",
        "Zhuyun Gu",
        "Mirac Dai",
        "Xinyun Suzgun",
        "Aakanksha Chen",
        "Dasha Chowdhery",
        "Sharan Valter",
        "Gaurav Narang",
        "Adams Mishra",
        "Vincent Yu",
        "Yanping Zhao",
        "Andrew Huang",
        "Hongkun Dai",
        "Slav Yu",
        "Ed Petrov",
        "Jeff Huai Hsin Chi",
        "Jacob Dean",
        "Adam Devlin",
        "Denny Roberts",
        "Quoc Zhou",
        "Jason Le",
        "Wei"
      ],
      "year": "2022",
      "venue": "Scaling instruction-finetuned language models"
    },
    {
      "citation_id": "14",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "North American Chapter"
    },
    {
      "citation_id": "15",
      "title": "From independent prediction to reordered prediction: Integrating relative position and global label information to emotion cause identification",
      "authors": [
        "Zixiang Ding",
        "Huihui He",
        "Mengran Zhang",
        "Rui Xia"
      ],
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "16",
      "title": "ECPE-2D: Emotion-cause pair extraction based on joint two-dimensional representation, interaction and prediction",
      "authors": [
        "Zixiang Ding",
        "Rui Xia",
        "Jianfei Yu"
      ],
      "year": "2020",
      "venue": "Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "17",
      "title": "End-toend emotion-cause pair extraction based on sliding window multi-label learning",
      "authors": [
        "Zixiang Ding",
        "Rui Xia",
        "Jianfei Yu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "18",
      "title": "Glm: General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Zhengxiao Du",
        "Yujie Qian",
        "Xiao Liu",
        "Ming Ding",
        "Jiezhong Qiu",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "19",
      "title": "Universals and cultural differences in facial expressions of emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1971",
      "venue": "Nebraska Symposium on Motivation. Nebraska Symposium on Motivation"
    },
    {
      "citation_id": "20",
      "title": "The nature of emotion: Fundamental questions",
      "authors": [
        "Ed Paul",
        "Richard Ekman",
        "Davidson"
      ],
      "year": "1994",
      "venue": "The nature of emotion: Fundamental questions"
    },
    {
      "citation_id": "21",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "22",
      "title": "LyS at SemEval-2024 task 3: An early prototype for endto-end multimodal emotion linking as graph-based parsing",
      "authors": [
        "Ana Ezquerro",
        "David Vilares"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "23",
      "title": "Reasoning implicit sentiment with chain-of-thought prompting",
      "authors": [
        "Bobo Hao Fei",
        "Qian Li",
        "Lidong Liu",
        "Fei Bing",
        "Tat Li",
        "Chua Seng"
      ],
      "year": "2023",
      "venue": "Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Overview of ntcir-13 eca task",
      "authors": [
        "Qinghong Gao",
        "Jiannan Hu",
        "Ruifeng Xu",
        "Gui Lin",
        "Yulan He",
        "Qin Lu",
        "Kam-Fai Wong"
      ],
      "year": "2017",
      "venue": "Proceedings of the NTCIR-13 Conference"
    },
    {
      "citation_id": "25",
      "title": "AIMA at SemEval-2024 task 3: Simple yet powerful emotion cause pair analysis",
      "authors": [
        "Alireza Ghahramani Kure",
        "Mahshid Dehghani",
        "Mohammad Mahdi Abootorabi",
        "Nona Ghazizadeh"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "26",
      "title": "Detecting emotion stimuli in emotion-bearing sentences",
      "authors": [
        "Diman Ghazi",
        "Diana Inkpen",
        "Stan Szpakowicz"
      ],
      "year": "2015",
      "venue": "International Conference on Intelligent Text Processing and Computational Linguistics"
    },
    {
      "citation_id": "27",
      "title": "Imagebind: One embedding space to bind them all",
      "authors": [
        "Rohit Girdhar",
        "Alaaeldin El-Nouby",
        "Zhuang Liu",
        "Mannat Singh",
        "Kalyan Vasudev Alwala",
        "Armand Joulin",
        "Ishan Misra"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Event-driven emotion cause extraction with corpus construction",
      "authors": [
        "Lin Gui",
        "Dongyin Wu",
        "Ruifeng Xu",
        "Qin Lu",
        "Yu Zhou"
      ],
      "year": "2016",
      "venue": "EMNLP"
    },
    {
      "citation_id": "29",
      "title": "UIR-ISC at SemEval-2024 task 3: Textual emotion-cause pair extraction in conversations",
      "authors": [
        "Hongyu Guo",
        "Xueyao Zhang",
        "Yiyang Chen",
        "Lin Deng",
        "Binyang Li"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "30",
      "title": "Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing",
      "authors": [
        "Pengcheng He",
        "Jianfeng Gao",
        "Weizhu Chen"
      ],
      "year": "2021",
      "venue": "Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing",
      "arxiv": "arXiv:2111.09543"
    },
    {
      "citation_id": "31",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "Chao-Chun",
        "Sheng-Yeh Hsu",
        "Chuan-Chun Chen",
        "Ting-Hao Kuo",
        "Lun-Wei Huang",
        "Ku"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "32",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Lakhotia"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "authors": [
        "Srinivas Iyer",
        "Xi Victoria Lin",
        "Ramakanth Pasunuru",
        "Todor Mihaylov",
        "Daniel Simig",
        "Ping Yu",
        "Kurt Shuster",
        "Tianlu Wang",
        "Qing Liu",
        "Punit Singh Koura",
        "Xian Li",
        "Brian O' Horo",
        "Gabriel Pereyra",
        "Jeff Wang",
        "Christopher Dewan",
        "Asli Celikyilmaz",
        "Luke Zettlemoyer",
        "Veselin Stoyanov",
        "; Mandar Joshi",
        "Danqi Chen",
        "Yinhan Liu",
        "Daniel Weld",
        "Luke Zettlemoyer",
        "Omer Levy"
      ],
      "year": "2019",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "34",
      "title": "PetKaz at SemEval-2024 task 3: Advancing emotion classification with an LLM for emotion-cause pair extraction in conversations",
      "authors": [
        "Roman Kazakov",
        "Kseniia Petukhova",
        "Ekaterina Kochmar"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "35",
      "title": "Who feels what and why? annotation of a literature corpus with semantic roles of emotions",
      "authors": [
        "Evgeny Kim",
        "Roman Klinger"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "36",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "Emoberta: Speaker-aware emotion recognition in conversation with roberta"
    },
    {
      "citation_id": "37",
      "title": "A text-driven rule-based system for emotion cause detection",
      "authors": [
        "Sophia Yat",
        "Mei Lee",
        "Ying Chen",
        "Chu-Ren Huang"
      ],
      "year": "2010",
      "venue": "NAACL HLT Workshop on Computational Approaches to Analysis and Generation of Emotion in Text"
    },
    {
      "citation_id": "38",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "39",
      "title": "PWEITINLP at SemEval-2024 task 3: Two step emotion cause analysis",
      "authors": [
        "Sofiia Levchenko",
        "Rafał Wolert",
        "Piotr Andruszkiewicz"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "40",
      "title": "NCL team at SemEval-2024 task 3: Fusing multimodal pre-training embeddings for emotion cause prediction in conversations",
      "authors": [
        "Shu Li",
        "Zicen Liao",
        "Huizhi Liang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "41",
      "title": "Ecpec: emotioncause pair extraction in conversations",
      "authors": [
        "Wei Li",
        "Yang Li",
        "Vlad Pandelea",
        "Mengshi Ge",
        "Luyao Zhu",
        "Erik Cambria"
      ],
      "year": "2022",
      "venue": "Ecpec: emotioncause pair extraction in conversations"
    },
    {
      "citation_id": "42",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "43",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "44",
      "title": "Visual instruction tuning. Advances in neural information processing systems",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "Visual instruction tuning. Advances in neural information processing systems"
    },
    {
      "citation_id": "45",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach"
    },
    {
      "citation_id": "46",
      "title": "NUS-emo at SemEval-2024 task 3: Instruction-tuning LLM for multimodal emotion-cause analysis in conversations",
      "authors": [
        "Meng Luo",
        "Han Zhang",
        "Shengqiong Wu",
        "Bobo Li",
        "Hong Han",
        "Hao Fei"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "47",
      "title": "LastResort at SemEval-2024 task 3: Exploring multimodal emotion cause pair extraction as sequence labelling task",
      "authors": [
        "Akshett Suyash Vardhan Mathur",
        "Hardik Jindal",
        "Manish Mittal",
        "Shrivastava"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "48",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "Trisha Mittal",
        "Uttaran Bhattacharya",
        "Rohan Chandra"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "49",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeff Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray",
        "John Schulman",
        "Jacob Hilton",
        "Fraser Kelton",
        "Luke Miller",
        "Maddie Simens",
        "Amanda Askell",
        "Peter Welinder",
        "Paul Francis Christiano",
        "Jan Leike",
        "Ryan Lowe"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback"
    },
    {
      "citation_id": "50",
      "title": "VerbaNexAI lab at SemEval-2024 task 3: Deciphering emotional causality in conversations using multimodal analysis approach",
      "authors": [
        "Victor Pacheco",
        "Elizabeth Martinez",
        "Juan Cuadrado",
        "Juan Martinez",
        "Edwin Puertas"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "51",
      "title": "Instruction tuning with gpt-4",
      "authors": [
        "Baolin Peng",
        "Chunyuan Li",
        "Pengcheng He",
        "Michel Galley",
        "Jianfeng Gao"
      ],
      "year": "2023",
      "venue": "Instruction tuning with gpt-4"
    },
    {
      "citation_id": "52",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "53",
      "title": "Recognizing emotion cause in conversations",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Deepanway Ghosal",
        "Rishabh Bhardwaj",
        "Samson Yu Bai Jian",
        "Pengfei Hong",
        "Romila Ghosh",
        "Abhinaba Roy",
        "Niyati Chhaya"
      ],
      "year": "2021",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "54",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "55",
      "title": "Know what you don't know: Unanswerable questions for squad",
      "authors": [
        "Pranav Rajpurkar",
        "Robin Jia",
        "Percy Liang"
      ],
      "year": "2018",
      "venue": "Know what you don't know: Unanswerable questions for squad"
    },
    {
      "citation_id": "56",
      "title": "nicolay-r at SemEval-2024 task 3: Using flan-t5 for reasoning emotion cause in conversations with chain-of-thought on emotion states",
      "authors": [
        "Nicolay Rusnachenko",
        "Huizhi Liang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "57",
      "title": "Emocause: an easy-adaptable approach to emotion cause contexts",
      "authors": [
        "Irene Russo",
        "Tommaso Caselli",
        "Francesco Rubino",
        "Ester Boldrini",
        "Patricio Martínez-Barco"
      ],
      "year": "2011",
      "venue": "Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA)"
    },
    {
      "citation_id": "58",
      "title": "Hidetsune at SemEval-2024 task 3: A simple textual approach to emotion classification and emotion cause analysis in conversations using machine learning and next sentence prediction",
      "authors": [
        "Hidetsune Takahashi"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "59",
      "title": "",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale",
        "Daniel Bikel",
        "Lukas Blecher",
        "Cantón Cristian",
        "Moya Ferrer",
        "Guillem Chen",
        "David Cucurull",
        "Jude Esiobu",
        "Jeremy Fernandes",
        "Wenyin Fu",
        "Brian Fu",
        "Cynthia Fuller",
        "Vedanuj Gao",
        "Naman Goswami",
        "Anthony Goyal",
        "Saghar Hartshorn",
        "Rui Hosseini",
        "Hakan Hou",
        "Marcin Inan",
        "Viktor Kardas",
        "Madian Kerkez",
        "Isabel Khabsa",
        "A Kloumann",
        "Punit Korenev",
        "Marie-Anne Singh Koura",
        "Thibaut Lachaux",
        "Jenya Lavril",
        "Diana Lee",
        "Yinghai Liskovich",
        "Yuning Lu",
        "Xavier Mao",
        "Todor Martinet",
        "Pushkar Mihaylov",
        "Igor Mishra",
        "Yixin Molybog",
        "Andrew Nie",
        "Jeremy Poulton",
        "Rashi Reizenstein",
        "Kalyan Rungta",
        "Alan Saladi",
        "Ruan Schelten",
        "Silva"
      ],
      "venue": ""
    },
    {
      "citation_id": "60",
      "title": "2023a. Multimodal emotion-cause pair extraction in conversations",
      "authors": [
        "Fanfan Wang",
        "Zixiang Ding",
        "Rui Xia",
        "Zhaoyu Li",
        "Jianfei Yu"
      ],
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2022.3226559"
    },
    {
      "citation_id": "61",
      "title": "2023b. Generative emotion cause triplet extraction in conversations with commonsense knowledge",
      "authors": [
        "Fanfan Wang",
        "Jianfei Yu",
        "Rui Xia"
      ],
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": "10.18653/v1/2023.findings-emnlp.260"
    },
    {
      "citation_id": "62",
      "title": "QFNU_CS at SemEval-2024 task 3: A hybrid pre-trained model based approach for multimodal emotion-cause pair extraction task",
      "authors": [
        "Zining Wang",
        "Yanchao Zhao",
        "Guanghui Han",
        "Yang Song"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "63",
      "title": "Emotion-cause pair extraction: A new task to emotion analysis in texts",
      "authors": [
        "Rui Xia",
        "Zixiang Ding"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "64",
      "title": "RTHN: A RNN-transformer hierarchical network for emotion cause extraction",
      "authors": [
        "Rui Xia",
        "Mengran Zhang",
        "Zixiang Ding"
      ],
      "year": "2019",
      "venue": "International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "65",
      "title": "The dawn of lmms: Preliminary explorations with gpt-4v (ision)",
      "authors": [
        "Zhengyuan Yang",
        "Linjie Li",
        "Kevin Lin",
        "Jianfeng Wang",
        "Chung-Ching Lin",
        "Zicheng Liu",
        "Lijuan Wang"
      ],
      "year": "2023",
      "venue": "The dawn of lmms: Preliminary explorations with gpt-4v (ision)",
      "arxiv": "arXiv:2309.17421"
    },
    {
      "citation_id": "66",
      "title": "Tsam: A two-stream attention model for causal emotion entailment",
      "authors": [
        "Duzhen Zhang",
        "Zhen Yang",
        "Fandong Meng",
        "Xiuyi Chen",
        "Jie Zhou"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "67",
      "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "Video-llama: An instruction-tuned audio-visual language model for video understanding"
    },
    {
      "citation_id": "68",
      "title": "Samsung research China-Beijing at SemEval-2024 task 3: A multi-stage framework for emotion-cause pair extraction in conversations",
      "authors": [
        "Shen Zhang",
        "Haojie Zhang",
        "Jing Zhang",
        "Xudong Zhang",
        "Yimeng Zhuang",
        "Jinting Wu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "69",
      "title": "M3ed: Multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "Jinming Zhao",
        "Tenggan Zhang",
        "Jingwen Hu",
        "Yuchen Liu",
        "Qin Jin",
        "Xinchao Wang",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "70",
      "title": "A facial expression-aware multimodal multitask learning framework for emotion recognition in multi-party conversations",
      "authors": [
        "Wenjie Zheng",
        "Jianfei Yu",
        "Rui Xia",
        "Shijin Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    }
  ]
}