{
  "paper_id": "2308.14059v1",
  "title": "Multi-Subdomain Adversarial Network For Cross-Subject Eeg-Based Emotion Recognition",
  "published": "2023-08-27T09:57:46Z",
  "authors": [
    "Guang Lin",
    "Jianhai Zhang"
  ],
  "keywords": [
    "Electroencephalogram (EEG)",
    "Emotion Recognition",
    "Cross-Subject",
    "Adversarial Network"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The individual difference between subjects is significant in EEG-based emotion recognition, resulting in the difficulty of sharing the model across subjects. Previous studies use domain adaptation algorithms to minimize the global domain discrepancy while ignoring the class information, which may cause misalignment of subdomains and reduce model performance. This paper proposes a multi-subdomain adversarial network (MSAN) for cross-subject EEG-based emotion recognition. MSAN uses adversarial training to model the discrepancy in the global domain and subdomain to reduce the intra-class distance and enlarge the inter-class distance. In addition, MSAN initializes parameters through a pre-trained autoencoder to ensure the stability and convertibility of the model. The experimental results show that the accuracy of MSAN is improved by 30.02% on the SEED dataset comparing with the nontransfer method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion plays an essential role in humans' daily routines. With the development of human-computer interaction  [1]  and machine learning, machines are expected to communicate with us according to human emotions, where emotion recognition is crucial  [2, 3] . EEG-based emotion recognition has been widely researched due to its high accuracy, low cost, and easy operation  [4] . However, different subjects perceive emotions differently  [5] , models trained for specific subjects have poor generalization when applied to new-subject, as shown in Figure  1  (a). In addition, training a specific model for each new-subject is inefficient, which requires the collection of labeled data and retrains the model  [6] . Therefore, cross-subject emotion recognition based on EEG signals is still a challenge.\n\nIn the cross-subject EEG-based emotion recognition, previous studies improved the cross-domain performance of the model by aligning source domain distribution with target domain distribution. Zheng et al. used transfer component analysis (TCA)  [7]  and transductive parameter transfer (TPT)  [8]  to map two domains with different distributions into the same feature space and select the corresponding classifier to reduce the impact of distribution discrepancy. Li et al.  [9]  proposed multisource transfer learning (MSTL), which aligned the data distribution of new-subject with the multiple existing-subject separately and used the joint decision of multiple SVM classifiers to output the predicted results. Luo et al.  [10]  used an adversarial domain network (DAN) with gradient reversal Layer (GRL)  [11]  to make the distribution of the two domains more similar. Also, based on the GRL, Zhong et al.  [12]  proposed a regularized graph neural network (RGNN) which used the graph network to extract features.\n\nThe above studies focus on minimizing the global domain discrepancy ignoring the class information of the target domain data. The misalignment of the subdomain distribution will cause poor recognition performance on the model, as shown at the bottom of Figure  1   unsupervised training on all data, and the parameters of the first three layers of AE after training are saved as the initial parameters of MSAN.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Organization",
      "text": "Considering the factor that physiological characteristics of EEG signals vary with frequency  [13]  and encephalic regions  [14] , this paper uses a 3D structure  [15, 16]  to organize the EEG signals, as shown in Figure  2  (a). In detail, the bandpass filter is performed on the EEG signals of all channels to divide the signal into the delta, theta, alpha, beta, and gamma. Then differential entropy (DE) feature  [17]  in a 1-second sliding window is extracted in each frequency band. In addition, the channel dimension information is organized into a 2D map according to the electrode distribution in the data acquisition. Therefore, each segment is a 3D structure X ∈ R h×w×d , h and w are the width and height of the 2D map.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Training",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Subdomain Adversarial Network",
      "text": "The Multi-subdomain adversarial network (MSAN) models the discrepancy in the global domain and subdomain, which reduces the intra-class distance and enlarges the inter-class distance to improve the recognition performance on the newsubject. As shown in Figure  2   There are two constraints in the global domain adversarial: the error of the D c is minimized to achieve accurate classification of task labels of the source domain data; the error of the D d is maximized to confuse source domain data with the target domain data which minimizes the two domains discrepancy. Error maximization can be achieved through gradient reversal layer (GRL), which reverses the gradient direction during backpropagation and realizes identity during forward. The related mathematical expressions are as follows:\n\nwhere I is an identity matrix. The complete optimization objective of G f , D c and D d is formulated as follows:\n\n(2) where X represents the input EEG signal, d n = 0 represents x n ∈ X s , and θ represents the corresponding network parameters. The optimal parameters are calculated by the following formula:\n\nθd = arg max\n\nIn the K-Means clustering  [18] , we use the number of task categories in the source domain dataset as the K value. Next, performing the following three steps: (1) Calculate the centroid of all source domain data points in each category and use it as the initial cluster center of the corresponding cluster.  (2)  Calculate the Euclidean distance between each data point in the target domain dataset and the cluster center, and assign it to the nearest cluster. (3) Recalculate the centroid of each cluster and repeat step (1) to (3) until the cluster center stabilizes. After the convergence of K-Means, the top 10% of the target domain data points closest to the cluster center are selected and temporarily marked according to the cluster. With the class information added for subdomain adversarial, the adjusted loss function L D d is as follows:\n\nWhere x ′ represents the data points reordered according to Euclidean distance and i and j represent the task labels of the samples in the two domains. When i = j, the domain discrepancy is reduced through GRL. When i ̸ = j, keep the gradient direction during the backpropagation to increase the domain discrepancy.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Autoencoder",
      "text": "Since extracted feature distributions vary dramatically in the initial training stage, subdomain adaptation may cause the model to train in the wrong direction, which leads to model collapse. In order to improve the stability of MSAN, an autoencoder is applied to the pre-training  [19]  of the model. Autoencoder  [20]  is a deep neural network that can learn the efficient representation of EEG signals through unsupervised learning. It consists of two main parts: encoder and decoder. The function of the encoder is to encode high-dimensional input into low-dimensional hidden variables, thereby forcing the neural network to learn the most informative features. The function of the decoder is to restore the hidden variables to the initial dimension. The autoencoder uses the data itself as supervision to guide the neural network to learn a mapping relationship. We consider the AE model defined as:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "The effectiveness of the proposed method is evaluated on the SEED dataset  [13] . This dataset is collected by BCMI Lab.\n\nIn the experiment, 15 film clips selected from the 6 Chinese films were used as the stimuli, and every 5 clips corresponded to one kind of emotion. Several types of emotions were stimulated during the experiment, including positive, negative, and neutral. Fifteen healthy subjects participated in the experiment and were asked to complete a questionnaire immediately to report the individual emotional response after watching each clip. The ESI NeuroScan System with 62-channels was used to record the EEG signals, and the sampling frequency was 1000 Hz. In order to reduce the storage space and the amount of calculation, after removing some basic noise from the data, the data was down-sampled to 200 Hz.\n\nFor SEED, the 62-channels' EEG signals is organized a 3D feature map of H × W × 5, where H = 17, W = 19 in the feature organization and leave-one-subject-out validation method is used to evaluate the model performance. Specifically, the data of 14 subjects are used as the training dataset, and the data of the remaining subject is used as the test dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "For SEED, we compared the performance achieved by MSAN with the best results reported in the literature. The average accuracy and standard deviation (STD) represent the model performance, and all results were obtained on the data of 15 subjects. As shown in Table  1 , the nontransfer method was set as the baseline. MSTL took negative transfer into account and used multiple global domains to match subjects for model selection, which achieved state of the art performance. However, the STD of MSTL was large and the performance of the algorithm varies greatly on subjects. When the matching degree of the new subject and all the existing subjects was low, it was difficult to identify on the new-subject accurately. MSAN utilized multiple subdomains and reached 88.70% accuracy rate, with minor STD and stable performance in all subjects, which proved the effectiveness of the proposed method.\n\nIn the ablation studies, we compared the nontransfer method, DAN, and MSAN to demonstrate the advantages of the proposed method. Figure  3 (a)  shows that the test results of the four models on the cross-subject task. In the algorithm without transfer, the average accuracy of the model was low, only 58.68%, which was caused by individual differences. In the DAN, global domain adversarial was introduced, and its average accuracy had been significantly improved, reaching 80.49%. Based on DAN, MSAN carried out subdomain adversarial, and compared with DAN, the average accuracy increased by 6.39% to 86.88%.\n\nIn order to analyze the characteristics of various models more specifically, Figure  3    71.80 13.99 13.12 TPT  [8]  76.31 15.89 17.63 DAN  [11]  80.49 6.00 21.81 RGNN  [12]  85.30 6.72 26.62 WGANDA  [10]  87.07 7.14 28.39 MSTL  [9]  88. In order to have a better view of the effectiveness of MSAN-Pt, we used t-SNE  [21]  to perform a two-dimensional visualization of the feature distributions of source domain data and target domain data, as shown in Figure  4 . In the nontransfer method, the data of source subjects and target subjects had different feature distributions due to the indi-  vidual differences between subjects. Therefore, when the model trained with existing-subject data was tested on the new-subject data, the classification accuracy was low. After global domain adversarial and subdomain adversarial in MSAN, the distribution of target domain data was similar to that of source domain data in the global domain and also aligned in the subdomains so that the D c correctly identified the target subjects' emotions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In the EEG-based emotion recognition task, since the individual differences of EEG signals, the model generalization is poor applied to new-subject. This paper designs a novel framework (MSAN) based on domain adversarial network, which considers both global domain adversarial and subdomain adversarial. The average accuracy of MSAN reaches 86.88% on the SEED dataset, which is 6.39% higher than that of the DAN. At the same time, to improve the model stability, an autoencoder is applied to the pre-training of MSAN.\n\nThe average accuracy of MSAN-Pt reaches 88.70%, which is 30.02% higher than that of the nontransfer method.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a). In addition, training a specific model",
      "page": 1
    },
    {
      "caption": "Figure 1: The motivation of the proposed method. (a) The dis-",
      "page": 1
    },
    {
      "caption": "Figure 1: (b). In this paper, a multi-",
      "page": 1
    },
    {
      "caption": "Figure 2: The framework of the proposed method. (a) Feature organization. (b) MSAN training structure. All data (X) is input",
      "page": 2
    },
    {
      "caption": "Figure 2: (a). In detail, the band-",
      "page": 2
    },
    {
      "caption": "Figure 2: (b), MSAN consists of four",
      "page": 2
    },
    {
      "caption": "Figure 3: (a) shows that the test results",
      "page": 3
    },
    {
      "caption": "Figure 3: (b) shows the change of accuracy",
      "page": 3
    },
    {
      "caption": "Figure 3: (a) The accuracy of the four models under the cross-subject task. (b) The accuracy changes in 50 iterations.",
      "page": 4
    },
    {
      "caption": "Figure 4: Feature visualization.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Nontransfer",
          "Mean": "58.68",
          "Std.": "11.21",
          "Improve": "-"
        },
        {
          "Model": "TCA [7]",
          "Mean": "71.80",
          "Std.": "13.99",
          "Improve": "13.12"
        },
        {
          "Model": "TPT [8]",
          "Mean": "76.31",
          "Std.": "15.89",
          "Improve": "17.63"
        },
        {
          "Model": "DAN [11]",
          "Mean": "80.49",
          "Std.": "6.00",
          "Improve": "21.81"
        },
        {
          "Model": "RGNN [12]",
          "Mean": "85.30",
          "Std.": "6.72",
          "Improve": "26.62"
        },
        {
          "Model": "WGANDA [10]",
          "Mean": "87.07",
          "Std.": "7.14",
          "Improve": "28.39"
        },
        {
          "Model": "MSTL [9]",
          "Mean": "88.92",
          "Std.": "10.35",
          "Improve": "30.24"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Unsupervised emotional state classification through physiological parameters for social robotics applications",
      "authors": [
        "Laura Fiorini",
        "Gianmaria Mancioppi",
        "Francesco Semeraro",
        "Hamido Fujita",
        "Filippo Cavallo"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie",
        "Nicolas Tsapatsoulis",
        "George Votsis",
        "Stefanos Kollias",
        "Winfried Fellenz",
        "John Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "4",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "A Rafael",
        "Sidney D' Calvo",
        "Mello"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "5",
      "title": "Continuous eeg monitoring in the intensive care unit: early findings and clinical efficacy",
      "authors": [
        "M Paul",
        "Val Vespa",
        "Marc Nenov",
        "Nuwer"
      ],
      "year": "1999",
      "venue": "Journal of Clinical Neurophysiology"
    },
    {
      "citation_id": "6",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "Christos-Nikolaos Anagnostopoulos",
        "Theodoros Iliou",
        "Ioannis Giannoukos"
      ],
      "year": "2015",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "7",
      "title": "Challenge for affective brain-computer interfaces: Non-stationary spatiospectral eeg oscillations of emotional responses",
      "authors": [
        "Yi-Wei Shen",
        "Yuan-Pin Lin"
      ],
      "year": "2019",
      "venue": "Frontiers in human neuroscience"
    },
    {
      "citation_id": "8",
      "title": "Transfer components between subjects for eeg-based emotion recognition",
      "authors": [
        "Wei-Long Zheng",
        "Yong-Qi Zhang",
        "Jia-Yi Zhu",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "2015 international conference on affective computing and intelligent interaction (ACII)"
    },
    {
      "citation_id": "9",
      "title": "Personalizing eeg-based affective models with transfer learning",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the twenty-fifth international joint conference on artificial intelligence"
    },
    {
      "citation_id": "10",
      "title": "Multisource transfer learning for cross-subject eeg emotion recognition",
      "authors": [
        "Jinpeng Li",
        "Shuang Qiu",
        "Yuan-Yuan",
        "Cheng-Lin Shen",
        "Huiguang Liu",
        "He"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "11",
      "title": "Wgan domain adaptation for eeg-based emotion recognition",
      "authors": [
        "Yun Luo",
        "Si-Yang Zhang",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "12",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Yaroslav Ganin",
        "Victor Lempitsky"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "13",
      "title": "Eegbased emotion recognition using regularized graph neural networks",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "15",
      "title": "Emotionet: A 3-d convolutional neural network for eeg-based emotion recognition",
      "authors": [
        "Yi Wang",
        "Zhiyi Huang",
        "Brendan Mccane",
        "Phoebe Neo"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "16",
      "title": "Continuous convolutional neural network with 3d input for eeg-based emotion recognition",
      "authors": [
        "Yilong Yang",
        "Qingfeng Wu",
        "Yazhen Fu",
        "Xiaowei Chen"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "17",
      "title": "Eeg-based emotion recognition using 4d convolutional recurrent neural network",
      "authors": [
        "Fangyao Shen",
        "Guojun Dai",
        "Guang Lin",
        "Jianhai Zhang",
        "Wanzeng Kong",
        "Hong Zeng"
      ],
      "year": "2020",
      "venue": "Cognitive Neurodynamics"
    },
    {
      "citation_id": "18",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "Jia-Yi Ruo-Nan Duan",
        "Bao-Liang Zhu",
        "Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "19",
      "title": "Towards k-means-friendly spaces: Simultaneous deep learning and clustering",
      "authors": [
        "Bo Yang",
        "Xiao Fu",
        "Nicholas Sidiropoulos",
        "Mingyi Hong"
      ],
      "year": "2017",
      "venue": "Towards k-means-friendly spaces: Simultaneous deep learning and clustering"
    },
    {
      "citation_id": "20",
      "title": "The difficulty of training deep architectures and the effect of unsupervised pre-training",
      "authors": [
        "Dumitru Erhan",
        "Pierre-Antoine Manzagol",
        "Yoshua Bengio",
        "Samy Bengio",
        "Pascal Vincent"
      ],
      "year": "2009",
      "venue": "Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "21",
      "title": "Deep convolution neural network and autoencoders-based unsupervised feature learning of eeg signals",
      "authors": [
        "Tingxi Wen",
        "Zhongnan Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "22",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}