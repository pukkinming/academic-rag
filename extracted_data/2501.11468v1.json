{
  "paper_id": "2501.11468v1",
  "title": "Llm Supervised Pre-Training For Multimodal Emotion Recognition In Conversations",
  "published": "2025-01-20T12:56:02Z",
  "authors": [
    "Soumya Dutta",
    "Sriram Ganapathy"
  ],
  "keywords": [
    "Multimodal emotion recognition",
    "LLM distillation",
    "Hierarchical training",
    "Conversational Analytics"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversations (ERC) is challenging due to the multimodal nature of the emotion expression. In this paper, we propose to pretrain a text-based recognition model from unsupervised speech transcripts with LLM guidance. These transcriptions are obtained from a raw speech dataset with a pre-trained ASR system. A text LLM model is queried to provide pseudo-labels for these transcripts, and these pseudo-labeled transcripts are subsequently used for learning an utterance level text-based emotion recognition model. We use the utterance level text embeddings for emotion recognition in conversations along with speech embeddings obtained from a recently proposed pretrained model. A hierarchical way of training the speech-text model is proposed, keeping in mind the conversational nature of the dataset. We perform experiments on three established datasets, namely, IEMOCAP, MELD, and CMU-MOSI, where we illustrate that the proposed model improves over other benchmarks and achieves state-of-the-art results on two out of these three datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion Recognition in Conversation (ERC) focuses on detecting emotions conveyed through multiple modalities during social conversational interactions, which is essential for natural human communication. Developing artificial systems that have improved emotional understanding and intelligence is a vital design step in conversational agents  [1] , social media analytics tools  [2] , customer service centers  [3] , mental health monitoring platforms  [4] , and wearable systems  [5] . ERC enables these technologies to better adapt to human emotions, enhancing user experiences.\n\nEmotion recognition in conversational data is challenging due to overlapping speakers, short-and long-term dependencies  [6] , shortspeaker turns, reverberation and background noise. Emotions are often multimodal, conveyed through various modes such as facial expressions  [7] , vocal cues  [8] , gestures  [9] , and physiological signals  [10] . To address these complexities, multimodal approaches are often preferred  [11] . This paper focuses on joint emotion recognition from audio and text, using the strengths of both modalities to enhance accuracy in detecting emotions in conversations.\n\nThe initial methods for Speech Emotion Recognition (SER) relied on handcrafted acoustic features like pitch  [12] , energy, and speaking rate  [13] . The introduction of deep learning techniques, including CNNs  [14] , LSTMs  [15] , and transformers  [16] , significantly improved the SER performance. Recently, self-supervised learning models like wav2vec2.0  [17] , HuBERT  [18] , and WavLM  [19]  have shown promise in recognition of emotions across multiple datasets and tasks. Large language models (LLMs) have also been explored for SER  [20] ,  [21] , though they demand significant computational resources.\n\nIn parallel, text-based emotion recognition (commonly known as sentiment analysis) initially relied on rule-based methods that linked This work was carried out with research grants from British Telecom and the Prime Minister's Research Fellowship. specific words to emotions  [22] ,  [23] . With the rise of deep learning, sentiment analysis progressed to CNNs  [24] , RNNs  [25] , and transformer architectures such as BERT  [26] ,  [27]  and RoBERTa  [28] ,  [29] . More recently, large language models (LLM) are seen as excellent tools for sentiment analysis  [30] .\n\nIn order to enhance emotion recognition in conversations, several works have also designed multi-modal fusion techniques combining audio and text data, using models like transformers  [31] , graph neural networks  [32] , and capsule networks  [33] .\n\nIn this paper, we propose a pre-training methodology through a multi-modal approach that leverages both text and speech representations. Specifically, we introduce a strategy to improve emotion classification from text by leveraging unsupervised speech data with large-scale language models (LLMs). To this end, we first utilize a pre-trained ASR based on Whisper-large model  [34]  to transcribe speech. The \"noisy\" speech transcripts are labeled with an LLM to automatically generate pseudo-labels of speech sentiments. These labelled text-transcripts are then used to fine-tune a RoBERTa text encoder model  [28]  for sentiment classification, allowing it to capture nuanced emotional patterns in textual data. We show substantial benefits from this unsupervised pre-training of the text-based model.\n\nOn the speech side, we extract features using the recently proposed CARE model  [35] . The CARE model is designed to generate high-quality embeddings that encapsulate both content and acoustic information from speech utterances. Using the speech and text embeddings derived at utterance level, we train a bi-directional gated recurrent unit (GRU) based model to assimilate the information across the entire conversation. The conversation-level embeddings from uni-modal speech-text models are integrated into our proposed multi-modal architecture for conversational emotion recognition. We propose a hierarchical fusion mechanism with a cross-attention-based network  [36]  to enable the interaction between the modalities, ensuring an fusion of the emotional information present in both speech and text. We call our method MERITS-L (Multimodal Emotion Recognition In Speech and Text with LLM guidance).\n\nThe experiments are performed on three established datasets, namely IEMOCAP  [37] , MELD  [38]  and CMU-MOSI  [39] . The key contributions from this work are:\n\n• We propose a pre-training methodology for improving text emotion recognition. We make use of an unsupervised speech corpus and a large language model (LLM) for this purpose. • We propose a hierarchical approach for the multi-modal emotion recognition, where the information is first processed at the utterance level in each of the modalities, followed by inter-utterance conversation modeling. Subsequently, multi-modal processing with co-attention is designed. • We evaluate the proposed MERITS-L model on three benchmark datasets and achieve state-of-the-art results for two out of these three datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "LLMs for Text Sentiment Analysis: The capabilities of large language models (LLMs) have been the focus of recent research efforts  [30] ,  [40] ,  [41] . Zhong et al.  [41]   ChatGPT 1 achieves performance comparable to fine-tuned BERT models. Zhang et al.  [30] , however, provided a more comprehensive evaluation across various sentiment analysis tasks. Their findings reveal that while LLMs under-perform in fine-grained sentiment analysis, they exhibit promising zero-shot capabilities in simpler tasks, such as binary sentiment classification. In this work, we leverage the ability of LLMs to coarsely annotate large corpora of emotional speech transcripts.\n\nEmotion Recognition in Conversations: Recent methods that achieve strong performance on benchmark datasets often incorporate speaker identity  [42] -  [45] . For instance, Hu et al.  [43]  introduced a supervised contrastive loss, where utterances with the same emotion and speaker are treated as positive samples in a contrastive learning framework. Yu et al.  [45]  appended speaker embeddings to the utterance representations and utilize a language model, such as RoBERTa  [28] , to predict emotions via masked language modeling. In contrast, our work does not access speaker labels for any utterance in the conversation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Method",
      "text": "A. Background 1) CARE: In our recently proposed CARE model  [35] , speech is processed by two encoders -one focusing on the semantic aspect of speech by aligning with the mean-pooled RoBERTa representation of corresponding ASR transcripts, while the other is trained to predict low-level descriptors of speech provided by the PASE+ model  [46] . The CARE embeddings are seen to perform better than most other base-sized models in the SUPERB  [47]  style evaluation. In this paper, 1 https://chatgpt.com we utilize the CARE embeddings from speech utterances and train conversational models along with speech-text fusion.\n\n2) RoBERTa: One of the significant contributions in creating a large scale language model was proposed by Devlin et al.  [26] . This architecture, called bidirectional encoder representations from transformer (BERT), was trained with two objectives on a corpus of textual data, namely, predicting words masked out in a sentence and to predict whether two sentences semantically follow each other (referred to as next sentence prediction). Liu et. al  [28]  trained this architecture on a larger corpus of textual data without the next sentence prediction task. This pre-trained model is known as robust optimized BERT approach (RoBERTa). We use the pre-trained large version of this model with 24 transformer layers as the text encoder.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "B. Proposed Merits-L Model",
      "text": "The block diagram of the proposed model is shown in Fig.  1 .\n\n1) Problem Description: Given a set of utterances U and set of emotion labels Y , a conversation consisting of K utterances is denoted by [(u1, y1), (u2, y2), . . . , (uK , yK )], where yj ∈ Y is the emotion of utterance uj in the conversation. Specifically, the task of ERC is achieved by using the speech and text modalities in our case, which means u k = {S k , T k }, where S k and T k refer to the speech and the text transcript associated with the utterance u k . The objective of ERC is to predict the emotion label y k of each utterance u k .\n\n2) LLM guided text pre-training: The text transcripts from the emotional speech corpus are first extracted from an ASR system (Whisper-large-v3  [34] ). Generally, the word error rates on emotional speech are higher than neutral speech  [48] . With the ASR generated transcripts of the speech corpus, a large language model (LLM) is prompted to annotate the transcript as three classes, \"positive\", \"negative\" or \"neutral\". The pre-trained RoBERTa-large model is fine-tuned to predict the pseudo-classes (from the LLM predictions)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross-Attention Block",
      "text": "Cross-attention block",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Self-Attention Block",
      "text": "Self-attention block Concatenate FC Layer and the resultant model is used subsequently as a text feature extractor at utterance-level. This model is referred to as RoBERTa-FT in the subsequent sections of the paper.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Training",
      "text": "The training methodology for MERITS-L is performed in stages as mentioned below: Stage I: All utterances, U , are collated and used to train text sentiment analysis and speech emotion recognition models for each dataset. While the RoBERTa-FT model is fine-tuned for each downstream dataset, small light weight networks with frozen CARE embeddings as input are trained for the speech modality for every dataset. This training stage aims to classify the text transcript (T k ) and the speech signal (S k ) into the correct emotion category (y k ). The final layer embeddings for each utterance (T 1 k and S 1 k for the text transcripts and speech signal respectively) are used for the next stage of training.\n\nStage II: This stage introduces the conversational nature of the data in the modeling framework. The text features for the utterances in a conversation from the previous stage, denoted by (T 1 1 , T 1 2 , . . . , T 1 K ) for a conversation having K utterances are processed by a bidirectional gated recurrent network (Bi-GRU) with self-attention over the conversational context. This stage encourages the model to predict the emotion class of the utterance u k , keeping the entire conversation as a part of the context. A similar modeling exercise is done for the speech modality as well. Similar to the previous stage, the features from the Bi-GRU with self-attention blocks are used for the final stage of training. These features are denoted by T 2 k and S 2 k for the text and speech modality, of utterance u k , respectively. Stage III: Notably, the previous stages included training the two modalities separately. In order to align the two modalities in a more effective way for emotion recognition, they are combined in this stage. We implement a co-attention fusion strategy for the two modalities as outlined in Fig.  2 . The query-key-value sequences for the two modalities are the features obtained after Stage II of training. E.g. in Fig.  2 , QA refers to the query sequence of the speech modality and is denoted as (S 2 1 , S 2 2 , . . . , S 2 K ) for the conversation C.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Datasets",
      "text": "Pre-training:The MSP-PODCAST corpus  [49]  is used for the task of pre-training. A total of 149, 307 speech-turns amounting to 230 hours of emotional speech data is used. Out of the total number of samples, 80% of the data is randomly chosen as the training set while the remaining 20% serves as the validation set. The Whisper-large-v3 model 2 is used for generating the transcripts. These transcripts are annotated using the GPT-3.5 Turbo 3 model.  ERC datasets: Three datasets are used for evaluating MERITS-L on the ERC task -IEMOCAP  [37] , MELD  [38]  and CMU-MOSI  [39] .\n\nIEMOCAP dataset: The IEMOCAP dataset consists of 151 video recordings split into 5 sessions. Keeping in line with previous works, we do a four-way classification task where we consider \"angry\", \"happy\", \"sad\", \"neutral\" and \"excited\" categories (with excited and happy categories merged). We have a total of 5531 utterances from the four emotion labels. We consider session 5 for testing purposes. We choose session 1 for validating our models and sessions 2 -4 for training.\n\nMELD dataset: The MELD dataset is a multi-party dataset created from video clippings of the popular TV show, \"Friends\". The training data consists of 9988 utterances, validation data consists of 1108 utterances and test data consists of 2610 utterances. A seven way classification task is performed on this dataset, with each utterance being labeled as one of the 7 emotions -\"angry\", \"sad\", \"joy\", \"neutral\", \"fear\", \"surprise\" or \"disgust\". The RoBERTa-large model is pre-trained with the LLM generated labels (3 classes) for a total of 10 epochs with a learning rate of 1e-4 and a batch size of 32. The different stages of MERITS-L are trained for a total of 50 epochs with a learning rate of 1e-4 and a batch size of 32. For all training purposes, the cross-entropy loss with AdamW  [51]  optimizer is used. The weighted F1-score is used as the metric for performance evaluation on the ERC datasets. Note that, we have not used any additional labeled datasets in pre-training as the pre-training framework for speech and text are purely based on selfsupervised learning principles from raw data. Further, the downstream datasets are used without any knowledge of speaker meta-data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cmu-Mosi",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Results",
      "text": "The results for the three stages are shown in Table  I . We note that the performance of both the modalities improve with every modeling stage. The introduction of the contextual information is seen to significantly improve the performance of IEMOCAP dataset (relative improvements of 16.46% and 20.06% for audio and text modality respectively). The performance of the two modalities are seen to be comparable for the IEMOCAP dataset, unlike the other two where text emotion recognition performance is considerably higher than the performance with the audio modality. Finally, the multi-modal fusion is seen to aid all the datasets, achieving relative improvements of 16.28%, 2.24% and 5.65% over the best performing modality (after Stage II) for IEMOCAP, MELD and CMU-MOSI respectively. This also shows that the multi-modal fusion strategy is more effective while combining modalities having comparable performance. This is perhaps due to the symmetrical nature of the co-attention based fusion mechanism.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Evaluation With Different Llms",
      "text": "We design an oracle experiment in this regard. Using a similar prompt template, as described in Sec. IV-B, we annotate the transcripts into the same three classes using Mixtral-8x7B-Instruct-v0.1 4  and Llama-3-8b-chat-hf  5  . Since the MSP-PODCAST dataset has valence-arousal-dominance values, ranging from 1 to 7, we assign a positive label to samples having valence in the range  (5, 7] , while negative label is assigned to samples having valence value in  [1, 3) . The rest of the samples are assigned the neutral label. With these labels serving as the ground truth, we notice that GPT-3.5 Turbo achieves a label overlap of 52.98%, while Llama-3-8b-chat-hf is comparable with an overlap of 50.91%. The performance of Mixtral-8x7B-Instruct-v0.1 is the lowest with an overlap of only 44.98%.\n\nWhile the above benchmarking used oracle emotion labels from the speech dataset, we perform a downstream evaluation using the three choices of LLM. The impact of the different LLM annotation ability is shown in Fig.  3 , where the performance of the model with the text modality after Stage I training is shown. The performance of the model after Stage I with pre-trained RoBERTa (without any LLM guidance) is also shown for reference. We notice that the RoBERTa model fine-tuned with labels provided by GPT-3.5 Turbo achieves relative improvements of 8.22%, 5.01% and 21.9% for IEMOCAP, MELD and CMU-MOSI respectively over the pretrained RoBERTa model. The relative improvements achieved by GPT-3.5 Turbo over Mixtral-8x7B-Instruct-v0.1 are 5.4%, 3.39% and 12.51% for IEMOCAP, MELD and CMU-MOSI respectively. The highest performance improvement in the CMU-MOSI dataset may be attributed to the binary classification task in this dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Importance Of Hierarchical Training",
      "text": "In order to understand the impact of hierarchical training, we combine Stages II and III of MERITS-L. The impact of such a training philosophy is shown in Fig.  4 . The impact of this change in the training methodology is found to be the most in the case of IEMOCAP where the performance of MERITS-L drops from 86.48% to 82.91%. A performance drop of around 2% in absolute terms is also noticed for MELD and CMU-MOSI. The benefits from the  hierarchical training arises from the fact that the end-to-end training of the different components of the model often leads to over-fitting as these datasets are relatively small in size.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "F. Comparison With Other Work",
      "text": "We compare the performance of MERITS-L with some recent works in Table  II . The proposed MERITS-L achieves the best performance when compared with state-of-the-art models for MELD and CMU-MOSI. For IEMOCAP however, the method by Lian et al.  [55]  outperforms MERITS-L by a margin of 1% absolute. Note that, there have been multiple methods like TelME  [57]  and EACL  [45]  which achieve higher performance than MERITS-L on MELD dataset. However, these methods use information about the speaker identity of each spoken utterance and hence are excluded from the comparison in this work.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Summary",
      "text": "In this paper, we first propose a novel way of super-vised pretraining of text based emotion recognition using LLM guidance. Text from an emotional speech corpus is extracted, following which a text emotion recognition model is trained to classify each transcript using the pseudo-labels. With this text based recognition model as the utterance-level text embedding extractor, we propose MERITS-L, wherein we develop the model for emotion recognition in conversations by using the speech and textual modalities. A hierarchical way of training the model is proposed, starting with utterances from a single modality, followed by the contextual modeling at the conversational level and subsequently, the alignment of the two modalities. Comparison with other state-of-art works indicate the superiority of our method for two out of the three datasets considered in this work.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Block diagram of the proposed model. The pre-training stage is shown in the grey box at the top. An ASR system is used to generate the transcripts",
      "page": 2
    },
    {
      "caption": "Figure 1: 1) Problem Description: Given a set of utterances U and set",
      "page": 2
    },
    {
      "caption": "Figure 2: The co-attention network used in the proposed model. It consists of",
      "page": 3
    },
    {
      "caption": "Figure 2: The query-key-value sequences for",
      "page": 3
    },
    {
      "caption": "Figure 2: , QA refers to the query sequence of the speech modality",
      "page": 3
    },
    {
      "caption": "Figure 3: The performance of the RoBERTa-large models on the different",
      "page": 4
    },
    {
      "caption": "Figure 4: The importance of hierarchical training in MERITS-L",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RoBERTa Mixtral LLaMA GPT-3.5 Turbo\n90\n67.5\n45\n22.5\n0\nIEMOCAP MELD CMU-MOSI\nFig. 3. The performance of the RoBERTa-large models on the different\ndatasets.DifferentLLMsareusedforgeneratingpseudoemotionlabelsfrom\nspeech transcripts. The performance of pre-trained RoBERTa without any\nsupervisedfine-tuningisalsoreported.\nCOMP": "",
          "Column_2": "urbo\nSI\non the different\ntionlabelsfrom\nTa without any\nCOMP"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective multimodal human-computer interaction",
      "authors": [
        "M Pantic"
      ],
      "year": "2005",
      "venue": "ACM international conference on Multimedia"
    },
    {
      "citation_id": "2",
      "title": "Emotion detection and analysis on social media",
      "authors": [
        "B Gaind"
      ],
      "year": "2019",
      "venue": "Emotion detection and analysis on social media",
      "arxiv": "arXiv:1901.08458"
    },
    {
      "citation_id": "3",
      "title": "Acoustic and lexical sentiment analysis for customer service calls",
      "authors": [
        "B Li"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "4",
      "title": "EmoKey: An emotion-aware smartphone keyboard for mental health monitoring",
      "authors": [
        "S Ghosh"
      ],
      "year": "2019",
      "venue": "COMSNETS"
    },
    {
      "citation_id": "5",
      "title": "K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations",
      "authors": [
        "C Park"
      ],
      "year": "2020",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition using facial expressions",
      "authors": [
        "P Tarnowski"
      ],
      "year": "2017",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "8",
      "title": "Vocal expression of emotion",
      "authors": [
        "K Scherer"
      ],
      "year": "2003",
      "venue": "Vocal expression of emotion"
    },
    {
      "citation_id": "9",
      "title": "Individuality in communicative bodily behaviours",
      "authors": [
        "C Navarretta"
      ],
      "year": "2012",
      "venue": "Cognitive Behavioural Systems"
    },
    {
      "citation_id": "10",
      "title": "Physiological signals and their use in augmenting emotion recognition for human-machine interaction",
      "authors": [
        "R Knapp"
      ],
      "year": "2011",
      "venue": "Emotionoriented systems"
    },
    {
      "citation_id": "11",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "12",
      "title": "Some aspects of fundamental frequency and envelope amplitude as related to the emotional content of speech",
      "authors": [
        "P Lieberman",
        "S Michaels"
      ],
      "year": "1962",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "13",
      "title": "Emotion in speech: Recognition and application to call centers",
      "authors": [
        "V Petrushin"
      ],
      "year": "1999",
      "venue": "Proceedings of artificial neural networks in engineering"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using spectrogram & phoneme embedding",
      "authors": [
        "P Yenigalla"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "15",
      "title": "Effective attention mechanism in dynamic models for speech emotion recognition",
      "authors": [
        "P.-W Hsiao",
        "C.-P Chen"
      ],
      "year": "2018",
      "venue": "ICASSP"
    },
    {
      "citation_id": "16",
      "title": "Multimodal transformer with learnable frontend and self attention for emotion recognition",
      "authors": [
        "S Dutta",
        "S Ganapathy"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "17",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "19",
      "title": "WavLM: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "SALMONN: Towards generic hearing abilities for large language models",
      "authors": [
        "C Tang"
      ],
      "year": "2023",
      "venue": "SALMONN: Towards generic hearing abilities for large language models",
      "arxiv": "arXiv:2310.13289"
    },
    {
      "citation_id": "21",
      "title": "WavLLM: Towards robust and adaptive speech large language model",
      "authors": [
        "S Hu"
      ],
      "year": "2024",
      "venue": "WavLLM: Towards robust and adaptive speech large language model",
      "arxiv": "arXiv:2404.00656"
    },
    {
      "citation_id": "22",
      "title": "Sentiwordnet: A publicly available lexical resource for opinion mining",
      "authors": [
        "F Sebastiani",
        "A Esuli"
      ],
      "year": "2006",
      "venue": "Proceedings of the 5th international conference on language resources and evaluation"
    },
    {
      "citation_id": "23",
      "title": "Lexicon-based methods for sentiment analysis",
      "authors": [
        "M Taboada"
      ],
      "year": "2011",
      "venue": "Computational linguistics"
    },
    {
      "citation_id": "24",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "25",
      "title": "Opinion mining with deep recurrent neural networks",
      "authors": [
        "O Irsoy",
        "C Cardie"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "26",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "27",
      "title": "Aspect-based sentiment analysis using bert",
      "authors": [
        "M Hoang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 22nd nordic conference on computational linguistics"
    },
    {
      "citation_id": "28",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "29",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel"
      ],
      "year": "2020",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "30",
      "title": "Sentiment analysis in the era of large language models: A reality check",
      "authors": [
        "W Zhang"
      ],
      "year": "2023",
      "venue": "Sentiment analysis in the era of large language models: A reality check",
      "arxiv": "arXiv:2305.15005"
    },
    {
      "citation_id": "31",
      "title": "HCAM-Hierarchical Cross Attention Model for Multi-modal Emotion Recognition",
      "authors": [
        "S Dutta",
        "S Ganapathy"
      ],
      "year": "2023",
      "venue": "HCAM-Hierarchical Cross Attention Model for Multi-modal Emotion Recognition",
      "arxiv": "arXiv:2304.06910"
    },
    {
      "citation_id": "32",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "33",
      "title": "EmoCaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Z Li"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "34",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "35",
      "title": "Leveraging content and acoustic representations for efficient speech emotion recognition",
      "authors": [
        "S Dutta",
        "S Ganapathy"
      ],
      "year": "2024",
      "venue": "Leveraging content and acoustic representations for efficient speech emotion recognition",
      "arxiv": "arXiv:2409.05566"
    },
    {
      "citation_id": "36",
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "J Lu"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "37",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso"
      ],
      "year": "2008",
      "venue": "LREC"
    },
    {
      "citation_id": "38",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "39",
      "title": "MOSI: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "A Zadeh"
      ],
      "year": "2016",
      "venue": "MOSI: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "40",
      "title": "Is ChatGPT a good sentiment analyzer? A preliminary study",
      "authors": [
        "Z Wang"
      ],
      "year": "2023",
      "venue": "Is ChatGPT a good sentiment analyzer? A preliminary study",
      "arxiv": "arXiv:2304.04339"
    },
    {
      "citation_id": "41",
      "title": "Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert",
      "authors": [
        "Q Zhong"
      ],
      "year": "2023",
      "venue": "Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert",
      "arxiv": "arXiv:2302.10198"
    },
    {
      "citation_id": "42",
      "title": "Emotionflow: Capture the dialogue level emotion transitions",
      "authors": [
        "X Song"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "43",
      "title": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "authors": [
        "D Hu"
      ],
      "year": "2023",
      "venue": "ACL"
    },
    {
      "citation_id": "44",
      "title": "Hierarchical dialogue understanding with special tokens and turn-level attention",
      "authors": [
        "X Liu"
      ],
      "year": "2023",
      "venue": "Hierarchical dialogue understanding with special tokens and turn-level attention",
      "arxiv": "arXiv:2305.00262"
    },
    {
      "citation_id": "45",
      "title": "Emotion-anchored contrastive learning framework for emotion recognition in conversation",
      "authors": [
        "F Yu"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: NAACL 2024"
    },
    {
      "citation_id": "46",
      "title": "Multi-task self-supervised learning for robust speech recognition",
      "authors": [
        "M Ravanelli"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "47",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "S Yang"
      ],
      "venue": "Interspeech 2021, 2021"
    },
    {
      "citation_id": "48",
      "title": "ASR and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition",
      "authors": [
        "Y Li"
      ],
      "year": "2023",
      "venue": "ASR and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition",
      "arxiv": "arXiv:2305.16065"
    },
    {
      "citation_id": "49",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "51",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "7th International Conference on Learning Representations, ICLR 2019"
    },
    {
      "citation_id": "52",
      "title": "Locally confined modality fusion network with a global perspective for multimodal human affective computing",
      "authors": [
        "S Mai"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "53",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "54",
      "title": "DialogueTRM: Exploring multi-modal emotional dynamics in a conversation",
      "authors": [
        "Y Mao"
      ],
      "year": "2021",
      "venue": "EMNLP"
    },
    {
      "citation_id": "55",
      "title": "SMIN: Semi-supervised Multi-modal Interaction Network for Conversational Emotion Recognition",
      "authors": [
        "Z Lian"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "56",
      "title": "UniMSE: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "G Hu"
      ],
      "year": "2022",
      "venue": "UniMSE: Towards unified multimodal sentiment analysis and emotion recognition",
      "arxiv": "arXiv:2211.11256"
    },
    {
      "citation_id": "57",
      "title": "TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation",
      "authors": [
        "T Yun"
      ],
      "year": "2024",
      "venue": "NAACL"
    }
  ]
}