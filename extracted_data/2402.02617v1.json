{
  "paper_id": "2402.02617v1",
  "title": "Layer-Wise Analysis Of Self-Supervised Acoustic Word Embeddings: A Study On Speech Emotion Recognition",
  "published": "2024-02-04T21:24:54Z",
  "authors": [
    "Alexandra Saliba",
    "Yuanchao Li",
    "Ramon Sanabria",
    "Catherine Lai"
  ],
  "keywords": [
    "HuBERT",
    "acoustic word embeddings",
    "self-supervised speech models",
    "speech emotion recognition * Equal contribution"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The efficacy of self-supervised speech models has been validated, yet the optimal utilization of their representations remains challenging across diverse tasks. In this study, we delve into Acoustic Word Embeddings (AWEs), a fixed-length feature derived from continuous representations, to explore their advantages in specific tasks. AWEs have previously shown utility in capturing acoustic discriminability. In light of this, we propose measuring layer-wise similarity between AWEs and word embeddings, aiming to further investigate the inherent context within AWEs. Moreover, we evaluate the contribution of AWEs, in comparison to other types of speech features, in the context of Speech Emotion Recognition (SER). Through a comparative experiment and a layer-wise accuracy analysis on two distinct corpora, IEMOCAP and ESD, we explore differences between AWEs and raw self-supervised representations, as well as the proper utilization of AWEs alone and in combination with word embeddings. Our findings underscore the acoustic context conveyed by AWEs and showcase the highly competitive SER accuracies by appropriately employing AWEs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Unlike discrete embeddings of word tokens, speech representations are presented in a continuous form with variable lengths, which can impede their utility in certain scenarios such as query-by-example, voice search, keyword spotting, and word discovery  [1, 2, 3] . Therefore, fixed-dimensional speech representations, specifically Acoustic Word Embeddings (AWEs), have been employed to capture acoustic information in variable-length speech segments  [4] . This process condenses the information in a manner that guarantees segments containing the same words are mapped to similar embeddings. Previous studies have demonstrated that the use of AWEs allows for the application of simple calculations, such as cosine similarity, to measure the distance between different speech segments, as opposed to using computationally expensive methods like dynamic time warping  [5] .\n\nMoreover, owing to the contextualized nature of selfsupervised representations, a simple mean-pooling approach is sufficient to construct AWEs that capture acoustic discriminability which implicitly contains word meanings and sequential information  [6] . To this end, we aim to investigate whether the self-supervised AWEs outperform their continuous form (i.e., the raw frame-level representations) and whether they present similar contextual meanings to their corresponding word embeddings. To achieve this goal, we use Speech Emotion Recognition (SER) as the downstream task for the following reasons:\n\n1) The variable length of frame-level speech representations has posed a longstanding challenge for SER. To standardize the input length, SER usually suffers from either noise introduced by padding or information loss caused by chopping  [7] . Thanks to the fixed length, we hypothesize that AWEs serve as an ideal solution to address this problem, especially when use in combination with word embeddings.\n\n2) SER usually requires both acoustic and lexical information for satisfactory performance. However, the inter-modal incongruity sometimes makes it challenging to combine the two input sources  [8] . As AWEs can discriminate words compared to continuous speech representations, we hypothesize that AWEs align better with word embeddings via proper fusion approaches (e.g., cross-attention) due to their discretized characteristics.\n\nTo validate our hypotheses, in this work, we first compute the layer-wise similarity between AWEs and word embeddings. Subsequently, we build an SER system utilizing AWEs from a self-supervised model HuBERT as input and compare its performance with the raw HuBERT representations, as well as with Mel spectrogram baselines. Additionally, we incorporate BERT embeddings through simple concatenation and cross-attention for comparison, aiming to further understand how AWEs differ from raw self-supervised representations.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Self-Supervised Awes",
      "text": "Self-supervised speech models have been intensively studied in recent years and have proven effective in various speech arXiv:2402.02617v1 [cs.CL] 4 Feb 2024 tasks  [9] . Building upon this foundation, there has been an emerging exploration of self-supervised AWEs. Sanabria et al.  [6]  compared AWEs constructed from wav2vec 2.0  [10] , XLSR-53  [11] , and HuBERT  [12] . They noted that HuBERT AWEs by mean-pooling rival the SOTA on English AWEs, and despite being trained only on English, HuBERT AWEs evaluated on Xitsonga, Mandarin, and French consistently outperform the multilingual model XLSR-53 as well as wav2vec 2.0 trained on English.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ser Via Self-Supervised Representations",
      "text": "Prior studies have explored the utilization of self-supervised representations in SER. Li et al.  [13]  conducted a thorough investigation of wav2vec 2.0 across different layers and observed that representations extracted from the middle layer lead to the best SER performance. Furthermore, they confirmed the absence of paralinguistic information in wav2vec 2.0 representations. However, with two simple linear layers, these representations are still sufficient to achieve satisfactory results. Additionally, Morais et al.  [14]  utilized wav2vec 2.0 and HuBERT for SER and demonstrated the effectiveness of both of the two models.\n\nDespite the aforementioned progress of AWEs and SER with self-supervised learning, a question persists: How do self-supervised AWEs differ from the raw self-supervised representations? With this question in mind, we follow the path of previous work, aiming to investigate self-supervised AWEs to provide insights into the optimal utilization of pre-trained self-supervised models in downstream tasks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Corpora And Self-Supervised Awes",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Corpora Description",
      "text": "We adopt two distinct corpora: IEMOCAP  [15]  and ESD  [16] . IEMOCAP comprises five dyadic sessions involving ten actors (five male and five female), each engaging in scripted and improvised multimodal interactions. The corpus contains approximately 12 hours of speech annotated by three annotators with ten emotion classes. Consistent with prior research  [13] , we merged Happy and Excited, and excluded utterances without transcripts. This process results in a total of 5,500 utterances utilized in this study, each assigned one label from the following four classes: Angry, Happy, Neutral, and Sad.\n\nESD consists of 17,500 utterances spoken by 10 native English actors, each labeled with one of five different emotions (Angry, Surprise, Sad, Happy, Neutral). A notable feature is the inclusion of parallel data: pairs of utterances with the same content and speaker but different emotions. This design enables control for linguistic content, facilitating the isolation of emotion-related acoustic information, thereby resulting in higher SER accuracies.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Hubert Awes",
      "text": "We use HuBERT-base-ls960 1  , which has a 7-layer convolutional encoder, followed by a BERT-like encoder with 12 Transformer layers, and a projection layer. The CNN encoder uses 25ms windows with a 20ms frame rate. The masking is applied to the CNN's output, and the masked sequence is passed on to the BERT-like encoder.\n\nWe downsample acoustic data to 16kHz and pass it through HuBERT to extract frame-level representations from each of the 12 layers (denoted as L1-L12), as well as from the output of the CNN (denoted as L0). Subsequently, we obtain word time steps using the Montreal Forced Aligner  [17] . Following  [6] , we apply mean-pooling over the frames of each word to obtain the AWEs as the mean-pooling has been proven highly effective compared to sophisticated learned pooling approaches  [18] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Similarity Between Awes And Lexical Embeddings",
      "text": "In previous research on the layer-wise analysis of selfsupervised speech models, Pasad et al.  [19, 20]  demonstrated that these models encode speech information, progressing from the frame-level to phonetic-level representations and then to word identity and meaning. Li et al.  [13] , Lin et al.  [21] , and Zhu et al.  [22]  applied the representations to prosody-related tasks, confirming the distinctive contributions of different layers. In this study, however, we compare HuBERT AWEs with BERT 2  embeddings, aiming to identify the layers of AWEs that are most sensitive to linguistic information and to explore the contextual distribution of AWEs.\n\nTo this end, we compute the Local Neighborhood Similarity (LNS)  [23]  of a word w between the lexical and acoustic embedding spaces. The underlying idea behind this practice is that if the feature vectors of a given word in two different embedding spaces encode similar contextual information, they will share common local neighbors. The LNS of a word w is determined by the similarity between the set of K nearest neighbors in the HuBERT AWEs (HBA) and BERT word embeddings (BERT ), respectively:\n\nwhere Jaccard distance (intersection over union) between two sets is defined as:\n\nAs an identical pattern is obtained on ESD, we present the results only on IEMOCAP for brevity, which are plotted in Fig.  1 . It is evident that the similarity between acoustic and lexical word embeddings is consistently low, typically ranging between 1% and 2.5%. Across all layers, there is a noticeable peak at layer 9, suggesting that layer 9 encodes acoustic word meanings that more align with the information encoded by the language model, which is consistent with  [20] . Even though, it is worth noting that the value remains low. Moreover, we extract the 5 closest acoustic and lexical neighbors for the words anyhow and after from layer 9 as examples to illustrate the dissimilarity between the two distributions. As seen in Table  1 , while there is one word in common between the two sets of neighbors, acoustic neighbors often share a common vowel sound with the target word, while lexical neighbors tend to be more syntactically related. Alongside the findings from Fig.  1 , it is noted that, despite AWEs being able to acoustically discriminate words  [6] , they demonstrate a distribution distinct from that of word embeddings in terms of context, which we refer to as acoustic context.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experimental Settings",
      "text": "To delve deeper into the distinct characteristics of AWEs, we conduct a downstream task-SER, using AWEs as input.\n\nGiven that SER leverages contributions from both acoustic and lexical information, and following a prior study that investigated the use of raw self-supervised representations  [13] , we perform a similar approach to explore the differences between AWEs and raw representations from HuBERT. We train a simple neural network classifier with two dense hidden layers (sizes 128 and 16), employing ReLU activation, as sophisticated networks are deemed unnecessary when utilizing powerful self-supervised representations for down-stream tasks  [13, 9] . As our objective is to self-contrast for novel findings rather than compare with the literature, we conduct an 80/20 train/test split instead of cross-validation. Results are measured using Weighted Accuracy (WA).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparative Study",
      "text": "We compare three types of acoustic features: Mel spectrograms, raw HuBERT representations, and HuBERT AWEs (referred to as Mel, HuBERT, and AWEs for brevity). For the latter two, we compute the accuracy from every layer and report the average score. Additionally, we explore their fusion with BERT embeddings through concatenation and cross-attention (audio and BERT features attend to each other and are then combined) using the best-performing layer. Note that the feature dimension of Mel differs from that of BERT. Although it is possible to encode Mel to the same dimension as that of BERT for cross-attention, the performance is significantly inferior to HuBERT and AWEs. Thus, for brevity, we only report concatenation for Mel. Each model is trained five times, and we report the average score.\n\nThe results are presented in Table  2  and 3 , and the following findings are observed. 1) Concatenating BERT embeddings significantly improves the performance of Mel. However, the contribution of such lexical information is less pronounced when integrated into HuBERT or AWEs. This observation highlights a previous finding that self-supervised speech features already convey implicit lexical information compared to raw speech features  [6, 19, 13] , rendering BERT embeddings somewhat redundant.\n\n2) On IEMOCAP: The performance using AWEs is slightly inferior to that of using HuBERT, which is reasonable due to information loss caused by mean-pooling during AWEs construction. While AWEs contain context, which may contribute as additional information, such an advantage does not manifest. On the contrary, although AWEs still exhibit lower performance than HuBERT when fused with BERT through cross-attention, the gap between them diminishes (only 0.04). This phenomenon can be attributed to discretized acoustic features (i.e., AWEs) being more easily aligned with word embeddings compared to continuous ones through cross-attention, a mechanism designed to capture relatedness between two representations. This alignment naturally leads to higher enhancement.\n\n3) On ESD: Concatenating BERT embeddings decreases performance compared to using acoustic features alone. On the other hand, cross-attention enhances performance for Hu-BERT, yet has the opposite effect for AWEs. These phenomena are attributed to the fact that sentences in ESD remain unchanged for every emotion, having no contribution to emotion discrimination. Concatenating BERT embeddings only brings redundancy, thereby decreasing performance. Nonetheless, cross-attention with BERT enhances performance as it models the relatedness between the source and target features. Un- like concatenation, which uses the two directly as input, the same BERT embeddings contribute differently to SER based on different acoustic features serving as the query in crossattention. Moreover, we observe that when using acoustic features alone, AWEs outperform HuBERT, contrary to the findings on IEMOCAP. Given that sentences in ESD remain consistent for every emotion, this setup facilitates the acoustic context (explained in Sec. 4.1) as an indicator for SER. In contrast, sentences in IEMOCAP vary, making the acoustic context less predictable and challenging to leverage.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Layer-Wise Accuracy Analysis",
      "text": "Subsequently, we perform a layer-wise accuracy analysis, illustrating mean and std scores in Fig.  2  and 3 . As similar layer-wise patterns have been found in both corpora, we omit IEMOCAP for brevity (the major differences have been described in the last section with Table  2 ). By comparing the two figures, we can observe several differences between HuBERT raw representations and AWEs: 1) Using AWEs alone consistently outperforms the two fusions, yet using HuBERT alone underperforms crossattention at most layers. 2) On the shallow layers, AWEs have significant advantages over raw HuBERT, and the optimal layer for using AWEs is around layer 3 instead of the middle layer. 3) When using raw HuBERT, the discrepancy among the three input types diminishes after the middle layer. This is because the deeper layers start encoding lexical information, which holds little value in ESD. In contrast, when using AWEs, the discrepancy persists, showcasing the advantage of  the acoustic context represented by AWEs in situations where lexical information has minimal impact. 4) When using AWEs, the last layer does not exhibit a drop that commonly observed in raw self-supervised representations  [19, 13, 20] . Instead, the performances rise, offering a solution for more effectively leveraging the last layer representations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we conduct a layer-wise analysis of AWEs derived from HuBERT. Specifically, we measure the similarity between AWEs and BERT embeddings, discovering that AWEs exhibit a distinct context distribution from word embeddings, oriented toward the acoustic aspect. Furthermore, through a comparative study on two distinct corpora, we demonstrate the advantages of AWEs over other types of speech features. Alongside a layer-wise accuracy analysis, we unveil the relationship between AWEs and raw HuBERT representations, as well as the practice of utilizing AWEs alone and in combination with word embeddings. Our findings are expected to provide insights into the realm of self-supervised speech models and inspire future research on better utilizing self-supervised representations in different speech tasks.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: It is evident that the similarity between acoustic and",
      "page": 2
    },
    {
      "caption": "Figure 1: LNS per layer with different numbers of neighbors.",
      "page": 3
    },
    {
      "caption": "Figure 1: , it is noted that, despite AWEs being",
      "page": 3
    },
    {
      "caption": "Figure 2: and 3. As similar",
      "page": 4
    },
    {
      "caption": "Figure 2: Layer-wise analysis of SER accuracy of HuBERT w/",
      "page": 4
    },
    {
      "caption": "Figure 3: Layer-wise analysis of SER accuracy of AWEs w/ and",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Word": "anyhow",
          "Lexical Neighbors": "anyway, didnâ€™t, say,\nit, well",
          "Acoustic Neighbors": "anyway, while, now,\nannie, why"
        },
        {
          "Word": "after",
          "Lexical Neighbors": "be, a, at,\ngive, i",
          "Acoustic Neighbors": "at, chapter, asked,\nact, grasshopper"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Unsupervised spoken keyword spotting via segmental DTW on gaussian posteriorgrams",
      "authors": [
        "Yaodong Zhang",
        "James Glass"
      ],
      "year": "2009",
      "venue": "2009 IEEE Workshop on Automatic Speech Recognition & Understanding"
    },
    {
      "citation_id": "3",
      "title": "Keyword spotting based on the analysis of template matching distances",
      "authors": [
        "Mohamed Barakat",
        "Christian Ritz",
        "David Stirling"
      ],
      "year": "2011",
      "venue": "2011 5th International Conference on Signal Processing and Communication Systems (ICSPCS)"
    },
    {
      "citation_id": "4",
      "title": "Template-based continuous speech recognition",
      "authors": [
        "Mathias De Wachter",
        "Mike Matton",
        "Kris Demuynck",
        "Patrick Wambacq",
        "Ronald Cools",
        "Dirk Van Compernolle"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "5",
      "title": "Word-level acoustic modeling with convolutional vector regression",
      "authors": [
        "Stephen Andrew L Maas",
        "Miller",
        "M Tyler",
        "Andrew O'neil",
        "Patrick Ng",
        "Nguyen"
      ],
      "year": "2012",
      "venue": "Proc. ICML Workshop Representation Learn"
    },
    {
      "citation_id": "6",
      "title": "Fixed-dimensional acoustic embeddings of variablelength segments in low-resource settings",
      "authors": [
        "Keith Levin",
        "Katharine Henry",
        "Aren Jansen",
        "Karen Livescu"
      ],
      "year": "2013",
      "venue": "2013 IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "7",
      "title": "Analyzing acoustic word embeddings from pre-trained selfsupervised speech models",
      "authors": [
        "Ramon Sanabria",
        "Hao Tang",
        "Sharon Goldwater"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "8",
      "title": "Chunk-level speech emotion recognition: A general framework of sequence-to-one dynamic temporal modeling",
      "authors": [
        "Wei-Cheng Lin",
        "Carlos Busso"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Cross-attention is not enough: Incongruity-aware dynamic hierarchical fusion for multimodal affect recognition",
      "authors": [
        "Yaoting Wang",
        "Yuanchao Li",
        "Paul Liang",
        "Louis-Philippe Morency",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2023",
      "venue": "Cross-attention is not enough: Incongruity-aware dynamic hierarchical fusion for multimodal affect recognition",
      "arxiv": "arXiv:2305.13583"
    },
    {
      "citation_id": "10",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Andy Yist Y Lin",
        "Jiatong Liu",
        "Xuankai Shi",
        "Guan-Ting Chang",
        "Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark"
    },
    {
      "citation_id": "11",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "12",
      "title": "Unsupervised crosslingual representation learning for speech recognition",
      "authors": [
        "Alexis Conneau",
        "Alexei Baevski",
        "Ronan Collobert",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2021",
      "venue": "Unsupervised crosslingual representation learning for speech recognition"
    },
    {
      "citation_id": "13",
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Exploration of a Self-Supervised Speech Model: A Study on Emotional Corpora",
      "authors": [
        "Yuanchao Li",
        "Yumnah Mohamied",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "Edmilson Morais",
        "Ron Hoory",
        "Weizhong Zhu",
        "Itai Gat",
        "Matheus Damasceno",
        "Hagai Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "IEMO-CAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "17",
      "title": "Emotional voice conversion: Theory, databases and ESD",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "18",
      "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi",
      "authors": [
        "Michael Mcauliffe",
        "Michaela Socolof",
        "Sarah Mihuc",
        "Michael Wagner",
        "Morgan Sonderegger"
      ],
      "year": "2017",
      "venue": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi"
    },
    {
      "citation_id": "19",
      "title": "A correspondence variational autoencoder for unsupervised acoustic word embeddings",
      "authors": [
        "Puyuan Peng",
        "Herman Kamper",
        "Karen Livescu"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "20",
      "title": "Layer-Wise Analysis of a Self-Supervised Speech Representation Model",
      "authors": [
        "Ankita Pasad",
        "Ju-Chieh Chou",
        "Karen Livescu"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "21",
      "title": "Comparative layer-wise analysis of self-supervised speech models",
      "authors": [
        "Ankita Pasad",
        "Bowen Shi",
        "Karen Livescu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "On the utility of self-supervised models for prosody-related tasks",
      "authors": [
        "Guan-Ting Lin",
        "Chi-Luen Feng",
        "Wei-Ping Huang",
        "Yuan Tseng",
        "Tzu-Han Lin",
        "Chen-An Li",
        "Hung-Yi Lee",
        "Nigel Ward"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "23",
      "title": "Deep investigation of intermediate representations in self-supervised learning models for speech emotion recognition",
      "authors": [
        "Zhi Zhu",
        "Yoshinao Sato"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)"
    },
    {
      "citation_id": "24",
      "title": "Embedding comparator: Visualizing differences in global structure and local neighborhoods via small multiples",
      "authors": [
        "Angie Boggust",
        "Brandon Carter",
        "Arvind Satyanarayan"
      ],
      "year": "2022",
      "venue": "27th international conference on intelligent user interfaces"
    }
  ]
}