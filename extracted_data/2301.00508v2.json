{
  "paper_id": "2301.00508v2",
  "title": "Emogator: A New Open Source Vocal Burst Dataset With Baseline Machine Learning Classification Methodologies",
  "published": "2023-01-02T03:02:10Z",
  "authors": [
    "Fred W. Buhl"
  ],
  "keywords": [
    "speech emotion recognition",
    "vocal bursts",
    "affect bursts",
    "nonverbal vocalizations",
    "affective computing",
    "machine learning",
    "dataset"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Vocal Bursts -short, non-speech vocalizations that convey emotions, such as laughter, cries, sighs, moans, and groans -are an often-overlooked aspect of speech emotion recognition, but an important aspect of human vocal communication. One barrier to study of these interesting vocalizations is a lack of large datasets. I am pleased to introduce the EmoGator dataset, which consists of 32,130 samples from 357 speakers, 16.9654 hours of audio; each sample classified into one of 30 distinct emotion categories by the speaker. Several different approaches to construct classifiers to identify emotion categories will be discussed, and directions for future research will be suggested. Data set is available for download from https://github.com/fredbuhl/EmoGator.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are central to human experience-they motivate & inform much of what we do. Recognizing emotions in others has been a longstanding area of interest. Perhaps the first scientific study of emotion recognition was the work of Duchenne  [1]  in 1862, who collected photographs of facial expressions elicited via electrically stimulating facial muscles.\n\nThe question of how many emotions there are remains open. Duchenne identified 13 primary emotions, and 60 combinations, from facial expression. A recent study by Cowen & Keltner found that humans were able to reliably identify 28 distinct emotions from facial expression  [2] . Another recent study by the same team  [3]  indicated that humans self-report as many 27 distinct emotions; these responses were collected from subjects reacting to short video clips. The emotion categories presented as gradients, which occasionally overlapped with other emotion categories; multiple emotions were elicited to varying degrees by a given stimulus.\n\nHumans often express emotion vocally by varying speech prosody-the audio characteristics of speech. One study  [4]  found that 12 distinct emotions could be recognized from speech prosody-and this across two cultures-a previous study  [5]  had found cross-cultural emotion recognition with subjects across five nations, although an in-group advantage was noted.\n\nHumans also express emotion via brief, non-speech sounds called vocal bursts, also referred to as \"affect bursts\", \"emotional vocalizations\", or \"nonverbal vocalizations\"-sounds like laughter, cries, sighs, moans, and groans-vocalizations that are not speech, and likely predate it, evolutionarily speaking. In  [6]  humans were found to be able to distinguish 14 emotional states from these vocal bursts. And a recent paper  [7]  by Cowen, Keltner, and others showed the ability to distinguish 24 emotional states from these brief vocalizations.\n\nThe ability to detect and express emotion via human vocalization appears early in human development  [8, 9, 10, 11, 12] . It is important to language and social development; people who have difficulties in discerning emotions in others, due to brain injury, or conditions like Autism Spectrum Disorder, experience difficulties communicating effectively. People with auditory affective agnosia  [13]  cannot discern emotional cues in speech, though they can still understand words, while people afflicted with dysprosody  [14]  speak in a monotone, without intonation or emotional affect; this can also appear in people with Parkinson's disease  [15] . Any impairment of these abilities has a severe effect on communication and socialization with others, underlining the importance of evoking and understanding emotional expression.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "The Problem At Hand",
      "text": "Interactions with computers via speech recognition is now commonplace via \"smart speakers\" and their associated virtual assistants such as Siri, Alexa, and Google Assistant. Currently, none of these systems are capable of detecting emotion from the speech audio signal; the signal is converted to text (sometimes with comic results) via speech-to-text deep learning models, but any emotional content present in the speech's prosody is ignored. For some applications, where how a word is said may be as important (or more important) than what word was said, this could be a severe limitation. And, given their non-speech nature, vocal bursts are completely ignored by these systems.\n\nComputers capable of emotion recognition from speech have numerous applications; more life-like responses from nonplayer characters in video games, for example. In early childhood education, awareness of the young user's emotional state would be helpful to gauge interest, frustration, or boredom; they could also be used to assess and improve the child's emotional intelligence (or \"EQ\")  [16] . The ability to detect emotion could detect signs of loneliness, agitation, or depression  [17] , a special concern for isolated people, such as aging-in-place seniors. Social Robots-robots designed to interact closely with humans-benefit from emotion recognition  [18] ; such systems can even be used to gauge the robot's appeal to its human users  [19] . The argument has been made that we will never claim human level performance in speech recognition until we can achieve human-level speech emotion recognition, since humans are capable of both  [20] . (It should be noted that this area is just one aspect of the larger field of Affective Computing pioneered by Rosalind Picard  [21] , which involve not only emotion recognition, but also emotional expression, and emotionally-aware decision making.) Despite the limitations of current commercial products, Speech Emotion Recognition (SER) is an area of longstanding interest in computer science  [22] . In 1996, Cowie et al.  [23]  developed a technique of automatically detecting landmarks in a speech signal and collect summary statistics, which were then used to quantify speech characteristics for four emotion categories. Various approaches have been used in speech emotion recognition over the years  [24] -Mel-Frequency Cepstrum Coefficients (MFCC), Gaussian Mixture Models (GMM), Support Vector Machines (SVM), Hidden Markov Models (HMM), and neural network techniques such as LSTM  [25]  and, more recently, deep learning neural networks have been used.\n\nThe research described here examines the largely-neglected area of vocal bursts, enabled by a newly-collected dataset. A number of machine learning techniques will be explored, with varying levels of performance, along with suggested directions for future research.\n\nThe primary inspiration for this work was  [7] ; the vocal burst dataset, which the authors graciously provide to other researchers, was the largest vocal burst dataset available when released. That dataset consisted of 2,032 vocal burst samples with 30 emotion categories; as mentioned, humans were able to reliably distinguish 24 categories. The fundamental question at the basis of this current work: if humans can distinguish 24 emotion categories from vocal bursts, can machines do so as well?\n\nWhile the Cowen et al. dataset was the largest available at the time, it was still relatively small, and the categories were not evenly represented; most machine learning approaches benefit greatly from larger numbers of samples, and balanced categories. This author determined that a larger dataset would need to be collected, and several different approaches evaluated, to find the best-performing emotion classifier.\n\n2 The dataset, and a spectrum of deep learning and other methodologies for classification",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Dataset",
      "text": "The EmoGator dataset consists of 32,130 vocal bursts, produced by 357 speakers, providing 16.9654 hours of audio; average sample length is 1.901 seconds. Each speaker recorded three samples for each of 30 emotion categories, providing 90 samples per speaker-this provided for an equal number of samples for each category, and for each speaker, assuring equal representation in the dataset. The emotion categories were the same 30 categories used in  [7] : Adoration, Amusement, Anger, Awe, Confusion, Contempt, Contentment, Desire, Disappointment, Disgust, Distress, Ecstasy, Elation, Embarrassment, Fear, Guilt, Interest, Neutral, Pain, Pride, Realization, Relief, Romantic Love, Sadness, Serenity, Shame, Surprise (Negative) Surprise (Positive), Sympathy, and Triumph. The speakers were provided text prompts with scenarios to help elicit the emotional response; the prompts used were a modified and expanded version used by  [7] , and listed in the online supplemental materials 1  .\n\nData was collected from unpaid volunteers, and also crowd-sourced workers via Mechanical Turk; a website was created where speakers could record and play back their samples using their own computer or mobile device.\n\nThe audio files were originally recorded at 44100 or 48000 Hz, depending on the participant's hardware, and stored as mp3 files. Each individual recording file is named with a six-digit non-sequential user id, a two-digit emotion ID , and a single-digit recording number  (1, 2, 3) . Since the files are labeled by user ID, researchers can break any train, test, or validation set by speaker, ensuring a given speaker's submission appears in only in one of the sets. (Efforts were taken to avoid a speaker providing more than one contribution, though this cannot be 100% guaranteed). All participants provided informed consent, and all aspects of the study procedures and design were approved by the University of Florida's Institutional Review Board (IRB).\n\nQuality assurance was a major part of the data collection process; there were entire submissions that were silent recordings, or only contained random background noise. Some contributors apparently misunderstood the assignment, recording themselves reading the names of the categories, or phrases related to the categories. Many speakers provided a large number of high quality samples, but also submitted problematic ones, usually due to audio issues such as background noises (for example, phone chimes or background traffic sounds); another issue was excessive breath noise picked up on the microphone. In these instances, speakers would be asked to re-record the problematic samples in order to maintain the same number of samples per speaker.\n\nIn addition, some speakers did not seem to be able to produce evocative speech from the prompts; their responses didn't convey distinct emotions. This last group was omitted from the dataset. As a result of all these factors, this dataset will therefore almost certainly have a bias toward the emotional expressions of North American English-speaking people, as the author, and sole evaluator, shares that personal history. The dataset will be publicly available at the following URL: https://github.com/fredbuhl/EmoGator.\n\nSeveral different steps were evaluated to preprocess the data. Normalizing the data so the range of each audio sample was within a [-1,1] range was universally used (for training, validation and testing). Denoising audio files and trimming silence from the beginning and end of audio files was evaluated as well. Augmenting data by creating pitch and time shifted variants of each sample was also explored.\n\nWhile this dataset was being collected, a company named Hume AI collected their own vocal burst dataset, a subset of which was made available for the The ICML 2022 Expressive Vocalizations Workshop and Competition  [26]  as the Hume-VB dataset. This dataset consists of 59,201 vocalizations from 1702 speakers, with 10 emotion categories (Amusement, Awe, Awkwardness, Distress, Excitement, Fear, Horror, Sadness, Surprise, and Triumph). Each sample has been rated by reviewers, with [0:100] intensity scores for every emotion category provided for each sample. This Hume-VB dataset was also used for the ACII 2022 Affective Vocal Bursts Workshop and Competition  [27]  There are several differences between the EmoGator dataset to Hume-VB dataset:\n\n1. EmoGator has 30 distinct emotion categories, with each sample belonging to a single category determined by the speaker's intent. Hume-VB has 0-100 ratings for all 10 of its categories provided by reviewers for each sample-the listener's interpretation, which may in some cases be very different than the speaker's intent. At time of publication, EmoGator appears to be the largest vocal burst dataset publicly available.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Classification Methodologies",
      "text": "A number of different techniques used in speech emotion recognition, sound classification, and elsewhere have been used for these sorts of audio classification problems.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Spectrogram Approaches",
      "text": "Some approaches to audio classification involve creating a time-frequency spectrogram (or spectrogram-like) representation of the audio signals, which can be created a number of ways. Typically, the Short-Time Fourier Transform, or STFT  [28]  is used, which provides the amplitude of different frequencies over time; a variant, the Mel spectrogram, modifies the frequencies to correspond to the Mel scale  [29] , which closely matches human perception of differences in pitch. MFCC provide a spectrum-like \"cepstrum\"  [30] , which, while using Mel frequencies, provides the log of the amplitude in decibels over the phase shift, instead of the time domain used for spectrograms. The resulting spectrograms or cepstrograms are used as features for other machine learning approaches.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "1D Cnn Training On Raw Waveforms",
      "text": "In  [31] , Dai et al. use a direct approach to sound classification; one-dimensional CNNs that work with the raw input waveforms, without using spectograms or some other representation as an intermediate-step feature detector. networks consisting of layers of one-dimensional convolutional neural networks (1D CNNs)  [32]  were used for this.  [31]  worked on the UrbanSound8k dataset  [33] , which, with its 10 categories and 8,732 samples, is a bit smaller than the EmoGator dataset. Testing various architectures, they reported up to 71.68% accuracy on an 18-layer model, which is competitive with CNNs using spectrograms of the same dataset. For the EmoGator, dataset, we developed an 18-layer network as in  [31] , and added dropout layers after each 1D convolution to help prevent overfitting.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Random Forests",
      "text": "Random forest classifiers  [34]  were also explored. A random forest is constructed by generating multiple random decision trees, each constructed from a random subset of the dataset, using a random subset of each sample's features.\n\nOnce constructed, each tree in the forest casts a single vote for a class, and the class with the most votes chosen the winner. This approach can be used on raw data or with spectrogram-like representations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Large Pre-Trained Speech Models",
      "text": "Several teams in the 2022 ICML Expressive Vocalizations Workshop and Competition made use of large pre-trained speech models  [35] ,  [36] ,  [37] ,  [38] ,  [39] ,  [40] . Two models were used frequently: WavLM  [41]  and HuBERT  [42] .\n\nBoth of these are self-supervised speech representation models, which are built using transformer architectures  [43] ; transformers have been applied successfully to a large number of domains-they are typically very large models, which have been trained on large datasets for significant amounts of time. Having access to these pre-trained models can produce better results then can be achieved by training other (usually smaller) datasets in isolation.\n\nWavLM is a large scale self-supervised pre-trained speech model-The \"Large\" version of WavLM was trained on 94k hours of speech, and has 316.62M parameters. HuBERT is a similar model, the \"large\" version has 317M parameters, and was trained on 60k hours of audio on 128 Graphic Processing Units (GPUs). Both WavLM and HuBERT are built upon wav2vec 2.0  [44] , a \"contrastive learning\" self-supervised speech model, which itself is trained on 64 GPUs; the output of wav2vec is used as the input to HuBERT or WavLM, providing them higher-level features to build and train upon.\n\nWavLM experiments were run by first running the EmoGator training, validation, and test data through a pre-trained WavLM model, storing the last hidden layer as a new representation for each sample, using a 70% / 15% / 15% train-validation-test split. The hidden layers from the training data were then used as input to train a single fully connected network, using validation data to find the appropriate stopping point; once the ideal models were determined, they were run on the test data. The HuBERT model was used in a identical fashion-using the last hidden later of the HuBERT model instead of WavLM as the input to the fully-connected layer.\n\nIncorporating WavLM and HuBERT in this work was greatly aided by the HuggingFace transformer libraries  [45] , which, while initially covering natural language processing, have now expanded into many other areas. The benefit of being able to incorporate an large pre-trained language model with a few lines of code cannot be overstated.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ensemble Methods",
      "text": "Ensemble methods attempt to improve performance by combining the outputs of multiple models, with suitable training and weighting; the aggregate often outperforms the individual models. Two approaches were used for the EmoGator data: Ensemble A took the n-length output (where n was the number of emotion categories) produced by the WavLM-and-HuBERT-single-layer model and averaged them together, using the resulting average to pick the most likely emotion category. Ensemble B concatenated the last hidden layers from WavLM and HuBERT, and then trained single fully-connected layer on those inputs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Platform & Hardware Requirements",
      "text": "Most work on this project was performed on the University of Florida's HiperGator-AI cluster, which uses 80G A100 GPUs; one A100 should be sufficient to run all the models included, but the code may not run directly on systems with lower memory GPUs unless modifications to parameters such as batch size etc. are implemented.\n\n3 3. Results",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "1D Cnn Training On Raw Waveforms",
      "text": "For one-dimensional convolutional neural networks, the best results against the full dataset were with a 70% / 15% / 15% train/validation/test split, using an 18-layer 1D CNN based on  [31] , but with dropout layers after each convolution.\n\nA relatively low dropout rate of 0.07 was optimal. All experiments were run with a batchsize of 128 and an Adam optimizer with a learning rate of 0.001. Several statistics were calculated; For the full 30-category dataset, the average F1 score was 0.270. F1 scores and other accuracy metrics, with breakdowns by category, are shown in Table  1 ; a confusion matrix is provided in Figure  1  based on the run with the highest F1 score.\n\nThe experiments above were all run with normalized audio data, but without denoising the audio signal or trimming silence from the beginning and end; earlier experiments with a 70%/30% train/test split revealed that denoising or trimming the audio signal reduced performance.\n\nData augmentation was also explored; two-to-three times larger \"stretched\" version of the 70% / 15% / 15% training set were produced by creating new samples by performing independent pitch and tempo shifts of the audio samples; however the stretched training sets produced lower performance than the original training set, despite making adjustments to the amount of pitch and tempo scaling.\n\nIn reviewing these results, it is clear that some categories are much harder (or easier) to identify; for example, the F1 score (0.056) for Embarrassment, the worst performing category, is much lower than the highest performing category, Amusement (0.627). The confusion matrix illustrates the problem well; it shows that certain types of vocal bursts are simply difficult to place in the correct category. Per the confusion matrix, Embarrassment (with only 7 samples correctly identified) was more likely to be interpreted as Shame  (16)  or Guilt  (10) ; all closely related concepts that can produce similar vocalizations. This is an inherently difficult problem, which helps explain why humans could only reliably distinguish 24 emotion categories in  [7] .\n\nBy selectively removing emotion categories that performed poorly, it would be expected that overall performance should improve. Using the F1 score as a metric, the lowest scoring categories were removed, creating 24-count, 16-count, and 10-count subsets of the dataset. Interestingly, three of the bottom-scoring six categories removed to make the 24-count subset were also not identifiable by humans in  [7] ; two other categories unidentifiable by humans were removed in the 16-count subset-showing some commonality between the two datasets, and also illustrating the difficulties humans and algorithms have with certain emotion categories, even across studies.\n\nThe same 1D CNN model architecture, hyperparameters, and validation approaches were used. Results are in Table  2 ; we do see improvement as the more ambiguous categories are eliminated.\n\nBy creating binary 1D CNN classifiers, with one classifier for each possible pair of emotion categories, we can illustrate which pairs are the easiest to distinguish. Using the same model architecture and 70%/15%/15% split, and using the F1 score as a similarity metric (on a [0,1] scale, where 1 is least similar), a similarity matrix was created based on the 435 permutations for the 30 categories, and a dendrogram displaying relationships between each category was generated from that matrix (Figure  2 ). The dendrogram illustrates the most easily confused or distinguished categories. For example, it shows how easily the Amusement category is distinguished from all other categories, and shows Realization and Contempt as the most similar-and therefore most confused-categories, despite being very different emotions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Random Forests",
      "text": "As shown in  [34] , an approach known as Random Forests has been used on a number of small-count, small number-ofcategory datasets, which suggested it might be an apt choice for the EmoGator dataset. The classifier (which is included in the scikit-learn library  [46] ) was trained against Mel-Frequency Cepstral Coefficients (MFCC) of the audio data; runs were completed for the full 30 category dataset, along with 24, 16, and 10 category subsets. Results all under-performed the 1D CNN results, however (see Table  3 ).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Large Pre-Trained Speech Models",
      "text": "Results were calculated using the last hidden layer of WavLM and HuBERT models connected to a single fullyconnected network layer. A variant of Ensemble B incorporated two fully-connected layers (labeled \"2-layer FC\"), which resulted in a moderate improvement. These results are presented, along with others, in Table  4 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ensemble Methods",
      "text": "Results were calculated using averaged output from the trained fully-connected layers appended on WavLM and HuBERT model runs (Ensemble A), and concatenated last-hidden-layer outputs from both models (Ensemble B), which were then used to train a single fully-connected layer. The WavLM and HuBERT single fully-connected layers that had the highest average F1 scores on the validation dataset were used to keep the test data from tainting the ensemble model.\n\nResults for the Ensemble methods are presented in Table  4 , along with summary data from all the EmoGator experiments.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "Returning to our research question-whether, like humans, machines could reliably identify 24 emotion categories-it appears that the results achieved for the 24-emotion category runs did not approach assumed human proficiency, with a top F1 score of only 0.344 via the 1D CNN method on a 24-category subset. Results for the 24, 16, and 10-category subsets were better than the full 30-category runs, with the 10-category runs performing the best, again using the 1D CNN approach, scoring 0.597. (To put these results into perspective, a random guess for a 24-category subset would be right only 4.2% of the time; a 10-category random guess would be right only 10% of the time-so these results are much better than pure chance.)\n\nOne potential use of this dataset would be to use it to measure how accurate human performance is for vocal burstswhether the category the speaker intended to convey is correctly identified by listeners. Other studies have used gradient rating scales for each category provided by the listener, without necessarily linking back to the ground truth of the Collecting more data would no doubt improve these results; this vocal burst dataset, while (currently) the largest publicly available, is still small by machine learning standards. Evaluating subsets of the dataset makes the situation even worse; when looking at say, 10-category subsets, only 1 3 of the dataset is used. Using more complex ensemble methods seems a promising way forward; while the ensemble results here did not exceed the 1D CNN results, it's possible that incorporating more individual models could increase accuracy beyond what we've been able to achieve. One topic that was not explored here is generating vocal bursts; the author will be next exploring methods such as Generative Adversarial Networks (GANs) and Stable Diffusion models to generate vocal bursts; ideally these could be tailored for an individual speaker by providing a few audio samples(the ICML competition had this as one of their challenges). More data will help, but it may be that audio data alone will be insufficient to properly classify vocal bursts. Datasets and models incorporating video as well as audio data-not only to look at facial expressions, but also any visual cues that might evoke a vocal burst-could improve accuracy. The words spoken by the utterer, and others around them, before or after a vocal burst may also aid in identification. (It may be, however, that there are inherent limits far short of certainty for vocal burst classification, regardless of any additional information that can be gathered-often cries of sadness and amusement sound the same, and people sometimes say they are not sure \"whether they should laugh or cry\".) Another area to explore are the demographics of the speakers; their age, gender, place of origin, and cultural background could all come into play on classifying bursts. These demographic concerns also extend to the person evaluating the quality of the sample; ideally, the demographic aspects of the reviewer should match those of the submitter for best quality.\n\nBeyond the demographic aspects, each individual's unique character and personality certainly comes into play when they generative vocal bursts-so prior experience with the utterer could be key in improving accuracy, especially if the model's weights can be fine-tuned based on these experiences.\n\nIt is hoped that the EmoGator dataset will be introduce researchers to the fascinating area of vocal bursts; hopefully other researchers could incorporate this dataset into still-larger collections in the future, \"paying it forward\" by making those datasets publicly available.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: based on the run with the highest F1 score.",
      "page": 5
    },
    {
      "caption": "Figure 2: ). The dendrogram illustrates the most easily confused or distinguished categories. For",
      "page": 5
    },
    {
      "caption": "Figure 1: The confusion matrix generated by the 18 layer 1D CNN with dropout layers.",
      "page": 7
    },
    {
      "caption": "Figure 2: The dendrogram generated from F1 scores (range [0,1]) between pairs of emotion categories.",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Precision, Recall, and F1 scores from a best run of the 18 layer 1D CNN, with dropout layers.",
      "page": 6
    },
    {
      "caption": "Table 2: 1D CNN runs with 24, 16, and 10 category subsets of the EmoGator dataset, compared to the 30 category full",
      "page": 6
    },
    {
      "caption": "Table 3: Random Forest runs with 24, 16, and 10 category subsets of the EmoGator dataset, compared to the 30 category",
      "page": 8
    },
    {
      "caption": "Table 4: , along with summary data from all the EmoGator experiments.",
      "page": 8
    },
    {
      "caption": "Table 4: All results from the various approaches and dataset subsets used.",
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The Mechanism of Human Facial Expression. Cambridge books online",
      "authors": [
        "G Duchenne",
        "G De Boulogne",
        "R Cuthbertson",
        "A Manstead",
        "K Oatley"
      ],
      "year": "1990",
      "venue": "The Mechanism of Human Facial Expression. Cambridge books online"
    },
    {
      "citation_id": "2",
      "title": "What the face displays: Mapping 28 emotions conveyed by naturalistic expression",
      "authors": [
        "Alan Cowen",
        "Dacher Keltner"
      ],
      "year": "2019",
      "venue": "American Psychologist"
    },
    {
      "citation_id": "3",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "Alan Cowen",
        "Dacher Keltner"
      ],
      "year": "2017",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "4",
      "title": "The primacy of categories in the recognition of 12 emotions in speech prosody across two cultures",
      "authors": [
        "Alan Cowen",
        "Petri Laukka",
        "Hillary Anger Elfenbein",
        "Runjing Liu",
        "Dacher Keltner"
      ],
      "year": "2019",
      "venue": "Nature Human Behaviour"
    },
    {
      "citation_id": "5",
      "title": "The expression and recognition of emotions in the voice across five nations: A lens model analysis based on acoustic features",
      "authors": [
        "Petri Laukka",
        "Hillary Anger Elfenbein",
        "Nutankumar Thingujam",
        "Thomas Rockstuhl",
        "Frederick Iraki",
        "Wanda Chui",
        "Jean Althoff"
      ],
      "year": "2016",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "6",
      "title": "The voice conveys specific emotions: Evidence from vocal burst displays",
      "authors": [
        "R Emiliana",
        "Dacher Simon-Thomas",
        "Disa Keltner",
        "Lara Sauter",
        "Anna Sinicropi-Yao",
        "Abramson"
      ],
      "year": "2009",
      "venue": "Emotion"
    },
    {
      "citation_id": "7",
      "title": "Mapping 24 emotions conveyed by brief human vocalization",
      "authors": [
        "Alan Cowen",
        "Hillary Anger Elfenbein",
        "Petri Laukka",
        "Petri Keltner"
      ],
      "year": "2019",
      "venue": "American Psychologist"
    },
    {
      "citation_id": "8",
      "title": "Emotion State Manifestation in Voice Features: Chimpanzees, Human Infants, Children, Adults",
      "authors": [
        "Elena Lyakso",
        "Olga Frolova"
      ],
      "year": "2015",
      "venue": "Speech and Computer"
    },
    {
      "citation_id": "9",
      "title": "Young Infants Match Facial and Vocal Emotional Expressions of Other Infants",
      "authors": [
        "Mariana Vaillant-Molina",
        "Lorraine Bahrick",
        "Ross Flom"
      ],
      "year": "2013",
      "venue": "Infancy : the official journal of the International Society on Infant Studies"
    },
    {
      "citation_id": "10",
      "title": "Are 6-month-old human infants able to transfer emotional information (happy or angry) from voices to faces? An eye-tracking study",
      "authors": [
        "Amaya Palama",
        "Jennifer Malsert",
        "Edouard Gentaz"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "11",
      "title": "Talking with Feeling: Integrating Affective and Linguistic Expression in Early Language Development",
      "authors": [
        "Lois Bloom",
        "Richard Beckwith"
      ],
      "year": "1989",
      "venue": "Cognition and Emotion",
      "doi": "10.1080/02699938908412711"
    },
    {
      "citation_id": "12",
      "title": "One-to four-year-olds connect diverse positive emotional vocalizations to their probable causes",
      "authors": [
        "Yang Wu",
        "Paul Muentener",
        "Laura Schulz"
      ],
      "year": "2017",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "13",
      "title": "Auditory affective agnosia. Disturbed comprehension of affective speech",
      "authors": [
        "K Heilman",
        "R Scholes",
        "R Watson"
      ],
      "year": "1975",
      "venue": "Neurosurgery & Psychiatry"
    },
    {
      "citation_id": "14",
      "title": "Dysprosody or altered \"melody of language",
      "authors": [
        "G Monrad-Krohn"
      ],
      "year": "1947",
      "venue": "Brain: A Journal of Neurology"
    },
    {
      "citation_id": "15",
      "title": "Progression of dysprosody in Parkinson's disease over time-A longitudinal study",
      "authors": [
        "Sabine Skodda",
        "Heiko Rinsche",
        "Uwe Schlegel"
      ],
      "year": "2009",
      "venue": "Movement Disorders",
      "doi": "10.1002/mds.22430"
    },
    {
      "citation_id": "16",
      "title": "Employing a Voice-Based Emotion-Recognition Function in a Social Chatbot to Foster Social and Emotional Learning Among Preschoolers",
      "authors": [
        "Tsai-Hsuan",
        "Hsien-Tsung Tsai",
        "Shin-Da Chang",
        "Hui-Fang Liao",
        "Ko-Chun Chiu",
        "Chun-Yi Hung",
        "Chih-Wei Kuo",
        "Yang"
      ],
      "year": "2019",
      "venue": "HCI International 2019 -Late Breaking Papers"
    },
    {
      "citation_id": "17",
      "title": "Diagnosis of Depressive Disorder Model on Facial Expression Based on Fast R-CNN",
      "authors": [
        "Young-Shin Lee",
        "Won-Hyung Park"
      ],
      "year": "2022",
      "venue": "Diagnosis of Depressive Disorder Model on Facial Expression Based on Fast R-CNN"
    },
    {
      "citation_id": "18",
      "title": "Emotion and sociable humanoid robots",
      "authors": [
        "Cynthia Breazeal"
      ],
      "year": "2003",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "19",
      "title": "Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of Multimodal Features in Spoken Human-Robot Interaction",
      "authors": [
        "Jekaterina Novikova",
        "Christian Dondrup",
        "Ioannis Papaioannou",
        "Oliver Lemon"
      ],
      "year": "2017",
      "venue": "Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of Multimodal Features in Spoken Human-Robot Interaction",
      "arxiv": "arXiv:1706.02757v1"
    },
    {
      "citation_id": "20",
      "title": "Speech Communications: Human and Machine",
      "authors": [
        "D Shaughnessy"
      ],
      "year": "2000",
      "venue": "Speech Communications: Human and Machine"
    },
    {
      "citation_id": "21",
      "title": "Affective Computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition from speech: a review",
      "authors": [
        "G Shashidhar",
        "K Koolagudi",
        "Rao"
      ],
      "year": "2012",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "23",
      "title": "Automatic statistical analysis of the signal and prosodic signs of emotion in speech",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie"
      ],
      "year": "1996",
      "venue": "Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96"
    },
    {
      "citation_id": "24",
      "title": "A Survey on Speech Emotion Recognition by Using Neural Networks",
      "authors": [
        "Akanksha Gadikar",
        "Omkar Gokhale",
        "Subodh Wagh",
        "Anjali Wankhede",
        "P Joshi"
      ],
      "year": "2020",
      "venue": "International Journal of Research and Analytical Reviews"
    },
    {
      "citation_id": "25",
      "title": "Long Short-Term Memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "26",
      "title": "The ICML 2022 Expressive Vocalizations Workshop and Competition: Recognizing, Generating, and Personalizing Vocal Bursts",
      "authors": [
        "Alice Baird",
        "Panagiotis Tzirakis",
        "Gauthier Gidel",
        "Marco Jiralerspong",
        "B Eilif",
        "Kory Muller",
        "Björn Mathewson",
        "Erik Schuller",
        "Dacher Cambria",
        "Alan Keltner",
        "Cowen"
      ],
      "year": "2022",
      "venue": "The ICML 2022 Expressive Vocalizations Workshop and Competition: Recognizing, Generating, and Personalizing Vocal Bursts",
      "arxiv": "arXiv:2205.01780"
    },
    {
      "citation_id": "27",
      "title": "The ACII 2022 Affective Vocal Bursts Workshop & Competition: Understanding a critically understudied modality of emotional expression",
      "authors": [
        "Alice Baird",
        "Panagiotis Tzirakis",
        "Jeffrey Brooks",
        "Christopher Gregory",
        "Björn Schuller",
        "Anton Batliner",
        "Dacher Keltner",
        "Alan Cowen"
      ],
      "year": "2022",
      "venue": "The ACII 2022 Affective Vocal Bursts Workshop & Competition: Understanding a critically understudied modality of emotional expression",
      "arxiv": "arXiv:2207.03572"
    },
    {
      "citation_id": "28",
      "title": "The sliding DFT",
      "authors": [
        "E Jacobsen",
        "R Lyons"
      ],
      "year": "2003",
      "venue": "Conference Name: IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "29",
      "title": "A Scale for the Measurement of the Psychological Magnitude Pitch",
      "authors": [
        "S Stevens",
        "J Volkmann",
        "E Newman"
      ],
      "year": "1937",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "30",
      "title": "The quefrency analysis of time series for echoes : cepstrum, pseudo-autocovariance, cross-cepstrum and saphe cracking",
      "authors": [
        "B Bogert"
      ],
      "year": "1963",
      "venue": "Proceedings of the Symposium on Time Series Analysis"
    },
    {
      "citation_id": "31",
      "title": "Very Deep Convolutional Neural Networks for Raw Waveforms",
      "authors": [
        "Wei Dai",
        "Chia Dai",
        "Shuhui Qu",
        "Juncheng Li",
        "Samarjit Das"
      ],
      "year": "2016",
      "venue": "Very Deep Convolutional Neural Networks for Raw Waveforms",
      "arxiv": "arXiv:1610.00087"
    },
    {
      "citation_id": "32",
      "title": "Convolutional Neural Networks for patient-specific ECG classification",
      "authors": [
        "S Kiranyaz",
        "T Ince",
        "R Hamila",
        "M Gabbouj"
      ],
      "year": "2015",
      "venue": "2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "33",
      "title": "A Dataset and Taxonomy for Urban Sound Research",
      "authors": [
        "Justin Salamon",
        "Christopher Jacoby",
        "Juan Bello"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia, MM '14"
    },
    {
      "citation_id": "34",
      "title": "Random Forests",
      "authors": [
        "Leo Breiman"
      ],
      "year": "2001",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "35",
      "title": "Exploring the Effectiveness of Self-supervised Learning and Classifier Chains in Emotion Recognition of Nonverbal Vocalizations",
      "authors": [
        "Shinnosuke Detai Xin",
        "Hiroshi Takamichi",
        "Saruwatari"
      ],
      "year": "2022",
      "venue": "Exploring the Effectiveness of Self-supervised Learning and Classifier Chains in Emotion Recognition of Nonverbal Vocalizations",
      "arxiv": "arXiv:2206.10695"
    },
    {
      "citation_id": "36",
      "title": "Synthesizing Personalized Non-speech Vocalization from Discrete Speech Representations",
      "authors": [
        "Chin-Cheng Hsu"
      ],
      "year": "2022",
      "venue": "Synthesizing Personalized Non-speech Vocalization from Discrete Speech Representations",
      "arxiv": "arXiv:2206.12662"
    },
    {
      "citation_id": "37",
      "title": "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers",
      "authors": [
        "Josh Belanich",
        "Krishna Somandepalli",
        "Brian Eoff",
        "Brendan Jou"
      ],
      "year": "2022",
      "venue": "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers",
      "arxiv": "arXiv:2206.12494"
    },
    {
      "citation_id": "38",
      "title": "Self-supervision and Learnable STRFs for Age, Emotion, and Country Prediction",
      "authors": [
        "Roshan Sharma",
        "Tyler Vuong",
        "Mark Lindsey",
        "Hira Dhamyal",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "year": "2022",
      "venue": "Self-supervision and Learnable STRFs for Age, Emotion, and Country Prediction",
      "arxiv": "arXiv:2206.12568"
    },
    {
      "citation_id": "39",
      "title": "Comparing supervised and self-supervised embedding for ExVo Multi-Task learning track",
      "authors": [
        "Tilak Purohit",
        "Imen Ben Mahmoud",
        "Bogdan Vlasenko",
        "Mathew Doss"
      ],
      "year": "2022",
      "venue": "Comparing supervised and self-supervised embedding for ExVo Multi-Task learning track",
      "arxiv": "arXiv:2206.11968"
    },
    {
      "citation_id": "40",
      "title": "Burst2Vec: An Adversarial Multi-Task Approach for Predicting Emotion, Age, and Origin from Vocal Bursts",
      "authors": [
        "Atijit Anuchitanukul",
        "Lucia Specia"
      ],
      "year": "2022",
      "venue": "Burst2Vec: An Adversarial Multi-Task Approach for Predicting Emotion, Age, and Origin from Vocal Bursts",
      "arxiv": "arXiv:2206.12469"
    },
    {
      "citation_id": "41",
      "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao",
        "Jian Wu",
        "Long Zhou",
        "Shuo Ren",
        "Yanmin Qian",
        "Jian Yao Qian",
        "Michael Wu",
        "Xiangzhan Zeng",
        "Furu Yu",
        "Wei"
      ],
      "year": "2022",
      "venue": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
      "arxiv": "arXiv:2110.13900"
    },
    {
      "citation_id": "42",
      "title": "Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed",
        "Hubert"
      ],
      "year": "2021",
      "venue": "Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "arxiv": "arXiv:2106.07447"
    },
    {
      "citation_id": "43",
      "title": "Attention is All you Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "31st NIPS Conference Proceedings"
    },
    {
      "citation_id": "44",
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "Alexei Baevski",
        "Henry Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "arxiv": "arXiv:2006.11477"
    },
    {
      "citation_id": "45",
      "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Rémi Louf",
        "Morgan Funtowicz",
        "Jamie Brew"
      ],
      "year": "2019",
      "venue": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
      "arxiv": "arXiv:1910.03771"
    },
    {
      "citation_id": "46",
      "title": "Scikit-learn: Machine Learning in Python",
      "authors": [
        "Fabian Pedregosa",
        "Gaël Varoquaux",
        "Alexandre Gramfort",
        "Vincent Michel",
        "Bertrand Thirion",
        "Olivier Grisel",
        "Mathieu Blondel",
        "Peter Prettenhofer",
        "Ron Weiss",
        "Vincent Dubourg",
        "Jake Vanderplas",
        "Alexandre Passos",
        "David Cournapeau",
        "Matthieu Brucher",
        "Matthieu Perrot",
        "Édouard Duchesnay"
      ],
      "year": "2011",
      "venue": "J. Mach. Learn. Res"
    }
  ]
}