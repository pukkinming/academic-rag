{
  "paper_id": "2204.08625v1",
  "title": "Self Supervised Adversarial Domain Adaptation For Cross-Corpus And Cross-Language Speech Emotion Recognition",
  "published": "2022-04-19T02:57:56Z",
  "authors": [
    "Siddique Latif",
    "Rajib Rana",
    "Sara Khalifa",
    "Raja Jurdak",
    "Björn Schuller"
  ],
  "keywords": [
    "Speech emotion recognition",
    "self-supervised learning",
    "domain adaptation",
    "adversarial learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Despite the recent advancement in speech emotion recognition (SER) within a single corpus setting, the performance of these SER systems degrades significantly for cross-corpus and cross-language scenarios. The key reason is the lack of generalisation in SER systems towards unseen conditions, which causes them to perform poorly in cross-corpus and cross-language settings. Recent studies focus on utilising adversarial methods to learn domain generalised representation for improving cross-corpus and cross-language SER to address this issue. However, many of these methods only focus on cross-corpus SER without addressing the cross-language SER performance degradation due to a larger domain gap between source and target language data. This contribution proposes an adversarial dual discriminator (ADDi) network that uses the three-players adversarial game to learn generalised representations without requiring any target data labels. We also introduce a self-supervised ADDi (sADDi) network that utilises self-supervised pre-training with unlabelled data. We propose synthetic data generation as a pretext task in sADDi, enabling the network to produce emotionally discriminative and domain invariant representations and providing complementary synthetic data to augment the system. The proposed model is rigorously evaluated using five publicly available datasets in three languages and compared with multiple studies on cross-corpus and cross-language SER. Experimental results demonstrate that the proposed model achieves improved performance compared to the state-of-the-art methods.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "S PEECH Emotion Recognition (SER) is widely explored by researchers to enable effective human-computer interaction. Speech is a major affect display, and it contains information about emotional expressions that can be automatically identified using machine learning (ML) models. SER systems can help businesses by improving their service delivery. Speech emotion identification can be used in call centres to track customer and agent reactions. Speech-based affect recognition can be effectively utilised in healthcare for diagnosis and monitoring of depression, distress, and bipolar disorder in patients  [1] ,  [2] . Many other sectors including smart cars  [3] , forensic sciences  [4] , education  [5] , to name a few, are also aiming to utilise SER techniques to improve their performances.\n\nOver the past few years, deep learning (DL) based architectures including deep belief networks (DBN)  [6] , convolutional neural networks (CNN)  [7] , and long short term memory (LSTM) networks  [8]  have significantly improved SER performance compared to the classical machine learning (ML) approaches  [9] -  [12] . SER systems based on deep neural networks (DNNs) perform satisfactorily when training and test data belong to the same corpus  [13] . The performance of these systems plummets significantly when the training speech corpus is very different from the testing corpusknown as cross-corpus SER.\n\nOne of the key reasons for poor performance in crosscorpus SER is the difference between training and testing speech data distributions. These differences become more prevalent when training and testing data belong to different languages (cross-language SER). As a solution to the problem, researchers use diverse corpora (including multilingual data) for training to create more generalised and robust SER systems  [14] . Studies show that an SER model trained on multiple corpora can achieve improved results  [9] . However, acoustic training using multiple labelled data is not feasible for all languages, as we have speech corpora in very few languages compared to the number of languages spoken around the world  [15] ,  [16]  and getting samples for adaptation in rarely spoken languages is challenging.\n\nAn alternative and more practical approach to address the above challenge is domain adaptation, which generalises SER systems to unseen conditions by minimising domain shiftthe gap between source and target data distributions. Domain adaptation approaches maximise the domain confusion to learn a common feature space by minimising some measures of domain shift such as (a) maximum mean discrepancy  [17] ,  [18]  or (b) correlation distances  [19] ,  [20] . Reconstruction of the target domain using source representation is another way to create a shared representation  [21] ,  [22] . These approaches are effectively used in the computer vision domain. However, achieving domain adaptation in speech emotion is more complex, as it requires keeping the emotional information while reducing domain shift in source and target data.\n\nAdversarial domain adaptation methods have become a popular manifestation in SER research to minimise an approximate domain discrepancy distance through an adversarial loss. These methods are closely related to the generative adversarial network (GAN)  [23]  training, which pits a generator and a discriminator against each other. The generator is trained to generate fake data in a way that confuses the discriminator. In adversarial domain adaptation, this principle is used among the feature encoder, and domain discriminator  [24] . In SER, different studies (e. g.,  [25] ,  [26] ) use domain discriminator-based adaptation approaches. However, it is difficult to capture all the useful information and complex structures (such as the emotions) in the feature and label spaces using a single domain discriminator  [27] .\n\nThis paper proposes an Adversarial Dual Discriminator (ADDi) network to learn a domain generalised emotional representation that improves cross-corpus and cross-language SER. Our proposed model is equipped with a dual discriminator, which is not explored in SER. This enables the proposed model to generate the domain invariant representations with a three-players adversarial game among generator and dual discriminator.\n\nTo address the challenge of limited labelled data, we further propose self-supervised learning for ADDi -we call it sADDi. We propose synthetic data generation as our pretext task -we utilise the unlabelled data to pre-train the encoder component to learn to produce features for emotional synthetic data generation, which can be used to augment the system and help minimise the required labelled training data. Most of the existing SER studies (e. g.,  [25] ,  [26] ,  [28] ) on adversarial domain adaptation do not consider crosslanguage, creating a research gap. This is likely due to the complexity of learning a generalised representation for crosslanguage SER. In this paper, we consider improving crosslanguage SER. We summarise the contributions of this paper below. We, 1) propose a novel adversarial domain adaptation technique: ADDi for cross-corpus and cross-language SER. ADDi, for the first time, introduces a dual discriminator for SER, enabling the generation of domain invariant representations with a three-players adversarial game among generator and dual discriminator. 2) enable self-supervised learning with ADDi (we call it sADDi) by generating synthetic data as a pretext task that effectively utilises unlabelled data to improve the performance and produce synthetic data to augment the SER system. 3) use five widely applied publicly available datasets to comprehensively evaluate the performance of ADDi and sADDi for cross-corpus and cross-language SER performance. Results show that ADDi outperforms the state-of-the-art methods and when including selfsupervised learning in ADDi, sADDi offers even higher performance improvement than the state-ofthe-art methods. Besides improving the performance, sADDi reduces the required amount of source labelled data by 15-20 % compared to the current and most relevant study (ADDoG)  [28]  while achieving comparable classification accuracy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cross-Corpus And Cross-Language Ser",
      "text": "Cross-corpus speech emotion recognition is an important task to enable real-life SER applications. It aims to build systems with improved generalisation to perform SER not only in variations in speaker and languages but also in unknown target conditions, including changes in recording environments, noise levels, and elicitation strategy. State-ofthe-art SER systems trained on a single corpus fail to perform well in cross-corpus settings. Previous studies explore various techniques to achieve better performance in crosscorpus SER. Schuller et al.  [14]  find that the SER performance degrades due to the acoustic and annotation differences. They perform experiments using six corpora to gain generalisation. They also evaluate multiple normalisation techniques and z-normalisation to achieve the best results. Eyben et al.  [37]  perform cross-corpus SER evaluations using speech databases with realistic and non-prompted emotions. They use a uni-variate ranking of the low-level descriptors (LLDs) to find the most important features and achieve improvement in some settings. They highlight that future efforts are required to address the inconsistencies among multiple corpora by carefully selecting annotations. Zhang et al.  [29]  evaluate unsupervised learning and feature normalisation for cross-corpus SER. They show that adding unlabelled data to agglomerate multi-corpus training sets and utterance level feature normalisation can improve performance. In  [38] , the authors show the effect of data agglomeration and decisionlevel fusion for cross-corpus SER. They use six datasets and demonstrate that joint training with multiple corpora and late fusion could help improve performance. These studies show the preliminary feasibility of cross-corpus learning and motivate further in-depth research.\n\nResearchers also explore different techniques to perform emotion identification in cross-language settings. Albornoz et al.  [39]  consider emotion profile-based ensemble support vector machines (SVM) for emotion classification in multiple languages. They model each language independently to preserve the cultural properties and apply the universality of emotions to map and predict emotions in different languages. They use the RML corpus  [40]  and achieve improved results using their model in a language-independent SER. Li et al.  [41]  develop a three-layered model of acoustic features, semantic primitives, and emotion dimensions to perform crosslanguage emotion classification. They apply feature selection and speaker normalisation and evaluate the proposed framework on Japanese, German, Chinese, and English emotional speech corpora. They achieve multilingual recognition performance comparable with a monolingual emotion recogniser. In  [16] , the authors evaluate cross-lingual SER and highlight the ways of designing an adaptive emotion recognition system for languages with a small available dataset. They show that training the model with multiple languages data can deliver comparable results with a model trained with monolingual data and that augmentation of the training set with a fraction of target language labelled data can help improve the performance. Various other studies (e. g.,  [37] ,  [38] ,  [42] ) explore cross-lingual SER, however, these studies evaluate classical ML models on relatively smaller datasets.\n\nMost recent studies on SER utilise deep representation learning techniques over low-level features. Particularly, studies use deep networks to learn generalised represen- tations to improve performance. For instance, the authors in  [9]  use DBNs for learning generalised features across multiple datasets. They evaluate the proposed model using six emotional corpora and showed that DBN can provide better performance in cross-corpus SER. They also observe that a DBN can learn a robust representation from many language datasets that helps improve SER performance. In  [31] , the authors train an attentive convolutional neural network (ACNN) for binary classification of arousal and valence in cross-language and multi-language training settings using French and English language datasets. They show that multilingual training can enhance the performance of the system. Also, they find that the ACNN can be fine-tuned using a fraction of target language data to produce sound results for cross-language SER. Ning et al.  [43]  employ multilingual Bidirectional Long Short-Term Memory (BLSTM) with the shared hidden layers across different languages for universal feature representation learning. They evaluate the proposed model for English and Mandarin corpora and found that cross-lingual knowledge learning using shared hidden layers helps improve SER performance compared to BLSTM variants without shared hidden layers. Some other studies (e. g.,  [44] -  [46] ) also exploit deep networks to improve cross-corpus and cross-language emotion detection. In general, the methods proposed in these studies require large aggregated speech labelled corpora to achieve generalisation for improved crosscorpus performance. Models training using aggregated corpora is not feasible in real life, as it requires multiple labelled datasets. In contrast, domain adaptation is a more practical approach that improves the system's generalisation without the need for multiple labelled corpora. We review the studies on domain adaptation in the next subsection.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Adversarial Domain Adaptation",
      "text": "Deep domain adaptation aims to improve the generalisation of SER systems by addressing the problem of domain shift among source and target datasets. Researchers explore different domain adaptation models (e. g.,  [47] -  [50] ) to improve cross-corpus and cross-lingual SER. To this end, adversarial domain adaptation techniques are becoming very popular in SER. For instance, the authors in  [32]  use domain adversarial neural networks (DANN)  [24]  for cross-corpus emotional attributes' prediction. They learn generalised representations between the source and target data by using a gradient reversal layer (GRL) which propagates back the negative of the gradient produced by the domain classifier to the shared network. They find that the DANN can learn domain invariant representations to cross-corpus SER. Xiao et al.  [25]  propose an adversarial network for class-aligned and generalised domain adaptation. They also exploit GRL to generalise representations among source and target data. They evaluate the proposed model against cross-corpus settings using IEMOCAP and MSP-IMPROV corpora and achieved improved results compared to DANN and AE-based deep architectures. Zhou et al.  [51]  present a class-wise domain adversarial adaptation method to learn common representation to address cross-corpus mismatch issues. They evaluated the proposed model on two datasets including AIBO and EMO-DB for the French language and show that the proposed model achieves better results when training is performed on target data with minimal labels for positive and negative emotion classes recognition. Gideon et al.  [28]  introduce an adversarial discriminative domain generalisation model that follows a \"meet in the middle\" approach for cross-corpus emotion recognition. The proposed approach utilises the critic network that enables the model to improve the crosscorpus generalisation by iteratively moving representations closer to source and target data. They perform evaluations using English datasets including IEMOCAP, MSP-IMPROV, and PRIORI emotion datasets  [28]  and show that the proposed framework generates generalised representations for improved cross-corpus SER.\n\nMost of the studies above evaluate adversarial domain adaptation methods for cross-corpus SER using similar language corpora. However, few studies show the effectiveness of their methods for different languages in cross-corpus SER. Ahn et al.  [36]  propose a few shots learning-based unsupervised domain adaptation techniques to learn emotional similarity among source and target domains. They evaluate the proposed model in three different languages and achieve improved results. However, their proposed method requires additional labelled training data to improve the generalisation. Latif et al.  [33]  present a GAN-based adversarial method to learn language invariant representations and evaluate the model for different language datasets. They train support vector machines (SVM) on language invariant representations to improve the performance of cross-language SER. In contrast to these studies, we propose an Adversarial Dual Discriminator (ADDi) network that utilises a dual discriminator to learn generalised representations to improve cross-corpus SER. One of the novel features of our model is the utilisation of self-supervised learning (SSL) for domain adaptation, which has not been explored for SER domain adaptation. Few studies exploited SSL for improving SER performance withincorpus settings. We discuss these studies in the following subsection.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Self-Supervised Ser",
      "text": "Self-supervised learning (SSL)  [52]  is a new paradigm in ML, which uses data for supervision. The self-supervised task, also known as the pretext task, uses the unlabelled data to guide downstream tasks. SSL-based models are getting tremendous interest in computer vision  [53] , natural language processing (NLP)  [54] , and automatic speech recognition (SER)  [55] ; however, few studies utilise SSL in SER. In  [56] , the authors propose a multitask SSL technique to learn a shared speech representation, where a single encoder network is followed by multiple workers that jointly solve different self-supervised tasks. They perform evaluations on speaker, phoneme, and emotional cue recognition, and achieve improved results. Self-supervised multi-modal representation learning though transformers  [57]  is increasingly gaining momentum to improve SER  [58] . Khare et al.  [59]  use transformer-based SSL to improve the performance of multimodal emotion recognition. They fine-tune a transformer trained on a masked language modelling task and can improve emotion recognition performance by 3 % on the CMU-MOSE dataset  [60] . A recent study  [61]  presents a visuallyguided SSL framework for improving the SER performance. The authors generate video frames using still images by conditioning the network on corresponding audio. In this way, the pre-trained encoder part of their network learns important features to generate realistic facial and lip movements. They hypothesise that the features learnt by the encoder are highly correlated with the presence of emotion and particular phonemes. They utilise these representations for ASR and SER to achieve state-of-the-art results. In contrast, we propose to generate synthetic emotional data as a pretext task, which adversarially enables the encoder to encode discriminative features for emotional data generations. We use this encoder for our downstream domain adaptation task, which helps produce emotionally discriminative features while minimising the gap between source and target domains. In addition, the synthetic emotional data generated in our downstream task acts as a by-product that can be utilised to augment the system.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Research Gap (Summary)",
      "text": "The related work can be summarised as follows.\n\n• Several studies show that DL models trained using multiple sources corpora can improve cross-corpus SER performance; however, acoustic training from multiple language data in real-life is not a feasible ap-proach due to the unavailability of sufficient labelled data for multiple languages. Therefore, there is a need for new methods to overcome this limitation.\n\n• Adversarial neural networks based domain adaptation approaches are widely used for cross-corpus SER; however, there is still room for performance improvement, particularly for cross-language SER.\n\n• Self-supervised learning can be used as an effective tool to address the limited label issue but has not been fully explored and used for cross-corpus SER.\n\nIn Table  1 , we contrast our work with the literature, briefly showcasing how we aim to address the research gaps.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Proposed Model",
      "text": "The core of the proposed model is the Adversarial Dual Discriminator (ADDi) network and the module for the pretext task enabling self-supervision. We propose the generation of synthetic data as a pretext task, wherein we essentially pretrain an encoder that we later use to realise the proposed Self-supervised Adversarial Dual Discriminator (sADDi) network.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Adversarial Dual Discriminator (Addi) Network",
      "text": "Our proposed Adversarial Dual Discriminator (ADDi) network is equipped with an encoder (E), a generator (G p ), and dual discriminators: (D s ) and (D t ). An overview of the proposed framework is shown in Figure  1 , where subfigure with label 2 shows ADDi network. It performs adversarial domain adaptation by learning domain invariant features. We represent the source domain data and target domain data as X s = {x s i , y s i } n i=1 and X t = {x t j } m j=1 , respectively. The encoder (E) attempts to map the input data either from source or target to a domain invariant latent representation (z d = E(X), X ∈ X s ∪ X t ). The generator (G d ) conditioned on domain code d uses this domain invariant latent representation (z d ) to generate source Xs or target Xt domain samples. The generator G d is adversarially connected with two discriminators. The objective function of the generator is as follows:\n\nwhere λ is a balancing parameter. The generator is connected to the dual discriminators D s and D t , which play the threeplayers minimax adversarial game to produce z d to be domain invariant. The generator also acts as the decoder and it reconstructs back the input samples X with the latent representation z d using the reconstruction loss in Equation (3). The dual discriminators are tasked to distinguish the real data from the fake data. Particularly, for domain code d = 0, the discriminator D s differentiates between Xt = G p (E(X), d) (fake) and source data X s (real), whereas the discriminator D t discriminates between Xs = G p (E(X), d) (fake) and source data X s (real), for domain code d = 1. The adversarial process of the generator G p and two discriminators minimises the divergence between the source and target data distributions and forces the encoder E to generate a generalised latent representation z d across the where we use pre-trained encoder from the pretext task as highlighted with a dashed line.\n\nsource and target domains. The objective function for the dual discriminators can be given as follows:\n\nThe classifier C d is connected with the latent representation z d and minimises the cross entropy loss for emotion classification during training using only the source data labels and the error is back-propagated through the network to update E. In this way, the encoder E gets influenced by the classifier and enforces z d to be emotionally discriminating as representation. This helps produce emotionally discriminative and domain invariant representations to perform crosscorpus and cross-language robust SER. When the pre-trained encoder E is fine-tuned in the domain adaptation task, it promotes the discriminative power of a domain invariant representation and further boosts the performance of the system. We discuses the encoder pre-training in the next section.\n\nDuring training, first, the autoencoder is updated using the equation  (3) . Afterwards, G d is updated to generate the fake samples using z d and the domain code d. We concatenate the one-hot domain code with the encoded representation z d and feed to G d . We further update the discriminators based on the domain codes. For the samples with d = 0, D s is updated, whereas D t is updated for the samples with d = 1. Finally, we update the C d for the source data samples.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Pretext: Synthetic Emotional Data Generation And Self Supervised Adversarial Dual Discriminator (Saddi)",
      "text": "A pretext task is used in self-supervised learning (SSL) to generate useful representations that can provide a supervisory signal to the down stream task. It is a predefined task for the network to solve learning the objective function  [53] .\n\nMost of the SSL pretext tasks are designed based on intuition or heuristics  [62] . There is no guarantee on the compatibility between the pretext task and the down stream task  [63] . For SER, solving multiple audio based self-supervised tasks can offer improvements  [56] . However, these tasks have been evaluated for within corpus SER settings. The design of an SSL pretext task for domain adaption is challenging, as emotionally discriminative generalised representations are required to effectively perform cross-corpus SER.\n\nWe use synthetic emotional data generation as pretext task for cross-corpus domain adaptation. The intuition here is that the encoder network pre-trained to encode discriminative features for emotional synthetic data generation when utilised in domain adaptation should help produce an emotionally discriminative generalised representation.\n\nThe architecture for our pretext task is shown in Figure  1  as a subfigure with label 1. It follows the GAN architecture consisting of a generator and a discriminator. Both these networks play an adversarial game defined by the following optimisation program in Equation  4 .\n\n(4) The generator network captures the data distribution and generates new samples by incorporating feedback from the discriminator network. The discriminator network in a GAN is simply a classifier. It tries to classify the real and fake data, generated by the generator network. While there are many variants of GAN architectures (e. g.,  [64] -  [66] ), we use the balancing GAN  [67]  like architecture due to its effectiveness in SER  [68] . It consists of an encoder (E), generator/decoder (G p ), and discriminator (D p ). The encoder network (E) takes the non-emotional speech data (X U ) and generates latent code z p . We concatenate the z p with the pseudo labels (y p ) and feed to the generator (G p ) to generate the synthetic data. Since the unlabelled samples do not belong to any emotional class, the pseudo labels in four classes are randomly generated and uniformly distributed to the unlabelled non-emotional speech. In this way, the G p network conditioned on the (y p ) has explicit emotion class label information during generation like the conditional GAN  [69] . During the adversarial training, G p is tasked to generate samples in different classes based on y p ; and D p is trained to differentiate the generated samples (by generator (G p )) as fake and real samples to their class labels. The generator tries to avoid the fake label and matches the desired emotional class labels. The discriminator is optimised to output N c + 1 neurons, where N c represents the emotional classes (happy, sad, neutral, or angry) and the last neuron represents the fake class as used in  [67] ,  [70] . Since the encoder (E) is coupled with the GAN, it learns to encode features for different emotional classes in the latent space of the generator G p . After pre-training, we fine-tune the encoder network in our ADDi network which we name self-supervised Adversarial Adversarial Dual Discriminator (sADDi). It is highlighted in the Figure  1  with blue dashed line.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup 4.1 Datasets",
      "text": "To evaluate the performance of our proposed model, we use five different emotional datasets, including IEMOCAP, MSP-IMPROV, RECOLA, EMODB, and FAU-AIBO, which are commonly used for cross-corpus and cross-language emotion classification research  [15] . In order to use additional unlabelled data for self-supervised learning (SSL), we use a subset of Librispeech  [71] , which is a corpus of read English speech, suitable for training and evaluating models on automatic speech and speaker recognition systems. Below, we briefly describe these datasets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iemocap",
      "text": "This database contains 12 hours of audiovisual data, including audio, video, textual transcriptions, and facial motion information  [72] . The recordings are collected from 10 professional actors (five males and five females) during dyadic interactions. In contrast to reading text with prototypical emotions, dyadic interactions allowed the actors to perform more spontaneous emotion  [73] . For categorical labels, each sentence is annotated by three annotators and the participant. Finally, an utterance is assigned a label if at least three annotators are assigned the same label. Overall, IEMOCAP contains nine emotions: excited, happy, sad, neutral, angry, disgust, frustrated, fearful, and surprised. Similarly to previous studies  [74] , we only use utterances of four categorical emotions, including happy, neutral, sad, and angry in this study by merging \"happy\" and \"excited\" as one emotion class \"happy\". The final dataset includes 5 531 utterances (1 636 happy, 1 708 neutral, 1 084 sad, and 1 103 angry instances). For continuous labels, IEMOCAP is also annotated for arousal and valence on a scale of 1 to 5. We map continuous labels to binary labels as presented in Table  2 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Msp-Improv",
      "text": "The MSP-IMPROV dataset is an acted audiovisual emotional database recorded from 12 speakers performing dyadic interactions  [75] . Overall, the recordings are grouped into six sessions and each session contains the recordings of one male, and one female actor similar to IEMOCAP  [72] . The scenarios were carefully designed to control emotion and lexical content while maintaining naturalness in the recordings. The MSP-IMPROV is annotated through perceptual evaluations using crowdsourcing  [76] . This corpus contains utterances in four categorical emotions: angry, happy, neutral, and sad. To be consistent with previous studies  [10] ,  [77] , we use all utterances with four emotions: anger (792), happy (2 644), sad (885), and neutral (3 477).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Recola",
      "text": "RECOLA  [78]  is a French multimodal corpus of spontaneous collaborative and affective interactions. While solving a collaborative task, speakers recorded the dyadic conversations during a video conference. 46 participants (27 females, and 19 males) were recruited to record this corpus. We use the publicly available portion of RECOLA, which contains 1, 308 utterances of 23 speakers. An open-source web-based tool AN-NEMO 1 was developed for its affective annotation. RECOLA is annotated with continuous labels, including arousal and valence in the range [-1, 1]. We use RECOLA for cross-corpus language SER and perform binary classification of arousal (low/high) and valence (negative/positive) as considered in  [31] . Table  2  shows the mapping of original annotations to a binary scheme for IEMOCAP and RECOLA.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Emodb",
      "text": "EMODB  [79]  is the most popular and widely used publicly available emotional dataset in German Language, recorded by the Institute of Communication Science, Technical University Berlin. It contains audio recordings of seven emotions recorded by ten professional speakers in 10 German sentences. This study selects four basic emotions: happy, sad, neutral, and angry to perform categorical cross-language emotion recognition.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Fau-Aibo",
      "text": "FAU-AIBO  [80]  corpus is a spontaneous emotional corpus in the German language. It contains 9.2 hours of speech from 51 children from different schools while interacting with Sony's pet robot AIBO. In this study, we select FAU-AIBO to evaluate the proposed framework against completely naturalist emotional speech. We map this corpus to binary valence for evaluations.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Librispeech",
      "text": "The LibriSpeech dataset  [71]  contains 1 000 hours of English read speech from 2 484 speakers. This corpus is derived from audiobooks and is commonly used for automatic speech and speaker recognition problems  [81] ,  [82] . The training portion of LibriSpeech is divided into three subsets, with an approximate recording time of 100, 360 and 500 hours. This paper uses the subset that contains 100 hours of recordings. These recordings span over 251 speakers.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Speech Features Extraction",
      "text": "We represent the speech sample in Mel Filter Banks (MFBs), a widely used speech representation in speech research  [28] ,  [83] . We use the Kaldi speech recognition toolkit  [84]  to extract 40-dimensional MFBs from each utterance. To extract 1. https://diuf.unifr.ch/main/diva/recola/annemo MFBs, we use default options, including a Povey window with a frame length of 25 ms and a frameshift of 10 ms, a preemphasis coefficient of 0.97, and a low cutoff of 20 Hz. These configurations are selected based on  [28]  to make a fair comparison. Due to the varying lengths of the audio samples, we pad the MFBs with zeros to the length of the longest emotional utterance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model Configuration",
      "text": "This subsection presents the configuration of three models, including a baseline Convolutional Neural Network (CNN), the proposed Adversarial Dual Discriminator (ADDi) network, and the pretext task GAN. Each of these models takes MFBs as the input feature set. Each experiment considers labelled source data for training, and target data is used for testing. We train all these models using Adam as the optimiser with default parameters and a starting learning rate of 0.0001. We compute the validation accuracy at the end of each epoch during training. If the validation accuracy did not improve after 5 epochs, we restore the model to the best epoch and halves the learning rate. This process continues until the learning rate reaches below 0.00001.\n\nThe CNN baseline network comprises a feature encoder and emotion classifier. The feature encoder consists of convolutional and max-pooling layers, whereas the classifier part utilises the fully connected layers for classification. Due to the unavailability of target data in the experiments, it is difficult to validate all the hyperparameters of the network for cross-dataset SER. Therefore, we select the parameters commonly used in prior studies  [28] ,  [85] ,  [86] . The feature encoder has three convolutional layers, each followed by the pooling layers. We start with a large filter size of 16 in the first convolutional layer as suggested by prior work  [86] . The convolutional layers capture the salient regions within the MFBs and create the feature maps. The pooling layers reduce the dimension of these feature maps by identifying the most relevant features. We use the max-pooling layer to give better performance than average pooling during experiments. The feature encoder encodes the entire utterances into the 256 features. The classifier uses these features for emotion classification. We have two dense layers with hidden units of 256 and 128. We employ a dropout layer between two dense layers with a dropout rate of 0.3 to avoid overfitting.\n\nThe ADDi network also has an encoder component to encode input MFBs to the domain invariant representation that is used by the generator. We apply a similar encoder architecture to the baseline CNN. The decoder/generator has three transposed convolutional layers to generate samples using the encoded latent representation. Two discriminators and the classifier of ADDi have two hidden layers containing hidden units of 256 and 128 in number. Like the baseline CNN and ADDi, our pre-training GAN also has an encoder network that follows a similar architecture. We employ the same architecture for the discriminator as for the ADDi network. We select the Rectified Linear Unit (ReLU) as a nonlinear activation function for all the models due to its better performance than hyperbolic tangent and leaky ReLU during validation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments And Evaluations",
      "text": "We apply a two-tiered evaluation approach: We evaluate ADDi to understand the significance of the proposed dualdiscriminator based framework. We then evaluate sADDi to understand the relative significance of self-supervised learning for ADDi. We evaluate the performance of the proposed ADDi and sADDi networks in cross-corpus and crosslanguage settings by comparing them with related studies that report similar results. To further extend the extent of our comparison, we implement related models including a CNN (baseline), a GAN  [33] , a DANN  [32] , a DBN  [9] , a CNN-LSTM  [45] , and an autoencoder-based model as used in  [87]  and compare our results with these models. We repeat each experiment ten time and calculated mean and standard deviation. Results are presented using the unweighted average recall rate (UAR), a widely accepted metric in the field.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Cross-Corpus Results",
      "text": "We evaluate the proposed ADDi and sADDi networks for cross-corpus SER using the IEMOCAP and MSP-IMPROV datasets. Both of these datasets are recorded in similar laboratory conditions in English. In this experiment, we consider no labels for the target dataset. We use a random 80:20 (train:test) split of the source data and train the model as used in  [28] . We compare the performance of the ADDi network with a baseline CNN, ADDOG  [28] , a DANN and a GAN  [33] . The results are presented in Table  3 . Compared to these existing methods and baseline, ADDi achieves better results. ADDi achieves 2.6% and 3.9% relative improvements compared to the baseline CNN for IEMOCAP to MSP-IMPROV and MSP-IMPROV to IEMOCAP experiments, respectively. Amongst the previous studies, ADDOG utilises the critic component similar to a Wasserstein GAN  [65]  to learn generalised representations for cross-corpus SER, while another study  [33]  applies a single discriminator based adversarial method to minimise the domain gap, and whereas in  [32] , a gradient reversal layer (GRL)  [24]  is used to minimise the gap between the source and target domains. In contrast to these studies, ADDi utilises a dual discriminator based network to learn a domain invariant representation by bringing source and target features closer to each other with three-players adversarial minimax games hence producing better results. Using the ablation study in subsection 5.8, we further quantify the relative significance of our dual discriminator based approach.\n\nTable  3  also shows the self-supervised learning (SSL) for ADDi which we called sADDi above. When we pre-train the encoder component in the sADDi network using the proposed synthetic data generation pretext task, it learns to encode discriminative representation for synthetic emotional data generation through the process of accomplishing the proposed pretext task. This helps to produce emotionally discriminative domain generalised features while fine-tuning the encoder in sADDi and the baseline CNN. Results by the SSL methods are separated with a bold line in Table  3 , which shows that the pre-training of the encoder considerably improves the cross-corpus SER. It is worth noting that the performance of the baseline CNN is also improved by utilising the pre-trained encoder by our proposed pretext task, which attests the effectiveness of the proposed selfsupervised pretext task.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Cross-Language Results",
      "text": "We evaluate ADDi and sADDi for cross-language SER using both dimensional and categorical emotions. We use the IEMOCAP and RECOLA datasets for dimensional emotion and perform binary arousal and valence classification. All data from one language is used as a training set and all samples of the respective target language are used as the test set. This is the same evaluation strategy used in  [31] . We also implement domain adaptive models, including the GAN and DANN for comparison on IEMOCAP and RECOLA. Cross-language SER results using IEMOCAP and RECOLA are presented in Table  4 . Using our proposed ADDi framework, we achieve better results compared to  [31] , where the authors use an Attentive Convolutional Neural Network (ACNN) to achieve promising results by fine-tuning the model on the target language. We also compare our results with domain adaptation architectures, including GAN and DANN in Table  4 . Compared to these models, ADDi is able to capture an emotion discriminative generalised representation by adversarially minimising the domain shift among source and target language data to improve SER across different language data. Performance is further improved when features learnt through SSL are utilised to guide the cross-language domain adaptation using sADDi. It is important to note that the performance of all the models is close to the chance level UAR (i.e., 50 %), which shows the complexity of cross-language SER. However, our model improves the baseline results compared to the previous studies. To further improve the baseline performance, we perform two experiments. In the first experiment, we utilise a fraction of target data in the training set. Results in Table  4  show that including only 250 target language data yields considerable improvements compared to ACNN  [31]  with 500 target samples. We incorporate language id with the source language data and 250 target language samples in the second experiment. Results are reported in Table  4 , which shows that the performance is improved for both arousal and valance prediction using the language information in the training data. However, the language information helps valence prediction more than the arousal prediction, which indicates that valence is more lexically dependent than arousal.\n\nWe also compare our results on categorical cross-language emotion classification with different studies  [9] ,  [32] ,  [33] ,  [45] ,  [87]  and present the results in Table  5 . In  [9] , the authors use transfer learning to improve cross-language SER using DBNs. A CNN-LSTM is suggested in  [45]  and an autoencoder is tested in  [87]  for cross-language SER. We also compare our results with GAN and DANN-based domain adaptive implementations for cross-language SER. ADDi achieves better results compared to all, which attests that ADDi learns greater domain generalised representation for cross-language scenarios. Compared to baseline, ADDi achieves 3.9 % and 2.8 % relative improvements for IEMOCAP to EMODB and EMODB to IEMOCAP experiments, respectively. Similar to the dimensional emotions, performance on categorical crosslanguage SER is further boosted by using a self-supervised ADDi (sADDi) network, which shows that the network is able to produce better generalised features for cross-language SER guided by the synthetic emotional data generation pretraining.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Impact Of Pretext Selection: Reconstruction Versus Synthetic Data Generation",
      "text": "We propose synthetic data generation as a pretext task for self-supervised learning. In this experiment, we evaluate the effectiveness of this pretext task by comparing it with reconstruction as a pretext task. We make the comparison for both, the baseline and the ADDi networks.\n\nReconstruction is widely used as a pretext task in the computer vision literature  [53] ,  [88] , wherein an autoencoder network is used to reconstruct the input back from the compressed representation. To use reconstruction as a pretext, we use unlabelled data (LibriSpeech) for unsupervised reconstruction and pre-train the encoder component to be utilised in the downstream task. We use the LibriSpeech data to generate the synthetic emotional data and pre-train the encoder component to use synthetic data generation for pretext.\n\nResults of the comparisons are presented in Figure  2  for cross-corpus and cross-language SER. For cross-corpus SER, we use IEMOCAP and MSO-IMPROV, and IEMOCAP and RECOLA are used for cross-language SER. Compared to the reconstruction-based pretext task, we achieve better results using synthetic emotional data generation for both the baseline CNN model and ADDi. However, despite the popularity of autoencoder-based reconstruction pretext tasks in computer vision, it could not produce strong representations for transfer tasks in SER. One possible reason might be that the autoencoder only learns to encode abstract bottleneck representations from non-emotional speech, which cannot provide supervisory signals in the downstream domain adaptation task. .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Impact Of Data Augmentation",
      "text": "We generate the synthetic data during our pre-training step and use it to augment the source training data. We evaluate the model in a cross-corpus setting using IEMOCAP as training and MSP-IMPROV as testing data to compare the results with  [85] ,  [89] ,  [90] . In  [89] , the authors investigate a GAN to generate the synthetic feature vectors using low dimensional features to augment the SER. Bao et al.  [90]  apply a CycleGAN based model for synthetic samples by transferring feature vectors extracted from a large unlabelled speech data into the target synthetic emotional samples. They augment the SER system with synthetic features to improve SER performance. Recently, Latif et al.  [85]  utilise the combination of a GAN and mixup  [91]  to generate synthetic samples for SER augmentation. Similar to these studies, we also augment the SER system with synthetic data and perform evaluations using real, synthetic, and real plus synthetic data. We also use MSP-IMPROV as the target data, as per these studies. We randomly select 30 % of the data as a development set for hyper-parameter selection and the remaining 70 % as testing data as used in these studies. We keep these splits speaker independent. Results are compared with these studies in Table  6 . As expected, the synthetic data alone cannot offer better results, but we get better performance when we augment source data with the synthetic data to train ADDi.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Impact Of Incorporation Of Source/Target Data",
      "text": "This experiment incorporates the labelled target data into the training and validation. Here, we present the results using IEMOCAP and MSP-IMPROV in Figure  3 . Similar results are achieved for cross-language datasets. We plot the results with different percentages of target data using the baseline approach and ADDi. ADDi improves the results considerably against baseline CNN in all the case, even for a small percentage target labelled data. Figure  3  shows that the margin of UAR improvement decreases with incorporating larger percentages of labelled target data. This may indicate that the generalisation effect diminishes once there is sufficient amount of labelled target domain data. We also explore the effect of decreasing the percentage of source data on the performance of cross-corpus SER using IEMOCAP and MSP-IMPROV. In both cases, sADDi performs better than the baseline. We also compare the results with ADDOG  [28]  in Figure  3c  and 3d . The red dot shows the performance achieved by ADDOG using 100 per cent of source data. We achieve these results using 80-86 % of source data as highlighted by a dotted blue line.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Evaluations In The Wild",
      "text": "In this section, we evaluate the performance of the proposed model on the naturalist speech. For this experiment, we use FAU-AIBO corpus that contains the natural speech of children in the German language. We perform binary valence classification as used by previous studies  [9] ,  [92] . This experiment is more difficult compared to the previous experiments, due to the difference in language, age, and elicitation strategy. We train the model on source data and 20% target data is used as validation and the remaining is used for testing. Results for both experiments are reported in Table  7 , which shows that the proposed model considerably improved performance  compared to the DBNs  [9]  and baseline CNNs. sADDi is improving the results by above 4% for both experiments presented in Table  7 . This confirms the effectiveness of our sAADi network that can produce generalised representations for evaluations against naturalist speech.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Size Of Pretext Task Training Data",
      "text": "We next examine the effect of the size of training data in pretext tasks on the performance of sADDi. We plot the results for IEMOCAP and MSP-IMPROV in Figure  4 . For both experiments, we find that the increase of training data in the pretext task helps improve the performance of the downstream emotion classification. This shows that increasing the training data in the pretext task enables the model to produce representations suitable for the downstream task of speech emotion recognition (SER).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ablation Experiments",
      "text": "In this experiment, we validate the necessity and effectiveness of each module integrated with our proposed model ADDi.\n\nResults are presented in Table  8  for cross-corpus and crosslanguage evaluations using IEMOCAP to MSP-IMPROV and IEMOCAP to EMODB. These results are computed without any data augmentation and pre-training. Results for synthetic data augmentation and pre-training are presented in Section 5.4 and 5.3. This experiment starts with the ADDi model (model 1) that contains all the components, including the encoder, two discriminators, the generator/decoder, and the classifier. We remove one discriminator in models 2 and 3. This makes the model similar to the standard GAN with an additional classifier and autoencoder. We keep removing different modules until we obtain the baseline CNN network (model 5), containing only the encoder and classifier. We also plot the configurations of these models 1-5 in Table  8 . There is a considerable drop in UAR when one or more components are removed. When a single discriminator -either D s or D t -is used in model 2 or 3, we see a performance drop for both cross-corpus and cross-language SER. This shows that the single discriminator networks cannot achieve better generalisation compared to the three-players adversarial learning performed by the dual discriminator and generator approach in the ADDi network. Similarly, ADDi also achieves considerably improved results compared to models 4 and 5 in cross-corpus and cross-language settings. In models 4 and 5, there is no component (i. e., discriminator) to promote generalised representations in the network by minimising the domain gap between source and target data. This shows that implanting a domain adaptation component in the pipeline of the deep model is important to learn to improve generalised features for cross-corpus and cross-language SER. Overall, these ablation experiments show that all the components in the proposed ADDi models are chosen carefully for effective domain adaptation for SER.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "This contribution addressed the open challenge of improving the speech emotion recognition (SER) performance in crosscorpus and cross-language settings. We proposed the Adversarial Dual Discriminator (ADDi) network that minimises the domain shift among emotional corpora adversarially. We focused on exploiting the unlabelled data with self-supervised pre-training and proposed self-supervised ADDi (sADDi).\n\nFor sADDi, we suggested synthetic data generation as a pretext task, which (1) helped improve the domain generalisation performance of an SER system to tackle the larger domain shift between training and test distributions in crosscorpus and cross-language SER; and (2) produced byproduct synthetic emotional data to augment the SER system and minimise the requirement of source labelled data. The key highlights are as follows:\n\n•\n\nThe introduced dual discriminator based ADDi network offers improved cross-corpus and cross- language SER without using any target data labels compared to the single discriminator and other stateof-the-art approaches. This is mainly due to the dual discriminator using a three-players adversarial game to learn generalised representations.\n\n• Considerable improvements in results were found when partial target labels were fed to the network training. This helped the ADDi to regulate the generalised representations based on the target data by maximally matching the data distributions.\n\n• Our proposed self-supervised pretext task produces synthetic data as a byproduct to augment the system to achieve better performance. We were able to reduce 15-20 % source training data using sADDi while achieving similar performance reported by a recent related study  [28] .\n\nFuture studies will include evaluating the ADDi and sADDi architectures to model other factors of speech variations, including age, subject, gender, phoneme, noise, and recording device. Further experiments may include evaluating the proposed methods in wild conditions like noisy speech and adversarial noise. We are also interested in exploring multimodal pretext task techniques in our future work. Multimodal human interaction in video and textual form can provide various opportunities for self-supervised learning to improve cross-corpus and cross-language SER.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , where subﬁgure",
      "page": 4
    },
    {
      "caption": "Figure 1: Overall structure of the proposed framework. We use numbers for different components, where (1) shows the pretext task that is trained on synthetic data",
      "page": 5
    },
    {
      "caption": "Figure 1: as a subﬁgure with label 1. It follows the GAN architecture",
      "page": 5
    },
    {
      "caption": "Figure 2: for cross-corpus and cross-language SER. For cross-corpus",
      "page": 8
    },
    {
      "caption": "Figure 2: Impact of self-supervised pre-training on cross-corpus SER (Figure 2a and 2b) using the IEMOCAP and MSP-IMPROV datasets and cross-language SER (Figure 2c",
      "page": 9
    },
    {
      "caption": "Figure 3: Similar results",
      "page": 9
    },
    {
      "caption": "Figure 3: shows that the margin",
      "page": 9
    },
    {
      "caption": "Figure 3: c and 3d. The red dot shows the",
      "page": 9
    },
    {
      "caption": "Figure 3: Results for cross-corpus SER with increasing amounts of labels from the target and source datasets.",
      "page": 10
    },
    {
      "caption": "Figure 4: Effect of increasing the training data in hours on the performance (UAR %).",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Author (Year)": "",
          "Technique": "",
          "Evaluations": "Cross-Corpus",
          "Adversarial\nLearning": "",
          "Self-Supervised\nLearning": ""
        },
        {
          "Author (Year)": "Schuller et al. (2010) [14]",
          "Technique": "Feature\nnormalisation",
          "Evaluations": "",
          "Adversarial\nLearning": "(cid:55)",
          "Self-Supervised\nLearning": "(cid:55)"
        },
        {
          "Author (Year)": "Zhang et al. (2011) [29]",
          "Technique": "Feature\nnormalisation",
          "Evaluations": "",
          "Adversarial\nLearning": "(cid:55)",
          "Self-Supervised\nLearning": "(cid:55)"
        },
        {
          "Author (Year)": "Kim et al. (2017) [30]",
          "Technique": "Aggregated corpora\ntraining",
          "Evaluations": "",
          "Adversarial\nLearning": "(cid:55)",
          "Self-Supervised\nLearning": "(cid:55)"
        },
        {
          "Author (Year)": "Latif et al. (2018) [9]",
          "Technique": "Transfer learning",
          "Evaluations": "",
          "Adversarial\nLearning": "(cid:55)",
          "Self-Supervised\nLearning": "(cid:55)"
        },
        {
          "Author (Year)": "Neumann et al. (2018) [31]",
          "Technique": "Aggregated corpora\ntraining",
          "Evaluations": "",
          "Adversarial\nLearning": "(cid:55)",
          "Self-Supervised\nLearning": "(cid:55)"
        },
        {
          "Author (Year)": "Abdelwahab et al. (2018) [32]",
          "Technique": "Domain adaptation",
          "Evaluations": "",
          "Adversarial\nLearning": "",
          "Self-Supervised\nLearning": "(cid:55)"
        },
        {
          "Author (Year)": "Latif et.al [33] (2019)",
          "Technique": "Domain adaptation",
          "Evaluations": "(cid:55)",
          "Adversarial\nLearning": "",
          "Self-Supervised\nLearning": "(cid:55)"
        },
        {
          "Author (Year)": "Song et.al [34] (2019)",
          "Technique": "Feature subspace\nlearning",
          "Evaluations": "",
          "Adversarial\nLearning": "(cid:55)",
          "Self-Supervised\nLearning": "(cid:55)"
        },
        {
          "Author (Year)": "Gideon et al. (2019) [28]",
          "Technique": "Domain adaptation",
          "Evaluations": "",
          "Adversarial\nLearning": "",
          "Self-Supervised\nLearning": "(cid:55)"
        },
        {
          "Author (Year)": "Xiao et al. (2020) [25]",
          "Technique": "Domain adaptation",
          "Evaluations": "",
          "Adversarial\nLearning": "",
          "Self-Supervised\nLearning": "(cid:55)"
        },
        {
          "Author (Year)": "Luo et.al [35] (2019)",
          "Technique": "Feature subspace\nlearning",
          "Evaluations": "",
          "Adversarial\nLearning": "(cid:55)",
          "Self-Supervised\nLearning": "(cid:55)"
        },
        {
          "Author (Year)": "Ahn et al. (2021) [36]",
          "Technique": "Domain adaptation &\naggregated corpora\ntraining",
          "Evaluations": "",
          "Adversarial\nLearning": "",
          "Self-Supervised\nLearning": "(cid:55)"
        },
        {
          "Author (Year)": "Ours (2022)",
          "Technique": "Domain adaptation",
          "Evaluations": "",
          "Adversarial\nLearning": "",
          "Self-Supervised\nLearning": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: Cross-corpus SER results in UAR (%) using IEMOCAP (English) and",
      "data": [
        {
          "Model": "CNN (baseline)",
          "IEMOCAP to\nMSP-IMPROV": "42.5±1.6",
          "MSP-IMPROV to\nIEMOCAP": "44.3±1.5"
        },
        {
          "Model": "DANN [32]",
          "IEMOCAP to\nMSP-IMPROV": "42.8±1.4",
          "MSP-IMPROV to\nIEMOCAP": "44.9±1.7"
        },
        {
          "Model": "GAN [33]",
          "IEMOCAP to\nMSP-IMPROV": "43.6±1.3",
          "MSP-IMPROV to\nIEMOCAP": "45.8±1.5"
        },
        {
          "Model": "ADDOG [28]",
          "IEMOCAP to\nMSP-IMPROV": "44.4±0.9",
          "MSP-IMPROV to\nIEMOCAP": "47.4±0.7"
        },
        {
          "Model": "ADDi (proposed)",
          "IEMOCAP to\nMSP-IMPROV": "45.1±0.8",
          "MSP-IMPROV to\nIEMOCAP": "48.2±0.6"
        },
        {
          "Model": "CNNSSL (baseline)",
          "IEMOCAP to\nMSP-IMPROV": "43.8±1.2",
          "MSP-IMPROV to\nIEMOCAP": "45.3 ± 1.1"
        },
        {
          "Model": "sADDi (proposed)",
          "IEMOCAP to\nMSP-IMPROV": "47.1±0.5",
          "MSP-IMPROV to\nIEMOCAP": "49.8±0.6"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: , results using synthetic emotional data generation for both",
      "data": [
        {
          "Model": "CNN (baseline)",
          "IEMOCAP (English)\nto EMODB (German)": "42.2± 1.9",
          "EMODB (German)\nto IEMOCAP (English)": "38.4±2.2"
        },
        {
          "Model": "DBN [9]",
          "IEMOCAP (English)\nto EMODB (German)": "42.5±2.1",
          "EMODB (German)\nto IEMOCAP (English)": "39.5±2.4"
        },
        {
          "Model": "CNN-LSTM [45]",
          "IEMOCAP (English)\nto EMODB (German)": "42.1±1.8",
          "EMODB (German)\nto IEMOCAP (English)": "38.9±2.1"
        },
        {
          "Model": "AE [87]",
          "IEMOCAP (English)\nto EMODB (German)": "43.2±2.3",
          "EMODB (German)\nto IEMOCAP (English)": "40.1±1.8"
        },
        {
          "Model": "GAN [33]",
          "IEMOCAP (English)\nto EMODB (German)": "44.3±1.7",
          "EMODB (German)\nto IEMOCAP (English)": "40.3±1.7"
        },
        {
          "Model": "DANN [32]",
          "IEMOCAP (English)\nto EMODB (German)": "43.5±1.8",
          "EMODB (German)\nto IEMOCAP (English)": "40.5±2.0"
        },
        {
          "Model": "46.1±1.6\nADDi (proposed)",
          "IEMOCAP (English)\nto EMODB (German)": "",
          "EMODB (German)\nto IEMOCAP (English)": "41.2±1.8"
        },
        {
          "Model": "43.5±1.7\nCNNSSL (baseline)",
          "IEMOCAP (English)\nto EMODB (German)": "",
          "EMODB (German)\nto IEMOCAP (English)": "40.2±1.9"
        },
        {
          "Model": "48.3±1.5\nsADDi (proposed)",
          "IEMOCAP (English)\nto EMODB (German)": "",
          "EMODB (German)\nto IEMOCAP (English)": "44.8±1.6"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: , results using synthetic emotional data generation for both",
      "data": [
        {
          "Model": "",
          "IEMOCAP (English)\nto RECOLA (French)": "arousal\nvalence",
          "RECOLA (French)\nto IEMOCAP (English)": "arousal\nvalence"
        },
        {
          "Model": "CNN (baseline)",
          "IEMOCAP (English)\nto RECOLA (French)": "59.2± 1.8\n48.5± 1.5",
          "RECOLA (French)\nto IEMOCAP (English)": "60.7± 1.6\n48.3± 2.0"
        },
        {
          "Model": "ACNN [31]",
          "IEMOCAP (English)\nto RECOLA (French)": "59.3\n49.1",
          "RECOLA (French)\nto IEMOCAP (English)": "61.2\n47.5"
        },
        {
          "Model": "GAN [33]",
          "IEMOCAP (English)\nto RECOLA (French)": "59.8±1.9\n49.8±1.7",
          "RECOLA (French)\nto IEMOCAP (English)": "60.3±1.3\n48.7± 1.5"
        },
        {
          "Model": "DANN [32]",
          "IEMOCAP (English)\nto RECOLA (French)": "60.1±2.1\n50.2±1.5",
          "RECOLA (French)\nto IEMOCAP (English)": "61.5±1.5\n49.2± 1.4"
        },
        {
          "Model": "ADDi (proposed)",
          "IEMOCAP (English)\nto RECOLA (French)": "61.5±1.2\n51.8±1.4",
          "RECOLA (French)\nto IEMOCAP (English)": "62.2±1.3\n50.9±1.2"
        },
        {
          "Model": "60.1±1.5\n49.2± 1.3\n61.2 ± 1.6\n49.0± 1.4\nCNNSSL (baseline)",
          "IEMOCAP (English)\nto RECOLA (French)": "",
          "RECOLA (French)\nto IEMOCAP (English)": ""
        },
        {
          "Model": "63.8±1.0\n53.8±1.2\n64.2±1.4\n52.5±1.3\nsADDi (proposed)",
          "IEMOCAP (English)\nto RECOLA (French)": "",
          "RECOLA (French)\nto IEMOCAP (English)": ""
        },
        {
          "Model": "using fraction of target date for ﬁne-tuning.",
          "IEMOCAP (English)\nto RECOLA (French)": "",
          "RECOLA (French)\nto IEMOCAP (English)": ""
        },
        {
          "Model": "ACNN [31] (500 target samples)\n67.03\n50.42\n63.07\n49.81",
          "IEMOCAP (English)\nto RECOLA (French)": "",
          "RECOLA (French)\nto IEMOCAP (English)": ""
        },
        {
          "Model": "70.3±1.3\n57.1±1.3\n68.6±1.0\n56.3±1.1\nsADDi (250 target samples)",
          "IEMOCAP (English)\nto RECOLA (French)": "",
          "RECOLA (French)\nto IEMOCAP (English)": ""
        },
        {
          "Model": "using language information.",
          "IEMOCAP (English)\nto RECOLA (French)": "",
          "RECOLA (French)\nto IEMOCAP (English)": ""
        },
        {
          "Model": "72.4±1.6\n60.1±1.4\n70.2±1.3\n59.3±1.2\nsADDi (250 target samples)",
          "IEMOCAP (English)\nto RECOLA (French)": "",
          "RECOLA (French)\nto IEMOCAP (English)": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "1",
          "Conﬁguration": "",
          "Discriminators": "2",
          "Decoder": "",
          "Encoder": "",
          "Classiﬁer": "",
          "Cross-corpus\nUAR (%)": "45.1±0.8",
          "Cross-Language\nUAR (%)": "46.1±1.6"
        },
        {
          "Model": "2",
          "Conﬁguration": "",
          "Discriminators": "1",
          "Decoder": "",
          "Encoder": "",
          "Classiﬁer": "",
          "Cross-corpus\nUAR (%)": "43.2±1.3",
          "Cross-Language\nUAR (%)": "44.0±1.9"
        },
        {
          "Model": "3",
          "Conﬁguration": "",
          "Discriminators": "1",
          "Decoder": "",
          "Encoder": "",
          "Classiﬁer": "",
          "Cross-corpus\nUAR (%)": "43.1±1.4",
          "Cross-Language\nUAR (%)": "43.5±1.8"
        },
        {
          "Model": "4",
          "Conﬁguration": "",
          "Discriminators": "(cid:55)",
          "Decoder": "",
          "Encoder": "",
          "Classiﬁer": "",
          "Cross-corpus\nUAR (%)": "42.7±1.2",
          "Cross-Language\nUAR (%)": "43.3±1.8"
        },
        {
          "Model": "5",
          "Conﬁguration": "",
          "Discriminators": "(cid:55)",
          "Decoder": "(cid:55)",
          "Encoder": "",
          "Classiﬁer": "",
          "Cross-corpus\nUAR (%)": "42.5±1.6",
          "Cross-Language\nUAR (%)": "41.2±1.9"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech technology for healthcare: Opportunities, challenges, and state of the art",
      "authors": [
        "S Latif",
        "J Qadir",
        "A Qayyum",
        "M Usama",
        "S Younis"
      ],
      "year": "2020",
      "venue": "IEEE Reviews in Biomedical Engineering"
    },
    {
      "citation_id": "2",
      "title": "Automated screening for distress: A perspective for the future",
      "authors": [
        "R Rana",
        "S Latif",
        "R Gururajan",
        "A Gray",
        "G Mackenzie",
        "G Humphris",
        "J Dunn"
      ],
      "year": "2019",
      "venue": "European journal of cancer care"
    },
    {
      "citation_id": "3",
      "title": "How to increase automated vehicles' acceptance through invehicle interaction design: A review",
      "authors": [
        "H Detjen",
        "S Faltaous",
        "B Pfleging",
        "S Geisler",
        "S Schneegass"
      ],
      "year": "2021",
      "venue": "International Journal of Human-Computer Interaction"
    },
    {
      "citation_id": "4",
      "title": "Processing of emotions in speech in forensic patients with schizophrenia: Impairments in identification, selective attention, and integration of speech channels",
      "authors": [
        "R Leshem",
        "M Icht",
        "R Bentaur",
        "B Ben-David"
      ],
      "year": "2020",
      "venue": "Frontiers in Psychiatry"
    },
    {
      "citation_id": "5",
      "title": "Affective computing in education: A systematic review and future research",
      "authors": [
        "E Yadegaridehkordi",
        "N Noor",
        "M Ayub",
        "H Affal",
        "N Hussin"
      ],
      "year": "2019",
      "venue": "Computers & Education"
    },
    {
      "citation_id": "6",
      "title": "A fast learning algorithm for deep belief nets",
      "authors": [
        "G Hinton",
        "S Osindero",
        "Y.-W Teh"
      ],
      "year": "2006",
      "venue": "Neural computation"
    },
    {
      "citation_id": "7",
      "title": "Handwritten digit recognition with a backpropagation network",
      "authors": [
        "Y Lecun",
        "B Boser",
        "J Denker",
        "D Henderson",
        "R Howard",
        "W Hubbard",
        "L Jackel"
      ],
      "year": "1989",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "8",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "9",
      "title": "Transfer learning for improving speech emotion classification accuracy",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Interspeech 2018: Proceedings"
    },
    {
      "citation_id": "10",
      "title": "Direct modelling of speech emotion from raw speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "12",
      "title": "Deep architecture enhancing robustness to noise, adversarial attacks, and cross-corpus setting for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21st Annual Conference of the International Speech Communication Association (INTERSPEECH 2020)"
    },
    {
      "citation_id": "13",
      "title": "Deep representation learning for improving speech emotion recognition",
      "authors": [
        "S Latif"
      ],
      "year": "2020",
      "venue": "Doctoral Consortium"
    },
    {
      "citation_id": "14",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "M Öllmer",
        "A Stuhlsatz",
        "A Wendemuth",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usman",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Frontiers of Information Technology (FIT)"
    },
    {
      "citation_id": "17",
      "title": "Learning transferable features with deep adaptation networks",
      "authors": [
        "M Long",
        "Y Cao",
        "J Wang",
        "M Jordan"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "18",
      "title": "Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation",
      "authors": [
        "H Yan",
        "Y Ding",
        "P Li",
        "Q Wang",
        "Y Xu",
        "W Zuo"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Return of frustratingly easy domain adaptation",
      "authors": [
        "B Sun",
        "J Feng",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Deep coral: Correlation alignment for deep domain adaptation",
      "authors": [
        "B Sun",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "21",
      "title": "Deep reconstruction-classification networks for unsupervised domain adaptation",
      "authors": [
        "M Ghifary",
        "W Kleijn",
        "M Zhang",
        "D Balduzzi",
        "W Li"
      ],
      "year": "2016",
      "venue": "Deep reconstruction-classification networks for unsupervised domain adaptation"
    },
    {
      "citation_id": "22",
      "title": "Domain generalization for object recognition with multi-task autoencoders",
      "authors": [
        "M Ghifary",
        "W Kleijn",
        "M Zhang",
        "D Balduzzi"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "23",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "24",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "25",
      "title": "Learning class-aligned and generalized domain-invariant representations for speech emotion recognition",
      "authors": [
        "Y Xiao",
        "H Zhao",
        "T Li"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence"
    },
    {
      "citation_id": "26",
      "title": "Speaker-invariant adversarial domain adaptation for emotion recognition",
      "authors": [
        "Y Yin",
        "B Huang",
        "Y Wu",
        "M Soleymani"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "27",
      "title": "Adversarial dual distinct classifiers for unsupervised domain adaptation",
      "authors": [
        "T Jing",
        "Z Ding"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "28",
      "title": "Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (addog)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Unsupervised learning in cross-corpus acoustic emotion recognition",
      "authors": [
        "Z Zhang",
        "F Weninger",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "2011 IEEE Workshop on Automatic Speech Recognition & Understanding"
    },
    {
      "citation_id": "30",
      "title": "Towards speech emotion recognition\" in the wild\" using aggregated corpora and deep multi-task learning",
      "authors": [
        "J Kim",
        "G Englebienne",
        "K Truong",
        "V Evers"
      ],
      "year": "2017",
      "venue": "18th Annual Conference of the International Speech Communication Association, INTERSPEECH 2017: Situated interaction. International Speech Communication Association (ISCA)"
    },
    {
      "citation_id": "31",
      "title": "Cross-lingual and multilingual speech emotion recognition on english and french",
      "authors": [
        "M Neumann"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "authors": [
        "S Latif",
        "J Qadir",
        "M Bilal"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "34",
      "title": "Transfer linear subspace learning for cross-corpus speech emotion recognition",
      "authors": [
        "P Song"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "35",
      "title": "Nonnegative matrix factorization based transfer subspace learning for cross-corpus speech emotion recognition",
      "authors": [
        "H Luo",
        "J Han"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "36",
      "title": "Cross-corpus speech emotion recognition based on few-shot learning and domain adaptation",
      "authors": [
        "Y Ahn",
        "S Lee",
        "J Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "37",
      "title": "Crosscorpus classification of realistic emotions-some pilot experiments",
      "authors": [
        "F Eyben",
        "A Batliner",
        "B Schuller",
        "D Seppi",
        "S Steidl"
      ],
      "year": "2010",
      "venue": "Proc. 7th Intern. Conf. on Language Resources and Evaluation (LREC 2010)"
    },
    {
      "citation_id": "38",
      "title": "Using multiple databases for training in emotion recognition: To unite or to vote?",
      "authors": [
        "B Schuller",
        "Z Zhang",
        "F Weninger",
        "G Rigoll"
      ],
      "year": "2011",
      "venue": "Twelfth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "39",
      "title": "Emotion recognition in neverseen languages using a novel ensemble method with emotion profiles",
      "authors": [
        "E Albornoz",
        "D Milone"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Recognizing human emotional state from audiovisual signals",
      "authors": [
        "Y Wang",
        "L Guan"
      ],
      "year": "2008",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "41",
      "title": "Improving multilingual speech emotion recognition by combining acoustic features in a three-layer model",
      "authors": [
        "X Li",
        "M Akagi"
      ],
      "year": "2019",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "42",
      "title": "Cross-language acoustic emotion recognition: An overview and some tendencies",
      "authors": [
        "S Feraru",
        "D Schuller"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "43",
      "title": "Learning cross-lingual knowledge with multilingual blstm for emphasis detection with limited training data",
      "authors": [
        "Y Ning",
        "Z Wu",
        "R Li",
        "J Jia",
        "M Xu",
        "H Meng",
        "L Cai"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Multi-task deep neural network with shared hidden layers: Breaking down the wall between emotion representations",
      "authors": [
        "Y Zhang",
        "Y Liu",
        "F Weninger",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "45",
      "title": "Analysis of deep learning architectures for cross-corpus speech emotion recognition",
      "authors": [
        "J Parry",
        "D Palaz",
        "G Clarke",
        "P Lecomte",
        "R Mead",
        "M Berger",
        "G Hofer"
      ],
      "year": "2019",
      "venue": "Analysis of deep learning architectures for cross-corpus speech emotion recognition"
    },
    {
      "citation_id": "46",
      "title": "Towards Speech Emotion Recognition \"in the Wild\" Using Aggregated Corpora and Deep Multi-Task Learning",
      "authors": [
        "J Kim",
        "G Englebienne",
        "K Truong",
        "V Evers"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "47",
      "title": "Autoencoder-based unsupervised domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "48",
      "title": "Universum autoencoder-based domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "X Xu",
        "Z Zhang",
        "S Fr",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "49",
      "title": "Supervised domain adaptation for emotion recognition from speech",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "50",
      "title": "Dual exclusive attentive transfer for unsupervised deep convolutional domain adaptation in speech emotion recognition",
      "authors": [
        "E Ocquaye",
        "Q Mao",
        "H Song",
        "G Xu",
        "Y Xue"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "51",
      "title": "Transferable positive/negative speech emotion recognition via class-wise adversarial domain adaptation",
      "authors": [
        "H Zhou",
        "K Chen"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "52",
      "title": "Self-taught learning: transfer learning from unlabeled data",
      "authors": [
        "R Raina",
        "A Battle",
        "H Lee",
        "B Packer",
        "A Ng"
      ],
      "year": "2007",
      "venue": "Proceedings of the 24th international conference on Machine learning"
    },
    {
      "citation_id": "53",
      "title": "Self-supervised visual feature learning with deep neural networks: A survey",
      "authors": [
        "L Jing",
        "Y Tian"
      ],
      "year": "2020",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "54",
      "title": "ALBERT: A Lite BERT for self-supervised learning of language representations",
      "authors": [
        "Z Lan",
        "M Chen",
        "S Goodman",
        "K Gimpel",
        "P Sharma",
        "R Soricut"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "55",
      "title": "VQ-WAV2VEC: Selfsupervised learning of discrete speech representations",
      "authors": [
        "A Baevski",
        "S Schneider",
        "M Auli"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "56",
      "title": "Learning problem-agnostic speech representations from multiple self-supervised tasks",
      "authors": [
        "S Pascual",
        "M Ravanelli",
        "J Serrà",
        "A Bonafonte",
        "Y Bengio"
      ],
      "year": "2019",
      "venue": "Learning problem-agnostic speech representations from multiple self-supervised tasks"
    },
    {
      "citation_id": "57",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "58",
      "title": "On the use of self-supervised pre-trained acoustic and linguistic features for continuous speech emotion recognition",
      "authors": [
        "M Macary",
        "M Tahon",
        "Y Estève",
        "A Rousseau"
      ],
      "year": "2020",
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "59",
      "title": "Self-supervised learning with cross-modal transformers for emotion recognition",
      "authors": [
        "A Khare",
        "S Parthasarathy",
        "S Sundaram"
      ],
      "year": "2020",
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "60",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "61",
      "title": "Visually guided self supervised learning of speech representations",
      "authors": [
        "A Shukla",
        "K Vougioukas",
        "P Ma",
        "S Petridis",
        "M Pantic"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "62",
      "title": "Selfsupervised domain adaptation with consistency training",
      "authors": [
        "L Xiao",
        "J Xu",
        "D Zhao",
        "Z Wang",
        "L Wang",
        "Y Nie",
        "B Dai"
      ],
      "year": "2021",
      "venue": "2020 25th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "63",
      "title": "Split-brain autoencoders: Unsupervised learning by cross-channel prediction",
      "authors": [
        "R Zhang",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "64",
      "title": "Stargan: Unified generative adversarial networks for multi-domain imageto-image translation",
      "authors": [
        "Y Choi",
        "M Choi",
        "M Kim",
        "J.-W Ha",
        "S Kim",
        "J Choo"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "65",
      "title": "Wasserstein generative adversarial networks",
      "authors": [
        "M Arjovsky",
        "S Chintala",
        "L Bottou"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "66",
      "title": "Large scale gan training for high fidelity natural image synthesis",
      "authors": [
        "A Brock",
        "J Donahue",
        "K Simonyan"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "67",
      "title": "Bagan: Data augmentation with balancing gan",
      "authors": [
        "G Mariani",
        "F Scheidegger",
        "R Istrate",
        "C Bekas",
        "C Malossi"
      ],
      "year": "2018",
      "venue": "Bagan: Data augmentation with balancing gan",
      "arxiv": "arXiv:1803.09655"
    },
    {
      "citation_id": "68",
      "title": "Ganbased data generation for speech emotion recognition",
      "authors": [
        "S Eskimez",
        "D Dimitriadis",
        "R Gmyr",
        "K Kumanati"
      ],
      "year": "2020",
      "venue": "INTER-SPEECH"
    },
    {
      "citation_id": "69",
      "title": "Conditional generative adversarial nets",
      "authors": [
        "M Mirza",
        "S Osindero"
      ],
      "year": "2014",
      "venue": "Conditional generative adversarial nets",
      "arxiv": "arXiv:1411.1784"
    },
    {
      "citation_id": "70",
      "title": "Data augmentation using gans for speech emotion recognition",
      "authors": [
        "A Chatziagapi",
        "G Paraskevopoulos",
        "D Sgouropoulos",
        "G Pantazopoulos",
        "M Nikandrou",
        "T Giannakopoulos",
        "A Katsamanis",
        "A Potamianos",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Data augmentation using gans for speech emotion recognition"
    },
    {
      "citation_id": "71",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "72",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "73",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "74",
      "title": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "75",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "76",
      "title": "Increasing the reliability of crowdsourcing evaluations using online quality assessment",
      "authors": [
        "A Burmania",
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "77",
      "title": "Progressive neural networks for transfer learning in emotion recognition",
      "authors": [
        "J Gideon",
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Progressive neural networks for transfer learning in emotion recognition",
      "arxiv": "arXiv:1706.03256"
    },
    {
      "citation_id": "78",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "79",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "80",
      "title": "Automatic classification of emotion related user states in spontaneous children's speech",
      "authors": [
        "S Steidl"
      ],
      "year": "2009",
      "venue": "Automatic classification of emotion related user states in spontaneous children's speech"
    },
    {
      "citation_id": "81",
      "title": "Endto-end automatic speech translation of audiobooks",
      "authors": [
        "A Bérard",
        "L Besacier",
        "A Kocabiyikoglu",
        "O Pietquin"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "82",
      "title": "Transfer learning using raw waveform sincnet for robust speaker diarization",
      "authors": [
        "H Dubey",
        "A Sangwan",
        "J Hansen"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "83",
      "title": "Convolutional gated recurrent neural network incorporating spatial features for audio tagging",
      "authors": [
        "Y Xu",
        "Q Kong",
        "Q Huang",
        "W Wang",
        "M Plumbley"
      ],
      "year": "2017",
      "venue": "2017 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "84",
      "title": "The kaldi speech recognition toolkit",
      "authors": [
        "D Povey",
        "A Ghoshal",
        "G Boulianne",
        "L Burget",
        "O Glembek",
        "N Goel",
        "M Hannemann",
        "P Motlicek",
        "Y Qian",
        "P Schwarz"
      ],
      "year": "2011",
      "venue": "IEEE 2011 workshop on automatic speech recognition and understanding"
    },
    {
      "citation_id": "85",
      "title": "Augmenting generative adversarial networks for speech emotion recognition",
      "authors": [
        "S Latif",
        "M Asim",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH"
    },
    {
      "citation_id": "86",
      "title": "Multi-task semi-supervised adversarial autoencoding for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "87",
      "title": "Sparse autoencoderbased feature transfer learning for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "88",
      "title": "Visual self-supervision by facial reconstruction for speech representation learning",
      "authors": [
        "A Shukla",
        "S Petridis",
        "M Pantic"
      ],
      "year": "2020",
      "venue": "Sight and Sound Workshop, CVPR"
    },
    {
      "citation_id": "89",
      "title": "On enhancing speech emotion recognition using generative adversarial networks",
      "authors": [
        "S Sahu",
        "R Gupta",
        "C Espy-Wilson"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "90",
      "title": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition",
      "authors": [
        "F Bao",
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition"
    },
    {
      "citation_id": "91",
      "title": "International Conference on Learning Representations",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "92",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    }
  ]
}