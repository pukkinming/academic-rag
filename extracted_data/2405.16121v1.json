{
  "paper_id": "2405.16121v1",
  "title": "Design And Implementation Of An Emotion Analysis System Based On Eeg Signals",
  "published": "2024-05-25T08:20:32Z",
  "authors": [
    "Zhang Yutian",
    "Huang Shan",
    "Zhang Jianing",
    "Fan Ci'en"
  ],
  "keywords": [
    "：electroencephalogram(EEG)",
    "deep learning",
    "emotional analysis",
    "ACPA ResNet",
    "attention mechanism"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Abstract：Traditional brain-computer systems are complex and expensive, and emotion classification algorithms lack representations of the intrinsic relationships between different channels of electroencephalogram (EEG) signals. There is still room for improvement in accuracy. To lower the research barrier for EEG and harness the rich information embedded in multi-channel EEG, we propose and implement a simple and user-friendly brain-computer system for classifying four emotions: happiness, sorrow, sadness, and tranquility. This system utilizes the fusion of convolutional attention mechanisms and fully pre-activated residual blocks, termed Attention-Convolution-based Pre-Activated Residual Network (ACPA-ResNet).In the hardware acquisition and preprocessing phase, we employ the ADS1299 integrated chip as the analog front-end and utilize the ESP32 microcontroller for initial EEG signal processing. Data is wirelessly transmitted to a PC through UDP protocol for further preprocessing. In the emotion analysis phase, ACPA-ResNet is designed to automatically extract and learn features from EEG signals, thereby enabling accurate classification of emotional states by learning time-frequency domain characteristics. ACPA-ResNet introduces an attention mechanism on the foundation of residual networks, adaptively assigning different weights to each channel. This allows it to focus on more meaningful EEG signals in both spatial and channel dimensions while avoiding the problems of gradient dispersion and explosion associated with deep network architectures.Through testing on 16 subjects, our system demonstrates stable EEG signal acquisition and transmission. The novel network significantly enhances emotion recognition accuracy, achieving an average emotion classification accuracy of 95.1%.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "System Design",
      "text": "To address the shortcomings in EEG signal collection related to data transmission, device portability, and research costs, as well as deficiencies in emotion classification analysis, this paper aims to design and implement a simple, portable , affordable, and highly accurate emotion analysis system based on EEG signals.\n\nThe system is divided into two main parts: the front-end hardware collection and preprocessing section (Figure  1 ), and the back-end emotion classification algorithm section (Figure  4 ). Subjects generate different emotions while watching videos. The signals are collected and amplified by the analog front-end, then wirelessly transmitted via an MCU to the PC for further preprocessing and emotion classification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Frontend Collection",
      "text": "The collection of EEG signals relies on the accurate placement of electrodes. This system utilizes the international 10-20 system (Figure  3 ) as the reference standard, selecting eight electrode channels: P3, Pz, P4, O1, Oz, O2, T5, and T6.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fig.2 International 10-20 System",
      "text": "These electrode channels cover various areas on the posterior scalp, allowing the collected EEG signals to comprehensively reflect the subject's emotional state. Additionally, a ground electrode and a bias electrode are introduced, connected to GND and the earlobes respectively, to enhance the common mode rejection ratio and reduce interference caused by subject movement. The electrodes chosen are gold disc electrodes, and a suitable amount of conductive gel is applied between the electrodes and the subject's scalp to minimize environmental disturbances, enhance system stability, and ensure high-quality EEG signal collection.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fig.3 Two-Dimensional Emotion Classification Model",
      "text": "After the subjects don the collection device, they generate four types of emotions-happiness, distress, sadness, and calmness-by watching related videos and through self-reflection. The choice of emotions is guided by typical values determined by the valence-arousal (VA) model  [12]  , a two-dimensional continuous model comprised of arousal and valence levels, as illustrated in Figure  3 .\n\nGiven that the EEG signals collected via non-invasive brain-computer interfaces typically range from 0.02 to 0.5μV and are prone to disturbances, the system employs an eight-channel, 24-bit high-precision AD chip-ADS1299-as the core component of the analog frontend. After sampling, the digital signals are transmitted to the MCU via the SPI communication protocol.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Transmission",
      "text": "To enhance the system's portability, enable wireless transmission of EEG signals, and reduce research costs, the system uses a domestically produced MCU equipped with a 2.4GHz WIFI chip-ESP32-as the core component for transmitting and receiving EEG data. The MCU communicates with the analog frontend via SPI to receive eight channels of 24-bit data. These data are then decoded into eight floating-point numbers in physical units (μV), re-encoded, and transmitted in real-time to the PC listening via the UDP protocol, which is minimal in overhead and reliable. Experiments show that the data transmission delay is as low as 0.02 ms, with no packet loss observed.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Preprocessing And Feature Extraction",
      "text": "The raw EEG signals received by the PC are mixed with multiple frequencies and include power frequency interference and artifacts such as eye movements, making them unsuitable for direct use. The system undertakes several preprocessing and feature extraction steps. It employs an 11th-order Butterworth band-pass digital filter to extract the 5-18 Hz frequency band (α band) from the raw EEG signals. Event marking and segmentation are followed by Independent Component Analysis (ICA) to remove artifacts related to eye movements. After normalization, the short-time Fourier transform (STFT) is used to extract time-frequency domain features from the artifact-free 5-18 Hz EEG signals. The dimensionally enhanced signals are then fed into a neural network for emotion classification.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Classification Algorithm",
      "text": "After the preprocessing phase, the PC receives the feature data of EEG signals, which then enters the emotion classification phase. The system incorporates a neural network that combines convolutional attention mechanisms and fully pre-activated residual blocks, known as the Attention-Convolution-based Pre-Activated Residual Network (ACPA-ResNet), to perform emotion classification. The specific architecture of this network is shown in Figure  4 .\n\nThis system architecture supports efficient processing and classification of emotional states from EEG signals, utilizing advanced signal processing techniques and neural network models to ensure high accuracy and reliability in emotion detection. Hidden Layers -Fusion Module: This module consists of three parts: a convolutional layer, a Convolutional Block Attention Module (CBAM)  [13]  , and a Fully Pre-Activated Residual Module (PARM). The convolutional layer increases the number of channels to facilitate the subsequent channel-wise attention mechanism in the CBAM. The CBAM is a simple yet effective attention module that computes attention weights in both spatial and channel dimensions, and then adapts the feature maps by multiplying them with the original feature maps, aligning well with the characteristics of EEG signals. The PARM is an improvement over the traditional Residual Block (ResBlock)  [14]  . Unlike the original ResBlock pattern of weight-BatchNorm-ReLU-weight-BatchNorm  [15]  , the Pre-activation ResBlock uses BatchNorm-ReLU-weight-BatchNorm-ReLU-weight. This configuration allows gradients to flow unimpeded through fast connections to any layer in the module, thus accelerating convergence and ensuring robust performance.\n\nHidden Layer -Fully Connected Layer (FC): This layer uses the ReLU activation function to introduce non -linearity, enhancing the ability to capture complex patterns in the data.\n\nOutput Layer: Comprises a fully connected layer with four neurons, each outputting the prediction results for one of the four emotional states analyzed (happiness, distress, sadness, and calmness). This layer is responsible for mapping the high-level features learned by the network into the final emotion classifications.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiments And Results Analysis 2.1 Experimental Setup",
      "text": "The hardware setup for this experiment, as shown in Figure  5",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Eeg Signal Acquisition Results",
      "text": "The ADS1299 analog front-end successfully captured high-quality EEG signals from subjects. Figure  6",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Accuracy Of Emotion Analysis Algorithm",
      "text": "The emotion classification model, constructed using ACPA-ResNet, classified emotional states from the collected EEG signals and was compared with existing studies. System recognition accuracy was evaluated against an emotion status survey filled out by subjects during the experiments. The results, as shown in Figure  7  and Table  1 , demonstrate that the system performs excellently in terms of emotion analysis accuracy, successfully classifying different emotional states.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "This study designed and implemented an emotion analysis system based on EEG signals using the ADS1299 analog front-end and the ESP32 microcontroller. Data transmission through the UDP protocol ensured real -time and stable performance. The experiments confirmed the system's ability to obtain high-quality EEG signals and to accurately reflect the brain's activity under different emotional states. The system demonstrated excellent real-time performance and stable data transmission capabilities. It showed outstanding accuracy in emotion analysis, accurately assessing subjects' emotional states. However, there are still limitations to the system, and future improvements could focus on optimizing algorithms and hardware design, expanding the range of test subjects, and diversifying emotion state classifications. This research makes a positive contribution to the field of EEG signal collection and emotion analysis, offering new ideas and direction.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 4: ). Subjects generate different emotions while watch-",
      "page": 2
    },
    {
      "caption": "Figure 3: ) as the reference standard, selecting eight electrode channels: P3, Pz, P4, O1, Oz, O2, T5, and",
      "page": 3
    },
    {
      "caption": "Figure 3: Given that the EEG signals collected via non-invasive brain-computer interfaces typically range from 0.02 to 0.5μV",
      "page": 4
    },
    {
      "caption": "Figure 4: This system architecture supports efficient processing and classification of emotional states from EEG signals, uti-",
      "page": 4
    },
    {
      "caption": "Figure 5: , includes an EEG cap, eight-channel gold disc elec-",
      "page": 5
    },
    {
      "caption": "Figure 6: displays the",
      "page": 6
    },
    {
      "caption": "Figure 7: and Table 1, demonstrate",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Design and Implementation of an Emotion Analysis": "System Based on EEG Signals"
        },
        {
          "Design and Implementation of an Emotion Analysis": "ZHANG Yutian,    HUANG Shan,    ZHANG Jianing,    FAN Ci’en"
        },
        {
          "Design and Implementation of an Emotion Analysis": "School of Electronic Information, Wuhan University, Wuhan Hubei 430072, China"
        },
        {
          "Design and Implementation of an Emotion Analysis": "Abstract：Traditional brain-computer systems are complex and expensive, and emotion classification algorithms lack"
        },
        {
          "Design and Implementation of an Emotion Analysis": "representations of the intrinsic relationships between different channels of electroencephalogram (EEG) signals. There"
        },
        {
          "Design and Implementation of an Emotion Analysis": "is still room for improvement in accuracy. To lower the research barrier for EEG and harness the rich information"
        },
        {
          "Design and Implementation of an Emotion Analysis": "embedded in multi-channel EEG, we propose and implement a simple and user-friendly brain-computer system for"
        },
        {
          "Design and Implementation of an Emotion Analysis": "classifying four emotions: happiness, sorrow, sadness, and tranquility. This system utilizes the fusion of convolutional"
        },
        {
          "Design and Implementation of an Emotion Analysis": "attention mechanisms and fully pre-activated residual blocks, termed Attention-Convolution-based Pre-Activated Re-"
        },
        {
          "Design and Implementation of an Emotion Analysis": "sidual Network (ACPA-ResNet).In the hardware acquisition and preprocessing phase, we employ the ADS1299 inte-"
        },
        {
          "Design and Implementation of an Emotion Analysis": "grated chip as the analog front-end and utilize the ESP32 microcontroller for initial EEG signal processing. Data is"
        },
        {
          "Design and Implementation of an Emotion Analysis": "wirelessly transmitted to a PC through UDP protocol for further preprocessing. In the emotion analysis phase, ACPA-"
        },
        {
          "Design and Implementation of an Emotion Analysis": "ResNet is designed to automatically extract and learn features from EEG signals, thereby enabling accurate classifi-"
        },
        {
          "Design and Implementation of an Emotion Analysis": "cation of emotional states by learning time-frequency domain characteristics. ACPA-ResNet introduces an attention"
        },
        {
          "Design and Implementation of an Emotion Analysis": "mechanism on the foundation of residual networks, adaptively assigning different weights to each channel. This allows"
        },
        {
          "Design and Implementation of an Emotion Analysis": "it to focus on more meaningful EEG signals in both spatial and channel dimensions while avoiding the problems of"
        },
        {
          "Design and Implementation of an Emotion Analysis": "gradient  dispersion  and  explosion  associated  with  deep  network  architectures.Through  testing  on  16  subjects,  our"
        },
        {
          "Design and Implementation of an Emotion Analysis": "system demonstrates stable EEG signal acquisition and transmission. The novel network significantly enhances emo-"
        },
        {
          "Design and Implementation of an Emotion Analysis": "tion recognition accuracy, achieving an average emotion classification accuracy of 95.1%."
        },
        {
          "Design and Implementation of an Emotion Analysis": "Key words：electroencephalogram(EEG); deep learning; emotional analysis; ACPA ResNet; attention mechanism"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": "Presently, deep neural networks (DNN) are widely applied for feature extraction and have achieved commendable"
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": "In summary, current EEG signal collection and emotion analysis techniques still face deficiencies in data transmis-"
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": "System Design"
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": "To address the shortcomings in EEG signal collection related to data transmission, device portability, and research"
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": "The system is divided into two main parts: the front-end hardware collection and preprocessing section (Figure 1),"
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        },
        {
          "Electroencephalogram (EEG) signals are weak electrical signals generated by neuronal activities in the brain, re-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Computer Engineering and Applications": "Frontend hardware acquisition and preprocessing process",
          "- 2 -": ""
        },
        {
          "Computer Engineering and Applications": "",
          "- 2 -": ""
        },
        {
          "Computer Engineering and Applications": "",
          "- 2 -": ""
        },
        {
          "Computer Engineering and Applications": "",
          "- 2 -": ""
        },
        {
          "Computer Engineering and Applications": "",
          "- 2 -": ""
        },
        {
          "Computer Engineering and Applications": "",
          "- 2 -": ""
        },
        {
          "Computer Engineering and Applications": "Fig.2 \n  International 10-20 System",
          "- 2 -": ""
        },
        {
          "Computer Engineering and Applications": "",
          "- 2 -": ""
        },
        {
          "Computer Engineering and Applications": "",
          "- 2 -": ""
        },
        {
          "Computer Engineering and Applications": "",
          "- 2 -": ""
        },
        {
          "Computer Engineering and Applications": "",
          "- 2 -": ""
        },
        {
          "Computer Engineering and Applications": "",
          "- 2 -": ""
        },
        {
          "Computer Engineering and Applications": "",
          "- 2 -": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig.3 Two-dimensional emotion classification model": "After the subjects don the collection device, they generate four types of emotions—happiness, distress, sadness, and"
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": "Given that the EEG signals collected via non-invasive brain-computer interfaces typically range from 0.02 to 0.5μV"
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": "Data Transmission"
        },
        {
          "Fig.3 Two-dimensional emotion classification model": "To enhance  the  system's portability,  enable  wireless  transmission  of EEG  signals,  and  reduce  research  costs,  the"
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": "Preprocessing and Feature Extraction"
        },
        {
          "Fig.3 Two-dimensional emotion classification model": "The raw EEG signals received by the PC are mixed with multiple frequencies and include power frequency inter-"
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": "After  the  preprocessing  phase,  the  PC  receives  the  feature  data  of  EEG  signals,  which  then  enters  the  emotion"
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": "This system architecture supports efficient processing and classification of emotional states from EEG signals, uti-"
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        },
        {
          "Fig.3 Two-dimensional emotion classification model": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "Input Layer: This layer receives input in the form of two-dimensional features from eight channels, each channel"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "providing a 16×63 matrix. These inputs are passed to the Fusion Module."
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "Hidden Layers — Fusion Module: This module consists of three parts: a convolutional layer, a Convolutional Block"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "Attention Module (CBAM) [13], and a Fully Pre-Activated Residual Module (PARM). The convolutional layer increases"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "the number of channels to facilitate the subsequent channel-wise attention mechanism in the CBAM. The CBAM is a"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "simple yet effective attention module that computes attention weights in both spatial and channel dimensions, and then"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "adapts the feature maps by multiplying them with the original feature maps, aligning well with the characteristics of EEG"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "signals. The PARM is an improvement over the traditional Residual Block (ResBlock) [14]. Unlike the original ResBlock"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "pattern of weight-BatchNorm-ReLU-weight-BatchNorm[15], the Pre-activation ResBlock uses BatchNorm-ReLU-weight-"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "BatchNorm-ReLU-weight. This configuration allows gradients to flow unimpeded through fast connections to any layer"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "in the module, thus accelerating convergence and ensuring robust performance."
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "Hidden Layer — Fully Connected Layer (FC): This layer uses the ReLU activation function to introduce non -line-"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "arity, enhancing the ability to capture complex patterns in the data."
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "Output Layer: Comprises a fully connected layer with four neurons, each outputting the prediction results for one"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "of the four emotional states analyzed (happiness, distress, sadness, and calmness). This layer is responsible for mapping"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "the high-level features learned by the network into the final emotion classifications."
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "2 \n  Experiments and Results Analysis"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "2.1 \n  Experimental Setup"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "The hardware setup for this experiment, as shown in Figure 5, includes an EEG cap, eight-channel gold disc elec-"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "trodes, the ADS1299 acquisition module, an SPI communication adapter board, and the ESP32 module. A deep learning"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "framework based on Python 3.7 and PyTorch 1.7.1 was established, utilizing a Lenovo Legion Y7000P with an NVIDIA"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "GeForce RTX 2060 for training. The model was trained using ten-fold cross-validation, with 855 samples for training"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "and the remaining 95 for testing. The average of the ten folds was taken as the recognition result for each subject. Sixteen"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "volunteers were recruited as subjects, including individuals of different genders and emotional states. The model's recog-"
        },
        {
          "Fig.4 \n  ACPA ResNet Network Architecture": "nition performance was assessed using the average recognition rate and standard deviation among these 16 subjects."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: , demonstrate",
      "data": [
        {
          "Accuracy of sentiment classification under different network structures": ""
        },
        {
          "Accuracy of sentiment classification under different network structures": "准确度"
        },
        {
          "Accuracy of sentiment classification under different network structures": "58.6%"
        },
        {
          "Accuracy of sentiment classification under different network structures": "65.3%"
        },
        {
          "Accuracy of sentiment classification under different network structures": "72.3%"
        },
        {
          "Accuracy of sentiment classification under different network structures": "88.5%"
        },
        {
          "Accuracy of sentiment classification under different network structures": "91.1%"
        },
        {
          "Accuracy of sentiment classification under different network structures": "92.2%"
        },
        {
          "Accuracy of sentiment classification under different network structures": "95.1%"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "[2] JIANG J F,ZENG Y,LIN Z M,et al. Review on EEG-Based Emotion Assessment[J]. Journal of Information Engineering Univer-"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "sity,2016,17(6):686-693."
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "[3] ZHANG H J,WANG H C. Research on classification and recognition of multi lead EEG signals [J]. Computer Engineering and"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "Applications, 2008,44 (24): 228-230"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "[4] LI J Y,DU X B,ZHU Z J,et al. Deep Learning for EEG-based Emotion Recognition: A Survey.Journal of Software, 2023, 34(1):"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "255-276."
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "[5] ZHANG C,GUO Y,LI M. Review of Development and Application of Artificial Neural Network Models[J]. Computer Engineer-"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "ing and Applications,2021,57(11):57-69."
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "[6] Kumar N,Khaund K,Hazarika S M. Bispectral analysis of EEG for emotion recognition[J]. Procedia Computer Sci-"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "ence,2016,84:31-35."
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "[7] CHEN J X,ZHENG R,ZHANG P W,et al. Electroencephalogram Emotion Classification Based on Data Space Adaptation and"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "Common Spatial Pattern. Computer Engineering,2019,45(4):296-301."
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "[8] LI Y J,HUANG J J,WANG H Y,et al. Study of emotion recognition based on fusion multi-modal bio-signal with SAE and LSTM"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "recurrent neural network[J]. Journal on Communications,2017,38(12):109-120."
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "[9] Li C, Yang H H, Wu X, et al. Improving EEG-Based Motor Imagery Classification Using Hybrid Neural Network[C]. IEEE 9th"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "International Conference on Information, Communication and Networks (ICICN), 2021: 486-489"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "[10] ZHOU Y J, LI D D,WANG Z, et al. Cepstrum feature fusion for EEG emotion classification. Computer Engineering and Appli-"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "cations,2020,56(21):164-169."
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "[11] ZHOU Y J, LI D D,WANG Z, et al. Cepstrum feature fusion for EEG emotion classification. Computer Engineering and Appli-"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "cations,2020,56(21):164-169."
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "[12] QING T P, SHENG H, YUE L,et al. Survey of Research on EEG Signal Emotion Recognition[J]. Computer Engineering and"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "Applications,2023,59(15):38-54."
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "[13] Woo, Sanghyun, Jongchan Park, Joon-Young Lee, and In So Kweon. 2018.  “Cbam: Convolutional Block Attention Mod-"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "ule.”  In Proceedings of the European Conference on Computer Vision (ECCV), 3–19."
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "[14] K. He, X. Zhang, S. Ren and J. Sun, \"Deep residual learning for image recognition\", Proc. IEEE Conf. Comput. Vis. Pattern"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "Recognit., pp. 770-778, 2016"
        },
        {
          "[1] WANG H Y ,HU J F,WANG Y L. A review of EEG signal processing methods[J]. Computer Era,2018(1):13 -15,19.": "[15]GUO Y X,YANG W,LIU Q,et al. Survey of residual network[J]. Application Research of Computers,2020,37(5):1292 -1297."
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A review of EEG signal processing methods[J]",
      "authors": [
        "Y Wang H",
        "Wang Y L Hu J F"
      ],
      "venue": "Computer Era"
    },
    {
      "citation_id": "2",
      "title": "Review on EEG-Based Emotion Assessment[J]",
      "authors": [
        "F Jiang",
        "Y Zeng",
        "Lin Z M"
      ],
      "year": "2016",
      "venue": "Journal of Information Engineering University"
    },
    {
      "citation_id": "3",
      "title": "Research on classification and recognition of multi lead EEG signals [J]",
      "authors": [
        "J Zhang H"
      ],
      "year": "2008",
      "venue": "Computer Engineering and Applications"
    },
    {
      "citation_id": "4",
      "title": "Deep Learning for EEG-based Emotion Recognition: A Survey",
      "authors": [
        "Y Li",
        "Zhu Z J Du X B"
      ],
      "year": "2023",
      "venue": "Journal of Software"
    },
    {
      "citation_id": "5",
      "title": "Review of Development and Application of Artificial Neural Network Models[J]",
      "authors": [
        "C Zhang",
        "Y Guo"
      ],
      "year": "2021",
      "venue": "Computer Engineering and Applications"
    },
    {
      "citation_id": "6",
      "title": "Bispectral analysis of EEG for emotion recognition[J]",
      "authors": [
        "N Kumar",
        "K Khaund",
        "S Hazarika"
      ],
      "year": "2016",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "7",
      "title": "Electroencephalogram Emotion Classification Based on Data Space Adaptation and Common Spatial Pattern",
      "authors": [
        "X Chen",
        "R Zheng",
        "Zhang P W"
      ],
      "year": "2019",
      "venue": "Computer Engineering"
    },
    {
      "citation_id": "8",
      "title": "Study of emotion recognition based on fusion multi-modal bio-signal with SAE and LSTM recurrent neural network[J]",
      "authors": [
        "J Li Y",
        "Wang H Y Huang J J"
      ],
      "year": "2017",
      "venue": "Journal on Communications"
    },
    {
      "citation_id": "9",
      "title": "Improving EEG-Based Motor Imagery Classification Using Hybrid Neural Network[C]",
      "authors": [
        "C Li",
        "H H Yang",
        "X Wu"
      ],
      "venue": "IEEE 9th International Conference on Information, Communication and Networks (ICICN)"
    },
    {
      "citation_id": "10",
      "title": "Cepstrum feature fusion for EEG emotion classification",
      "authors": [
        "J Zhou Y"
      ],
      "year": "2020",
      "venue": "Computer Engineering and Applications"
    },
    {
      "citation_id": "11",
      "title": "Cepstrum feature fusion for EEG emotion classification",
      "authors": [
        "J Zhou Y"
      ],
      "year": "2020",
      "venue": "Computer Engineering and Applications"
    },
    {
      "citation_id": "12",
      "title": "Survey of Research on EEG Signal Emotion Recognition[J]",
      "authors": [
        "Qing T P",
        "H Sheng",
        "L Yue"
      ],
      "year": "2023",
      "venue": "Computer Engineering and Applications"
    },
    {
      "citation_id": "13",
      "title": "Cbam: Convolutional Block Attention Module",
      "authors": [
        "Sanghyun Woo",
        "Jongchan Park"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "14",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "15",
      "title": "Survey of residual network",
      "authors": [
        "Yang Guo Y X",
        "Liu Q"
      ],
      "year": "2020",
      "venue": "Application Research of Computers"
    }
  ]
}