{
  "paper_id": "2303.11421v1",
  "title": "Improving Eeg-Based Emotion Recognition By Fusing Time-Frequency And Spatial Representations",
  "published": "2023-03-14T07:26:51Z",
  "authors": [
    "Kexin Zhu",
    "Xulong Zhang",
    "Jianzong Wang",
    "Ning Cheng",
    "Jing Xiao"
  ],
  "keywords": [
    "electroencephalogram",
    "graph convolution network",
    "feature fusion",
    "emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Using deep learning methods to classify EEG signals can accurately identify people's emotions. However, existing studies have rarely considered the application of the information in another domain's representations to feature selection in the time-frequency domain. We propose a classification network of EEG signals based on the cross-domain feature fusion method, which makes the network more focused on the features most related to brain activities and thinking changes by using the multi-domain attention mechanism. In addition, we propose a two-step fusion method and apply these methods to the EEG emotion recognition network. Experimental results show that our proposed network, which combines multiple representations in the time-frequency domain and spatial domain, outperforms previous methods on public datasets and achieves state-of-the-art at present.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Compared to the natural world, we humans know very little about ourselves. A person can interact with other people, machines, or nature in many ways  [1, 2] , such as sight, hearing, speech, gestures, writing, etc. As the research progresses, especially in the field of brain science, we have the opportunity to bypass these cumbersome interaction methods and interact directly through the electrical signals of the brain.\n\nElectroencephalogram (EEG) signal is a spontaneous electrical signal generated by the conscious activity of the human brain  [3] , and the information extracted from EEG signal can be used as an important indicator to study the conscious state of the human brain . For example, classifying EEG signals according to certain rules can accurately identify human emotions, or control the activities of mechanical prostheses. Compared with other external manifestations such as expressions, gestures, language, etc., extracting raw information from spontaneous EEG signals is of great significance because one cannot control or mask these spontaneously generated EEG signals  [4] . In addition, it is extremely difficult ‡These authors contributed equally to this work.\n\n* Corresponding author: Jianzong Wang, jzwang@188.com.\n\nfor people with language barriers or physical disabilities to recognize information from sounds, gestures, etc. It can be said that EEG is one of the most suitable means to extract information about human conscious activities.\n\nMedical research has proved that human brain activities are mainly reflected in the changes of several frequency bands of EEG signals (i.e. features of frequency domain), and these changes are related in both time series and brain region  [5]  , which are reflected in the context correlation of signal changes and correlation between electrodes at different locations. In recent years, many EEG classification models based on temporal, frequency, and spatial features have been proposed. Hou and Jia et al.  [6]  applied long short-term memory (LSTM) to extract features and used graph convolution networks (GCN) to model topological structure. He et al.  [7]  applied channel attention to multi-layer perceptron to adaptively learn the importance of each channel. Specific to the task of EEG emotion recognition, many researchers have also proposed some approaches. Yin et al.  [8]  used GCN to extract the relationship between channels as spatial features and use LSTM to memorize the relationship changes. He and Zhong et al.  [9]  proposed a model that combines temporal convolution networks with adversarial discriminative domain adaptation to solve the problem of domain drift in the crosssubject emotion recognition task. Some of the existing works focus on the representations of different domains, lacking the mapping process of features between representations, and some fusion methods are difficult to combine different levels of feature information to comprehensively model EEG signals.\n\nIn this paper, we propose a multi-domain feature fusion model for emotion recognition based on EEG, which fuses features from multiple domains such as temporal, frequency, and spatial representations. In particular, we propose a twostep fusion method to improve the multi-domain feature fusion process, to better preserve the respective information of each domain. Different from other methods in the same task, we believe that the application of the multi-domain fusion method can introduce the connection between each channel obtained by the graph neural network to select features in the time-frequency feature representations, and better help the model to extract correlations in time-frequency features. We conducted rigorous experiments, including the comparison Fig.  1 . Overall architecture. Our proposed model takes multi-channel EEG time series data as input and outputs binary prediction results. During the two-step fusion process, the CDA block completes the first fusion, and then the feature vectors output by each block are concatenated to complete the second fusion.\n\nwith other existing methods, the selection of graph encoder, and the ablation experiment of each block in the network. According to experimental results, our proposed method has achieved the highest accuracy in user-independent emotion recognition.\n\nOur main contributions are listed below:\n\n1) We proposed a method for EEG multi-domain feature fusion using cross-domain attention, which utilizes information from spatial representations to assist in the selection of time-frequency features. 2) Based on applying the multi-domain method for feature fusion, we proposed a two-step fusion method to preserve more feature information in time-frequency and spatial feature vectors. 3) We applied described method in the proposed emotion recognition network. We performed experiments and the results show that our proposed method has a better effect on EEG-based emotion recognition and achieves the best performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "The overview is illustrated in Figure  1 . Our network is mainly composed of three parts, namely, a time-domain embedding & encoding block (TDEE block), a spatial-domain embedding & encoding block (SDEE block), and a cross-domain attention block (CDA block). The processing flow of the model is as follows: after pre-processing, EEG signals are sent to the TDEE block and SDEE block to construct time-frequency domain and spatial domain features. CDA block fuses these features in the first step, and the outputs of our proposed three blocks are concatenated to complete the second step of fusion. Finally, multiple linear layers complete the classification task of emotion recognition. During the training process, the parameters in each block are updated by back-propagation.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Tdee Block",
      "text": "Our proposed TDEE block has a similar structure to CRNN  [10] .\n\nIt can preliminarily extract channel correlation information by convolution operation between channels and encode the context relationship by LSTM layers.\n\nThe block contains three one-dimensional convolution layers and several LSTM layers. The first two convolutional layers are followed by a ReLU, and the last layer is followed by a batch normalization layer. We improved the original CRNN and added an LSTM layer after the Bidirectional LSTM layer to enhance the ability of the network to extract temporal context information.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Sdee Block",
      "text": "Our proposed SDEE block contains an embedding layer, a graph encoder, and a series of processing steps for constructing graphs. The multi-channel EEG signal is constructed as a graph structure with each channel as nodes, and the adjacency matrix A and degree matrix D are obtained by using K nearest neighbors (KNN) to calculate the connection relationship between nodes . The Laplacian matrix L is calculated by L = D -A and used in the following graph encoder.\n\nMany graph encoders have been proposed in previous studies, including  GCN [11]  and GAT  [12] . In each layer of GCN, nodes of the graph acquire information through the adjacent or further nodes. Suppose i is a graph node, N i is the set of all neighboring nodes of node i, and l is the current GCN layer. The feature of l + 1 layer nodes can be calculated as follows:\n\nwhere j is one of the neighboring nodes of node i, d i and d j are the degrees of nodes i and j respectively. h l i is feature vector of node i in l layer. GAT improves the normalization constant in the GCN layer into the neighbor node feature aggregation function using attention weight:  4 ) and (  5 ) respectively calculate the attention weight of nodes and the attention between pairs of nodes. We have applied different graph encoders for comparative experiments to determine the best graph encoding approach.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cda Block",
      "text": "In the CDA block, we introduce multi-head cross-domain attention to applying the graph representation information of channels to feature selection, so that the network can focus on the features most related to human emotions. Cross-domain attention is inspired by cross-modal attention  [13, 14]  , it is also a mapping relationship similar to self-attention  [15] , but query and key-value pairs come from different domain representations respectively.\n\nLet α represents the domain of the SDEE block output feature vector (i.e. spacial domain), and β represents the domain of the TDEE block output feature vector (i.e. timefrequency domain). The query Q α of each attention head can be obtained by linear mapping:\n\nwhere X α is the input with domain α, and W Q is a trainable parameter matrix. Key K and value V are calculated in a similar way to the query:\n\nexcept that their input comes from domain β. Each attention head is calculated separately:\n\nwhere (•) T is matrix transposition, d represents the dimension of K. The attention of each head is concatenated and multiplied by the weight matrix to get multi-head attention:\n\nM ultiHead(X α , X β ) = Concat(head 1 , ..., head H )W O (12) where 1 ≤ i ≤ H, hyperparameter H is the number of attention heads, W O is a parameter matrix. These trainable weight matrices enhance the fitting ability of the proposed model. Multi-head attention provides different representation subspaces to learn different features, while input data from different domains provide different features, which are fused by the attention mechanism.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Two-Step Feature Fusion",
      "text": "By applying cross-domain attention, our proposed model can introduce spatial topological information extracted from graph networks and pay more attention to the channels (i.e. features output by TDEE block) most related to brain emotional changes. However, the information contained in the feature vectors obtained by the graph encoder and the time series encoder is more than that. For example, the change of spatial feature vector or time-frequency feature vector itself can reflect the change in brain electrical signal when a certain emotion is generated to a certain extent.\n\nWe propose a two-step feature fusion strategy to verify this hypothesis: in the first step, the feature vectors output by the SDEE block and TDEE block are fused in the CDA block and transformed into fusion vectors, as described in the previous subsection. In the second step, the fused vector and the feature vector before fusion are concatenated and then sent to the dense layers to complete emotion classification:\n\nwhere X F C is the input of the classifier, X α and X β are features extracted by the SDEE block and TDEE block respectively, and X CM is the cross-domain fusion feature calculated by CDA block in the first step.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Preparations",
      "text": "We use the open source dataset DEAP  [16]  to evaluate our proposed model. DEAP dataset consists of EEG data of 32 participants, including equal numbers of male and female subjects. Each participant watched 40 videos of about one minute and EEG signals of the participants were recorded during the watching process. After watching, each participant scored Valence, Arousal, and other measures of the videos. The collected raw EEG data has been subjected to many pre-processing steps, including down-sampling and eliminating the electrooculogram. Before the experiment, we used some other pre-processing methods for the DEAP dataset. We divide the time-series EEG signals into time slices by using a sliding window,",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Settings",
      "text": "Many experiments are conducted to determine the hyperparameters that can make our proposed model work best. Adam with a learning rate of 0.001 is used as the optimizer and cross-entropy is used as the loss function. The number of attention heads H is set to 8. The sliding window has a width of 2 seconds and moves in steps of 0.125 seconds.\n\nTo confirm our experimental results, we used leave-onesubject-out (LOSO) cross-validation method. In each validation, one subject's data is not involved in the training process but used as test data, thus realizing user-independent emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "We conducted three main tasks to evaluate the performance of our proposed model. Comparative experiments are performed to verify the improvement of classification accuracy of our proposed model compared with other approaches. GCN and GAT are applied for performance comparison, to determine the best graph encoding approach. The ablation experiments of several blocks are performed to verify the functions of each block and the feature fusion approach in our proposed model.\n\nIn the first two experiments, we compared the results of our proposed method with previous works. Table  1  shows the experimental results of existing methods and our proposed methods, in which Ours (with GCN) and Ours (with GAT) represent the experimental results when GCN or GAT are used as graph encoders, respectively. Experimental results have proved our proposed method achieves the best in the classification task of emotion recognition and surpasses previous works. The accuracy is close when GCN and GAT are used as graph encoders, but GAT has higher computational In the third experiment shown in Table  2 , we have done the ablation experiment for each block in the proposed method and respectively experimented with one-step and two-step fusion methods. Results show that our proposed multi-domain feature fusion method can effectively improve the classification accuracy, both in Valence and Arousal dimensions, and the effect of two-step fusion is better than that of one-step fusion.\n\nIt is worth noting that the channels that the CDA block makes the model focus on do not refer to the specific original EEG input channels, because these channels have undergone one-dimensional convolution in the TDEE block and generated new feature representations, which contain the preliminary relationship between channels and information of different brain regions. Besides, our proposed method can be used not only for emotion recognition tasks but also for other tasks related to EEG signals, as other tasks may use similar EEG bands and features.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We proposed a multi-domain feature fusion model for EEGbased emotion recognition. Specifically, we use a crossdomain feature fusion method to combine the spatial domain information to make the model focus on the time-frequency domain features most related to our task, and further combine these features by using a two-step fusion method. Experiments show the effectiveness of the proposed model. We will explore the feasibility of applying federated learning methods to deploy the model on distributed devices and use it to help more people.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgement",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Supported By The Key Research And Development Program Of Guangdong Province (Grant No. 2021B0101400003) And",
      "text": "Corresponding author is Jianzong Wang (jzwang@188.com).",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall architecture. Our proposed model takes multi-channel EEG time series data as input and outputs binary",
      "page": 2
    },
    {
      "caption": "Figure 1: Our network is mainly",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "for people with language barriers or physical disabilities to"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "recognize information from sounds, gestures, etc.\nIt can be"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "said that EEG is one of\nthe most suitable means to extract"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "information about human conscious activities."
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "Medical\nresearch has proved that human brain activi-"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "ties are mainly reﬂected in the changes of several frequency"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "bands of EEG signals\n(i.e.\nfeatures of\nfrequency domain),"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "and these changes are related in both time series and brain"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "region\n[5] , which are reﬂected in the context correlation of"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "signal changes and correlation between electrodes at different"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "locations.\nIn recent years, many EEG classiﬁcation models"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "based on temporal, frequency, and spatial features have been"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "proposed.\nHou and Jia\net\nal.\n[6]\napplied long short-term"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "memory (LSTM) to extract features and used graph convolu-"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "tion networks (GCN)\nto model\ntopological structure. He et"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "al. [7] applied channel attention to multi-layer perceptron to"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "adaptively learn the importance of each channel. Speciﬁc to"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "the task of EEG emotion recognition, many researchers have"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "also proposed some approaches. Yin et al. [8] used GCN to"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "extract\nthe relationship between channels as spatial\nfeatures"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "and use LSTM to memorize the relationship changes. He and"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "Zhong et al.\n[9] proposed a model\nthat combines temporal"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "convolution networks with adversarial discriminative domain"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "adaptation to solve the problem of domain drift\nin the cross-"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "subject emotion recognition task. Some of the existing works"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "focus on the\nrepresentations of different domains,\nlacking"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "the mapping process of features between representations, and"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "some fusion methods are difﬁcult\nto combine different\nlev-"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "els of\nfeature information to comprehensively model EEG"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "signals."
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "In this paper, we propose a multi-domain feature fusion"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "model\nfor emotion recognition based on EEG, which fuses"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "features from multiple domains such as temporal, frequency,"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "and spatial representations.\nIn particular, we propose a two-"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "step fusion method to improve the multi-domain feature fu-"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "sion process,\nto better preserve the respective information of"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "each domain. Different from other methods in the same task,"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "we believe that\nthe application of\nthe multi-domain fusion"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "method can introduce the connection between each channel"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "obtained by the graph neural network to select\nfeatures\nin"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "the time-frequency feature representations, and better help the"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "model\nto extract correlations in time-frequency features. We"
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "Ping An Technology (Shenzhen) Co., Ltd., China": "conducted rigorous experiments,\nincluding the comparison"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "prediction results. During the two-step fusion process,": "output by each block are concatenated to complete the second fusion.",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "with other existing methods,\nthe selection of graph encoder,",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "2.1. TDEE Block"
        },
        {
          "prediction results. During the two-step fusion process,": "and the ablation experiment of each block in the network.",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "Our proposed TDEE block has a similar structure to CRNN [10]."
        },
        {
          "prediction results. During the two-step fusion process,": "According to experimental results, our proposed method has",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "It can preliminarily extract channel correlation information"
        },
        {
          "prediction results. During the two-step fusion process,": "achieved the highest accuracy in user-independent emotion",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "by convolution operation between channels and encode the"
        },
        {
          "prediction results. During the two-step fusion process,": "recognition.",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "context relationship by LSTM layers."
        },
        {
          "prediction results. During the two-step fusion process,": "Our main contributions are listed below:",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "The block contains\nthree one-dimensional\nconvolution"
        },
        {
          "prediction results. During the two-step fusion process,": "1) We proposed a method for EEG multi-domain feature",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "layers and several LSTM layers. The ﬁrst\ntwo convolutional"
        },
        {
          "prediction results. During the two-step fusion process,": "fusion using cross-domain attention, which utilizes in-",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "layers are followed by a ReLU, and the last layer is followed"
        },
        {
          "prediction results. During the two-step fusion process,": "formation from spatial\nrepresentations to assist\nin the",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "by a batch normalization layer. We improved the original"
        },
        {
          "prediction results. During the two-step fusion process,": "selection of time-frequency features.",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "CRNN and added an LSTM layer\nafter\nthe Bidirectional"
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "LSTM layer to enhance the ability of the network to extract"
        },
        {
          "prediction results. During the two-step fusion process,": "2) Based on applying the multi-domain method for feature",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "temporal context information."
        },
        {
          "prediction results. During the two-step fusion process,": "fusion, we proposed a two-step fusion method to pre-",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "serve more feature information in time-frequency and",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "spatial feature vectors.",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "2.2.\nSDEE Block"
        },
        {
          "prediction results. During the two-step fusion process,": "3) We applied described method in the proposed emotion",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "Our proposed SDEE block contains an embedding layer, a"
        },
        {
          "prediction results. During the two-step fusion process,": "recognition network. We performed experiments and",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "graph encoder, and a series of processing steps for construct-"
        },
        {
          "prediction results. During the two-step fusion process,": "the results show that our proposed method has a better",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "ing graphs. The multi-channel EEG signal\nis constructed as"
        },
        {
          "prediction results. During the two-step fusion process,": "effect on EEG-based emotion recognition and achieves",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "a graph structure with each channel as nodes, and the adja-"
        },
        {
          "prediction results. During the two-step fusion process,": "the best performance.",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "cency matrix A and degree matrix D are obtained by using K"
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "nearest neighbors (KNN) to calculate the connection relation-"
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "ship between nodes . The Laplacian matrix L is calculated by"
        },
        {
          "prediction results. During the two-step fusion process,": "2. METHOD",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "L = D − A and used in the following graph encoder."
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "Many graph encoders have been proposed in previous"
        },
        {
          "prediction results. During the two-step fusion process,": "The overview is illustrated in Figure 1. Our network is mainly",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "studies,\nincluding GCN [11] and GAT [12].\nIn each layer"
        },
        {
          "prediction results. During the two-step fusion process,": "composed of three parts, namely, a time-domain embedding",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "of GCN, nodes of the graph acquire information through the"
        },
        {
          "prediction results. During the two-step fusion process,": "& encoding block (TDEE block), a spatial-domain embed-",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "is\nadjacent or further nodes. Suppose i is a graph node, Ni"
        },
        {
          "prediction results. During the two-step fusion process,": "ding & encoding block (SDEE block), and a cross-domain at-",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "the set of all neighboring nodes of node i, and l is the current"
        },
        {
          "prediction results. During the two-step fusion process,": "tention block (CDA block). The processing ﬂow of the model",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "GCN layer. The feature of l + 1 layer nodes can be calculated"
        },
        {
          "prediction results. During the two-step fusion process,": "is as follows:\nafter pre-processing, EEG signals are sent\nto",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "as follows:"
        },
        {
          "prediction results. During the two-step fusion process,": "the TDEE block and SDEE block to construct time-frequency",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": ""
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "iW l"
        },
        {
          "prediction results. During the two-step fusion process,": "",
          "the CDA block completes the ﬁrst fusion, and then the feature vectors": "hl+1\n= σ(\n)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "constant in the GCN layer into the neighbor node feature ag-",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "(12)"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "gregation function using attention weight:",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "where 1 ≤ i ≤ H, hyperparameter H is the number of at-"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "These trainable\ntention heads, WO is a parameter matrix."
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "(cid:88) j\nhl+1\n= σ(\nαl\n(3)\nijzl\nj)",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "i",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "weight matrices enhance the ﬁtting ability of\nthe proposed"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "model. Multi-head attention provides different representation"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "exp(el",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "subspaces to learn different\nfeatures, while input data from"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "ij)",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "αl\n(4)\nij =",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "(cid:80)",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "different domains provide different features, which are fused"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "k exp(el\nik)",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "by the attention mechanism."
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "el\n(5)\nij = LeakyReLU (((cid:126)al)T (zl\ni||zl\nj))",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "zl\n(6)\ni = hl\niW l",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "2.4. Two-step Feature Fusion"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "W l is a trainable parameter matrix, k ∈ Ni. Formulas (4) and",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "By\napplying\ncross-domain\nattention,\nour\nproposed model"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "(5)\nrespectively calculate the attention weight of nodes and",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "can introduce spatial\ntopological\ninformation extracted from"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "the attention between pairs of nodes. We have applied differ-",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "graph networks and pay more attention to the channels (i.e."
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "ent graph encoders for comparative experiments to determine",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "features output by TDEE block) most\nrelated to brain emo-"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "the best graph encoding approach.",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "tional changes.\nHowever,\nthe information contained in the"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "feature vectors obtained by the graph encoder and the time"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "2.3. CDA Block",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "series encoder is more than that. For example,\nthe change of"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "spatial\nfeature vector or\ntime-frequency feature vector\nitself"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "In the CDA block, we introduce multi-head cross-domain at-",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "can reﬂect the change in brain electrical signal when a certain"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "tention to applying the graph representation information of",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "emotion is generated to a certain extent."
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "channels to feature selection, so that the network can focus on",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "We propose a two-step feature fusion strategy to verify"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "the features most related to human emotions. Cross-domain",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "this hypothesis:\nin the ﬁrst step,\nthe feature vectors output"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "attention is inspired by cross-modal attention [13, 14]\n,\nit\nis",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "by the SDEE block and TDEE block are fused in the CDA"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "also a mapping relationship similar to self-attention [15], but",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "block and transformed into fusion vectors, as described in the"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "query and key-value pairs come from different domain repre-",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "previous subsection.\nIn the second step,\nthe fused vector and"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "sentations respectively.",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "the feature vector before fusion are concatenated and then sent"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "Let α represents the domain of\nthe SDEE block output",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": "to the dense layers to complete emotion classiﬁcation:"
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "feature vector\n(i.e.\nspacial domain),\nand β represents\nthe",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        },
        {
          "vector of node i in l\nlayer. GAT improves the normalization": "domain of the TDEE block output feature vector (i.e.\ntime-",
          "M ultiHead(Xα, Xβ) = Concat(head1, ..., headH )WO": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Ablation study of blocks in proposed method and",
      "data": [
        {
          "Table 1. Comparison between our proposed method and other": "methods, where T-F represents the time-frequency feature.",
          "Table 2. Ablation study of blocks in proposed method and": "comparison experiment of different fusion methods."
        },
        {
          "Table 1. Comparison between our proposed method and other": "Accuracy",
          "Table 2. Ablation study of blocks in proposed method and": "Accuracy"
        },
        {
          "Table 1. Comparison between our proposed method and other": "Study\nFeature(s)",
          "Table 2. Ablation study of blocks in proposed method and": "SDEE\nTDEE\nCDA\nFusion"
        },
        {
          "Table 1. Comparison between our proposed method and other": "Valence\nArousal",
          "Table 2. Ablation study of blocks in proposed method and": "Valence\nArousal"
        },
        {
          "Table 1. Comparison between our proposed method and other": "",
          "Table 2. Ablation study of blocks in proposed method and": "(cid:88)"
        },
        {
          "Table 1. Comparison between our proposed method and other": "Li et al. [17]\nT-F\n0.691\n0.710",
          "Table 2. Ablation study of blocks in proposed method and": "-\n0.530\n0.512"
        },
        {
          "Table 1. Comparison between our proposed method and other": "",
          "Table 2. Ablation study of blocks in proposed method and": "(cid:88)"
        },
        {
          "Table 1. Comparison between our proposed method and other": "Wang et al. [5]\nSFM\n0.712\n0.713",
          "Table 2. Ablation study of blocks in proposed method and": "-\n0.834\n0.840"
        },
        {
          "Table 1. Comparison between our proposed method and other": "",
          "Table 2. Ablation study of blocks in proposed method and": "(cid:88)\n(cid:88)"
        },
        {
          "Table 1. Comparison between our proposed method and other": "Atkinson et al. [18]\nmRMR\n0.731\n0.730",
          "Table 2. Ablation study of blocks in proposed method and": "Concat\n0.849\n0.864"
        },
        {
          "Table 1. Comparison between our proposed method and other": "Guo et al. [19]\nT-F, FuzzyEn\n0.844\n0.856",
          "Table 2. Ablation study of blocks in proposed method and": ""
        },
        {
          "Table 1. Comparison between our proposed method and other": "",
          "Table 2. Ablation study of blocks in proposed method and": "(cid:88)\n(cid:88)\n(cid:88)"
        },
        {
          "Table 1. Comparison between our proposed method and other": "",
          "Table 2. Ablation study of blocks in proposed method and": "One-step\n0.855\n0.867"
        },
        {
          "Table 1. Comparison between our proposed method and other": "",
          "Table 2. Ablation study of blocks in proposed method and": "(cid:88)\n(cid:88)\n(cid:88)"
        },
        {
          "Table 1. Comparison between our proposed method and other": "Ours (with GAT)\nT-F, Graph\n0.859\n0.878",
          "Table 2. Ablation study of blocks in proposed method and": "Two-step\n0.861\n0.884"
        },
        {
          "Table 1. Comparison between our proposed method and other": "Ours (with GCN)\nT-F, Graph\n0.861\n0.884",
          "Table 2. Ablation study of blocks in proposed method and": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "tion,”\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 39,"
        },
        {
          "6. REFERENCES": "[1] Xulong Zhang, Jianzong Wang, Ning Cheng, and Jing",
          "recognition and its application to scene text\nrecogni-": "no. 11, pp. 2298–2304, 2017."
        },
        {
          "6. REFERENCES": "Xiao,\n“Metasid:\nSinger\nidentiﬁcation with domain",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "[11] Andac Demir, Toshiaki Koike-Akino, Ye Wang, Masaki"
        },
        {
          "6. REFERENCES": "adaptation for metaverse,”\nin International Joint Con-",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "Haruna, and Deniz Erdogmus, “EEG-GNN: graph neu-"
        },
        {
          "6. REFERENCES": "ference on Neural Networks. IEEE, 2022, pp. 1–7.",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "ral networks for classiﬁcation of electroencephalogram"
        },
        {
          "6. REFERENCES": "[2] Wei Duan, Yi Yu, Xulong Zhang, Suhua Tang, Wei Li,",
          "recognition and its application to scene text\nrecogni-": "(EEG) signals,”\nin Annual International Conference of"
        },
        {
          "6. REFERENCES": "and Keizo Oyama, “Melody generation from lyrics with",
          "recognition and its application to scene text\nrecogni-": "the IEEE Engineering in Medicine & Biology Society,"
        },
        {
          "6. REFERENCES": "local interpretability,” ACM Transactions on Multimedia",
          "recognition and its application to scene text\nrecogni-": "2021, pp. 1061–1067."
        },
        {
          "6. REFERENCES": "Computing, Communications and Applications, vol. 19,",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "[12] Petar Velickovic, Guillem Cucurull, Arantxa Casanova,"
        },
        {
          "6. REFERENCES": "no. 3, pp. 1–21, 2023.",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "Adriana Romero,\nPietro\nLi`o,\nand Yoshua Bengio,"
        },
        {
          "6. REFERENCES": "[3] Pooja, S. K. Pahuja,\nand Karan Veer,\n“Recent\nap-",
          "recognition and its application to scene text\nrecogni-": "“Graph attention networks,”\nin International Confer-"
        },
        {
          "6. REFERENCES": "proaches on classiﬁcation and feature extraction of EEG",
          "recognition and its application to scene text\nrecogni-": "ence on Learning Representations, 2018."
        },
        {
          "6. REFERENCES": "signal: A review,” Robotica, vol. 40, no. 1, pp. 77–101,",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "[13] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,"
        },
        {
          "6. REFERENCES": "2022.",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "J. Zico Kolter, Louis-Philippe Morency,\nand Ruslan"
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "Salakhutdinov,\n“Multimodal\ntransformer for unaligned"
        },
        {
          "6. REFERENCES": "[4] Guozhen Zhao, Yan Ge, Biying Shen, Xingjie Wei, and",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "multimodal language sequences,” in Proceedings of the"
        },
        {
          "6. REFERENCES": "Hao Wang, “Emotion analysis for personality inference",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "57th Conference of\nthe Association for Computational"
        },
        {
          "6. REFERENCES": "from EEG signals,” IEEE Trans. Affect. Comput., vol. 9,",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "Linguistics, 2019, pp. 6558–6569."
        },
        {
          "6. REFERENCES": "no. 3, pp. 362–371, 2018.",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "[14]\nJiahao\nZhao,\nGanghui\nRu,\nYi\nYu,\nYulun Wu,"
        },
        {
          "6. REFERENCES": "[5] Chen Wang,\nJingzhao Hu, Ke Liu, Qiaomei\nJia,\nJi-",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "Dichucheng Li, and Wei Li,\n“Multimodal music emo-"
        },
        {
          "6. REFERENCES": "ayue Chen, Kun Yang, and Jun Feng, “Eeg-based emo-",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "tion recognition with hierarchical cross-modal attention"
        },
        {
          "6. REFERENCES": "tion recognition fusing spacial-frequency domain fea-",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "network,”\nin IEEE International Conference on Multi-"
        },
        {
          "6. REFERENCES": "tures and data-driven spectrogram-like features,”\nin In-",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "media and Expo, 2022, pp. 1–6."
        },
        {
          "6. REFERENCES": "ternational Symposium on Bioinformatics Research and",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "Application, 2021, vol. 13064, pp. 460–470.",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "[15] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob"
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "Uszkoreit,\nLlion\nJones,\nAidan N. Gomez,\nLukasz"
        },
        {
          "6. REFERENCES": "[6] Yimin Hou, Shuyue Jia, Shu Zhang, Xiangmin Lun,",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "Kaiser, and Illia Polosukhin, “Attention is all you need,”"
        },
        {
          "6. REFERENCES": "Yan Shi, Y. Li, Hanrui Yang, Rui Zeng,\nand Jinglei",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "Information Processing Sys-\nin Conference on Neural"
        },
        {
          "6. REFERENCES": "Lv,\n“Deep feature mining via attention-based bilstm-",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "tems, 2017, pp. 5998–6008."
        },
        {
          "6. REFERENCES": "gcn for human motor imagery recognition,” ArXiv, vol.",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "abs/2005.00777, 2020.",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "[16] Sander Koelstra,\nChristian M¨uhl, Mohammad\nSo-"
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "leymani,\nJong-Seok Lee, Ashkan Yazdani,\nTouradj"
        },
        {
          "6. REFERENCES": "[7] Yanbin He, Zhiyang Lu,\nJun Wang, and Jun Shi,\n“A",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Pa-"
        },
        {
          "6. REFERENCES": "channel attention based mlp-mixer network for motor",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "tras,\n“DEAP: A database for emotion analysis ;using"
        },
        {
          "6. REFERENCES": "imagery decoding with EEG,”\nin IEEE International",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "physiological\nsignals,”\nIEEE Trans. Affect. Comput.,"
        },
        {
          "6. REFERENCES": "Conference on Acoustics, Speech and Signal Process-",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "vol. 3, no. 1, pp. 18–31, 2012."
        },
        {
          "6. REFERENCES": "ing, 2022, pp. 1291–1295.",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "[17] Chao Li, Boyang Chen, Ziping Zhao, Nicholas Cum-"
        },
        {
          "6. REFERENCES": "[8] Yongqiang Yin,\nXiangwei\nZheng,\nBin Hu,\nYuang",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "mins, and Bj¨orn W. Schuller,\n“Hierarchical attention-"
        },
        {
          "6. REFERENCES": "Zhang, and Xinchun Cui,\n“EEG emotion recognition",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "based temporal\nconvolutional networks\nfor\neeg-based"
        },
        {
          "6. REFERENCES": "using fusion model of graph convolutional neural net-",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "emotion recognition,” in IEEE International Conference"
        },
        {
          "6. REFERENCES": "works and LSTM,”\nAppl. Soft Comput., vol. 100, pp.",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "on Acoustics, Speech and Signal Processing, 2021, pp."
        },
        {
          "6. REFERENCES": "106954, 2021.",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "1240–1244."
        },
        {
          "6. REFERENCES": "[9] Zhipeng He, Yongshi Zhong, and Jiahui Pan, “Joint tem-",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "[18]\nJohn Atkinson and Daniel Campos,\n“Improving bci-"
        },
        {
          "6. REFERENCES": "poral convolutional networks and adversarial discrim-",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "based emotion recognition by combining EEG feature"
        },
        {
          "6. REFERENCES": "inative domain adaptation for eeg-based cross-subject",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "selection and kernel classiﬁers,” Expert Syst. Appl., vol."
        },
        {
          "6. REFERENCES": "emotion recognition,” in IEEE International Conference",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "47, pp. 35–41, 2016."
        },
        {
          "6. REFERENCES": "on Acoustics, Speech and Signal Processing, 2022, pp.",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "3214–3218.",
          "recognition and its application to scene text\nrecogni-": "[19] G. Guo and Y. Gao,\n“Multi feature fusion eeg emotion"
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "recognition,”\nin International Conference on Big Data"
        },
        {
          "6. REFERENCES": "[10] Baoguang Shi, Xiang Bai, and Cong Yao,\n“An end-to-",
          "recognition and its application to scene text\nrecogni-": ""
        },
        {
          "6. REFERENCES": "",
          "recognition and its application to scene text\nrecogni-": "and Information Analytics, 2021, pp. 280–284."
        },
        {
          "6. REFERENCES": "end trainable neural network for image-based sequence",
          "recognition and its application to scene text\nrecogni-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Metasid: Singer identification with domain adaptation for metaverse",
      "authors": [
        "Xulong Zhang",
        "Jianzong Wang",
        "Ning Cheng",
        "Jing Xiao"
      ],
      "year": "2022",
      "venue": "International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "3",
      "title": "Melody generation from lyrics with local interpretability",
      "authors": [
        "Wei Duan",
        "Yi Yu",
        "Xulong Zhang",
        "Suhua Tang",
        "Wei Li",
        "Keizo Oyama"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Multimedia Computing, Communications and Applications"
    },
    {
      "citation_id": "4",
      "title": "Recent approaches on classification and feature extraction of EEG signal: A review",
      "authors": [
        "S Pooja",
        "Karan Pahuja",
        "Veer"
      ],
      "year": "2022",
      "venue": "Robotica"
    },
    {
      "citation_id": "5",
      "title": "Emotion analysis for personality inference from EEG signals",
      "authors": [
        "Guozhen Zhao",
        "Yan Ge",
        "Biying Shen",
        "Xingjie Wei",
        "Hao Wang"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "6",
      "title": "Eeg-based emotion recognition fusing spacial-frequency domain features and data-driven spectrogram-like features",
      "authors": [
        "Chen Wang",
        "Jingzhao Hu",
        "Ke Liu",
        "Qiaomei Jia",
        "Jiayue Chen",
        "Kun Yang",
        "Jun Feng"
      ],
      "year": "2021",
      "venue": "in International Symposium on Bioinformatics Research and Application"
    },
    {
      "citation_id": "7",
      "title": "Deep feature mining via attention-based bilstmgcn for human motor imagery recognition",
      "authors": [
        "Yimin Hou",
        "Shuyue Jia",
        "Shu Zhang",
        "Xiangmin Lun",
        "Yan Shi",
        "Y Li",
        "Hanrui Yang",
        "Rui Zeng",
        "Jinglei Lv"
      ],
      "year": "2020",
      "venue": "ArXiv"
    },
    {
      "citation_id": "8",
      "title": "A channel attention based mlp-mixer network for motor imagery decoding with EEG",
      "authors": [
        "Yanbin He",
        "Zhiyang Lu",
        "Jun Wang",
        "Jun Shi"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "EEG emotion recognition using fusion model of graph convolutional neural networks and LSTM",
      "authors": [
        "Yongqiang Yin",
        "Xiangwei Zheng",
        "Bin Hu",
        "Yuang Zhang",
        "Xinchun Cui"
      ],
      "year": "2021",
      "venue": "Appl. Soft Comput"
    },
    {
      "citation_id": "10",
      "title": "Joint temporal convolutional networks and adversarial discriminative domain adaptation for eeg-based cross-subject emotion recognition",
      "authors": [
        "Zhipeng He",
        "Yongshi Zhong",
        "Jiahui Pan"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "An end-toend trainable neural network for image-based sequence recognition and its application to scene text recognition",
      "authors": [
        "Baoguang Shi",
        "Xiang Bai",
        "Cong Yao"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "12",
      "title": "EEG-GNN: graph neural networks for classification of electroencephalogram (EEG) signals",
      "authors": [
        "Andac Demir",
        "Toshiaki Koike-Akino",
        "Ye Wang",
        "Masaki Haruna",
        "Deniz Erdogmus"
      ],
      "year": "2021",
      "venue": "Annual International Conference of the IEEE Engineering in Medicine"
    },
    {
      "citation_id": "13",
      "title": "Graph attention networks",
      "authors": [
        "Petar Velickovic",
        "Guillem Cucurull",
        "Arantxa Casanova",
        "Adriana Romero",
        "Pietro Liò",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "14",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the"
    },
    {
      "citation_id": "15",
      "title": "Multimodal music emotion recognition with hierarchical cross-modal attention network",
      "authors": [
        "Jiahao Zhao",
        "Ganghui Ru",
        "Yi Yu",
        "Yulun Wu",
        "Dichucheng Li",
        "Wei Li"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "16",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "DEAP: A database for emotion analysis ;using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Mühl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "18",
      "title": "Hierarchical attentionbased temporal convolutional networks for eeg-based emotion recognition",
      "authors": [
        "Chao Li",
        "Boyang Chen",
        "Ziping Zhao",
        "Nicholas Cummins",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Improving bcibased emotion recognition by combining EEG feature selection and kernel classifiers",
      "authors": [
        "John Atkinson",
        "Daniel Campos"
      ],
      "year": "2016",
      "venue": "Expert Syst. Appl"
    },
    {
      "citation_id": "20",
      "title": "Multi feature fusion eeg emotion recognition",
      "authors": [
        "G Guo",
        "Y Gao"
      ],
      "year": "2021",
      "venue": "International Conference on Big Data and Information Analytics"
    }
  ]
}