{
  "paper_id": "2308.00246v1",
  "title": "Eeg-Based Cognitive Load Classification Using Feature Masked Autoencoding And Emotion Transfer Learning",
  "published": "2023-08-01T02:59:19Z",
  "authors": [
    "Dustin Pulver",
    "Prithila Angkan",
    "Paul Hungler",
    "Ali Etemad"
  ],
  "keywords": [
    "Classification Evaluation Self-Supervised Masked Autoencoding Transfer Learning Transformer Cognitive Load Emotion Affective computing",
    "Cognitive load classification",
    "Deep learning",
    "EEG",
    "Transfer learning"
  ],
  "sections": [
    {
      "section_name": "Electroencephalogram (Eeg)",
      "text": "Figure  1:  A complete overview of the pipeline used in our paper is presented.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Abstract",
      "text": "Cognitive load, the amount of mental effort required for task completion, plays an important role in performance and decision-making outcomes, making its classification and analysis essential in various sensitive domains. In this paper, we present a new solution for the classification of cognitive load using electroencephalogram (EEG). Our model uses a transformer architecture employing transfer learning between emotions and cognitive load. We pre-train our model using self-supervised masked autoencoding on emotionrelated EEG datasets and use transfer learning with both frozen weights and fine-tuning to perform downstream cognitive load classification. To evaluate our method, we carry out a series of experiments utilizing two publicly available EEG-based emotion datasets, namely SEED and SEED-IV, for pre-training, while we use the CL-Drive dataset for downstream cognitive load classification. The results of our experiments show that our proposed approach achieves strong results and outperforms conventional single-stage fully supervised learning. Moreover, we perform detailed ablation and sensitivity studies to evaluate the impact of different aspects of our proposed solution. This research contributes to the growing body of literature in affective computing with a focus on cognitive load, and opens up new avenues for future research in the field of cross-domain transfer learning using self-supervised pre-training.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Cognitive load refers to the amount of mental effort required to process information and complete a task. It is a crucial concept in educational psychology  [34] , human-computer interaction  [21] , healthcare  [16] , as well as other domains concerned with learning and information processing. The concept of cognitive load has been used to explain how people learn, perform tasks, and interact with technology. The level of cognitive load experienced by an individual performing a task can significantly influence their success or failure in performance  [10] . This makes the ability to automatically classify cognitive load extremely important in a variety of sensitive domains. For instance, in healthcare, the cognitive load of medical professionals can affect their decision-making and patient outcomes  [14] . In aviation, pilots' cognitive load can affect their ability to respond to unexpected events and make critical decisions  [23] . In the context of driving, prior research has investigated the relationship between cognitive load and driving performance, suggesting that an excessive amount of cognitive load may elevate the likelihood of road accidents  [15] .\n\nMany approaches have been presented to determine one's cognitive load including the use of speech  [20] , vision  [5] , and bio-signals such as EEG  [3] . This paper further explores the use of EEG to detect cognitive load. EEG is a non-invasive technique used to record the electrical activity of the brain with high temporal resolution. These signals are generated by the synchronous firing of groups of neurons and are recorded by placing electrodes on the scalp which detect the voltage fluctuations resulting from neural activity  [17] . The resultant signal encompasses a succession of waves exhibiting diverse frequencies and amplitudes, which are indicative of distinct cognitive and physiological states. Numerous deep learning techniques have been employed for EEG analysis, resulting in significant advancements in the field  [45] [46] [47] .\n\nThe investigation of cognitive load remains a relatively unexplored area of research but has recently gained momentum, particularly with the development of comprehensive cognitive load datasets in different contexts such as detecting passenger cognitive states  [2] . The detection and quantification of cognitive load among vehicle passengers could enhance their safety, comfort, and overall experience. In order to effectively learn EEG data, both spatial and temporal relationships need to be captured. Transformers  [36]  have recently emerged as a powerful tool, capable of learning both spatial and temporal information, and have shown impressive performances in a variety of different domains, such as natural language processing  [11] , computer vision  [12] , and healthcare  [18] . Transformers, however, require large amounts of data for effective training, making them difficult to use for applications such as EEG-based cognitive load analysis, for which few datasets exist.\n\nTo tackle the problem mentioned above we propose a transformer architecture for classifying cognitive load based on EEG signal. In order to overcome the data scarcity issue, we explore transfer learning between EEG-based emotion datasets (namely SEED  [13, 49]  and SEED-IV  [48] ) and a cognitive load dataset (namely CL-Drive  [2] ). We pre-train the transformer model using self-supervised masked autoencoding on the emotion datasets, following freezing of the transformer blocks, we transfer the model for downstream cognitive load classification on CL-Drive. In this stage, we keep the transformer blocks frozen, and only train the classification head. Our results demonstrate strong performance in comparison to fully supervised training directly on the downstream cognitive load dataset, demonstrating the important potential for emotion datasets to be used for cognitive load pre-training.\n\nThe contributions of this paper are summarized as follows:\n\n‚Ä¢ For the first time, we perform EEG-based cognitive load classification using masked autoencoding of features and a transformer architecture, representing a previously unexplored approach in this domain. ‚Ä¢ Our method uses transfer learning between emotions and cognitive load using EEG signals. We pre-train our model using self-supervised masked autoencoding on emotion-related EEG datasets and use transfer learning with frozen weights to perform downstream cognitive load classification. ‚Ä¢ Our method achieves strong results on cognitive load classification. Moreover, our results suggest that transfer learning between emotions and cognitive load is indeed a viable path for cognitive load analysis with deep learning given that our approach outperforms the conventional single-stage fully supervised learning.\n\nThe rest of this paper is organized as follows. In Section 2, we present a summary of EEG learning with transformer architectures and discuss the application of self-supervised pre-training to various bio-signals, including EEG. The proposed methodology is outlined in Section 3, which is illustrated in Figure  1 . We then describe the experimental setup, including the datasets, evaluation protocol, and implementation details, in Section 4. In Section 5, we present the experimental results and report the conducted ablation studies and sensitivity analysis. Finally, we conclude the paper and suggest future works in Section 6.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "In this section we first explore the use of transformer architectures for learning EEG signals, followed by an investigation into the promising technique of self-supervised pre-training and their use with bio-signals. Furthermore, we provide a more in-depth examination of cross-domain transfer learning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Eeg Learning With Transformers",
      "text": "Since their introduction in  [36] , transformers have outperformed many state-of-the-art models on a variety of different sequence modeling tasks, including machine translation  [36, 37] , language modeling  [6, 11, 29, 36] , and image captioning  [1, 40] . The attention mechanism in transformers allows the model to attend to different parts of the input sequence, capturing long-range dependencies and improving performance. Recently, transformers have been adopted for use in the domain of bio-signals  [4] , including for analysis of EEG data. In  [35] , an approach was proposed for EEG-based emotion recognition using channel-wise attention and self-attention mechanisms. Combining transformer architectures with convolutional neural networks (CNN) has shown great promise for EEG signal recognition, as demonstrated by recent research  [33] . Despite their promising performance in other domains, the adoption of transformers has been less prominent for bio-signals like EEG. This can be attributed to the substantial amount of training data required to effectively train transformers, which may not always be feasible for bio-signals such as EEG. More specifically, to the best of our knowledge, the current literature has yet to explore the use of transformer architectures for EEG-based cognitive load classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Self-Supervised Pre-Training",
      "text": "Recent advancements in self-supervised learning have significantly enhanced the effectiveness of models across various domains such as language  [6, 11] , vision  [7, 8] , and bio-signals  [30, 31] . The prevalent approach in this field involves utilizing self-supervision for pre-training, subsequently fine-tuning the model for downstream classification tasks, or maintaining it in a frozen state. Recently, masked autoencoding using transformers  [19]  has emerged as an effective and scalable representation learning framework. This approach incorporates masked autoencoders in a self-supervised fashion, where random segments of the input image are concealed, and an encoder-decoder configuration is used to reconstruct the missing pixels. Self-supervised techniques, such as masked autoencoding, have been extensively employed in conjunction with features (rather than raw data) across various domains  [24, 38, 41] .\n\nRecently, the efficacy of pre-training techniques has been explored in the context of EEG signal analysis. For instance, in  [9]  self-supervised masked autoencoding has been utilized for EEG sleep stage classification. For pre-training, their approach involves extracting features from raw EEG signals with the use of a CNN, followed by a transformer encoder tasked with reconstructing partially masked features. Following the encoder, two additional layers map the model output to the dimension of the raw EEG signal to compute a reconstructive loss. For fine-tuning, an additional linear classification layer is added to perform a variety of downstream tasks. A similar approach was proposed in  [25] , where a contrastive loss was calculated by directly comparing the encoder output with CNN output features. The results of  [9]  and  [25]  demonstrate the effectiveness of self-supervised learning approaches for EEG learning, suggesting their applicability to other classification tasks, including cognitive load classification.\n\nLastly, the notion of cross-domain transfer learning has been recently explored for EEG representation learning. In  [42] , crossdomain transfer learning is applied between patient-specific seizure prediction and sleep staging. First, a simple CNN was trained to conduct the cross-domain prediction tasks in both directions, seizure to sleep and vice versa. After pre-training the model for one specific task, 6 layers are then frozen to enable cross-domain transfer for the other task. Recently, in  [26] , the use of self-supervised crossdomain transfer learning was investigated for EEG learning. Their approach implements a CNN pre-trained with a contrastive selfsupervised learning task. For pre-training a clinical EEG dataset labeled as normal or abnormal was used, although the labels were excluded for the self-supervised task. For the downstream task, multiple linear layers were added to the CNN as a prediction head to classify EEG binary motor imagery tasks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "The goal of this paper is to effectively classify cognitive load from EEG signals using transformer architectures. However, our initial experiments suggest that conventional fully supervised single-stage training is not sufficiently effective for this task. Consequently, we investigate self-supervised pre-training using cross-domain datasets due to the lack of EEG cognitive load data sources. Our proposed method exploits the use of cross-domain transfer learning to effectively classify cognitive load from EEG signals. Next, we describe the pipeline of our method, beginning with data tokenization, followed by model pre-training, and downstream cognitive load classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Tokenization",
      "text": "To enhance the quality of the EEG recordings, we first apply preprocessing to all the datasets used for both pre-training and downstream cognitive load classification (see Section 4.1). Specifically, we apply a 2nd order Butterworth bandpass filter with a passband frequency of 1-75 Hz to eliminate unwanted noise and artifacts. Additionally, a notch filter with a quality factor of 30 was utilized to remove powerline noise at a frequency of 60 Hz.\n\nWe then conduct feature extraction from all the datasets used in this study. Following prior works  [2, 39, 44, 46] , two key features namely power spectral density (PSD) and Differential entropy (DE), are extracted. Both features were extracted over 5 frequency bands, from 1 to 75 Hz, namely, Delta (1-4 Hz), Theta (4-8 Hz), Alpha (8-12 Hz), Beta  (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30) (31)  and Gamma  (31-  important characteristics such as the distribution of power across different frequency bands and the complexity of the signal, and are thus widely used as the main features in the area. Effectively, given pre-processed EEG signals ùëã ‚àà R ùëè √óùëê , where ùëã are the EEG signal values, ùëè is the number of bands, and ùëê is the number of channels. Successive to feature extraction, we have ùëã ùëÉùëÜùê∑ ‚àà R ùëè √óùëê and ùëã ùê∑ùê∏ ‚àà R ùëè √óùëê . Given our focus on 5 frequency bands and the use of 4 channels (as will be discussed in Section 4.1.3), ùëè = 5 and ùëê = 4 respectively. PSD is a measure of signal power across different frequency components. To calculate this feature, we utilize Welch's method for each frequency band  [32] . The Welch method segments the signal into smaller sections, applies a window function to each segment, computes the discrete Fourier Transform of each segment, and then averages their squared magnitudes. This approach effectively reduces noise and provides a more accurate representation of the power spectrum across different frequency bands. Differential entropy (DE) is a measure of the complexity or irregularity of an EEG signal, and it is derived from the concept of differential entropy in information theory  [13] . To calculate DE, we assume that the EEG signal has a Gaussian distribution.\n\nNext, we concatenate and z-score normalize ùëã ùëÉùëÜùê∑ and ùëã ùê∑ùê∏ to obtain ùëã ùëì ùëíùëéùë° ‚àà R 2√óùëè √óùëê . We then tokenize ùëã ùëì ùëíùëéùë° into smaller, more manageable sequences, which can be processed and analyzed more efficiently by the model. Figure  2  depicts the tokenization process. We choose 10-second segments with no overlap as our downstream cognitive load dataset (see Section 4.1) provide output labels for every 10-second window. Let ùëã ùëñ ùëì ùëíùëéùë° be the set of features for the ùëñth 10-second window, where ùëñ = 1, 2, ..., ùëõ, and ùëõ is the number of 10-second segments (with no overlap) available in each participant's trial. Accordingly, we define a tokenized sequence as\n\nwhere ùëÜ ùëó is the ùëóth sequence of features, and ùëó = 1, 2, ..., ùëõ -2. Now that the data is arranged in sequences, it can be used to train the transformer architecture found in our proposed approach in the following sections.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Masked Autoencoding And Transfer Learning",
      "text": "In the pre-training stage, we conduct self-supervised masked autoencoding on the tokenized sequences of EEG feature segments, ùëÜ ùëó . Our model consists of a masked self-supervised autoencoder, which learns to reconstruct partially masked sequences of features during the training process. To create these masked sequences, we randomly mask one of the three segments in each tokenized sequence ùëÜ ùëó , which we denote by ùëî ùëó . The model consists of a transformer encoder comprising 4 transformer blocks, followed by a prediction head. The first component of the prediction head is a flattening layer that flattens the output of the transformer encoder. This is followed by 2 prediction blocks consisting of FC layers followed by ReLU activation with dropout. The size of the FC layers are 256 and 128 respectively. This is followed by a linear layer with size equal to the dimension of ùëî ùëó . The architecture of our model is depicted in Figure  3 . The model is tasked with predicting the features of the masked segment using information from the remaining unmasked segments in the sequence. During the pre-training stage, the model is tasked with performing regression on the features of the masked segment. We feed the masked sequences to the model, and train it using the L1 loss function measuring the mean absolute error (MAE) between the ground truth masked segments, ùëî ùëó , and their predicted values ƒùùëó , follows:\n\nwhere ùëÄ refers to the number of masked segments. Following pre-training, we aim to use the model for downstream EEG cognitive load classification. To this end, we discard the prediction head transfer the transformer blocks. In order to enable the model to classify cognitive load, we incorporate a new head consisting of a flattening layer that flattens the output of the transformer encoder, followed by 2 prediction blocks comprising an FC layer followed by ReLU activation with dropout. The size of the FC layers are 32 and 16, respectively. This is followed by a linear layer with the size of 3, and an additional Sigmoid activation function used to output the predicted binary cognitive load values for each 10-second segment ùëã ùëñ ùëì ùëíùëéùë° of ùëÜ ùëó . To investigate the effectiveness of the pre-trained model used in the downstream cognitive load classification task, we explore two distinct approaches. The first approach involves using the transformer blocks in a frozen fashion and only training the prediction head, while the latter allows them to be trained along with the new prediction head. In both cases, we train the model (either only the prediction head or the entire model) using the Binary Cross Entropy loss function measuring the difference between the predicted binary cognitive load labels ≈∑ and the ground truth labels ùë¶, as follows: where ùê∂ refers to the number of segments.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Segment Vote Aggregation",
      "text": "Once the model is ready for the downstream classification task, it will generate a sequence of binary cognitive load predictions for each segment ùëã ùëñ ùëì ùëíùëéùë° in the tokenized input sequence ùëÜ ùëó . Consequently, due to the overlapping sequences (see Figure  2 ), each segment will be present in three consecutive sequences, and therefore result in 3 separate predictions. Thus, an aggregation process is necessary to determine a final label for each segment. To generate a predicted value for each segment, we use a voting mechanism among the three predicted labels for that segment. It should be noted that the first two and the last two segments in each batch will not have 3 predictions on which to vote, and therefore those segments are excluded from evaluation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "Here, we provide the details on the experiments carried out in this study. First, we describe the emotion datasets used during pretraining along with the dataset used for downstream cognitive load classification. These datasets have been collected with differing numbers of EEG channels. We address the discrepancy between the datasets and explain how we mitigated it. Next, we outline the evaluation protocol employed to assess the performance of our approach. Lastly, we present the implementation details required to replicate our experiments.     [13, 49]  and SEED-IV  [48] , are used for pre-training, both individually as well as combined. These datasets are both emotion-related and commonly used for affective computing. As the pre-training task is self-supervised, the labels of these datasets will not be used. The SEED  [13, 49]  dataset consists of 15 film clips used as stimuli during data collection. Each clip is categorized into 3 class labels (positive, neutral, and negative emotions). The data collection involved presenting each film clip for 4 minutes, with each of the 15 participants repeating this process twice. Of the 15 participants 7 were male and 8 were female, with an average of 23.27. The EEG signals of each participant have been recorded using 62 channels at a 1000 Hz sampling rate, which was later downsampled to 200 Hz. SEED-IV  [48]  consists of 72 short film clips used as stimuli during data collection. Each clip is categorized into 4 class labels (happy, sad, neutral, and fear emotions). The data collection involved presenting 24 short film clips at each session. Of the 15 subjects who participated in this study, 7 were male and 8 were female, all between the ages of 20-24. With each participant completing 3 sessions each on separate days, with their EEG signals recorded using 62 channels at a 1000 Hz sampling rate. The signals were later downsampled to 200 Hz. 4.1.3 Sensor placement. All three datasets have been recorded using EEG sensor setups that follow the 10-20 system for electrode placement  [27] . This system is widely used for consistent EEG electrode placement across individuals and studies, aiding in data consistency. A diagram of the electrode placements for both the SEED and SEED-IV datasets can be seen in Figure  4 . The CL-Drive dataset and the SEED datasets differ in their electrode placements, with the former using only 4 electrodes, at locations, TP9, AF7, AF8, and TP10, while the latter consists of 62 electrodes. To address this difference, we selected four electrodes from the SEED datasets that closely matched the placements in the CL-Drive dataset. Figure  5  shows the most similar electrode pairs, with the first element of each pair being from the SEED datasets (depicted in Red) and the second from the CL-Drive dataset (depicted in Blue), namely (TP7, TP9), (F7, AF7), (F8, AF8), and (TP8, TP10).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Protocol",
      "text": "To evaluate our proposed pipeline, we follow the evaluation protocol used for cognitive load classification in the paper that presents the CL-Drive dataset  [2] . A 10-fold cross-validation is performed on the downstream cognitive load classification. Given the imbalance between low and high cognitive load data (38% high cognitive load and 62% low cognitive load), we report both macro F1 scores as well as accuracy over the segment vote criteria for each fold.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "We provide a description of the specific parameters that are used to implement our proposed method. For both pre-training and downstream classification, a batch size of 64 and the Adam optimizer were used. The learning rate for the Adam optimizer was held constant at 0.0001 during pre-training, but a dynamic learning rate was applied during the downstream task, where fine-tuning was applied. We also experimented with learning rate schedulers, the results for which are presented in the next sections of the paper. During both the pre-training and downstream stages, we trained the model over 1000 epochs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "In this section, we conduct a series of experiments to evaluate the proposed approach for EEG cognitive load classification. First, we explore the impact of fine-tuning the entire model on the downstream task vs. keeping the transformer blocks frozen. We then evaluate the impact of using PSD, DE, or both, and also further experiment with fully-supervised training as a baseline. Then we perform ablation experiments followed by sensitivity studies on the important components and parameters of our solution. The numerical values presented in brackets beside each score denote the standard deviation of the respective values. In every table, the best score is highlighted in bold.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Performance",
      "text": "We investigate the effect of fine-tuning the pre-trained encoder on the performance of the downstream EEG cognitive load classification task. The results of these experiments are summarized in Table  1 , where we use PSD features from the SEED, SEED-IV, and a combination of the two datasets to pre-train the encoder. Our results demonstrate that freezing the pre-trained transformer architecture and only training the prediction head is more effective than fine-tuning the entire model. We believe one explanation for this observation is that when fine-tuned, the model is not able to retain the knowledge learned from the pre-training stage. This itself could be due to overfitting, which could occur when the downstream target dataset is relatively small in size. Research has demonstrated that the decision to fine-tune a downstream task hinges on the size of the target dataset, as evidenced in  [43] . On the other hand, when the transformer blocks are kept frozen, given the relative similarity of emotions and cognitive load  [22] [28], the model is able to retain and use the learned knowledge in the pre-training stage to extract effective representations for the downstream classification task.\n\nIn Table  1 , we investigate the impact of pre-training datasets on downstream task performance and observe that while using both SEED and SEED-IV is slightly better in performance than using either of them alone, the difference is not significant as SEED, SEED-IV, and SEED + SEED-IV yield relatively similar downstream results despite the expectation that a larger pre-training dataset would enhance performance. To further explore the reason behind this, we analyze the distributions of these datasets and present the outcome in Figure  6 . We observe in this figure that the distributions of SEED and SEED-IV are almost identical, which explains the reason for the lack of meaningful changes in downstream performance.\n\nTo further evaluate the impact of pre-training, we compare the performance of our solution pre-trained on SEED and SEED-IV to a fully supervised setup where no pre-training is performed. Moreover, we experiment with using PSD, DE, and PSD+DE as the input representations. The results of these experiments are presented in Table  2 , which demonstrate a significant improvement in performance when utilizing pre-training in comparison to fully supervised learning without the use of pre-training. An average",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Studies",
      "text": "Here, we ablate the key components of our method to investigate their importance and contribution towards the final performance.\n\nTo this end, we explore the impact of downstream prediction head architecture, number of transformer blocks, and the use of positional encoding. To standardize these studies we pre-trained on the combination of SEED and SEED-IV datasets and only used PSD features. First, we ablate the number of transformer blocks and explore 5, 4, and 3 blocks respectively. The results of this study can be seen  in Table  3 , which demonstrate that the use of 4 transformer blocks achieves the best performance. Next, we ablate the layers of the downstream prediction head. 3 different prediction head architectures were tested in this ablation study to determine the most effective parameter choices. The first architecture includes 3 FC layers, with sizes 32, 13, and 8 respectively, which we refer to as ùê¥1. The second architecture includes 2 FC layers, with sizes 32 and 13, respectively, named ùê¥2. The last architecture includes 1 FC layer with a size of 32, which we refer to as ùê¥3. All three architectures are followed by a linear layer with a size of 3. As per the previous ablation study, we use 4 transformer blocks in all three variants. The results of this study are presented in Table  4 , which demonstrates prediction head architecture ùê¥2 as the ideal choice.\n\nFinally, we explore the use of positional encoding, which was not used originally during our main solution and the previous experiments, as initial testing indicated a slight decrease in performance. To confirm these initial findings we conduct a more detailed analysis. The results of this study are presented in Table  5 , which confirm our initial findings that positional encoding does not improve performance and in fact decreases performance by a considerable margin. Positional encoding may introduce additional complexity when handling inherently noisy EEG signals. This can particularly be a problem if the noise pattern is not consistent across time, which is often the case with EEG data",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Sensitivity Analysis",
      "text": "We conduct sensitivity analyses on a variety of learning rate scheduler configurations to determine the most effective configuration. These configurations and the results are described in Table  6 . For each learning rate scheduler configuration, there is an initial learning rate value, a gamma value that represents the multiplicative factor by which the learning rate decreases for every step, and the step size which determines the frequency of epochs where the learning rate will decrease. The results indicate that a learning rate of 0.0001 and Gamma of 250 with a step size of 0.75 is the most effective configuration for the downstream EEG cognitive load classification task. With regard to architecture and pre-training datasets, we use the pipeline optimized through our ablation studies in this analysis.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "We presented a new approach for cognitive load classification from EEG using transformer architectures. Our method used masked encoding of tokenized features for pre-training on emotion datasets. This was followed by transferring the model for downstream classification of cognitive load. Our method demonstrates the potential of self-supervised pre-training through masked autoencoding in combination with cross-domain transfer learning as a promising approach for classifying cognitive load from EEG data. This approach surpasses conventional single-stage fully supervised learning when classifying EEG cognitive load, by an average increase of 8.33% in accuracy and 11.33% in F1 score. Detailed ablation and sensitivity studies demonstrated the impact of different components and variants of our method. Our new approach makes a valuable contribution to the advancement in the field of cognitive load analysis, which currently remains relatively under-explored in the context of EEG signals. The insights gained from this study can guide future research aimed at improving the detection of high levels of cognitive load in high-risk situations, thereby enhancing human performance and safety.\n\nFor future work, to leverage unlabeled pre-training data (e.g., from emotion datasets) together with labeled downstream data (e.g., from cognitive load datasets), semi-supervised learning could be explored. Semi-supervised learning has shown promising results in the area of emotion recognition from EEG; however, its use in crossdomain transfer learning has remained unexplored in the context of cognitive load classification. Moreover, given the difficulty of generating accurate labels for cognitive load datasets, the notion of partial label learning could be studied. This notion has been recently explored for emotion recognition from EEG, but is yet to be applied for cognitive load classification.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A complete overview of the pipeline used in our paper is presented.",
      "page": 1
    },
    {
      "caption": "Figure 2: The process of converting raw EEG signals into tokenized se-",
      "page": 3
    },
    {
      "caption": "Figure 2: depicts the tokenization",
      "page": 3
    },
    {
      "caption": "Figure 3: The model is tasked with predicting the features of the",
      "page": 4
    },
    {
      "caption": "Figure 3: A diagram of the transformer architecture used in our solution",
      "page": 4
    },
    {
      "caption": "Figure 4: The 10-20 system for electrode placements used by the SEED",
      "page": 5
    },
    {
      "caption": "Figure 5: Comparison of the electrode placements in the CL-DRIVE dataset",
      "page": 5
    },
    {
      "caption": "Figure 4: The CL-Drive",
      "page": 5
    },
    {
      "caption": "Figure 5: shows the most similar electrode pairs, with the first element of",
      "page": 5
    },
    {
      "caption": "Figure 6: We observe in this figure that the distributions of SEED",
      "page": 6
    },
    {
      "caption": "Figure 6: The distributions of the datasets used in pre-training.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "ABSTRACT"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "Cognitive load, the amount of mental effort required for task com-"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "pletion, plays an important role in performance and decision-making"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "outcomes, making its classification and analysis essential\nin var-"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "ious sensitive domains. In this paper, we present a new solution"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "for the classification of cognitive load using electroencephalogram"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "(EEG). Our model uses a transformer architecture employing trans-"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "fer learning between emotions and cognitive load. We pre-train"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "our model using self-supervised masked autoencoding on emotion-"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "related EEG datasets and use transfer learning with both frozen"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "weights and fine-tuning to perform downstream cognitive load"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "classification. To evaluate our method, we carry out a series of"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "experiments utilizing two publicly available EEG-based emotion"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "datasets, namely SEED and SEED-IV, for pre-training, while we use"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "the CL-Drive dataset for downstream cognitive load classification."
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "The results of our experiments show that our proposed approach"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "achieves strong results and outperforms conventional single-stage"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "fully supervised learning. Moreover, we perform detailed ablation"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "and sensitivity studies to evaluate the impact of different aspects"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "of our proposed solution. This research contributes to the growing"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "body of literature in affective computing with a focus on cognitive"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "load, and opens up new avenues for future research in the field of"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "cross-domain transfer learning using self-supervised pre-training."
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "CCS CONCEPTS"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "‚Ä¢ Human-centered computing ‚Üí Ubiquitous computing;"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "This paper has been accepted to the 25th International Conference on Multi-"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": "modal Interaction (ICMI 2023),\n,"
        },
        {
          "Figure 1: A complete overview of the pipeline used in our paper is presented.": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "load dataset, demonstrating the important potential for emotion": "datasets to be used for cognitive load pre-training."
        },
        {
          "load dataset, demonstrating the important potential for emotion": ""
        },
        {
          "load dataset, demonstrating the important potential for emotion": "‚Ä¢ For the first"
        },
        {
          "load dataset, demonstrating the important potential for emotion": ""
        },
        {
          "load dataset, demonstrating the important potential for emotion": ""
        },
        {
          "load dataset, demonstrating the important potential for emotion": "plored approach in this domain."
        },
        {
          "load dataset, demonstrating the important potential for emotion": ""
        },
        {
          "load dataset, demonstrating the important potential for emotion": ""
        },
        {
          "load dataset, demonstrating the important potential for emotion": ""
        },
        {
          "load dataset, demonstrating the important potential for emotion": ""
        },
        {
          "load dataset, demonstrating the important potential for emotion": ""
        },
        {
          "load dataset, demonstrating the important potential for emotion": ""
        },
        {
          "load dataset, demonstrating the important potential for emotion": ""
        },
        {
          "load dataset, demonstrating the important potential for emotion": ""
        },
        {
          "load dataset, demonstrating the important potential for emotion": ""
        },
        {
          "load dataset, demonstrating the important potential for emotion": ""
        },
        {
          "load dataset, demonstrating the important potential for emotion": "supervised learning."
        },
        {
          "load dataset, demonstrating the important potential for emotion": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "The resultant signal encompasses a succession of waves exhibit-",
          "describe the experimental setup, including the datasets, evaluation": "protocol, and implementation details, in Section 4. In Section 5, we"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "ing diverse frequencies and amplitudes, which are indicative of",
          "describe the experimental setup, including the datasets, evaluation": "present the experimental results and report the conducted ablation"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "distinct cognitive and physiological states. Numerous deep learn-",
          "describe the experimental setup, including the datasets, evaluation": "studies and sensitivity analysis. Finally, we conclude the paper and"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "ing techniques have been employed for EEG analysis, resulting in",
          "describe the experimental setup, including the datasets, evaluation": "suggest future works in Section 6."
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "significant advancements in the field [45‚Äì47].",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "The investigation of cognitive load remains a relatively unex-",
          "describe the experimental setup, including the datasets, evaluation": "2\nRELATED WORK"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "plored area of research but has recently gained momentum, par-",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "In this section we first explore the use of\ntransformer architec-"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "ticularly with the development of comprehensive cognitive load",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "tures for learning EEG signals, followed by an investigation into"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "datasets in different contexts such as detecting passenger cogni-",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "the promising technique of self-supervised pre-training and their"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "tive states [2]. The detection and quantification of cognitive load",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "use with bio-signals. Furthermore, we provide a more in-depth"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "among vehicle passengers could enhance their safety, comfort, and",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "examination of cross-domain transfer learning."
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "overall experience. In order to effectively learn EEG data, both spa-",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "tial and temporal relationships need to be captured. Transformers",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "2.1\nEEG Learning with Transformers"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "[36] have recently emerged as a powerful tool, capable of learning",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "Since their introduction in [36], transformers have outperformed"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "both spatial and temporal information, and have shown impressive",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "many state-of-the-art models on a variety of different sequence"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "performances in a variety of different domains, such as natural lan-",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "modeling tasks, including machine translation [36, 37], language"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "guage processing [11], computer vision [12], and healthcare [18].",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "modeling [6, 11, 29, 36], and image captioning [1, 40]. The attention"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "Transformers, however, require large amounts of data for effec-",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "mechanism in transformers allows the model to attend to different"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "tive training, making them difficult to use for applications such as",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "parts of the input sequence, capturing long-range dependencies and"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "EEG-based cognitive load analysis, for which few datasets exist.",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "improving performance. Recently, transformers have been adopted"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "To tackle the problem mentioned above we propose a transformer",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "for use in the domain of bio-signals [4], including for analysis of"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "architecture for classifying cognitive load based on EEG signal. In",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "EEG data. In [35], an approach was proposed for EEG-based emo-"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "order to overcome the data scarcity issue, we explore transfer learn-",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "tion recognition using channel-wise attention and self-attention"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "ing between EEG-based emotion datasets (namely SEED [13, 49]",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "mechanisms. Combining transformer architectures with convolu-"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "and SEED-IV [48]) and a cognitive load dataset (namely CL-Drive",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "tional neural networks (CNN) has shown great promise for EEG"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "[2]). We pre-train the transformer model using self-supervised",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "signal recognition, as demonstrated by recent research [33]. De-"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "masked autoencoding on the emotion datasets, following freezing",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "spite their promising performance in other domains, the adoption"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "of the transformer blocks, we transfer the model for downstream",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "of transformers has been less prominent for bio-signals like EEG."
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "cognitive load classification on CL-Drive. In this stage, we keep",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "This can be attributed to the substantial amount of training data"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "the transformer blocks frozen, and only train the classification",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "required to effectively train transformers, which may not always"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "head. Our results demonstrate strong performance in comparison",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "be feasible for bio-signals such as EEG. More specifically, to the"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "to fully supervised training directly on the downstream cognitive",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "best of our knowledge, the current literature has yet to explore"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "load dataset, demonstrating the important potential for emotion",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "the use of transformer architectures for EEG-based cognitive load"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "datasets to be used for cognitive load pre-training.",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "classification."
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "The contributions of this paper are summarized as follows:",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "‚Ä¢ For the first\ntime, we perform EEG-based cognitive load",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "2.2\nSelf-Supervised Pre-Training"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "classification using masked autoencoding of features and a",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "Recent advancements in self-supervised learning have significantly"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "transformer architecture, representing a previously unex-",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "enhanced the effectiveness of models across various domains such"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "plored approach in this domain.",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "as language [6, 11], vision [7, 8], and bio-signals [30, 31]. The preva-"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "‚Ä¢ Our method uses transfer learning between emotions and",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "lent approach in this field involves utilizing self-supervision for"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "cognitive load using EEG signals. We pre-train our model us-",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "pre-training, subsequently fine-tuning the model for downstream"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "ing self-supervised masked autoencoding on emotion-related",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "classification tasks, or maintaining it in a frozen state. Recently,"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "EEG datasets and use transfer learning with frozen weights",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "masked autoencoding using transformers [19] has emerged as an"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "to perform downstream cognitive load classification.",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "effective and scalable representation learning framework. This ap-"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "‚Ä¢ Our method achieves strong results on cognitive load classi-",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "proach incorporates masked autoencoders in a self-supervised fash-"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "fication. Moreover, our results suggest that transfer learning",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "ion, where random segments of the input image are concealed, and"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "between emotions and cognitive load is indeed a viable path",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "an encoder-decoder configuration is used to reconstruct the miss-"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "for cognitive load analysis with deep learning given that our",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "ing pixels. Self-supervised techniques, such as masked autoencod-"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "approach outperforms the conventional single-stage fully",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "ing, have been extensively employed in conjunction with features"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "supervised learning.",
          "describe the experimental setup, including the datasets, evaluation": ""
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "",
          "describe the experimental setup, including the datasets, evaluation": "(rather than raw data) across various domains [24, 38, 41]."
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "The rest of this paper is organized as follows. In Section 2, we",
          "describe the experimental setup, including the datasets, evaluation": "Recently, the efficacy of pre-training techniques has been ex-"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "present a summary of EEG learning with transformer architec-",
          "describe the experimental setup, including the datasets, evaluation": "plored in the context of EEG signal analysis. For instance,\nin [9]"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "tures and discuss the application of self-supervised pre-training",
          "describe the experimental setup, including the datasets, evaluation": "self-supervised masked autoencoding has been utilized for EEG"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "to various bio-signals, including EEG. The proposed methodology",
          "describe the experimental setup, including the datasets, evaluation": "sleep stage classification. For pre-training, their approach involves"
        },
        {
          "detect the voltage fluctuations resulting from neural activity [17].": "is outlined in Section 3, which is illustrated in Figure 1. We then",
          "describe the experimental setup, including the datasets, evaluation": "extracting features from raw EEG signals with the use of a CNN,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "followed by a transformer encoder tasked with reconstructing par-": "tially masked features. Following the encoder, two additional layers",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "map the model output to the dimension of the raw EEG signal to",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "compute a reconstructive loss. For fine-tuning, an additional linear",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "classification layer is added to perform a variety of downstream",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "Feature Extraction"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "tasks. A similar approach was proposed in [25], where a contrastive",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "loss was calculated by directly comparing the encoder output with",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "CNN output features. The results of [9] and [25] demonstrate the ef-",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "fectiveness of self-supervised learning approaches for EEG learning,",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "suggesting their applicability to other classification tasks, including",
          "Pre-Processed EEG Signal": "Tokenizaiton"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "cognitive load classification.",
          "Pre-Processed EEG Signal": "10 seconds\n10 seconds\n10 seconds"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "Lastly, the notion of cross-domain transfer learning has been",
          "Pre-Processed EEG Signal": "EEG FEATURES\nEEG FEATURES\nEEG FEATURES"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "SEGMENT 2\nSEGMENT 1\nSEGMENT 3"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "recently explored for EEG representation learning. In [42], cross-",
          "Pre-Processed EEG Signal": "Tokenized Sequence 1"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "domain transfer learning is applied between patient-specific seizure",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "prediction and sleep staging. First, a simple CNN was trained to con-",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "EEG FEATURES\nEEG FEATURES\nEEG FEATURES"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "duct the cross-domain prediction tasks in both directions, seizure to",
          "Pre-Processed EEG Signal": "SEGMENT 2\nSEGMENT 3\nSEGMENT 4"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "Tokenized Sequence 2"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "sleep and vice versa. After pre-training the model for one specific",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "task, 6 layers are then frozen to enable cross-domain transfer for",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "the other task. Recently, in [26], the use of self-supervised cross-",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "domain transfer learning was investigated for EEG learning. Their",
          "Pre-Processed EEG Signal": "Masking"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "approach implements a CNN pre-trained with a contrastive self-",
          "Pre-Processed EEG Signal": "Masked"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "supervised learning task. For pre-training a clinical EEG dataset",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "Masked"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "labeled as normal or abnormal was used, although the labels were",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "excluded for the self-supervised task. For the downstream task,",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "Figure 2: The process of converting raw EEG signals into tokenized se-"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "multiple linear layers were added to the CNN as a prediction head",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "quences of EEG features is presented."
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "to classify EEG binary motor imagery tasks.",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "important characteristics such as the distribution of power across"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "3\nMETHOD",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "different frequency bands and the complexity of the signal, and are"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "The goal of this paper is to effectively classify cognitive load from",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "thus widely used as the main features in the area."
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "EEG signals using transformer architectures. However, our initial",
          "Pre-Processed EEG Signal": "Effectively, given pre-processed EEG signals ùëã ‚àà Rùëè √óùëê , where"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "experiments suggest that conventional fully supervised single-stage",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "ùëã are the EEG signal values, ùëè is the number of bands, and ùëê is"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "training is not sufficiently effective for this task. Consequently,",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "the number of channels. Successive to feature extraction, we have"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "we investigate self-supervised pre-training using cross-domain",
          "Pre-Processed EEG Signal": "ùëãùëÉùëÜùê∑ ‚àà Rùëè √óùëê and ùëãùê∑ùê∏ ‚àà Rùëè √óùëê . Given our focus on 5 frequency"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "datasets due to the lack of EEG cognitive load data sources. Our",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "bands and the use of 4 channels (as will be discussed in Section 4.1.3),"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "proposed method exploits the use of cross-domain transfer learning",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "ùëè = 5 and ùëê = 4 respectively. PSD is a measure of signal power"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "to effectively classify cognitive load from EEG signals. Next, we",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "across different frequency components. To calculate this feature,"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "describe the pipeline of our method, beginning with data tokeniza-",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "we utilize Welch‚Äôs method for each frequency band [32]. The Welch"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "tion, followed by model pre-training, and downstream cognitive",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "method segments the signal into smaller sections, applies a window"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "load classification.",
          "Pre-Processed EEG Signal": ""
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "function to each segment, computes the discrete Fourier Transform"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "",
          "Pre-Processed EEG Signal": "of each segment, and then averages their squared magnitudes. This"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "3.1\nTokenization",
          "Pre-Processed EEG Signal": "approach effectively reduces noise and provides a more accurate"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "To enhance the quality of the EEG recordings, we first apply pre-",
          "Pre-Processed EEG Signal": "representation of the power spectrum across different frequency"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "processing to all the datasets used for both pre-training and down-",
          "Pre-Processed EEG Signal": "bands. Differential entropy (DE) is a measure of the complexity or"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "stream cognitive load classification (see Section 4.1). Specifically,",
          "Pre-Processed EEG Signal": "irregularity of an EEG signal, and it is derived from the concept of"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "we apply a 2nd order Butterworth bandpass filter with a passband",
          "Pre-Processed EEG Signal": "differential entropy in information theory [13]. To calculate DE, we"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "frequency of 1-75 Hz to eliminate unwanted noise and artifacts.",
          "Pre-Processed EEG Signal": "assume that the EEG signal has a Gaussian distribution."
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "Additionally, a notch filter with a quality factor of 30 was utilized",
          "Pre-Processed EEG Signal": "Next, we concatenate and z-score normalize ùëãùëÉùëÜùê∑ and ùëãùê∑ùê∏"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "to remove powerline noise at a frequency of 60 Hz.",
          "Pre-Processed EEG Signal": "to obtain ùëãùëì ùëíùëéùë°\n‚àà R2√óùëè √óùëê . We then tokenize ùëãùëì ùëíùëéùë°\ninto smaller,"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "We then conduct feature extraction from all the datasets used in",
          "Pre-Processed EEG Signal": "more manageable sequences, which can be processed and analyzed"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "this study. Following prior works [2, 39, 44, 46], two key features",
          "Pre-Processed EEG Signal": "more efficiently by the model. Figure 2 depicts the tokenization"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "namely power spectral density (PSD) and Differential entropy (DE),",
          "Pre-Processed EEG Signal": "process. We choose 10-second segments with no overlap as our"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "are extracted. Both features were extracted over 5 frequency bands,",
          "Pre-Processed EEG Signal": "downstream cognitive load dataset (see Section 4.1) provide output"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "from 1 to 75 Hz, namely, Delta (1-4 Hz), Theta (4-8 Hz), Alpha (8-",
          "Pre-Processed EEG Signal": "labels for every 10-second window. Let ùëã ùëñ\nùëì ùëíùëéùë° be the set of features"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "12 Hz), Beta (12-31 Hz) and Gamma (31-75 Hz), over a 10-second",
          "Pre-Processed EEG Signal": "for the ùëñth 10-second window, where ùëñ = 1, 2, ..., ùëõ, and ùëõ is the"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "window. PSD and DE are considered to encompass a large portion",
          "Pre-Processed EEG Signal": "number of 10-second segments (with no overlap) available in each"
        },
        {
          "followed by a transformer encoder tasked with reconstructing par-": "of the necessary information from raw EEG signals as they capture",
          "Pre-Processed EEG Signal": "participant‚Äôs trial. Accordingly, we define a tokenized sequence as"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "is the ùëóth sequence of features,\nùëì ùëíùëéùë°\nùëì ùëíùëéùë°\nùëì ùëíùëéùë° ], where ùëÜ ùëó"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "and ùëó = 1, 2, ..., ùëõ ‚àí 2. Now that the data is arranged in sequences,"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "it can be used to train the transformer architecture found in our"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "proposed approach in the following sections."
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "3.2\nMasked Autoencoding and Transfer"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "Learning"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "In the pre-training stage, we conduct self-supervised masked au-"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "toencoding on the tokenized sequences of EEG feature segments, ùëÜ ùëó ."
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "Our model consists of a masked self-supervised autoencoder, which"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "learns to reconstruct partially masked sequences of features during"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "the training process. To create these masked sequences, we ran-"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "domly mask one of the three segments in each tokenized sequence"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "ùëÜ ùëó , which we denote by ùëîùëó . The model consists of a transformer"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "encoder comprising 4 transformer blocks, followed by a prediction"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "head. The first component of the prediction head is a flattening"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "layer that flattens the output of the transformer encoder. This is"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "followed by 2 prediction blocks consisting of FC layers followed by"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "ReLU activation with dropout. The size of the FC layers are 256 and"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "128 respectively. This is followed by a linear layer with size equal"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "to the dimension of ùëîùëó . The architecture of our model is depicted"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "in Figure 3. The model is tasked with predicting the features of the"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "masked segment using information from the remaining unmasked"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "segments in the sequence. During the pre-training stage, the model"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "is tasked with performing regression on the features of the masked"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "segment. We feed the masked sequences to the model, and train"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "it using the L1 loss function measuring the mean absolute error"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "(MAE) between the ground truth masked segments, ùëîùëó , and their"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "predicted values\nùëîùëó , as follows:"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "1 ùëÄ\nùëÄ‚àëÔ∏Å ùëñ\nùëîùëó ) =\n|ùëîùëó ‚àí ÀÜùëîùëó |,\n(1)\nùêø1 (ùëîùëó ,"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "=1"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "where ùëÄ refers to the number of masked segments."
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "Following pre-training, we aim to use the model for downstream"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "EEG cognitive load classification. To this end, we discard the pre-"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "diction head and transfer the transformer blocks. In order to enable"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "the model to classify cognitive load, we incorporate a new head"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "consisting of a flattening layer that flattens the output of the trans-"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "former encoder, followed by 2 prediction blocks comprising an FC"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "layer followed by ReLU activation with dropout. The size of the FC"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "layers are 32 and 16, respectively. This is followed by a linear layer"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "with the size of 3, and an additional Sigmoid activation function"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "used to output the predicted binary cognitive load values for each"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "10-second segment ùëã ùëñ\nùëì ùëíùëéùë° of ùëÜ ùëó . To investigate the effectiveness"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "of the pre-trained model used in the downstream cognitive load"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "classification task, we explore two distinct approaches. The first"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "approach involves using the transformer blocks in a frozen fashion"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "and only training the prediction head, while the latter allows them"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "to be trained along with the new prediction head. In both cases,"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "we train the model (either only the prediction head or the entire"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "model) using the Binary Cross Entropy loss function measuring"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "the difference between the predicted binary cognitive load labels ÀÜùë¶"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "and the ground truth labels ùë¶, as follows:"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "1 ùê∂\nùê∂‚àëÔ∏Å ùëñ\n[ùë¶ùëñ\nBCE(ùë¶, ÀÜùë¶) = ‚àí\nlog ÀÜùë¶ùëñ + (1 ‚àí ùë¶ùëñ ) log (1 ‚àí ÀÜùë¶ùëñ )],\n(2)"
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": ""
        },
        {
          ", ùëã ùëñ+1\n, ùëã ùëñ+2\nùëÜ ùëó = [ùëã ùëñ": "=1"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "[2] as the downstream dataset for cognitive load classification. CL-"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "Drive is a driver cognitive load assessment dataset, containing a"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "number of different physiological signals including EEG. The data"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "was collected in a physical vehicle driving simulator, where 23"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "participants completed 3-minute tasks ranging over 9 complexity"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "levels. Of\nthe 23 participants, 6 were male and 17 were female."
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "During the tasks, the participants reported their subjective cognitive"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "load every 10 seconds on a scale from 1-9, thus providing cognitive"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "load labels for each 10-second segment of EEG data. The categorical"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "labels were transformed into binary values, with values in the range"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "of 1 to 5 representing low cognitive load (0), and values in the range"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "of 6 to 9 representing high cognitive load (1). The EEG signals were"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "recorded using a 4-channel wearable headband at a 256 Hz sampling"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "rate."
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "4.1.3\nSensor placement. All three datasets have been recorded"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "using EEG sensor setups that follow the 10-20 system for electrode"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "placement [27]. This system is widely used for consistent EEG"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "electrode placement across individuals and studies, aiding in data"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "consistency. A diagram of the electrode placements for both the"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "SEED and SEED-IV datasets can be seen in Figure 4. The CL-Drive"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "dataset and the SEED datasets differ in their electrode placements,"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "with the former using only 4 electrodes, at locations, TP9, AF7, AF8,"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "and TP10, while the latter consists of 62 electrodes. To address this"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "difference, we selected four electrodes from the SEED datasets that"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "closely matched the placements in the CL-Drive dataset. Figure 5"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "shows the most similar electrode pairs, with the first element of"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "each pair being from the SEED datasets (depicted in Red) and the"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "second from the CL-Drive dataset (depicted in Blue), namely (TP7,"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "TP9), (F7, AF7), (F8, AF8), and (TP8, TP10)."
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "4.2\nEvaluation Protocol"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "To evaluate our proposed pipeline, we follow the evaluation proto-"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "col used for cognitive load classification in the paper that presents"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "the CL-Drive dataset [2]. A 10-fold cross-validation is performed on"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "the downstream cognitive load classification. Given the imbalance"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "between low and high cognitive load data (38% high cognitive load"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "and 62% low cognitive load), we report both macro F1 scores as"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "well as accuracy over the segment vote criteria for each fold."
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "4.3\nImplementation Details"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "We provide a description of the specific parameters that are used to"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "implement our proposed method. For both pre-training and down-"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "stream classification, a batch size of 64 and the Adam optimizer"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "were used. The learning rate for the Adam optimizer was held con-"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "stant at 0.0001 during pre-training, but a dynamic learning rate"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "was applied during the downstream task, where fine-tuning was"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "applied. We also experimented with learning rate schedulers, the"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "results for which are presented in the next sections of the paper."
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "During both the pre-training and downstream stages, we trained"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "the model over 1000 epochs."
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": ""
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "5\nRESULTS"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "In this section, we conduct a series of experiments to evaluate the"
        },
        {
          "4.1.2\nDownstream (Cognitive Load) Dataset. We use CL-Drive": "proposed approach for EEG cognitive load classification. First, we"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "strategies, with pre-training on SEED, SEED-IV, and SEED+SEED-IV."
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "Frozen\nFine-Tuned"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "Datasets\nAccuracy\nF1 score\nAccuracy\nF1 score"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "SEED\n73.57 (0.016)\n68.99 (0.026)\n67.94 (0.021)\n60.35 (0.043)"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "SEED-IV\n73.81 (0.029)\n69.59 (0.031)\n68.26 (0.043)\n57.60 (0.067)"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "SEED + SEED-IV 74.07 (0.025) 70.28 (0.025)\n63.06 (0.028)\n52.75 (0.070)"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "explore the impact of fine-tuning the entire model on the down-"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "stream task vs. keeping the transformer blocks frozen. We then"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "evaluate the impact of using PSD, DE, or both, and also further"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "experiment with fully-supervised training as a baseline. Then we"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "perform ablation experiments followed by sensitivity studies on"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "the important components and parameters of our solution. The"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "numerical values presented in brackets beside each score denote"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "the standard deviation of the respective values. In every table, the"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "best score is highlighted in bold."
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "5.1\nPerformance"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "We investigate the effect of fine-tuning the pre-trained encoder on"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "the performance of the downstream EEG cognitive load classifi-"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "cation task. The results of these experiments are summarized in"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "Table 1, where we use PSD features from the SEED, SEED-IV, and"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "a combination of the two datasets to pre-train the encoder. Our"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "results demonstrate that freezing the pre-trained transformer archi-"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "tecture and only training the prediction head is more effective than"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "fine-tuning the entire model. We believe one explanation for this"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "observation is that when fine-tuned, the model is not able to retain"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "the knowledge learned from the pre-training stage. This itself could"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "be due to overfitting, which could occur when the downstream"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "target dataset is relatively small in size. Research has demonstrated"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "that the decision to fine-tune a downstream task hinges on the size"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "of the target dataset, as evidenced in [43]. On the other hand, when"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "the transformer blocks are kept frozen, given the relative similarity"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": ""
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "of emotions and cognitive load [22][28], the model is able to retain"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "and use the learned knowledge in the pre-training stage to extract"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "effective representations for the downstream classification task."
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "In Table 1, we investigate the impact of pre-training datasets on"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "downstream task performance and observe that while using both"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "SEED and SEED-IV is slightly better in performance than using"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "either of them alone, the difference is not significant as SEED, SEED-"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "IV, and SEED + SEED-IV yield relatively similar downstream results"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "despite the expectation that a larger pre-training dataset would"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "enhance performance. To further explore the reason behind this, we"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "analyze the distributions of these datasets and present the outcome"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "in Figure 6. We observe in this figure that the distributions of SEED"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "and SEED-IV are almost identical, which explains the reason for"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "the lack of meaningful changes in downstream performance."
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "To further evaluate the impact of pre-training, we compare the"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "performance of our solution pre-trained on SEED and SEED-IV"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "to a fully supervised setup where no pre-training is performed."
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "Moreover, we experiment with using PSD, DE, and PSD+DE as the"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "input representations. The results of these experiments are pre-"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "sented in Table 2, which demonstrate a significant improvement"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "in performance when utilizing pre-training in comparison to fully"
        },
        {
          "Table 1: Cognitive load classification results using frozen and fine-tuning": "supervised learning without the use of pre-training. An average"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "be applied for cognitive load classification."
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "REFERENCES"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "[1]\nPeter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson,"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "Stephen Gould, and Lei Zhang. 2018. Bottom-up and top-down attention for"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "image captioning and visual question answering.\nIn Proceedings of\nthe IEEE"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "conference on Computer Vision and Pattern Recognition. 6077‚Äì6086."
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "[2]\nPrithila Angkan, Behnam Behinaein, Zunayed Mahmud, Anubhav Bhatti, Dirk"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "Rodenburg, Paul Hungler, and Ali Etemad. 2023. Multimodal Brain-Computer In-"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "terface for In-Vehicle Driver Cognitive Load Measurement: Dataset and Baselines."
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "(2023). arXiv:cs.LG/2304.04273"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "[3]\nPavlo Antonenko, Fred Paas, Roland Grabner, and Tamara Van Gog. 2010. Using"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "electroencephalography to measure cognitive load. Educational psychology review"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "22 (2010), 425‚Äì438."
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "[4] Behnam Behinaein, Anubhav Bhatti, Dirk Rodenburg, Paul Hungler, and Ali"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "Etemad. 2021. A transformer architecture for stress detection from ecg. In 2021"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "International Symposium on Wearable Computers. 132‚Äì134."
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "Francesco N Biondi, Babak Saberi, Frida Graf, Joel Cort, Prarthana Pillai, and\n[5]"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "Balakumar Balasingam. 2023. Distracted worker: Using pupil size and blink rate"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "to detect cognitive load during manufacturing tasks. Applied ergonomics 106"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "(2023), 103867."
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\nJared D Kaplan,"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "Askell, et al. 2020. Language models are few-shot learners. Advances in neural"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "information processing systems 33 (2020), 1877‚Äì1901."
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "simple framework for contrastive learning of visual representations. In Interna-"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "tional Conference on Machine Learning. PMLR, 1597‚Äì1607."
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "[8] Xinlei Chen and Kaiming He. 2021. Exploring simple siamese representation"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "learning.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "Pattern Recognition. 15750‚Äì15758."
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "[9] Hsiang-Yun Sherry Chien, Hanlin Goh, Christopher M Sandino, and Joseph Y"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "Cheng. 2022. MAEEG: Masked Auto-encoder for EEG Representation Learning."
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "arXiv preprint arXiv:2211.02625 (2022)."
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "[10] Rajat Das, Debatri Chatterjee, Diptesh Das, Arijit Sinharay, and Aniruddha Sinha."
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "2014. Cognitive load measurement-a methodology to compare low cost com-"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "mercial eeg devices. In 2014 International Conference on Advances in Computing,"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "Communications and Informatics. IEEE, 1188‚Äì1194."
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "[11]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": ""
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "Pre-training of deep bidirectional transformers for language understanding. arXiv"
        },
        {
          "recently explored for emotion recognition from EEG, but is yet to": "preprint arXiv:1810.04805 (2018)."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "Positional Enc.\nAccuracy\nF1 score"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "‚úì\n70.87 (0.024)\n66.93 (0.023)"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "‚úó\n74.07 (0.025)\n70.28 (0.025)"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "Table 6: Sensitivity analysis on the scheduler configurations."
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "Learning Rate Gamma\nStep Size Accuracy\nF1 score"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "70.28 (0.027) 71.17 (0.031)\n0.0001\n250\n0.75"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "0.00001\n250\n0.75\n68.25 (0.036)\n60.42 (0.044)"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "0.0001\n100\n0.5\n72.24 (0.026)\n66.23 (0.038)"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "0.0001\n150\n0.5\n74.43 (0.014)\n71.12 (0.026)"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "in Table 3, which demonstrate that the use of 4 transformer blocks"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "achieves the best performance."
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "Next, we ablate the layers of the downstream prediction head. 3"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "different prediction head architectures were tested in this ablation"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "study to determine the most effective parameter choices. The first"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "architecture includes 3 FC layers, with sizes 32, 13, and 8 respec-"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "tively, which we refer to as ùê¥1. The second architecture includes"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "2 FC layers, with sizes 32 and 13, respectively, named ùê¥2. The last"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "architecture includes 1 FC layer with a size of 32, which we refer to"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "as ùê¥3. All three architectures are followed by a linear layer with a"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "size of 3. As per the previous ablation study, we use 4 transformer"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "blocks in all three variants. The results of this study are presented"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "in Table 4, which demonstrates prediction head architecture ùê¥2 as"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "the ideal choice."
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "Finally, we explore the use of positional encoding, which was not"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "used originally during our main solution and the previous experi-"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "ments, as initial testing indicated a slight decrease in performance."
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "To confirm these initial findings we conduct a more detailed analysis."
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "The results of this study are presented in Table 5, which confirm our"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "initial findings that positional encoding does not improve perfor-"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "mance and in fact decreases performance by a considerable margin."
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "Positional encoding may introduce additional complexity when"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "handling inherently noisy EEG signals. This can particularly be a"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "problem if the noise pattern is not consistent across time, which is"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "often the case with EEG data"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "5.3\nSensitivity Analysis"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "We conduct sensitivity analyses on a variety of learning rate sched-"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "uler configurations to determine the most effective configuration."
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "These configurations and the results are described in Table 6. For"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "each learning rate scheduler configuration, there is an initial learn-"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "ing rate value, a gamma value that represents the multiplicative"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "factor by which the learning rate decreases for every step, and the"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "step size which determines the frequency of epochs where the learn-"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "ing rate will decrease. The results indicate that a learning rate of"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "0.0001 and Gamma of 250 with a step size of 0.75 is the most effective"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "configuration for the downstream EEG cognitive load classification"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "task. With regard to architecture and pre-training datasets, we use"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "the pipeline optimized through our ablation studies in this analysis."
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "6\nCONCLUSION AND FUTURE WORK"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "We presented a new approach for cognitive load classification from"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "EEG using transformer architectures. Our method used masked"
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": ""
        },
        {
          "Table 5: Ablation experiment on the impact of positional encoding.": "encoding of tokenized features for pre-training on emotion datasets."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg",
          "arXiv preprint arXiv:1906.01787 (2019).": "[38] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers",
          "arXiv preprint arXiv:1906.01787 (2019).": "Feichtenhofer. 2022. Masked feature prediction for self-supervised visual pre-"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).",
          "arXiv preprint arXiv:1906.01787 (2019).": "training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[13] Ruo-Nan Duan, Jia-Yi Zhu, and Bao-Liang Lu. 2013. Differential entropy feature",
          "arXiv preprint arXiv:1906.01787 (2019).": "Recognition. 14668‚Äì14678."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "for EEG-based emotion classification. In International IEEE/EMBS Conference on",
          "arXiv preprint arXiv:1906.01787 (2019).": "[39] Xun Wu, Wei-Long Zheng, and Bao-Liang Lu. 2019.\nIdentifying functional"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Neural Engineering. 81‚Äì84.",
          "arXiv preprint arXiv:1906.01787 (2019).": "brain connectivity patterns for EEG-based emotion recognition. In International"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[14] Daniel E Ehrmann, Sara N Gallant, Sujay Nagaraj, Sebastian D Goodfellow, Danny",
          "arXiv preprint arXiv:1906.01787 (2019).": "IEEE/EMBS Conference on Neural Engineering. 235‚Äì238."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Eytan, Anna Goldenberg, and Mjaye L Mazwi. 2022. Evaluating and reducing",
          "arXiv preprint arXiv:1906.01787 (2019).": "[40] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "cognitive load should be a priority for machine learning in healthcare. Nature",
          "arXiv preprint arXiv:1906.01787 (2019).": "Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Medicine 28, 7 (2022), 1331‚Äì1333.",
          "arXiv preprint arXiv:1906.01787 (2019).": "image caption generation with visual attention. In International conference on"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[15]\nJohan Engstr√∂m, Gustav Markkula, Trent Victor, and Natasha Merat. 2017. Effects",
          "arXiv preprint arXiv:1906.01787 (2019).": "machine learning. PMLR, 2048‚Äì2057."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "of cognitive load on driving performance: The cognitive control hypothesis.",
          "arXiv preprint arXiv:1906.01787 (2019).": "Siming Yan, Yuqi Yang, Yuxiao Guo, Hao Pan, Peng-shuai Wang, Xin Tong, Yang\n[41]"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Human factors 59, 5 (2017), 734‚Äì764.",
          "arXiv preprint arXiv:1906.01787 (2019).": "Liu, and Qixing Huang. 2023. 3D Feature Prediction for Masked-AutoEncoder-"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[16] Kristin L Fraser, Paul Ayres, and John Sweller. 2015. Cognitive load theory for the",
          "arXiv preprint arXiv:1906.01787 (2019).": "Based Point Cloud Pretraining. arXiv preprint arXiv:2304.06911 (2023)."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "design of medical simulations. Simulation in Healthcare 10, 5 (2015), 295‚Äì307.",
          "arXiv preprint arXiv:1906.01787 (2019).": "[42] Chia-Yen Yang, Pin-Chen Chen, and Wen-Chen Huang. 2023. Cross-Domain"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[17]\nPierre Gloor. 1969. Hans Berger on electroencephalography. American Journal of",
          "arXiv preprint arXiv:1906.01787 (2019).": "Transfer of EEG to EEG or ECG Learning for CNN Classification Models. Sensors"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "EEG Technology 9, 1 (1969), 1‚Äì8.",
          "arXiv preprint arXiv:1906.01787 (2019).": "23, 5 (2023), 2458."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[18] Rishabh Gupta, Abdul Q Javaid, and S Ali Etemad. 2017. Towards unsupervised",
          "arXiv preprint arXiv:1906.01787 (2019).": "[43]\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "coherence-based assessment of ECG quality in different posture and movement",
          "arXiv preprint arXiv:1906.01787 (2019).": "are features in deep neural networks? Advances in neural information processing"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "conditions. In IEEE EMBS International Conference on Biomedical & Health Infor-",
          "arXiv preprint arXiv:1906.01787 (2019).": "systems 27 (2014)."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "matics. 413‚Äì416.",
          "arXiv preprint arXiv:1906.01787 (2019).": "[44] Guangyi Zhang, Vandad Davoodnia, and Ali Etemad. 2022. Parse: Pairwise align-"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, and Ross Girshick.",
          "arXiv preprint arXiv:1906.01787 (2019).": "ment of representations in semi-supervised eeg learning for emotion recognition."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "2022. Masked autoencoders are scalable vision learners. In Proceedings of the",
          "arXiv preprint arXiv:1906.01787 (2019).": "IEEE Transactions on Affective Computing 13, 4 (2022), 2185‚Äì2200."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16000‚Äì16009.",
          "arXiv preprint arXiv:1906.01787 (2019).": "[45] Guangyi Zhang, Vandad Davoodnia, Alireza Sepas-Moghaddam, Yaoxue Zhang,"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[20]\nPascal Hecker, Arpita M Kappattanavar, Maximilian Schmitt, Sidratul Moontaha,",
          "arXiv preprint arXiv:1906.01787 (2019).": "and Ali Etemad. 2019. Classification of hand movements from EEG using a deep"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Johannes Wagner, Florian Eyben, Bj√∂rn W Schuller, and Bert Arnrich. 2022.",
          "arXiv preprint arXiv:1906.01787 (2019).": "attention-based LSTM network.\nIEEE Sensors Journal 20, 6 (2019), 3113‚Äì3122."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Quantifying Cognitive Load from Voice using Transformer-Based Models and a",
          "arXiv preprint arXiv:1906.01787 (2019).": "Spatio-temporal EEG representation\n[46] Guangyi Zhang and Ali Etemad. 2020."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Cross-Dataset Evaluation. In IEEE International Conference on Machine Learning",
          "arXiv preprint arXiv:1906.01787 (2019).": "learning on Riemannian manifold and Euclidean space.\narXiv e-prints (2020),"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "and Applications. 337‚Äì344.",
          "arXiv preprint arXiv:1906.01787 (2019).": "arXiv‚Äì2008."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[21] Nina Hollender, Cristian Hofmann, Michael Deneke, and Bernhard Schmitz. 2010.",
          "arXiv preprint arXiv:1906.01787 (2019).": "[47] Guangyi Zhang and Ali Etemad. 2023. Partial Label Learning for Emotion Recog-"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Integrating cognitive load theory and concepts of human‚Äìcomputer interaction.",
          "arXiv preprint arXiv:1906.01787 (2019).": "nition from EEG. arXiv preprint arXiv:2302.13170 (2023)."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Computers in human behavior 26, 6 (2010), 1278‚Äì1288.",
          "arXiv preprint arXiv:1906.01787 (2019).": "[48] Wei-Long Zheng, Wei Liu, Yifei Lu, Bao-Liang Lu, and Andrzej Cichocki. 2018."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[22] Andrea Hunziker Heeb, Caroline Lehr, and Maureen Ehrensberger-Dow. 2021.",
          "arXiv preprint arXiv:1906.01787 (2019).": "Emotionmeter: A multimodal framework for recognizing human emotions.\nIEEE"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Situated translators: cognitive load and the role of emotions.\nIn Advances in",
          "arXiv preprint arXiv:1906.01787 (2019).": "Transactions on Cybernetics 49, 3 (2018), 1110‚Äì1122."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Cognitive Translation Studies. Springer, 47‚Äì65.",
          "arXiv preprint arXiv:1906.01787 (2019).": "[49] Wei-Long Zheng and Bao-Liang Lu. 2015.\nInvestigating critical frequency bands"
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[23] Kerttu Huttunen, Heikki Ker√§nen, Eero V√§yrynen, Rauno P√§√§kk√∂nen, and Tuomo",
          "arXiv preprint arXiv:1906.01787 (2019).": "and channels for EEG-based emotion recognition with deep neural networks."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Leino. 2011. Effect of cognitive load on speech prosody in aviation: Evidence",
          "arXiv preprint arXiv:1906.01787 (2019).": "IEEE Transactions on Autonomous Mental Development 7, 3 (2015), 162‚Äì175."
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "from military simulator flights. Applied ergonomics 42, 2 (2011), 348‚Äì357.",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[24]\nLonglong Jing and Yingli Tian. 2020.\nSelf-supervised visual\nfeature learning",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "with deep neural networks: A survey.\nIEEE transactions on Pattern Analysis and",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Machine Intelligence 43, 11 (2020), 4037‚Äì4058.",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[25] Demetres Kostas, Stephane Aroca-Ouellette, and Frank Rudzicz. 2021. BENDR:",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "using transformers and a contrastive self-supervised learning task to learn from",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "massive amounts of EEG data. Frontiers in Human Neuroscience 15 (2021), 653659.",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[26] Xiaomin Li and Vangelis Metsis. 2022. SPP-EEGNET: An Input-Agnostic Self-",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "supervised EEG Representation Model for Inter-dataset Transfer Learning. In",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Proceedings of the International Conference on Computing and Information Tech-",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "nology. 173‚Äì182.",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[27] Andrew Morley, Lizzie Hill, and AG Kaditis. 2016. 10-20 system EEG Placement.",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "European Respiratory Society, European Respiratory Society (2016).",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[28]\nJan L Plass and Slava Kalyuga. 2019.\nFour ways of considering emotion in",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "cognitive load theory. Educational Psychology Review 31 (2019), 339‚Äì359.",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Ilya Sutskever, et al. 2018.\n[29] Alec Radford, Karthik Narasimhan, Tim Salimans,",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Improving language understanding by generative pre-training.\n(2018).",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Pritam Sarkar and Ali Etemad. 2020. Self-supervised ECG representation learning\n[30]",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "for emotion recognition.\nIEEE Transactions on Affective Computing 13, 3 (2020),",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "1541‚Äì1554.",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[31]\nPritam Sarkar, Silvia Lobmaier, Bibiana Fabre, Diego Gonz√°lez, Alexander Mueller,",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Martin G Frasch, Marta C Antonelli, and Ali Etemad. 2021. Detection of maternal",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "and fetal stress from the electrocardiogram with self-supervised representation",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "learning. Scientific reports 11, 1 (2021), 1‚Äì10.",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[32] Otis M Solomon Jr. 1991.\nPSD computations using Welch‚Äôs method.\nNASA",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "STI/Recon Technical Report N 92 (1991), 23584.",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[33]\nJiayao Sun, Jin Xie, and Huihui Zhou. 2021. EEG classification with transformer-",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "based models. In IEEE 3rd Global Conference on Life Sciences and Technologies.",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "92‚Äì93.",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[34]\nJohn Sweller. 2016. Cognitive load theory, evolutionary educational psychology,",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "and instructional design.\nEvolutionary perspectives on child development and",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "education (2016), 291‚Äì306.",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[35] Wei Tao, Chang Li, Rencheng Song, Juan Cheng, Yu Liu, Feng Wan, and Xun",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Chen. 2020. EEG-based emotion recognition via channel-wise attention and self",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "attention.\nIEEE Transactions on Affective Computing (2020).",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[36] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones,",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017.\nAttention is all",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "you need. Advances in neural information processing systems 30 (2017).",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "[37] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        },
        {
          "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-": "Lidia S Chao. 2019. Learning deep transformer models for machine translation.",
          "arXiv preprint arXiv:1906.01787 (2019).": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Bottom-up and top-down attention for image captioning and visual question answering",
      "authors": [
        "Peter Anderson",
        "Xiaodong He",
        "Chris Buehler",
        "Damien Teney",
        "Mark Johnson",
        "Stephen Gould",
        "Lei Zhang"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "2",
      "title": "Multimodal Brain-Computer Interface for In-Vehicle Driver Cognitive Load Measurement: Dataset and Baselines",
      "authors": [
        "Prithila Angkan",
        "Zunayed Behnam Behinaein",
        "Anubhav Mahmud",
        "Dirk Bhatti",
        "Paul Rodenburg",
        "Ali Hungler",
        "Etemad"
      ],
      "year": "2023",
      "venue": "Multimodal Brain-Computer Interface for In-Vehicle Driver Cognitive Load Measurement: Dataset and Baselines"
    },
    {
      "citation_id": "3",
      "title": "Using electroencephalography to measure cognitive load",
      "authors": [
        "Pavlo Antonenko",
        "Fred Paas",
        "Roland Grabner",
        "Tamara Van Gog"
      ],
      "year": "2010",
      "venue": "Educational psychology review"
    },
    {
      "citation_id": "4",
      "title": "A transformer architecture for stress detection from ecg",
      "authors": [
        "Anubhav Behnam Behinaein",
        "Dirk Bhatti",
        "Paul Rodenburg",
        "Ali Hungler",
        "Etemad"
      ],
      "year": "2021",
      "venue": "2021 International Symposium on Wearable Computers"
    },
    {
      "citation_id": "5",
      "title": "Distracted worker: Using pupil size and blink rate to detect cognitive load during manufacturing tasks",
      "authors": [
        "Francesco Biondi",
        "Babak Saberi",
        "Frida Graf",
        "Joel Cort",
        "Prarthana Pillai",
        "Balakumar Balasingam"
      ],
      "year": "2023",
      "venue": "Applied ergonomics"
    },
    {
      "citation_id": "6",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "7",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "8",
      "title": "Exploring simple siamese representation learning",
      "authors": [
        "Xinlei Chen",
        "Kaiming He"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "MAEEG: Masked Auto-encoder for EEG Representation Learning",
      "authors": [
        "Hanlin Hsiang-Yun Sherry Chien",
        "Christopher Goh",
        "Joseph Sandino",
        "Cheng"
      ],
      "year": "2022",
      "venue": "MAEEG: Masked Auto-encoder for EEG Representation Learning",
      "arxiv": "arXiv:2211.02625"
    },
    {
      "citation_id": "10",
      "title": "Cognitive load measurement-a methodology to compare low cost commercial eeg devices",
      "authors": [
        "Rajat Das",
        "Debatri Chatterjee",
        "Diptesh Das",
        "Arijit Sinharay",
        "Aniruddha Sinha"
      ],
      "year": "2014",
      "venue": "2014 International Conference on Advances in Computing, Communications and Informatics"
    },
    {
      "citation_id": "11",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "12",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "13",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "Jia-Yi Ruo-Nan Duan",
        "Bao-Liang Zhu",
        "Lu"
      ],
      "year": "2013",
      "venue": "International IEEE/EMBS Conference on Neural Engineering"
    },
    {
      "citation_id": "14",
      "title": "Evaluating and reducing cognitive load should be a priority for machine learning in healthcare",
      "authors": [
        "Sara Daniel E Ehrmann",
        "Sujay Gallant",
        "Sebastian Nagaraj",
        "Danny Goodfellow",
        "Anna Eytan",
        "Mjaye Goldenberg",
        "Mazwi"
      ],
      "year": "2022",
      "venue": "Nature Medicine"
    },
    {
      "citation_id": "15",
      "title": "Effects of cognitive load on driving performance: The cognitive control hypothesis",
      "authors": [
        "Johan Engstr√∂m",
        "Gustav Markkula",
        "Trent Victor",
        "Natasha Merat"
      ],
      "year": "2017",
      "venue": "Human factors"
    },
    {
      "citation_id": "16",
      "title": "Cognitive load theory for the design of medical simulations",
      "authors": [
        "Kristin L Fraser",
        "Paul Ayres",
        "John Sweller"
      ],
      "year": "2015",
      "venue": "Simulation in Healthcare"
    },
    {
      "citation_id": "17",
      "title": "Hans Berger on electroencephalography",
      "authors": [
        "Pierre Gloor"
      ],
      "year": "1969",
      "venue": "American Journal of EEG Technology"
    },
    {
      "citation_id": "18",
      "title": "Towards unsupervised coherence-based assessment of ECG quality in different posture and movement conditions",
      "authors": [
        "Rishabh Gupta",
        "Abdul Javaid",
        "Ali Etemad"
      ],
      "year": "2017",
      "venue": "IEEE EMBS International Conference on Biomedical & Health Informatics"
    },
    {
      "citation_id": "19",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr Doll√°r",
        "Ross Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Quantifying Cognitive Load from Voice using Transformer-Based Models and a Cross-Dataset Evaluation",
      "authors": [
        "Pascal Hecker",
        "M Arpita",
        "Maximilian Kappattanavar",
        "Sidratul Schmitt",
        "Johannes Moontaha",
        "Florian Wagner",
        "Bj√∂rn Eyben",
        "Bert Schuller",
        "Arnrich"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Machine Learning and Applications"
    },
    {
      "citation_id": "21",
      "title": "Integrating cognitive load theory and concepts of human-computer interaction",
      "authors": [
        "Nina Hollender",
        "Cristian Hofmann",
        "Michael Deneke",
        "Bernhard Schmitz"
      ],
      "year": "2010",
      "venue": "Computers in human behavior"
    },
    {
      "citation_id": "22",
      "title": "Situated translators: cognitive load and the role of emotions",
      "authors": [
        "Andrea Hunziker Heeb",
        "Caroline Lehr",
        "Maureen Ehrensberger-Dow"
      ],
      "year": "2021",
      "venue": "Advances in Cognitive Translation Studies"
    },
    {
      "citation_id": "23",
      "title": "Effect of cognitive load on speech prosody in aviation: Evidence from military simulator flights",
      "authors": [
        "Kerttu Huttunen",
        "Heikki Ker√§nen",
        "Eero V√§yrynen",
        "Rauno P√§√§kk√∂nen",
        "Tuomo Leino"
      ],
      "year": "2011",
      "venue": "Applied ergonomics"
    },
    {
      "citation_id": "24",
      "title": "Self-supervised visual feature learning with deep neural networks: A survey",
      "authors": [
        "Longlong Jing",
        "Yingli Tian"
      ],
      "year": "2020",
      "venue": "IEEE transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "25",
      "title": "BENDR: using transformers and a contrastive self-supervised learning task to learn from massive amounts of EEG data",
      "authors": [
        "Demetres Kostas",
        "Stephane Aroca-Ouellette",
        "Frank Rudzicz"
      ],
      "year": "2021",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "26",
      "title": "SPP-EEGNET: An Input-Agnostic Selfsupervised EEG Representation Model for Inter-dataset Transfer Learning",
      "authors": [
        "Xiaomin Li",
        "Vangelis Metsis"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Conference on Computing and Information Technology"
    },
    {
      "citation_id": "27",
      "title": "10-20 system EEG Placement",
      "authors": [
        "Andrew Morley",
        "Lizzie Hill",
        "A Kaditis"
      ],
      "year": "2016",
      "venue": "10-20 system EEG Placement"
    },
    {
      "citation_id": "28",
      "title": "Four ways of considering emotion in cognitive load theory",
      "authors": [
        "L Jan",
        "Slava Plass",
        "Kalyuga"
      ],
      "year": "2019",
      "venue": "Educational Psychology Review"
    },
    {
      "citation_id": "29",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "Alec Radford",
        "Karthik Narasimhan",
        "Tim Salimans",
        "Ilya Sutskever"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training"
    },
    {
      "citation_id": "30",
      "title": "Self-supervised ECG representation learning for emotion recognition",
      "authors": [
        "Pritam Sarkar",
        "Ali Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Detection of maternal and fetal stress from the electrocardiogram with self-supervised representation learning",
      "authors": [
        "Pritam Sarkar",
        "Silvia Lobmaier",
        "Bibiana Fabre",
        "Diego Gonz√°lez",
        "Alexander Mueller",
        "Martin Frasch",
        "Marta Antonelli",
        "Ali Etemad"
      ],
      "year": "2021",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "32",
      "title": "PSD computations using Welch's method",
      "authors": [
        "M Otis",
        "Solomon"
      ],
      "year": "1991",
      "venue": "PSD computations using Welch's method"
    },
    {
      "citation_id": "33",
      "title": "EEG classification with transformerbased models",
      "authors": [
        "Jiayao Sun",
        "Jin Xie",
        "Huihui Zhou"
      ],
      "year": "2021",
      "venue": "IEEE 3rd Global Conference on Life Sciences and Technologies"
    },
    {
      "citation_id": "34",
      "title": "Cognitive load theory, evolutionary educational psychology, and instructional design",
      "authors": [
        "John Sweller"
      ],
      "year": "2016",
      "venue": "Cognitive load theory, evolutionary educational psychology, and instructional design"
    },
    {
      "citation_id": "35",
      "title": "EEG-based emotion recognition via channel-wise attention and self attention",
      "authors": [
        "Wei Tao",
        "Chang Li",
        "Rencheng Song",
        "Juan Cheng",
        "Yu Liu",
        "Feng Wan",
        "Xun Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "≈Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "37",
      "title": "Learning deep transformer models for machine translation",
      "authors": [
        "Qiang Wang",
        "Bei Li",
        "Tong Xiao",
        "Jingbo Zhu",
        "Changliang Li",
        "Derek Wong",
        "Lidia Chao"
      ],
      "year": "2019",
      "venue": "Learning deep transformer models for machine translation",
      "arxiv": "arXiv:1906.01787"
    },
    {
      "citation_id": "38",
      "title": "Masked feature prediction for self-supervised visual pretraining",
      "authors": [
        "Chen Wei",
        "Haoqi Fan",
        "Saining Xie",
        "Chao-Yuan Wu",
        "Alan Yuille",
        "Christoph Feichtenhofer"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "39",
      "title": "Identifying functional brain connectivity patterns for EEG-based emotion recognition",
      "authors": [
        "Xun Wu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2019",
      "venue": "International IEEE/EMBS Conference on Neural Engineering"
    },
    {
      "citation_id": "40",
      "title": "Show, attend and tell: Neural image caption generation with visual attention",
      "authors": [
        "Kelvin Xu",
        "Jimmy Ba",
        "Ryan Kiros",
        "Kyunghyun Cho",
        "Aaron Courville",
        "Ruslan Salakhudinov",
        "Rich Zemel",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "41",
      "title": "Xin Tong, Yang Liu, and Qixing Huang. 2023. 3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining",
      "authors": [
        "Siming Yan",
        "Yuqi Yang",
        "Yuxiao Guo",
        "Peng-Shuai Hao Pan",
        "Wang"
      ],
      "year": "2023",
      "venue": "Xin Tong, Yang Liu, and Qixing Huang. 2023. 3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining",
      "arxiv": "arXiv:2304.06911"
    },
    {
      "citation_id": "42",
      "title": "Cross-Domain Transfer of EEG to EEG or ECG Learning for CNN Classification Models",
      "authors": [
        "Chia-Yen Yang",
        "Pin-Chen Chen",
        "Wen-Chen Huang"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "43",
      "title": "How transferable are features in deep neural networks?",
      "authors": [
        "Jason Yosinski",
        "Jeff Clune",
        "Yoshua Bengio",
        "Hod Lipson"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "44",
      "title": "Parse: Pairwise alignment of representations in semi-supervised eeg learning for emotion recognition",
      "authors": [
        "Guangyi Zhang",
        "Vandad Davoodnia",
        "Ali Etemad"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Classification of hand movements from EEG using a deep attention-based LSTM network",
      "authors": [
        "Guangyi Zhang",
        "Vandad Davoodnia",
        "Alireza Sepas-Moghaddam",
        "Yaoxue Zhang",
        "Ali Etemad"
      ],
      "year": "2019",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "46",
      "title": "Spatio-temporal EEG representation learning on Riemannian manifold and Euclidean space",
      "authors": [
        "Guangyi Zhang",
        "Ali Etemad"
      ],
      "year": "2020",
      "venue": "Spatio-temporal EEG representation learning on Riemannian manifold and Euclidean space"
    },
    {
      "citation_id": "47",
      "title": "Partial Label Learning for Emotion Recognition from EEG",
      "authors": [
        "Guangyi Zhang",
        "Ali Etemad"
      ],
      "year": "2023",
      "venue": "Partial Label Learning for Emotion Recognition from EEG",
      "arxiv": "arXiv:2302.13170"
    },
    {
      "citation_id": "48",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "Wei-Long Zheng",
        "Wei Liu",
        "Yifei Lu",
        "Bao-Liang Lu",
        "Andrzej Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "49",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    }
  ]
}