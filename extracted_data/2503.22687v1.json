{
  "paper_id": "2503.22687v1",
  "title": "Qieemo: Speech Is All You Need In The Emotion Recognition In Conversations",
  "published": "2025-03-05T07:02:30Z",
  "authors": [
    "Jinming Chen",
    "Jingyi Fang",
    "Yuanzhong Zheng",
    "Yaoxuan Wang",
    "Haojun Fei"
  ],
  "keywords": [
    "emotion recognition in conversation",
    "multimodal fusion",
    "cross-modal attention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition plays a pivotal role in intelligent human-machine interaction systems. Multimodal approaches benefit from the fusion of diverse modalities, thereby improving the recognition accuracy. However, the lack of highquality multimodal data and the challenge of achieving optimal alignment between different modalities significantly limit the potential for improvement in multimodal approaches. In this paper, the proposed Qieemo framework effectively utilizes the pretrained automatic speech recognition (ASR) model backbone which contains naturally frame aligned textual and emotional features, to achieve precise emotion classification solely based on the audio modality. Furthermore, we design the multimodal fusion (MMF) module and cross-modal attention (CMA) module in order to fuse the phonetic posteriorgram (PPG) and emotional features extracted by the ASR encoder for improving recognition accuracy. The experimental results on the IEMOCAP dataset demonstrate that Qieemo outperforms the benchmark unimodal, multimodal, and self-supervised models with absolute improvements of 3.0%, 1.2%, and 1.9% respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech emotion recognition (SER) technology is essential in augmenting the interactions naturalness of intelligent humanmachine systems by accurate identifying speakers' emotional state  [1] . The multimodal emotion recognition methods, which leverage the complementary information provided by different modalities, can achieve higher levels of recognition accuracy compared to unimodal methods  [2] . The effectiveness of multimodal methods, however, heavily relies on the quality of data acquired from each modality and the alignment of features across them. In conversational scenarios, the text modality typically depends on ASR models which may introduce errors in recognition. The presence of these biases can significantly impair the performance of multimodal methods  [3] . Additionally, the alignment among different modalities appears to be crucial in enhancing the accuracy of emotion recognition in multimodal methods. Additionally, these approaches are often constrained to scenarios where they can extract information from speech signals alone, such as voice assistants and telephone customer service. Therefore, our works focus on developing and improving unimodal speech emotion recognition.\n\nIn unimodal emotion recognition methods, CNN  [4] ,  [5]  and RNN  [6]  architectures are commonly employed for local feature extraction, while transformer architectures are utilized to capture global information. The CNN-Transformer  [7]  enhances the representation of emotional features in speech by stacking CNN blocks to extract local features and incorporating transformer to extract global features. The MS-SENet incorporates spatial dropout to enhance feature robustness and integrates squeeze-and-excitation (SE) modules to fuse multiscale features  [8] . Besides, some studies utilize various audio features, including mel frequency cepstral coefficents (MFCC), spectrograms and high-dimensional embedded acoustic information, combined with co-attention mechanisms to improve the emotional representation of acoustic features  [9] .\n\nThe remarkable performance of speech self-supervised learning in the downstream tasks has paved new avenues for developing SER. B Nasersharif et al.  [10]  propose a dimension reduction module to apply the output of the wav2vec 2.0 to generate the emotion feature. Qifei Li et al.  [11]  propose a frame-level emotional state alignment method that finetunes the HuBERT model to obtain emotional features. Xinxin Jiao et al.  [12]  propose a novel method called MFHCA, which employs multi-spatial fusion and the HuBERT model to achieve hierarchical cooperative attention on spectrograms and raw audio. The self-supervised learning (SSL) methods leverage extensive speech data to extract sparse vectors, which are subsequently fine-tuned on downstream tasks for the acquisition of emotion-related features. Nevertheless, this approach remains reliant solely on audio modality and lacks the enriched information that can be provided by audio inputs.\n\nIn this paper, we propose a comprehensive framework which can effectively integrates both textual and emotional information extracted from an ASR pretrained model, enabling highly accurate emotion recognition solely based on speech modality input. We propose MMF and CMA modules that combine the text-related PPG features with naturally frame aligned emotional features in order to enhance the robustness of emotion recognition. First, we validated the importance of the features from different layers of the pre-trained ASR encoders for downstream emotion recognition tasks. It has been proved by experiments that the features from the middle blocks contain strong emotion classification capabilities. Then, by leveraging our proposed MMF and CMA modules, we effectively fuse different blocks of ASR features, resulting in highly expressive emotion representations. Finally, through ablation experiments and contrast experiments on the IEMOCAP dataset, the performance of our unimodal emotion recognition approach has been validated to surpass the state-of-the-art accuracy achieved by multimodal and self-supervised models in emotion recognition. Moreover we prove the universality of Qieemo framework by conducting experiments with different pretrained ASR backbone architectures. This also confirms the feasibility of integrating our emotion recognition scheme into the existing ASR systems. In summary, the main contributions of our work are as follows:\n\n• We examine the significant impact of high-dimensional features extracted from different blocks in the pretrained ASR for the task of emotion classification. This discovery lays the foundation for the design of features fusion from pretrained ASR. • We propose the MMF and CMA module to effectively integrate features from different layers of the ASR backbone, demonstrating that this design achieves the highest classification accuracy results on the IEMOCAP dataset compared to multimodal and self-supervised models. • Our proposed Qieemo method is proved to be a universal framework capable of achieving high-accuracy emotion recognition with different ASR backbones, which high-lighting its potential integration ability in current ASR systems for emotion recognition in conversation (ERC).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Proposed Method As Shown In",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Pretrained Asr Backbone",
      "text": "The pretrained ASR structure utilizes an end-to-end attention-based encoder-decoder (AED) framework to extract multi-dimensional speech features from the spectrograms. Specifically, we employ the efficient conformer model as ASR backbone for extracting PPG features. The diagram in Fig.  1  illustrates that the encoder of the efficient conformer model is comprised of multiple conformer blocks  [13] . Each conformer block consists of a multi-head self-attention module (MHSA), a convolution module and two feed-forward (FFN) modules. The multi-head self-attention module effectively captures global information, while the convolution module has a natural advantage in extracting local information. More significantly, the efficient conformer introduces progressive down-sampling operation to the conformer encoder, which effectively reducing temporal redundancy caused by the speech frame step and enhancing the extraction of broader acoustic features.\n\nRegarding the spectrogram feature S t of the input audio x t , the frame-level acoustic features\n\nextracted by a series of conformer blocks can be computed as follows:\n\nwhere l denotes the quantity of conformer blocks employed in the efficient conformer model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Multimodal Fusion Module",
      "text": "The multi-dimensional acoustic features H t extracted from different conformer blocks are believed to possess varying levels of input audio representation, including textual information and a diverse range of linguistic cues, encompassing emotional information. The output feature of the final conformer block h l t , which contains crucial text-related information and is commonly named as PPG features, is fed into the ASR decoder.\n\nTo achieve optimal accuracy in emotion classification task, we design a MMF module that effectively combines PPG features and emotional features from different conformer blocks extracted by the ASR Encoder. As shown in Fig.  1 , a weighted concatenation method is used as the input for the subsequent modal fusion. The integration of features from different weighted blocks through the application of Conv2D enables the extraction of utterance-level emotional features\n\nt , e 2 t , . . . , e n t , thereby facilitating the classification of various emotions in conversation corpus.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Cross-Modal Attention Module",
      "text": "The text mode is considered to play a crucial role in the recognition of emotions during conversations among various multimodal solutions. Due to the utilization of the pretrained ASR backbone, we are able to extract PPG textual features from the output of the ASR encoder. This allows us to effectively employ these features without relying on any text modal input, thereby achieving emotion recognition solely through unimodal input.\n\nIn order to leverage the textual information embedded in the PPG features and enhance the representation of emotional features, we propose a CMA module to enhance the integration of emotional features with PPG features generated by the ASR encoder. Specifically, the emotional features E t are utilized as queries while the PPG features h l t function as keys and values. Both sets of features are fed into the CMA to compute their feature correlations as follows:\n\nwhere W Q t , W K t and W V t are trainable weight matrix, the division of the similarity matrix and √ d a tt in (  7 ) and (  8 ) contributes to steady gradient descent while training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. The Multi-Stage Training Strategy",
      "text": "In the first training stage, we train the ASR backbone model with a substantial amount of speech-text paired data, while freezing the MMF and CMA modules. The pretrained ASR model is employed as the initial model in the second training stage, and it is jointly trained with the MMF and CMA modules using emotion-labeled speech data. By utilizing the ASR pretrained model, which has been trained on extensive speech-text paired data, we can progressively enhance the generation ability of emotion features and consequently obtain a highly accurate emotion classification model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Experiment A. Datasets And Training Method",
      "text": "IEMOCAP  [14]  is a well-known multimodal emotion corpus, including audio, visual and text modalities. The experiments in our work exclusively employ audio data as the input modality. The audio corpus comprises five separate recording sessions, each of which showcases a male and a female speaker. To mitigate the potential disclosure of speaker identities and labels, we implement a five-fold crossvalidation methodology, wherein one session was excluded in each iteration. Our experiments conduct involved 5531 audio utterances, which are categorized into four emotional states: happy (including both happiness and excitement), sad, neutral, and angry  [15] . As for the performance evaluation of SER, we apply three commonly used metrics  [16] : weighted accuracy (WA), unweighted accuracy (UA) and weighted average F1 (WF1).\n\nTo demonstrate that Qieemo achieves superior results among unimodal, multimodal, and unsupervised approaches, we select the following benchmark models representing each category. MS-SENet  [17]  was chosen for unimodal speechbased emotion classification, MSMSER  [18]  for multimodal, and Emotion2Vec  [16]  for self-supervised emotion classification models. Each model has achieved high recognition accuracy within its respective framework.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Results And Comparison",
      "text": "Firstly we pretrain the ASR backbone model without MMF and CMA modules on the LibriSpeech English dataset. After completing the first stage training, the features extracted from the ASR encoder are utilized as input to the downstream emotion classifier, which is constructed with 256 dimensional linear layers. As illustrated in Table  I , among block 6 to 12, layer 9 achieves the highest emotion classification performance, with WA, UA and WF1 scores of 67.80 69.72 and 67.52 respectively. The analysis reveals that the accuracy of emotion recognition increases with the depth of ASR encoder initially, reaching its peak at block 9, and then declines. By contrast, the PPG features from the last ASR block contain more textual information, which results in lower accuracy for the emotion classification. Therefore, we can conclude that the highest emotional representation can be attributed to the features extracted from block 9, while the PPG features perform better in representing textual information. We also utilize features extracted from self-supervised models pretrained on the same dataset to implement downstream emotion classification. Our pretrained ASR encoder exhibits a recognition accuracy that is only 0.78% lower than that of data2vec 2.0. Furthermore, emotion2vec, which extra pretrained on an emotion-labeled corpus, achieves the highest recognition accuracy in our experiments.\n\nSecondly, the pretrained ASR model is finetuned together with the MMF and CMA modules on the IEMOCAP dataset, where the MMF module integrates features from the last six ASR encoder blocks, while the CMA module achieves framelevel fusion of textual and emotional features. As shown in Table  II , our approach achieves a WA of 76.42, an UA of 77.71 and a WF1 of 76.20. The performance exceeds the unimodal speech input method MS-SENet (73.38% WA) by 3.04% (absolute) and surpasses the multimodal input method MSMSER (75.2% WA) by 1.22% (absolute). Notably, Qieemo finetuned results on the IEMOCAP dataset, using only speech modality input, still exceed the current state-of-the-art SSL emotion recognition methods on IEMOCAP downstream tasks (74.48% WA) by 1.94% (absolute). The results demonstrate that Qieemo achieves the highest level of recognition accuracy among unimodal, multimodal and self-supervised model downstream fine-tuning scenarios.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Ablation Study",
      "text": "In order to verify the rationality of Qieemo framework design, we conduct the following ablation experiments. Firstly, we validate the proposed MMF and CMA modules in extracting features for enhancing the accuracy of emotion recognition. When removing the CMA module, the WA, UA and WF1 respectively drop to 74.71%, 75.33% and 74.74%, which demonstrates that the frame-level cross-modal fusion between PPG features and emotion features enhances emotion classification accuracy. Additionally, when we used only PPG features from the final block without incorporating the MMF module, the WA, UA and WF1 results drop to 74.29%, 75.16% and 74.20%. Besides, the UA absolutely decreases by 3.42% when the features from different blocks of the ASR pretrained encoder are not fused.\n\nSecondly, we demonstrate the pretraining process of the ASR backbone also facilitates the extraction of emotional feature representation and implicit textual information. The WA, UA and WF1 results of training the same Qieemo framework on IEMOCAP without utilizing the weights of a pretrained ASR encoder indicate a significant decrease to 57.57%, 59.60%, and 56.88% respectively, which are considerably lower compared to the results achieved using emotional features and textual features from the pretrained ASR encoder.\n\nFinally, in order to demonstrate the universality of our proposed Qieemo framework, we also employed an ASR encoder scheme without progressive down-sampling structure between conformer blocks [19] as a pretrained ASR backbone 2. The WA, UA and WF1 results presented in  all other compared approaches, thereby demonstrating the method's generalizability.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "In this paper, we propose the Qieemo framework based on a pretrained ASR backbone which effectively utilizes the emotion and PPG features from different blocks of ASR encoder. The integration design of MMF and CMA modules facilitates the model in achieving an effective fusion of naturallyaligned emotional and textual features, thereby mitigating the challenges associated with modality alignment in multimodal systems and addressing biases introduced by ASR. Through multiple experiments on IEMOCAP dataset, Qieemo attains the highest recognition accuracy among unimodal, multimodal and self-supervised models. This framework also demonstrates its substantial practicality in emotion recognition in real-time conversation.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the proposed Qieemo method",
      "page": 2
    },
    {
      "caption": "Figure 1: , we provide an overview of the proposed",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates that the encoder of the efficient conformer",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Self-supervised\nModel": "wav2vec 2.0 [20]",
          "Pre-training\nCorpus": "LS-960",
          "Upstream\nParams": "95.04M\n94.68M\n94.70M\n93.78M",
          "Down\nstream": "linear",
          "WA\n(%)": "63.43\n64.92\n65.94\n68.58\n71.79\n66.64\n67.71\n66.96\n67.80\n67.69\n65.31\n65.66"
        },
        {
          "Self-supervised\nModel": "HuBERT [21]",
          "Pre-training\nCorpus": "",
          "Upstream\nParams": "",
          "Down\nstream": "",
          "WA\n(%)": ""
        },
        {
          "Self-supervised\nModel": "WavLM [22]",
          "Pre-training\nCorpus": "",
          "Upstream\nParams": "",
          "Down\nstream": "",
          "WA\n(%)": ""
        },
        {
          "Self-supervised\nModel": "data2vec 2.0 [23]",
          "Pre-training\nCorpus": "",
          "Upstream\nParams": "",
          "Down\nstream": "",
          "WA\n(%)": ""
        },
        {
          "Self-supervised\nModel": "emotion2vec [16]",
          "Pre-training\nCorpus": "LS-960+Emo-262",
          "Upstream\nParams": "93.79M",
          "Down\nstream": "",
          "WA\n(%)": ""
        },
        {
          "Self-supervised\nModel": "ASR Encoder-{block 6}",
          "Pre-training\nCorpus": "LS-960",
          "Upstream\nParams": "49.35M",
          "Down\nstream": "",
          "WA\n(%)": ""
        },
        {
          "Self-supervised\nModel": "ASR Encoder-{block 7}",
          "Pre-training\nCorpus": "",
          "Upstream\nParams": "",
          "Down\nstream": "",
          "WA\n(%)": ""
        },
        {
          "Self-supervised\nModel": "ASR Encoder-{block 8}",
          "Pre-training\nCorpus": "",
          "Upstream\nParams": "",
          "Down\nstream": "",
          "WA\n(%)": ""
        },
        {
          "Self-supervised\nModel": "ASR Encoder-{block 9}",
          "Pre-training\nCorpus": "",
          "Upstream\nParams": "",
          "Down\nstream": "",
          "WA\n(%)": ""
        },
        {
          "Self-supervised\nModel": "ASR Encoder-{block 10}",
          "Pre-training\nCorpus": "",
          "Upstream\nParams": "",
          "Down\nstream": "",
          "WA\n(%)": ""
        },
        {
          "Self-supervised\nModel": "ASR Encoder-{block 11}",
          "Pre-training\nCorpus": "",
          "Upstream\nParams": "",
          "Down\nstream": "",
          "WA\n(%)": ""
        },
        {
          "Self-supervised\nModel": "ASR Encoder-{layer 12}",
          "Pre-training\nCorpus": "",
          "Upstream\nParams": "",
          "Down\nstream": "",
          "WA\n(%)": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modality": "S",
          "Method": "MS-SENet\n[17]",
          "UA (%)": "73.67",
          "WA (%)": "73.38",
          "WF1 (%)": "-"
        },
        {
          "Modality": "S",
          "Method": "SenseVoice-L [24]",
          "UA (%)": "73.9",
          "WA (%)": "75.3",
          "WF1 (%)": "73.2"
        },
        {
          "Modality": "S",
          "Method": "emotion2vec\nlarge [16]",
          "UA (%)": "70.7",
          "WA (%)": "67.3",
          "WF1 (%)": "68.5"
        },
        {
          "Modality": "S",
          "Method": "emotion2vec+fintune [16]",
          "UA (%)": "-",
          "WA (%)": "74.48",
          "WF1 (%)": "-"
        },
        {
          "Modality": "S",
          "Method": "MFHCA [12]",
          "UA (%)": "74.57",
          "WA (%)": "74.24",
          "WF1 (%)": "-"
        },
        {
          "Modality": "S+T",
          "Method": "MSMSER [18]",
          "UA (%)": "76.4",
          "WA (%)": "75.2",
          "WF1 (%)": "-"
        },
        {
          "Modality": "V+S",
          "Method": "MultiMAE-DER-FSLF [25]",
          "UA (%)": "63.21",
          "WA (%)": "63.73",
          "WF1 (%)": "-"
        },
        {
          "Modality": "V+S+T",
          "Method": "TelME [26]",
          "UA (%)": "-",
          "WA (%)": "-",
          "WF1 (%)": "70.48"
        },
        {
          "Modality": "S",
          "Method": "Qieemo-{ASR backbone 2}",
          "UA (%)": "76.79",
          "WA (%)": "75.98",
          "WF1 (%)": "75.89"
        },
        {
          "Modality": "S",
          "Method": "Qieemo-{ASR backbone 1}",
          "UA (%)": "77.71",
          "WA (%)": "76.42",
          "WF1 (%)": "76.20"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "2",
      "title": "Multimodal emotion recognition with deep learning: advancements, challenges, and future directions",
      "authors": [
        "A Geetha",
        "T Mala",
        "D Priyanka",
        "E Uma"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition with asr transcripts: A comprehensive study on word error rate and fusion techniques",
      "authors": [
        "Y Li",
        "P Bell",
        "C Lai"
      ],
      "venue": "Speech emotion recognition with asr transcripts: A comprehensive study on word error rate and fusion techniques"
    },
    {
      "citation_id": "4",
      "title": "Masa-tcn: Multi-anchor space-aware temporal convolutional neural networks for continuous and discrete eeg emotion recognition",
      "authors": [
        "Y Ding",
        "S Zhang",
        "C Tang",
        "C Guan"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "5",
      "title": "Efficient speech emotion recognition using multi-scale cnn and attention",
      "authors": [
        "Z Peng",
        "Y Lu",
        "S Pan",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/icassp39728.2021.9414286"
    },
    {
      "citation_id": "6",
      "title": "Exploring spatio-temporal representations by integrating attentionbased bidirectional-lstm-rnns and fcns for speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Y Zheng",
        "Z Zhang",
        "H Wang",
        "Y Zhao",
        "C Li"
      ],
      "year": "2018",
      "venue": "Exploring spatio-temporal representations by integrating attentionbased bidirectional-lstm-rnns and fcns for speech emotion recognition",
      "doi": "10.21437/interspeech.2018-1477"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition via cnn-transforemr and multidimensional attention mechanism",
      "authors": [
        "X Tang",
        "Y Lin",
        "T Dang",
        "Y Zhang",
        "J Cheng"
      ],
      "year": "2024",
      "venue": "Speech emotion recognition via cnn-transforemr and multidimensional attention mechanism",
      "arxiv": "arXiv:2403.04743"
    },
    {
      "citation_id": "8",
      "title": "Ms-senet: Enhancing speech emotion recognition through multi-scale feature fusion with squeeze-and-excitation blocks",
      "authors": [
        "M Li",
        "Y Wu",
        "D Li",
        "Y Zheng",
        "Y Wang",
        "H Fei"
      ],
      "year": "2023",
      "venue": "Ms-senet: Enhancing speech emotion recognition through multi-scale feature fusion with squeeze-and-excitation blocks"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "venue": "Speech emotion recognition with co-attention based multi-level acoustic information"
    },
    {
      "citation_id": "10",
      "title": "Exploring the potential of wav2vec 2.0 for speech emotion recognition using classifier combination and attention-based feature fusion",
      "authors": [
        "B Nasersharif",
        "M Namvarpour"
      ],
      "year": "2024",
      "venue": "The Journal of Supercomputing"
    },
    {
      "citation_id": "11",
      "title": "Frame-level emotional state alignment method for speech emotion recognition",
      "authors": [
        "Q Li",
        "Y Gao",
        "C Wang",
        "Y Deng",
        "J Xue",
        "Y Han",
        "Y Li"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Mfhca: Enhancing speech emotion recognition via multi-spatial fusion and hierarchical cooperative attention",
      "authors": [
        "X Jiao",
        "L Wang",
        "Y Yu"
      ],
      "venue": "Mfhca: Enhancing speech emotion recognition via multi-spatial fusion and hierarchical cooperative attention"
    },
    {
      "citation_id": "13",
      "title": "Efficient conformer-based speech recognition with linear attention",
      "authors": [
        "S Li",
        "M Xu",
        "X Zhang"
      ],
      "year": "2021",
      "venue": "arXiv: Sound,arXiv: Sound"
    },
    {
      "citation_id": "14",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: interactive emotional dyadic motion capture database",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "15",
      "title": "Representation learning with spectro-temporal-channel attention for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "C Xu",
        "J Dang",
        "E Chng",
        "H Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/icassp39728.2021.9414006"
    },
    {
      "citation_id": "16",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Z Ma",
        "Z Zheng",
        "J Ye",
        "J Li",
        "Z Gao",
        "S Zhang",
        "X Chen"
      ],
      "year": "2023",
      "venue": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "arxiv": "arXiv:2312.15185"
    },
    {
      "citation_id": "17",
      "title": "Ms-senet: Enhancing speech emotion recognition through multi-scale feature fusion with squeeze-and-excitation blocks",
      "authors": [
        "M Li",
        "Y Zheng",
        "D Li",
        "Y Wu",
        "Y Wang",
        "H Fei"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Exploring complementary features in multi-modal speech emotion recognition",
      "authors": [
        "S Wang",
        "Y Ma",
        "Y Ding"
      ],
      "venue": "Exploring complementary features in multi-modal speech emotion recognition"
    },
    {
      "citation_id": "19",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "20",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/taslp.2021.3122291"
    },
    {
      "citation_id": "21",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Efficient self-supervised learning with contextualized target representations for vision, speech and language",
      "authors": [
        "A Baevski",
        "A Babu",
        "W.-N Hsu",
        "M Auli"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "23",
      "title": "Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms",
      "authors": [
        "T Speechteam"
      ],
      "year": "2024",
      "venue": "Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms",
      "arxiv": "arXiv:2407.04051"
    },
    {
      "citation_id": "24",
      "title": "Multimae-der: Multimodal masked autoencoder for dynamic emotion recognition",
      "authors": [
        "P Xiang",
        "C Lin",
        "K Wu",
        "O Bai"
      ],
      "year": "2024",
      "venue": "Multimae-der: Multimodal masked autoencoder for dynamic emotion recognition"
    },
    {
      "citation_id": "25",
      "title": "Telme: Teacher-leading multimodal fusion network for emotion recognition in conversation",
      "authors": [
        "T Yun",
        "H Lim",
        "J Lee",
        "M Song"
      ],
      "year": "2024",
      "venue": "Telme: Teacher-leading multimodal fusion network for emotion recognition in conversation",
      "arxiv": "arXiv:2401.12987"
    }
  ]
}