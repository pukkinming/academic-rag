{
  "paper_id": "2108.04228v2",
  "title": "Iterative Distillation For Better Uncertainty Estimates In Multitask Emotion Recognition",
  "published": "2021-07-21T09:49:16Z",
  "authors": [
    "Didan Deng",
    "Liang Wu",
    "Bertram E. Shi"
  ],
  "keywords": [
    "0 1 Emotion Uncertainty Arousal: 0",
    "4 Valence: 0",
    "5 Happiness AU26: 0 AU25: 1 AU24: 0 AU23: 0 AU15: 1 AU12: 1 AU10: 1 AU7: 1 AU6: 1 AU4: 0 AU2: 0 AU1: 0 Facial Image Appendices A",
    "Data Distribution"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "When recognizing emotions, subtle nuances in displays of emotion generate ambiguity or uncertainty in emotion perception. Emotion uncertainty has been previously interpreted as inter-rater disagreement among multiple annotators. In this paper, we consider a more common and challenging scenario: modeling emotion uncertainty when only single emotion labels are available. From a Bayesian perspective, we propose to use deep ensembles to capture uncertainty for multiple emotion descriptors, i.e., action units, discrete expression labels and continuous descriptors. We further apply iterative self-distillation. Iterative distillation over multiple generations significantly improves performance in both emotion recognition and uncertainty estimation. Our method generates single student models that provide accurate estimates of uncertainty for in-domain samples and a student ensemble that can detect out-of-domain samples. Our experiments on emotion recognition and uncertainty estimation using the Aff-wild2 dataset demonstrate that our algorithm gives more reliable uncertainty estimates than both Temperature Scaling and Monte Carol Dropout.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Understanding human affective states is an essential task for many interactive systems (e.g., social robots) or data mining systems (e.g., user profiling). However, unlike object recognition tasks, emotion perception is strongly affected by personal bias, cultural backgrounds and contextual information (e.g., environment), which increases the uncertainty of emotion perception.\n\nTo obtain a gold standard for emotion recognition, it is common to invite a number of annotators and take the most-agreed emotions as hard labels in emotion datasets  [27, 33, 38, 36] . In datasets with huge number of samples  [29] , it is expensive to invite many annotators. Therefore, Figure  1 : An example facial image with classifications generated by our model. The classifications are shown in the labels in the left. The estimated uncertainty is shown in the bar graph in the right. For example, the emotion is classified as \"Happiness\". The estimated valence and arousal scores are 0.5 and 0.4. The presence or absence of each AU is indicated by 1 or 0. The emotion uncertainty is the normalized Shannon's entropy (i.e., divided by its information length). Large uncertainty indicates low confidence. For example, AU15 \"lip corner depressor\" is indicated as present, but with high uncertainty (low confidence). It is not present in the example image. each sample is often annotated by one expert only. Single emotion labels cannot capture inter-rater disagreement.\n\nPrevious work often related emotion ambiguity (uncertainty) with the variability among multiple raters' annotations. For example, Mower et al.  [30]  characterized ambiguity using probability distributions assigned to the emotion classes. Han et al.  [13]  defined emotion uncertainty as perception uncertainty (i.e., inter-rater disagreement). The solutions cannot be applied to emotion datasets with only single labels.\n\nTo address this problem, we adopt the Bayesian view-point and interpret emotion uncertainty as uncertainty in the posterior distribution over model weights. It is affected by the nosie, data distribution, and the model we choose. It does not require multiple raters' annotations, as required by the perception uncertainty.\n\nIn addition, we consider uncertainty simultaneously for multiple types of emotion labels (i.e., facial action units, basic emotions, valence and arousal), whereas past studies considered only single label types. Our intuition is that human affective states are quite complex, and should be described using a comprehensive set of emotional descriptors. Uncertainty among different emotion labels may be correlated. For example, the relationship between valence and arousal may be related to the uncertainty in perceived valence  [2] .\n\nThe recently released Aff-wild2  [22, 17]  facilitates multitask emotion solutions  [20, 24, 7] . The Aff-wild2 dataset has three types of emotion labels: facial action units, emotion categories, valence and arousal. Past emotion datasets  [27, 33, 38, 21]  usually have one or two types of emotion labels. However, Aff-wild2 dataset only provides single emotion labels, not multiple annotators' labels.\n\nUsing data from the Aff-wild2 dataset, we train deep ensembles with self-distillation algorithm to improve emotion recognition and uncertainty estimation. The obtained networks produce both emotions labels and the estimated uncertainty. The uncertainty is measured by Shannon's entropy computed over the probabilistic output. We give an example in Figure  1 , showing the outputs of our model given a facial image input.\n\nOur primary contributions are as follows:\n\n• For better uncertainty estimation performance, we propose to apply deep ensembles learned by multigenerational self-distillation. The iterative training of neural networks improves not only uncertainty estimation, but also multitask emotion recognition.\n\n• We design Efficient Multitask Emotion Networks (EMENet) for video emotion recognition. The visual model (EMENet-V) only has 1.68M parameters. The visual-audio (EMENet-VA) model has 1.91M parameters.\n\n• We show that single models can estimate uncertainty reliably on in-domain data, and that the ensembles can detect out-of-distribution (OOD) samples.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Uncertainty In Emotion",
      "text": "In emotion recognition, uncertainty often refers to perception uncertainty, in other words, inter-rater disagreement, requiring multiple annotators. Han et al.  [13]  took the standard deviation of K emotion labels given by K annotators as perception uncertainty. Zhang et al.  [37]  used Kappa coefficient to represent inter-rater agreement level. Uncertainty in emotion recognition has also been used to refer to the uncertainty in probabilistic models. A work in speech emotion recognition used a probabilistic Gaussian Mixture Regression (GMR) model to get the uncertainty of samples  [5] . The authors found the emotion model performs better in low-uncertainty regions than high-uncertainty regions. Dang et al.  [6]  also used probabilistic models, and applied uncertainty when fusing predictions from sub-systems of multiple modalities. These past methods relied on handcrafted features.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Uncertainty Estimation",
      "text": "Ensemble-based methods are alternatives to Bayesian methods for estimating decision uncertainty. A Deep Ensemble  [25]  consists of several neural networks with the same architecture, but their weights are initialized independently. From a Bayesian viewpoint, the learned weights are \"sampled\" from a posterior distribution. Deep ensembles have been shown to provide uncertainty estimates robust to dataset shifts  [32] . Similar to deep ensembles, the Monte Carol Dropout (MC Dropout)  [11]  is a Bayesian approximation method for estimating uncertainty. MC Dropout method uses dropout during both training and testing. During inference, a dropout model is sampled T times, and the T predictions are averaged. Temperature Scaling  [12]  (TS) is a post-hoc calibration method to improve uncertain estimation. It optimizes the temperature value of the softmax function on a held-out validation set. The advantage of TS is that it does not increase computation during inference, but it is prone to overfitting.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Knowledge Distillation",
      "text": "Knowledge Distillation  [14]  was firstly proposed by Hinton et al. for model compression. A special case of knowledge distillation is the self-distillation algorithm  [10] , where the student model has the same architecture as its teacher model. The student model usually outperforms its teacher model, as shown in  [10, 35] . The multi-generational selfdistillation algorithm uses the student model in the previous generation as the teacher model in the next generation. As the number of generations increases, generalization performance improves  [28] . Some studies have studied the reasons behind this phenomenon. For example, Mobahi et al.  [28]  proved mathematically that self-distillation amplifies regularization in the Hilbert space. Zhang et al.  [39]  related self-distillation to label smoothing, a commonlyused technique to prevent models from being over-confident  [31] . They suggested that the regularization effect of selfdistillation results from instance-level label smoothing. In this work, we aim to investigate the self-distillation for im-proving uncertainty performance by extending single models to deep ensembles.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Notations",
      "text": "We denote the training set as {X, Y }, where X denotes the input data and Y denotes the ground truth labels. The input data can be divided into two categories {X vis , X aud }. X vis represents the visual data and X aud represents the audio data. X vis contains facial images, where\n\nThe facial images are RGB images with a height (width) of H pixels. X aud contains mel spectrograms:\n\n. The mel spectrograms have two dimensions: the number of mel-filterbank features and the number of audio frames. They are both W in our experiments.\n\nThe ground labels Y can be divided into three types:\n\nY AU contains 12 facial action units labels, including AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23, AU24, AU25 and AU26. They are multi-label binary values, denoting the presence or absence of corresponding action unit. Y EXP R are one-hot vectors denoting 7 basic emotions: neutral, anger, disgust, fear, happiness, sadness and surprise. Y V A are given by continuous values representing valence and arousal in range {-1, 1}. In our experiments, we transform regression tasks into classification tasks by discretizing continuous values. We discretize the valence score or the arousal scores into 20 bins, so that the shape of Y V A changes to N × 40.\n\nThe single model function is denoted by f θ , where θ denotes the parameters. The ensemble model with T models is denoted by F T , where\n\nis an activation function. The output of the ensemble is the average of its members' outputs.\n\nIn our teacher-student algorithm, a teacher ensemble with T models can be denoted as F tea T . The soft labels generated by the teacher ensemble are denote as F tea T (X). The student ensemble in the k th generation is denoted as F stu k T . The soft labels generated by this ensemble for the k +1 generation is F stu k T (X). We run the self-distillation algorithm for K generations in total.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Architectures",
      "text": "We aim to design efficient architectures while maintaining high performance in emotion recognition. Previous studies  [7, 8, 23, 19]  on video emotion recognition showed that CNN-RNN architectures generally outperformed CNN architectures. This indicates that affective states have strong temporal dependencies. Therefore, we choose efficient CNN architectures as feature extractors, and use GRU layers as temporal models to integrate information over time.  For visual modality, the feature extractor is the MobileFaceNet, and visual feature vector is a 512-dimensional vector. The weights surrounded by dashed curves are only included in the multimodal model architecture. For mutlimodal model, the MarbleNet is the audio feature extractor, and the audio feature vector is a 128-dimensional vector. The visual feature vector and the audio feature vector are concatenated before they are fed into temporal models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mobilefacenet",
      "text": "For the visual modality, our model receives a sequence of facial images as input. The facial images are firstly processed by the MobileFaceNet  [4] . The MobileFaceNet is a light-weighed CNN originally designed for face recognition on mobile devices. The model was pretrained on face alignment task  [3] . We then finetuned the pretrained CNN weights. The feature vector is a 512-dimensional vector for each input image. The detailed architecture of the Mobile-FaceNet is given in  [4] .\n\nFor the visual-audio model, we used a 1D-CNN to extract audio features from mel spectrograms. The audio CNN (MarbleNet) was proposed by Jia et al.  [15]  for voice activity detection. It has only 88K parameters. The audio feature vector extracted from one mel spectrogram is a 128dimensional vector. The input is a 64×64 mel spectrogram, which is produced by extracting 64 mel-filterbank features from 64 audio frames (640ms). In our experiments, we sample a sequence of facial images as well as a sequence of mel spectrograms. The sample rate is same as the frame rate of the input video file. We denote the sequence length by L and the batch size by B. The shapes of inputs to our visual-audio model are (B, L, 3, 112, 112) for X vis and (B, L, 64, 64) for X aud . After the inputs are processed by feature extractors, the visual features and audio features are concatenated. This results in (B, L, 640) feature vectors that are fed into the temporal models.\n\nEach task has their own temporal model, which consists of one GRU layer, a ReLU activation function, and a linear output layer. The hidden sizes of all GRU layers are 128. We apply a 50% random dropout on the input features to the temporal models. For the final activation function, we use Sigmoid for the AU detection, and Softmax for 7 basic emotions, and valence/arousal prediction.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Loss Functions",
      "text": "To train teacher models, we minimize the loss functions between the teacher outputs and the ground labels. We refer to these losses as supervision losses. To train student models, we minimize the loss functions between the student outputs and the soft labels, which are generated by the teacher models or the student models in the previous generation. We refer to these loss functions as the distillation losses.\n\nSupervision Losses. For facial action units detection, we use a sum of class-reweighted binary cross entropy (BCE) functions. We reweight the losses using p c based on the ratios between positive samples and negative samples in the training data.\n\nVariables y denote ground truth labels and ỹ denote inferences generated by the teacher model. σ(•) in Equation  2 denotes the Sigmoid function. C is the total number of action units.\n\nFor basic emotion categories, we use a reweighted cross entropy (CE) function. The weights are determined by the distribution of different classes in the training set.\n\nFor valence/arousal predictions, we use the Concordance Correlation Coefficient (CCC) between the scalar outputs and the ground truth labels. The CCC is defined as follows:\n\nwhere y c denotes ground truth labels in a batch, and ỹc denotes the scalar predictions of valence or arousal. ρ is the correlation coefficient between the ground truth labels and the predictions. µ y , µ ỹ , σ y and σ ỹ are the means and standard deviations computed over the batch. Since our model produces a 20-dimensional softmax vector for valence/arousal, we compute the expectation values over the 20 bins in the range of [-1, 1] to transform probabilistic outputs to scalar outpus. We compute two CCCs: one for valence, and one for arousal. The supervision loss for valence and arousal prediction is:\n\nDistillation losses. For action units detection, we use the binary cross entropy between the soft labels and the outputs of the student models.\n\nwhere p c in Equation  9 is the same as p c in Equation  2 . y tea represents the soft labels.\n\nFor expression recognition, the distillation loss we use is the KL divergence (KLD) loss between the soft labels and the student outputs.\n\nFor valence and arousal prediction, we still use negative CCC loss between the soft labels and scalar outputs of the student model.\n\n(1 -CCC(y tea , ỹstu )),  (11)  where y tea and ỹstu are the scalar outputs of the teacher ensemble and the student model.\n\nCombination of losses. We take a weighted sum of the losses for different tasks. For example, when training teacher models, we use a combination of supervision losses for different emotion tasks:\n\nWhen training student models, we use a combination of distillation losses for all tasks.\n\nWhen training multiple tasks, it is important to balance the weights of different losses according to the difficulty levels of different tasks. We propose a heuristic method to balance the weights. The weight of the i th task's loss  depends on the number of epochs with no performance improvement on the validation set. If it is larger, we assign larger weights to this task's loss to increase its influence on the gradients. Appendix B shows the pseudo code for this heuristic method and ablation studies.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Algorithm",
      "text": "Our algorithm is a special case of the self-distillation algorithm. In the original self-distillation algorithm  [10, 28] , the teacher model and the student model are single models with the same architecture. In our algorithm, we propose to use deep ensembles for the following benefits:\n\n1. The deep ensembles can be naturally trained on a distributed system. The parallel computing facilities parallel training of each local model, which saves training time.\n\n2. The soft labels provided by the teacher ensemble contains more reliable uncertainty information than that provided by a single teacher model.\n\n3. We can use one single model or a few models in our student ensemble to perform emotion tasks, which brings more flexibility when it comes to computation cost.\n\nIn Figure  3 , we illustrate the teacher-student algorithm for deep ensembles. {X, Y } is the original dataset. Most of instances in {X, Y } only have one type of emotion labels, while other two types of emotion labels are missing. The t th teacher model f tea t learns to fill in the missing labels. In the training batch of {X, Y }, we sample an equal number of instances for three tasks, and then compute the loss in Equation  12 . Note that {f tea t } T t=1 are all trained on the same dataset {X, Y }, but start with different random initialization. After training {f tea t } T t=1 parallelly, we take average over their predictions on the training data. This generates the soft labels for the student models in the first generation. The soft labels are denoted as F tea T (X). In the first generation, student models f stu1 t are trained on {X, F tea T (X)}. After all student models are trained, we use the student ensemble to generate the soft labels for the next generation. We iterative the teacher-student training in order to find the best number of generations.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset",
      "text": "We only used the video data from the Aff-wild2  [22]  dataset. The Aff-wild2 dataset has three subsets, one for each emotion task. In each subset, the data distributions are quite unbalanced. Appendix A shows the data distributions for the three subsets. The data distributions determine the class weights p c in Equation 2 and 5. Appendix A also gives the choices of p c .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Hyper-Parameters",
      "text": "We used the Adam  [16]  optimizer. The learning rate was initialized as 1e -3 . For visual model training, we trained the model for 10 epochs, and decreased the learning rate by a factor of 10 after every 3 epochs. For multimodal training, we trained the models for 15 epochs and decreased the learning rate by a factor of 10 after every 4 epochs.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Metrics",
      "text": "Emotion metrics. We used the same evaluation metrics as suggested in  [18] . For facial AU detection, the evaluation metric is 0.5 • F1 + 0.5 • Acc, where F1 denotes the unweighted F1 score for all 12 AUs, and Acc denotes the total accuracy. For expression classification, we used 0.67 • F1 + 0.33 • Acc as the metric, where F1 denotes the unweighted F1 score for 7 classes, and Acc is the total accuracy. For valence and arousal, we evaluated them with CCC.\n\nUncertainty metric. Same to  [25] , we evaluated the  in-domain uncertainty estimation performance using the negative log-likelihood (NLL) for classification tasks (i.e., EXPR and AU) and root mean square error (RMSE) for regression tasks (i.e., valence and arousal). Lower NLL or RMSE means better uncertainty estimation performance. For out-of-domain uncertainty performance, we created an OOD detection task by importing non-facial images, and evaluated the binary classification performance using ROC (receiver operating characteristic) curves and AUC (area under the ROC curve) scores. Computation cost. We designed two model architectures for the visual modality and visual-audio modalities respectively. We refer to them as EMENet-V and EMENet-VA. The number of parameters and FLOPs for EMENet-V are 1.68M and 228M . For EMENet-VA, they are 1.91M and 234M . The FLOPs are the number of floating-point operations when the visual input is one RGB image (112x112) and audio input is one spectrogram (64x64).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Task Performance",
      "text": "Class reweighting. We show the effect of class reweighting in Table  1 . After applying class reweighting, we found the EXPR metric for single models was improved significantly, where the F1 score increased by 12.6%, and the accuracy of EXPR increased by 1.7%. Although the AU metric degraded after using class reweighting, its F1 score increased by 12.5%. The AU metric degraded because its accuracy dropped from 0.8947 to 0.8249. We think this is due to the highly unbalanced data distribution in the AU subset.\n\nEnsemble size. We changed the ensemble size when training teacher models without class reweighting. The results are reported in Table  1 . From single models (T = 1) to ensemble models (T = 5), the total emotion metric increased by 4%. However, from T = 10 to T = 5, the total emotion metric only increased by 0.58%. We kept the ensemble size T = 5 for the rest of experiments because of its relative efficiency.\n\nTeacher-Student training. We trained our teacher models and student models using our proposed algorithm (Figure  3 ). The total emotion metrics for both the teachers and students in multiple generations are shown in Figure  4 . Our first finding is that ensembles always outperform single models on the total emotion metric. As we increased the number of generations, the performance gap between the single models and ensembles became smaller. This is probably because the variability between models becomes smaller and smaller after more and more generations of selfdistillation.\n\nOur second finding on the results of EMENet-V is that the emotion performance does not increase monotonically as the number of generations increases. This is consistent with  [28] , where they interpreted increasing generations of   3 : The NLL values for 12 action units, which are evaluated on the validation set of the Aff-wild set. We compare our single teacher models and single student models with other methods, i.e., TS (temperature scaling  [12] ) and MC (Monte-Carol Dropout  [11] ). The model architecture used in this comparison is EMENet-V. self-distillation as amplifying regularization. The best number of generations for EMENet-V was three. More generations added too much regularization, leading to poorer performance on the validation set. We found the same phenomenon in the results of EMENet-VA. The best number of generations was also three. We evaluated our models on the test set of the Aff-wild2. The results are listed in Table  2 . The visual-audio models always have better performance than visual models, although with slightly larger computation cost.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Uncertainty Performance",
      "text": "There are two types of uncertainty we are interest in: the aleatoric uncertainty and the epistemic uncertainty. The aleatoric uncertainty arises from the natural complexities of the underlying distribution, such as class overlap, label noise, input data noise, etc. The epistemic uncertainty arises from a lack of knowledge about the best model parameters. It can be explained away given enough data in that region. The latter uncertainty is an indicator of out-of-distribution (OOD) samples.\n\nWe capture the two types of uncertainty with deep en-sembles. Following  [9]  and  [26] , we compute the two types of uncertainty as follows:\n\nP (y) is the probabilistic distribution output of a single model. H is the Shannon's entropy. p(θ|D) is the posterior distribution of the model weights θ which is trained on dataset D.\n\nIn-domain uncertainty. The aleatoric uncertainty is an indicator of noise inherent in in-domain data. It is computed as the average entropy of single models' outputs in an ensemble, as shown in Equation  14 . To evaluate the performance of in-domain uncertainty, we computed the average NLL (or RMSE) for single models to measure the similarity between predicted probability distributions and the true probability distributions.\n\nTo evaluate AU uncertainty estimation performance, we computed the NLL values on the AU validation set. Besides the NLL values of our single teacher models and single student models, the NLLs values of Temperature scaling (TS)  [12]  and Monte-Carol Dropout (MC Dropout)  [11]  were also computed. For a fair comparison, we adopted the test-time cross-validation in  [1]  to compute the NLL in TS. The optimal temperature was optimized on a randomlysplit half of the validation set. The NLL was then evaluated on the other half of the validation set. For MC Dropout, we averaged the probability outputs of ten forward passes, where the model weights were sampled randomly for every forward pass from the dropout.\n\nTable  3  shows the NLL values of single models, where the model architecture is EMENet-V. The validation performance for the EMENet-VA architecture is given in Appendix C. Table  3  shows that the single student models in later generations (i.e., 2, 3 and 4) have better uncertainty estimation performance than TS and MC Dropout. When it comes to the averaged NLL values, our method outperforms TS by 10.8% and MC Dropout by 9.5%.\n\nIn Table  4 , we list the uncertainty metrics for facial expressions (EXPR), valence and arousal. We find that the single student models have better uncertainty performance than TS and MC Dropout for both tasks. Our algorithm improves the EXPR NLL by 10.3% when compared with TS and 15.5% when compared with MC Dropout. As for valence/arousal RMSE scores, our method outperforms MC Dropout by 7.0%/8.0%.\n\nOut-of-domain Uncertainty. Epistemic uncertainty is an indicator of data insufficiency in certain regions of the input space. We expect that out-of-domain samples will be associated with large epistemic uncertainty. The epistemic uncertainty is computed by subtracting the aleatoric uncertainty from the total uncertainty.\n\nTo evaluate the performance of the epistemic uncertainty computed by our models for detecting out-of-domain samples, we chose the Fashion-MNIST  [34]  training set as OOD samples. It contains 60,000 gray-scale images with objects like bags, shirts, trousers, etc. The Aff-wild2 validation set has over 500,000 images. By mixing the Aff-wild2 validation set with the the Fashion-MNIST training set, we created an OOD detection task. We used the ensembles to compute the epistemic uncertainty of each sample. Since we performed multitask emotion prediction, the ensemble model produced an epistemic uncertainty value for each task. We averaged the epistemic uncertainties over all emotion tasks, and used a threshold to classify samples into in-domain and out-of-domain samples. We plotted the ROC curves showing the OOD performance across all thresholds. The ROC curves computed on the averaged epistemic uncertainty are shown in Figure  5 . Each ROC curve corresponds to the ensemble in one generation. The AUC scores are also computed to show the OOD performance. Higher AUC scores indicate more accurate differentiation between in-domain and out-of-domain samples.\n\nFrom Figure  5 , we had the best AUC score for the student ensemble in the second generation. The AUC scores show a unimodal relationship with the number of generations. As we distill more and more, the uncertainty performance gradually increases to its best value, then decreases.\n\nSummary. We find that self-distillation improves both types of uncertainty. The epistemic uncertainty produced by our ensembles can detect out-of-domain samples accurately. The best AUC score was 0.898. When estimating in-domain uncertainty, ensembling is not required. If the computation resources are limited, it is acceptable to use the probability outputs of a single model to compute the entropy: as an approximation to the aleatoric uncertainty computed from an ensemble's output (Equation  14 ).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose to apply deep ensemble models learned by a multi-generational self-distillation algorithm to improve emotion uncertainty estimation. Our designed model architectures are efficient, and can be potentially applied in mobile devices. Our experimental results show that our algorithm can improve both the emotion metrics and uncertainty metrics as the number of generations increases. The uncertainty estimates given by our models are reliable indicators of in-domain and out-of-domain samples. In the future, we will study the regularization effect of the self-distillation algorithm, and seek better regularization methods to replace the time-consuming progress of self-distillation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Acknowledgements",
      "text": "This work was supported in part by the Hong Kong Research Grants Council under grant number 16213617.   The p c for class reweighting depends on the data distributions.\n\nFrom AU1 to AU26, p = [7.7, 24.7, 5.3, 2.9, 1.5, 1.9, 3, 32.3, 32.3, 32.3, 0.59, 11.5] to alleviate the unbalanced data problem. For the EXPR subset, p = [0.02, 0.2, 0.33, 0.24, 0.03, 0.05, 0.1] in Equation  5 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Multitask Balancing",
      "text": "Our multitask balancing algorithm is given as follows: The number of epochs with no performance improvement M = {m AU , m EXP R , m V A .\n\nThe weights for all tasks Λ = {λ AU , λ EXP R , λ V A }.\n\nThe number of training epochs M .\n\n1: procedure 2:\n\nwhile i < n(T ) do 3:\n\nOptimize f on D train .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "7:",
      "text": "Evaluate f on D val .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "8:",
      "text": "while i < n(T ) do 9:\n\nV al i ← validation performance of the i th task.\n\nwhile i < n(T ) do 15:\n\n16:\n\nwhile i < n(T ) do  T is the number of models in an ensemble. Total emotion metric is the sum of all metrics of the three emotion tasks.\n\nThe main idea of this algorithm is to increase the weight of certain task if this task has not been improved on the validation set for a number of epochs. Once this task has been improved, the weight of the loss function for this task is set to its initial value.\n\nWe conducted ablation studies on the effect of Algorithm 1. The model we used is the EMENet-V trained on original dataset. From the experiment results in The NLL values for 12 action units, which are evaluated on the validation set of the Aff-wild set. We compare our single teacher models and single student models with other methods, i.e., TS (temperature scaling  [12] ) and MC (Monte-Carol Dropout  [11] ). The model architecture used in this comparison is EMENet-VA. better when using multitask balancing algorithm. We value the performance of each emotion tasks equally. The Algorithm 1 was used in all other experiments for better total emotion metric.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "C. In-Domain Uncertainty For Emenet-Va",
      "text": "We show the AU uncertainty performance using the EMENet-VA in Table  6 . We also compare our single teacher models and single student models with Temerature Scaling (TS) and Monte-Carol (MC) Dropout. Similar to the results in Table  3  (EMENet-V), the models using our algorithm achieved the lowest NLL value, compared with TS and MC Dropout. The lowest AVg. NLL value is 0.348 for EMENet-VA, while for EMENet-V, the lowest average NLL value is 0.381. We find that when using audio features with visual features, the uncertainty (NLL) of facial actions can be improved by about 8.7%.\n\nTable  7  shows the uncertainty performance for the EXPR task, valence and arousal detection. We evaluated NLL for classification tasks and RMSE for regression tasks. Comparing Table  7  with Table  4 , we find that incorporating audio features with visual features, it improved the EXPR NLL by 5.2%. However, it failed to improve the valence RMSE, and barely had an influence on the arousal RMSE.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example facial image with classifications gen-",
      "page": 1
    },
    {
      "caption": "Figure 1: , showing the outputs of our model",
      "page": 2
    },
    {
      "caption": "Figure 2: Our efficient model architecture.",
      "page": 3
    },
    {
      "caption": "Figure 3: The diagram of our teacher-student algorithm.",
      "page": 5
    },
    {
      "caption": "Figure 3: , we illustrate the teacher-student algorithm",
      "page": 5
    },
    {
      "caption": "Figure 4: The total emotion metrics for the visual model",
      "page": 6
    },
    {
      "caption": "Figure 4: Our first finding is that ensembles always outperform",
      "page": 6
    },
    {
      "caption": "Figure 5: Each ROC curve corre-",
      "page": 8
    },
    {
      "caption": "Figure 5: , we had the best AUC score for the stu-",
      "page": 8
    },
    {
      "caption": "Figure 5: The ROC curves and AUC scores for the OOD",
      "page": 8
    },
    {
      "caption": "Figure 6: The data distributions of the Aff-wild2 dataset.",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 2: The emotion metrics on the test set of the Aff-",
      "data": [
        {
          "Experiments": "",
          "T": "",
          "AU": "",
          "EXPR": "",
          "VA\nTotal Emotion": "Valence"
        },
        {
          "Experiments": "w/o re.\nw/o re.\nw/o re.\nw/ re.\nw/ re.",
          "T": "1\n5\n10\n1\n5",
          "AU": "0.6773\n0.6858\n0.6843\n0.6632\n0.6808",
          "EXPR": "0.5128\n0.5354\n0.5449\n0.5541\n0.5779",
          "VA\nTotal Emotion": "0.3830\n0.4099\n0.4105\n0.4202\n0.4423"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: The NLL values for EXPR recognition and the",
      "data": [
        {
          "Methods": "Tea\nStu1\nStu2\nStu3\nStu4\nTS [12]\nMC [11]",
          "AU1": "0.407\n0.366\n0.358\n0.351\n0.338\n0.405\n0.408",
          "AU2": "0.348\n0.326\n0.319\n0.312\n0.301\n0.345\n0.350",
          "AU4": "0.424\n0.354\n0.355\n0.369\n0.377\n0.420\n0.455",
          "AU6": "0.435\n0.387\n0.388\n0.395\n0.401\n0.435\n0.429",
          "AU7": "0.477\n0.471\n0.468\n0.479\n0.482\n0.476\n0.472",
          "AU10": "0.437\n0.427\n0.424\n0.435\n0.440\n0.435\n0.422",
          "AU12": "0.380\n0.379\n0.383\n0.400\n0.421\n0.379\n0.362",
          "AU15": "0.389\n0.322\n0.314\n0.310\n0.322\n0.388\n0.357",
          "AU23": "0.464\n0.388\n0.367\n0.353\n0.347\n0.462\n0.402",
          "AU24": "0.459\n0.328\n0.303\n0.287\n0.271\n0.449\n0.499",
          "AU25": "0.491\n0.482\n0.487\n0.488\n0.490\n0.491\n0.485",
          "AU26": "0.442\n0.413\n0.400\n0.398\n0.386\n0.441\n0.414",
          "Avg.": "0.430\n0.387\n0.381\n0.381\n0.381\n0.427\n0.421"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: The NLL values for EXPR recognition and the",
      "data": [
        {
          "Methods": "Tea\nStu1\nStu2\nStu3\nStu4\nTS [12]\nMC [11]",
          "EXPR\nNLL": "1.052\n0.911\n0.905\n0.918\n0.957\n0.998\n1.071",
          "Valence\nRMSE": "0.400\n0.379\n0.377\n0.373\n0.370\n-\n0.398",
          "Arousal\nRMSE": "0.256\n0.234\n0.231\n0.232\n0.233\n-\n0.251"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Experiments": "w/o ba.\nw/o ba.\nw/ ba.\nw/ ba.",
          "T": "1\n5\n1\n5",
          "AU": "0.6307\n0.6487\n0.6632\n0.6808",
          "EXPR": "0.5620\n0.5802\n0.5541\n0.5779",
          "VA\nTotal Emotion\nValence\nArousal": "0.3902\n0.5344\n2.1173\n0.5585\n0.4221\n2.2095\n0.4202\n0.5192\n2.1527\n0.4423\n2.2465\n0.5455"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 7: The NLL values for EXPR recognition and the",
      "data": [
        {
          "Methods": "Tea\nStu1\nStu2\nStu3\nTS [12]\nMC [11]",
          "AU1": "0.380\n0.326\n0.320\n0.308\n0.380\n0.356",
          "AU2": "0.302\n0.264\n0.261\n0.255\n0.301\n0.274",
          "AU4": "0.427\n0.366\n0.368\n0.359\n0.415\n0.419",
          "AU6": "0.407\n0.381\n0.384\n0.388\n0.406\n0.411",
          "AU7": "0.472\n0.459\n0.465\n0.469\n0.472\n0.473",
          "AU10": "0.422\n0.412\n0.419\n0.427\n0.421\n0.416",
          "AU12": "0.352\n0.355\n0.366\n0.383\n0.351\n0.351",
          "AU15": "0.307\n0.256\n0.256\n0.262\n0.306\n0.311",
          "AU23": "0.378\n0.289\n0.280\n0.276\n0.377\n0.345",
          "AU24": "0.394\n0.256\n0.251\n0.233\n0.381\n0.384",
          "AU25": "0.466\n0.458\n0.467\n0.474\n0.465\n0.463",
          "AU26": "0.401\n0.359\n0.352\n0.342\n0.400\n0.382",
          "Avg.": "0.392\n0.348\n0.349\n0.348\n0.390\n0.382"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 7: The NLL values for EXPR recognition and the",
      "data": [
        {
          "Methods": "Tea\nStu1\nStu2\nStu3\nTS [12]\nMC [11]",
          "EXPR\nNLL": "1.060\n0.825\n0.846\n0.858\n0.955\n0.994",
          "Valence\nRMSE": "0.416\n0.397\n0.392\n0.383\n-\n0.416",
          "Arousal\nRMSE": "0.240\n0.233\n0.231\n0.230\n-\n0.237"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Pitfalls of in-domain uncertainty estimation and ensembling in deep learning",
      "authors": [
        "Arsenii Ashukha",
        "Alexander Lyzhov",
        "Dmitry Molchanov",
        "Dmitry Vetrov"
      ],
      "year": "2020",
      "venue": "Pitfalls of in-domain uncertainty estimation and ensembling in deep learning",
      "arxiv": "arXiv:2002.06470"
    },
    {
      "citation_id": "2",
      "title": "The emotional-ambiguity hypothesis: A largescale test",
      "authors": [
        "Cj Brainerd"
      ],
      "year": "2018",
      "venue": "Psychological science"
    },
    {
      "citation_id": "3",
      "title": "Pytorch face landmark: A fast and accurate facial landmark detector",
      "authors": [
        "Cunjian Chen"
      ],
      "venue": "Pytorch face landmark: A fast and accurate facial landmark detector"
    },
    {
      "citation_id": "4",
      "title": "Mobilefacenets: Efficient cnns for accurate real-time face verifica-tion on mobile devices",
      "authors": [
        "Sheng Chen",
        "Yang Liu",
        "Xiang Gao",
        "Zhen Han"
      ],
      "year": "2018",
      "venue": "Chinese Conference on Biometric Recognition"
    },
    {
      "citation_id": "5",
      "title": "An investigation of emotion prediction uncertainty using gaussian mixture regression",
      "authors": [
        "Ting Dang",
        "Vidhyasaharan Sethu",
        "Julien Epps",
        "Eliathamby Ambikairajah"
      ],
      "year": "2017",
      "venue": "An investigation of emotion prediction uncertainty using gaussian mixture regression"
    },
    {
      "citation_id": "6",
      "title": "Investigating word affect features and fusion of probabilistic predictions incorporating uncertainty in avec 2017",
      "authors": [
        "Ting Dang",
        "Brian Stasak",
        "Zhaocheng Huang",
        "Sadari Jayawardena",
        "Mia Atcheson",
        "Munawar Hayat",
        "Phu Le",
        "Vidhyasaharan Sethu",
        "Roland Goecke",
        "Julien Epps"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "7",
      "title": "Multitask emotion recognition with incomplete labels",
      "authors": [
        "Didan Deng",
        "Zhaokang Chen",
        "Bertram Shi"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "8",
      "title": "Mimamo net: Integrating micro-and macro-motion for video emotion recognition",
      "authors": [
        "Didan Deng",
        "Zhaokang Chen",
        "Yuqian Zhou",
        "Bertram Shi"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "9",
      "title": "Decomposition of uncertainty in bayesian deep learning for efficient and risksensitive learning",
      "authors": [
        "Stefan Depeweg",
        "Jose-Miguel Hernandez-Lobato",
        "Finale Doshi-Velez",
        "Steffen Udluft"
      ],
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "10",
      "title": "Born again neural networks",
      "authors": [
        "Tommaso Furlanello",
        "Zachary Lipton",
        "Michael Tschannen",
        "Laurent Itti",
        "Anima Anandkumar"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "11",
      "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
      "authors": [
        "Yarin Gal",
        "Zoubin Ghahramani"
      ],
      "year": "2007",
      "venue": "international conference on machine learning"
    },
    {
      "citation_id": "12",
      "title": "On calibration of modern neural networks",
      "authors": [
        "Chuan Guo",
        "Geoff Pleiss",
        "Yu Sun",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "13",
      "title": "From hard to soft: Towards more humanlike emotion recognition by modelling the perception uncertainty",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "Maximilian Schmitt",
        "Maja Pantic",
        "Björn Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM international conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "15",
      "title": "Marblenet: Deep 1d time-channel separable convolutional neural network for voice activity detection",
      "authors": [
        "Fei Jia",
        "Somshubra Majumdar",
        "Boris Ginsburg"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "16",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "17",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Irene Kotsia",
        "Elnar Hajiyev",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Analysing affective behavior in the second abaw2 competition",
      "arxiv": "arXiv:2106.15318"
    },
    {
      "citation_id": "18",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Dimitrios Kollias",
        "Attila Schulc",
        "Elnar Hajiyev",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "19",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "20",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "21",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "22",
      "title": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2018",
      "venue": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "arxiv": "arXiv:1811.07770"
    },
    {
      "citation_id": "23",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "24",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "25",
      "title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
      "authors": [
        "Alexander Balaji Lakshminarayanan",
        "Charles Pritzel",
        "Blundell"
      ],
      "year": "2005",
      "venue": "Simple and scalable predictive uncertainty estimation using deep ensembles",
      "arxiv": "arXiv:1612.01474"
    },
    {
      "citation_id": "26",
      "title": "",
      "authors": [
        "Andrey Malinin",
        "Bruno Mlodozeniec",
        "Mark Gales"
      ],
      "year": "2019",
      "venue": "",
      "arxiv": "arXiv:1905.00076"
    },
    {
      "citation_id": "27",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "Mohammad Mohammad Mavadati",
        "Kevin Mahoor",
        "Philip Bartlett",
        "Jeffrey Trinh",
        "Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Self-distillation amplifies regularization in hilbert space",
      "authors": [
        "Hossein Mobahi",
        "Mehrdad Farajtabar",
        "Peter Bartlett"
      ],
      "year": "2006",
      "venue": "Self-distillation amplifies regularization in hilbert space",
      "arxiv": "arXiv:2002.05715"
    },
    {
      "citation_id": "29",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Interpreting ambiguous emotional expressions",
      "authors": [
        "Emily Mower",
        "Angeliki Metallinou",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Carlos Busso",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "31",
      "title": "When does label smoothing help? arXiv preprint",
      "authors": [
        "Rafael Müller",
        "Simon Kornblith",
        "Geoffrey Hinton"
      ],
      "year": "2019",
      "venue": "When does label smoothing help? arXiv preprint",
      "arxiv": "arXiv:1906.02629"
    },
    {
      "citation_id": "32",
      "title": "Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift",
      "authors": [
        "Yaniv Ovadia",
        "Emily Fertig",
        "Jie Ren",
        "Zachary Nado",
        "David Sculley",
        "Sebastian Nowozin",
        "Joshua Dillon",
        "Balaji Lakshminarayanan",
        "Jasper Snoek"
      ],
      "year": "2019",
      "venue": "Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift",
      "arxiv": "arXiv:1906.02530"
    },
    {
      "citation_id": "33",
      "title": "Fera 2015-second facial expression recognition and analysis challenge",
      "authors": [
        "Timur Michel F Valstar",
        "Jeffrey Almaev",
        "Gary Girard",
        "Marc Mckeown",
        "Lijun Mehu",
        "Maja Yin",
        "Jeffrey Pantic",
        "Cohn"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "34",
      "title": "Fashionmnist: a novel image dataset for benchmarking machine learning algorithms",
      "authors": [
        "Han Xiao",
        "Kashif Rasul",
        "Roland Vollgraf"
      ],
      "year": "2017",
      "venue": "Fashionmnist: a novel image dataset for benchmarking machine learning algorithms",
      "arxiv": "arXiv:1708.07747"
    },
    {
      "citation_id": "35",
      "title": "Self-training with noisy student improves imagenet classification",
      "authors": [
        "Qizhe Xie",
        "Minh-Thang Luong",
        "Eduard Hovy",
        "Quoc V Le"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "36",
      "title": "Aff-wild: valence and arousal'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "37",
      "title": "An agreement and sparseness-based learning instance selection and its application to subjective speech phenomena",
      "authors": [
        "Zixing Zhang",
        "Florian Eyben",
        "Jun Deng",
        "Björn Schuller"
      ],
      "year": "2014",
      "venue": "Proceedings of the 5th International Workshop on Emotion Social Signals, Sentiment & Linked Open Data (ES3LOD 2014), satellite of the 9th Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "38",
      "title": "From facial expression recognition to interpersonal relation prediction",
      "authors": [
        "Zhanpeng Zhang",
        "Ping Luo",
        "Chen Loy",
        "Xiaoou Tang"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "39",
      "title": "Self-distillation as instance-specific label smoothing",
      "authors": [
        "Zhilu Zhang",
        "R Mert",
        "Sabuncu"
      ],
      "year": "2020",
      "venue": "Self-distillation as instance-specific label smoothing",
      "arxiv": "arXiv:2006.05065"
    }
  ]
}