{
  "paper_id": "2505.23631v3",
  "title": "Human Empathy As Encoder: Ai-Assisted Depression Assessment In Special Education",
  "published": "2025-05-29T16:37:15Z",
  "authors": [
    "Boning Zhao",
    "Xinnuo Li",
    "Yutong Hu"
  ],
  "keywords": [
    "Human-AI Collaboration",
    "Depression Assessment",
    "Empathy Encoding",
    "Responsible AI"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Assessing student depression in sensitive environments like special education is challenging. Standardized questionnaires may not fully reflect students' true situations. Furthermore, automated methods often falter with rich student narratives, lacking the crucial, individualized insights stemming from teachers' empathetic connections with students. Existing methods often fail to address this ambiguity or effectively integrate educator understanding. To address these limitations by fostering a synergistic human-AI collaboration, this paper introduces Human Empathy as Encoder (HEAE), a novel, humancentered AI framework for transparent and socially responsible depression severity assessment. Our approach uniquely integrates student narrative text with a teacher-derived, 9-dimensional \"Empathy Vector\" (EV), its dimensions guided by the PHQ-9 framework,to explicitly translate tacit empathetic insight into a structured AI input enhancing rather than replacing human judgment. Rigorous experiments optimized the multimodal fusion, text representation, and classification architecture, achieving 82.05% ± 0.58% accuracy via 5-fold cross-validation for 7-level severity classification. This work demonstrates a path toward more responsible and ethical affective computing by structurally embedding human empathy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The motivation for this research stems from firsthand observations in special education settings, where supporting student mental well-being, particularly regarding depression, is paramount. These environments, with their typically low student-to-teacher ratios, foster deep teacher familiarity with individual needs and contexts. However, depression assessment here presents unique challenges: standardized self-report questionnaires like PHQ-9  [1]  often fail to capture students' authentic emotional states, while their natural conversations and writings can reveal richer insights. This creates a critical tension-educators possess invaluable empathy and contextual understanding from sustained interactions, yet without extensive psychological training, they struggle to systematically interpret vague narrative texts for signs of depression.\n\nThe challenge of interpreting student narratives is further compounded when considering purely automated approaches. Existing affective computing models often struggle with the nuances of expressed mental states like depression, potentially oversimplifying the assessment  [2] . Moreover, the growing adoption of large language models, especially cloud-based services, raises significant privacy concerns regarding sensitive student data  [3] . Thus, the limitations of standalone human interpretation and the shortcomings of current AI tools collectively demand a new generation of assessment solutions grounded in social responsibility and synergistic human-AI collaboration  [4]  To navigate these complex challenges, this paper introduces the \"Human Empathy as Encoder (HEAE)\" framework, a novel human-centered AI paradigm. HEAE fundamentally reorients depression assessment by structurally integrating teachers' tacit empathetic insights and experiential understanding directly into the AI analysis process. This approach complements and amplifies human judgment rather than attempting to replace it, fosters greater transparency compared to opaque automated systems and captures individualized nuances often missed by standardized questionnaires. Our work addresses implementation challenges through an innovative LLM-assisted annotation pipeline leveraging \"golden seeds\"-expert-annotated examples, to produce high-quality, nuanced training data for the HEAE model. Architecturally, HEAE embodies the design principle of \"structured simplicity + targeted complexity,\" which strategically allocates model sophistication where it most benefits human-AI collaboration. Beyond achieving 82.05% ± 0.58% accuracy via 5-fold crossvalidation accuracy in 7-level depression severity classification, this work provides both a privacy-preserving assessment tool for educational settings and a broader template for developing responsible, human-centered affective computing systems.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work A. Natural Language Processing For Depression Detection",
      "text": "The use of Natural Language Processing (NLP) to detect depression from text data has become a significant area of research. Researchers leverage techniques like sentiment analysis, linguistic feature extraction, and increasingly, deep learning models to analyze text from sources such as social media posts, web forums, and electronic health records  [5] . Deep learning architectures, including transformer-based models like RoBERTa  [6]  and sequence models like BiLSTM  [7] , have shown considerable success in capturing linguistic cues associated with depression, often achieving high accuracy in distinguishing between depressed and non-depressed individuals  [8] . However, significant challenges remain. While deep learning models are increasingly applied to classify depression severity or subtypes based on linguistic patterns, accurately capturing these fine-grained distinctions often proves difficult, especially when models rely heavily on surface-level features like keyword frequencies or sentiment scores  [9] . Furthermore, these models frequently struggle with other nuanced emotional information, such as masked distress or ambiguous suicidal cues  [10] . Ethical considerations regarding data privacy and bias in NLP models are also paramount  [11] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Multimodal Approaches In Mental Health Assessment",
      "text": "Recognizing that mental health states manifest through various channels, multimodal approaches aim to achieve more comprehensive, robust, and accurate assessments by integrating complementary information from diverse sources like text, audio, and visual data.  [12] ,  [13] .\n\nHowever, many current multimodal approaches, particularly in high-context settings like special education, tend to rely heavily on sensor-derived or readily quantifiable data. This focus can lead to overlooking the rich background information and nuanced judgments possessed by human experts (such as teachers) who are familiar with the individuals  [14] . Furthermore, even when diverse data streams are considered, including potentially qualitative human insights, Multimodal Machine Learning (MMML) techniques face ongoing challenges. Effectively aligning and fusing such heterogeneous data types, and developing models that can meaningfully interpret these complex combined signals, remain significant technical hurdles in the field  [15] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Human-In-The-Loop (Hitl) And Human-Ai Collaboration",
      "text": "In sensitive domains like healthcare and education, the concept of Human-in-the-Loop (HITL) AI and, more broadly, Human-AI Collaboration is gaining traction. These approaches intentionally incorporate human expertise into the AI workflow-which can involve humans providing labels, verifying AI predictions, or taking actions based on AI suggestions-with goals that often include improving accuracy, handling edge cases, ensuring alignment with human values, and maintaining human oversight over critical decisions  [16] ,  [17] ,  [18] ,  [19] . However, while these general HITL and collaborative frameworks are broadly applied, effectively eliciting, representing, and incorporating deep, contextual, and empathetic understanding from domain experts remains a significant challenge, particularly for nuanced tasks such as interpreting student narratives in special education settings  [20] . This highlights an ongoing opportunity to develop more specialized mechanisms for such integration. Such humancentered collaborative systems contrast with AI designed to operate autonomously or replace human professionals entirely.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Explainability, Ethics, And Responsibility In Affective Computing",
      "text": "As affective computing systems become more capable, ethical considerations-including data privacy, potential biases in emotion recognition across different demographics, the risk of emotional manipulation, and the need for user consent-become crucial  [21] ,  [22] . The \"black box\" nature of many complex AI models raises concerns about transparency and accountability, which Explainable AI (XAI) seeks to address by making AI decision-making processes more understandable to humans  [23] ,  [24] . Within mental health, principles of responsible AI emphasize human dignity, avoiding manipulation, preventing bias, ensuring human supervision over critical decisions, building trust, and showing empathy; consequently, there is a growing call for affective computing systems that are not only accurate but also transparent, fair, accountable, and ethically deployed  [25] ,  [22] . However, fully achieving these ideals and ensuring that AI systems, particularly in affective computing, are truly transparent and reliably align with human values and oversight remains an ongoing challenge.  [4]",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "E. Positioning The Current Work",
      "text": "Our work builds upon these areas while proposing a novel approach distinct from existing methods. While we leverage NLP and operate within a multimodal framework, our second modality is unique: the structured EV, which is derived from special education teachers applying their empathetic insights and contextual understanding to student narratives, using PHQ-9 dimensions as a structuring framework. This differs significantly from typical multimodal approaches that use raw audio/visual data  [12] ,  [13] . Our HEAE framework represents a specific instantiation of Human-AI Collaboration  [18] , going beyond standard HITL  [17]  by structurally encoding teacher's empathetic insights a core input signal, designed explicitly to augment teacher insight. This design inherently promotes transparency by making the human contribution explicit, addressing ethical concerns regarding black-box AI  [23]  in sensitive assessments, aligning with the objectives of developing reliable, safe, and trustworthy human-Centered AI  [26] . By structurally embedding empathy into AI systems, our work not only demonstrates an effective assessment approach but also offers foundational insights for designing more socially responsible, transparent, and ultimately more trustworthy affective computing systems for real-world educational contexts.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "This section details our HEAE framework, the dataset creation process, the optimized model architecture, and the experimental setup.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Conceptual Framework: The Human Empathy As Encoder",
      "text": "The HEAE framework structurally encodes teachers' tacit empathetic insights and experiential understanding into the depression assessment process, addressing limitations of both standardized questionnaires and automated text analysis. In our approach, special education teachers analyze student narratives through the lens of PHQ-9's nine dimensions, converting their contextual understanding into a quantified EV. Unlike conventional PHQ-9 administration, this dimension-focused method accommodates the fragmentary nature of emotional signals in natural writing and student narratives, where not all depression indicators may be explicitly present.\n\nTeachers assign scores using a custom 0-5 severity scale (0=Not present, 5=Extreme impact) for each of these nine dimensions, rather than the standard 0-3 scale, providing the necessary sensitivity for our target 7-level classification  1  . The resulting EV serves alongside the raw narrative as dual model input, creating an inherently transparent process that preserves individualized insights while making them algorithmically actionable.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Dataset Generation And Features",
      "text": "To create a robust dataset for training our model, we developed a novel annotation pipeline using public narratives from social media.\n\n1) Golden Seed Development: To establish a high-quality foundation and encapsulate our target annotation logic, we created a set of initial examples termed \"golden seeds.\" Raw narratives were sourced from Reddit forums, with all useridentifying information removed.\n\nThese golden seeds underwent manual annotation by experts with substantial experience in special education settings. This crucial step defined two key components.First, we established a machine-interpretable mapping between narrative content and scores across nine EV dimensions using the previously described 0-5 severity scale for each dimension, without calculating a total score. Second, these expert-generated EV scores were converted into a final 7-level depression severity classification. To ensure this EV-to-severity conversion method aligns with established understanding, its consistency with the PHQ-9's approach to categorizing severity was validated on all 200 golden seeds, yielding a Cohen's Kappa of 0.705, indicating substantial inter-rater agreement, and an overall accuracy of 75.5%. Each golden seed subsequently included the narrative text, its expert-generated 9-dimensional EV, the resulting final 7-level severity label, and detailed explanations justifying the EV scores, thus encapsulating the target reasoning process.\n\n2) Novel Automated Annotation Pipeline: To efficiently scale our dataset while maintaining consistent interpretation patterns, we developed an LLM-assisted annotation pipeline (Figure  1 ). From the full set of 200 validated golden seeds, we carefully selected 20 diverse examples to serve as fewshot demonstrations for the LLM. This number was chosen to provide a rich variety of narrative styles and severity levels, while respecting the input token limitations of the language model. To further guide the LLM and ensure annotation consistency, the prompt was supplemented with explicit details regarding the Empathy Vector (EV) structure and our validated EV-to-severity conversion method, alongside instructions for the LLM to adopt a persona reflecting expertise in empathetic mental well-being assessment in educational contexts.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Raw Data (From Public) Llm(Api) (Deepseek R1) Golden Seed Example Prompt (Ev & Conversion)",
      "text": "Labeled Data (with Explanation) Fig.  1 . Automated annotation pipeline using golden seeds and an LLM.\n\n3) Dataset Finalization and Characteristics: This annotation pipeline initially yielded approximately 30,000 labeled samples. After balancing various levels of depression to address skewness often found in online data which commonly overrepresented severe cases, making intermediate levels like moderate depression relatively harder to source sufficiently, we obtained approximately 17,500 samples for model development.\n\n4) Model Inputs, Target: The model utilizes two primary inputs: (i) Narrative Text, and (ii) EV (9-dim, 0-5 scale from teacher interpretation). The classification target is the 7-level depression severity assigned during data generation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Final Model Architecture",
      "text": "The final model architecture (Figure  2 ) integrates the text and EV modalities using components optimized through experiments detailed in Section IV.\n\n1) Text Encoder: We utilize a pre-trained RoBERTa model. The bottom 8 layers were frozen, while the top 4 layers were fine-tuned.\n\n2) Text Representation: A sliding window (512 token chunks, 256 stride) handles long narratives. Chunk embeddings are aggregated into a single document representation using Max Pooling, which effectively captures peak emotional signals, followed by Layer Normalization.\n\n3) Empathy Vector Processing: The 9-dimensional EV is projected into a higher-dimensional space suitable for fusion using two linear layers (transforming dimensions 9 → 64 → 128) with GELU activation.\n\n4) Multimodal Fusion: Our Asymmetric Cross-Modal Enhancement (ACME) mechanism is designed to effectively fuse the narrative text and EV modalities. It employs a controlled, gated interaction with fixed asymmetric factors to guide the enhancement process, prioritizing the influence of the EV on text representations. Let X t be the processed text embedding and X e be the processed EV embedding. To dynamically control the information flow between these modalities, gate signals G t (for text) and G e (for EV) are first computed from the concatenated embeddings [X t ; X e ] using separate linear layers followed by sigmoid activations. These gates modulate the subsequent cross-modal influence. The core asymmetric enhancement is then defined as: The resulting enhanced embeddings, X enh t and X enh e , are then individually passed through Layer Normalization. This entire controlled interaction, stabilized by Layer Normalization, proved superior to complex attention mechanisms in our experiments (see Section IV-B).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "5) Classifier Head:",
      "text": "The concatenated features from enhanced text (768D) and EV (128D) representations are processed by a deep classifier head designed for effective feature transformation before final classification. This architecture employs sequential Gated Linear Units (GLUs)  [27]  with GELU activation, progressively reducing dimensionality from the initial combined 896D (768D + 128D) to 448D and finally to 224D. Each GLU layer is followed by Layer Normalization with another GELU activation. A final linear layer maps the 224-dimensional features to the 7 output classes, followed by Softmax activation. This strategic depth proved essential for the fine-grained classification task.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Experimental Setup And Training",
      "text": "Our curated dataset (detailed in Section III-B3) was evaluated using 5-fold cross-validation to ensure robust performance assessment. We employed the AdamW optimizer with a learning rate of 1e-5 and a batch size of 8. Model performance was evaluated using accuracy and macro F1-score across all folds.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "This section presents the empirical results of systematically optimizing the multimodal architecture for the 7-level depression severity classification task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Baseline Model Performance",
      "text": "We first established a baseline model using RoBERTa text embeddings and processed EV embeddings, combined via simple feature concatenation followed by a basic classifier head. This baseline achieved an accuracy of approximately 73%, serving as the starting point for evaluating more sophisticated integration strategies.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Optimizing Multimodal Fusion",
      "text": "We then investigated methods to improve the fusion of text and EV embeddings:\n\n1) Evaluating Fusion Strategies: To effectively integrate the text and EV modalities, we first investigated the performance of standard multi-head cross-attention mechanisms, a common approach for multimodal fusion. As shown in Figure  3 , even the most effective configuration of these attention models offered only a slight improvement over the simple concatenation baseline. This modest outcome motivated our exploration of more tailored fusion strategies. Consequently, we developed and evaluated our advanced fusion mechanisms: Symmetric Cross-Modal Enhancement (SCME) and Asymmetric Cross-Modal Enhancement (ACME). The design of the ACME mechanism is detailed in Section III-C4. In stark contrast, while SCME demonstrated further improvement, our ACME approach achieved significantly superior performance, reaching 77.12% accuracy. This result clearly outperformed both the baseline and all tested attention-based models, thus validating this new direction. 2) Optimizing Asymmetric Cross-Modal Enhancement: Having established the superior potential of ACME over both complex attention and SCME , we further analyzed its optimization. Initial tests evaluating SCME found that the best-performing symmetric factor was 0.15, yielding 76.00% accuracy. However, subsequent experiments exploring dynamically learned factors for the enhancement mechanism indicated a natural asymmetry was beneficial. This suggested that optimally leveraging the teacher-derived EV required careful, asymmetric calibration to provide essential grounding for interpreting unstructured text narratives, a core principle of our approach.\n\nBased on this observation, we tested several fixed asymmetric factor configurations to identify the optimal setup. As shown in Table  I , the configuration with α e→t = 0.85 and α t→e = 0.30 achieved the best performance at 77.12% accuracy, outperforming both other asymmetric variants and dynamic factor learning. This result empirically confirmed the value of stronger, asymmetrically weighted guidance from the teacher's structured and empathetic judgment provided by the EV. This optimized ACME mechanism significantly outperformed the more complex multi-head cross-attention mechanisms (Sec IV.B.1, Figure  3 ). We hypothesize that complex attention struggles in this context due to potential challenges including:\n\n• The significant dimensionality mismatch between highdimensional text embeddings (768D from RoBERTa) and the lower-dimensional processed EV (128D, see Sec III-C3), hindering effective alignment; • The potential risk of diluting the sparse but critical information within the EV; • The moderate dataset size ( 17.5k samples), which may be insufficient to robustly train complex attention for this specific cross-modal interaction. In contrast, the optimized ACME mechanism provided controlled interaction, effectively preserving teacher's empathetic insights while appropriately guiding the model's interpretation of emotional narratives.\n\nThis optimized ACME configuration, demonstrating the benefits of calibrated human insight integration and strategic simplicity in fusion, was used in all subsequent experiments.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Text Representation Optimization",
      "text": "Following the optimization of our cross-modal fusion mechanism (Sec IV-B2), we next refined the strategy for representing long student narratives. Our initial approach involved segmenting narratives using a sliding window (512 tokens, 256 stride) to produce chunk embeddings.\n\nWe first explored methods to explicitly leverage inter-chunk information. However, experiments revealed that incorporating explicit chunk order via positional embeddings significantly reduced accuracy, and employing a more complex multi-head Transformer-based chunk aggregator failed to yield improvements over simpler aggregation of individual chunk CLS embeddings. These findings suggested that for our task, capturing the most salient emotional signals within each chunk and relying on the implicit context from a substantial token overlap (256-token stride proved optimal) was more effective than explicitly modeling precise chunk order or complex sequential dependencies between chunks. We therefore proceeded without explicit positional embeddings and focused on optimizing the pooling of chunk CLS embeddings.\n\nEvaluating standard document pooling strategies (Figure  4 ), Max Pooling demonstrated clearly superior performance over alternatives like Mean Pooling, Attention Pooling, and Adaptive Hierarchical pooling. This highlighted the importance of capturing peak emotional signals from the narrative texts for assessing depression severity. Further significant improvement was achieved by incorporating Layer Normalization after Max Pooling, yielding an accuracy of 79.08% and establishing 'Max Pooling + LayerNorm' as our optimal text representation strategy.\n\nImportantly, while Max Pooling identifies these \"peak signals,\" these are not interpreted naively (e.g., based solely on keywords such as \"I want to die\") within the HEAE framework. Instead, the ACME mechanism (Sec IV-B2) leverages the teacher-derived EV to provide essential context. This effectively grounds and validates text-based signals against the teacher's holistic, empathetic assessment, underscoring the indispensable role of structured human empathy in achieving nuanced and reliable judgment from student narratives.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Classifier Head Optimization",
      "text": "With optimized fusion (Sec IV-B2) and text representation (Sec IV-C), we refined the classifier head to better handle the fine-grained 7-level severity classification. We found that increasing the classifier depth by implementing a multi-layer architecture ((896D → → 224D → 7 classes) significantly improved performance. Within this deeper structure, GELU activation yielded the best performance, outperforming ReLU and SiLU. Further enhancement was achieved by incorporating GLUs into this GELU-activated structure, bringing the final accuracy to 82.05% (Figure  5 ). This classifier's improved performance highlights a key architectural principle: unlike earlier model stages (e.g., fusion, document pooling) where tailored simpler mechanisms were optimal for preserving key signals, the final classification of rich, aggregated multimodal features demands significantly increased representational capacity. The deeper, gated architecture, incorporating GELU activation and GLUs, provided this necessary capacity, enabling effective learning of complex non-linearities and adaptive features to discern subtle differences among the 7 severity levels from the consolidated inputs.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Final Model Performance",
      "text": "The final optimized HEAE model, incorporating all optimal components (asymmetric fusion, max pooling, and deep GELU+GLUs classifier head), achieved 82.05% ± 0.58% accuracy and 82.08% macro F1-score for our 7-level depression severity classification task. This represents a significant improvement of approximately 9% points over the baseline model (73% accuracy), demonstrating the effectiveness of our human-empathy guided approach.\n\nV. DISCUSSION",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Responsible Design Principles For Human-Ai Collaboration",
      "text": "The HEAE framework introduces a new paradigm for depression assessment in special education. By structuring teacher empathy into the EV as an explicit input, it achieves an organic fusion of human judgment and AI analysis. This method makes teachers' tacit professional insight explicit. It preserves the primacy of human judgment while enabling teachers, through AI assistance, to more effectively apply their rich empathetic understanding. This leads to assessments of student depression that are more nuanced and contextualized than what traditional standardized questionnaires typically allow. Our framework rejects the \"AI replaces human\" approach. Instead, it builds a complementary relationship, positioning technology as a means to enhance human empathy, not substitute it. More importantly, this reorientation creates an inherently more transparent and ethically sound evaluation system. It offers a blueprint for sensitive domains where maintaining core human judgment is essential.\n\nOur experiments reveal a key design principle for human-AI collaboration: complexity should be strategically allocated, not uniformly applied. The tailored simplicity in modal fusion and representation extraction contrasts sharply with the targeted depth in classification. This reflects more than just technical choices; it embodies a philosophy regarding the transparency of the human-AI interaction. This \"structured simplicity + targeted complexity\" approach challenges the \"more complex is better\" tendency often seen in the pursuit of model performance. It suggests that truly effective human-AI systems should respect and leverage the respective strengths of both humans and AI. Therefore, we advocate prioritizing understandability and signal fidelity during earlystage information integration. At the same time, necessary computational sophistication should be permitted for the final, fine-grained decision-making stage. This balancing principle extends beyond specific algorithm choices and aligns with current ethical demands for AI transparency and explainability. It charts a new course for designing future responsible affective computing systems-ones that are both effective and ethically considerate, especially in highly sensitive areas like special education",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Implications For Socially Responsible And Explainable Ai",
      "text": "With its ability to structurally integrate nuanced teacher empathy via the EV and student narratives, the HEAE framework offers significant implications for responsible, explainable, and privacy-preserving affective AI systems.\n\nAt its core, HEAE achieves transparency through its fundamental structure rather than relying on after-the-fact explanations. The EV creates a clear connection between teacher judgment and AI analysis, making the influence pathway visible from the outset. This design approach offers inherent clarity that conventional black-box models lack. The explicit nature of this structure makes both the AI's reasoning process more traceable and the human input more examinable-qualities essential for identifying potential biases and building trust in the human-AI collaborative assessment process.\n\nHEAE also embodies privacy-by-design principles essential for sensitive educational contexts. Its efficient architecture enables local processing, minimizing the need to transmit sensitive student data to external servers. This approach not only protects privacy but also makes sophisticated assessment technology accessible to resource-limited special education environments, demonstrating that responsible AI development can balance advanced capabilities with ethical deployment constraints.\n\nBy maintaining educators as integral participants, HEAE preserves human agency and accountability in high-stakes assessment. The system functions as an extension of professional judgment rather than a replacement, creating a collaborative framework where technology enhances rather than diminishes human expertise. This partnership model ensures that emotional intelligence in the assessment process stems from the synergy of human empathy and computational analysis, not from AI in isolation.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vi. Limitations And Future Work",
      "text": "While HEAE demonstrates promise for human-AI collaborative depression assessment, several important limitations and future directions warrant consideration:\n\nEmpathy Vector Subjectivity and Nuance: The EV effectively structures teacher empathy, yet fully capturing the richness and variability of empathetic judgment remains challenging. The inherent subjectivity across different educators and student contexts suggests the need for research into consensus mechanisms among multiple raters and contextsensitive EV refinements, while preserving authentic human insight.\n\nReal-World Generalizability and Implementation: Our current validation, while encouraging, occurred within a controlled experimental environment. Real-world special education settings present additional challenges including diverse narrative styles, varying teacher technological familiarity, and integration with existing workflows. Future deployment studies in authentic educational environments are essential to assess practical usability, effectiveness, and impact on teacher workload.\n\nPrivacy and Ethical Deployment Considerations: While our design prioritizes privacy through local processing, practical deployment requires further attention to data governance in educational settings. Future work should explore privacypreserving techniques that enable model improvement without centralized data collection, and develop ethical protocols appropriate for vulnerable student populations and sensitive mental health data. These considerations are especially important given the heightened privacy requirements in special education contexts.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "This paper introduced the HEAE framework, demonstrating a novel human-AI collaborative paradigm for depression severity assessment in special education by structurally integrating teacher empathy. Beyond achieving effective classification performance, this approach offers a blueprint for developing more transparent, ethically sound, and human-centered affective computing systems in sensitive domains.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Data Availability Statement",
      "text": "The \"Golden Seeds\" dataset, validation code are available on https://huggingface.co/datasets/Plum551/golden_seeds_ev_ depression ACKNOWLEDGMENTS This work is dedicated to the one whose profound inspiration and unwavering belief have reshaped my life -a soul whose quiet presence still guides this research.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "This statement outlines the measures undertaken to ensure responsible data handling and alignment with ethical AI principles in the development of the Human Empathy as Encoder (HEAE) framework.\n\nData Source The dataset consists of publicly accessible posts from online mental health communities, with typical demographic representation of young adults. All data were sourced from public domains, with no data from minors or private sources collected. The dataset is used strictly for non-commercial research purposes, specifically advancing AIdriven tools for understanding and assessing mental well-being within educational contexts.\n\nData Privacy Recognizing the highly sensitive nature of the content-including self-disclosures of trauma, suicidality, and other deeply personal experiences-we implemented a comprehensive, multi-stage anonymization protocol:\n\n• Removal of direct Personally Identifiable Information (PII) using Named Entity Recognition (NER) and rulebased techniques. • Generalization or removal of potentially identifying details such as precise ages, institutional names, exact geographical locations, and specific familial relationships.\n\n• Removal of date-specific information and unique circumstantial identifiers that could contribute to reidentification.\n\n• Additional NER tools with manual review to ensure thoroughness where automated methods might prove insufficient.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). From the full set of 200 validated golden seeds,",
      "page": 3
    },
    {
      "caption": "Figure 1: Automated annotation pipeline using golden seeds and an LLM.",
      "page": 3
    },
    {
      "caption": "Figure 2: ) integrates the text",
      "page": 3
    },
    {
      "caption": "Figure 2: The architecture of the proposed Human Empathy as Encoder (HEAE) model.",
      "page": 4
    },
    {
      "caption": "Figure 3: Performance comparison of different multimodal fusion strategies.",
      "page": 5
    },
    {
      "caption": "Figure 3: ). We hypothesize that complex",
      "page": 5
    },
    {
      "caption": "Figure 4: Comparison of different pooling strategies for aggregating text chunk",
      "page": 6
    },
    {
      "caption": "Figure 5: Performance comparison of different classifier head architectures.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "77.12%": "76.00%",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "Attention-Based\nSCME"
        },
        {
          "77.12%": "75.3\n74.71%",
          "Column_2": "7%",
          "Column_3": "75.5",
          "Column_4": "4%",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "ACME"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Limitations of the patient health questionnaire (PHQ-9) in identifying anxiety and depression: many cases are undetected",
      "authors": [
        "K Button",
        "D Kounali",
        "L Thomas",
        "N Wiles",
        "T Peters",
        "G Lewis"
      ],
      "year": "2014",
      "venue": "British Journal of General Practice"
    },
    {
      "citation_id": "2",
      "title": "Machine learning approaches for mental illness detection on social media: A systematic review of biases and methodological challenges",
      "authors": [
        "S Kapadia",
        "K Saha",
        "P Bhattacharya"
      ],
      "year": "2003",
      "venue": "Machine learning approaches for mental illness detection on social media: A systematic review of biases and methodological challenges"
    },
    {
      "citation_id": "3",
      "title": "Implementing large language models in healthcare while balancing control, collaboration, costs and security",
      "authors": [
        "H Harvey",
        "A Ashok"
      ],
      "year": "2024",
      "venue": "BMJ Health & Care Informatics"
    },
    {
      "citation_id": "4",
      "title": "Responsible ai integration in mental health research: Issues, guidelines, and best practices",
      "authors": [
        "G Sowemimo-Coker",
        "J Robles-Zurita",
        "R Ryan",
        "I Tachtsidis"
      ],
      "year": "2024",
      "venue": "JMIR Mental Health"
    },
    {
      "citation_id": "5",
      "title": "Depression detection from social media textual data using natural language processing and machine learning techniques",
      "authors": [
        "S Idrissi",
        "B Ben Amor",
        "N Abid"
      ],
      "year": "2024",
      "venue": "2024 International Conference on Digital Technologies and Applications (ICDTA)"
    },
    {
      "citation_id": "6",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "7",
      "title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
      "authors": [
        "A Graves",
        "J Schmidhuber"
      ],
      "year": "2005",
      "venue": "Neural networks"
    },
    {
      "citation_id": "8",
      "title": "Deep learning-based detection of depression and suicidal tendencies in social media data with feature selection",
      "authors": [
        "M Bhuiyan",
        "S Akintoye",
        "E Deniz",
        "R Mathew",
        "M Islam",
        "M Nasim"
      ],
      "year": "2024",
      "venue": "Diagnostics (Basel)"
    },
    {
      "citation_id": "9",
      "title": "Methods in predictive techniques for mental health status on social media: a critical review",
      "authors": [
        "S Chancellor",
        "M Choudhury"
      ],
      "year": "2020",
      "venue": "NPJ Digital Medicine"
    },
    {
      "citation_id": "10",
      "title": "Clpsych 2019 shared task: Predicting the degree of suicide risk in reddit posts",
      "authors": [
        "A Zirikly",
        "P Resnik",
        "Ö Uzuner",
        "K Hollingshead"
      ],
      "year": "2019",
      "venue": "Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology"
    },
    {
      "citation_id": "11",
      "title": "Screening for depression using natural language processing: Literature review",
      "authors": [
        "S Almaiman"
      ],
      "year": "2024",
      "venue": "Journal of Medical Internet Research"
    },
    {
      "citation_id": "12",
      "title": "Multimodal assessment of depression from behavioral signals",
      "authors": [
        "J Cohn",
        "N Cummins",
        "J Epps",
        "R Goecke",
        "J Joshi",
        "S Scherer"
      ],
      "year": "2019",
      "venue": "Computational Interaction"
    },
    {
      "citation_id": "13",
      "title": "Deep learning for depression recognition with audiovisual cues: A review",
      "authors": [
        "M Islam",
        "M Islam",
        "M Rahman",
        "M Hossain"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "14",
      "title": "Multimodal machine learning in mental health: A survey of data, algorithms, and challenges",
      "authors": [
        "Z Sahili",
        "P Li",
        "G Dong",
        "M Alhamid",
        "M Ali",
        "C Cooijmans",
        "M Abdelmoneim",
        "M Moni"
      ],
      "year": "2024",
      "venue": "Multimodal machine learning in mental health: A survey of data, algorithms, and challenges"
    },
    {
      "citation_id": "15",
      "title": "Enhancing multimodal depression diagnosis through representation learning and knowledge transfer",
      "authors": [
        "Y Li",
        "S Wang",
        "X Zhu",
        "Y Zhou"
      ],
      "year": "2024",
      "venue": "BMC Medical Informatics and Decision Making"
    },
    {
      "citation_id": "16",
      "title": "A survey on human-in-the-loop for machine learning",
      "authors": [
        "M Wu",
        "S Yuan",
        "J Zhang"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys",
      "doi": "10.1145/3502260"
    },
    {
      "citation_id": "17",
      "title": "Humans in the loop: The design of interactive ai systems",
      "authors": [
        "Hai Stanford"
      ],
      "venue": "Humans in the loop: The design of interactive ai systems"
    },
    {
      "citation_id": "18",
      "title": "Human-ai collaboration in healthcare: A review and research agenda",
      "authors": [
        "L Lai",
        "J Wiens",
        "W Weber"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "19",
      "title": "Human and ai collaboration in the higher education environment: opportunities and concerns",
      "authors": [
        "O Zawacki-Richter",
        "V Marín",
        "M Bond",
        "F Gouverneur"
      ],
      "year": "2024",
      "venue": "International Journal of Educational Technology in Higher Education"
    },
    {
      "citation_id": "20",
      "title": "Human-in-the-loop or ai-in-the-loop? automate or collaborate?",
      "authors": [
        "S Natarajan",
        "S Mathur",
        "S Sidheekh",
        "W Stammer",
        "K Kersting"
      ],
      "year": "2024",
      "venue": "Human-in-the-loop or ai-in-the-loop? automate or collaborate?",
      "arxiv": "arXiv:2412.14232"
    },
    {
      "citation_id": "21",
      "title": "Ethics and information privacy in affective computing",
      "authors": [
        "C Reynolds",
        "R Picard"
      ],
      "year": "2004",
      "venue": "Proc. Workshop on Affective Computing, CHI 2004"
    },
    {
      "citation_id": "22",
      "title": "Ethical issues in affective computing",
      "authors": [
        "J Gratch",
        "C Lisetti",
        "N Mavridis"
      ],
      "year": "2014",
      "venue": "Oxford Handbook of Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Explainable ai in personalized mental healthcare",
      "authors": [
        "Deeploy"
      ],
      "year": "2023",
      "venue": "Deeploy Blog"
    },
    {
      "citation_id": "24",
      "title": "Leveraging explainable AI and multimodal data for stress level prediction in mental health diagnostics",
      "authors": [
        "A Destiny"
      ],
      "year": "2025",
      "venue": "International Journal of Research and Analytical Reviews (IJRIAS)"
    },
    {
      "citation_id": "25",
      "title": "Establishing ethical guidelines for human-ai collaboration: Best practices and frameworks",
      "authors": [
        "Smythos"
      ],
      "year": "2023",
      "venue": "Establishing ethical guidelines for human-ai collaboration: Best practices and frameworks"
    },
    {
      "citation_id": "26",
      "title": "Human-centered artificial intelligence: Reliable, safe & trustworthy",
      "authors": [
        "B Shneiderman"
      ],
      "year": "2020",
      "venue": "International Journal of Human-Computer Interaction",
      "doi": "10.1145/3313831.3376275"
    },
    {
      "citation_id": "27",
      "title": "Language modeling with gated convolutional networks",
      "authors": [
        "Y Dauphin",
        "A Fan",
        "M Auli",
        "D Grangier"
      ],
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research"
    }
  ]
}