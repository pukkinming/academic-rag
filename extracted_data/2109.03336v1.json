{
  "paper_id": "2109.03336v1",
  "title": "Multi-Branch Deep Radial Basis Function Networks For Facial Emotion Recognition *",
  "published": "2021-09-07T21:05:56Z",
  "authors": [
    "Fernanda Hernández-Luquin",
    "Hugo Jair Escalante"
  ],
  "keywords": [
    "Locally weighted learning",
    "Radial basis function networks",
    "Emotion recognition",
    "Convolutional neural network",
    "Looking at people"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition (ER) from facial images is one of the landmark tasks in affective computing with major developments in the last decade. Initial efforts on ER relied on handcrafted features that were used to characterize facial images and then feed to standard predictive models. Recent methodologies comprise end-to-end trainable deep learning methods that simultaneously learn both, features and predictive model. Perhaps the most successful models are based on convolutional neural networks (CNNs). While these models have excelled at this task, they still fail at capturing local patterns that could emerge in the learning process. We hypothesize these patterns could be captured by variants based on locally weighted learning. Specifically, in this paper we propose a CNN based architecture enhanced with multiple branches formed by radial basis function (RBF) units that aims at exploiting local information at the final stage of the learning process. Intuitively, these RBF units capture local patterns shared by similar instances using an intermediate representation, then the outputs of the RBFs are feed to a softmax layer that exploits this information to improve the predictive performance of the model. This feature could be particularly advantageous in ER as cultural / ethnicity differences may be identified by the local units. We evaluate the proposed method in several ER datasets and show the proposed methodology achieves state-of-the-art in some of them, even when we adopt a pre-trained VGG-Face model as backbone. We show it is the incorporation of local information what makes the proposed model competitive.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Automated emotion Recognition (ER) is the ability to identify human emotional states by analyzing speech, facial expressions, and body gestures  [24] . ER has been proved to be very useful in areas as: affective computing, humancomputer interaction, and as support tool for psychology, psychiatry, neurology, and related applications and sub fields, e.g., for pain assessment, deception detection, etc. (see e.g.,  [20, 37, 30, 11] ). Clearly, automated methods for ER have a great potential impact in a wide variety of fields.\n\nPerhaps the most studied and one of the most useful modalities for ER from an affective computing perspective is that comprising visual information, including still images and sequences. In particular, facial expression recognition (FER) focuses on the analysis of facial imagery with the aim of building predictive models to match faces with emotions. Traditional approaches to FER were based on standard machine learning methods (e.g., support vector machines, neural networks etc.) feed with handcrafted features extracted from images, see e.g.,  [26, 25] .\n\nRecently, methods based on deep learning methodologies, in particular those based on convolutional neural networks (CNNs), have been used successfully to approach the FER task  [20] . The benefits of these methods is that both, features and model, are learned simultaneously and there is no need of designing handcrafted features. Many solutions based on CNNs have been proposed recently, obtaining outstanding performance in a number of FER benchmarks. Despite its effectiveness, these methods may be overlooking important information that could be useful for improving the recognition process. In particular, local patterns that may arise in the context of FER and that are shared by subsets of subjects could be exploited to improve the recognition ability of the model. This is particularly relevant in FER where cultural, and ethnicity specific traits of faces make harder the task for global CNN models; that is, generic models trained to learn common features for all of the subjects in the dataset under consideration. We hypothesize that enhancements to CNN models that take into account this local information may lead to improvements in performance, in particular, for minority class examples and samples that are difficult to classify with such global models.\n\nIn this paper we introduce an enhanced CNN based architecture equipped with multiple sets of radial basis function (RBF) units that aim at capturing local patterns. We call our proposed method a Multi Branch Deep RBF Network. The proposed method comprises several branches of RBF units coupled with a standard CNN that acts as feature extractor, the outputs of the multi branch component are then concatenated and feed to a softmax layer that plays the role of classifier. The model is trainable end-to-end and inherits all of the benefits of CNNs for FER, with the additional advantage of exploiting local information. The proposed methodology is evaluated in a number of benchmark FER datasets and we show that the proposed enhancement outperforms considerably baselines that include a CNN without any locality component. In addition, the proposed method obtains state-of-the-art performance in some of the considered datasets, this is remarkable, given the simplicity of the considered backbone architecture (VGG-Face).\n\nThe contributions of this paper are threefold:\n\n• The formulation of a variant of CNNs that aim at incorporating local information explicitly in the model. As such, we are exploring the first steps of locally 2  weighted learning in the context of deep learning.\n\nTo the best of our knowledge this is the first work adopting a locally weighted learning scheme in the task of FER. Please note that there are very few efforts on locally weighted deep learning in general, see e.g.,  [51, 35, 46] .\n\n• The introduction of Multi Branch Deep RBF networks for Emotion Recognition. This is an enhancement to CNNs that successfully exploits local information at the instances level, we show its effectiveness in the FER task.\n\n• An experimental evaluation showing the proposed methodology coupled with a standard architecture (VGG-Face) as backbone outperforms baseline models and achieves state-of-the-art performance that is comparable to more sophisticated and complex methodologies for some datasets. More importantly we provide evidence that the competitive performance of the proposed model is due to the incorporation of local information.\n\nThe findings and conclusions draw from this paper motivate further research on the study of locally weighted learning in the context FER and in general machine learning.\n\nThe remainder of this paper is organized as follows. Section 2 reviews related work on FER and local learning. Next, Section 3 introduces the proposed Multi Branch Deep RBF network model in detail. Then, Section 4 presents an experimental evaluation of the proposed method in benchmark FER datasets. Finally, Section 5 outlines conclusions and future work directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "This section briefly reviews related work on FER and on the intersection of locally weighted learning with deep learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Automated ER is a task that has been studied for a while now, where the definition of emotion and a characterization of human emotions was inherited from the psychology field. Despite there are too many taxonomies and definitions, the most widely accepted categorization is that of Paul Ekman who defined six universal emotions: anger, disgust, fear, happiness, sadness and surprise  [6] ; subsequently contempt was also considered as a basic emotion. We adhere to this categorization in the remainder of the paper.\n\nThe FER task was initially faced with a standard machine learning models feed with handcrafted features that aimed to capture discriminative facial characteristics. Common feature extractors comprise Histogram of oriented gradients (HoG)  [13, 2, 44] , Local Binary Patterns (LBP)  [14, 40, 54] , among others  [9, 10] . While the considered classification models include Support Vector Machines (SVM)  [43, 8] , AdaBoost  [9, 3]  and Decision trees among others  [49, 5] . While competitive, most of the methods from the first wave relied on handcrafted features that not necessarily are representative or descriptive of the data  [28] .\n\nThe advances in deep learning have motivated a second waive of methodologies that rely on deep learning  [20] . Contrary to the traditional approaches, these methods simultaneously learn the representation for the input and the predictive model. In this way, features are derived entirely form data and these are tied to the classification model being learned. The most commonly used deep learning architectures that have been used for ER are Convolutional Neural Networks (CNNs), see e.g.,  [4, 42, 45, 38, 41, 33, 21] . These models take as input raw images and learn multiple layers of convolutional filters that are applied to the inputs of the layer and their outputs feed to the next one. These models are coupled with other types of layers including dense and softmax layers to learn the predictive part of the model. Other popular architectures comprise Residual Networks  [39, 47, 7, 48, 22]  and sequential models (e.g., LSTMs)  [53] .\n\nAdditionally, these methods are able to incorporate additional mechanisms and features into the learning process making them quite effective and self contained, for instance, architectures with attention mechanisms  [18, 7, 48, 31] , ad hoc loss functions  [39]  and other complex procedures  [23, 52, 55] . The author is referred to  [20]  for a comprehensive survey on FER with deep learning based methodologies.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Deep Learning Models With Local Learning",
      "text": "Despite the effectiveness of deep learning based solutions, there are still open questions that deserve attention from the community and that could have a great impact into the field. One of these questions has to do with the lack of specific mechanisms in CNNs for taking into account local information at the instance level. Local information has proven to be very helpful in classical models within machine learning. Consider for instance locally weighted regression  [32] , where the incorporation of samples close to a query sample are used to approximate a regression function locally. This feature enables a linear model (e.g., least squares regression) to approximate non linear decision surfaces. There are many other cases where local information has proven to be very useful including support vector machines  [17] , learning vector quantization  [34] , decision trees and even there is a variant of neural network that implements locally weighted learning: the radial basis function network  [32] . The next section reviews related work on some efforts on locally weighted learning for deep learning.\n\nIn the context of deep learning, local learning has been scarcely studied in the context of CNNs and other Deep Neural Networks (DNNs). Zadeh et al. introduced a deep RBF model that aimed to make robust predictions against adversarial attacks  [51] . The model is formed by a CNN architecture tied with RBF units in the output layer (these were used to perform classification). A loss function tailored to be robust against adversarial attacks was proposed. Vinderová et al. presented a method where a DNN and RBF networks are concatenated to classify adverse examples correctly  [46] . Although these efforts combine deep learning with locally weighted learning the aim is not to improve the predictive performance of the model in general, but for specific adversarial scenarios. Moreover in  [46]  the DNN and the RBF network are trained separately, which results in the combination of two independent models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Discussion",
      "text": "The FER task has been approached for a while and the most effective solutions are those based on deep learning methodologies. These methods have the appealing features that they can learn simultaneously features and predictive model. While these models have obtained outstanding performance, there are several questions around these models that deserve to be explored. In this paper we aim to explore the benefits of locally weighted learning (LWL) into deep learning models for approaching the FER task. LWL has been scarcely studied in the context of deep learning. There are few efforts in this direction and all of them target very specific scenarios, for instance, classification with adversarial examples and interpretability. We argue that LWL could be beneficial for FER because there are samples that in order to be correctly classified, the model should build a sub-classifier that considers only samples similar to the query point. Intuitively, consider ER datasets in which a minority group is underrepresented, and dominated by another group (see Section 4.4 for an example). Global models are prone to fail to correctly classify such instances and this type of issues could be alleviated with LWL. For these reasons we propose in this paper a LWL deep learning model that does not target specific scenarios (like adversarial samples). We show the potential of this model in an experimental evaluation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Branch Deep Radial Basis Function Networks",
      "text": "The working hypothesis of this work is that the incorporation of local information at the instance level into the learning process of CNN based models improves the recognition performance of the enhanced model for the FER task. The intuition behind this hypothesis is that in FER there may exist groups of instances that share similarities to each other in one or more aspects (e.g., in terms of ethnicity or age). Therefore, when classifying a query sample, a prediction that is build by taking into account information of similar instances should improve the recognition performance.\n\nWe introduce in this section a model that implements such an idea, the so called Multi-Branch Deep RBF Network model. In a nutshell, the model uses a CNN as backbone (e.g., VGG-face) and it is enhanced with a new layer formed by multiple branches of RBF units. Such RBF layer receives as input feature maps from the preceding layers of the CNN and its outputs are concatenated and connected to a softmax layer that makes predictions over the considered classes. This enhancement allows a CNN to implicitly incorporate local information that can have a positive impact in recognition performance. A graphical diagram of the proposed model is depicted in Figure  1 , the remainder of this section describes the proposed model in detail.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Radial Basis Function Networks",
      "text": "An RBF unit is type of neuron that is associated to a center and a radius, it can be considered as a prototype in the input space whose position is updated (learned) from data. These units are commonly used in RBF networks and LVQ-based models. In the case of the former models, a set of units defines a layer, and commonly there is an RBF unit per class associated to the problem at hand (i.e., RBF units are often used instead of softmax ones for the predictive part of the model). The output h i of an RBF unit i given input x j is computed as follows:\n\nwhere x j ∈ R d is a d-dimensional feature vector, and µ i ∈ R d , σ i ∈ R are the center and radius of RBF unit i.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Branch Rbf Layer",
      "text": "As previously mentioned the proposed model extends a backbone CNN with RBF units arranged into branches as shown in Figure  1 . We relied on VGG-Face  [36]  as backbone because is a well known and generic enough CNN for facial analysis that has proven to be very helpful in FER and related tasks when used as pre-trained model. One should note that VGG-Face is not a state-of-the-art methodology as those that are being currently proposed in the context of FER (see Section 2). Our decision for relying in a generic model lies in that we wanted to prove the proposed extension could lead to improvements with a standard model. Relying on more complex or elaborated models would make it more complicated to assess the actual improvement due to our local modules.\n\nVGG-Face is an architecture formed by a series of convolutional layers followed by fully connected layers that are in turn followed by a softmax layer in charge of the classification process  [36] . We modify the last few layers of backbone architecture as follows. We dropped all of the fully connected layers, and instead connected multiple branches of RBF units (see Expression (1)) to the output of the last convolutional layer. Such convolutional layer (see Figure  1 ) returns as output 512 feature maps of dimensionality 7 × 7. We take the activation of these maps as inputs to the multiple RBF branches.\n\nThe motivation behind having multiple branches of RBFs is that if one would have a single RBF, the input would be very high dimensional (i.e., 7 × 7 × 512 = 25088) and potentially useful local information would get lost or would be very difficult to process. In contrast, having multiple branches of RBFs each taking as input a low dimensional vector could lead to exploit local information easily. In fact, it would be expected that each branch could capture a local pattern different from the rest (see  Section 4.3) . Therefore, we propose to process the outputs of the last convolution layer in such a way that each branch takes an input of manageable size. Specifically, we process the last convolutional layer with a set of filters that yield a 7 × 7 output and we use as many of these filters as branches are considered in the model (see Figure  1 ). These outputs are flattened and feed to the RBF branches in a fixed order.\n\nIn the proposed model, the pre-trained convolutional layers of the VGG-Face model are used  [36] . The RBF centers and their corresponding radius are initialized randomly. The model is then trained end-to-end using backpropagation and stochastic gradient descent (more details in Section 4) using the FER dataset at hand. We performed experiments on different ways of freezing and updating weights for the whole architecture, we observed there was no significant difference in performance when updating or not the (convolutional) weights inherited from the backbone architecture, therefore we decided to froze the weights of VGG-Face and learn only the remainder of the parameters. One should note that even when the displayed in Figure  1  shows a branch per feature map, a reduced number of branches could be used as well, see Section 4.\n\nIn the next section we present an experimental evaluation that shows the proposed model outperforms strong baseline models. In particular, we compare the proposed model to a reference model that replaces the RBF branches by dense layers, this is illustrated in Figure  2 . This is important to mention as this comparison will allow us to determine the actual benefits of having local information instead of fully connected units as it is standard in CNN models.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Discussion",
      "text": "We just introduced the Multi-Branch Deep RBF Network model: an enhancement to CNNs in which a layer formed by several branches of RBF units is added before the softmax classification layer. The main novelty of this proposal is the adoption of multiple RBF sub networks that allow the model to deal with the high dimensionality of the input space, likewise, having multiple branches allow the model to capture specific local information in each of these. Compared to alternative solutions from deep RBF networks, which use a single branch of RBF units, in our model the outputs of multiple branches are feed to a softmax layer that makes predictions, whereas in reference work (see e.g.,  [51, 46] ) RBF units are used to make the predictions directly. Additionally, one should note that the focus of previous work has been on using local information for under covering adversarial attacks (see, e.g.,  [51, 46, 35] ), while in this paper our goal is to improve the overall classification process. Finally, to the best of our knowledge this is the first effort on trying to incorporate local information at the instance level into the task of FER. As shown in the next section the proposed enhancement improves considerably the performance of reference models and performs favorably with state of the art solutions that are based on much elaborated mechanisms and models (e.g., attention based models).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experimental Evaluation",
      "text": "This section presents an experimental evaluation of the proposed model in the FER task, the goal is to show the competitiveness and benefits of the Multi-Branch Deep RBF Network model when compared to reference models and to state-of-the-art solutions. We first introduce the datasets and experimental settings; then, we present an ablation study that analyzes the performance of our model under different parameter settings; next we show some visualizations that aim at highlighting the benefits of our model; finally, we compare the performance of our model to reference and state-of-the-art methods and conclude with a discussion.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Settings",
      "text": "For the experimental comparison we used the following benchmark datasets that have been widely used in the literature (see  [20] ): Real-world Affective Faces Database (RAF-DB), RAF-DB Compound  [21, 19] , the Extended Cohn-Kanade Dataset (CK+)  [25] , the Japanese Female Facial Expression (JAFFE) Dataset  [26, 27] , and FER 2013  [12] . Additionally we performed experiments in a challenging dataset combining both CK+ and JAFFE datasets. Samples from the considered datasets are shown in Figures  3  and 4  and some statistics are presented in Table  1 .\n\nThe considered datasets comprise a diversity in terms of the number of samples, background/recording conditions, and complexity. Where one should distinguish datasets under the standard ER setting from datasets of greater difficulty.\n\nStandard datasets including CK+, JAFFE, FER2013 and RAF-DB comprise basic emotions  3  and images coming from the same distribution. The challenging datasets are CK+-JAFFE and RAF-DB Compound, the former made up by merging images of the CK+ and JAFFE datasets, and the latter considering a fine-grained classification of emotions, see Figure  4  .\n\nThe intuition behind experimenting with the merged CK+-JAFFE dataset lies in that we wanted to assess the performance of our model when there are clear differences across samples from the same category. On the other hand, the RAF-DB Compound is challenging because it considers compound emotion categories (e.g., Fearfully-surprised, sadly-angry, happily-disgusted, etc), 11 categories are considered, see Figure  4 . The idea of considering this dataset is to show the benefits of incorporating local information into the recognition process for approaching a fine grained FER task. It is expected that the proposed model is more advantageous in the two considered challenging datasets.\n\nFor all of the datasets we used the top-1 accuracy on the test set as the evaluation measure of performance. This is in agreement with previous work using the same datasets. The same partitions for training and testing were used in datasets where these were available (RAF-DB, RAF-DB Compound and FER2013) and random splits of 80% for training and validation and 20% for testing were used CK+ and JAFFE. For the latter datasets multiple partitions were generated and their results averaged in each experiment.\n\nThe model was trained using the Adam  [15]  optimizer with a batch size of 32 during 100 epochs. The performance in validation was used to monitor convergence of the model. We determined the value of σ experimentally as σ = 0.0528. The model was trained in a laptop with a Nvidia GTX card 2080 with 8Gb of VRAM, and a processor I7 6700K with 32 Gb of RAM.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "In this section we evaluate the performance of the proposed model when varying the number of branches and units. We present in Figure  5  the results of this evaluation for the six considered datasets. Results are shown as heat maps (the darker the better), the number of branches is specified in the x axis and the number of unites is shown in the y axis.\n\nAs it can be seen from this figure, mixed results are obtained for the different datasets. Being the CK+ dataset the easiest and RAF-DB Compound the toughest in terms of recognition performance. The difference between the lowest and highest performance achieved for every dataset makes clear that it is necessary to adequately tune both of these parameters (e.g., compare the lowest and highest performance in Figures  5 (b ) and (f)).\n\nAlthough no general conclusion can be drawn on the values of parameters, a pattern that seems to be present in all of the datasets is that a larger the number of branches seems to result in better performance of the model. Also, it seems that a small number of RBF units combined with large number of branches is a somewhat robust combination of parameters.\n\nIn general, the obtained performance in most datasets is competitive with the state-of-the-art (see Section 4.5). For the experiments reported in the next sections the best configuration of parameters for each dataset was used.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Visualization Of Centers",
      "text": "We now present visualizations of learned RBF centers for two configurations of parameters of the proposed model for the CK+-JAFFE dataset. We chose this particular dataset because it is one formed by instances from two different datasets and we expect the local information to be particularly helpful (see Section 4.5). Also, one should note that performance for this dataset did not vary too much for the different choices of parameters as shown in Figure  5  (c). Figure  6  shows the centers for a configuration with 4 branches and 8 RBF units per branch, the reported performance for this configuration was 0.943. From this figure it can be seen that centers across the branches are very different to each other. Corroborating the hypothesis that different centers are modeling different aspects of the input feature maps. It is only for branch 1 that there seem to be similarities among centers (column 1, rows 4-7 of the left plot). In general, it seems that the relevant information is located near the center of the image (blue values in the center, yellow for the background), which make sense given the approached task is ER. However, there are a few centers that are also giving large weights to the region surrounding the face (blue background).\n\nFigure  7  shows the centers but for a different configuration: 8 branches and 4 RBF units each, with a reported performance of 0.953. Again, centers seem to be visually different to each other, although the differences across RBF units of the same branch (rows, right plot) are less notorious. This could be reflecting the fact that branches are capturing local patterns with subtle differences across RBF units (except branch 5, fifth row in Figure  7  that seems to be learning the same pattern in the 3 RBF units). In fact this type of centers result in better performance for the approached dataset. Finally, it is worth to emphasize that in both cases the centers seem to converge to an useful representation, starting from random numbers (left plots in Figures  6  and 7 ).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Comparison With Reference Models",
      "text": "Table  2  shows a comparison of performance of the proposed model with other variants  4  of CNN that approach the same task. We report the average and standard deviation obtained from 10 experiments with different random initialization. As reference models we considered: (1) the backbone model, VGG-Face, the pre-trained network was subject of a fine tinning process with the new classes, the last layer was removed and replaced by a softmax one with as many units as ER classes; the fully connected layers were re initialized and subject to the fine tuning process too. (2) Multi-branch CNN is a model in which the branches of RBF units are replaced by dense layers (see Figure  2 ). The idea is to determine whether adding parameters to the backbone is the cause of improvement. Overall, the goal of this experiment is to assess the benefits of the proposed model when compared to competitive models that do not incorporate local information.\n\nFrom Table  2  it is clear that the proposed model outperforms both of the reference models. The differences in performance are significant for most datasets and there are also dramatic improvements in some cases. Compare for instance the performance of VGG-Face and the proposed model for the RAF-DB and RAF-DB Compound datasets. The differences in performance are impressive. This could be due to the mismatch between the datasets (both in terms type of images and classes) used for training VGG-Face and the ones considered for evaluation, even when we fine tuned the FC layers of the model, the mismatch seems to be too large as to be learned by the FC layers. Actually the We further analyze the differences in performance between VGG-Face and MN-RBFN. Figure  8  shows the confusion matrices for VGG-Face and the proposed model on the challenging CK+-JAFFE dataset. It can be seen that VGG-Face makes considerably more mistakes for the anger, fear and sadness categories. Our model miss classified 4 images from the JAFFE dataset and 6 from CK+, whereas the VGG-Face model made 28 and 29 mistakes for JAFFE and CK+ images, respectively. This represents 58% of images from JAFFE in the test set and only 10% if CK+ images. This clearly illustrates the benefits of incorporating local information into the CNN model: underrepresented samples are better classified (similar behavior was observed for the other baseline model). We refer the reader to Appendix A for a comparison of confusion matrices for the three models in the RAF-DB Compound dataset.\n\nIn order to further analyze these errors, Figure  9  shows sample images from the surprise and fear categories. The former being one of the best classified by both models  5  and the latter the most difficult class for the VGG-Face model.\n\nIt can be seen from this figure that samples for the surprise category share a notable pattern regardless of their origin:\n\nthe mouth is open in all cases, this makes the generic VGG-Face model to correctly classify most of the test instances in the mixed dataset. However, for the fear category images coming from CK+ and JAFFE look visually different to each other, yet sharing similarities within each dataset. This makes this class particularly challenging to VGG-Face, while the proposed model is able to correctly classify every instance from this class. This could be due to the local information incorporated into the model, and we think this is the main distinctive feature of our proposal.\n\nRegarding the MB-CNN baseline (column 3 in Table  2 ), it is also outperformed by the proposed model in every considered dataset, where the lowest improvement obtained was for the FER2013 dataset. We hypothesize this could be due to the large number of images available for training in this dataset (more than 28,000), that allow the MB-CNN model to find a competitive configuration of parameters with the extended dense layer added to the VGG-Face model.\n\nStill, the proposed model obtained the highest performance overall (see Appendix A). This difference in performance shows that is the local information, as captured by the proposed model, was the decisive factor for obtaining better performance across the considered datasets.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Comparison With The State Of The Art",
      "text": "In this section we compare the performance obtained by the proposed model with state-of-the-art references that have used the same datasets. Table  3  shows the results of the comparison. For each of the considered datasets we report the performance of recent references including the best result reported so far in each dataset to the best of our knowledge. One should note that for our model we report the average over 10 runs as reported in Table  2 , while for the reference models we take the single best result in the corresponding references. We include the results obtained by the baseline models for completion. 0.9800  [42]  0.9531  [18]  0.7582  [7]  0.8778  [21]  0.5795  [38]  0.9732  [31]  0.9280  [31]  0.7002  [52]  0.8690  [21]  0.5354  [48]  0.9730  [53]  0.9238  [33]  0.6640  [55]  0.8677  [23]  0.5020  [29]  0.9537  [45]  0.7810  [41]  0.6617  [23]   From Table  3  it can be seen that it is only in two datasets, FER2013 and RAF-DB, out of the five considered for this evaluation that the proposed model does not achieve performance competitive with the state-of-the-art. Interestingly, these are precisely the two datasets with the largest number of samples with 28,807 and 12,271 respectively. This result seems to indicate that the proposed model is particularly helpful for low-mid sized datasets. Likewise, since the references under comparison are based on extremely complex models and sophisticated procedures, it is not strange that they perform better when enough data is available.\n\nOn the other hand, the proposed model achieves very competitive performance in CK+, JAFFE and RAF-DB Compound datasets. In CK+ our work establishes a new reference result and in the JAFFE and RAF-DB Compound datasets the model achieves comparable performance. It is remarkable the performance obtained by the proposed model in the RAF-DB Compound dataset, as this features a problem of (very) fine grained classification with overlap among classes (compare the classes Sadly-Disgusted and Sadly-Angry, see Figure  10 ) and highly imbalanced (4 classes comprise 72% of the samples, and the 7 remaining classes with 6% of less out of the total number of samples). This result provides further evidence that the model is particularly helpful for this type of problems.\n\nIt is important to emphasize that among the references considered in the comparison we are including all types of recent models and mechanisms and it is to some extend unfair to compare a model like ours, which uses a simple backbone (VGG-Face). For instance, for JAFFE the only method that obtains better performance than our model is based on a complex CNN equipped with attention mechanisms and taking advantage of both learned and handcrafted features. Whereas for RAF-DB Compound, the method with the highest accuracy is a ResNet18 model (that has proven to be superior to VGG-Face) with a so called separate loss that enhances the initial architecture to consider intra and inter class information for the ER process. Clearly, our model is advantageous in terms of simplicity, besides, it is possible that if we rely on a more complex backbone models the performance of the multi-branch RBF model could be even superior. Finally, please note that our model is not doing any ad hoc feature learning process: we are relying on the pre-trained VGG-Face model, while most other references comprise expensive learning-from-scratch or fine-tuning processes that often use additional external data.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Discussion",
      "text": "We have presented an experimental evaluation of the proposed Multi-Branch Deep RBF Network model. We reported experiments in six datasets widely used for ER, two of them were variants that presented particular challenges with which state-of-the-art methods struggle. Our experimental evaluation showed that the proposed model outperforms considerably to reference models that included a similar model formed by dense layers only and the backbone. The comparison with the reference models together with a visual inspection of the learned centers comprises evidence that local information as captured by the proposed model is useful for approaching the ER task.\n\nOn the other hand, the proposed model compared favorably with recent methodologies that are based on much more complex techniques and procedures. This is an outstanding result given that the proposed model relies on a very generic, yet effective, backbone model: VGG-Face. Interestingly, it was shown that the proposed model offers is more advantageous in datasets with more challenging conditions, namely: small-medium sample size, with high class-imbalance, class overlap and with images coming from two different distributions. The obtained results are thus encouraging and motivate further research on the incorporation of local information into deep learning.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Conclusions",
      "text": "We introduced the Multi-Branch Deep RBF Network, a model that improves CNNs by a mechanism that allow it to incorporate local information in the recognition process. The proposed model relies on VGG-Face as backbone for feature extraction, where the last convolutional layer of this model is connected to multiple branches of RBF units. The outputs of these are concatenated and connected to a softmax layer. The proposed model is initialized with VGG-Face and the RBF layers are fine tuned. Experimental results are reported in six ER datasets.\n\nThe following summarize the main findings of this work:\n\n• The inclusion of local information, via the multi-branch RBF units, improves significantly the performance of a CNN model. In fact, the proposed model outperforms a similar model extended with a dense layer, showing that the RBF units are responsible of the improvement in performance.\n\n• The proposed model is competitive with state-of-the-art methods based on more complex architectures and mechanisms, even when we rely on a standard backbone (VGG-Face). The model achieved competitive results in 3 out of 5 datasets with a much simpler implementation.\n\n• The proposed model proved to be more advantageous for datasets with challenging conditions that include small sample size, high overlap among classes, datasets with mixed distributions in the test set and with high imbalance ratios.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the remainder of this",
      "page": 4
    },
    {
      "caption": "Figure 1: Diagram of the proposed Multi-branch Deep Radial Basis Function Network architecture. A backbone",
      "page": 4
    },
    {
      "caption": "Figure 1: We relied on VGG-Face [36] as backbone because is a well known and generic enough CNN for",
      "page": 4
    },
    {
      "caption": "Figure 1: ). These outputs are ﬂattened and feed to the RBF branches in a ﬁxed order.",
      "page": 5
    },
    {
      "caption": "Figure 1: shows a branch per feature map, a reduced number of branches could be",
      "page": 5
    },
    {
      "caption": "Figure 2: This is important to mention as this comparison will allow us to determine the",
      "page": 5
    },
    {
      "caption": "Figure 2: Illustration of units used in the proposed model (left) and the CNN variant, Multi-branch CNN (right). Each",
      "page": 5
    },
    {
      "caption": "Figure 4: The idea of",
      "page": 6
    },
    {
      "caption": "Figure 3: Sample images associated to different emotions for the CK+, JAFFE, FER2013 and RAF-DB datasets. Please",
      "page": 7
    },
    {
      "caption": "Figure 5: the results of this evaluation for the six considered datasets. Results are shown as heat maps (the",
      "page": 7
    },
    {
      "caption": "Figure 4: Sample images associated to different emotions considered in the RAF-DB Compound dataset.",
      "page": 8
    },
    {
      "caption": "Figure 6: shows the centers for a conﬁguration with 4 branches and 8 RBF units per branch, the reported performance",
      "page": 8
    },
    {
      "caption": "Figure 7: shows the centers but for a different conﬁguration: 8 branches and 4 RBF units each, with a reported",
      "page": 8
    },
    {
      "caption": "Figure 7: that seems to be learning",
      "page": 8
    },
    {
      "caption": "Figure 5: FER recognition performance when varying the number of branches and RBF units per branch in the proposed",
      "page": 9
    },
    {
      "caption": "Figure 6: Visualization of the centers of the RBF units for a model with 4 branches and 8 RBF units. The left plot",
      "page": 10
    },
    {
      "caption": "Figure 2: ). The idea is to determine",
      "page": 10
    },
    {
      "caption": "Figure 7: Visualization of the centers of the RBF units for a model with 8 branches and 4 RBF units. The left plot",
      "page": 11
    },
    {
      "caption": "Figure 8: shows the confusion",
      "page": 11
    },
    {
      "caption": "Figure 9: shows sample images from the surprise and fear categories. The",
      "page": 11
    },
    {
      "caption": "Figure 10: ) and highly imbalanced (4 classes comprise 72%",
      "page": 12
    },
    {
      "caption": "Figure 11: shows these confusion matrices. From this Figure it can be seen that the proposed model is",
      "page": 16
    },
    {
      "caption": "Figure 8: Confusion matrices obtained by VGG-Face (top) and the proposed model (down) for the CK+-JAFFE dataset.",
      "page": 17
    },
    {
      "caption": "Figure 9: Images from the test set of CK+-JAFFE for the surprise and fear categories. In each sub-ﬁgure the top row",
      "page": 18
    },
    {
      "caption": "Figure 10: Sample images from two of the classes in the RAF-DB Compound dataset, image taken from http:",
      "page": 18
    },
    {
      "caption": "Figure 11: Confusion matrices obtained by VGG-Face (top) MB-CNN (middle) and the proposed model (down) for the",
      "page": 19
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "CK+ [25]\nJAFFE [27]\nCK+JAFFE\nFER 2013 [12]\nRAF-DB [19]\nRAF-DB Compound [21]",
          "#Tr.": "877\n143\n1,020\n28,709\n12,271\n3,162",
          "#Val.": "94\n35\n129\n3,589\n3,068\n792",
          "#Test": "123\n35\n158\n3,589\n3,068\n792",
          "# E": "7\n7\n8\n7\n7\n11"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.466\n0.635": "0.829\n0.812\n0.810\n0.843",
          "0.824\n0.791": "0.881\n0.881\n0.910\n0.927",
          "0.891\n0.891\n0.893\n0.885\n0.916\n0.916\n0.879\n0.889\n0.897\n0.885\n0.937\n0.875": "0.916\n0.897\n0.862\n0.889\n0.916\n0.854\n0.895\n0.920\n0.908\n0.874\n0.958\n0.937\n0.899\n0.918\n0.908\n0.908\n0.958\n0.979\n0.916\n0.887\n0.893\n0.870\n0.916\n0.791"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Datasets": "CK+\nJAFFE\nCK+-JA\nFER13\nRAF\nRAF-C",
          "VGG-Face": "0.8291 ± 0.003\n0.6352 ± 0.012\n0.8341 ± 0.0018\n0.4731 ± 0.035\n0.4289 ± 0.058\n0.2330 ± 0.0012",
          "MB-CNN": "0.8381 ± 0.051\n0.5971 ± 0.032\n0.8594 ± 0.021\n0.6751 ± 0.0082\n0.7237 ± 0.041\n0.4739 ± 0.0034",
          "MB-RBFN": "0.9964 ± 0.0037\n0.9796 ± 0.0314\n0.9872 ± 0.0024\n0.6815 ± 0.0097\n0.810 ± 0.0014\n0.5768 ± 0.0074"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 2: ), it is also outperformed by the proposed model in every",
      "data": [
        {
          "CK+": "Ref.",
          "JAFFE": "Ref.",
          "FER2013": "Ref.",
          "RAF-DB": "Ref.",
          "RAF-DB C": "Ref."
        },
        {
          "CK+": "[4]\n[31]\n[38]\n[48]\n[29]\nVGG-Face\nMB-CNN",
          "JAFFE": "[18]\n[42]\n[31]\n[53]\n[45]",
          "FER2013": "[23]\n[18]\n[31]\n[33]\n[41]",
          "RAF-DB": "[47]\n[7]\n[52]\n[55]\n[23]",
          "RAF-DB C": "[22]\n[21]\n[21]\n[23]\n[16]\n-\n-"
        },
        {
          "CK+": "MB-RBF",
          "JAFFE": "",
          "FER2013": "",
          "RAF-DB": "",
          "RAF-DB C": "-"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep k-nn for noisy labels",
      "authors": [
        "Dara Bahri",
        "Heinrich Jiang",
        "Maya Gupta"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning"
    },
    {
      "citation_id": "2",
      "title": "Facial expression recognition with multithreaded cascade of rotation-invariant hog",
      "authors": [
        "Jinhui Chen",
        "Tetsuya Takiguchi",
        "Yasuo Ariki"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "3",
      "title": "Adaboost-knn with direct optimization for dynamic emotion recognition",
      "authors": [
        "Luefeng Chen",
        "Min Wu",
        "Witold Pedrycz",
        "Kaoru Hirota"
      ],
      "venue": "Emotion Recognition and Understanding for Emotional Human-Robot Interaction Systems"
    },
    {
      "citation_id": "4",
      "title": "Facial motion prior networks for facial expression recognition",
      "authors": [
        "Yuedong Chen",
        "Jianfeng Wang",
        "Shikai Chen",
        "Zhongchao Shi",
        "Jianfei Cai"
      ],
      "year": "2019",
      "venue": "IEEE Visual Communications and Image Processing"
    },
    {
      "citation_id": "5",
      "title": "Facial expression classification based on svm, knn and mlp classifiers",
      "authors": [
        "Hivi Ismat",
        "Maiwan Bahjat"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Advanced Science and Engineering (ICOASE)"
    },
    {
      "citation_id": "6",
      "title": "Universals and cultural differences in facial expressions of emotion",
      "authors": [
        "Paul Ekman"
      ],
      "year": "2917",
      "venue": "Nebraska Symposium on Motivation"
    },
    {
      "citation_id": "7",
      "title": "Facial expression recognition in the wild via deep attentive center loss",
      "authors": [
        "Amir Hossein",
        "Xiaojun Qi"
      ],
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "8",
      "title": "Local learning with deep and handcrafted features for facial expression recognition",
      "authors": [
        "Mariana-Iuliana Georgescu",
        "Tudor Ionescu",
        "Marius Popescu"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "9",
      "title": "Geometric feature-based facial expression recognition in image sequences using multi-class adaboost and support vector machines",
      "authors": [
        "Deepak Ghimire",
        "Joonwhoan Lee"
      ],
      "year": "2013",
      "venue": "Sensors"
    },
    {
      "citation_id": "10",
      "title": "Recognition of facial expressions based on salient geometric features and support vector machines",
      "authors": [
        "Deepak Ghimire",
        "Joonwhoan Lee",
        "Ze-Nian Li",
        "Sunghwan Jeong"
      ],
      "year": "2017",
      "venue": "Recognition of facial expressions based on salient geometric features and support vector machines"
    },
    {
      "citation_id": "11",
      "title": "Recognition of facial expressions based on CNN features",
      "authors": [
        "M Sonia",
        "Jorge González-Lozoya",
        "Luis De La Calleja",
        "Hugo Pellegrin",
        "Ma Jair Escalante",
        "Antonio Medina",
        "Ruiz"
      ],
      "year": "2020",
      "venue": "Multim. Tools Appl"
    },
    {
      "citation_id": "12",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "Ian Goodfellow",
        "Dumitru Erhan",
        "Pierre Carrier",
        "Aaron Courville",
        "Mehdi Mirza",
        "Ben Hamner",
        "Will Cukierski",
        "Yichuan Tang",
        "David Thaler",
        "Dong-Hyun Lee"
      ],
      "year": "2013",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "13",
      "title": "Face expression detection on kinect using active appearance model and fuzzy logic",
      "authors": [
        "Alexander As Gunawan"
      ],
      "year": "2015",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "14",
      "title": "A real time facial expression classification system using local binary patterns",
      "authors": [
        "Anjith Sl Happy",
        "Aurobinda George",
        "Routray"
      ],
      "year": "2012",
      "venue": "2012 4th International conference on intelligent human computer interaction (IHCI)"
    },
    {
      "citation_id": "15",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "16",
      "title": "Face behavior à la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior à la carte: Expressions, affect and action units in a single network"
    },
    {
      "citation_id": "17",
      "title": "Locally linear support vector machines",
      "authors": [
        "L'ubor Ladický",
        "H Philip",
        "Torr"
      ],
      "year": "2011",
      "venue": "Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML'11"
    },
    {
      "citation_id": "18",
      "title": "Attention mechanism-based cnn for facial expression recognition",
      "authors": [
        "Jing Li",
        "Kan Jin",
        "Dalin Zhou",
        "Naoyuki Kubota",
        "Zhaojie Ju"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "19",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "20",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "22",
      "title": "Separate loss for basic and compound facial expression recognition in the wild",
      "authors": [
        "Yingjian Li",
        "Yao Lu",
        "Jinxing Li",
        "Guangming Lu"
      ],
      "year": "2019",
      "venue": "Asian Conference on Machine Learning"
    },
    {
      "citation_id": "23",
      "title": "Fine-grained facial expression recognition in the wild",
      "authors": [
        "Liqian Liang",
        "Congyan Lang",
        "Yidong Li",
        "Songhe Feng",
        "Jian Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "24",
      "title": "Multimodal local-global ranking fusion for emotion recognition",
      "authors": [
        "Paul Pu Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "25",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognition-workshops"
    },
    {
      "citation_id": "26",
      "title": "Coding facial expressions with gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "27",
      "title": "The Japanese Female Facial Expression (JAFFE) Database",
      "authors": [
        "Michael Lyons",
        "Miyuki Kamachi",
        "Jiro Gyoba"
      ],
      "year": "1998",
      "venue": "The Japanese Female Facial Expression (JAFFE) Database"
    },
    {
      "citation_id": "28",
      "title": "Shape analysis of local facial patches for 3d facial expression recognition",
      "authors": [
        "Ahmed Maalej",
        "Boulbaba Ben Amor",
        "Mohamed Daoudi",
        "Anuj Srivastava",
        "Stefano Berretti"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "29",
      "title": "Identity-aware convolutional neural network for facial expression recognition",
      "authors": [
        "Zibo Meng",
        "Ping Liu",
        "Jie Cai",
        "Shizhong Han",
        "Yan Tong"
      ],
      "year": "2017",
      "venue": "2017 12th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "30",
      "title": "Chinese multimodal emotion recognition in deep and traditional machine leaming approaches",
      "authors": [
        "Haotian Miao",
        "Yifei Zhang",
        "Weipeng Li",
        "Haoran Zhang",
        "Daling Wang",
        "Shi Feng"
      ],
      "year": "2018",
      "venue": "2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia)"
    },
    {
      "citation_id": "31",
      "title": "Deep-emotion: Facial expression recognition using attentional convolutional network",
      "authors": [
        "Shervin Minaee",
        "Amirali Abdolrashidi"
      ],
      "year": "2019",
      "venue": "Deep-emotion: Facial expression recognition using attentional convolutional network",
      "arxiv": "arXiv:1902.01019"
    },
    {
      "citation_id": "32",
      "title": "Machine learning",
      "authors": [
        "Tom Mitchell"
      ],
      "year": "1997",
      "venue": "International Edition. McGraw-Hill Series in Computer Science. McGraw-Hill"
    },
    {
      "citation_id": "33",
      "title": "Going deeper in facial expression recognition using deep neural networks",
      "authors": [
        "Ali Mollahosseini",
        "David Chan",
        "Mohammad Mahoor"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter conference on applications of computer vision (WACV)"
    },
    {
      "citation_id": "34",
      "title": "A review of learning vector quantization classifiers",
      "authors": [
        "David Nova",
        "Pablo Estévez"
      ],
      "year": "2014",
      "venue": "Neural Comput. Appl"
    },
    {
      "citation_id": "35",
      "title": "Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning",
      "authors": [
        "Nicolas Papernot",
        "Patrick Mcdaniel"
      ],
      "year": "2018",
      "venue": "Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning",
      "arxiv": "arXiv:1803.04765"
    },
    {
      "citation_id": "36",
      "title": "Deep face recognition",
      "authors": [
        "M Omkar",
        "Andrea Parkhi",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2015",
      "venue": "British Machine Vision Conference"
    },
    {
      "citation_id": "37",
      "title": "Enhancing cnn with preprocessing stage in automatic emotion recognition",
      "authors": [
        "Diah Anggraeni Pitaloka",
        "Ajeng Wulandari",
        "T Basaruddin",
        "Dewi Yanti"
      ],
      "year": "2017",
      "venue": "Procedia computer science"
    },
    {
      "citation_id": "38",
      "title": "A face expression recognition using cnn & lbp",
      "authors": [
        "Rahul Ravi",
        "Yadhukrishna"
      ],
      "year": "2020",
      "venue": "2020 Fourth International Conference on Computing Methodologies and Communication (ICCMC)"
    },
    {
      "citation_id": "39",
      "title": "A deep framework for facial emotion recognition using light field images",
      "authors": [
        "Alireza Sepas-Moghaddam",
        "Ali Etemad",
        "Paulo Correia",
        "Fernando Pereira"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "40",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "Caifeng Shan",
        "Shaogang Gong",
        "Peter Mcowan"
      ],
      "year": "2009",
      "venue": "Image and vision Computing"
    },
    {
      "citation_id": "41",
      "title": "E-fcnn for tiny facial expression recognition",
      "authors": [
        "Jie Shao",
        "Qiyu Cheng"
      ],
      "year": "2021",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "42",
      "title": "Image augmentation for classifying facial expression images by using deep neural network pre-trained with object image database",
      "authors": [
        "Yoshihiro Shima",
        "Yuki Omori"
      ],
      "year": "2018",
      "venue": "Proceedings of the 3rd International Conference on Robotics, Control and Automation"
    },
    {
      "citation_id": "43",
      "title": "Real-time mobile facial expression recognition system-a case study",
      "authors": [
        "Myunghoon Suk",
        "Balakrishnan Prabhakaran"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "44",
      "title": "Histogram-based local descriptors for facial expression recognition (fer): A comprehensive study",
      "authors": [
        "Cigdem Turan",
        "Kin-Man Lam"
      ],
      "year": "2018",
      "venue": "Journal of visual communication and image representation"
    },
    {
      "citation_id": "45",
      "title": "Facial expression classification using vanilla convolution neural network",
      "authors": [
        "Sarvani Lakshmi",
        "Pm Ashok Videla",
        "Kumar"
      ],
      "year": "2020",
      "venue": "2020 7th International Conference on Smart Structures and Systems (ICSSS)"
    },
    {
      "citation_id": "46",
      "title": "Deep networks with rbf layers to prevent adversarial examples",
      "authors": [
        "Petra Vidnerová",
        "Roman Neruda"
      ],
      "year": "2018",
      "venue": "International Conference on Artificial Intelligence and Soft Computing"
    },
    {
      "citation_id": "47",
      "title": "Suppressing uncertainties for large-scale facial expression recognition",
      "authors": [
        "Kai Wang",
        "Xiaojiang Peng",
        "Jianfei Yang",
        "Shijian Lu",
        "Yu Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "48",
      "title": "Region attention networks for pose and occlusion robust facial expression recognition",
      "authors": [
        "Kai Wang",
        "Xiaojiang Peng",
        "Jianfei Yang",
        "Debin Meng",
        "Yu Qiao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "49",
      "title": "Real-time facial expression recognition for affective computing based on kinect",
      "authors": [
        "Wei Wei",
        "Qingxuan Jia",
        "Gang Chen"
      ],
      "year": "2016",
      "venue": "2016 IEEE 11th Conference on Industrial Electronics and Applications (ICIEA)"
    },
    {
      "citation_id": "50",
      "title": "Fine-grained comparisons with attributes",
      "authors": [
        "Aron Yu",
        "Kristen Grauman"
      ],
      "year": "2017",
      "venue": "Visual Attributes"
    },
    {
      "citation_id": "51",
      "title": "Deep-rbf networks revisited: Robust classification with rejection",
      "authors": [
        "Pourya Habib Zadeh",
        "Reshad Hosseini",
        "Suvrit Sra"
      ],
      "year": "2018",
      "venue": "Deep-rbf networks revisited: Robust classification with rejection",
      "arxiv": "arXiv:1812.03190"
    },
    {
      "citation_id": "52",
      "title": "Facial expression recognition with inconsistently annotated datasets",
      "authors": [
        "Jiabei Zeng",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "53",
      "title": "Facial expression recognition based on deep convolution long short-term memory networks of double-channel weighted mixture",
      "authors": [
        "Hepeng Zhang",
        "Bin Huang",
        "Guohui Tian"
      ],
      "year": "2020",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "54",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "Guoying Zhao",
        "Matti Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "55",
      "title": "Peak-piloted deep network for facial expression recognition",
      "authors": [
        "Xiangyun Zhao",
        "Xiaodan Liang",
        "Luoqi Liu",
        "Teng Li",
        "Yugang Han",
        "Nuno Vasconcelos",
        "Shuicheng Yan"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    }
  ]
}