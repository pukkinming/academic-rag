{
  "paper_id": "2501.02177v2",
  "title": "Imuface: Real-Time, Low-Power, Continuous 3D Facial Reconstruction Through Earphones",
  "published": "2025-01-04T03:38:37Z",
  "authors": [
    "Xianrong Yao",
    "Lingde Hu",
    "Yincheng Jin",
    "Yang Gao",
    "Zhanpeng Jin"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial expression reconstruction technology offers considerable potential in areas like human-computer interaction, affective computing, and virtual reality. To tackle the privacy challenges and environmental constraints inherent in traditional camerabased systems, researchers have recently introduced ear-worn devices as a viable solution. Nevertheless, these methods still demand enhancements, particularly in aspects such as design appeal and energy efficiency, to achieve broader applicability and practical use. This paper introduces a system called IMUFace. It uses inertial measurement units (IMUs) embedded in wireless earphones to detect subtle ear movements caused by facial muscle activities, allowing for covert and low-power facial reconstruction. A user study involving 12 participants was conducted, and a deep learning model named IMUTwinTrans was proposed. The results show that IMUFace can accurately predict users' facial landmarks with a precision of 2.21 mm, using only five minutes of training data. The predicted landmarks can be utilized to reconstruct a three-dimensional facial model. IMUFace operates at a sampling rate of 30 Hz with a relatively low power consumption of 58 mW. The findings presented in this study demonstrate the real-world applicability of IMUFace and highlight potential directions for further research to facilitate its practical adoption.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial expressions play a crucial role in interpersonal communication, as they convey a wide range of information, including emotional states  [Bassili, 1979 , Krumhuber et al., 2023]  and physical health conditions  [Argaud et al., 2018 , Roter et al., 2006] . As a result, facial expression reconstruction has become an essential tool for applications requiring facial analysis. This technology is now foundational in domains like human-computer interaction (HCI), virtual reality (VR), affective computing, and healthcare. For instance, in VR, it can enhance users' real-time perception of emotions and facial expressions, leading to a more immersive experience  [Kumari et al., 2015] . In metaverse platforms, it facilitates rapid understanding and connection within virtual environments  [Cha et al. ,   2020 , Frith, 2009]] . Furthermore, it supports the development of silent speech interfaces, enabling seamless communication between humans and computers  [Denby et al., 2010] .\n\nTraditional methods for reconstructing facial expressions often relied on manual annotation or geometric modeling. These early approaches were labor-intensive, difficult to automate, and prone to practical limitations. However, with advancements in machine learning and computer vision, automated methods have significantly improved the field. Vision-based techniques  [Cootes et al., 2001 , 1995 , Wang et al., 2020]  enable precise facial landmark detection and high-quality animation generation. Nevertheless, their reliance on direct camera views introduces several challenges. These methods require users to face the camera under controlled conditions, making them less effective in environments with varying lighting, resolution, or occlusion. Additionally, the direct capture of facial imagery raises privacy concerns, particularly in dynamic or sensitive scenarios involving 3D head movements.\n\nTo address these drawbacks, researchers have shifted toward non-visual approaches for facial expression reconstruction. Wearable devices combined with multimodal sensors have emerged as a promising solution, offering greater flexibility and enhanced privacy. Examples include necklaces  [Chen et al., 2021] , hats  [Bello et al., 2023] , glasses  [Xie et al., 2021b , Li et al., 2024a,b] , and ear-mounted devices  [Choi et al., 2022 , Song et al., 2022 , Wu et al., 2021 , Li et al., 2022 , Zhang et al., 2023] . Earlier designs, such as C-Face  [Chen et al., 2020] , utilized miniature cameras mounted on the ear to capture expressions, providing a flexible deployment option. However, privacy concerns persisted due to the camera's proximity and mobility. Subsequent innovations like BioFace-3D  [Wu et al., 2021]  and EarIO  [Li et al., 2022]  introduced non-visual 3D facial reconstruction with ear-mounted devices, though their designs sometimes caused discomfort due to protrusions or facial contact. More recent developments, such as EARFace  [Zhang et al., 2023] , incorporated acoustic sensors into earplug designs, achieving higher usability ratings. Nonetheless, the high power consumption of acoustic sensors remains a limitation for extended use. Therefore, creating a portable, comfortable, and energy-efficient system capable of real-time facial expression reconstruction continues to be a significant challenge.\n\nTo address these challenges, we present IMUFace, an earphone platform equipped with two IMUs for precise facial reconstruction. These IMUs are highly sensitive, allowing them to capture subtle ear movements caused by facial muscle activity quickly and accurately.\n\nWe developed a deep learning model called IMUTwinTrans to achieve 3D facial reconstruction beyond the limitations of cameras. This model, based on the ConvTransformer architecture, is capable of simultaneously leveraging the temporal and frequency features of IMU signals to establish the correspondence between IMU signals and facial landmarks. Additionally, the model adopts a lightweight design to meet the requirements for real-time performance. During testing, the well-trained model can localize facial landmarks directly from IMU signals without visual input. The predicted facial landmarks are then filtered and fitted into a generic 3D head model to generate continuous 3D facial animation. Fig.  1  showcases 3D facial reconstruction by the IMUFace system, highlighting its ability to track diverse facial motions. The embedded IMU gyroscope, a critical sensor, precisely measures subtle angular changes from facial muscle movements. As depicted in Fig.  1 , real-time IMU data (X, Y, Z axes, one-minute window) distinctly reflects varying facial expressions through characteristic signal patterns. For instance, eyebrow raises and mouth movements produce discernible peaks and troughs. This visual correlation confirms the IMU sensor's high sensitivity and discrimination for facial expressions, providing rich input for deep learning models and validating our IMU-based recognition method. This explanation clarifies the physical meaning of IMU data in facial expression reconstruction.\n\nThe main contributions of this study are as follows:\n\n1. We presented the first attempt to use IMU sensor signals on wireless earphones for facial landmark tracking to achieve user-friendly, continuous 3D facial expression reconstruction. 2. We designed a lightweight deep learning model, IMUTwinTrans, based on the ConvTransformer architecture, to achieve 3D facial reconstruction by leveraging both temporal and frequency features of IMU signal while meeting real-time performance requirements. 3. We optimized the power consumption of the system, enabling it to operate at a sampling rate of 30 Hz with a power consumption of only 58 mW.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "The field of non-wearable 3D face reconstruction has advanced significantly, driven by developments in morphable models, photometric stereo methods, and deep learning approaches. These methods address challenges such as variations in pose, illumination, and image quality, enabling high-fidelity reconstruction from unconstrained inputs.\n\nMorphable Models serve as a foundational tool for compactly representing facial geometry and texture. Roth et al.  [Roth et al., 2016]  introduced an adaptive 3DMM-based method for reconstructing faces from unconstrained photo collections. Their approach utilized a coarse-to-fine fitting strategy, enhancing robustness against low-quality images and diverse imaging conditions. An earlier work  [Roth et al., 2015]  proposed an algorithm for generating accurate 3D models from 2D images without relying on metadata, incorporating Laplace editing to improve resilience to pose and illumination variations.\n\nPhotometric Stereo techniques have been pivotal in capturing surface normals from multiple viewpoints, contributing to detailed facial reconstructions. For instance, Roth et al.  [Roth et al., 2015]  integrated photometric stereo with landmark constraints to achieve higher reconstruction accuracy. Xie et al.  [Xie et al., 2021a ] demonstrated a practical smartphone-based method using calibrated screen lighting, addressing challenges like hand jitter through alignment and outlier removal techniques.\n\nDeep Learning has transformed 3D face reconstruction, enabling high-fidelity results from single images. Gecer et al.  [Gecer et al., 2019]  combined Generative Adversarial Networks (GANs) with Deep Convolutional Neural Networks (DCNNs) in GANFIT, achieving photorealistic reconstructions of facial shape and texture. Fast-GANFIT  [Gecer et al., 2021] , their subsequent work, introduced a self-supervised regression-based initialization scheme, improving speed and stability.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "3D Face Reconstruction With Different Models",
      "text": "The 3D Morphable Model (3DMM)  [Blanz and Vetter, 2023]  remains a cornerstone in 3D face reconstruction. While traditional methods optimize coefficients for reconstruction, which is computationally expensive, recent advancements utilize deep learning to directly predict these coefficients, offering greater efficiency and precision.\n\nConvolutional Neural Networks (CNNs) are widely employed in this field.  Richardson et al. [Richardson et al., 2017]  developed an end-to-end CNN framework for reconstructing facial geometry in a coarse-to-fine manner. Tian et al.  [Tian et al., 2018]  presented a CNN-based method for reconstructing 3D faces from two images, demonstrating superior performance on benchmark datasets.\n\nEncoder-Decoder Architectures have also gained prominence.  Ji et al. [Ji et al., 2020]  proposed a multi-task encoder-decoder framework based on a Siamese network to ensure view-consistent reconstruction of facial shapes and textures from single images. Similarly, Li et al.  [Li et al., 2020]  enhanced accuracy by employing a dual-pathway encoder-decoder network that separates facial attributes.\n\nTransformer Models have recently achieved remarkable progress. Chen et al.  [Chen et al., 2022]  utilized a conditional Generative Adversarial Network (cGAN) combined with a mesh transformer for cross-domain face synthesis, introducing a reprojection consistency loss for self-supervised learning.  Yaermaimaiti et al. [Yaermaimaiti et al., 2024]  integrated ResNet and Transformer modules, leveraging self-attention mechanisms to improve parameter prediction and reconstruction quality.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Non-Wearable 3D Face Reconstruction",
      "text": "Traditional methods in 3D face reconstruction rely on external imaging devices like RGB cameras to process extensive photo collections. Roth et al.  [Roth et al., 2016]  proposed a method for reconstructing personalized 3D face models from photos captured under varying poses and lighting conditions. Their approach employed a 3D Morphable Model and a novel photometric stereo formulation to achieve high-quality results. Furthermore, Roth et al.  [Roth et al., 2015]  introduced an algorithm that integrates photometric stereo with Laplace editing to handle unconstrained 2D image collections effectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Wearable 3D Face Reconstruction",
      "text": "Recent research has focused on wearable devices for 3D face reconstruction, enabling real-time applications with minimal interference. Chen et al.  [Chen et al., 2020]  designed an ear-mounted device equipped with two miniature cameras to capture subtle facial contour changes. Their deep learning model continuously outputs facial feature points, enabling real-time expression reconstruction. Aoki et al.  [Aoki et al., 2021]  developed a glasses-shaped device featuring two cameras and three mirrors to capture facial expressions unobtrusively, achieving high accuracy in emotion recognition.\n\nMultimodal Approaches have further enhanced wearable devices.  Wu et al. [Wu et al., 2021]  introduced a single-earpiece biosensing system that tracks facial landmarks and generates 3D animations from biosignals, achieving performance comparable to camera-based methods. Li et al.  [Li et al., 2024b]  utilized IMUs in smart glasses to estimate action unit intensities, enabling real-time, lowpower facial reconstruction. Zhang et al.  [Zhang et al., 2023]  employed in-ear acoustic sensors to track facial landmarks by detecting ear canal shape changes, providing a privacy-preserving solution for 3D facial reconstruction.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Background",
      "text": "This section provides an overview of the foundational concepts, covering: (1) the role of facial muscles in generating expressions and (2) the operational principle of IMUFace.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Facial Muscles And Expressions",
      "text": "Facial expressions play an essential role in human communication by conveying emotions and intentions that can be observed and interpreted by others. The human face contains 43 distinct mimetic muscles, which collectively enable the production of over 10,000 unique expressions. These muscles are categorized into three primary groups: orbital, nasal, and oral. The orbital muscles are responsible for eyelid movements, the nasal muscles manage actions around the nose, and the oral muscles control the lips and mouth. The coordination of these groups enables the depiction of a broad range of emotions, such as happiness, sadness, surprise, and fear.\n\nThe intricate nature of facial expressions stems from the synchronized activity of these muscle groups, which exert forces on the skin to form visible emotional cues. For instance, the frontalis muscle elevates the eyebrows during expressions of surprise, while jaw movements in amazement or joy involve the temporalis and lateral pterygoid muscles.\n\nFigure  2 : Overview of the IMUFace system.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Sensing Principle",
      "text": "The interplay between facial muscle activity and associated anatomical changes provides the basis for innovative sensing methods that indirectly capture facial expressions. Muscle contractions during expressions not only generate visible changes but also induce subtle structural alterations in nearby areas, such as the ear canal, due to the dynamics of interconnected tissues and bones.\n\nFor example, jaw movements that occur during expressions like joy or surprise influence the mandibular condyle, causing changes in the ear canal's shape. Similarly, the temporalis muscle, a key component of the head's musculature, transmits mechanical effects of facial expressions to the ear region.\n\nBy embedding an Inertial Measurement Unit (IMU) in an ear-mounted device, such as a wireless earbud, these changes can be detected. As facial expressions deform the ear canal, they alter the earbud's position and orientation. The IMU captures these variations using accelerometers and gyroscopes, converting them into measurable signals. Each facial expression produces a distinct motion pattern, allowing the IMU to differentiate expressions based on the recorded data. Advanced machine learning algorithms process these signals to identify specific expressions, enabling unobtrusive and practical facial expression recognition. This approach capitalizes on the natural linkage between facial musculature and ear canal dynamics, offering a novel solution for wearable expression recognition systems.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Method",
      "text": "In Fig.  2 , we present an overview of the architecture of our proposed IMUFace system, which is divided into two distinct phases: a training phase and a testing phase.\n\nThe training phase aims to teach a deep learning model to map IMU signals to facial movements using two synchronized data streams: video and IMU. The video stream provides ground truth by detecting and aligning faces in each frame, from which 2D facial landmarks are extracted. Meanwhile, the IMU stream, collected from earphone-embedded sensors, captures subtle facial muscle movements. This data undergoes a three-step preprocessing pipeline: a quick calibration to adapt to individual facial structures, a Short-Time Fourier Transform (STFT) to convert signals into the frequency domain, and windowed segmentation for temporal analysis. The processed IMU data is then fed into our transformer-based model, IMUTwinTrans, which predicts facial landmarks. A loss function measures the difference between the predicted and ground-truth landmarks, enabling the model to optimize its parameters through backpropagation.\n\nOnce training is complete, the system enters the testing phase for inference. In this phase, only the IMU data stream is required. The incoming IMU data undergoes the same preprocessing steps as in the training phase (calibration, STFT, and segmentation). This processed data is then fed into the fully trained model. The model, having learned the correlation between IMU signals and facial expressions, directly outputs the estimated facial landmarks in real-time. As the final step, these predicted landmarks are fitted to the parameters of the FLAME model  [Li et al., 2017]  to reconstruct and animate a realistic 3D facial avatar.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Hardware Prototype Design",
      "text": "We constructed an earphone-based, proof-of-concept prototype capable of capturing IMU signals generated by facial expression movements. It primarily consists of two parts: a tiny sensor board and an earphone frame. We selected the XIAO nRF52840 Sense as the sensor board, with a Bluetooth chip and a 6-axis IMU chip. The Bluetooth chip supports Bluetooth 5.0 for data transmission and directly interfaces with the IMU chip, extracting data using the I2C protocol. The sensor board measures 21.0 mm × 17.5 mm. It provides a compact and efficient solution with low power consumption, meeting our requirements perfectly.\n\nThe earphone frame we chose is a standard wireless Bluetooth earphone found in daily life, which can be any standard earphone, ensuring the broad applicability of the system. We stream the 6-axis IMU data from both earphones at a sampling rate of 30 Hz with a 16-bit resolution, ensuring the comprehensiveness of the collected muscle information. The schematic diagram of the device is shown in Fig.  3 . First, we attach the battery to the back of the XIAO nRF52840 Sense, then mount it on the earphone. Actually, many modern wireless earphones/earbuds are already equipped with the IMU module inside, which could be easily adapted for deploying the IMUFace approach.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Processing Pipeline",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Calibration",
      "text": "Since different users may have different neutral or natural head orientations, we ask users to wear the earbuds and keep their heads naturally upright before starting the video recording. We then record 4 seconds of IMU data from all channels and take the sample mean of these values, referred to as the offset. To compensate for the drift in gyroscope measurements, we adjust the gyroscope readings of each frame by the calculated offset.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Signal Synchronization And Filtering",
      "text": "To ensure synchronization between the two modalities of data streams, we record timestamps for both video data and IMU sensor data. Using the starting timestamp of the video data as a reference, we find the closest corresponding timestamp in the IMU sensor data. Since the sampling rate of the IMU sensor data fluctuates around 30 Hz, we resample the IMU sensor data to 30 Hz. Although our hardware device includes a built-in bandpass filter for reading IMU sensor data, we apply a high-pass filter with a cutoff frequency of 0.1 Hz to improve the signal-to-noise ratio.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Signal Normalization",
      "text": "We utilize the open-source tool OpenFace  [Baltrusaitis et al., 2018]  to extract facial landmarks from facial videos. However, the positions of these landmarks can be affected by factors such as head position, pose variations, and changes in camera angle and placement. Following the approach used in the 300W dataset  [Sagonas et al., 2013] , we standardize the facial landmarks to a consistent reference frame through a series of transformations.\n\nFirst, we select the nose tip as the origin and shift all landmarks accordingly, as defined in Eq. (  1 ):\n\nwhere P i = (x i , y i ) denotes the original landmark coordinates, and (x nose , y nose ) represents the position of the nose tip.\n\nNext, we perform a coordinate rotation to align the x-axis with the direction from the left outer eye corner to the right outer eye corner. The rotation angle θ is computed using Eq. (  2 ):\n\nwhere (x left , y left ) and (x right , y right ) correspond to the coordinates of the outer corners of the left and right eyes, respectively.\n\nThe rotated landmark coordinates P ′′ i are then obtained by applying the transformation in Eq. (3):\n\nTo ensure uniform scaling, we normalize the distance between the two outer eye corners to 1. The normalization factor d is calculated according to Eq. (  4 ):\n\nFinally, the normalized landmark coordinates P ′′′ i are obtained by applying Eq. (  5 ):\n\nFig.  4  shows the facial landmark coordinates before and after alignment.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Short-Time Fourier Transform",
      "text": "Each facial expression movement is associated with unique frequency components. Since the STFT better isolates noise components and enhances the clarity of extracted features from time-series Figure  5 : Illustration of the model structure.\n\ndata  [Zhang et al., 2022a] , we use STFT to convert time-domain signals into time-frequency domain spectrograms. The STFT of a signal x(t) is computed as (Eq. (  6 )):\n\nwhere w(τ -t) is a window function centered at t, f is the frequency, and X(t, f ) represents the time-frequency representation of the signal. Frequency-domain features improve the training of our model. We apply STFT to each channel of the IMU data with a window length of 30, a step size of 1, and 32 frequency components. Considering the model complexity, we concatenate six spectrograms (sharing the same time axis) to form a single feature representation input. The concatenation is along the frequency axis, resulting in a final frequency-domain feature dimension of 12 × 17 = 204, with a length of IMU signal length.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Data Segmentation",
      "text": "We collect IMU signal data with 12 channels and extract frequency-domain features for each channel.\n\nBoth the time-domain data and the extracted frequency-domain features are used as inputs to the model. To enable the model to utilize rich temporal relationships when predicting facial landmarks, we input ten continuous frames of data, each with a size of 1 × 216.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Model Architecture And Training Strategy",
      "text": "In this study, we designed a deep-learning model called IMUTwinTrans. This model, based on a ConvTransformer architecture, is capable of simultaneously leveraging the temporal and frequency features of IMU signals to establish the correspondence between IMU signals and facial landmarks, as illustrated in Fig.  5 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Cnn-Based Feature Extraction",
      "text": "We utilize a CNN to extract robust features from the input data frames. The architecture consists of sequentially stacked 1D convolutional layers with residual connections to enhance feature representation. The network begins with an initial convolutional block followed by three residual blocks that progressively increase the number of channels from 64 to 512 while maintaining a consistent kernel size of 3 and a stride of 1. This configuration allows the network to capture both fine-grained and hierarchical information. To mitigate overfitting, we incorporate dropout layers with a probability of 0.15 after each convolutional layer.\n\nAdditionally, each residual block includes shortcut connections to preserve gradient flow during training, improving model stability and convergence. Batch normalization is applied after each convolution to ensure faster training and better generalization. The input frames of size 10×216 are passed through this network, yielding an output of size 10×512, effectively capturing informative features for downstream tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Transformer Structure",
      "text": "We perform value embedding and position encoding on the feature data derived from the CNN output (size 10×512) and feed it into a two-layer Transformer Encoder. The encoder includes a self-attention module within the multi-head attention layers, a multilayer perceptron, and normalization layers.\n\nResidual connections are incorporated at both the multi-head attention and multilayer perceptron layers to boost the convergence of the machine learning model. Regarding model parameters, we set d model to 512, n head to 4, d f f to 1024, and the dropout ratio to 0.1 in the encoder.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Loss Function",
      "text": "In the task of facial landmark prediction, standard MSE and MAE loss functions treat all errors equally, regardless of their magnitude. In particular, MSE heavily penalizes large errors while neglecting small ones, resulting in important but less active landmarks (such as pupils compared to lips) not receiving sufficient attention during training. To address this issue, we adopt the Wing Loss function  [Feng et al., 2018] , which applies a relatively smooth treatment to small errors while assigning higher weights to large errors. This helps the model predict points closer to the true values more accurately without overly penalizing the model for a few large errors. The loss for each facial landmark is defined as (Eq. (  7 )):\n\nwhere |x i | is the L2 distance between the ground truth and the reconstructed coordinate for the i landmark. w represents the threshold of the small error, which is set to 20 in our case. ϵ means the curvature in the small error range, and C = w -wln(1 + w/ϵ) which links the linear part and non-linear part together.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Optimization",
      "text": "In addition, the network is trained using the Adam optimizer. The learning rate scheduler uses a Warmup + Cosine Annealing strategy. Initially, during the Warmup phase, the learning rate gradually increases to stabilize gradient updates and prevent divergence. This is followed by the Cosine Annealing phase, where the learning rate smoothly decreases following a cosine curve, allowing for more precise parameter adjustments and reducing overfitting. This strategy accelerates convergence while improving the model's generalization.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "3D Reconstruction",
      "text": "To enhance system usability and simplify the modeling process, we aim to use a compact head model that balances reduced complexity with sufficient detail to produce expressive facial animations.\n\nThe IMUFace system adopts the FLAME model (2020 version) for 3D facial representation. Specifically, the 3D reconstruction is primarily carried out using the FLAME PyTorch framework. FLAME is a statistical head model built from a dataset of approximately 33,000 3D scans. It efficiently captures complex facial geometries and expressions through a combination of identity PCA space, rotational degrees of freedom for the neck, jaw, and eyes, pose-dependent corrective blend shapes, and global expression blend shapes. The model is mathematically represented as (Eq. (  8 )):\n\nA series of optimization steps is implemented to reconstruct a 3D face model that accurately reflects the user's expressions. The process begins with camera calibration, where the parameters for scale, rotation, and translation are optimized to minimize the L2 distance between the 2D projections of the 3D model's facial landmarks and the user's facial landmarks. Following this, the FLAME model parameters-covering shape ( ⃗ β), pose ( ⃗ θ), and expression ( ⃗ ψ)-are fine-tuned to further reduce the L2 distance between the projected landmarks of the 3D model and the user's facial landmarks.\n\nUsing the optimized FLAME parameters, the user's 3D face model can then be rendered with high accuracy. While we customize the shape parameters for each user to capture their unique head geometry, we uniformly apply FLAME's default texture parameters. This decision was primarily driven by our application's focus on the dynamic representation of facial expressions and the accuracy of geometric shapes. The default texture provides a sufficient basic visual effect while avoiding the additional complexity and data acquisition costs associated with obtaining and fitting individual textures. Furthermore, a unified texture simplifies the rendering process, contributing to the system's consistency and robustness.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Performance Evaluation",
      "text": "To validate the effectiveness of our system, we conducted extensive and detailed user studies to collect training data and evaluate the performance of our system in facial reconstruction. We also performed a series of micro-benchmark tests using the trained model to further ensure the system's reliability and better understand its capabilities. These tests were designed to examine the system's performance across various scenarios.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experimental Methodology",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Data Collection",
      "text": "In the study, we recruited 12 participants to evaluate IMUFace's performance. The participants included nine males and three females from the local institution, aged 18 to 28. The experimental equipment comprises the IMUFace prototype, a Hikvision camera, and a desktop computer. Participants were asked to wear the prototype of the IMUFace wireless earphones, which were connected to the computer via Bluetooth. This study was reviewed and approved by the Ethics Review Committee (ERC) at the South China University of Technology.\n\nTo evaluate the performance of tracking 51 facial landmarks, we focused on seven universal facial expressions of emotion involving happiness, sadness, anger, surprise, fear, disgust, and contempt. Before the experiment, we showed the participants examples of the seven facial expressions and had them repeatedly practice making the corresponding facial expressions while wearing our implemented IMUFace prototype.\n\nEach expression was separated by a neutral facial expression (e.g., a relaxed facial expression). To assist participants with their data collection, seven pictures of the corresponding faces were displayed on a screen for them to imitate. The pace and extent to which each expression was performed were not controlled throughout the experiments. While the users performed the facial motion, IMU data from both earphones were collected at a sampling rate of 30 Hz. To validate the accuracy of IMUFace, the built-in camera recorded videos of the user's face on the computer at 720p resolution and 30 fps.\n\nEach participant completed 12 sessions of the experiment. In each session, the participants were required to make seven expressions, which were prompted in a random order. Each expression was made twice. Each data collection session lasted 60 seconds, and the participants took adequate rest between sessions. The earphone prototype was removed and remounted across sessions to validate the robustness to natural changes in sensor positions during daily usage.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experimental Settings",
      "text": "The training was performed on a desktop with an Intel i9-12700K CPU, 32 GB of RAM, and an Nvidia GTX 4090 GPU. We used the Adam optimizer with a learning rate of 0.001. To avoid overfitting issues that may happen in the training process, we added dropouts with a parameter of 0.1 following each ReLU activation.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Evaluation Metric",
      "text": "The IMUFace system is evaluated primarily by two metrics  [Wu et al., 2021] :\n\n, where g and r denote the facial landmarks extracted from the ground truth of vision and IMUFace in a normalized coordinate frame. d real denotes the actual outer corner distance in millimeters(we measured d real for each user who participated in the study). d norm denotes the outer corner distance in the normalized coordinate frame. From Section 4.2.3, we know that d norm = 1. This provided the MAE in units of millimeters, which is the primary metric used to evaluate IMUFace.\n\n2) Normalized Mean Error (NME): the average error between the predicted facial landmarks and the ground truth facial landmarks, represented by\n\n, where g and r denote the facial landmarks extracted from the ground truth of vision and IMUFace in a normalized coordinate frame. d norm denotes the outer corner distance in the normalized coordinate frame. From Section 4.2.3, we know that d norm = 1.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "User-Dependent Evaluation",
      "text": "For each user's 12 recorded sessions, we randomly selected five sessions for training, while the remaining sessions were used for testing in a randomized cross-validation manner. Fig.  6 (a) presents the average Mean Absolute Error (MAE) and Normalized Mean Error (NME), along with their corresponding standard deviations, for all 51 facial landmarks of each participant. Specifically, the blue bars represent the average MAE (in millimeters), which reflects the absolute spatial deviation between the predicted and ground truth landmarks. The red bars represent the average NME (in percentage), which measures the relative deviation normalized by the facial size, enabling cross-user performance comparison regardless of individual facial dimensions. These two complementary metrics provide a comprehensive evaluation of both absolute and relative reconstruction accuracy. IMUFace achieves an average MAE of 2.21 mm and an NME of 3.40%, with standard deviations of 0.99 mm and 0.90%, respectively. Among all participants, U4 achieves the best performance with an MAE of only 1.46 mm, while U8 exhibits the highest MAE at 2.72 mm. Fig.  6(b ) illustrates the Cumulative Density Function (CDF) of MAE for each individual participant as well as across all participants. The CDF curve reveals that 60.0% of the samples have an MAE below 2.21 mm, indicating a high overall prediction accuracy. This distribution further confirms the robustness of our model in reconstructing facial landmarks with low error rates across different users. In addition, different landmarks tend to exhibit varying error levels due to their distinct movement patterns during facial expressions. Fig.  6 (c) visualizes the average MAE for each of the 51 facial landmarks. We observe that landmarks located on the eyebrows and mouth tend to have higher reconstruction errors, while those around the eyes generally exhibit lower errors. This is mainly because the eyebrows and mouth undergo more dynamic deformations during expression changes, making their accurate reconstruction more challenging.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "User-Adaptive Evaluation",
      "text": "Due to the need for improvement in cross-user testing results, we attempted to introduce domain adaptation techniques. First, we trained a model using data from other users, then fine-tuned the model's linear layers with 2-minute data from a specific user, and finally tested the model using the    [Zhang et al., 2015]  2.57 mm 3.97% TCN  [Verma et al., 2021]  2.39 mm 3.68% IF-ConvTransformer  [Zhang et al., 2022b]  2.27 mm 3.50% Mamba2  [Dao and Gu, 2024]  2.37 mm 3.64% IMUTwinTrans 2.21 mm 3.40%\n\ndata from the same user. Fig.  7  compares the performance differences between the user-dependent and domain-adapted models. It can be observed that the performance of the domain-adapted model is significantly improved compared to the user-dependent model, with an average MAE of 2.05 mm for the specific user. This improvement may be attributed to the unique characteristics of each user's facial muscle movements. Through fine-tuning, the model can learn specific patterns from the target user's data, thereby enhancing its prediction performance.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Micro-Benchmark Tests",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Model Selection",
      "text": "To verify the effectiveness of our model, we compared it with several deep learning network methods commonly used for IMU data analysis to demonstrate the superiority of our model. We evaluated the performance of different networks under user-dependent experimental settings, and the results are presented in Table  1 .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Number Of Earphones",
      "text": "To validate the system's efficacy under single-earphone conditions, we investigated the accuracy when using only a single earphone. As shown in Fig.  8 , although using both earphones can provide the best accuracy by integrating sensor data from both ears, the accuracy with individual earphones (left or right) is also very close (2.40 mm, 2.36 mm).",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Impact Of Training Data Size",
      "text": "In practical applications, the amount of data that needs to be collected from users for training can affect user experience. Therefore, it is necessary to explore further the relationship between system performance and the required training data to minimize the burden of data collection on users. Fig.  9  shows the overall system performance when the training data size varies from 1 minute to 5 minutes for each participant. We observe that with 1 minute of training data, the accuracy already reaches 2.66 mm, and with 5 minutes of training data, the accuracy saturates at around 2.21 mm. It is shown  that IMUFace can still achieve a reasonable level of accuracy even with a relatively short period of user data collection, which offers a high level of user-friendliness.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Latency And Power Consumption",
      "text": "The inference time for the 51 facial landmarks was measured on a single NVIDIA GTX 4090 GPU.\n\nOur designed model requires only about 1.26 ms to reconstruct a single frame, which is sufficient for real-time applications.\n\nIn addition, we used a power monitoring device to measure the power consumption of IMUFace. Specifically, when the system is idle, that is, the microcontroller unit (MCU) is in sleep mode and does not transmit data via Bluetooth, the average power consumption of IMUFace is 50 mW (13.5 mA @ 3.70 V). When IMUFace actively collects IMU data and transmits it via Bluetooth, the total system power consumption is 58 mW (15.7 mA @ 3.70 V). This indicates that with a 100 mAh lithium polymer battery, IMUFace can continuously record data for up to 6.5 hours, meeting the needs of most applications. We examined the power consumption of recent studies on 3D facial reconstruction and summarized the overall power consumption of the system, including the use of sensing and communication power, in Table  2 . Our analysis shows that IMUFace consumes the least power among all sensing technologies evaluated, with energy consumption 58.0% lower than that of the next most efficient technology, Bioface-3D  [Wu et al., 2021] .   [Chen et al., 2021]  Cameras 4 W C-Face  [Chen et al., 2020]  Cameras >4 W BioFace-3D  [Wu et al., 2021]  Biosensors 138 mW EarIO  [Li et al., 2022]  Acoustics 153.7 mW EyeEcho  [Li et al., 2024a]  Acoustics 167 mW IMUFace IMUs 58 mW",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Discussion",
      "text": "This study explores the use of conventional motion sensors, such as accelerometers and gyroscopes, for capturing facial expression data, emphasizing their practicality and portability for a wide range of applications. A key challenge identified is the variability in facial movements across different users, which impacts the model's performance. To address this, incorporating a small amount of user-specific data has been shown to improve accuracy. This approach offers promising applications in fields like healthcare, gaming, and human-computer interaction by advancing real-time facial expression tracking and enabling innovative functionalities.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "User-Independent Model",
      "text": "Experimental results reveal that fine-tuning the model with user-specific data significantly enhances performance compared to a purely user-independent model. This improvement arises from the inherent variability in facial movement characteristics, such as intensity, responsiveness, and sensitivity, among individuals. These differences create unique data domains, posing challenges for generalized models. Although fine-tuning improves accuracy, it introduces a trade-off in usability, as new users must register and provide data for personalization.\n\nTo address this limitation, future work will focus on generating extensive virtual IMU datasets using multimodal inputs, such as video or depth sensors. By leveraging synthetic data, we aim to train a robust and generalized user-independent model capable of adapting to a broader user base with minimal personalization. Additionally, investigating techniques like domain adaptation and transfer learning could help bridge domain gaps, enhancing the model's generalization capabilities. These strategies aim to maintain high performance while improving the system's usability for diverse users.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Effects Of Body Movements",
      "text": "While the system accounts for variations in initial head pose, dynamic body movements-such as repetitive head bounces during activities like walking or running-introduce significant noise into IMU signals  [Prakash et al., 2019] . This noise complicates the accurate detection of facial expressions, reducing the system's reliability in real-world scenarios. To mitigate this issue, future research will explore advanced filtering techniques designed to isolate and remove noise caused by body movements while preserving subtle facial expression signals. Methods such as adaptive filtering, wavelet transforms, or machine-learning-based noise suppression may improve signal quality. Furthermore, integrating complementary sensors, such as magnetometers or pressure sensors, could provide additional context to distinguish between facial and body-induced movements. These enhancements aim to ensure reliable system performance in dynamic and mobile environments.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Practical Applications And Industry Implications",
      "text": "The proposed methodology demonstrates significant potential across various sectors, highlighting its practical value. In healthcare, it enables remote monitoring of patients with neurological disorders, facilitating the assessment of facial muscle activity and therapy progress. In gaming and virtual reality, it offers camera-free, real-time facial expression tracking, enhancing user privacy and reducing hardware complexity. For human-computer interaction, it supports more intuitive communication between users and devices. In the automotive industry, the system can monitor driver fatigue or emotional states, improving safety and user experience. These diverse applications underline the method's versatility and its potential to address practical challenges across multiple disciplines.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Conclusion",
      "text": "This article proposes and designs IMUFace, a system that reconstructs the 3D facial expressions of users using IMUs that are attached to (and potentially built into) wireless earphones. The system can detect the motion changes associated with facial expressions and achieve user-friendly, loweffort, continuous 3D facial reconstruction. We extracted frequency-domain features from the data and designed a lightweight deep learning model, IMUTwinTrans, based on a ConvTransformer architecture to achieve 3D facial reconstruction by leveraging both temporal and frequency features of the IMU signal. It achieves an average facial landmark tracking accuracy of 2.21 mm across 12 users. The deep learning model inference time is only 1.26 ms, satisfying real-time requirements.\n\nOur device consumes only 58 mW of power during operation and can run for 6.25 hours on a 3.7 V, 100 mAh battery. Numerous applications in the areas of affective computing, emotional well-being monitoring, facial expression recognition, AR/VR, accessibility, and user interfaces can be further explored in future work.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Demonstration of continuous 3D facial reconstruction of IMUFace.",
      "page": 2
    },
    {
      "caption": "Figure 1: showcases 3D facial reconstruction by the IMUFace system,",
      "page": 3
    },
    {
      "caption": "Figure 2: Overview of the IMUFace system.",
      "page": 5
    },
    {
      "caption": "Figure 2: , we present an overview of the architecture of our proposed IMUFace system, which is",
      "page": 5
    },
    {
      "caption": "Figure 3: Device Prototype: Composed of a pair of earbuds and XLAO nRF52840 Sense MCU.",
      "page": 6
    },
    {
      "caption": "Figure 3: First, we attach the battery to the back of the XIAO nRF52840 Sense, then mount it",
      "page": 6
    },
    {
      "caption": "Figure 4: Illustration of facial landmark alignment. (a) Before landmark alignment. (b) After",
      "page": 7
    },
    {
      "caption": "Figure 4: shows the facial landmark coordinates before and after alignment.",
      "page": 7
    },
    {
      "caption": "Figure 5: Illustration of the model structure.",
      "page": 8
    },
    {
      "caption": "Figure 6: Performance of continuous facial landmark tracking for each participant. (a) Per-participant",
      "page": 11
    },
    {
      "caption": "Figure 6: (a) presents",
      "page": 11
    },
    {
      "caption": "Figure 6: (b) illustrates",
      "page": 11
    },
    {
      "caption": "Figure 6: (c) visualizes the average MAE for each of the 51 facial landmarks. We",
      "page": 11
    },
    {
      "caption": "Figure 7: Domain adaptation.",
      "page": 12
    },
    {
      "caption": "Figure 7: compares the performance differences between the user-dependent",
      "page": 12
    },
    {
      "caption": "Figure 8: , although using both earphones can provide",
      "page": 12
    },
    {
      "caption": "Figure 9: shows the overall system performance when the training data size varies from 1 minute to 5 minutes",
      "page": 12
    },
    {
      "caption": "Figure 8: Number of earbuds.",
      "page": 13
    },
    {
      "caption": "Figure 9: Training size vs error.",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison result of different Models.",
      "page": 12
    },
    {
      "caption": "Table 2: Our analysis shows that IMUFace consumes the least",
      "page": 13
    },
    {
      "caption": "Table 2: Comparison of System Power Consumption among Related Existing Studies",
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Facerecglasses: a wearable system for recognizing self facial expressions using compact wearable cameras",
      "authors": [
        "H Aoki",
        "A Ohnishi",
        "N Isoyama",
        "T Terada",
        "M Tsukamoto"
      ],
      "year": "2021",
      "venue": "Proceedings of the Augmented Humans International Conference"
    },
    {
      "citation_id": "2",
      "title": "Facial emotion recognition in parkinson's disease: a review and new hypotheses",
      "authors": [
        "S Argaud",
        "M Vérin",
        "P Sauleau",
        "D Grandjean"
      ],
      "year": "2018",
      "venue": "Movement Disorders"
    },
    {
      "citation_id": "3",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "13th IEEE International Conference on Automatic Face & Gesture Recognition",
      "doi": "10.1109/FG.2018.00019"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition: the role of facial movement and the relative importance of upper and lower areas of the face",
      "authors": [
        "J Bassili"
      ],
      "year": "1979",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "5",
      "title": "Inmyface: Inertial and mechanomyography-based sensor fusion for wearable facial activity recognition",
      "authors": [
        "H Bello",
        "L Marin",
        "S Suh",
        "B Zhou",
        "P Lukowicz"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "6",
      "title": "A morphable model for the synthesis of 3d faces",
      "authors": [
        "V Blanz",
        "T Vetter"
      ],
      "year": "2023",
      "venue": "Seminal Graphics Papers: Pushing the Boundaries"
    },
    {
      "citation_id": "7",
      "title": "Real-time recognition of facial expressions using facial electromyograms recorded around the eyes for social virtual reality applications",
      "authors": [
        "H.-S Cha",
        "S.-J Choi",
        "C.-H Im"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "8",
      "title": "C-face: Continuously reconstructing facial expressions by deep learning contours of the face with ear-mounted miniature cameras",
      "authors": [
        "T Chen",
        "B Steeper",
        "K Alsheikh",
        "S Tao",
        "F Guimbretière",
        "C Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology"
    },
    {
      "citation_id": "9",
      "title": "Neckface: Continuously tracking full facial expressions on neck-mounted wearables",
      "authors": [
        "T Chen",
        "Y Li",
        "S Tao",
        "H Lim",
        "M Sakashita",
        "R Zhang",
        "F Guimbretiere",
        "C Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "10",
      "title": "Transformer-based 3d face reconstruction with end-to-end shape-preserved domain transfer",
      "authors": [
        "Z Chen",
        "Y Wang",
        "T Guan",
        "L Xu",
        "W Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "11",
      "title": "Ppgface: Like what you are watching? earphones can\" feel\" your facial expressions",
      "authors": [
        "S Choi",
        "Y Gao",
        "Y Jin",
        "S Kim",
        "J Li",
        "W Xu",
        "Z Jin"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "12",
      "title": "Active shape models-their training and application",
      "authors": [
        "T Cootes",
        "C Taylor",
        "D Cooper",
        "J Graham"
      ],
      "year": "1995",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "13",
      "title": "Active appearance models",
      "authors": [
        "T Cootes",
        "G Edwards",
        "C Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Transformers are ssms: Generalized models and efficient algorithms through structured state space duality",
      "authors": [
        "T Dao",
        "A Gu"
      ],
      "year": "2024",
      "venue": "Transformers are ssms: Generalized models and efficient algorithms through structured state space duality",
      "arxiv": "arXiv:2405.21060"
    },
    {
      "citation_id": "15",
      "title": "Silent speech interfaces",
      "authors": [
        "B Denby",
        "T Schultz",
        "K Honda",
        "T Hueber",
        "J Gilbert",
        "J Brumberg"
      ],
      "year": "2010",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "16",
      "title": "Wing loss for robust facial landmark localisation with convolutional neural networks",
      "authors": [
        "Z.-H Feng",
        "J Kittler",
        "M Awais",
        "P Huber",
        "X.-J Wu"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "Role of facial expressions in social interactions",
      "authors": [
        "C Frith"
      ],
      "year": "1535",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "18",
      "title": "Ganfit: Generative adversarial network fitting for high fidelity 3d face reconstruction",
      "authors": [
        "B Gecer",
        "S Ploumpis",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Fast-ganfit: Generative adversarial network for high fidelity 3d face reconstruction",
      "authors": [
        "B Gecer",
        "S Ploumpis",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Fast-ganfit: Generative adversarial network for high fidelity 3d face reconstruction",
      "arxiv": "arXiv:2105.07474"
    },
    {
      "citation_id": "20",
      "title": "View consistent 3d face reconstruction using siamese encoder-decoders",
      "authors": [
        "P Ji",
        "M Zeng",
        "X Liu"
      ],
      "year": "2020",
      "venue": "15th Chinese Conference on Image and Graphics Technologies and Applications"
    },
    {
      "citation_id": "21",
      "title": "The role of facial movements in emotion recognition",
      "authors": [
        "E Krumhuber",
        "L Skora",
        "H Hill",
        "K Lander"
      ],
      "year": "2023",
      "venue": "Nature Reviews Psychology"
    },
    {
      "citation_id": "22",
      "title": "Facial expression recognition: A survey",
      "authors": [
        "J Kumari",
        "R Rajesh",
        "K Pooja"
      ],
      "year": "2015",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "23",
      "title": "Eario: A low-power acoustic sensing earable for continuously tracking detailed facial movements",
      "authors": [
        "K Li",
        "R Zhang",
        "B Liang",
        "F Guimbretière",
        "C Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "24",
      "title": "Eyeecho: Continuous and low-power facial expression tracking on glasses",
      "authors": [
        "K Li",
        "R Zhang",
        "S Chen",
        "B Chen",
        "M Sakashita",
        "F Guimbretière",
        "C Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of the CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "25",
      "title": "Learning a model of facial shape and expression from 4d scans",
      "authors": [
        "T Li",
        "T Bolkart",
        "M Black",
        "H Li",
        "J Romero"
      ],
      "year": "2017",
      "venue": "ACM Transactions on Graphics"
    },
    {
      "citation_id": "26",
      "title": "A novel two-pathway encoder-decoder network for 3d face reconstruction",
      "authors": [
        "X Li",
        "Z Weng",
        "J Liang",
        "L Cei",
        "Y Xiang",
        "Y Fu"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "27",
      "title": "Auglasses: Continuous action unit based facial reconstruction with low-power imus on smart glasses",
      "authors": [
        "Y Li",
        "T Zhang",
        "X Zeng",
        "Y Wang",
        "H Zhang",
        "Y Chen"
      ],
      "year": "2024",
      "venue": "Auglasses: Continuous action unit based facial reconstruction with low-power imus on smart glasses",
      "arxiv": "arXiv:2405.13289"
    },
    {
      "citation_id": "28",
      "title": "Stear: Robust step counting from earables",
      "authors": [
        "J Prakash",
        "Z Yang",
        "Y.-L Wei",
        "R Choudhury"
      ],
      "year": "2019",
      "venue": "Proceedings of the 1st International Workshop on Earable Computing"
    },
    {
      "citation_id": "29",
      "title": "Learning detailed face reconstruction from a single image",
      "authors": [
        "E Richardson",
        "M Sela",
        "R Or-El",
        "R Kimmel"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "The expression of emotion through nonverbal behavior in medical visits: mechanisms and outcomes",
      "authors": [
        "D Roter",
        "R Frankel",
        "J Hall",
        "D Sluyter"
      ],
      "year": "2006",
      "venue": "Journal of General Internal Medicine"
    },
    {
      "citation_id": "31",
      "title": "Unconstrained 3d face reconstruction",
      "authors": [
        "J Roth",
        "Y Tong",
        "X Liu"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "32",
      "title": "Adaptive 3d face reconstruction from unconstrained photo collections",
      "authors": [
        "J Roth",
        "Y Tong",
        "X Liu"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "33",
      "title": "300 faces in-the-wild challenge: The first facial landmark localization challenge",
      "authors": [
        "C Sagonas",
        "G Tzimiropoulos",
        "S Zafeiriou",
        "M Pantic"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Computer Vision Workshops",
      "doi": "10.1109/ICCVW.2013.59"
    },
    {
      "citation_id": "34",
      "title": "Facelistener: Recognizing human facial expressions via acoustic sensing on commodity headphones",
      "authors": [
        "X Song",
        "K Huang",
        "W Gao"
      ],
      "year": "2022",
      "venue": "21st ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)"
    },
    {
      "citation_id": "35",
      "title": "Landmark-based 3d face reconstruction from an arbitrary number of unconstrained images",
      "authors": [
        "W Tian",
        "F Liu",
        "Q Zhao"
      ],
      "year": "2018",
      "venue": "13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "36",
      "title": "Expressear: Sensing fine-grained facial expressions with earables",
      "authors": [
        "D Verma",
        "S Bhalla",
        "D Sahnan",
        "J Shukla",
        "A Parnami"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "37",
      "title": "Deep high-resolution representation learning for visual recognition",
      "authors": [
        "J Wang",
        "K Sun",
        "T Cheng",
        "B Jiang",
        "C Deng",
        "Y Zhao",
        "D Liu",
        "Y Mu",
        "M Tan",
        "X Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Bioface-3d: Continuous 3d facial reconstruction through lightweight single-ear biosensors",
      "authors": [
        "Y Wu",
        "V Kakaraparthi",
        "Z Li",
        "T Pham",
        "J Liu",
        "P Nguyen"
      ],
      "year": "2021",
      "venue": "Proceedings of the 27th Annual International Conference on Mobile Computing and Networking"
    },
    {
      "citation_id": "39",
      "title": "Scifi: 3d face reconstruction via smartphone screen lighting",
      "authors": [
        "W Xie",
        "Z Kuang",
        "M Wang"
      ],
      "year": "2021",
      "venue": "Optics Express"
    },
    {
      "citation_id": "40",
      "title": "Acoustic-based upper facial action recognition for smart eyewear",
      "authors": [
        "W Xie",
        "Q Zhang",
        "J Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "41",
      "title": "Research on 3d face reconstruction algorithm based on resnet and transformer",
      "authors": [
        "Y Yaermaimaiti",
        "T Yan",
        "Y Zhao",
        "T Kari"
      ],
      "year": "2024",
      "venue": "International Journal of Computational Intelligence and Applications"
    },
    {
      "citation_id": "42",
      "title": "Bidirectional long short-term memory networks for relation classification",
      "authors": [
        "S Zhang",
        "D Zheng",
        "X Hu",
        "M Yang"
      ],
      "year": "2015",
      "venue": "Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation"
    },
    {
      "citation_id": "43",
      "title": "I am an earphone and i can hear my user's face: Facial landmark tracking using smart earphones",
      "authors": [
        "S Zhang",
        "T Lu",
        "H Zhou",
        "Y Liu",
        "R Liu",
        "M Gowda"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Internet of Things"
    },
    {
      "citation_id": "44",
      "title": "Force-aware interface via electromyography for natural vr/ar interaction",
      "authors": [
        "Y Zhang",
        "B Liang",
        "B Chen",
        "P Torrens",
        "S Atashzar",
        "D Lin",
        "Q Sun"
      ],
      "year": "2022",
      "venue": "ACM Transactions on Graphics"
    },
    {
      "citation_id": "45",
      "title": "If-convtransformer: A framework for human activity recognition using imu fusion and convtransformer",
      "authors": [
        "Y Zhang",
        "L Wang",
        "H Chen",
        "A Tian",
        "S Zhou",
        "Y Guo"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    }
  ]
}