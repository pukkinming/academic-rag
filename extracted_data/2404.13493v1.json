{
  "paper_id": "2404.13493v1",
  "title": "Authentic Emotion Mapping: Benchmarking Facial Expressions In Real News",
  "published": "2024-04-21T00:14:03Z",
  "authors": [
    "Qixuan Zhang",
    "Zhifeng Wang",
    "Yang Liu",
    "Zhenyue Qin",
    "Kaihao Zhang",
    "Sabrina Caldwell",
    "Tom Gedeon"
  ],
  "keywords": [
    "emotion recognition",
    "graph neural network",
    "human-centered computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we present a novel benchmark for Emotion Recognition using facial landmarks extracted from realistic news videos. Traditional methods relying on RGB images are resource-intensive, whereas our approach with Facial Landmark Emotion Recognition (FLER) offers a simplified yet effective alternative. By leveraging Graph Neural Networks (GNNs) to analyze the geometric and spatial relationships of facial landmarks, our method enhances the understanding and accuracy of emotion recognition. We discuss the advancements and challenges in deep learning techniques for emotion recognition, particularly focusing on Graph Neural Networks (GNNs) and Transformers. Our experimental results demonstrate the viability and potential of our dataset as a benchmark, setting a new direction for future research in emotion recognition technologies. The codes and models are at: https://github.com/wangzhifengharrison/benchmark real news",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion recognition has become a part of interactive technology, finding its place in numerous applications ranging from content delivery  [1]  to medical diagnostics  [2]  and immersive virtual experiences  [3] . Traditional approaches to emotion recognition have heavily relied on RGB images, demanding significant processing resources to analyze complex facial features in such high-resolution data  [4] ,  [5] .\n\nFacial Landmark Emotion Recognition (FLER) presents an alternative that simplifies this process through the use of basic geometric constructs on the face  [6] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Introduction To Facial Landmark Emotion Recognition",
      "text": "Emotion recognition through facial expressions is a significant aspect of artificial intelligence, aiming to interpret the emotional states by human facial gestures. The use of facial landmarks, or distinct facial points, is essential in this domain, enabling the detection of subtle emotional variations. The subtle shifts in these landmarks are closely linked to different emotional states and their precise recognition is critical for enhancing the performance of emotion recognition systems. Compared to traditional image-based emotion recognition, facial landmark-based emotion recognition can protect privacy since it does not involve processing entire facial images  [6] . Moreover, in resource-constrained scenarios like edge computing, landmark-based methods are favored for their efficiency and low resource consumption. Consequently, Email:\n\n{qixuan.zhang, zhifeng.wang, kaihao.zhang, sabrina.caldwell}@anu.edu.au {lyf1082, kf.zy.qin}@gmail.com, tom.gedeon@curtin.edu.au\n\nFacial Landmark Emotion Recognition (FLER) has emerged as a pivotal technology across various applications, prominently in smart devices and real-time interactive systems  [7] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Graph Neural Networks In Emotion Recognition",
      "text": "The Graph Neural Networks (GNNs)  [8] ,  [9]  has provided novel methods to handle complex structured graph data. In the domain of facial landmark emotion recognition, the GNNs are especially valuable because they can directly work with graphs composed of facial landmarks, where the edges define the structure of facial expressions. GNNs are adept at capturing the intricate interactions among facial landmarks, which are essential for understanding the facial dynamics associated with different emotional states. Although the application of GNNs in facial emotion recognition is still in its nascent stages, their theoretical advantages in handling graph-structured data suggest immense potential. For example,  [4]  introduces a directed graph neural network (DGNN) using graph convolution and landmark features for Facial Emotion Recognition (FER), efficiently capturing geometric and temporal information from facial landmarks and addressing the vanishing gradient issue through a stable temporal block. Rao et al.  [10]  presents a novel multiscale graph convolutional network based on facial landmark graphs for Facial Expression Recognition (FER), addressing issues of data redundancy and bias. However, the deployment of GNNs in this field also presents challenges such as how to design effective graph structures to accurately reflect the dynamics of facial expressions, and balancing the complexity of the models with the performance constraints of edge computing devices  [11] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "C. Gaps In Current Research And Our Contribution",
      "text": "Despite numerous studies in the field of facial landmark emotion recognition, there is still a lack of a systematic benchmarking framework for evaluating and comparing different neural network models. Such a benchmark is vital for understanding how various models perform in real-world applications. Most existing studies have focused on specific models or algorithms without providing a comprehensive perspective on the performance of different methods when dealing with facial landmark data. Furthermore, the absence of benchmarking makes it difficult for researchers and developers to determine which models are best suited for deployment on resource-limited edge devices. To address these issues, this paper presents a benchmarking test, including a range of neural network models and their performance evaluations across arXiv:2404.13493v1 [cs.CV] 21 Apr 2024 different emotion classification tasks. We consider not only the accuracy of the models but also their computational efficiency, thus providing significant references for applications on edge computing devices. The motivation behind this study is to advance model performance and facilitate practical applications on edge devices. We believe that through these benchmarks, the research community can gain a deeper understanding of the strengths and limitations of different algorithms, leading to the design of solutions that are more in tune with practical requirements.\n\nThe contribution of this paper lies in establishing a comprehensive and systematic benchmarking for facial landmark emotion recognition. We ensure the reliability and consistency of our test results through strict experimental design, providing a solid benchmark for assessing the performance of various neural network models in the task of facial emotion recognition. Furthermore, we delve into how the number and configuration of landmarks impact model performance, revealing the significance of landmark selection in optimizing model efficiency. Additionally, we have created our dataset and benchmarking code open source for the community to facilitate further research and development. These contributions not only provide the academic community with research resources but also lay a solid foundation for the industry to implement efficient algorithms, thereby promoting the development of technology geared towards practical applications.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we explore the landscape of emotion recognition, starting from its fundamental tasks and the role of facial landmarks, to the latest advancements in deep learning, particularly focusing on Convolutional Neural Networks (CNNs) and Transformers. Each subsection delves into significant developments, challenges, and potential future directions, laying out a comprehensive overview of the field's progression and the cutting-edge techniques shaping the future of facial landmark-based emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Introduction To Emotion Recognition And Facial Landmarks",
      "text": "The journey toward automating emotion recognition has covered various fields, from early psychophysiological studies to the current artificial intelligence techniques. This section will introduce significant past works, specifically focusing on the use of facial landmarks as indicators for emotion recognition. Traditional emotion recognition systems mainly utilized methods based on geometric features, employing distances and angles between facial landmarks as direct inputs to classifiers such as Support Vector Machines (SVMs)  [12]  and simple neural networks  [13] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Advancements In Deep Learning For Emotion Recognition",
      "text": "With the revival of neural networks, especially Convolutional Neural Networks (CNNs), the focus shifted to deeper neural networks for emotion recognition. Yang et al.  [14]  introduces a 3D facial expression recognition algorithm using CNNs and landmark features, robust to pose and lighting changes due to its reliance on 3D geometric models. Akhand et al.  [15]  introduces a highly accurate facial emotion recognition system employing a deep CNN model optimized via transfer learning and a novel pipeline strategy, addressing limitations of shallow networks and frontal-only images by fine-tuning with emotion data for enhanced feature extraction. However, these techniques often overlooked facial expressions' temporal dynamics. Addressing this, Kollias et al.  [16]  introduces a CNN-RNN approach for dimensional emotion recognition using multiple features and multi-task frameworks on large emotion datasets, significantly outperforming existing methods, but these still fell short in capturing the long-range dependencies crucial for understanding emotional expressions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Emotion Recognition With Transformers",
      "text": "Enter the era of Transformers, which have redefined the possibilities in handling sequential data thanks to their selfattention mechanisms. Vaswani et al.  [17]  revolutionized sequence modeling with the introduction of the Transformer model, which has since been adapted beyond the boundaries of language processing. In the realm of facial emotion recognition, recent studies have begun to explore the application of Transformers. For instance, Zhao et al.  [18]  introduces a geometry-guided FER framework using graph convolutional networks and transformers, enhancing emotion recognition from videos by constructing spatial-temporal graphs with facial landmarks and incorporating attention mechanisms for more informative feature emphasis. Zheng et al.  [19]  introduces POSTER, a two-stream Pyramid cross-fusion Transformer network, aiming to address the key FER challenges: inter-class similarity, intra-class discrepancy, and scale sensitivity by fusing facial landmark and image features and employing a pyramid structure for scale invariance. Hybrid models have also emerged, combining the strengths of CNNs and Transformers. Karatay et al.  [20]  presents a deep neural network framework combining Gaussian mixture models with CNN and Transformer for emotion detection from videos and images using facial and body features extracted by OpenPose, addressing various basic emotions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Addressing Diversity And Data Augmentation In Emotion Recognition",
      "text": "Moreover, the impact of data diversity and representativeness has also been a focal point in the literature. Studies like that of Li et al.  [21]  highlighted the challenges posed by variations in ethnicity, age, and lighting conditions, prompting the development of more robust models. The employment of Generative Adversarial Networks (GANs) for data augmentation, as explored by Hajarolasvadi et al.  [22] , has been one avenue to enhance the diversity and quantity of training data, thereby improving the generalizability of emotion recognition systems. The review of related work underscores a gradual but significant shift towards sophisticated models that consider both the spatial and temporal aspects of facial landmarks in emotion recognition. The exploration into Transformer-based models marks a new frontier in this domain, promising enhanced accuracy and deeper understanding of emotional states through the advanced modeling of sequential landmark data. This paper builds upon these foundations, aiming to further the efficacy and applicability of facial landmark-based emotion recognition systems.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Facial Expressions In Real News Dataset",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Dataset Overview",
      "text": "In Table  I , striking feature of this distribution is the significant variance in the number of samples across different emotions. For instance, Neutral, Happy, and Fear emotions are notably more represented in the dataset, with counts exceeding 3,500, 3,000, and 2,000 respectively in the Training subset. The Total column sums the Training and Validation and Test samples, offering a holistic view of the dataset's composition, with a grand total of 14,172 instances. This comprehensive distribution is critical for understanding the dataset's balance and evaluating the potential biases in the training, validating testing of emotion recognition models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Basic Emotion Categories",
      "text": "We capture 5 different daily basic emotion categories in real news. Fig.  1  illustrates the distribution of 5 different emotion categories, each represented by a unique color. The largest portion is 'Neutral' at 27.1%, followed closely by 'Sad' at 24.0%. 'Happy' represents 23.9% of the chart, while 'Fear' is at 16.4%. The smallest slice is 'Angry', making up 8.6% of the chart. Each category's percentage is provided.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Data Acquisition And Preparation",
      "text": "We initiated our dataset by downloading 318 news videos from internationally recognized media sources such as ABC, Al Jazeera, NBC News, BBC, CBC, and CCTV and involved extracting clear, well-defined facial images from these videos. We established a quality control process that included evaluating image sharpness, ensuring all facial features were visible and distinct, and preventing the selection of repetitive images. Additionally, we employed adaptive padding technology to adjust each image's frame size based on the facial dimensions, thereby enhancing the overall quality and consistency of our image dataset.\n\nTo efficiently process and extract faces, we developed scripts to organize data into a structured directory, calculate adaptive padding, and use the Laplacian method for sharpness and mean squared error for image similarity. We also used Dlib's  [23]  frontal face detector to identify each face and extract its key features, making sure all essential landmarks were fully captured and within the image frame. This method ensured efficient and high-quality face extraction.\n\nOur comprehensive facial analysis tool is a product of these efforts, adeptly identifying faces, pinpointing facial features, and recognizing emotional expressions. We utilized libraries such as DeepFace  [24]  and Mediapipe  [25]  to detect faces, analyze facial characteristics, and determine prevalent emotional states. Each piece of analysis was precisely documented, offering a rich dataset for future machine learning applications or detailed statistical analysis. This tool marks a significant advancement in the field of facial analysis.\n\nWe manually reviewed the images and corresponding landmark data, removing any irrelevant or low-quality images. After ensuring the integrity and relevance of the data, we shuffled and divided it into training and test sets, maintaining a balanced representation of each emotion category. A 'type' column was added to distinguish between the sets, laying the groundwork for a robust and reliable facial landmark emotion recognition benchmark. This structured, detailed approach to data acquisition and processing ensures a high-quality, diverse dataset that will significantly contribute to advancements in emotion recognition and facial analysis.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Methods",
      "text": "In the field of emotion recognition via facial landmarks, leveraging Graph Neural Networks (GNNs) offers significant potential. This stems from the structured arrangement of facial features. Facial landmarks, including mouth corners, eyebrow edges, and nose tips, naturally form a graph-like network.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Innovative Convolution Techniques In Graph Neural Networks",
      "text": "The application of convolution techniques in graph neural networks, particularly through Graph Convolutional Networks (GCNs)  [26]  and Chebyshev Spectral CNNs (ChebNets)  [27] , has been pivotal in decoding complex spatial interactions among facial landmarks. GCNs contribute by learning localized features, capturing the subtleties of emotional expressions. In contrast, ChebNets employ spectral graph theory, focusing on landmark configuration frequencies for feature extraction. Collectively, these methods provide a thorough framework for analyzing facial expressions, integrating both spatial and frequency domain insights to interpret static and dynamic expression elements.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Attention And Aggregation Strategies For Enhanced Feature Analysis",
      "text": "Graph Attention Networks (GATs)  [28]  and Graph Sample and Aggregate (GraphSAGE)  [29]  networks adopt distinct approaches to emphasize the importance of individual facial landmarks. GATs utilize attention mechanisms to dynamically highlight key features, thereby improving the model's ability to focus on facial regions most indicative of emotions. Conversely, GraphSAGE introduces a versatile aggregation framework. This framework is adaptable to various facial structures and expressions, ensuring stable performance across a wide range of facial data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Dynamic And Edge-Enhanced Learning For Expressive Feature Interpretation",
      "text": "Dynamic Graph CNNs (DGCNNs)  [30]  and Edge-Conditioned Convolution (ECC)  [31]  networks present innovative approaches for analyzing the evolving characteristics of facial expressions. DGCNNs are specifically designed for time-series data, effectively tracking the development of emotional indicators over time, a critical aspect in videobased emotion recognition. ECC networks complement this approach by factoring in the attributes of edges connecting facial landmarks. This addition enhances the model's insight into the impact of landmark interrelations on the expression of emotions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Spatial-Temporal And Topological Insights With Advanced Gnn Architectures",
      "text": "Spatial Graph Convolutional Networks  [32]  and Graph Isomorphism Networks (GIN)  [33]  offer novel perspectives in understanding facial landmark structures, utilizing both spatial-temporal and topological data. Spatial networks are adept at directly learning from the distribution of landmarks, effectively identifying the spatial patterns associated with various emotions. Conversely, GIN focuses on the intrinsic topological structures of these patterns. This approach enables the model to distinguish between closely related yet distinct emotional states by analyzing the configuration and characteristics of neighboring landmarks. V. EXPERIMENTS",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Experiment Settings",
      "text": "We executed experiments on our proposed dataset to evaluate 5 classes of expressions, utilizing an Ubuntu 20.04 workstation equipped with an NVIDIA GeForce GTX 3090Ti GPU. Network implementation was carried out using the Pytorch framework.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Accuracy",
      "text": "Accuracy is calculated by the formula:\n\nwhere P denotes the number of correct predictions, and N is the total observations for each emotion category.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Experimental Results",
      "text": "Multilayer Perceptron (MLP)  [34]  is a fundamental type of neural network architecture that has been extensively applied in the field of machine learning. As a form of a feedforward artificial neural network, MLP consists of an input layer, multiple hidden layers, and an output layer. In our experiments, we use one hidden layer as baseline for comparison with other methods. Each node, except for the input nodes, is a neuron with a RELU activation function. MLP utilizes backpropagation for training the network. In our proposed dataset, MLP achieved average score of 28.00% on five basic emotion categories shown in Table  II . Compared with GIN  [33] , MLP can't capture the intrinsic topological structures of facial landmarks. GIN enables the model to distinguish between closely related yet distinct emotional states by analyzing the configuration and characteristics of neighboring landmarks. So, GIN achieved higher accuracy with average score of 30.20%. SAGE  [29]  is a novel inductive framework for facial landmarks embedding that leverages facial landmark features to learn a function capable of generalizing to unseen facial landmarks, distinguishing it from matrix factorization-based methods and allowing simultaneous learning of neighborhood topology and facial landmarks distribution, applicable to both feature-rich and standard structural graphs. Compared with GIN, SAGE can learn the distribution of facial landmarks and give prediction of unseen facial landmarks which are important for unseen emotion categories in the inference process. So, SAGE achieve higher average accuracy with score of 32.80%. In order to achieve linear complexity, GINFormer  [35]  proposes a Graph Transformer, adept at handling diverse benchmarks. This paper provides a foundational understanding of positional and structural encodings, categorizing them into local, global, and relative types. They introduce a novel The figure in 3 provides an illustrative comparative study of facial expression analysis using both advanced deep learning algorithms and classical psychological techniques. The first row displays a series of original images portraying a range of emotions-anger, fear, neutrality, sadness, and happiness-extracted from various news broadcasts. The second row showcases the application of facial landmark detection by utilizing a system like MediaPipe  [25] , which maps out key facial points such as the corners of the mouth or eyes that are essential for analyzing expressions. The third row introduces a layer of complexity with the inclusion of a GINFormer's  [35]  attention mechanism, a state-of-the-art deep learning model that employs transformer architecture to decode the web of facial landmarks and their connections, pinpointing the areas most relevant for identifying each emotion. The fourth row adopts a psychological lens, employing the Facial Action Coding System (FACS) to focus on the activation of Action Units (AUs)-specific facial muscles associated with emotion display. The patterns overlaying the faces in this row indicate the intensity and activity of AUs, offering a more granular perspective on how emotions manifest physically.\n\nThe differences of the third and fourth rows illuminates the methodological divergence between computational and psychological analysis: the GINFormer model abstracts emotion recognition into patterns within the facial landmark network, while psychological analysis, through FACS, delves into the intricate details of muscle movements linked with emotional states. This comparative visualization underscores the distinctive, yet potentially complementary, natures of these",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this study, we propose a novel benchmark using realistic news videos, complemented with RGB images and facial landmark coordinates, to enhance emotion recognition research. Our benchmark provides detailed emotion labels and facial landmark data, proving to be a reliable and effective tool in our evaluations. We have demonstrated the practicality and reliability of our dataset, making it a valuable asset for understanding and analyzing emotions. We believe this work will inspire further research in emotion recognition, landmark analysis, and psychological studies. Our contributions aim to support and advance these fields, offering a robust dataset that reflects real-world complexity and variability in human emotions.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The number of emotion categories.",
      "page": 3
    },
    {
      "caption": "Figure 2: Landmarks faces",
      "page": 4
    },
    {
      "caption": "Figure 3: Five emotional landscapes on transformer attention.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "Australian National University1, Curtin University2"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "Abstract—In this paper, we present\na novel benchmark for"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "Emotion Recognition using facial landmarks extracted from real-"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "istic news videos. Traditional methods relying on RGB images are"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "resource-intensive, whereas our approach with Facial Landmark"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "Emotion Recognition\n(FLER)\noffers\na\nsimplified\nyet\neffective"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "alternative. By\nleveraging Graph Neural Networks\n(GNNs)\nto"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "analyze\nthe geometric and spatial\nrelationships of\nfacial\nland-"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "marks, our method enhances the understanding and accuracy of"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "emotion recognition. We discuss the advancements and challenges"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "in deep learning techniques for emotion recognition, particularly"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "focusing on Graph Neural Networks (GNNs) and Transformers."
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "Our experimental results demonstrate the viability and potential"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "of our dataset as a benchmark, setting a new direction for future"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "research in emotion recognition technologies. The codes and mod-"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "els\nare\nat: https://github.com/wangzhifengharrison/benchmark"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "real news"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "Index\nTerms—emotion\nrecognition,\ngraph\nneural\nnetwork,"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "human-centered computing"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "I.\nINTRODUCTION"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "Emotion recognition has become a part of\ninteractive tech-"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "nology, finding\nits\nplace\nin\nnumerous\napplications\nranging"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "from content\ndelivery\n[1]\nto medical\ndiagnostics\n[2]\nand"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "immersive virtual experiences\n[3]. Traditional approaches\nto"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "emotion recognition have heavily relied on RGB images, de-"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "manding significant processing resources to analyze complex"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "facial\nfeatures in such high-resolution data [4],\n[5]."
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "Facial Landmark Emotion Recognition (FLER) presents an"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "alternative that simplifies this process through the use of basic"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "geometric constructs on the face [6]."
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "A.\nIntroduction to Facial Landmark Emotion Recognition"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "Emotion\nrecognition\nthrough\nfacial\nexpressions\nis\na\nsig-"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "nificant\naspect\nof\nartificial\nintelligence,\naiming\nto\ninterpret"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "the\nemotional\nstates\nby\nhuman\nfacial\ngestures. The\nuse\nof"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "facial\nlandmarks, or distinct\nfacial points,\nis essential\nin this"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "domain, enabling the detection of subtle emotional variations."
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "The\nsubtle\nshifts\nin\nthese\nlandmarks\nare\nclosely\nlinked\nto"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "different\nemotional\nstates\nand\ntheir\nprecise\nrecognition\nis"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "critical\nfor\nenhancing the performance of\nemotion recogni-"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "tion systems. Compared to traditional\nimage-based emotion"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "recognition,\nfacial\nlandmark-based\nemotion\nrecognition\ncan"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "protect\nprivacy\nsince\nit\ndoes\nnot\ninvolve\nprocessing\nentire"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "facial\nimages [6]. Moreover,\nin resource-constrained scenarios"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "like edge computing,\nlandmark-based methods are favored for"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "their efficiency and low resource consumption. Consequently,"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "zhifeng.wang,\nkaihao.zhang,\nsab-\n{qixuan.zhang,\nEmail:"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": ""
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "rina.caldwell}@anu.edu.au"
        },
        {
          "Qixuan Zhang1, Zhifeng Wang1, Yang Liu1, Zhenyue Qin1, Kaihao Zhang1, Sabrina Caldwell1, Tom Gedeon2": "tom.gedeon@curtin.edu.au\n{lyf1082, kf.zy.qin}@gmail.com,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "different emotion classification tasks. We consider not only the": "accuracy of the models but also their computational efficiency,",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "changes due to its reliance on 3D geometric models. Akhand et"
        },
        {
          "different emotion classification tasks. We consider not only the": "thus providing significant\nreferences for applications on edge",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "al. [15] introduces a highly accurate facial emotion recognition"
        },
        {
          "different emotion classification tasks. We consider not only the": "computing devices. The motivation behind this study is to ad-",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "system employing a deep CNN model optimized via transfer"
        },
        {
          "different emotion classification tasks. We consider not only the": "vance model performance and facilitate practical applications",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "learning and a novel pipeline strategy, addressing limitations of"
        },
        {
          "different emotion classification tasks. We consider not only the": "on edge devices. We believe that\nthrough these benchmarks,",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "shallow networks and frontal-only images by fine-tuning with"
        },
        {
          "different emotion classification tasks. We consider not only the": "the research community can gain a deeper understanding of",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "emotion data for enhanced feature extraction. However,\nthese"
        },
        {
          "different emotion classification tasks. We consider not only the": "the strengths and limitations of different algorithms,\nleading",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "techniques often overlooked facial expressions’\ntemporal dy-"
        },
        {
          "different emotion classification tasks. We consider not only the": "to the design of solutions that are more in tune with practical",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "namics. Addressing this, Kollias et al. [16] introduces a CNN-"
        },
        {
          "different emotion classification tasks. We consider not only the": "requirements.",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "RNN approach\nfor\ndimensional\nemotion\nrecognition\nusing"
        },
        {
          "different emotion classification tasks. We consider not only the": "The contribution of\nthis paper\nlies\nin establishing a com-",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "multiple features and multi-task frameworks on large emotion"
        },
        {
          "different emotion classification tasks. We consider not only the": "prehensive and systematic benchmarking for\nfacial\nlandmark",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "datasets,\nsignificantly\noutperforming\nexisting methods,\nbut"
        },
        {
          "different emotion classification tasks. We consider not only the": "emotion recognition. We ensure the reliability and consistency",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "these still\nfell short\nin capturing the long-range dependencies"
        },
        {
          "different emotion classification tasks. We consider not only the": "of\nour\ntest\nresults\nthrough\nstrict\nexperimental\ndesign,\npro-",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "crucial\nfor understanding emotional expressions."
        },
        {
          "different emotion classification tasks. We consider not only the": "viding a\nsolid benchmark for\nassessing the performance of",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "various neural network models\nin the task of\nfacial emotion",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "C. Emotion Recognition with Transformers"
        },
        {
          "different emotion classification tasks. We consider not only the": "recognition.\nFurthermore, we\ndelve\ninto\nhow the\nnumber",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "and configuration of\nlandmarks\nimpact model performance,",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "Enter\nthe\nera of Transformers, which have\nredefined the"
        },
        {
          "different emotion classification tasks. We consider not only the": "revealing the significance of landmark selection in optimizing",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "possibilities\nin handling sequential data thanks\nto their\nself-"
        },
        {
          "different emotion classification tasks. We consider not only the": "model efficiency. Additionally, we have created our dataset and",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "attention mechanisms. Vaswani et al.\n[17]\nrevolutionized se-"
        },
        {
          "different emotion classification tasks. We consider not only the": "benchmarking code open source for\nthe community to facili-",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "quence modeling with\nthe\nintroduction\nof\nthe Transformer"
        },
        {
          "different emotion classification tasks. We consider not only the": "tate further research and development. These contributions not",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "model, which has since been adapted beyond the boundaries"
        },
        {
          "different emotion classification tasks. We consider not only the": "only provide the academic community with research resources",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "of\nlanguage processing.\nIn the realm of\nfacial emotion recog-"
        },
        {
          "different emotion classification tasks. We consider not only the": "but also lay a solid foundation for\nthe industry to implement",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "nition,\nrecent\nstudies have begun to explore\nthe\napplication"
        },
        {
          "different emotion classification tasks. We consider not only the": "efficient\nalgorithms,\nthereby\npromoting\nthe\ndevelopment\nof",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "of Transformers. For\ninstance, Zhao et al.\n[18]\nintroduces a"
        },
        {
          "different emotion classification tasks. We consider not only the": "technology geared towards practical applications.",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "geometry-guided FER framework using graph convolutional"
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "networks\nand\ntransformers,\nenhancing\nemotion\nrecognition"
        },
        {
          "different emotion classification tasks. We consider not only the": "II. RELATED WORK",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "from videos\nby\nconstructing\nspatial-temporal\ngraphs with"
        },
        {
          "different emotion classification tasks. We consider not only the": "In this section, we explore the landscape of emotion recog-",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "facial\nlandmarks and incorporating attention mechanisms\nfor"
        },
        {
          "different emotion classification tasks. We consider not only the": "nition,\nstarting\nfrom its\nfundamental\ntasks\nand\nthe\nrole\nof",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "more\ninformative\nfeature\nemphasis. Zheng et al.\n[19]\nintro-"
        },
        {
          "different emotion classification tasks. We consider not only the": "facial\nlandmarks,\nto the\nlatest\nadvancements\nin deep learn-",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "duces POSTER,\na\ntwo-stream Pyramid\ncross-fusion Trans-"
        },
        {
          "different emotion classification tasks. We consider not only the": "ing, particularly focusing on Convolutional Neural Networks",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "former network, aiming to address\nthe key FER challenges:"
        },
        {
          "different emotion classification tasks. We consider not only the": "(CNNs) and Transformers. Each subsection delves into signifi-",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "inter-class\nsimilarity,\nintra-class discrepancy,\nand scale\nsen-"
        },
        {
          "different emotion classification tasks. We consider not only the": "cant developments, challenges, and potential future directions,",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "sitivity\nby\nfusing\nfacial\nlandmark\nand\nimage\nfeatures\nand"
        },
        {
          "different emotion classification tasks. We consider not only the": "laying out a comprehensive overview of the field’s progression",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "employing a pyramid structure\nfor\nscale\ninvariance. Hybrid"
        },
        {
          "different emotion classification tasks. We consider not only the": "and the cutting-edge techniques\nshaping the future of\nfacial",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "models have also emerged, combining the strengths of CNNs"
        },
        {
          "different emotion classification tasks. We consider not only the": "landmark-based emotion recognition.",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "and Transformers. Karatay et al.\n[20] presents a deep neural"
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "network framework combining Gaussian mixture models with"
        },
        {
          "different emotion classification tasks. We consider not only the": "A.\nIntroduction\nto Emotion Recognition\nand Facial\nLand-",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "CNN and Transformer for emotion detection from videos and"
        },
        {
          "different emotion classification tasks. We consider not only the": "marks",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "images using facial and body features extracted by OpenPose,"
        },
        {
          "different emotion classification tasks. We consider not only the": "The\njourney\ntoward\nautomating\nemotion\nrecognition\nhas",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "addressing various basic emotions."
        },
        {
          "different emotion classification tasks. We consider not only the": "covered various fields, from early psychophysiological studies",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "to the\ncurrent\nartificial\nintelligence\ntechniques. This\nsection",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "will\nintroduce significant past works, specifically focusing on",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "D. Addressing Diversity and Data Augmentation in Emotion"
        },
        {
          "different emotion classification tasks. We consider not only the": "the use of facial\nlandmarks as indicators for emotion recogni-",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "Recognition"
        },
        {
          "different emotion classification tasks. We consider not only the": "tion. Traditional emotion recognition systems mainly utilized",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "methods based on geometric features, employing distances and",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "Moreover,\nthe impact of data diversity and representative-"
        },
        {
          "different emotion classification tasks. We consider not only the": "angles between facial\nlandmarks as direct\ninputs to classifiers",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "ness has also been a focal point\nin the literature. Studies like"
        },
        {
          "different emotion classification tasks. We consider not only the": "such as Support Vector Machines\n(SVMs)\n[12]\nand simple",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "et\nal.\nthat\nof Li\n[21]\nhighlighted\nthe\nchallenges\nposed\nby"
        },
        {
          "different emotion classification tasks. We consider not only the": "neural networks [13].",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "variations in ethnicity, age, and lighting conditions, prompting"
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "the development of more robust models. The employment of"
        },
        {
          "different emotion classification tasks. We consider not only the": "B. Advancements in Deep Learning for Emotion Recognition",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": ""
        },
        {
          "different emotion classification tasks. We consider not only the": "",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "Generative Adversarial Networks (GANs)\nfor data augmenta-"
        },
        {
          "different emotion classification tasks. We consider not only the": "With the\nrevival of neural networks,\nespecially Convolu-",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "tion, as explored by Hajarolasvadi et al.\n[22], has been one"
        },
        {
          "different emotion classification tasks. We consider not only the": "tional Neural Networks\n(CNNs),\nthe focus\nshifted to deeper",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "avenue to enhance the diversity and quantity of\ntraining data,"
        },
        {
          "different emotion classification tasks. We consider not only the": "et\nal.\nneural\nnetworks\nfor\nemotion\nrecognition. Yang\n[14]",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "thereby improving the generalizability of emotion recognition"
        },
        {
          "different emotion classification tasks. We consider not only the": "introduces a 3D facial expression recognition algorithm using",
          "CNNs\nand\nlandmark\nfeatures,\nrobust\nto\npose\nand\nlighting": "systems."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I: Emotion Distribution Summary.": ""
        },
        {
          "TABLE I: Emotion Distribution Summary.": ""
        },
        {
          "TABLE I: Emotion Distribution Summary.": "Training"
        },
        {
          "TABLE I: Emotion Distribution Summary.": ""
        },
        {
          "TABLE I: Emotion Distribution Summary.": "914"
        },
        {
          "TABLE I: Emotion Distribution Summary.": "2,031"
        },
        {
          "TABLE I: Emotion Distribution Summary.": "3,091"
        },
        {
          "TABLE I: Emotion Distribution Summary.": "3,536"
        },
        {
          "TABLE I: Emotion Distribution Summary.": "3,100"
        },
        {
          "TABLE I: Emotion Distribution Summary.": "12,672"
        },
        {
          "TABLE I: Emotion Distribution Summary.": ""
        },
        {
          "TABLE I: Emotion Distribution Summary.": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Neutral\n3,536\n100\n200\n3,836": "Sad\n3,100\n100\n200\n3,400"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "Total\n12,672\n500\n1,000\n14,172"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "E. Conclusion\nand Future Directions\nin Facial Landmark-"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "based Emotion Recognition"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "The review of related work underscores a gradual but signif-"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "icant shift\ntowards sophisticated models that consider both the"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "spatial and temporal aspects of\nfacial\nlandmarks\nin emotion"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "recognition. The\nexploration into Transformer-based models"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "marks\na\nnew frontier\nin\nthis\ndomain,\npromising\nenhanced"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "accuracy and deeper understanding of emotional states through"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "the advanced modeling of sequential landmark data. This paper"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "builds upon these foundations, aiming to further\nthe efficacy"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "and applicability of facial landmark-based emotion recognition"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "systems."
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "III. FACIAL EXPRESSIONS IN REAL NEWS DATASET"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "A. Dataset overview"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "In Table\nI,\nstriking feature of\nthis distribution is\nthe\nsig-"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "nificant variance\nin the number of\nsamples\nacross different"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "emotions. For instance, Neutral, Happy, and Fear emotions are"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "notably more represented in the dataset, with counts exceeding"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "3,500, 3,000,\nand 2,000 respectively in the Training subset."
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "The Total column sums the Training and Validation and Test"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "samples, offering a holistic view of the dataset’s composition,"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "with a grand total of 14,172 instances. This\ncomprehensive"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "distribution is critical\nfor understanding the dataset’s balance"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "and evaluating the potential biases\nin the training, validating"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "testing of emotion recognition models."
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "B. Basic emotion categories"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "We capture 5 different daily basic emotion categories in real"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "news. Fig.1 illustrates the distribution of 5 different emotion"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "categories,\neach represented by a unique\ncolor. The\nlargest"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "portion is\n‘Neutral’\nat 27.1%,\nfollowed closely by ‘Sad’\nat"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "24.0%. ‘Happy’ represents 23.9% of the chart, while ‘Fear’ is"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "at 16.4%. The smallest\nslice is\n‘Angry’, making up 8.6% of"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "the chart. Each category’s percentage is provided."
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "C. Data acquisition and preparation"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "We initiated our dataset by downloading 318 news videos"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "from internationally recognized media sources such as ABC,"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "Al Jazeera, NBC News, BBC, CBC, and CCTV and involved"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": ""
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "extracting clear, well-defined facial\nimages from these videos."
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "We established a quality control process that\nincluded evaluat-"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "ing image sharpness, ensuring all\nfacial\nfeatures were visible"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "and distinct, and preventing the selection of repetitive images."
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "Additionally, we\nemployed\nadaptive\npadding\ntechnology\nto"
        },
        {
          "Neutral\n3,536\n100\n200\n3,836": "adjust each image’s frame size based on the facial dimensions,"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2: Landmarks faces": "structures and expressions, ensuring stable performance across"
        },
        {
          "Fig. 2: Landmarks faces": "a wide range of\nfacial data."
        },
        {
          "Fig. 2: Landmarks faces": ""
        },
        {
          "Fig. 2: Landmarks faces": "C. Dynamic\nand\nEdge-Enhanced\nLearning\nfor\nExpressive"
        },
        {
          "Fig. 2: Landmarks faces": "Feature Interpretation"
        },
        {
          "Fig. 2: Landmarks faces": ""
        },
        {
          "Fig. 2: Landmarks faces": "Dynamic\nGraph\nCNNs\n(DGCNNs)\n[30]\nand\nEdge-"
        },
        {
          "Fig. 2: Landmarks faces": ""
        },
        {
          "Fig. 2: Landmarks faces": "Conditioned Convolution\n(ECC)\n[31]\nnetworks\npresent\nin-"
        },
        {
          "Fig. 2: Landmarks faces": "novative\napproaches\nfor\nanalyzing the\nevolving characteris-"
        },
        {
          "Fig. 2: Landmarks faces": "tics of\nfacial expressions. DGCNNs are specifically designed"
        },
        {
          "Fig. 2: Landmarks faces": "for\ntime-series data, effectively tracking the development of"
        },
        {
          "Fig. 2: Landmarks faces": "emotional\nindicators\nover\ntime,\na\ncritical\naspect\nin\nvideo-"
        },
        {
          "Fig. 2: Landmarks faces": "based emotion recognition. ECC networks\ncomplement\nthis"
        },
        {
          "Fig. 2: Landmarks faces": "approach by factoring in the\nattributes of\nedges\nconnecting"
        },
        {
          "Fig. 2: Landmarks faces": "facial\nlandmarks. This addition enhances\nthe model’s\ninsight"
        },
        {
          "Fig. 2: Landmarks faces": "into the impact of\nlandmark interrelations on the expression"
        },
        {
          "Fig. 2: Landmarks faces": "of emotions."
        },
        {
          "Fig. 2: Landmarks faces": ""
        },
        {
          "Fig. 2: Landmarks faces": "D.\nSpatial-Temporal and Topological\nInsights with Advanced"
        },
        {
          "Fig. 2: Landmarks faces": "GNN Architectures"
        },
        {
          "Fig. 2: Landmarks faces": "Spatial Graph Convolutional Networks\n[32]\nand Graph"
        },
        {
          "Fig. 2: Landmarks faces": ""
        },
        {
          "Fig. 2: Landmarks faces": "Isomorphism Networks\n(GIN)\n[33] offer novel perspectives"
        },
        {
          "Fig. 2: Landmarks faces": ""
        },
        {
          "Fig. 2: Landmarks faces": "in\nunderstanding\nfacial\nlandmark\nstructures,\nutilizing\nboth"
        },
        {
          "Fig. 2: Landmarks faces": "spatial-temporal\nand\ntopological\ndata. Spatial\nnetworks\nare"
        },
        {
          "Fig. 2: Landmarks faces": "adept at directly learning from the distribution of\nlandmarks,"
        },
        {
          "Fig. 2: Landmarks faces": "effectively\nidentifying\nthe\nspatial\npatterns\nassociated with"
        },
        {
          "Fig. 2: Landmarks faces": "various\nemotions. Conversely, GIN focuses on the\nintrinsic"
        },
        {
          "Fig. 2: Landmarks faces": "topological structures of these patterns. This approach enables"
        },
        {
          "Fig. 2: Landmarks faces": "the model\nto distinguish between closely related yet distinct"
        },
        {
          "Fig. 2: Landmarks faces": "emotional states by analyzing the configuration and character-"
        },
        {
          "Fig. 2: Landmarks faces": "istics of neighboring landmarks."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II: Emotion": "evaluated approaches on our proposed dataset.",
          "recognition\nperformance": "",
          "by\ndifferent": "",
          "TABLE III: Performance": "gories on the our proposed dataset.",
          "comparison": "",
          "of five": "",
          "emotion\ncate-": ""
        },
        {
          "TABLE II: Emotion": "Emotion\nMLP [34]",
          "recognition\nperformance": "GINFormer [35]\nGIN [33]",
          "by\ndifferent": "SAGE [29]",
          "TABLE III: Performance": "MLP [34]",
          "comparison": "GINFormer [35]",
          "of five": "GIN [33]",
          "emotion\ncate-": "SAGE [29]"
        },
        {
          "TABLE II: Emotion": "Ave Acc\n28.00%",
          "recognition\nperformance": "33.10%\n30.20%",
          "by\ndifferent": "32.80%",
          "TABLE III: Performance": "33.50%",
          "comparison": "46.50%",
          "of five": "40.00%",
          "emotion\ncate-": "49.50%"
        },
        {
          "TABLE II: Emotion": "",
          "recognition\nperformance": "",
          "by\ndifferent": "",
          "TABLE III: Performance": "40.50%",
          "comparison": "31.50%",
          "of five": "27.00%",
          "emotion\ncate-": "27.00%"
        },
        {
          "TABLE II: Emotion": "",
          "recognition\nperformance": "",
          "by\ndifferent": "",
          "TABLE III: Performance": "31.50%",
          "comparison": "45.50%",
          "of five": "29.50%",
          "emotion\ncate-": "19.00%"
        },
        {
          "TABLE II: Emotion": "",
          "recognition\nperformance": "",
          "by\ndifferent": "",
          "TABLE III: Performance": "27.50%",
          "comparison": "23.50%",
          "of five": "18.00%",
          "emotion\ncate-": "43.00%"
        },
        {
          "TABLE II: Emotion": "",
          "recognition\nperformance": "V. EXPERIMENTS",
          "by\ndifferent": "",
          "TABLE III: Performance": "7.00%",
          "comparison": "18.50%",
          "of five": "36.50%",
          "emotion\ncate-": "25.50%"
        },
        {
          "TABLE II: Emotion": "A. Experiment settings",
          "recognition\nperformance": "",
          "by\ndifferent": "",
          "TABLE III: Performance": "",
          "comparison": "",
          "of five": "",
          "emotion\ncate-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "V. EXPERIMENTS": "A. Experiment settings",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": ""
        },
        {
          "V. EXPERIMENTS": "We executed experiments on our proposed dataset\nto evalu-",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": ""
        },
        {
          "V. EXPERIMENTS": "",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": "architecture that\nscales\nlinearly with the graph size,\nserving"
        },
        {
          "V. EXPERIMENTS": "ate 5 classes of expressions, utilizing an Ubuntu 20.04 work-",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": ""
        },
        {
          "V. EXPERIMENTS": "",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": "as a universal function approximator. The proposed framework"
        },
        {
          "V. EXPERIMENTS": "station equipped with an NVIDIA GeForce GTX 3090Ti GPU.",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": ""
        },
        {
          "V. EXPERIMENTS": "",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": "integrates three core elements—positional/structural encoding,"
        },
        {
          "V. EXPERIMENTS": "Network implementation was\ncarried out using the Pytorch",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": ""
        },
        {
          "V. EXPERIMENTS": "",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": "local message-passing, and global attention—to achieve state-"
        },
        {
          "V. EXPERIMENTS": "framework.",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": ""
        },
        {
          "V. EXPERIMENTS": "",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": "of-the-art\nresults in graph representation learning."
        },
        {
          "V. EXPERIMENTS": "B. Accuracy",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": "The Table III presents a comparison of performance across"
        },
        {
          "V. EXPERIMENTS": "",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": "different methods on a proposed dataset. The methods com-"
        },
        {
          "V. EXPERIMENTS": "Accuracy is calculated by the formula:",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": ""
        },
        {
          "V. EXPERIMENTS": "",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": "pared include MLP, GINformer, GIN, and SAGE, with their"
        },
        {
          "V. EXPERIMENTS": "(cid:19)",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": ""
        },
        {
          "V. EXPERIMENTS": "(cid:18) P",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": ""
        },
        {
          "V. EXPERIMENTS": "",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": "performances\nreported in percentages\nfor five different emo-"
        },
        {
          "V. EXPERIMENTS": "× 100\n(1)\nAccuracy (%) =",
          "Angry\n7.00%\n18.50%\n36.50%\n25.50%": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "analysis, and psychological studies. Our contributions aim to\napproaches. Deep learning models\nlike GINFormer\nexcel\nin"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "support\nand\nadvance\nthese fields,\noffering\na\nrobust\ndataset\nidentifying and leveraging facial\nfeature patterns\nto deduce"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "that\nreflects\nreal-world complexity and variability in human\nemotions, while psychological methods provide an in-depth,"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "emotions.\nmuscle-by-muscle dissection of\nfacial expressions as per\nthe"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "FACS. Such findings highlight\nthe synergy between AI’s ob-"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "REFERENCES\njective pattern recognition and the qualitative, detailed muscle"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "movement\nanalysis\nat\nthe\nheart\nof\npsychological\nresearch,"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "[1] A. Saxena, A. Khanna, and D. Gupta, “Emotion recognition and detec-"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "suggesting a multidisciplinary fusion that could enrich the field"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "tion methods: A comprehensive survey,” Journal of Artificial Intelligence"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "and Systems, vol. 2, no. 1, pp. 53–79, 2020.\nof emotion recognition."
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "[2]\nJ. Chen, M. Short, and E. Kemps, “Interpretation bias in social anxiety:"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "A systematic review and meta-analysis,” Journal of Affective Disorders,"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "VI. CONCLUSION"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "vol. 276, pp. 1119–1130, 2020."
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "[3]\nJ. Mar´ın-Morales, C. Llinares,\nJ. Guixeres, and M. Alca˜niz, “Emotion"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "In\nthis\nstudy, we\npropose\na\nnovel\nbenchmark\nusing\nre-"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "recognition\nin\nimmersive\nvirtual\nreality: From statistics\nto\naffective"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "alistic\nnews\nvideos,\ncomplemented with RGB images\nand\ncomputing,” Sensors, vol. 20, no. 18, p. 5163, 2020."
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "[4] Q. T. Ngoc, S. Lee, and B. C. Song, “Facial\nlandmark-based emotion\nfacial\nlandmark coordinates,\nto enhance emotion recognition"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "recognition via directed graph neural network,” Electronics, vol. 9, no. 5,"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "research. Our benchmark provides detailed emotion labels and"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "p. 764, 2020."
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "facial\nlandmark data, proving to be\na\nreliable\nand effective"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "[5] K. Zhang, Y. Huang, Y. Du, and L. Wang, “Facial expression recognition"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "based on deep evolutional\nspatial-temporal networks,” IEEE Transac-\ntool\nin our evaluations. We have demonstrated the practicality"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "tions on Image Processing, vol. 26, no. 9, pp. 4193–4203, 2017."
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "and reliability of our dataset, making it a valuable asset\nfor"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "[6] B. Chen, W. Guan, P. Li, N.\nIkeda, K. Hirasawa, and H. Lu, “Residual"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "understanding and analyzing emotions. We believe this work"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "multi-task\nlearning\nfor\nfacial\nlandmark\nlocalization\nand\nexpression"
        },
        {
          "Fig. 3: Five emotional\nlandscapes on transformer attention.": "recognition,” Pattern Recognition, vol. 115, p. 107893, 2021.\nwill\ninspire further research in emotion recognition,\nlandmark"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "technique\nfor\nfacial\nemotion\nrecognition\nusing\nfacial\nlandmarks,”\nin",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "in neural\ninformation processing\nlearning on large graphs,” Advances"
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "2021\nInternational Conference\non\nInformation\nand Communication",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "systems, vol. 30, 2017."
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "Technology Convergence (ICTC).\nIEEE, 2021, pp. 1072–1076.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "[30] Y. Liu, X. Zhang, Y. Li, J. Zhou, X. Li, and G. Zhao, “Graph-based facial"
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[8] X.\nJin, Z. Lai, and Z.\nJin, “Learning dynamic relationships\nfor\nfacial",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "affect analysis: A review,” IEEE Transactions on Affective Computing,"
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "IEEE\nexpression recognition based on graph convolutional network,”",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "2022."
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "Transactions on Image Processing, vol. 30, pp. 7143–7155, 2021.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "[31]\nJ. Han, L. Du, X. Ye, L. Zhang, and J. Feng, “The devil\nis in the face:"
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[9]\nL. Zhong, C. Bai, J. Li, T. Chen, S. Li, and Y. Liu, “A graph-structured",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "Exploiting harmonious representations for facial expression recognition,”"
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "representation with brnn for static-based facial expression recognition,”",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "Neurocomputing, vol. 486, pp. 104–113, 2022."
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "2019\n14th\nIEEE International Conference\non Automatic Face &\nin",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "[32]\nJ. Bai, W. Yu, Z. Xiao, V. Havyarimana, A. C. Regan, H.\nJiang, and"
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "Gesture Recognition (FG 2019).\nIEEE, 2019, pp. 1–5.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "L. Jiao, “Two-stream spatial–temporal graph convolutional networks for"
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[10]\nT. Rao, J. Li, X. Wang, Y. Sun, and H. Chen, “Facial expression recog-",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "driver drowsiness detection,” IEEE Transactions on Cybernetics, vol. 52,"
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "nition with multiscale graph convolutional networks,” IEEE MultiMedia,",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "no. 12, pp. 13 821–13 833, 2021."
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "vol. 28, no. 2, pp. 11–19, 2021.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "[33] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph"
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "neural networks?” arXiv preprint arXiv:1810.00826, 2018."
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[11] R. Zhao, T. Liu, Z. Huang, D. P.-K. Lun, and K. K. Lam, “Geometry-",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "[34] H. Palo, M. N. Mohanty, and M. Chandra, “Use of different features for"
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "aware\nfacial\nexpression recognition via\nattentive graph convolutional",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "emotion recognition using mlp network,” in Computational Vision and"
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "networks,” IEEE Transactions on Affective Computing, 2021.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "Robotics: Proceedings of\nICCVR 2014.\nSpringer, 2015, pp. 7–15."
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[12] X. Liu, X. Cheng, and K. Lee, “Ga-svm-based facial emotion recognition",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "[35]\nL. Ramp´aˇsek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf,\nand"
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "using facial geometric features,” IEEE Sensors Journal, vol. 21, no. 10,",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "D. Beaini,\n“Recipe\nfor\na General, Powerful, Scalable Graph Trans-"
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "pp. 11 532–11 542, 2020.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "in Neural\nformer,” Advances\nInformation Processing Systems, vol. 35,"
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[13] B. Hasani\nand M. H. Mahoor,\n“Facial\nexpression\nrecognition\nusing",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": "2022."
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "the\nenhanced deep 3d convolutional neural networks,” in Proceedings of",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "IEEE conference on computer vision and pattern recognition workshops,",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "2017, pp. 30–40.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[14] H. Yang\nand L. Yin,\n“Cnn\nbased\n3d\nfacial\nexpression\nrecognition",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "using masking and landmark features,”\nin 2017 seventh international",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "conference\non\naffective\ncomputing\nand\nintelligent\ninteraction\n(ACII).",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "IEEE, 2017, pp. 556–560.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[15] M. Akhand, S. Roy, N. Siddique, M. A. S. Kamal, and T. Shimamura,",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "“Facial\nemotion recognition using transfer\nlearning in the deep cnn,”",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "Electronics, vol. 10, no. 9, p. 1036, 2021.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[16] D. Kollias and S. Zafeiriou, “Exploiting multi-cnn features\nin cnn-rnn",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "based dimensional emotion recognition on the omg in-the-wild dataset,”",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "IEEE Transactions on Affective Computing, vol. 12, no. 3, pp. 595–606,",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "2020.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "in\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "neural\ninformation processing systems, vol. 30, 2017.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[18] R. Zhao, T. Liu, Z. Huang, D.\nP. Lun,\nand K.-M. Lam,\n“Spatial-",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "temporal graphs plus transformers for geometry-guided facial expression",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "recognition,” IEEE Transactions on Affective Computing, 2022.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[19] C. Zheng, M. Mendieta, and C. Chen, “Poster: A pyramid cross-fusion",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "transformer network for\nfacial expression recognition,” in Proceedings",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "of\nthe IEEE/CVF International Conference on Computer Vision, 2023,",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "pp. 3146–3155.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "¨\n[20] B. Karatay, D. Bes¸tepe, K. Sailunaz, T.\nOzyer,\nand R. Alhajj,\n“Cnn-",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "transformer\nbased\nemotion\nclassification\nfrom facial\nexpressions\nand",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "body gestures,” Multimedia Tools and Applications, pp. 1–43, 2023.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[21]\nS. Li and W. Deng, “Deep facial expression recognition: A survey,” IEEE",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "transactions on affective computing, vol. 13, no. 3, pp. 1195–1215, 2020.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[22] N. Hajarolasvadi, M. A. Ramirez, W. Beccaro, and H. Demirel, “Gener-",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "ative adversarial networks in human emotion synthesis: A review,” IEEE",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "Access, vol. 8, pp. 218 499–218 529, 2020.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "Journal\nof\n[23] D. E. King,\n“Dlib-ml: A machine\nlearning\ntoolkit,” The",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "Machine Learning Research, vol. 10, pp. 1755–1758, 2009.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[24] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, “Deepface: Closing the",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "gap to human-level performance in face verification,” in Proceedings of",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "the IEEE conference on computer vision and pattern recognition, 2014,",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "pp. 1701–1708.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[25] C. Lugaresi, J. Tang, H. Nash, C. McClanahan, E. Uboweja, M. Hays,",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "et\nF.\nZhang, C.-L. Chang, M. G. Yong,\nJ.\nLee\nal.,\n“Mediapipe:",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "arXiv\npreprint\nA\nframework\nfor\nbuilding\nperception\npipelines,”",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "arXiv:1906.08172, 2019.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[26]\nL. Q. Nguyen, Y. Li, H. Wang, L. M. Dang, H.-K. Song, H. Moon",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "et\nal.,\n“Facial\nlandmark\ndetection with\nlearnable\nconnectivity\ngraph",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "convolutional network,” IEEE Access, vol. 10, pp. 94 354–94 362, 2022.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[27] Q.\nLi,\nS.\nLiu,\nL. Hu,\nand X.\nLiu,\n“Shape\ncorrespondence\nusing",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "the IEEE/CVF\nanisotropic chebyshev spectral cnns,” in Proceedings of",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "conference\non Computer Vision\nand Pattern Recognition,\n2020,\npp.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "14 658–14 667.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "[28] A. Prados-Torreblanca, J. M. Buenaposada, and L. Baumela, “Shape pre-",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "serving facial\nlandmarks with graph attention networks,” arXiv preprint",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        },
        {
          "[7] A.\nPoulose,\nJ. H. Kim,\nand D.\nS. Han,\n“Feature\nvector\nextraction": "arXiv:2210.07233, 2022.",
          "[29] W. Hamilton,\nZ. Ying,\nand\nJ.\nLeskovec,\n“Inductive\nrepresentation": ""
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition and detection methods: A comprehensive survey",
      "authors": [
        "A Saxena",
        "A Khanna",
        "D Gupta"
      ],
      "year": "2020",
      "venue": "Journal of Artificial Intelligence and Systems"
    },
    {
      "citation_id": "2",
      "title": "Interpretation bias in social anxiety: A systematic review and meta-analysis",
      "authors": [
        "J Chen",
        "M Short",
        "E Kemps"
      ],
      "year": "2020",
      "venue": "Journal of Affective Disorders"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition in immersive virtual reality: From statistics to affective computing",
      "authors": [
        "J Marín-Morales",
        "C Llinares",
        "J Guixeres",
        "M Alcañiz"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "4",
      "title": "Facial landmark-based emotion recognition via directed graph neural network",
      "authors": [
        "Q Ngoc",
        "S Lee",
        "B Song"
      ],
      "year": "2020",
      "venue": "Electronics"
    },
    {
      "citation_id": "5",
      "title": "Facial expression recognition based on deep evolutional spatial-temporal networks",
      "authors": [
        "K Zhang",
        "Y Huang",
        "Y Du",
        "L Wang"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "6",
      "title": "Residual multi-task learning for facial landmark localization and expression recognition",
      "authors": [
        "B Chen",
        "W Guan",
        "P Li",
        "N Ikeda",
        "K Hirasawa",
        "H Lu"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Feature vector extraction technique for facial emotion recognition using facial landmarks",
      "authors": [
        "A Poulose",
        "J Kim",
        "D Han"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Information and Communication Technology Convergence (ICTC)"
    },
    {
      "citation_id": "8",
      "title": "Learning dynamic relationships for facial expression recognition based on graph convolutional network",
      "authors": [
        "X Jin",
        "Z Lai",
        "Z Jin"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "9",
      "title": "A graph-structured representation with brnn for static-based facial expression recognition",
      "authors": [
        "L Zhong",
        "C Bai",
        "J Li",
        "T Chen",
        "S Li",
        "Y Liu"
      ],
      "year": "2019",
      "venue": "Gesture Recognition"
    },
    {
      "citation_id": "10",
      "title": "Facial expression recognition with multiscale graph convolutional networks",
      "authors": [
        "T Rao",
        "J Li",
        "X Wang",
        "Y Sun",
        "H Chen"
      ],
      "year": "2021",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "11",
      "title": "Geometryaware facial expression recognition via attentive graph convolutional networks",
      "authors": [
        "R Zhao",
        "T Liu",
        "Z Huang",
        "D .-K. Lun",
        "K Lam"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Ga-svm-based facial emotion recognition using facial geometric features",
      "authors": [
        "X Liu",
        "X Cheng",
        "K Lee"
      ],
      "year": "2020",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "13",
      "title": "Facial expression recognition using enhanced deep 3d convolutional neural networks",
      "authors": [
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "14",
      "title": "Cnn based 3d facial expression recognition using masking and landmark features",
      "authors": [
        "H Yang",
        "L Yin"
      ],
      "year": "2017",
      "venue": "2017 seventh international conference on affective computing and intelligent interaction (ACII)"
    },
    {
      "citation_id": "15",
      "title": "Facial emotion recognition using transfer learning in the deep cnn",
      "authors": [
        "M Akhand",
        "S Roy",
        "N Siddique",
        "M Kamal",
        "T Shimamura"
      ],
      "year": "2021",
      "venue": "Electronics"
    },
    {
      "citation_id": "16",
      "title": "Exploiting multi-cnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "18",
      "title": "Spatialtemporal graphs plus transformers for geometry-guided facial expression recognition",
      "authors": [
        "R Zhao",
        "T Liu",
        "Z Huang",
        "D Lun",
        "K.-M Lam"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Poster: A pyramid cross-fusion transformer network for facial expression recognition",
      "authors": [
        "C Zheng",
        "M Mendieta",
        "C Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Cnntransformer based emotion classification from facial expressions and body gestures",
      "authors": [
        "B Karatay",
        "D Bes ¸tepe",
        "K Sailunaz",
        "T Özyer",
        "R Alhajj"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "21",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "22",
      "title": "Generative adversarial networks in human emotion synthesis: A review",
      "authors": [
        "N Hajarolasvadi",
        "M Ramirez",
        "W Beccaro",
        "H Demirel"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "23",
      "title": "Dlib-ml: A machine learning toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "24",
      "title": "Deepface: Closing the gap to human-level performance in face verification",
      "authors": [
        "Y Taigman",
        "M Yang",
        "M Ranzato",
        "L Wolf"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "25",
      "title": "Mediapipe: A framework for building perception pipelines",
      "authors": [
        "C Lugaresi",
        "J Tang",
        "H Nash",
        "C Mcclanahan",
        "E Uboweja",
        "M Hays",
        "F Zhang",
        "C.-L Chang",
        "M Yong",
        "J Lee"
      ],
      "year": "2019",
      "venue": "Mediapipe: A framework for building perception pipelines",
      "arxiv": "arXiv:1906.08172"
    },
    {
      "citation_id": "26",
      "title": "Facial landmark detection with learnable connectivity graph convolutional network",
      "authors": [
        "L Nguyen",
        "Y Li",
        "H Wang",
        "L Dang",
        "H.-K Song",
        "H Moon"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "27",
      "title": "Shape correspondence using anisotropic chebyshev spectral cnns",
      "authors": [
        "Q Li",
        "S Liu",
        "L Hu",
        "X Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Shape preserving facial landmarks with graph attention networks",
      "authors": [
        "A Prados-Torreblanca",
        "J Buenaposada",
        "L Baumela"
      ],
      "year": "2022",
      "venue": "Shape preserving facial landmarks with graph attention networks",
      "arxiv": "arXiv:2210.07233"
    },
    {
      "citation_id": "29",
      "title": "Inductive representation learning on large graphs",
      "authors": [
        "W Hamilton",
        "Z Ying",
        "J Leskovec"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "30",
      "title": "Graph-based facial affect analysis: A review",
      "authors": [
        "Y Liu",
        "X Zhang",
        "Y Li",
        "J Zhou",
        "X Li",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "The devil is in the face: Exploiting harmonious representations for facial expression recognition",
      "authors": [
        "J Han",
        "L Du",
        "X Ye",
        "L Zhang",
        "J Feng"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "32",
      "title": "Two-stream spatial-temporal graph convolutional networks for driver drowsiness detection",
      "authors": [
        "J Bai",
        "W Yu",
        "Z Xiao",
        "V Havyarimana",
        "A Regan",
        "H Jiang",
        "L Jiao"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "33",
      "title": "How powerful are graph neural networks?",
      "authors": [
        "K Xu",
        "W Hu",
        "J Leskovec",
        "S Jegelka"
      ],
      "year": "2018",
      "venue": "How powerful are graph neural networks?",
      "arxiv": "arXiv:1810.00826"
    },
    {
      "citation_id": "34",
      "title": "Use of different features for emotion recognition using mlp network",
      "authors": [
        "H Palo",
        "M Mohanty",
        "M Chandra"
      ],
      "year": "2015",
      "venue": "Computational Vision and Robotics: Proceedings of ICCVR 2014"
    },
    {
      "citation_id": "35",
      "title": "Recipe for a General, Powerful, Scalable Graph Transformer",
      "authors": [
        "L Rampášek",
        "M Galkin",
        "V Dwivedi",
        "A Luu",
        "G Wolf",
        "D Beaini"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    }
  ]
}