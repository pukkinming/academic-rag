{
  "paper_id": "2507.15914v1",
  "title": "Msgm: A Multi-Scale Spatiotemporal Graph Mamba For Eeg Emotion Recognition",
  "published": "2025-07-21T17:18:00Z",
  "authors": [
    "Hanwen Liu",
    "Yifeng Gong",
    "Zuwei Yan",
    "Zeheng Zhuang",
    "Jiaxuan Lu"
  ],
  "keywords": [
    "Electroencephalogram (EEG)",
    "emotion recognition",
    "multi-scale",
    "graph neural networks",
    "Mamba e-mail: (liuhw56",
    "gongyf9)@mail2.sysu.edu.cn"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "EEG-based emotion recognition struggles with capturing multi-scale spatiotemporal dynamics and ensuring computational efficiency for real-time applications. Existing methods often oversimplify temporal granularity and spatial hierarchies, limiting accuracy. To overcome these challenges, we propose the Multi-Scale Spatiotemporal Graph Mamba (MSGM), a novel framework integrating multi-window temporal segmentation, bimodal spatial graph modeling, and efficient fusion via the Mamba architecture. By segmenting EEG signals across diverse temporal scales and constructing global-local graphs with neuroanatomical priors, MSGM effectively captures fine-grained emotional fluctuations and hierarchical brain connectivity. A multi-depth Graph Convolutional Network (GCN) and token embedding fusion module, paired with Mamba's state-space modeling, enable dynamic spatiotemporal interaction at linear complexity. Notably, with just one MSST-Mamba layer, MSGM surpasses leading methods in the field on the SEED, THU-EP, and FACED datasets, outperforming baselines in subject-independent emotion classification while achieving robust accuracy and millisecondlevel inference on the NVIDIA Jetson Xavier NX.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTION recognition has emerged as a critical re- search frontier with far-reaching implications for humancomputer interaction, mental health monitoring, and neuroscientific exploration  [1]  [2]  [3] . The ability to decode emotional states in real-time promises to revolutionize intelligent systems by enhancing user adaptability and bolstering clinical applications through early detection and management of emotional disorders  [4]    [5] . As these capabilities become increasingly vital in healthcare and artificial intelligence, there is an urgent need for robust, efficient, and neurophysiologically grounded approaches to overcome both theoretical complexities and practical deployment challenges  [6] .\n\nElectroencephalography (EEG) stands out as a premier modality for emotion recognition, owing to its unparalleled capacity to non-invasively record brain activity with high temporal resolution, directly capturing the neural signatures of emotional processes  [7] . Unlike indirect proxies such as facial expression analysis or speech intonation, EEG offers immediate access to the brain's dynamic responses, making it ideally suited for applications requiring precision and responsiveness. However, the efficacy of EEG-based emotion recognition is tempered by significant hurdles: the signals' susceptibility to noise, their spatial heterogeneity across brain regions  [8] , and their complex temporal dynamics, which span short-term fluctuations and long-term trends yet are often inadequately modeled by fixed-scale approaches  [9] .\n\nThe progression of EEG-based emotion recognition encapsulates a diverse methodological landscape  [10] . Traditional techniques leaned heavily on manually engineered features, such as wavelet transforms  [11]    [12] , augmented by neuroscientific priors to distill interpretable patterns. While these methods afford a degree of transparency, their reliance on laborintensive processes and specialized expertise limits scalability, tethering them to a conventional paradigm. In contrast, modern strategies harness deep learning architectures, including convolutional neural networks (CNNs)  [13] , recurrent neural networks (RNNs)  [14] , and Transformers  [6] , to automate feature extraction and elevate performance. Within this advanced framework, features like power spectral density (PSD), relative power spectral density (rPSD), and differential entropy (DE) are commonly integrated  [15] , marrying neuroscientific insight with computational prowess. Yet, despite these advances, significant limitations persist in fully capturing the complexity of emotional dynamics, raising critical questions about the adequacy of current approaches.\n\nDespite these advancements, contemporary EEG-based emotion recognition approaches remain hampered by critical deficiencies that our research addresses with the Multi-scale Spatiotemporal Graph Mamba (MSGM). To bridge these gaps, our MSGM framework integrates a novel graph-based Mamba structure with multi-scale spatiotemporal analysis to comprehensively model the intricate dynamics of EEG signals. Firstly, emotional states manifest temporal dynamics across multiple scales, short-term fluctuations reflecting immediate responses and long-term trends signifying sustained shifts, yet prevailing models rely on fixed temporal windows  [16] , neglecting this multi-granular complexity; MSGM tackles this by unifying multi-scale temporal analysis through a multi-window sliding strategy, extracting rPSD features from seven frequency bands to capture both fleeting shifts and prolonged trends via its Temporal Multi-scale Feature Extraction module.\n\nSecondly, the spatial topology of emotional processing in the brain is inherently distributed and hierarchical  [17] , encompassing both global functional connectivity and localized regional interactions  [18]  [19], a sophistication that singlescale spatial representations fail to encapsulate  [20] ; MSGM addresses this with hierarchical spatial graphs, using neuroanatomical priors to build adaptive global graphs and local subgraphs, fused via multi-depth GCNs and token embeddings through its Spatial Multi-scale Prior Information Initialization and Spatiotemporal Feature Adaptive Fusion modules.\n\nThirdly, the pursuit of high recognition accuracy often incurs substantial computational costs  [21] , hindering real-time deployment on resource-constrained edge devices essential for clinical and consumer applications  [22] . Recently, Mamba has garnered attention for its linear-time sequence modeling via selective state spaces, providing superior efficiency and scalability over Transformers, especially for long sequences  [23] . In EEG-based emotion recognition, initial explorations have shown the efficacy of Mamba's state-space models (SSMs) in handling complex neural dynamics with lower overhead; for example, MS-iMamba uses an inverted structure for spatiotemporal fusion to achieve high benchmark accuracies  [24] , and Global Context MambaVision combines SSMs with contextual modeling to improve efficiency and performance  [25] . Drawing on these foundations, MSGM addresses these challenges through a graph-based Mamba structure, enabling edge efficiency with inference times below 151 ms on the NVIDIA Jetson Xavier NX.\n\nOur MSGM framework advances EEG-based emotion recognition with the following contributions:\n\n1) We propose the MSGM network to address subjectindependent emotion classification, decoding complex EEG emotional dynamics with high precision.\n\n2) We introduce the Temporal Multi-scale Feature Extraction, Spatial Multi-scale Prior Information Initialization, and Spatiotemporal Feature Adaptive Fusion modules to enhance modeling of temporal granularity and spatial connectivity.\n\n3) MSGM delivers superior performance on the SEED  [26] , THU-EP  [27] , and FACED datasets  [28] , surpassing baselines such as DGCNN  [29]  in subject-independent settings. Notably, with only a single MSST-Mamba layer, it outperforms leading methods in the field on the same datasets.\n\n4) Deployed on the NVIDIA Jetson Xavier NX, MSGM delivers real-time inference within 151 ms, enabling efficient performance on resource-constrained edge devices.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Graph Neural Networks In Eeg Analysis",
      "text": "Graph neural networks (GNNs) have become a cornerstone for modeling the spatial topology of electroencephalography (EEG) signals, leveraging their inherent graph structure where channels represent nodes and inter-channel relationships form edges. Spectral GNNs, such as ChebyNet  [30] , employ Chebyshev polynomials to approximate graph Laplacian filters, enabling efficient spatial feature extraction across EEG channels. Similarly, graph convolutional networks (GCNs) with first-order approximations, as in DGCNN, dynamically learn adjacency matrices to capture spatial dependencies, achieving robust emotion recognition performance, demonstrating the feasibility of this direction. RGNN  [31]  further incorporates neuroscientifically inspired constraints into its adjacency matrix, enhancing biological interpretability and achieving an impressive accuracy on SEED, demonstrating remarkable progress in the field.\n\nLikewise, BiDANN  [32]  excellently leverages hemispheric asymmetry for emotion recognition, proving the potential of GNN-based approaches. These methods excel in modeling local spatial patterns, reflecting functional connectivity among brain regions. However, their reliance on static or single-scale adjacency matrices often overlooks the dynamic, hierarchical interactions across the brain's distributed emotional networks  [33] . Moreover, many GNN-based approaches, such as GCB-Net  [34] , utilize averaged features as node attributes, sacrificing critical temporal context essential for capturing emotional dynamics. To address this limitation, adaptive spatial modeling that evolves during training and incorporates multiscale dependencies becomes essential, a challenge our MSGM effectively resolves through its bimodal spatial graph structure, initialized with neuroanatomical priors and dynamically refined.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Multi-Scale Representation Learning",
      "text": "Multi-scale representation learning has emerged as a potent strategy for time series analysis, adept at capturing both finegrained details and overarching trends, with growing applications in EEG signal processing. In domains like time series prediction, methods such as TimesNet  [35]  transform onedimensional sequences into two-dimensional tensors across multiple scales, modeling intra-and inter-periodic variations with convolutional kernels. In EEG emotion recognition, AMCNN-DGCN  [36]  employs multi-scale convolutional blocks to extract temporal features, circumventing manual feature engineering, while Pathformer  [37]  integrates dual attention mechanisms across varying time resolutions to balance local and global dependencies. These approaches demonstrate the power of multi-scale frameworks in addressing the limitations of fixed-scale models, which often fail to represent the diverse temporal dynamics of emotional states.\n\nNotably, Visual-to-EEG Cross-Modal Knowledge Distillation (CKD)  [38]  effectively leverages TCN to extract temporal features from EEG, achieving an impressive RMSE of 0.064 on MAHNOB-HCI, proving the viability of multi-scaleinspired methods. Similarly, DMATN  [39]  excels in crosssubject EEG analysis with multi-source feature extraction, highlighting the promise of this direction. Nevertheless, their application in EEG remains nascent, with most methods either focusing on a single temporal granularity or neglecting spatial integration  [40] . This shortfall restricts their ability to holistically model the brain's multi-scale neural oscillations underlying emotions. Our MSGM advances this paradigm by introducing a multi-window sliding segmentation strategy, capturing both short-term continuity and long-term evolutionary patterns, thereby enhancing temporal granularity within a biologically informed framework.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Spatio-Temporal Fusion In Eeg Emotion Recognition",
      "text": "The fusion of spatial and temporal features has gained traction in EEG emotion recognition, aiming to harness the brain's complex spatiotemporal dynamics. Hybrid architectures, such as Conformer  [41] , combine convolutional neural networks (CNNs) with transformers to integrate shortterm spatial patterns and long-term temporal dependencies, achieving promising classification results. Similarly, ASTDF-Net  [42]  employs dual-stream attention to learn a joint spatiotemporal subspace, while SGCN-LSTM  [43]  hybrids pair graph convolutions with recurrent units to model spatial topology and temporal continuity. Notably, Soleymani's work on continuous emotion detection  [14]  excels by using LSTM-RNN to fuse EEG and facial expression features, achieving a robust Pearson correlation of 0.48 on MAHNOB-HCI  [44] , proving the efficacy of spatiotemporal integration.\n\nLikewise, BiDANN brilliantly combines LSTM with adversarial training to capture hemispheric dynamics, yielding an impressive accuracy on SEED, showcasing the potential of this approach. DMATN further demonstrates excellence by integrating multi-source EEG features, reinforcing the viability of this direction. These methods offer significant advances over isolated spatial or temporal approaches, yet they frequently process these dimensions in parallel branches, limiting interactive feature integration. Moreover, their computational complexity, often driven by attention mechanisms or deep convolutional stacks, renders them impractical for real-time applications on resource-constrained edge devices, which is a critical requirement for clinical and consumer use. Additionally, few models incorporate biologically inspired mechanisms to reflect the brain's hierarchical emotional processing or prioritize efficiency alongside accuracy. Our MSGM addresses these deficiencies with a unified spatiotemporal fusion module that blends multi-depth GCNs with token embeddings for dynamic interaction, while leveraging the efficient Mamba architecture to capture global and local dependencies, ensuring both biological interpretability and practical applicability.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Method",
      "text": "In this section, we present the details of the proposed method, which comprises temporal multi-scale feature extraction, spatial multi-scale prior information initialization, spatiotemporal feature adaptive fusion, MSST-Mamba and classifier. The overall architecture of the proposed method is depicted in Figure  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Temporal Multi-Scale Feature Extraction",
      "text": "To effectively analyze emotional states from EEG signals, a multi-scale feature extraction process is employed. This section details the three key stages: multi-scale temporal segmentation, frequency-domain feature extraction using relative power spectral density (rPSD), and multi-scale feature tensor generation.\n\n1) Multi-Scale Temporal Segmentation: The raw EEG signal, denoted as X ∈ R c×L where c represents the number of channels and L indicates the total number of time samples, is processed through a two-level segmentation method to effectively capture its multi-scale temporal dynamics. In the initial stage, known as first-level segmentation, the signal X is segmented into larger portions X ∈ R c×l by applying a sliding window of length l = 20sec = 20 * f s with a hop step s = 4sec = 4 * f s , where f s represents the sampling frequency of EEG signals, resulting in overlapping segments that encompass wider temporal contexts within the EEG data. Following this, the second-level segmentation takes each of these larger segments X and further divides them into smaller sub-segments using k distinct sliding windows, each characterized by specific lengths l ′ k and hop steps s ′ k for k = 1, 2, 3, . . . , k. This process yields k sets of subsegments Xk ∈ R c×l ′ k , with each set offering a unique temporal resolution of the brain activity contained within the same larger segment.\n\n2) Frequency-Domain Feature Extraction: For each subsegment Xk derived from the k different time window lengths, spectral features are extracted by applying the Fast Fourier Transform (FFT) to each channel. The signal is decomposed into seven frequency bands: delta (1-4 Hz), theta (4-8 Hz), alpha (8-12 Hz), low beta  (12) (13) (14) (15) (16) , beta  (16) (17) (18) (19) (20) , high beta (20-28 Hz), and gamma  (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45) . The relative power spectral density (rPSD) is then computed for each band using Welch's method, yielding a feature matrix F k ∈ R c×f for each sub-segment Xk , where f = 7. Experimental comparisons with alternatives like PSD and differential entropy (DE) confirmed that rPSD provides superior performance in distinguishing emotional states. These rPSD values are later used as node attributes in the graph representation.\n\n3) Multi-Scale Feature Tensor Generation: The rPSD features extracted from the previous step are organized into k distinct feature tensors, each corresponding to one of the temporal scales defined by the window sizes l ′ 1 , l ′ 2 , . . . , l ′ k . For each scale k, the resulting feature tensor is structured as F k ∈ R b×n k ×c×f , where b is the batch size, n k is the number of segments for the k-th window size. This multiscale tensor representation preserves the temporal information at different granularities and provides a comprehensive spatiotemporal characterization of the EEG signals.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Spatial Multi-Scale Prior Information Initialization",
      "text": "This subsection outlines a method for initializing spatial prior information across multiple scales in EEG analysis. The approach involves three key steps: preprocessing and feature preparation to extract relevant EEG features, construction of global and local graphs to model channel interactions, and enhancement of multi-scale spatial priors to improve the representation of connectivity patterns.\n\n1) Preprocessing and Feature Preparation: Using the preprocessed multi-scale feature tensor F k ∈ R b×n k ×c×f , spatial graphs are constructed to represent channel interactions. To establish a consistent graph structure across the batch, we compute the average of F k over the batch dimension, yielding F k ∈ R n k ×c×f . This averaging reduces computational complexity while preserving common spatial patterns within the data. Subsequently, F k is reshaped into a matrix Z ∈ R c×(f n k ) by flattening the sequence and feature dimensions. To adaptively combine features across frequency bands and time segments, a learnable transformation is applied:\n\nwhere W ∈ R (f n k )×n k is a trainable weight matrix initialized using Xavier uniform initialization to ensure stable gradient flow during training, and V ∈ R c×n k is a bias matrix initialized as zeros. This transformation enables the model to learn optimal feature combinations, enhancing its sensitivity to emotional patterns embedded in the EEG signals.\n\n2) Construction of Global and Local Graphs: At each scale k, two graphs are defined: a global graph G G,k = (U, E G,k ) and a local graph G L,k = (U, E L,k ). Both graphs share the same node set U = {u 1 , u 2 , . . . , u c }, where each node u i corresponds to an EEG channel, and the feature vector for node u i , denoted u i,k ∈ R n k , is extracted directly from Z. The global adjacency matrix W G,k ∈ R c×c is constructed using a hybrid metric that integrates the Pearson Correlation Coefficient (PCC) and Manhattan Distance (MD) to eliminate weak or noisy connections while retaining meaningful spatial relationships. The PCC, κ i,j,k , is calculated after normalizing the feature vectors by subtracting their mean and dividing by their standard deviation, with a small constant (1e-6) added to the denominator to avoid division by zero in cases of constant features. The MD is computed as d i,j,k = ∥u i,k -u j,k ∥ 1 , capturing the absolute differences between feature vectors. The weights in W G,k are then defined as:\n\nwhere σ is the Gaussian kernel bandwidth, adaptively set to (µ d + σ d )/2-the average of the mean (µ d ) and standard deviation (σ d ) of Euclidean distances across all node pairs-unless specified otherwise. The thresholds κ θ and d θ are set to the 75th percentile of PCC values and the 25th percentile of MD values, respectively, ensuring data-driven robustness without requiring manual tuning. In contrast, the local adjacency matrix W L,k ∈ R c×c restricts connectivity to channels within predefined scalp regions (see Figure  2 ), defined as:\n\nThis dual-graph strategy effectively encapsulates both extensive inter-channel dependencies and localized interactions, forming a comprehensive spatial prior for EEG analysis.\n\n3) Multi-Scale Spatial Prior Enhancement: To enhance the multi-scale spatial priors, the global and local adjacency matrices W G,k and W L,k at each scale k are duplicated and stacked along a new dimension, resulting in tensors\n\nAlthough the duplicated graphs are identical in this initial setup, this structure provides flexibility for subsequent layers to apply distinct transformations or attention mechanisms, potentially enriching the representation of spatial relationships.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "C. Spatiotemporal Feature Adaptive Fusion",
      "text": "This subsection introduces the spatiotemporal feature adaptive fusion module, which captures dynamic spatial relationships among EEG channels for emotion analysis by integrating multi-depth Graph Convolutional Networks (GCNs) and temporal fusion via token embeddings.\n\n1) Adaptive Graph Encoding with Multi-Depth GCNs: The core of the spatiotemporal feature adaptive fusion module leverages four distinct Graph Encoders, each implemented using ChebyNet, a variant of GCN that employs Chebyshev polynomials to approximate spectral graph convolutions. The ChebyNet formulation is expressed as:\n\nwhere F ∈ R (b•n k )×c×f is the input feature matrix, A ∈ R c×c is the adjacency matrix, σ denotes the ReLU activation function, θ i are learnable parameters, T i represent Chebyshev polynomials, L = -D -1 2 AD -1 2 is the normalized Laplacian with degree matrix D, B is the bias term, and I is the polynomial order controlling the receptive field. This formulation allows the GCNs to capture spatial dependencies efficiently by approximating the graph's spectral properties.\n\nThese four Graph Encoders process the graphs in the following manner. The Shallow Global GCN (Φ G g,shallow ) applies a shallow GCN to the first global graph G 2) Spatiotemporal Fusion Via Token Embeddings: A linear projection layer, LP(•), transforms the flattened input features\n\n, where h is the hidden dimension, providing a nonfiltered representation of the input graph.\n\nThe outputs from the GCNs and the base embedding are combined separately for the global and local graphs at each scale k. The Global Graph Embedding is computed as:\n\nwhere s G,k ∈ R (b•n k )×h . Similarly, the Local Graph Embedding is defined as:\n\nwhere s L,k ∈ R (b•n k )×h . These embeddings are generated for each temporal scale, producing a set of global and local token embeddings {s G,k , s L,k } that encapsulate multi-view spatial representations. These tokens are subsequently passed to the MSST-Mamba, which learns temporal dependencies across scales, effectively integrating both spatial and temporal patterns present in the EEG data.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Msst-Mamba",
      "text": "The MSST-Mamba module constitutes a crucial component within a broader framework designed to capture the multiscale spatiotemporal dynamics of electroencephalogram (EEG) signals for emotion recognition. The MSST-Mamba module processes an input tensor through a stack of m MSSTBlock layers, followed by a final normalization step. Given an input x in ∈ R b×n k ×h reshaped from the preceding spatiotemporal feature adaptive fusion module, the module's computation can be expressed as:\n\nwhere m = 1, 2, . . . , M denotes the number of layers in MSST-Mamba module, and h is the embedding dimension.\n\nEach MSSTBlock m encapsulates a MambaBlock with a residual connection, while RMSNorm ensures numerical stability across layers. The input tensor x in is sequentially transformed through the stack, culminating in a stabilized output x out of the same shape. 1) MSSTBlock: The MSSTBlock, the foundational unit of MSST-Mamba, integrates normalization, temporal modeling, and a residual connection:\n\nwhere RMSNorm and MambaBlock preserve the shape R b×n k ×h . The residual connection ensures robust gradient flow and feature retention across layers.\n\n2) RMSNorm: Root Mean Square Normalization (RM-SNorm) stabilizes training by standardizing the input tensor. For an input z ∈ R b×n k ×h , RMSNorm is defined as:\n\nwhere w ∈ R h is a learnable weight vector initialized to ones, and ϵ = 10 -5 prevents division by zero. This normalization mitigates scale variability, enhancing learning in deep architectures.\n\n3) MambaBlock: The MambaBlock performs temporal modeling within the MSSTBlock. Given a normalized input z norm ∈ R b×n k ×h , it first applies a linear projection to an expanded dimension:\n\nwhere d inner = expand × h. The projected tensor is split into two components, u and res, each of shape R b×n k ×dinner . The u component undergoes a depthwise 1D convolution to capture local temporal dependencies:\n\nwhere the reshape transforms u from (b, n k , d inner ) to (b, d inner , n k ). The convolution uses a kernel size of 4 with padding 3, trimmed to preserve the sequence length n k . The output is reshaped to (b, n k , d inner ) and activated using SiLU:\n\nThe activated tensor u act is then processed by the selective state-space model (SSM), yielding u ssm = SSM(u act ) ∈ R b×n k ×dinner . Meanwhile, the res component is activated separately:\n\nThe SSM output is modulated by the activated residual:\n\nFinally, a linear projection maps the result back to the embedding dimension:\n\n4) Selective State-Space Model (SSM): The SSM within the MambaBlock efficiently models long-range dependencies. Starting with u act ∈ R b×n k ×dinner , it projects to SSM parameters:\n\nwhich splits into δ ∈ R b×n k ×dtrank , B, and C, both in R b×n k ×dstate . The time-step parameter is computed as:\n\nwhere Softplus(x) = ln(1+e x ) ensures positivity. A learnable matrix A ∈ R dinner×dstate , initialized as A = -exp(A log), and a diagonal D ∈ R dinner , initialized to ones, are used. The selective scan updates the hidden state x t and computes the output y t over t = 1, 2, . . . , n k :\n\nwhere v t = u act [:, t, :], ∆ t , B t , and C t are slices at time t, and x 0 = 0 ∈ R b×dinner×dstate . The final output y ∈ R b×n k ×dinner stacks y t and adds a skip connection with D, capturing both short-and long-term dependencies with linear complexity in n k .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "E. Classifier",
      "text": "After processing through the MSST-Mamba module, the global and local x out are mean-pooled along the sequence dimension and L2-normalized, then averaged to produce scalespecific representations, which are fused across all k scales via mean pooling to form a unified embedding x final ∈ R b×h . This embedding captures multi-scale spatiotemporal information from the EEG signals.\n\nThe final classification output, ŷ ∈ R b×dout , where d out represents the number of emotion classes, is generated by a linear classifier applied to the unified embedding:\n\nHere, W ∈ R h×dout and b ∈ R dout are the learnable weights and bias, respectively. This linear layer maps the multi-scale embedding to the logit space, producing logits that can be transformed into a probability distribution over emotion classes using the softmax function during inference or training with a cross-entropy loss.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experiment And Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Datasets",
      "text": "To assess the performance of our proposed model, we conducted comprehensive experiments utilizing five publicly available datasets: the SJTU Emotion EEG Dataset (SEED)  [26] , the Emotion Profiles dataset (THU-EP)  [27] , and its expanded counterpart, the FACED dataset  [28] .\n\nThe SEED dataset, developed by Shanghai Jiao Tong University's BCMI laboratory, contains EEG recordings from 15 native Chinese participants (7 males, 8 females; mean age: 23.27 years). These subjects watched 15 Chinese film clips, each lasting about 4 minutes, selected to evoke three emotional states: positive, neutral, and negative (five clips per category). Following each clip, participants rated their emotions based on valence and arousal. Brain activity was captured using a 62-channel electrode cap configured per the 10-20 system, with signals recorded at a 1000 Hz sampling rate. The data was preprocessed with a 0.3-50 Hz bandpass filter to enhance signal quality for emotion analysis.\n\nThe THU-EP dataset includes EEG data from 80 college students (50 females, 30 males; aged 17-24, mean: 20.16 years) exposed to 28 video clips averaging 67 seconds each. These clips were designed to trigger nine emotions: anger, disgust, fear, sadness, amusement, joy, inspiration, tenderness, and neutral, with four clips for neutral and three for each of the others. The experiment was divided into seven blocks of four trials, with participants solving 20 arithmetic problems between blocks to reset their emotional baseline. After each clip, subjects self-reported scores for arousal, valence, familiarity, and liking. EEG signals were recorded using the NeuSen.W32 wireless system with 32 channels at a 250 Hz sampling rate, preprocessed with a 0.05-47 Hz bandpass filter, and cleaned via independent component analysis (ICA) to remove artifacts.\n\nThe FACED dataset builds on THU-EP, expanding to 123 subjects by adding 43 participants to the original 80, while retaining the same experimental framework. It employs the identical 28 video clips to elicit the nine emotions from THU-EP, following the same seven-block, four-trial structure with arithmetic tasks between blocks. Post-clip self-reports of emotional scores mirror THU-EP's methodology. EEG data was collected with the 32-channel NeuSen.W32 system at 250 Hz, and preprocessing aligns with THU-EP, using a 0.05-47 Hz bandpass filter and ICA for artifact removal. This larger dataset enhances the scope for studying EEG-based emotional responses.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Baseline Methods",
      "text": "This investigation appraises the effectiveness of our methodology in EEG-based emotion recognition. We benchmark it against a suite of recognized baseline approaches, detailed hereafter:\n\n1) DGCNN (graph-based)  [13] : The Dynamical Graph Convolutional Neural Network (DGCNN) dynamically discerns inter-channel EEG relationships via a trainable adjacency matrix, refined throughout the neural network's learning process. This adaptability markedly enhances the extraction of discriminative spatial features, bolstering emotion recognition precision.\n\n2) RGNN (graph-based)  [31] : The Regularized Graph Neural Network (RGNN) leverages neuroscientific insights into brain topology to model local and global EEG channel interactions. By embedding sparsity-inducing regularization within its graph convolutions, RGNN prunes extraneous connections, thereby amplifying emotionally salient features and ensuring robust classification across diverse stimuli.\n\n3) PGCN (graph-based)  [45] : The Pyramidal Graph Convolutional Network (PGCN) constructs a triadic hierarchy-encompassing local electrode clusters, mesoscopic regions (e.g., seven lobes), and global cortex-using sparse adjacency matrices. This hierarchical synthesis mitigates oversmoothing, yielding a precise and interpretable emotional activity map.\n\n4) TSception (CNN-based)  [46] : TSception, a multi-scale convolutional architecture, integrates dynamic temporal, asymmetric spatial, and fusion layers. By concurrently extracting temporal dynamics and spatial asymmetries, it excels in discerning rapid emotional fluctuations across EEG channels.\n\n5) LSTM (temporal-learning)  [47] : Long Short-Term Memory (LSTM) networks , equipped with dual memory cells and gating mechanisms, process 4 Hz EEG sequences to capture long-term temporal dependencies. Such capability proves invaluable for tracking gradual emotional transitions, e.g., neutral to positive valence.\n\n6) TCN (temporal-learning)  [48] : The Temporal Convolutional Network (TCN) employs adjustable dilated convolutions, augmented by visual-to-EEG distillation, to encapsulate extended temporal patterns, outperforming LSTM in multimodal regression tasks.\n\n7) BiDANN (adversarial-based)  [32] : The Bi-Hemisphere Domain Adversarial Neural Network (BiDANN) deploys dualhemisphere LSTM extractors feeding three discriminators, interlinked via Gradient Reversal Layers. This adversarial domain alignment, preserving hemispheric distinctions, ensures robust cross-subject generalization. 8) DMATN (adversarial-based)  [49] : The Deep Multi-Source Adaptation Transfer Network (DMATN) synthesizes multi-source EEG through attention-weighted fusion and an adversarial classifier. By harmonizing diverse inputs, it achieves consistent cross-subject performance. 9) EmT (Graph-Transformer-Based)  [50] : The Emotion Transformer (EmT), a leading method in graph-transformerbased EEG emotion recognition, leverages a graph-transformer architecture to model spatiotemporal dynamics. By converting signals into temporal graphs, its residual multi-view pyramid GCN (RMPG) captures diverse spatial patterns of emotional cognition, while the temporal contextual transformer (TCT) excels at learning long-term dependencies, achieving superior cross-subject generalization in classification and regression tasks.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "C. Experimental Protocol",
      "text": "In this study, we adopt a training strategy consistent with the approach in EmT to implement a subject-independent evaluation framework, ensuring effective generalization to unseen individuals across tailored cross-validation strategies for the SEED, THU-EP, and FACED datasets. For the SEED dataset, which includes data from fifteen experimental subjects, we employ a leave-one-subject-out (LOSO) cross-validation approach, where in each of the fifteen iterations, data from one subject are set aside as the test set, and the remaining fourteen subjects' data are pooled and split randomly into training and validation sets at an 8:2 ratio -80% for training and 20% for validation. As previously noted, we sliced the SEED dataset into time windows of varying lengths to serve as input, resulting in data segments of different sizes. To accommodate this variability, we created multiple dataloaders to feed the network, ensuring consistent labeling across all segments. For the THU-EP and FACED datasets, we use a leave-n-subjectout cross-validation strategy, with n set to 8 for THU-EP and 12 for FACED; in each fold, data from n subjects form the test set, while the remaining subjects' data are divided so that 90% go to training and 10% to validation. Across all three datasets, we classify emotions binarily into positive and negative categories, and for THU-EP and FACED, this involves converting valence scores into high and low categories using a 3.0 threshold. The model is trained on the training set, using the validation set to tune hyperparameters and avoid overfitting, and its performance is assessed on the test set; this process repeats for each fold, with final performance metrics averaged across all iterations.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "D. Parameter Settings",
      "text": "The training configuration of the proposed MSGM model is detailed in Table  II . The model employs cross-entropy loss for optimization, guided by the AdamW optimizer with an initial learning rate of 3 × 10 -4 . To address overfitting, label smoothing (0.1) and dropout (0.25) are applied. A batch size of 32 is used across all datasets, with training epochs set to 20 for SEED and 30 for THU-EP and FACED, incorporating an early stopping mechanism with a patience of 5. The model with the highest validation accuracy is selected for testing.\n\nThe MSST-Mamba architecture, summarized in Table  III , leverages two Chebyshev graph encoders with layers  [1, 2]  to enhance graph processing and handle complex relationships effectively. It employs an embedding dimension h = 32 and a convolutional kernel size of 4 to capture local temporal patterns efficiently. A single MSST-Mamba layer is adopted Hardware configurations are presented in Table  IV . Training and testing leverage an NVIDIA GeForce RTX 3070Ti (8 GB GDDR6), enabling rapid optimization of the model's parameters. For real-world deployment, the NVIDIA Jetson Xavier NX, featuring a 6-core Carmel ARM v8.2 CPU and a Volta GPU with 48 Tensor Cores (up to 21 TOPS, INT8), offers lowpower (10-20 W) and high-efficiency inference, supported by 8 GB LPDDR4x memory and 51.2 GB/s ideal for edge computing applications.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Numerical Results",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Emotion Recognition Performance",
      "text": "The experimental results are presented in Table  I , which evaluates the performance of various methods for generalized emotion classification across three datasets-SEED, THU-EP, and FACED-using accuracy (ACC %) and F1 score (F1 %) as metrics. On the SEED dataset, our proposed method achieves an outstanding accuracy of 83.43% and an F1 score of 85.03%, outperforming all other approaches. The next best performers are EmT with an accuracy of 80.20% and an F1 score of 82.10% (second-highest accuracy and F1 score), while traditional methods like KNN and SVM trail far behind with accuracies of 49.26% and 51.68%, and F1 scores of 48.89% and 50.31%, respectively. For the THU-EP dataset, our method continues to lead with the highest accuracy of 62.39% and F1 score of 73.28%, followed closely by BiDANN at 61.44% accuracy and EmT at 72.40% F1 score. On the FACED dataset, BiDANN achieves a slightly higher accuracy of 63.36% compared to our method's 63.17%. However, our method outperforms BiDANN in terms of F1 score, achieving 76.01% against BiDANN's 73.82%, making our model the top performer in F1 score. Across all three datasets, our proposed method consistently delivers superior performance, particularly excelling in accuracy, thereby affirming its robustness and effectiveness in emotion classification tasks. Advanced architectures such as EmT and BiDANN also demonstrate strong capabilities; for instance, EmT achieves the second-best accuracy on SEED and competitive results on THU-EP and FACED, while BiDANN stands out with the second-highest accuracy on THU-EP. Methods leveraging temporal dynamics, such as TSception (66.22% ACC on SEED) and TCN (76.54% ACC on SEED), generally outperform those relying solely on spatial features, highlighting the critical role of temporal information in EEGbased emotion recognition. The substantial performance gap between these advanced methods and traditional approaches like KNN and SVM underscores the limitations of simpler models in this complex domain.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Ablation Study On Component Modules",
      "text": "To evaluate the contributions of the temporal multi-scale feature extraction, spatial multi-scale prior information initialization, spatiotemporal feature adaptive fusion, and MSST-Mamba and classifier modules, we conducted an ablation analysis by systematically removing each component individually and assessing its impact on classification performance. This included omitting the temporal multi-scale feature extraction (w/o Temporal Multi-Scale), spatial multi-scale prior information initialization (w/o Spatial Multi-Scale), spatiotemporal feature adaptive fusion (w/o Spatiotemporal Fusion), and MSST-Mamba (w/o MSST-Mamba), as well as replacing multi-depth GCNs with a single layer (w Single GCN), to measure each component's effect. The results are detailed in Table  V .\n\nThe removal of the MSST-Mamba and classifier module results in the most significant performance decline, with accuracy decreasing by 3.90% on the SEED dataset and 4.47% on the THU-EP dataset, alongside F1 score drops of 7.10% and 8.24%, respectively. This underscores its critical role in processing and integrating multi-scale spatiotemporal features effectively. Excluding the spatiotemporal feature adaptive fusion module also leads to substantial reductions, with accuracy dropping by 3.68% on SEED and 3.22% on THU-EP, highlighting its importance in unifying temporal and spatial information.\n\nThe absence of the temporal multi-scale feature extraction module decreases accuracy by 3.39% on SEED and 2.84% on THU-EP, indicating its value in capturing diverse temporal dynamics. Removing the spatial multi-scale prior information initialization module results in smaller but notable declines of 1.06% on SEED and 0.57% on THU-EP, suggesting its contribution to initializing robust spatial representations, though its impact is less pronounced than other modules. Additionally, using a single GCN instead of multiple GCN layers reduces accuracy by 1.51% on SEED and 0.38% on THU-EP, demonstrating that multi-layer GCNs more effectively capture spatial information.\n\nC. Performance and Sensitive Analysis of Hyperparameters  to 79.77% and 80.38%, respectively. This pattern suggests that a single block achieves the best performance, with additional blocks leading to fluctuations and an overall decline at higher counts.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "3) Effect Of Prior Information On Brain Region Partitioning:",
      "text": "The human brain comprises multiple functional regions, each contributing uniquely to emotional processing  [51] . The way these regions are partitioned into subgraphs can influence the structure of the EEG data representation and, consequently, the model's performance  [52] . To explore this, we conducted experiments on the SEED dataset by dividing the 62 EEG channels into 7, 10, and 17 regions, as shown in Figure  4 . Our results indicate (see Figure  3c ) that the 7-region partitioning yields the highest accuracy (83.43%) and F1 score (85.03%), followed by the 10-region partitioning with an accuracy of 82.79% and an F1 score of 83.56%, while the 17-region partitioning produces the lowest accuracy (81.27%) and F1 score (82.28%). These findings suggest that the 7-region scheme may strike an effective balance between capturing essential functional patterns and maintaining a manageable level of complexity for the model. In contrast, the finer 17region partitioning might overly fragment the data, diluting key inter-regional relationships, while the 10-region approach, despite performing better than 17 regions, may still not align as optimally with the underlying functional organization as the 7-region configuration.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Performance On Edge Devices",
      "text": "The MSGM model, deployed on the NVIDIA Jetson Xavier NX edge computing platform, as shown in Figure  6 , exhibits efficient performance on the SEED dataset. To enable deployment on this platform, we replaced the Mamba core component in the MSST-Mamba module with Mamba-minimal 1 , a lightweight implementation of Mamba, since the PyTorch version on the edge device does not support the official Mamba library. This substitution preserves the model's input-output functionality but results in lower runtime efficiency compared 1 https://github.com/johnma2006/mamba-minimal to the official Mamba implementation. With this configuration, the model utilizes 349,218 parameters and achieves an inference time of 151.0 ms, maintaining millisecond-level inference and demonstrating robust real-time processing capabilities. This efficiency underscores its suitability for edge device applications requiring rapid data handling.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "E. Comparison With Emt",
      "text": "In this section, we compare our MSGM model with EmT, a leading method in graph-transformer-based EEG emotion recognition. Both models adopt a graph-Transformer/Mambabased architecture to process spatial-temporal patterns in EEG signals. EmT incorporates an 8-layer TCT module, while MSGM employs a single-layer MSST-Mamba module. We evaluate their performance in terms of accuracy, parameter count, and inference time. To ensure a fair comparison, this evaluation was conducted on the GeForce RTX 3070Ti platform (see Table  IV ), rather than on edge devices, allowing both MSGM and EmT to run in a consistent environment without the influence of Mamba-minimal, which was used for edge deployment.\n\nMSGM, with its single-layer MSST-Mamba, achieves superior accuracy and F1 scores compared to EmT, despite using only 349,218 parameters-approximately half of EmT's 703,530. This highlights MSGM's efficiency, as its linear-complexity MSST-Mamba outperforms the quadraticcomplexity TCT module with a simpler structure. The reduced parameter count underscores MSGM's suitability for resourceconstrained settings, such as edge devices.\n\nIn terms of inference time, MSGM records 7.9 ms, slightly higher than EmT's 4.3 ms. This minor gap arises from MSGM's multi-scale architecture, which limits full parallelization. Nevertheless, both models maintain millisecond-level inference, ensuring negligible impact on real-time applications.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "F. Visualization",
      "text": "Figure  5  presents two diagrams that illustrate the connectivity between different electrodes, derived from the initial and learned perspectives, respectively. We utilized a Local Graph derived from the Global Graph as the representation, which effectively reflects the model's learning outcomes. These diagrams highlight the evolution of connectivity patterns identified during the training process, based on data from the EEG dataset.\n\nIn the initial connectivity map (Figure  5a ), the strongest connections are observed between electrodes such as C1-Pz, FC2-FPz, and C6-CP4. These connections primarily involve the central and parietal regions, with some involvement of the frontal areas  [53] , suggesting a baseline interaction that may reflect general neural communication prior to task-specific learning. The prominence of these connections indicates an initial focus on central-parietal and frontal-central interactions, which are often associated with sensory and motor coordination in early-stage processing.\n\nIn contrast, the learned connectivity map (Figure  5b ) reveals a more refined set of connections, with the strongest links being C6-CP4 and C1-Pz. These retained and strengthened connections continue to emphasize interactions within the central and parietal regions, which are known to play critical roles in sensory integration and spatial processing. The persistence of these specific connections suggests that the model has prioritized and enhanced these pathways, likely due to their relevance to the task at hand  [54] . Additionally, the color intensity, ranging from 0.6 to 1.0, highlights the varying strengths of these learned connections, with warmer colors indicating stronger interactions.\n\nThe connectivity patterns observed in Figure  5  demonstrate the model's ability to refine and focus on key electrode relationships, transitioning from a broader initial state to a more targeted, task-driven network. This evolution underscores the model's effectiveness in capturing and enhancing critical neural relationships, particularly in the central and parietal regions, tailored to the cognitive demands of the classification task.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we propose the Multi-Scale Spatiotemporal Graph Mamba (MSGM), a novel framework for EEG-based emotion recognition that integrates temporal multi-scale feature extraction, spatial multi-scale prior information initialization, spatiotemporal feature adaptive fusion, and the MSST-Mamba module. By capturing short-term emotional continuity and long-term evolutionary trends through multi-scale temporal analysis, alongside hierarchical spatial connectivity via bimodal graph modeling, MSGM addresses critical gaps in prior methodologies. Extensive experiments on the SEED, THU-EP, and FACED datasets demonstrate its superior performance over baseline methods, validated through rigorous subjectindependent evaluation. The model achieves millisecond-level inference speed on edge devices like the NVIDIA Jetson Xavier NX, underscoring its practical applicability in clinical and consumer settings, while its neuroanatomical grounding enhances interpretability of the brain's distributed emotional dynamics. Nevertheless, challenges remain in achieving optimal cross-subject generalization, constrained by variability in EEG data across individuals. Looking ahead, future developments in EEG-based emotion recognition could focus on integrating multimodal physiological signals-such as ECG or eye-tracking data-to enrich emotional context, optimizing lightweight architectures for broader deployment on wearable devices, and exploring real-time adaptive learning to dynamically adjust to individual neurophysiological profiles, thereby advancing both precision and accessibility in realworld applications.",
      "page_start": 12,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A. Temporal Multi-Scale Feature Extraction",
      "page": 3
    },
    {
      "caption": "Figure 1: The framework of MSGM. The multi-scale feature tensors from Temporal Multi-Scale Feature Extraction module are",
      "page": 4
    },
    {
      "caption": "Figure 2: The division method of 62-channel and 32-channel",
      "page": 5
    },
    {
      "caption": "Figure 3: (a) Effect of feature types on emotion classification performances using SEED. (b) Effect of the number of MSST",
      "page": 10
    },
    {
      "caption": "Figure 4: Three methods for dividing 62 EEG channels into dif-",
      "page": 10
    },
    {
      "caption": "Figure 3: c) that the 7-region partitioning",
      "page": 10
    },
    {
      "caption": "Figure 5: (a) Connectivity between the electrodes of the initial",
      "page": 11
    },
    {
      "caption": "Figure 6: , exhibits",
      "page": 11
    },
    {
      "caption": "Figure 6: NVIDIA Jetson Xavier NX.",
      "page": 11
    },
    {
      "caption": "Figure 5: presents two diagrams that illustrate the connec-",
      "page": 11
    },
    {
      "caption": "Figure 5: a), the strongest",
      "page": 12
    },
    {
      "caption": "Figure 5: b) reveals",
      "page": 12
    },
    {
      "caption": "Figure 5: demonstrate",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "Abstract—EEG-based emotion recognition struggles with cap-",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "expression analysis or speech intonation, EEG offers immedi-"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "turing multi-scale\nspatiotemporal dynamics and ensuring com-",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "ate access to the brain’s dynamic responses, making it\nideally"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "putational efficiency for real-time applications. Existing methods",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "suited for applications requiring precision and responsiveness."
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "often oversimplify temporal granularity and spatial hierarchies,",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "However,\nthe\nefficacy of EEG-based emotion recognition is"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "limiting accuracy. To overcome these challenges, we propose the",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "Multi-Scale\nSpatiotemporal Graph Mamba\n(MSGM),\na\nnovel",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "tempered\nby\nsignificant\nhurdles:\nthe\nsignals’\nsusceptibility"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "framework integrating multi-window temporal segmentation, bi-",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "to noise,\ntheir\nspatial heterogeneity across brain regions\n[8],"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "modal spatial graph modeling, and efficient fusion via the Mamba",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "and their complex temporal dynamics, which span short-term"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "architecture. By segmenting EEG signals across diverse temporal",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "fluctuations and long-term trends yet are often inadequately"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "scales and constructing global-local graphs with neuroanatom-",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "modeled by fixed-scale approaches [9]."
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "ical priors, MSGM effectively\ncaptures fine-grained emotional",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "fluctuations and hierarchical brain connectivity. A multi-depth",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "The progression of EEG-based emotion recognition encap-"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "Graph Convolutional Network\n(GCN)\nand\ntoken\nembedding",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "sulates\na diverse methodological\nlandscape\n[10]. Traditional"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "fusion module, paired with Mamba’s state-space modeling, enable",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "techniques\nleaned heavily on manually engineered features,"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "dynamic spatiotemporal interaction at linear complexity. Notably,",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "such as wavelet\ntransforms [11] [12], augmented by neurosci-"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "with\njust\none MSST-Mamba\nlayer, MSGM surpasses\nleading",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "entific priors to distill interpretable patterns. While these meth-"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "methods\nin\nthe\nfield\non\nthe\nSEED,\nTHU-EP,\nand\nFACED",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "datasets, outperforming baselines in subject-independent emotion",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "ods afford a degree of\ntransparency,\ntheir\nreliance on labor-"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "classification while achieving robust accuracy and millisecond-",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "intensive processes and specialized expertise limits scalability,"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "level\ninference on the NVIDIA Jetson Xavier NX.",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "tethering them to a conventional paradigm.\nIn contrast, mod-"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "Index Terms—Electroencephalogram (EEG), emotion recogni-",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "ern strategies harness deep learning architectures,\nincluding"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "tion, multi-scale, graph neural networks, Mamba",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "convolutional neural networks\n(CNNs)\n[13],\nrecurrent neural"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "networks (RNNs) [14], and Transformers [6],\nto automate fea-"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "ture extraction and elevate performance. Within this advanced"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "I.\nINTRODUCTION",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "framework, features like power spectral density (PSD), relative"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "has\nemerged\nas\na\ncritical\nre-",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "power\nspectral density (rPSD), and differential entropy (DE)"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "search frontier with far-reaching implications for human-\nE MOTION recognition",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "are commonly integrated [15], marrying neuroscientific insight"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "computer interaction, mental health monitoring, and neurosci-",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "with\ncomputational\nprowess. Yet,\ndespite\nthese\nadvances,"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "entific exploration [1] [2] [3]. The ability to decode emotional",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "significant\nlimitations persist\nin fully capturing the complexity"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "states in real-time promises to revolutionize intelligent systems",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "of\nemotional\ndynamics,\nraising\ncritical\nquestions\nabout\nthe"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "by enhancing user adaptability and bolstering clinical applica-",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "adequacy of current approaches."
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "tions\nthrough early detection and management of emotional",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "Despite\nthese\nadvancements,\ncontemporary\nEEG-based"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "disorders\n[4]\n[5]. As\nthese\ncapabilities become\nincreasingly",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "emotion recognition approaches\nremain hampered by critical"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "vital\nin healthcare and artificial\nintelligence,\nthere is an urgent",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "deficiencies\nthat our\nresearch addresses with the Multi-scale"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "need for\nrobust, efficient, and neurophysiologically grounded",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "Spatiotemporal Graph Mamba (MSGM). To bridge these gaps,"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "approaches\nto\novercome\nboth\ntheoretical\ncomplexities\nand",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "our MSGM framework integrates a novel graph-based Mamba"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "practical deployment challenges [6].",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "structure with multi-scale spatiotemporal analysis to compre-"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "Electroencephalography\n(EEG)\nstands\nout\nas\na\npremier",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "hensively model the intricate dynamics of EEG signals. Firstly,"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "modality for\nemotion recognition, owing to its unparalleled",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "emotional states manifest\ntemporal dynamics across multiple"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "capacity\nto\nnon-invasively\nrecord\nbrain\nactivity with\nhigh",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "scales, short-term fluctuations reflecting immediate responses"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "temporal resolution, directly capturing the neural signatures of",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "and long-term trends signifying sustained shifts, yet prevailing"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "emotional processes [7]. Unlike indirect proxies such as facial",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "models\nrely on fixed temporal windows\n[16], neglecting this"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "multi-granular\ncomplexity; MSGM tackles\nthis by unifying"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "# Contributed equally to this work.",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "multi-scale temporal analysis through a multi-window sliding"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "Hanwen Liu\nand Yifeng Gong\nare with\nthe School\nof Electronics\nand",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "strategy, extracting rPSD features from seven frequency bands"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "Communication Engineering,\nSun Yat-sen University,\nShenzhen,\n518107,",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "China, e-mail:\n(liuhw56, gongyf9)@mail2.sysu.edu.cn.",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "to capture both fleeting shifts\nand prolonged trends via\nits"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "Zuwei Yan\nis with\nthe College\nof Communication Engineering,\nJilin",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "Temporal Multi-scale Feature Extraction module."
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "University, Changchun, 130012, China, e-mail: yanzw2422@mails.jlu.edu.cn.",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "Secondly,\nthe spatial\ntopology of emotional processing in"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "Zeheng\nZhuang\nis\nwith\nthe\nSchool\nof\nPharmaceutical\nSciences",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "(Shenzhen),\nSun Yat-sen University,\nShenzhen,\n518107,\nChina,\ne-mail:",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "the brain is\ninherently distributed and hierarchical\n[17],\nen-"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "zhuangzh23@mail2.sysu.edu.cn.",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "compassing both global\nfunctional connectivity and localized"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "Jiaxuan Lu is with Shanghai Artificial\nIntelligence Laboratory, Shanghai,",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "regional\ninteractions\n[18]\n[19],\na\nsophistication that\nsingle-"
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "200232, China, e-mail:\nlujiaxuan@pjlab.org.cn",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": ""
        },
        {
          "Hanwen Liu#\n, Yifeng Gong#\n, Zuwei Yan#": "* Corresponding Author.",
          ", Zeheng Zhuang#\n, Jiaxuan Lu*": "scale spatial\nrepresentations\nfail\nto encapsulate [20]; MSGM"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "addresses\nthis with\nhierarchical\nspatial\ngraphs,\nusing\nneu-",
          "2": "achieving\nan\nimpressive\naccuracy\non SEED,\ndemonstrating"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "roanatomical priors to build adaptive global graphs and local",
          "2": "remarkable progress in the field."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "subgraphs, fused via multi-depth GCNs and token embeddings",
          "2": "Likewise, BiDANN [32] excellently leverages hemispheric"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "through its Spatial Multi-scale Prior Information Initialization",
          "2": "asymmetry for emotion recognition, proving the potential of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and Spatiotemporal Feature Adaptive Fusion modules.",
          "2": "GNN-based\napproaches. These methods\nexcel\nin modeling"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Thirdly,\nthe\npursuit\nof\nhigh\nrecognition\naccuracy\noften",
          "2": "local spatial patterns, reflecting functional connectivity among"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "incurs substantial computational costs [21], hindering real-time",
          "2": "brain regions. However,\ntheir reliance on static or single-scale"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "deployment on resource-constrained edge devices essential for",
          "2": "adjacency matrices often overlooks the dynamic, hierarchical"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "clinical\nand\nconsumer\napplications\n[22]. Recently, Mamba",
          "2": "interactions across the brain’s distributed emotional networks"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "has garnered attention for\nits\nlinear-time sequence modeling",
          "2": "[33]. Moreover, many GNN-based approaches, such as GCB-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "via\nselective\nstate\nspaces, providing superior\nefficiency and",
          "2": "Net\n[34], utilize\naveraged features\nas node\nattributes,\nsacri-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "scalability over Transformers,\nespecially for\nlong sequences",
          "2": "ficing critical\ntemporal\ncontext\nessential\nfor\ncapturing emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[23].\nIn EEG-based emotion recognition,\ninitial explorations",
          "2": "tional dynamics. To address\nthis\nlimitation,\nadaptive\nspatial"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "have\nshown\nthe\nefficacy\nof Mamba’s\nstate-space models",
          "2": "modeling that evolves during training and incorporates multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(SSMs) in handling complex neural dynamics with lower over-",
          "2": "scale dependencies becomes essential, a challenge our MSGM"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "head; for example, MS-iMamba uses an inverted structure for",
          "2": "effectively resolves\nthrough its bimodal\nspatial graph struc-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "spatiotemporal\nfusion to achieve high benchmark accuracies",
          "2": "ture,\ninitialized with neuroanatomical priors and dynamically"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[24], and Global Context MambaVision combines SSMs with",
          "2": "refined."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "contextual modeling to improve\nefficiency and performance",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[25]. Drawing on these foundations, MSGM addresses\nthese",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "B. Multi-Scale Representation Learning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "challenges through a graph-based Mamba structure, enabling",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "edge\nefficiency with inference\ntimes below 151 ms on the",
          "2": "Multi-scale representation learning has emerged as a potent"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "NVIDIA Jetson Xavier NX.",
          "2": "strategy for\ntime series analysis, adept at capturing both fine-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Our MSGM framework\nadvances\nEEG-based\nemotion",
          "2": "grained details and overarching trends, with growing applica-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition with the following contributions:",
          "2": "tions\nin EEG signal processing.\nIn domains\nlike time series"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1) We\npropose\nthe MSGM network\nto\naddress\nsubject-",
          "2": "prediction, methods\nsuch\nas TimesNet\n[35]\ntransform one-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "independent\nemotion\nclassification,\ndecoding\ncomplex EEG",
          "2": "dimensional\nsequences\ninto\ntwo-dimensional\ntensors\nacross"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotional dynamics with high precision.",
          "2": "multiple\nscales, modeling\nintra-\nand\ninter-periodic\nvaria-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2) We introduce the Temporal Multi-scale Feature Extrac-",
          "2": "tions with\nconvolutional\nkernels.\nIn EEG emotion\nrecogni-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion, Spatial Multi-scale Prior\nInformation Initialization, and",
          "2": "tion, AMCNN-DGCN [36] employs multi-scale convolutional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Spatiotemporal Feature Adaptive Fusion modules to enhance",
          "2": "blocks\nto\nextract\ntemporal\nfeatures,\ncircumventing manual"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "modeling of\ntemporal granularity and spatial connectivity.",
          "2": "feature engineering, while Pathformer\n[37]\nintegrates dual at-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "3) MSGM delivers superior performance on the SEED [26],",
          "2": "tention mechanisms across varying time resolutions to balance"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "THU-EP [27], and FACED datasets [28], surpassing baselines",
          "2": "local and global dependencies. These approaches demonstrate"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "such as DGCNN [29] in subject-independent settings. Notably,",
          "2": "the power of multi-scale frameworks in addressing the limita-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with only a single MSST-Mamba layer,\nit outperforms leading",
          "2": "tions of fixed-scale models, which often fail\nto represent\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "methods in the field on the same datasets.",
          "2": "diverse temporal dynamics of emotional states."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "4) Deployed on the NVIDIA Jetson Xavier NX, MSGM",
          "2": "Notably, Visual-to-EEG Cross-Modal Knowledge Distilla-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "delivers real-time inference within 151 ms, enabling efficient",
          "2": "tion (CKD)\n[38]\neffectively leverages TCN to extract\ntem-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "performance on resource-constrained edge devices.",
          "2": "poral\nfeatures\nfrom EEG, achieving an impressive RMSE of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "0.064 on MAHNOB-HCI, proving the viability of multi-scale-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "inspired methods. Similarly, DMATN [39]\nexcels\nin\ncross-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "II. RELATED WORKS",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "subject EEG analysis with multi-source\nfeature\nextraction,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A. Graph Neural Networks in EEG Analysis",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "highlighting the promise of\nthis direction. Nevertheless,\ntheir"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Graph neural networks (GNNs) have become a cornerstone",
          "2": "application in EEG remains nascent, with most methods ei-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for modeling\nthe\nspatial\ntopology\nof\nelectroencephalogra-",
          "2": "ther\nfocusing on a single temporal granularity or neglecting"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "phy (EEG)\nsignals,\nleveraging their\ninherent graph structure",
          "2": "spatial\nintegration [40]. This shortfall\nrestricts their ability to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "where channels represent nodes and inter-channel relationships",
          "2": "holistically model\nthe brain’s multi-scale neural oscillations"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "form edges. Spectral GNNs, such as ChebyNet\n[30], employ",
          "2": "underlying\nemotions. Our MSGM advances\nthis\nparadigm"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Chebyshev polynomials\nto approximate graph Laplacian fil-",
          "2": "by introducing a multi-window sliding segmentation strategy,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ters, enabling efficient\nspatial\nfeature extraction across EEG",
          "2": "capturing both short-term continuity and long-term evolution-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "channels.\nSimilarly,\ngraph\nconvolutional\nnetworks\n(GCNs)",
          "2": "ary patterns,\nthereby enhancing temporal granularity within a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with first-order approximations, as\nin DGCNN, dynamically",
          "2": "biologically informed framework."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "learn\nadjacency matrices\nto\ncapture\nspatial\ndependencies,",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "achieving\nrobust\nemotion\nrecognition\nperformance,\ndemon-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "C.\nSpatio-Temporal Fusion in EEG Emotion Recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "strating the\nfeasibility of\nthis direction. RGNN [31]\nfurther",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "incorporates\nneuroscientifically\ninspired\nconstraints\ninto\nits",
          "2": "The\nfusion\nof\nspatial\nand\ntemporal\nfeatures\nhas\ngained"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "adjacency matrix,\nenhancing\nbiological\ninterpretability\nand",
          "2": "traction in EEG emotion recognition,\naiming to harness\nthe"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "brain’s\ncomplex\nspatiotemporal\ndynamics. Hybrid\narchitec-",
          "3": "a\nsliding window of\nlength\nl = 20sec = 20 ∗ fs with"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tures,\nsuch as Conformer\n[41],\ncombine\nconvolutional neu-",
          "3": "a\nhop\nstep\nrepresents\nthe\ns = 4sec = 4 ∗ fs, where\nfs"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ral\nnetworks\n(CNNs) with\ntransformers\nto\nintegrate\nshort-",
          "3": "sampling frequency of EEG signals,\nresulting in overlapping"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "term spatial\npatterns\nand\nlong-term temporal\ndependencies,",
          "3": "segments that encompass wider\ntemporal contexts within the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "achieving promising classification results. Similarly, ASTDF-",
          "3": "EEG data. Following this,\nthe second-level segmentation takes"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Net\n[42] employs dual-stream attention to learn a joint\nspa-",
          "3": "further\ndivides\nthem\neach\nof\nthese\nlarger\nsegments X and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tiotemporal\nsubspace, while SGCN-LSTM [43] hybrids pair",
          "3": "into smaller\nsub-segments using k distinct\nsliding windows,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "graph convolutions with recurrent units to model spatial topol-",
          "3": "l′\ns′\neach characterized by specific\nlengths\nand hop steps"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "k\nk"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ogy and temporal continuity. Notably, Soleymani’s work on",
          "3": "k\nfor\nk = 1, 2, 3, . . . , k. This\nprocess\nyields\nsets\nof\nsub-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "˜\nRc×l′"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "continuous\nemotion\ndetection\n[14]\nexcels\nby\nusing LSTM-",
          "3": "∈\nsegments\nk , with\neach\nset\noffering\na\nunique\nXk"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "RNN to fuse EEG and facial expression features, achieving",
          "3": "temporal\nresolution of\nthe brain activity contained within the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "a robust Pearson correlation of 0.48 on MAHNOB-HCI\n[44],",
          "3": "same larger segment."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "proving the efficacy of spatiotemporal\nintegration.",
          "3": "2) Frequency-Domain Feature Extraction:\nFor\neach sub-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Likewise, BiDANN brilliantly\ncombines LSTM with\nad-",
          "3": "˜"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "segment\nXk derived from the k different time window lengths,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "versarial\ntraining to capture hemispheric dynamics, yielding",
          "3": "spectral\nfeatures\nare\nextracted by applying the Fast Fourier"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "an impressive\naccuracy on SEED,\nshowcasing the potential",
          "3": "Transform (FFT)\nto each channel. The signal\nis decomposed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of this approach. DMATN further demonstrates excellence by",
          "3": "into seven frequency bands: delta\n(1-4 Hz),\ntheta\n(4-8 Hz),"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "integrating multi-source EEG features, reinforcing the viability",
          "3": "alpha (8-12 Hz),\nlow beta (12-16 Hz), beta (16-20 Hz), high"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of\nthis\ndirection. These methods\noffer\nsignificant\nadvances",
          "3": "beta (20-28 Hz), and gamma (30-45 Hz). The relative power"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "over\nisolated\nspatial\nor\ntemporal\napproaches,\nyet\nthey\nfre-",
          "3": "spectral density (rPSD) is then computed for each band using"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "quently process these dimensions in parallel branches, limiting",
          "3": "Welch’s method, yielding a feature matrix Fk ∈ Rc×f for each"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "interactive feature integration. Moreover,\ntheir computational",
          "3": "˜"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "sub-segment\nXk, where f = 7. Experimental comparisons with"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "complexity,\noften\ndriven\nby\nattention mechanisms\nor\ndeep",
          "3": "alternatives like PSD and differential entropy (DE) confirmed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "convolutional\nstacks,\nrenders\nthem impractical\nfor\nreal-time",
          "3": "that\nrPSD provides\nsuperior\nperformance\nin\ndistinguishing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "applications on resource-constrained edge devices, which is a",
          "3": "emotional\nstates. These\nrPSD values\nare\nlater used as node"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "critical\nrequirement\nfor clinical and consumer use. Addition-",
          "3": "attributes in the graph representation."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ally, few models incorporate biologically inspired mechanisms",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "3) Multi-Scale Feature Tensor Generation: The rPSD fea-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to\nreflect\nthe\nbrain’s\nhierarchical\nemotional\nprocessing\nor",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "tures\nextracted from the previous\nstep are organized into k"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "prioritize efficiency alongside accuracy. Our MSGM addresses",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "distinct\nfeature\ntensors,\neach\ncorresponding\nto\none\nof\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "these deficiencies with a unified spatiotemporal fusion module",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "l′\ntemporal\nscales\ndefined\nby\nthe window sizes\n1, l′\n2, . . . , l′"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "k."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "that\nblends multi-depth GCNs with\ntoken\nembeddings\nfor",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "For\neach\nscale k,\nthe\nresulting\nfeature\ntensor\nis\nstructured"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dynamic\ninteraction, while\nleveraging\nthe\nefficient Mamba",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "b\nis\nis\nthe\nas Fk ∈ Rb×nk×c×f , where\nthe batch size, nk"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "architecture to capture global and local dependencies, ensuring",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "number of\nsegments\nfor\nthe k-th window size. This multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "both biological\ninterpretability and practical applicability.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "scale tensor representation preserves the temporal\ninformation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "at different granularities and provides a comprehensive spatio-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "III. METHOD",
          "3": "temporal characterization of\nthe EEG signals."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In\nthis\nsection, we\npresent\nthe\ndetails\nof\nthe\nproposed",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "method, which\ncomprises\ntemporal multi-scale\nfeature\nex-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "traction,\nspatial multi-scale\nprior\ninformation\ninitialization,",
          "3": "B.\nSpatial Multi-Scale Prior Information Initialization"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "spatiotemporal\nfeature\nadaptive\nfusion, MSST-Mamba\nand",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "This\nsubsection outlines\na method for\ninitializing spatial"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "classifier. The overall architecture of\nthe proposed method is",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "prior information across multiple scales in EEG analysis. The"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "depicted in Figure 1.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "approach involves\nthree key steps: preprocessing and feature"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "preparation\nto\nextract\nrelevant\nEEG features,\nconstruction"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A. Temporal Multi-Scale Feature Extraction",
          "3": "of\nglobal\nand\nlocal\ngraphs\nto model\nchannel\ninteractions,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "To effectively analyze emotional\nstates\nfrom EEG signals,",
          "3": "and enhancement of multi-scale spatial priors to improve the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "a multi-scale\nfeature\nextraction\nprocess\nis\nemployed. This",
          "3": "representation of connectivity patterns."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "section details the three key stages: multi-scale temporal seg-",
          "3": "1) Preprocessing and Feature Preparation: Using the pre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mentation,\nfrequency-domain feature extraction using relative",
          "3": "processed multi-scale feature tensor Fk ∈ Rb×nk×c×f , spatial"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "power spectral density (rPSD), and multi-scale feature tensor",
          "3": "graphs\nare\nconstructed to represent\nchannel\ninteractions. To"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "generation.",
          "3": "establish\na\nconsistent\ngraph\nstructure\nacross\nthe\nbatch, we"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1) Multi-Scale Temporal Segmentation: The raw EEG sig-",
          "3": "the batch dimension, yield-\ncompute the average of Fk over"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "nal, denoted as X ∈ Rc×L where c represents the number of",
          "3": "averaging reduces\ncomputational\ning F k ∈ Rnk×c×f . This"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "channels and L indicates\nthe total number of\ntime samples,",
          "3": "complexity while preserving common spatial patterns within"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "is\nprocessed\nthrough\na\ntwo-level\nsegmentation method\nto",
          "3": "is\nreshaped into a matrix Z ∈\nthe data. Subsequently, F k"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "effectively capture its multi-scale temporal dynamics.\nIn the",
          "3": "Rc×(f nk) by flattening the sequence and feature dimensions."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "initial\nstage, known as first-level\nsegmentation,\nthe signal X",
          "3": "To adaptively combine\nfeatures\nacross\nfrequency bands\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "by\napplying\nis\nsegmented\ninto\nlarger\nportions X ∈ Rc×l",
          "3": "time segments, a learnable transformation is applied:"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0\n1",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Classifier",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Label:",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Linear",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "I",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "MSST-Mamba",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "RMSNorm",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "m",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "MSSTBlock",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Reshape",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Spatiotemporal",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Feature Adaptive",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Fusion",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Linear\nDG/SG GCN",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "𝐺𝐺,𝑘",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Spatial Multi-Scale Prior",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Information Initialization",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ℱ1 ∈ ℝ𝑛1(cid:3400)𝑐(cid:3400)𝑓",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Extraction",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Ch1",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Ch2",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "𝑋\n(cid:3364) ∈ ℝ𝑐(cid:3400)𝑙",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "…",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Ch(n)",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Temporal Multi-Scale",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Feature Extraction",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "′  \n′  \n𝑙1\n𝑙2",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Fig. 1: The framework of MSGM. The multi-scale feature tensors from Temporal Multi-Scale Feature Extraction module are",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "used as the input",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "local graphs. Then Spatiotemporal Feature Adaptive Fusion module extract dynamic spatial relationships among EEG channels",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "via GCNs and temporal",
          "4": "followed by the final Classifier"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "module.",
          "4": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Z = ZW + V,\n(1)",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "where W ∈ R(f nk)×nk\nis\na\ntrainable weight matrix ini-",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tialized using Xavier uniform initialization to ensure\nstable",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "gradient flow during training, and V ∈ Rc×nk\nis a bias matrix",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "initialized as zeros. This transformation enables the model\nto",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "learn optimal\nfeature combinations, enhancing its\nsensitivity",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to emotional patterns embedded in the EEG signals.",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2) Construction of Global and Local Graphs: At each scale",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "k,\ntwo graphs are defined: a global graph GG,k = (U, EG,k)",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "share the\nand a local graph GL,k = (U, EL,k). Both graphs",
          "5": "and 32-channel"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "same node\nset U = {u1, u2, . . . , uc}, where\neach node ui",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "corresponds\nto an EEG channel,\nand the\nfeature vector\nfor",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "is extracted directly from Z.\nnode ui, denoted ui,k ∈ Rnk ,",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The\nglobal\nis\nconstructed\nadjacency matrix WG,k ∈ Rc×c",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "using a hybrid metric that\nintegrates\nthe Pearson Correlation",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "This subsection introduces the spatiotemporal feature adap-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Coefficient (PCC) and Manhattan Distance (MD) to eliminate",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "relation-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "weak or noisy connections while retaining meaningful spatial",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "ships among EEG channels for emotion analysis by integrating"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "is calculated after normalizing\nrelationships. The PCC, κi,j,k,",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "multi-depth Graph Convolutional Networks (GCNs) and tem-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the feature vectors by subtracting their mean and dividing by",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "their standard deviation, with a small constant (1e−6) added to",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "1) Adaptive Graph Encoding with Multi-Depth GCNs: The"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the denominator to avoid division by zero in cases of constant",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "fusion module"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "features. The MD is\ncomputed as di,j,k = ∥ui,k − uj,k∥1,",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "implemented"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "capturing the absolute differences between feature vectors. The",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "using ChebyNet, a variant of GCN that employs Chebyshev"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "weights in WG,k are then defined as:",
          "5": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "3) MambaBlock:\nThe MambaBlock\nperforms\ntemporal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sG,k = mean(cid:0)Hg−base,k, ΦG\ng,shallow(Fk, A(1)\nG,k),",
          "6": "modeling within the MSSTBlock. Given a normalized input"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(5)",
          "6": "it first\napplies\na\nlinear projection to an\nznorm ∈ Rb×nk×h,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ΦG\ng,deep(Fk, A(2)\nG,k)(cid:1),",
          "6": "expanded dimension:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the Local Graph Em-\nwhere sG,k ∈ R(b·nk)×h. Similarly,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "(11)\nzproj = Linear(znorm) ∈ Rb×nk×2dinner ,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "bedding is defined as:",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "is split\ninto\nwhere dinner = expand × h. The projected tensor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "two components, u and res, each of shape Rb×nk×dinner ."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sL,k = mean(cid:0)Hg−base,k, ΦL\ng,shallow(Fk, A(1)\nL,k),",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(6)",
          "6": "The u component undergoes a depthwise 1D convolution to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ΦL\ng,deep(Fk, A(2)\nL,k)(cid:1),",
          "6": "capture local\ntemporal dependencies:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "where sL,k ∈ R(b·nk)×h. These embeddings are generated",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "(12)\nuconv = Conv1D(Reshape(u))[:, :, : nk] ∈ Rb×dinner×nk ,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for each temporal\nscale, producing a set of global and local",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "token\nembeddings\nthat\nencapsulate multi-view\n{sG,k, sL,k}",
          "6": "u\nwhere\nthe\nreshape\ntransforms\nfrom\nto\n(b, nk, dinner)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "spatial\nrepresentations. These tokens are subsequently passed",
          "6": "convolution uses\na kernel\nsize of 4 with\n(b, dinner, nk). The"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to\nthe MSST-Mamba, which\nlearns\ntemporal\ndependencies",
          "6": "padding 3,\ntrimmed to preserve the sequence length nk. The"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "across scales, effectively integrating both spatial and temporal",
          "6": "output\nis reshaped to (b, nk, dinner) and activated using SiLU:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "patterns present\nin the EEG data.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "(13)\nuact = SiLU(uconv)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "D. MSST-Mamba",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "is then processed by the selective\nThe activated tensor uact"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The MSST-Mamba module constitutes a crucial component",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "∈\nstate-space model\n(SSM),\nyielding\nussm\n= SSM(uact)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "within a broader\nframework designed to capture\nthe multi-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "Rb×nk×dinner . Meanwhile,\nthe res component\nis activated sep-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "scale spatiotemporal dynamics of electroencephalogram (EEG)",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "arately:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "signals\nfor\nemotion recognition. The MSST-Mamba module",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "processes an input\ntensor\nthrough a stack of m MSSTBlock",
          "6": "(14)\nresact = SiLU(res)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "layers, followed by a final normalization step. Given an input",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "The SSM output\nis modulated by the activated residual:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "reshaped from the preceding spatiotemporal\nxin ∈ Rb×nk×h",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "feature adaptive fusion module,\nthe module’s computation can",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "(15)\numod = ussm · resact."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "be expressed as:",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "Finally, a linear projection maps the result back to the embed-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(7)\nxm = MSSTBlockm(RMSNorm(xm−1)) + xm−1,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "ding dimension:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(8)\nxout = RMSNorm(xm),",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "(16)\nzmamba = Linear(umod) ∈ Rb×nk×h."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "where m = 1, 2, . . . , M denotes\nthe\nnumber\nof\nlayers\nin",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "4)\nSelective State-Space Model\n(SSM):\nThe SSM within"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "MSST-Mamba module,\nand h is\nthe\nembedding dimension.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "the MambaBlock efficiently models long-range dependencies."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Each MSSTBlockm encapsulates a MambaBlock with a resid-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "it projects to SSM parame-\nStarting with uact ∈ Rb×nk×dinner ,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ual connection, while RMSNorm ensures numerical\nstability",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "ters:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "across layers. The input\nis sequentially transformed\ntensor xin",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "through the stack, culminating in a stabilized output xout of",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "(17)\nudbl = Linear(uact) ∈ Rb×nk×(dtrank+2dstate),"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the same shape.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1) MSSTBlock: The MSSTBlock,\nthe foundational unit of",
          "6": "δ\n∈\nwhich\nsplits\ninto\nRb×nk×dtrank, B,\nand C,\nboth\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "MSST-Mamba,\nintegrates normalization,\ntemporal modeling,",
          "6": "Rb×nk×dstate . The time-step parameter\nis computed as:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and a residual connection:",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "∆ = Softplus(Linear(δ)) ∈ Rb×nk×dinner,\n(18)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(9)\nzres = MambaBlock(RMSNorm(z)) + z,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "where Softplus(x) = ln(1+ex) ensures positivity. A learnable"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "where\nRMSNorm and MambaBlock\npreserve\nthe\nshape",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "matrix A ∈ Rdinner×dstate ,\ninitialized as A = − exp(A log), and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Rb×nk×h. The\nresidual\nconnection\nensures\nrobust\ngradient",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "a diagonal D ∈ Rdinner ,\ninitialized to ones, are used."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "flow and feature retention across layers.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "The selective scan updates the hidden state xt and computes"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2) RMSNorm:\nRoot Mean\nSquare Normalization\n(RM-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "the output yt over t = 1, 2, . . . , nk:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "SNorm)\nstabilizes\ntraining by standardizing the input\ntensor.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "For an input z ∈ Rb×nk×h, RMSNorm is defined as:",
          "6": "(19)\nxt = exp(∆t · A) · xt−1 + ∆t · Bt · vt,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "−1",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": " \n(cid:118)(cid:117)(cid:117)(cid:116)\n ",
          "6": "(20)\nyt = Ct · xt + D · vt,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1 h\n· w,\nz2\n(10)",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "h(cid:88) i\nznorm = z ·\n:,:,i + ϵ",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "=1",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "are slices at\ntime t,\nwhere vt = uact[:, t, :], ∆t, Bt, and Ct"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "where w ∈ Rh is a learnable weight vector initialized to ones,",
          "6": "and x0 = 0 ∈ Rb×dinner×dstate . The final output y ∈ Rb×nk×dinner"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and ϵ = 10−5 prevents division by zero. This normalization",
          "6": "stacks yt and adds a skip connection with D, capturing both"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mitigates\nscale variability, enhancing learning in deep archi-",
          "6": "short- and long-term dependencies with linear complexity in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tectures.",
          "6": "nk."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "E. Classifier",
          "7": "identical\n28\nvideo\nclips\nto\nelicit\nthe\nnine\nemotions\nfrom"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "THU-EP,\nfollowing the same seven-block,\nfour-trial structure"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "After\nprocessing\nthrough\nthe MSST-Mamba module,\nthe",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "with arithmetic tasks between blocks. Post-clip self-reports of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "global\nand\nare mean-pooled\nalong\nthe\nsequence\nlocal xout",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "emotional\nscores mirror THU-EP’s methodology. EEG data"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dimension and L2-normalized, then averaged to produce scale-",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "was collected with the 32-channel NeuSen.W32 system at 250"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "specific representations, which are fused across all k scales via",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Hz, and preprocessing aligns with THU-EP, using a 0.05–47"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mean pooling to form a unified embedding xfinal ∈ Rb×h. This",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Hz bandpass filter and ICA for artifact\nremoval. This\nlarger"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "embedding\ncaptures multi-scale\nspatiotemporal\ninformation",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "dataset enhances the scope for studying EEG-based emotional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "from the EEG signals.",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "responses."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ˆ\nThe final\nclassification\noutput,\n∈ Rb×dout , where\ndout",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "represents\nthe number of emotion classes,\nis generated by a",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "B. Baseline Methods"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "linear classifier applied to the unified embedding:",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "This investigation appraises the effectiveness of our method-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "ology in EEG-based emotion recognition. We benchmark it"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(21)\ny = xfinalW + b",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "against\na\nsuite\nof\nrecognized\nbaseline\napproaches,\ndetailed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Rh×dout\nRdout",
          "7": "hereafter:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "b\n∈\nHere, W ∈\nand\nare\nthe\nlearnable",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "1) DGCNN (graph-based) [13]: The Dynamical Graph Con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "weights\nand\nbias,\nrespectively. This\nlinear\nlayer maps\nthe",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "volutional Neural Network\n(DGCNN)\ndynamically\ndiscerns"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multi-scale embedding to the logit space, producing logits that",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "inter-channel\nEEG relationships\nvia\na\ntrainable\nadjacency"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "can be transformed into a probability distribution over emotion",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "matrix,\nrefined throughout\nthe neural network’s learning pro-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "classes using the softmax function during inference or training",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "cess. This\nadaptability markedly enhances\nthe\nextraction of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with a cross-entropy loss.",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "discriminative spatial features, bolstering emotion recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "precision."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IV. EXPERIMENT AND RESULTS",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "2) RGNN (graph-based) [31]: The Regularized Graph Neu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A. Datasets",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "ral Network (RGNN)\nleverages neuroscientific\ninsights\ninto"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "To\nassess\nthe\nperformance\nof\nour\nproposed model, we",
          "7": "brain topology to model\nlocal and global EEG channel\ninter-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "conducted comprehensive experiments utilizing five publicly",
          "7": "actions. By embedding sparsity-inducing regularization within"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "available datasets:\nthe SJTU Emotion EEG Dataset\n(SEED)",
          "7": "its graph convolutions, RGNN prunes extraneous connections,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[26],\nthe Emotion Profiles\ndataset\n(THU-EP)\n[27],\nand\nits",
          "7": "thereby amplifying emotionally salient\nfeatures and ensuring"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "expanded counterpart,\nthe FACED dataset\n[28].",
          "7": "robust classification across diverse stimuli."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The SEED dataset, developed by Shanghai Jiao Tong Uni-",
          "7": "3) PGCN (graph-based)\n[45]: The Pyramidal Graph Con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "versity’s BCMI\nlaboratory, contains EEG recordings from 15",
          "7": "volutional\nNetwork\n(PGCN)\nconstructs\na\ntriadic\nhierar-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "native Chinese participants\n(7 males, 8 females; mean age:",
          "7": "chy—encompassing local\nelectrode\nclusters, mesoscopic\nre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "23.27 years). These subjects watched 15 Chinese film clips,",
          "7": "gions\n(e.g.,\nseven\nlobes),\nand\nglobal\ncortex—using\nsparse"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "each lasting about 4 minutes, selected to evoke three emotional",
          "7": "adjacency matrices. This hierarchical synthesis mitigates over-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "states: positive, neutral, and negative (five clips per category).",
          "7": "smoothing,\nyielding\na\nprecise\nand\ninterpretable\nemotional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Following each clip, participants\nrated their\nemotions based",
          "7": "activity map."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "on valence and arousal. Brain activity was captured using a",
          "7": "4) TSception (CNN-based)\n[46]: TSception,\na multi-scale"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "62-channel\nelectrode\ncap\nconfigured\nper\nthe\n10-20\nsystem,",
          "7": "convolutional architecture, integrates dynamic temporal, asym-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with signals\nrecorded at a 1000 Hz sampling rate. The data",
          "7": "metric spatial, and fusion layers. By concurrently extracting"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "was preprocessed with a 0.3–50 Hz bandpass filter to enhance",
          "7": "temporal dynamics and spatial asymmetries,\nit excels\nin dis-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "signal quality for emotion analysis.",
          "7": "cerning rapid emotional fluctuations across EEG channels."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The THU-EP dataset\nincludes EEG data\nfrom 80 college",
          "7": "5) LSTM (temporal-learning) [47]: Long Short-Term Mem-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "students\n(50\nfemales,\n30 males;\naged\n17–24, mean:\n20.16",
          "7": "ory\n(LSTM)\nnetworks\n,\nequipped with\ndual memory\ncells"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "years) exposed to 28 video clips averaging 67 seconds each.",
          "7": "and\ngating mechanisms,\nprocess\n4 Hz EEG sequences\nto"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "These\nclips were designed to trigger nine\nemotions:\nanger,",
          "7": "capture\nlong-term temporal\ndependencies.\nSuch\ncapability"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "disgust, fear, sadness, amusement,\njoy,\ninspiration,\ntenderness,",
          "7": "proves\ninvaluable for\ntracking gradual emotional\ntransitions,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and neutral, with four clips for neutral and three for each of",
          "7": "e.g., neutral\nto positive valence."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the others. The experiment was divided into seven blocks of",
          "7": "6) TCN (temporal-learning)\n[48]: The Temporal Convolu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "four trials, with participants solving 20 arithmetic problems be-",
          "7": "tional Network\n(TCN)\nemploys\nadjustable\ndilated\nconvolu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tween blocks to reset their emotional baseline. After each clip,",
          "7": "tions, augmented by visual-to-EEG distillation,\nto encapsulate"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "subjects\nself-reported scores\nfor arousal, valence,\nfamiliarity,",
          "7": "extended temporal patterns, outperforming LSTM in multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and liking. EEG signals were recorded using the NeuSen.W32",
          "7": "modal\nregression tasks."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "wireless system with 32 channels at a 250 Hz sampling rate,",
          "7": "7) BiDANN (adversarial-based)\n[32]: The Bi-Hemisphere"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "preprocessed with a 0.05–47 Hz bandpass filter, and cleaned",
          "7": "Domain Adversarial Neural Network (BiDANN) deploys dual-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "via independent component analysis (ICA) to remove artifacts.",
          "7": "hemisphere LSTM extractors\nfeeding\nthree\ndiscriminators,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The FACED dataset builds on THU-EP, expanding to 123",
          "7": "interlinked via Gradient Reversal Layers. This adversarial do-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "subjects by adding 43 participants\nto the original 80, while",
          "7": "main alignment, preserving hemispheric distinctions, ensures"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "retaining the\nsame\nexperimental\nframework.\nIt\nemploys\nthe",
          "7": "robust cross-subject generalization."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "AND THE NEXT BEST ARE MARKED USING UNDERLINES."
        },
        {
          "TABLE I": "THU-EP"
        },
        {
          "TABLE I": "ACC (%)"
        },
        {
          "TABLE I": "23.45 ± 4.82"
        },
        {
          "TABLE I": "24.72 ± 5.91"
        },
        {
          "TABLE I": "56.71 ± 3.37"
        },
        {
          "TABLE I": "57.23 ± 3.07"
        },
        {
          "TABLE I": "60.34 ± 5.41"
        },
        {
          "TABLE I": "59.18 ± 5.93"
        },
        {
          "TABLE I": "57.79 ± 3.13"
        },
        {
          "TABLE I": "56.92 ± 4.31"
        },
        {
          "TABLE I": "55.83 ± 3.52"
        },
        {
          "TABLE I": "61.44 ± 5.51"
        },
        {
          "TABLE I": "59.50 ± 4.70"
        },
        {
          "TABLE I": "62.39 ± 3.13"
        },
        {
          "TABLE I": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "",
          "TABLE II": ""
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "Source Adaptation Transfer Network (DMATN)\nsynthesizes",
          "TABLE II": ""
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "multi-source\nEEG\nthrough\nattention-weighted\nfusion\nand",
          "TABLE II": "Hyperparameters"
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "an\nadversarial\nclassifier. By\nharmonizing\ndiverse\ninputs,\nit",
          "TABLE II": ""
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "",
          "TABLE II": "Loss function"
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "achieves consistent cross-subject performance.",
          "TABLE II": "Optimizer"
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "9)\nEmT\n(Graph-Transformer-Based)\n[50]:\nThe\nEmotion",
          "TABLE II": "Initial\nlearning rate"
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "Transformer\n(EmT), a leading method in graph-transformer-",
          "TABLE II": "Label smoothing"
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "",
          "TABLE II": "Dropout\nrate"
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "based EEG emotion recognition, leverages a graph-transformer",
          "TABLE II": ""
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "",
          "TABLE II": "Batch size"
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "architecture to model spatiotemporal dynamics. By converting",
          "TABLE II": ""
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "",
          "TABLE II": "Training epochs (SEED)"
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "signals into temporal graphs,\nits residual multi-view pyramid",
          "TABLE II": ""
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "",
          "TABLE II": "Training epochs (THU-EP, FACED)"
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "GCN (RMPG) captures diverse spatial patterns of emotional",
          "TABLE II": ""
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "",
          "TABLE II": "Early stopping patience"
        },
        {
          "8) DMATN (adversarial-based)\n[49]:\nThe Deep Multi-": "cognition, while\nthe\ntemporal\ncontextual\ntransformer\n(TCT)",
          "TABLE II": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": ""
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "GCN (RMPG) captures diverse spatial patterns of emotional"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": ""
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "cognition, while\nthe\ntemporal\ncontextual\ntransformer\n(TCT)"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "excels at\nlearning long-term dependencies, achieving superior"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "cross-subject\ngeneralization\nin\nclassification\nand\nregression"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": ""
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "tasks."
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": ""
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": ""
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "C. Experimental Protocol"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": ""
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "In this\nstudy, we adopt a training strategy consistent with"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": ""
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "the approach in EmT to implement a subject-independent eval-"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": ""
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "uation framework, ensuring effective generalization to unseen"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": ""
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "individuals\nacross\ntailored cross-validation strategies\nfor\nthe"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "SEED, THU-EP, and FACED datasets. For the SEED dataset,"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": ""
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "which includes data\nfrom fifteen experimental\nsubjects, we"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "employ a leave-one-subject-out\n(LOSO)\ncross-validation ap-"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "proach, where in each of\nthe fifteen iterations, data from one"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "subject are set aside as the test set, and the remaining fourteen"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "subjects’\ndata\nare\npooled\nand\nsplit\nrandomly\ninto\ntraining"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "and\nvalidation\nsets\nat\nan\n8:2\nratio\n-\n80% for\ntraining\nand"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "20% for validation. As previously noted, we sliced the SEED"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "dataset into time windows of varying lengths to serve as input,"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "resulting in data segments of different sizes. To accommodate"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "this variability, we\ncreated multiple dataloaders\nto feed the"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "network, ensuring consistent\nlabeling across all segments. For"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "the THU-EP and FACED datasets, we use a leave-n-subject-"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "out\ncross-validation\nstrategy, with n set\nto\n8\nfor THU-EP"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "and 12 for FACED;\nin each fold, data from n subjects\nform"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "the\ntest\nset, while\nthe\nremaining subjects’ data\nare divided"
        },
        {
          "signals into temporal graphs,\nits residual multi-view pyramid": "so\nthat\n90% go\nto\ntraining\nand\n10% to\nvalidation. Across"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": "w/o Spatiotemporal Fusion\n79.75\n80.23\n59.17\n66.10"
        },
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": "w/o MSST-Mamba\n79.53\n77.93\n57.92\n65.04"
        },
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": ""
        },
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": "83.43\n85.03\n62.39\n73.28\nMSGM (Proposed)"
        },
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": ""
        },
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": ""
        },
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": "Across all\nthree datasets, our proposed method consistently"
        },
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": ""
        },
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": "delivers\nsuperior\nperformance,\nparticularly\nexcelling\nin\nac-"
        },
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": ""
        },
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": "curacy,\nthereby affirming its\nrobustness\nand effectiveness\nin"
        },
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": ""
        },
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": "emotion classification tasks. Advanced architectures\nsuch as"
        },
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": ""
        },
        {
          "w Single GCN\n81.92\n81.50\n62.01\n70.68": "EmT and BiDANN also demonstrate strong capabilities;\nfor"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III": "ARCHITECTURAL HYPERPARAMETERS OF THE MSGM MODEL",
          "TABLE V": "GENERALIZED EMOTION CLASSIFICATION RESULTS OF ABLATION"
        },
        {
          "TABLE III": "",
          "TABLE V": "STUDIES ON THE SEED AND THU-EP DATASETS (%)"
        },
        {
          "TABLE III": "Hyperparameters\nValue",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "Method\nSEED\nTHU-EP"
        },
        {
          "TABLE III": "GCN layers\n[1, 2]",
          "TABLE V": ""
        },
        {
          "TABLE III": "Embedding dimension (h)\n32",
          "TABLE V": "ACC (%)\nF1 (%)\nACC (%)\nF1 (%)"
        },
        {
          "TABLE III": "MSST-Mamba layers\n1",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "w/o Temporal Multi-Scale\n80.04\n80.14\n59.55\n72.13"
        },
        {
          "TABLE III": "⌈h/16⌉\ndtrank",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "w/o Spatial Multi-Scale\n82.37\n82.57\n61.82\n72.02"
        },
        {
          "TABLE III": "16\ndstate",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "w Single GCN\n81.92\n81.50\n62.01\n70.68"
        },
        {
          "TABLE III": "",
          "TABLE V": "w/o Spatiotemporal Fusion\n79.75\n80.23\n59.17\n66.10"
        },
        {
          "TABLE III": "",
          "TABLE V": "w/o MSST-Mamba\n79.53\n77.93\n57.92\n65.04"
        },
        {
          "TABLE III": "TABLE IV",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "83.43\n85.03\n62.39\n73.28\nMSGM (Proposed)"
        },
        {
          "TABLE III": "HARDWARE SPECIFICATIONS FOR TRAINING AND DEPLOYMENT",
          "TABLE V": ""
        },
        {
          "TABLE III": "Property\nGeForce RTX 3070Ti\nJetson Xavier NX",
          "TABLE V": ""
        },
        {
          "TABLE III": "GPU\nNVIDIA GeForce RTX 3070Ti\nNVIDIA Volta",
          "TABLE V": "Across all\nthree datasets, our proposed method consistently"
        },
        {
          "TABLE III": "CPU\nCore i7-8700K\nCarmel Arm v8.2",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "delivers\nsuperior\nperformance,\nparticularly\nexcelling\nin\nac-"
        },
        {
          "TABLE III": "RAM\n64 GB\n8 GB",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "curacy,\nthereby affirming its\nrobustness\nand effectiveness\nin"
        },
        {
          "TABLE III": "Power usage\n240 W\n10 W / 15 W / 20 W",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "emotion classification tasks. Advanced architectures\nsuch as"
        },
        {
          "TABLE III": "Purpose\nTraining and testing\nReal-world deployment",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "EmT and BiDANN also demonstrate strong capabilities;\nfor"
        },
        {
          "TABLE III": "",
          "TABLE V": "instance, EmT achieves the second-best accuracy on SEED and"
        },
        {
          "TABLE III": "",
          "TABLE V": "competitive results on THU-EP and FACED, while BiDANN"
        },
        {
          "TABLE III": "for\nfeature\nextraction\nand\nspatiotemporal\nprocessing\nacross",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "stands\nout with\nthe\nsecond-highest\naccuracy\non THU-EP."
        },
        {
          "TABLE III": "datasets,\nachieving\nhigh\naccuracy while maintaining\ncom-",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "Methods\nleveraging\ntemporal\ndynamics,\nsuch\nas TSception"
        },
        {
          "TABLE III": "putational efficiency. The selective state-space model\n(SSM)",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "(66.22% ACC on SEED) and TCN (76.54% ACC on SEED),"
        },
        {
          "TABLE III": "operates with a dynamically computed dtrank = ⌈h/16⌉ and",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "generally outperform those relying solely on spatial\nfeatures,"
        },
        {
          "TABLE III": "a\nstate\ndimension\n= 16,\noptimizing\nspatiotemporal\ndstate",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "highlighting the critical role of temporal\ninformation in EEG-"
        },
        {
          "TABLE III": "modeling.",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "based emotion recognition. The substantial performance gap"
        },
        {
          "TABLE III": "Hardware configurations are presented in Table IV. Training",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "between these advanced methods and traditional approaches"
        },
        {
          "TABLE III": "and testing leverage an NVIDIA GeForce RTX 3070Ti (8 GB",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "like KNN and SVM underscores\nthe\nlimitations of\nsimpler"
        },
        {
          "TABLE III": "GDDR6), enabling rapid optimization of\nthe model’s param-",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "models in this complex domain."
        },
        {
          "TABLE III": "eters. For\nreal-world deployment,\nthe NVIDIA Jetson Xavier",
          "TABLE V": ""
        },
        {
          "TABLE III": "NX,\nfeaturing a 6-core Carmel ARM v8.2 CPU and a Volta",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "B. Ablation Study on Component Modules"
        },
        {
          "TABLE III": "GPU with 48 Tensor Cores (up to 21 TOPS, INT8), offers low-",
          "TABLE V": ""
        },
        {
          "TABLE III": "power (10–20 W) and high-efficiency inference, supported by",
          "TABLE V": "To evaluate\nthe\ncontributions of\nthe\ntemporal multi-scale"
        },
        {
          "TABLE III": "8 GB LPDDR4x memory and 51.2 GB/s bandwidth,\nideal for",
          "TABLE V": "feature extraction, spatial multi-scale prior information initial-"
        },
        {
          "TABLE III": "edge computing applications.",
          "TABLE V": "ization,\nspatiotemporal\nfeature\nadaptive\nfusion,\nand MSST-"
        },
        {
          "TABLE III": "",
          "TABLE V": "Mamba\nand\nclassifier modules, we\nconducted\nan\nablation"
        },
        {
          "TABLE III": "",
          "TABLE V": "analysis by systematically removing each component\nindivid-"
        },
        {
          "TABLE III": "V. NUMERICAL RESULTS",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "ually and assessing its\nimpact on classification performance."
        },
        {
          "TABLE III": "A. Emotion Recognition Performance",
          "TABLE V": ""
        },
        {
          "TABLE III": "",
          "TABLE V": "This\nincluded omitting the\ntemporal multi-scale\nfeature\nex-"
        },
        {
          "TABLE III": "The\nexperimental\nresults\nare presented in Table\nI, which",
          "TABLE V": "traction (w/o Temporal Multi-Scale), spatial multi-scale prior"
        },
        {
          "TABLE III": "evaluates the performance of various methods for generalized",
          "TABLE V": "information initialization (w/o Spatial Multi-Scale), spatiotem-"
        },
        {
          "TABLE III": "emotion classification across three datasets—SEED, THU-EP,",
          "TABLE V": "poral\nfeature\nadaptive\nfusion\n(w/o Spatiotemporal Fusion),"
        },
        {
          "TABLE III": "and FACED—using\naccuracy\n(ACC %)\nand F1\nscore\n(F1",
          "TABLE V": "and MSST-Mamba (w/o MSST-Mamba), as well as replacing"
        },
        {
          "TABLE III": "%)\nas metrics. On the SEED dataset, our proposed method",
          "TABLE V": "multi-depth GCNs with a\nsingle\nlayer\n(w Single GCN),\nto"
        },
        {
          "TABLE III": "achieves an outstanding accuracy of 83.43% and an F1 score",
          "TABLE V": "measure each component’s effect. The results are detailed in"
        },
        {
          "TABLE III": "of 85.03%, outperforming all other approaches. The next best",
          "TABLE V": "Table V."
        },
        {
          "TABLE III": "performers\nare EmT with\nan\naccuracy\nof\n80.20% and\nan",
          "TABLE V": "The\nremoval of\nthe MSST-Mamba\nand classifier module"
        },
        {
          "TABLE III": "F1 score of 82.10% (second-highest accuracy and F1 score),",
          "TABLE V": "results\nin\nthe most\nsignificant\nperformance\ndecline, with"
        },
        {
          "TABLE III": "while traditional methods like KNN and SVM trail far behind",
          "TABLE V": "accuracy\ndecreasing\nby\n3.90% on\nthe\nSEED dataset\nand"
        },
        {
          "TABLE III": "with\naccuracies\nof\n49.26% and\n51.68%,\nand F1\nscores\nof",
          "TABLE V": "4.47% on the THU-EP dataset, alongside F1 score drops of"
        },
        {
          "TABLE III": "48.89% and 50.31%,\nrespectively. For\nthe THU-EP dataset,",
          "TABLE V": "7.10% and 8.24%,\nrespectively. This underscores\nits critical"
        },
        {
          "TABLE III": "our method continues\nto lead with the highest\naccuracy of",
          "TABLE V": "role\nin\nprocessing\nand\nintegrating multi-scale\nspatiotempo-"
        },
        {
          "TABLE III": "62.39% and F1 score of 73.28%, followed closely by BiDANN",
          "TABLE V": "ral\nfeatures effectively. Excluding the spatiotemporal\nfeature"
        },
        {
          "TABLE III": "at 61.44% accuracy and EmT at 72.40% F1 score. On the",
          "TABLE V": "adaptive\nfusion module\nalso leads\nto substantial\nreductions,"
        },
        {
          "TABLE III": "FACED dataset, BiDANN achieves a slightly higher accuracy",
          "TABLE V": "with accuracy dropping by 3.68% on SEED and 3.22% on"
        },
        {
          "TABLE III": "of 63.36% compared to our method’s 63.17%. However, our",
          "TABLE V": "THU-EP, highlighting its importance in unifying temporal and"
        },
        {
          "TABLE III": "method outperforms BiDANN in terms of F1 score, achieving",
          "TABLE V": "spatial\ninformation."
        },
        {
          "TABLE III": "76.01% against BiDANN’s 73.82%, making our model the top",
          "TABLE V": "The absence of\nthe temporal multi-scale feature extraction"
        },
        {
          "TABLE III": "performer\nin F1 score.",
          "TABLE V": "module decreases\naccuracy by 3.39% on SEED and 2.84%"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "using a single GCN instead of multiple GCN layers\nreduces"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "accuracy by 1.51% on SEED and 0.38% on THU-EP, demon-"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "strating that multi-layer GCNs more effectively capture spatial"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "information."
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": ""
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "C. Performance and Sensitive Analysis of Hyperparameters"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "1)\nImpact of EEG Feature Types: As\nillustrated\nin Fig-"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": ""
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "ure 3a, which compares the accuracy and F1 scores of Power"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": ""
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "Spectral Density (PSD), Differential Entropy (DE), and rela-"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": ""
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "tive Power Spectral Density (rPSD) features for emotion clas-"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": ""
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "sification on the SEED dataset. Specifically, rPSD achieved an"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "accuracy of 83.43% and an F1 score of 85.03%, surpassing DE"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "by 5.77 percentage points\nin accuracy and 11.75 percentage"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "points\nin F1 score. Compared to PSD,\nrPSD exhibited even"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "greater\nimprovements, with\nan\naccuracy\nincrease\nof\n11.27"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "percentage points and an F1 score increase of 16.69 percentage"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "points. These findings demonstrate\nthat\nrPSD is\na\nsuperior"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "feature\nin\nour model\nfor EEG-based\nemotion\nclassification"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "tasks compared to both DE and PSD."
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "2)\nInfluence of\nthe Number of MSST-Mamba Blocks: The"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "effect of varying the number of MSST-Mamba Blocks on the"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "emotion classification performance is illustrated in Figure 3b."
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "The analysis considered block counts of 1, 2, 4, 6, and 8, with"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "corresponding impacts on accuracy and F1 score. With a single"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "block,\nthe model achieved an accuracy of 83.43% and an F1"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "score of 85.03%.\nIncreasing to 2 blocks led to a decrease in"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "performance, with accuracy dropping to 80.85% and F1 score"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "to 81.16%. A slight\nrecovery was observed with 4 blocks,"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "where accuracy reached 83.41% and F1 score 84.82%, nearly"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "matching the single-block performance. Further increasing the"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "block count to 6 resulted in a decline, with accuracy at 82.12%"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "and F1 score at 82.56%, and this downward trend persisted"
        },
        {
          "impact\nis\nless pronounced than other modules. Additionally,": "with 8 blocks, where accuracy and F1 score further decreased"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "to the official Mamba implementation. With this configuration,"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "the model utilizes 349,218 parameters and achieves an infer-"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "ence time of 151.0 ms, maintaining millisecond-level inference"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "and\ndemonstrating\nrobust\nreal-time\nprocessing\ncapabilities."
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "This\nefficiency\nunderscores\nits\nsuitability\nfor\nedge\ndevice"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "applications requiring rapid data handling."
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": ""
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "E. Comparison with EmT"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "In this section, we compare our MSGM model with EmT,"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "a\nleading method in graph-transformer-based EEG emotion"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "recognition. Both models adopt a graph-Transformer/Mamba-"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "based architecture to process spatial-temporal patterns in EEG"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "signals. EmT incorporates\nan\n8-layer TCT module, while"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "MSGM employs\na\nsingle-layer MSST-Mamba module. We"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "evaluate\ntheir performance\nin terms of\naccuracy, parameter"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "count, and inference time. To ensure a fair comparison,\nthis"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "evaluation was conducted on the GeForce RTX 3070Ti plat-"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "form (see Table IV), rather than on edge devices, allowing both"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "MSGM and EmT to run in a consistent environment without"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "the\ninfluence of Mamba-minimal, which was used for\nedge"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "deployment."
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "MSGM, with its\nsingle-layer MSST-Mamba,\nachieves\nsu-"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "perior\naccuracy\nand\nF1\nscores\ncompared\nto\nEmT,\nde-"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "spite using only 349,218 parameters—approximately half of"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "EmT’s\n703,530. This\nhighlights MSGM’s\nefficiency,\nas\nits"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "linear-complexity MSST-Mamba\noutperforms\nthe\nquadratic-"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "complexity TCT module with a simpler structure. The reduced"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "parameter count underscores MSGM’s suitability for resource-"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": ""
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "constrained settings, such as edge devices."
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": ""
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "In terms of inference time, MSGM records 7.9 ms, slightly"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": ""
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "higher\nthan\nEmT’s\n4.3 ms.\nThis minor\ngap\narises\nfrom"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "MSGM’s multi-scale architecture, which limits full paralleliza-"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "tion. Nevertheless,\nboth models maintain millisecond-level"
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": ""
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": "inference, ensuring negligible impact on real-time applications."
        },
        {
          "Fig. 6: NVIDIA Jetson Xavier NX.": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In the\ninitial\nconnectivity map (Figure 5a),\nthe\nstrongest",
          "12": "dynamically adjust\nto individual neurophysiological profiles,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "connections are observed between electrodes\nsuch as C1-Pz,",
          "12": "thereby\nadvancing\nboth\nprecision\nand\naccessibility\nin\nreal-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "FC2-FPz, and C6-CP4. These connections primarily involve",
          "12": "world applications."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the central and parietal regions, with some involvement of the",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "frontal areas [53], suggesting a baseline interaction that may",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "REFERENCES"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "reflect\ngeneral\nneural\ncommunication\nprior\nto\ntask-specific",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "learning. The prominence of\nthese\nconnections\nindicates\nan",
          "12": "[1]\nS. K. Khare and V. Bajaj, “Time–frequency representation and convolu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "initial focus on central-parietal and frontal-central interactions,",
          "12": "tional neural network-based emotion recognition,” IEEE Transactions on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "Neural Networks and Learning Systems, vol. 32, no. 7, pp. 2901–2909,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "which are often associated with sensory and motor coordina-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion in early-stage processing.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[2]\nZ.\nCheng,\nX.\nBu,\nQ. Wang,\nT.\nYang,\nand\nJ.\nTu,\n“Eeg-based"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In contrast, the learned connectivity map (Figure 5b) reveals",
          "12": "emotion\nrecognition\nusing\nmulti-scale\ndynamic\ncnn\nand\ngated"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "transformer,” Scientific Reports, vol. 14, Dec 2024. [Online]. Available:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "a more\nrefined set of\nconnections, with the\nstrongest\nlinks",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "https://doi.org/10.1038/s41598-024-82705-z"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "being C6-CP4 and C1-Pz. These\nretained and strengthened",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[3]\nJ. Guo et al., “A transformer based neural network for emotion recog-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "connections\ncontinue\nto\nemphasize\ninteractions within\nthe",
          "12": "nition and visualizations of crucial eeg channels,” Physica A: Statistical"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "Mechanics and its Applications, vol. 603, p. 127700, 2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "central and parietal\nregions, which are known to play critical",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[4] M. Jafari et al., “Emotion recognition in eeg signals using deep learning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "roles\nin sensory integration and spatial processing. The per-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "methods: A review,” Comput Biol Med, vol. 165, p. 107450, 2023."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sistence of\nthese specific connections suggests that\nthe model",
          "12": "[5]\nJ. Xiao, W. Ding, D. Zhang, Y. Ma, Y. Wang, Z. Shao, and J. Wang,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "“Multi-periodicity dependency transformer based on spectrum offset for"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "has prioritized and enhanced these pathways,\nlikely due\nto",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "radio frequency fingerprint\nidentification,” Measurement, 2025."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "their\nrelevance\nto\nthe\ntask\nat\nhand\n[54]. Additionally,\nthe",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[6]\nE. Vafaei and M. Hosseini, “Transformers in eeg analysis: A review of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "color intensity, ranging from 0.6 to 1.0, highlights the varying",
          "12": "architectures and applications\nin motor\nimagery,\nseizure, and emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "classification,”\nSensors,\nvol.\n25,\nno.\n5,\np.\n1293,\n2025.\n[Online]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "strengths of\nthese\nlearned connections, with warmer\ncolors",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "Available: https://doi.org/10.3390/s25051293"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "indicating stronger\ninteractions.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[7] X. Yao, T. Li, P. Ding, F. Wang, L. Zhao, A. Gong, W. Nan,\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The connectivity patterns observed in Figure 5 demonstrate",
          "12": "Y\n. Fu,\n“Emotion classification based on transformer\nand cnn for\neeg"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "spatial–temporal\nfeature\nlearning,” Brain Sci., vol. 14, no. 3, p. 268,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the model’s\nability\nto\nrefine\nand\nfocus\non\nkey\nelectrode",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "2024.\n[Online]. Available: https://doi.org/10.3390/brainsci14030268"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "relationships,\ntransitioning from a broader\ninitial\nstate\nto a",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[8] C. Liu, X. Zhou, Y. Wu, Y. Ding, L. Zhai, K. Wang, Z. Jia, and Y. Liu,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "more targeted, task-driven network. This evolution underscores",
          "12": "“A comprehensive survey on eeg-based emotion recognition: A graph-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "based perspective,” arXiv preprint arXiv:2408.06027, 2024."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the model’s effectiveness\nin capturing and enhancing critical",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[9] H. Liu, Y. Zhang, Y. Li, and X. Kong, “Review on emotion recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "neural\nrelationships,\nparticularly\nin\nthe\ncentral\nand\nparietal",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "in Computational Neuro-\nbased on electroencephalography,” Frontiers"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "regions,\ntailored to the cognitive demands of the classification",
          "12": "science, vol. 15, p. 758212, 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "task.",
          "12": "[10] Y. Xiao, Y. Zhang, X. Peng, S. Han, X. Zheng, D. Fang, and X. Chen,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "“Multi-source eeg emotion recognition via dynamic contrastive domain"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "adaptation,” arXiv preprint arXiv:2408.10235, 2024."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[11]\nE. Sathiya, T. D. Rao, and T. S. Kumar, “A comparative study of wavelet"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "VI. CONCLUSION",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "families for schizophrenia detection,” Frontiers in Human Neuroscience,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "vol. 18, 2024.\n[Online]. Available: https://www.frontiersin.org/journals/"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In this paper, we propose the Multi-Scale Spatiotemporal",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "human-neuroscience/articles/10.3389/fnhum.2024.1463819"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Graph Mamba (MSGM), a novel\nframework for EEG-based",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[12] N.\nJmail, M. Zaghdoud, A. Hadriche, T. Frikha, C. Ben Amar,\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion recognition that\nintegrates\ntemporal multi-scale fea-",
          "12": "C. B´enar,\n“Integration of\nstationary wavelet\ntransform on a dynamic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "partial\nreconfiguration for\nrecognition of pre-ictal gamma oscillations,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ture extraction, spatial multi-scale prior information initializa-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "Heliyon,\nvol.\n4,\nno.\n2,\np.\ne00530, mar\n2018.\n[Online]. Available:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion,\nspatiotemporal\nfeature adaptive fusion, and the MSST-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "https://doi.org/10.1016/j.heliyon.2018.e00530"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Mamba module. By capturing short-term emotional continuity",
          "12": "[13]\nT. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "dynamical graph convolutional neural networks,” IEEE Transactions on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and long-term evolutionary trends through multi-scale tempo-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "Affective Computing, vol. 11, no. 3, pp. 532–541, July-September 2020,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ral analysis, alongside hierarchical spatial connectivity via bi-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "manuscript\nreceived 10 Oct. 2017;\nrevised 28 Jan. 2018;\naccepted 2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "modal graph modeling, MSGM addresses critical gaps in prior",
          "12": "Mar. 2018; date of publication 21 Mar. 2018; date of current version 4"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "Sept. 2020. Corresponding author: Wenming Zheng. Recommended for"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "methodologies. Extensive\nexperiments on the SEED, THU-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "acceptance by B. Hu."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "EP, and FACED datasets demonstrate its superior performance",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[14] M. Soleymani, S. Asghari-Esfeden, Y. Fu, and M. Pantic, “Analysis of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "over\nbaseline methods,\nvalidated\nthrough\nrigorous\nsubject-",
          "12": "eeg signals\nand facial\nexpressions\nfor\ncontinuous\nemotion detection,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "independent evaluation. The model achieves millisecond-level",
          "12": "IEEE Transactions on Affective Computing, vol. 7, no. 1, pp. 17–28,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "2016."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "inference\nspeed\non\nedge\ndevices\nlike\nthe NVIDIA Jetson",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[15] H. Yan, K. Guo, X. Xing, and X. Xu, “Bridge graph attention based"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Xavier NX, underscoring its practical applicability in clinical",
          "12": "graph convolution network with multi-scale transformer for eeg emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and consumer\nsettings, while its neuroanatomical grounding",
          "12": "recognition,” IEEE Transactions on Affective Computing, 2024."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[16] Y. Ding, N. Robinson,\nS. Zhang, Q. Zeng,\nand C. Guan,\n“Masa-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "enhances\ninterpretability of\nthe brain’s distributed emotional",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "tcn: Multi-anchor\nspace-aware temporal convolutional neural networks"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dynamics. Nevertheless,\nchallenges\nremain in achieving op-",
          "12": "for\ncontinuous\nand discrete\neeg emotion recognition,” arXiv preprint"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "timal\ncross-subject generalization,\nconstrained by variability",
          "12": "arXiv:2308.16207, 2023."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[17]\nP.\nZhang, C. Min, K.\nZhang, W. Xue,\nand\nJ. Chen,\n“Hierarchi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in EEG data across individuals. Looking ahead,\nfuture devel-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "cal\nspatiotemporal electroencephalogram feature learning and emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "opments\nin EEG-based emotion recognition could focus on",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "recognition with attention-based antagonism neural network,” Frontiers"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "integrating multimodal physiological\nsignals—such as ECG",
          "12": "in Neuroscience, vol. 15, p. 738167, dec 2021, eCollection 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[18] M. Ye, C. L. P. Chen,\nand T. Zhang,\n“Hierarchical\ndynamic\ngraph"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "or eye-tracking data—to enrich emotional context, optimizing",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "convolutional network with interpretability for eeg-based emotion recog-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "lightweight\narchitectures\nfor\nbroader\ndeployment\non wear-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "nition,” IEEE Transactions on Neural Networks and Learning Systems,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "able\ndevices,\nand\nexploring\nreal-time\nadaptive\nlearning\nto",
          "12": "pp. 1–12, 2022."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[19] Y. Ding, N. Robinson, C. Tong, Q. Zeng,\nand C. Guan,\n“Lggnet:",
          "13": "International Conference Science and Technology for Humanity (TIC-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Learning from local-global-graph representations for brain–computer in-",
          "13": "STH), 2009, pp. 456–461."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "terface,” IEEE Transactions on Neural Networks and Learning Systems,",
          "13": "[41] Y. Song, Q. Zheng, B. Liu, and X. Gao, “Eeg conformer: Convolutional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 35, no. 7, pp. 9773–9786, 2024.",
          "13": "transformer\nfor eeg decoding and visualization,” IEEE Transactions on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[20] Y. Xue, W.\nZheng, Y.\nZong, H. Chang,\nand X.\nJiang,\n“Adaptive",
          "13": "Neural\nSystems\nand Rehabilitation Engineering,\ndec\n2023.\n[Online]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "hierarchical graph convolutional network for eeg emotion recognition,”",
          "13": "Available: https://doi.org/10.1109/TNSRE.2022.3230250"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in 2022 International Joint Conference on Neural Networks\n(IJCNN),",
          "13": "[42]\nP. Gong, Z. Jia, P. Wang, Y. Zhou, and D. Zhang, “Astdf-net: attention-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2022, pp. 1–8.",
          "13": "based spatial-temporal dual-stream fusion network for eeg-based emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[21] N. B. Ye Qiao, Mohammed Alnemari,\n“A two-stage\nefficient\n3-d",
          "13": "the 31st ACM international confer-\ntion recognition,” in Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv\npreprint\ncnn\nframework\nfor\neeg\nbased\nemotion\nrecognition,”",
          "13": "ence on multimedia, 2023, pp. 883–892."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv:2208.00883, 2022.",
          "13": "[43]\nL.\nFeng, C. Chen, M. Zhao, H. Deng,\nand Y. Zhang,\n“Eeg-based"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[22] W.-C.\nFang, K.-Y. Wang, N.\nFahier, Y.-L. Ho,\nand Y.-D. Huang,",
          "13": "emotion\nrecognition\nusing\nspatial-temporal\ngraph\nconvolutional\nlstm"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Development and validation of an eeg-based real-time emotion recog-",
          "13": "IEEE Journal of Biomedical and Health\nwith attention mechanism,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "nition\nsystem using\nedge\nai\ncomputing\nplatform with\nconvolutional",
          "13": "Informatics,\nvol.\n26,\nno.\n11,\npp.\n5406–5417,\nnov\n2022.\n[Online]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "neural network system-on-chip design,” IEEE Journal on Emerging and",
          "13": "Available: https://doi.org/10.1109/JBHI.2022.3198688"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Selected Topics\nin Circuits and Systems, vol. 9, no. 4, pp. 645–657,",
          "13": "[44] M. Soleymani,\nJ. Lichtenauer, T. Pun, and M. Pantic, “A multimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2019.",
          "13": "database for affect recognition and implicit\ntagging,” IEEE transactions"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[23] A. Gu\nand T. Dao,\n“Mamba: Linear-time\nsequence modeling with",
          "13": "on affective computing, vol. 3, no. 1, pp. 42–55, 2011."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "selective state spaces,” arXiv preprint arXiv:2312.00752, 2023.",
          "13": "[45] M.\nJin,\nE.\nZhu,\nC.\nDu,\nH.\nHe,\nand\nJ.\nLi,\n“Pgcn:\nPyramidal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "graph convolutional network for\neeg emotion recognition,”\nfeb 2023,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[24] X.\nZhou\nand\nX.\nPeng,\n“Multi-scale\nspatiotemporal\nrepresenta-",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv\npreprint",
          "13": "submitted on 6 Feb 2023 (v1),\nlast\nrevised 24 Feb 2023 (v1).\n[Online]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion\nlearning\nfor\neeg-based\nemotion\nrecognition,”",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "Available: https://arxiv.org/abs/2302.02520"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv:2409.07589, 2024.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "[46] Y. Ding, N. Robinson, S. Zhang, Q. Zeng, and C. Guan, “Tsception:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[25] H. Wang, L. Xu, Y. Yu, W. Ding, and Y. Xu, “Global context mambav-",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "Capturing temporal dynamics and spatial asymmetry from eeg for emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ision for eeg-based emotion recognition,” pp. 1–5, 2025.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "tion recognition,” IEEE Transactions on Affective Computing, vol. 14,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[26] W.-L. Zheng and B.-L. Lu, “Investigating critical\nfrequency bands and",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "no. 3, pp. 2238–2250, 2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "channels for eeg-based emotion recognition with deep neural networks,”",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "[47] M. Soleymani, S. Asghari-Esfeden, Y. Fu, and M. Pantic, “Analysis of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Transactions on Autonomous Mental Development, vol. 7, no. 3,",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "eeg signals\nand facial\nexpressions\nfor\ncontinuous\nemotion detection,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pp. 162–175, 2015.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "IEEE Transactions on Affective Computing, pp. 1–13, 2016, extended"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[27] X. Hu,\nF. Wang,\nand D. Zhang,\n“Similar\nbrains\nblend\nemotion\nin",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "version of previously published work [11]; Database: MAHNOB-HCI"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "similar ways: Neural representations of individual difference in emotion",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "(http://mahnob-db.eu/hci-tagging/)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "profiles,” Neuroimage, vol. 247, p. 118819, 2022.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "[48]\nS.\nZhang,\nC.\nTang,\nand\nC.\nGuan,\n“Visual-to-eeg\ncross-modal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[28]\nJ. Chen, X. Wang, C. Huang, X. Hu, X. Shen,\nand D. Zhang,\n“A",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "knowledge\ndistillation\nfor\ncontinuous\nemotion\nrecognition,” Pattern"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "large finer-grained\naffective\ncomputing\neeg\ndataset,”\nScientific Data,",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "Recognition,\nvol.\n130,\np.\n108833,\n2022.\n[Online]. Available:\nhttps:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol.\n10,\np.\n740,\n2023.\n[Online]. Available:\nhttps://doi.org/10.1038/",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "//www.elsevier.com/locate/patcog"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "s41597-023-02650-w",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "[49]\nF. Wang, W. Zhang, Z. Xu, J. Ping, and H. Chu, “A deep multi-source"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[29]\nT. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "adaptation transfer network for cross-subject electroencephalogram emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dynamical graph convolutional neural networks,” IEEE Transactions on",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "tion\nrecognition,” Neural Computing\nand Applications,\nvol.\n33,\npp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Affective Computing, vol. 11, no. 3, pp. 532–541, 2020.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "9061–9073, 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[30] Y. Xin,\nJ. Zhang, Z. Zheng, Y. Wang, W. Dai, C. Li,\nJ. Zou,\nand",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "[50] Y. Ding, C. Tong, S. Zhang, M. Jiang, Y. Li, K. J. Lim, and C. Guan,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "H. Xiong,\n“Chebynet: Boosting neural network fitting and efficiency",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "“Emt: A novel\ntransformer\nfor generalized cross-subject\neeg emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "through\nchebyshev\npolynomial\nlayer\nconnections,”\n2024.\n[Online].",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "IEEE Transactions\non Neural Networks\nand\nLearning\nrecognition,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Available: https://openreview.net/forum?id=17U3nlco2r",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "Systems, vol. 36, no. 6, pp. 10 381–10 393, 2025."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[31]\nP. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition using",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "[51]\nS. M. Alarcao\nand M.\nJ. Fonseca,\n“Emotions\nrecognition using eeg"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Transactions\non Affective\nregularized\ngraph\nneural\nnetworks,”",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "signals: A survey,” IEEE transactions on affective computing, vol. 10,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Computing, vol. 13, no. 3, pp. 1290–1301, 2020.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "no. 3, pp. 374–393, 2017."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[32] Y. Li, W. Zheng, Y. Zong, Z. Cui, T. Zhang,\nand X. Zhou,\n“A bi-",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "[52]\nT. Song, S. Liu, W. Zheng, Y. Zong, Z. Cui, Y. Li,\nand X. Zhou,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "hemisphere domain adversarial neural network model\nfor eeg emotion",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "“Variational\ninstance-adaptive graph for eeg emotion recognition,” IEEE"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition,” IEEE Transactions on Affective Computing, vol. 12, no. 2,",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "Trans. Affect. Comput., vol. 14, no. 1, p. 343–356, Jan. 2023.\n[Online]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pp. 494–504, 2021.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "Available: https://doi.org/10.1109/TAFFC.2021.3064940"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[33] C.\nLu,\nX.\nZhou,\nY\n.\nDing,\nZ.\nKun, W.\nZiyu,\nJ.\nYinglu,\nand",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "[53] W. Wan, X. Cui, Z. Gao,\nand Z. Gu,\n“Frontal\neeg-based multi-level"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Y\n. Kun,\n“A comprehensive\nsurvey on eeg-based emotion recognition:",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "attention\nstates\nrecognition\nusing\ndynamic\ncomplexity\nand\nextreme"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A graph-based\nperspective,”\narXiv,\n2024.\n[Online]. Available:\nhttps:",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "Frontiers\ngradient\nboosting,”\nin Human Neuroscience,\nvol.\n15,\np."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "//arxiv.org/abs/2408.06027",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "673756,\njun 2021.\n[Online]. Available: https://doi.org/10.3389/fnhum."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[34]\nT. Zhang, X. Wang, X. Xu, and C. P. Chen, “Gcb-net: Graph convolu-",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "2021.673756"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tional broad network and its application in emotion recognition,” IEEE",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "[54] N.\nK.\nAl-Qazzaz,\nM.\nK.\nSabir,\nS.\nH.\nB.\nM.\nAli,\nS.\nA."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transactions on Affective Computing, vol. 13, no. 1, pp. 379–388, 2019.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "Ahmad, and K. Grammer, “Electroencephalogram profiles for emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[35] H. Wu, T. Hu, Y. Liu, H. Zhou,\nJ. Wang,\nand M. Long,\n“Timesnet:",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "identification\nover\nthe\nbrain\nregions\nusing\nspectral,\nentropy\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Temporal\n2d-variation modeling\nfor\ngeneral\ntime\nseries\nanalysis,”",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "temporal\nbiomarkers,”\nSensors,\nvol.\n20,\nno.\n1,\np.\n59,\ndec\n2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv, 2023.\n[Online]. Available: https://arxiv.org/abs/2210.02186",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "[Online]. Available: https://doi.org/10.3390/s20010059"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[36] H. Wang, L. Xu, A. Bezerianos, C. Chen,\nand Z. Zhang,\n“Linking",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "attention-based multiscale cnn with dynamical gcn for driving fatigue",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Transactions\non\nInstrumentation\ndetection,”\nand Measurement,",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 70, pp. 1–11, 2020.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[37]\nP. Chen, Y. Zhang, Y. Cheng, Y. Shu, Y. Wang, Q. Wen, B. Yang, and",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "C. Guo, “Pathformer: Multi-scale transformers with adaptive pathways",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for\ntime series forecasting,” arXiv preprint arXiv:2402.05956, 2024.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[38]\nS.\nZhang,\nC.\nTang,\nand\nC.\nGuan,\n“Visual-to-eeg\ncross-modal",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "knowledge\ndistillation\nfor\ncontinuous\nemotion\nrecognition,” Pattern",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Recognition,\nvol.\n130,\np.\n108833,\n2022.\n[Online]. Available:\nhttps:",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "//www.sciencedirect.com/science/article/pii/S0031320322003144",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[39]\nF. Wang, W. Zhang, Z. Xu, J. Ping, and H. Chu, “A deep multi-source",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "adaptation transfer network for cross-subject electroencephalogram emo-",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion recognition,” Neural Computing and Applications, 2021, published",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "online: 16 January 2021.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[40] W. Y. Yan, A.\nShaker,\nand W. Zou,\n“Panchromatic\nikonos\nimage",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2009\nIEEE Toronto\nclassification\nusing wavelet\nbased\nfeatures,”\nin",
          "13": ""
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Time-frequency representation and convolutional neural network-based emotion recognition",
      "authors": [
        "S Khare",
        "V Bajaj"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "2",
      "title": "Eeg-based emotion recognition using multi-scale dynamic cnn and gated transformer",
      "authors": [
        "Z Cheng",
        "X Bu",
        "Q Wang",
        "T Yang",
        "J Tu"
      ],
      "year": "2024",
      "venue": "Scientific Reports",
      "doi": "10.1038/s41598-024-82705-z"
    },
    {
      "citation_id": "3",
      "title": "A transformer based neural network for emotion recognition and visualizations of crucial eeg channels",
      "authors": [
        "J Guo"
      ],
      "year": "2022",
      "venue": "Physica A: Statistical Mechanics and its Applications"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition in eeg signals using deep learning methods: A review",
      "authors": [
        "M Jafari"
      ],
      "year": "2023",
      "venue": "Comput Biol Med"
    },
    {
      "citation_id": "5",
      "title": "Multi-periodicity dependency transformer based on spectrum offset for radio frequency fingerprint identification",
      "authors": [
        "J Xiao",
        "W Ding",
        "D Zhang",
        "Y Ma",
        "Y Wang",
        "Z Shao",
        "J Wang"
      ],
      "year": "2025",
      "venue": "Measurement"
    },
    {
      "citation_id": "6",
      "title": "Transformers in eeg analysis: A review of architectures and applications in motor imagery, seizure, and emotion classification",
      "authors": [
        "E Vafaei",
        "M Hosseini"
      ],
      "year": "2025",
      "venue": "Sensors",
      "doi": "10.3390/s25051293"
    },
    {
      "citation_id": "7",
      "title": "Emotion classification based on transformer and cnn for eeg spatial-temporal feature learning",
      "authors": [
        "X Yao",
        "T Li",
        "P Ding",
        "F Wang",
        "L Zhao",
        "A Gong",
        "W Nan",
        "Y Fu"
      ],
      "year": "2024",
      "venue": "Brain Sci",
      "doi": "10.3390/brainsci14030268"
    },
    {
      "citation_id": "8",
      "title": "A comprehensive survey on eeg-based emotion recognition: A graphbased perspective",
      "authors": [
        "C Liu",
        "X Zhou",
        "Y Wu",
        "Y Ding",
        "L Zhai",
        "K Wang",
        "Z Jia",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "A comprehensive survey on eeg-based emotion recognition: A graphbased perspective",
      "arxiv": "arXiv:2408.06027"
    },
    {
      "citation_id": "9",
      "title": "Review on emotion recognition based on electroencephalography",
      "authors": [
        "H Liu",
        "Y Zhang",
        "Y Li",
        "X Kong"
      ],
      "year": "2021",
      "venue": "Frontiers in Computational Neuroscience"
    },
    {
      "citation_id": "10",
      "title": "Multi-source eeg emotion recognition via dynamic contrastive domain adaptation",
      "authors": [
        "Y Xiao",
        "Y Zhang",
        "X Peng",
        "S Han",
        "X Zheng",
        "D Fang",
        "X Chen"
      ],
      "year": "2024",
      "venue": "Multi-source eeg emotion recognition via dynamic contrastive domain adaptation",
      "arxiv": "arXiv:2408.10235"
    },
    {
      "citation_id": "11",
      "title": "A comparative study of wavelet families for schizophrenia detection",
      "authors": [
        "E Sathiya",
        "T Rao",
        "T Kumar"
      ],
      "year": "2024",
      "venue": "Frontiers in Human Neuroscience",
      "doi": "10.3389/fnhum.2024.1463819"
    },
    {
      "citation_id": "12",
      "title": "Integration of stationary wavelet transform on a dynamic partial reconfiguration for recognition of pre-ictal gamma oscillations",
      "authors": [
        "N Jmail",
        "M Zaghdoud",
        "A Hadriche",
        "T Frikha",
        "C Amar",
        "C Bénar"
      ],
      "year": "2018",
      "venue": "Heliyon",
      "doi": "10.1016/j.heliyon.2018.e00530"
    },
    {
      "citation_id": "13",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Analysis of eeg signals and facial expressions for continuous emotion detection",
      "authors": [
        "M Soleymani",
        "S Asghari-Esfeden",
        "Y Fu",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Bridge graph attention based graph convolution network with multi-scale transformer for eeg emotion recognition",
      "authors": [
        "H Yan",
        "K Guo",
        "X Xing",
        "X Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Masatcn: Multi-anchor space-aware temporal convolutional neural networks for continuous and discrete eeg emotion recognition",
      "authors": [
        "Y Ding",
        "N Robinson",
        "S Zhang",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2023",
      "venue": "Masatcn: Multi-anchor space-aware temporal convolutional neural networks for continuous and discrete eeg emotion recognition",
      "arxiv": "arXiv:2308.16207"
    },
    {
      "citation_id": "17",
      "title": "Hierarchical spatiotemporal electroencephalogram feature learning and emotion recognition with attention-based antagonism neural network",
      "authors": [
        "P Zhang",
        "C Min",
        "K Zhang",
        "W Xue",
        "J Chen"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "18",
      "title": "Hierarchical dynamic graph convolutional network with interpretability for eeg-based emotion recognition",
      "authors": [
        "M Ye",
        "C Chen",
        "T Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "19",
      "title": "Lggnet: Learning from local-global-graph representations for brain-computer interface",
      "authors": [
        "Y Ding",
        "N Robinson",
        "C Tong",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "20",
      "title": "Adaptive hierarchical graph convolutional network for eeg emotion recognition",
      "authors": [
        "Y Xue",
        "W Zheng",
        "Y Zong",
        "H Chang",
        "X Jiang"
      ],
      "year": "2022",
      "venue": "2022 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "21",
      "title": "A two-stage efficient 3-d cnn framework for eeg based emotion recognition",
      "authors": [
        "N Ye Qiao",
        "Mohammed Alnemari"
      ],
      "year": "2022",
      "venue": "A two-stage efficient 3-d cnn framework for eeg based emotion recognition",
      "arxiv": "arXiv:2208.00883"
    },
    {
      "citation_id": "22",
      "title": "Development and validation of an eeg-based real-time emotion recognition system using edge ai computing platform with convolutional neural network system-on-chip design",
      "authors": [
        "W.-C Fang",
        "K.-Y Wang",
        "N Fahier",
        "Y.-L Ho",
        "Y.-D Huang"
      ],
      "year": "2019",
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems"
    },
    {
      "citation_id": "23",
      "title": "Mamba: Linear-time sequence modeling with selective state spaces",
      "authors": [
        "A Gu",
        "T Dao"
      ],
      "year": "2023",
      "venue": "Mamba: Linear-time sequence modeling with selective state spaces",
      "arxiv": "arXiv:2312.00752"
    },
    {
      "citation_id": "24",
      "title": "Multi-scale spatiotemporal representation learning for eeg-based emotion recognition",
      "authors": [
        "X Zhou",
        "X Peng"
      ],
      "year": "2024",
      "venue": "Multi-scale spatiotemporal representation learning for eeg-based emotion recognition",
      "arxiv": "arXiv:2409.07589"
    },
    {
      "citation_id": "25",
      "title": "Global context mambavision for eeg-based emotion recognition",
      "authors": [
        "H Wang",
        "L Xu",
        "Y Yu",
        "W Ding",
        "Y Xu"
      ],
      "year": "2025",
      "venue": "Global context mambavision for eeg-based emotion recognition"
    },
    {
      "citation_id": "26",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "27",
      "title": "Similar brains blend emotion in similar ways: Neural representations of individual difference in emotion profiles",
      "authors": [
        "X Hu",
        "F Wang",
        "D Zhang"
      ],
      "year": "2022",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "28",
      "title": "A large finer-grained affective computing eeg dataset",
      "authors": [
        "J Chen",
        "X Wang",
        "C Huang",
        "X Hu",
        "X Shen",
        "D Zhang"
      ],
      "year": "2023",
      "venue": "Scientific Data",
      "doi": "10.1038/s41597-023-02650-w"
    },
    {
      "citation_id": "29",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Chebynet: Boosting neural network fitting and efficiency through chebyshev polynomial layer connections",
      "authors": [
        "Y Xin",
        "J Zhang",
        "Z Zheng",
        "Y Wang",
        "W Dai",
        "C Li",
        "J Zou",
        "H Xiong"
      ],
      "year": "2024",
      "venue": "Chebynet: Boosting neural network fitting and efficiency through chebyshev polynomial layer connections"
    },
    {
      "citation_id": "31",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "A bihemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "A comprehensive survey on eeg-based emotion recognition: A graph-based perspective",
      "authors": [
        "C Lu",
        "X Zhou",
        "Y Ding",
        "Z Kun",
        "W Ziyu",
        "J Yinglu",
        "Y Kun"
      ],
      "year": "2024",
      "venue": "arXiv"
    },
    {
      "citation_id": "34",
      "title": "Gcb-net: Graph convolutional broad network and its application in emotion recognition",
      "authors": [
        "T Zhang",
        "X Wang",
        "X Xu",
        "C Chen"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Timesnet: Temporal 2d-variation modeling for general time series analysis",
      "authors": [
        "H Wu",
        "T Hu",
        "Y Liu",
        "H Zhou",
        "J Wang",
        "M Long"
      ],
      "year": "2023",
      "venue": "arXiv"
    },
    {
      "citation_id": "36",
      "title": "Linking attention-based multiscale cnn with dynamical gcn for driving fatigue detection",
      "authors": [
        "H Wang",
        "L Xu",
        "A Bezerianos",
        "C Chen",
        "Z Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "37",
      "title": "Pathformer: Multi-scale transformers with adaptive pathways for time series forecasting",
      "authors": [
        "P Chen",
        "Y Zhang",
        "Y Cheng",
        "Y Shu",
        "Y Wang",
        "Q Wen",
        "B Yang",
        "C Guo"
      ],
      "year": "2024",
      "venue": "Pathformer: Multi-scale transformers with adaptive pathways for time series forecasting",
      "arxiv": "arXiv:2402.05956"
    },
    {
      "citation_id": "38",
      "title": "Visual-to-eeg cross-modal knowledge distillation for continuous emotion recognition",
      "authors": [
        "S Zhang",
        "C Tang",
        "C Guan"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "39",
      "title": "A deep multi-source adaptation transfer network for cross-subject electroencephalogram emotion recognition",
      "authors": [
        "F Wang",
        "W Zhang",
        "Z Xu",
        "J Ping",
        "H Chu"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "40",
      "title": "Panchromatic ikonos image classification using wavelet based features",
      "authors": [
        "W Yan",
        "A Shaker",
        "W Zou"
      ],
      "year": "2009",
      "venue": "2009 IEEE Toronto International Conference Science and Technology for Humanity"
    },
    {
      "citation_id": "41",
      "title": "Eeg conformer: Convolutional transformer for eeg decoding and visualization",
      "authors": [
        "Y Song",
        "Q Zheng",
        "B Liu",
        "X Gao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering",
      "doi": "10.1109/TNSRE.2022.3230250"
    },
    {
      "citation_id": "42",
      "title": "Astdf-net: attentionbased spatial-temporal dual-stream fusion network for eeg-based emotion recognition",
      "authors": [
        "P Gong",
        "Z Jia",
        "P Wang",
        "Y Zhou",
        "D Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM international conference on multimedia"
    },
    {
      "citation_id": "43",
      "title": "Eeg-based emotion recognition using spatial-temporal graph convolutional lstm with attention mechanism",
      "authors": [
        "L Feng",
        "C Chen",
        "M Zhao",
        "H Deng",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/JBHI.2022.3198688"
    },
    {
      "citation_id": "44",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "45",
      "title": "Pgcn: Pyramidal graph convolutional network for eeg emotion recognition",
      "authors": [
        "M Jin",
        "E Zhu",
        "C Du",
        "H He",
        "J Li"
      ],
      "year": "2023",
      "venue": "Pgcn: Pyramidal graph convolutional network for eeg emotion recognition"
    },
    {
      "citation_id": "46",
      "title": "Tsception: Capturing temporal dynamics and spatial asymmetry from eeg for emotion recognition",
      "authors": [
        "Y Ding",
        "N Robinson",
        "S Zhang",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "47",
      "title": "Analysis of eeg signals and facial expressions for continuous emotion detection",
      "authors": [
        "M Soleymani",
        "S Asghari-Esfeden",
        "Y Fu",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Database: MAHNOB-HCI"
    },
    {
      "citation_id": "48",
      "title": "Visual-to-eeg cross-modal knowledge distillation for continuous emotion recognition",
      "authors": [
        "S Zhang",
        "C Tang",
        "C Guan"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "49",
      "title": "A deep multi-source adaptation transfer network for cross-subject electroencephalogram emotion recognition",
      "authors": [
        "F Wang",
        "W Zhang",
        "Z Xu",
        "J Ping",
        "H Chu"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "50",
      "title": "Emt: A novel transformer for generalized cross-subject eeg emotion recognition",
      "authors": [
        "Y Ding",
        "C Tong",
        "S Zhang",
        "M Jiang",
        "Y Li",
        "K Lim",
        "C Guan"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "51",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "52",
      "title": "Variational instance-adaptive graph for eeg emotion recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "Y Li",
        "X Zhou"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2021.3064940"
    },
    {
      "citation_id": "53",
      "title": "Frontal eeg-based multi-level attention states recognition using dynamic complexity and extreme gradient boosting",
      "authors": [
        "W Wan",
        "X Cui",
        "Z Gao",
        "Z Gu"
      ],
      "year": "2021",
      "venue": "Frontiers in Human Neuroscience",
      "doi": "10.3389/fnhum.2021.673756"
    },
    {
      "citation_id": "54",
      "title": "Electroencephalogram profiles for emotion identification over the brain regions using spectral, entropy and temporal biomarkers",
      "authors": [
        "N Al-Qazzaz",
        "M Sabir",
        "S Ali",
        "S Ahmad",
        "K Grammer"
      ],
      "year": "2019",
      "venue": "Sensors",
      "doi": "10.3390/s20010059"
    }
  ]
}