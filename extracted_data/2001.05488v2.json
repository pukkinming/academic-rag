{
  "paper_id": "2001.05488v2",
  "title": "Eev: A Large-Scale Dataset For Studying Evoked Expressions From Video",
  "published": "2020-01-15T18:59:51Z",
  "authors": [
    "Jennifer J. Sun",
    "Ting Liu",
    "Alan S. Cowen",
    "Florian Schroff",
    "Hartwig Adam",
    "Gautam Prasad"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Video Content Expressions Evoked by Video Content (15 total) Plus Others Time (seconds) Figure 1: The EEV dataset contains publicly available content videos alongside detected viewer facial expressions while viewers are watching the video. There are 15 expression labels annotated at 6Hz over each content video.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Videos can be described by their semantic content and affective content. The semantic content focuses on \"What is in the video?\", while the affective content focuses on \"What does the video make people feel?\"  [43] . Our work focuses on understanding the affective content evoked by visual and audio information from videos. We introduce a scalable framework to annotate viewer expressions evoked by video content, and release the Evoked Expressions from Videos (EEV) dataset to study these evoked expressions.\n\nRecent studies have shown promising results in describing videos using their semantic content, and in parallel, we would like to work on understanding how videos evoke different affect in viewers. For semantic content, video datasets such as Sports1M  [25] , Kinetics  [9] , YouTube8M [3], and Moments in Time  [37]  have provided benchmarks for performance and enabled advances in semantic video understanding. Currently, datasets for studying viewer affective responses to video content are much smaller  [28, 7, 44]  and typically focus on one video type, such as films. These limitations can be attributed to the challenge of collecting frame-level affective labels in videos. It is difficult to find reliably affect-evoking stimuli  [20]  and affective labels are more subjective given they depend on viewer background and context  [43] .\n\nTo build a large-scale dataset for understanding affec-tive content, we introduce a scalable method for annotating viewer expressions evoked by video content by using a video-based model to annotate facial expressions in our data collection pipeline. The expression recognition model we use is shown to have higher correlation to the population average than single human annotators (Section 4.2). We use publicly available videos with associated viewer reactions to generate evoked expression labels. The result is the EEV dataset, with 23,547 videos densely annotated at 6 Hz for a total of 36.7 million annotations. The diversity of our dataset is shown in Figure  2 . To the best of our knowledge, the EEV dataset is currently the largest dataset for studying affective responses to video from evoked viewer facial expressions.\n\nThe size and diversity of our dataset enables us to study unique questions related to affective content. These include: how well can we predict evoked viewer facial expressions directly from video content?; are some evoked facial expressions easier to predict?; and how well do video themes correspond to different expressions? We explore these areas by analyzing the characteristics of the EEV dataset and establishing baseline expression prediction benchmarks.\n\nOur contributions in this paper are:\n\n• The Evoked Expressions from Videos (EEV) dataset, which is a large-scale dataset annotated at 6 Hz for studying evoked expressions from diverse videos. There are 15 annotated expressions. A subset of EEV is publicly available at https://github.com/googleresearch-datasets/eev.\n\n• A scalable method for annotating evoked viewer facial expressions in online videos.\n\n• A baseline model based on  [46]  and performance benchmark for predicting evoked expressions using the EEV dataset.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Affective Video Content Analysis",
      "text": "Our work is closely related to the field of affective video content analysis, which aims to predict viewer reactions evoked by videos  [5, 50] . Hanjalic and Xu  [19]  proposed one of the earliest efforts in the field, mapping movies to continuously evoked valence and arousal. Often, both viewer and video information are incorporated to predict video affect  [42, 44, 28, 8, 44] . In these works, physiological signals are measured from a viewer as they watch the video, and combined with visual features for prediction. Other studies  [7, 22, 51, 53]  use the direct approach: they aim to predict viewer responses directly from video content. Given that we do not use any additional viewer information, our work most closely relates to the direct approach. Figure  2 : Video themes in EEV annotated using an automatic system described in section 3.3. The themes are not mutually exclusive. Models in this area are often multimodal, given that both visual and auditory features contribute to a video's affective content  [49] . A review paper  [50]  observed that modeling frameworks often consist of video feature extraction, modality fusion, and classification or regression (depending on the model of affect). Recent models generally follow this framework using neural networks  [38, 30, 46, 4, 6, 24] . In particular, the LIRIS-ACCEDE dataset  [6, 7] , part of the MediaEval benchmark  [13]  provides a way to compare model performances for affective content analysis. The top performing models  [30, 46]  in the MediaEval competition applied RNNs to model the temporal aspects of video with a multi-modal approach. We establish the performance baseline on the EEV dataset based on these top performing models from the MediaEval benchmark.\n\nFacial response to media . Videos have often been used to evoke viewer facial expressions in studies across psychology and affective computing  [11, 5, 20, 34, 41 ] (e.g. choos-",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Source",
      "text": "Annot. Type Annot. Freq Num Videos COGNIMUSE  [53]  movies affective labels (valence, arousal, etc.) frame 7 HUMAINE  [16]  selected clips affective labels (valence, arousal, etc.) global and frame 50 FilmStim  [39]  movies affective labels (valence, arousal, etc.) global and frame 70 DEAP  [28]  music videos affective labels (valence, arousal, etc.), face video frame 120 VideoEmotion  [22]  online videos discrete emotions global 1101 LIRIS-ACCEDE (discrete)  [6, 7]  movies valence, arousal frame 160 LIRIS-ACCEDE (MediaEval)  [6, 7]  movies valence, arousal frame 66 EEV (ours) online videos evoked expressions frame 23,574\n\nTable  1 : Dataset size comparison for predicting viewer affect from video content.\n\ning a funny video so that viewers laugh). Our corpus of videos can be used to identify content that evoke distinct facial expressions and thus may be useful for these studies. Facial expressions have been also used as predictors in studies on self-reported emotions  [41, 35] , facial landmark locations  [15]  and viewer video preferences  [52, 32, 33] . These studies have also used automated systems to annotate facial expressions in videos. However, rather than using facial expressions as the predictors, we predict evoked viewer facial expressions directly from online videos.\n\nFacial expression recognition. The expressions in the EEV dataset are annotated using a facial expression recognition model similar to  [48] , with expressions based on  [12] . Traditionally, automated methods predict facial action units from the facial action coding system (FACS)  [17, 31] . However, it remains difficult to obtain accurate FACS annotations in natural contexts, such as user uploaded videos, that have uncontrolled conditions of viewpoint, lighting, occlusion, and demographics. We instead apply a semantic space approach to classifying facial expressions, which was recently introduced by  [12] . This approach has revealed that in natural images, people reliably recognize a much wider array of facial expressions than have traditionally been studied using FACS. Information conveyed by facial expressions can be represented efficiently in terms of categories such as \"amusement\" and \"surprise\".",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets For Predicting Affective Responses",
      "text": "Datasets with viewer affective response annotations are important for affective video content analysis. The size of current datasets for predicting viewer affective response to video are small relative to the size of other video benchmark datasets [3,  25, 9] . Existing datasets such as DEAP  [28] , VideoEmotion  [22] , and FilmStim  [39]  are labelled with one annotation per video clip, and the largest (VideoEmotion) contains 1,101 videos. While these datasets are useful for other applications, they are too small to test complex models, and cannot be used to understand temporal changes in affect. Some datasets annotate evoked viewer affect over time, such as LIRIS-ACCEDE  [6, 7] , COGNIMUSE  [53] , and HUMAINE  [16] . These have frame level annotations based on viewer self-reports. The largest, LIRIS-ACCEDE  [6, 7] , consists of 160 films with annotations every second. A subset of LIRIS-ACCEDE (66 films) is annotated continuously with valence and arousal from self-report used in the MediaEval benchmark  [13] . These datasets often focus on one category of video (films or music videos).\n\nCompared to existing datasets, EEV is significantly larger, as shown in Table  1 . To achieve scale, we use a facial expression recognition model. Our evaluations in Section 4.2 shows that this model has higher correlations to the population mean than human raters on average. Although we use facial expressions instead of self-report, both methods for measuring affect have been found to share significant variance  [26] . The results by  [26]  also found that analyzing facial expressions can provide unique insights into affective experiences. Another characteristic of the EEV dataset is that it contains diverse video themes, shown in Figure  2 , enabling affective content to be studied across categories.\n\nSubjectivity. We recognize that there is a need for personalization in understanding affective content due to the fact that viewer response depends on the experience of the subject  [5, 50, 43] . However, as we observe and as argued by  [43] , affective response is not arbitrary, and often agreement can be found across viewers. The challenge, as identified by  [43] , is to recognize common affective triggers in videos. By providing the largest video dataset annotated with evoked viewer facial expressions, EEV enables this challenge to be explored further in the future.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dataset Analysis",
      "text": "The EEV dataset consists of 23,574 videos annotated at 6 Hz for a total of 36.7 million automatic annotations. The video length distribution is show in Figure  3  and there are a total of 1,700 hours. The annotations are over 15 expression classes from  [12] : amusement, anger, awe, concentration, confusion, contempt, contentment, disappointment, doubt,  elation, interest, pain, sadness, surprise, and triumph. Each annotation is a 15-dimensional vector with values ranging from 0 to 1, corresponding to the confidence of each expression.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Expressions In Eev",
      "text": "The expressions in EEV are based on  [11, 12] . The study on naturalistic facial expressions,  [12] , has a more detailed analysis of each expression. In particular, Table  2 , reproduced from with permission  [12] , shows the free-response terms from a user study most correlated with each expression.    [12] , which are also categories of evoked expressions studied in EEV.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evoked Expression Labels",
      "text": "Expression distribution. The distribution of each expression over videos is plotted in Figure  5 . We observe that the distribution varies across expressions. Some expressions, such as anger and contempt, have relatively rare high confidence samples, while other expressions, such as concentration and interest, have relatively common high confidence samples. This property makes EEV an interesting dataset for studying regression on labels of different distributions, or classification (by binarizing expressions using a threshold).\n\nExpression correlation. Using EEV, we can examine the correlation matrix of different evoked expressions in Figure  4 . There is a positive correlation between expressions associated with happiness, such as amusement, contentment and elation. A strong correlation exists between anger and contempt (associated with disapproval), as well as surprise and awe. For negative correlation, sadness is anti-correlated with interest and concentration. Contentment tends to be negatively correlated with concentration and interest is negatively correlated with disappointment. This correlation is reasonable given the association of each expression with different keywords provided in supplementary materials.\n\nDataset split. We split the EEV dataset at the video level with a roughly 60:15:25 split into 18,124 training videos, 4,530 validation videos and 7,551 test videos for machine learning applications. We verified that there are similar distributions of expressions in each dataset split. Challenges. The EEV dataset can be challenging because the viewer expression can depend on viewer background, external context and other information not present in the video from visual and audio based data. This is a challenge for directly predicting viewer response from general, \"inthe-wild\" videos. Despite this, we show that our baseline model can learn useful information from the EEV dataset using only video content in section 5.4.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Video Themes",
      "text": "Theme annotation. We characterize the EEV dataset in terms of video themes (key topics that can be used to describe the video). The video themes are obtained from the video annotation system described in  [1] . These annotations correspond to the Knowledge Graph entities [2] of the video, which are computed based on video content and metadata  [1] . We summarize each video into a set of video themes using the Knowledge Graph entities, similar to the approach used by YouTube8M [3]. This is so that we can better understand the video composition of the EEV dataset.\n\nThe distribution of the themes in EEV is shown in Figure  2 .\n\nEvoked Expressions and Themes. We sort each video theme with more than 50 videos by the evoked expressions, and obtain themes that are most and least associated with each expression. This result is shown in Table  3 . Notably, amusement occurs more in comedy as compared to horror or rapping. Sorting by anger or triumph results in sports and gameplay as two of the top three themes. There tends to be more evoked surprise in horror and lifestyle videos than comedy or singer videos. This association of themes and evoked viewer expressions can be useful for content creators to better understand the affective impact of their videos. Additionally, sorting videos by evoked expressions using EEV can be helpful in selecting videos to evoke specific affective signals for studies.\n\nWe hope these unique characteristics of the EEV dataset can encourage further studies in video understanding and affective computing.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Collection",
      "text": "The EEV dataset leverages facial expression recognition models in order to study affective response to videos at a large scale. The source of the EEV dataset is publicly available reaction videos, which depict viewers reacting to another video. The video that viewers are watching is typically another publicly available video, which we will call the content video. The facial expressions in the reaction video are used to generate evoked expression labels for the content video. An outline is illustrated in Figure  6 .\n\nTo start, we first compile a list of public reaction videos on an online video corpus by performing a crawl based on keywords in the video title (step 1 in Figure  6 ). The reaction videos contain the facial expression of the viewer overlaid on top of the video content they are watching. Using these reaction videos, we find the corresponding content video from the reaction video description (step 2 in Figure  6 ). The facial expressions of the viewer in the reaction video is annotated using the model described in Section 4.2 (step 3 in Figure  6 ). Then, the evoked expressions from the reaction   video is paired with the content video at each timestamp using SIFT features  [29]  as the facial reactions and content video frame is synchronized (step 4 in Figure  6 ). The final dataset consists of the content videos with their evoked expressions from viewer reactions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Facial Expressions",
      "text": "Our facial expression categories are based on the work of  [11, 12] . By analyzing the self-report of viewers from 2,185 emotionally evocative videos,  [11]  suggests that reported user experiences are better captured by emotion categories rather than dimensional labels. This work defines 27 emotion categories, which they found to be linked by smooth gradients. A followup work  [12]  examined 28 expression categories that are expressed by face and body us-ing human annotations. The facial expression categories in EEV are based on this work. In our approach, we focus on 16 of the 28 expressions in  [12]  for which our automatic annotation model performs well. A detailed description of each expression is in supplementary materials and  [12] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Facial Expression Annotation",
      "text": "EEV is annotated using a facial expression recognition model. We evaluate the automatic method against human annotator agreement on the Berkeley faces dataset  [12] .\n\nAnnotation Model. The facial expression recognition model, similar to  [48] , uses facial movements over 3 seconds at 6 Hz to predict facial expression labels at each video  In this step, we first compute face-based features using the NN2 FaceNet architecture  [40]  from input video frames. Specifically, we extract the inception (5a) block with a 7x7 feature map composed of 1,024 channels fed into a 7x7 average pooling layer to create a 1,024 dimensional feature vector representing a single face at a single time point in a video. We feed the face features computed over a given video segment into two long short-term memory (LSTM) layers each with 64 recurrent cells to capture temporal information. The output of the LSTM is then fed through a mixture of experts model (2 mixtures in addition to the dummy expert). The network is trained on 274K ratings of 187K clips of faces from a public video corpus. The data is manually annotated by human raters who selected all facial expression categories that applied to each face.\n\nComparing to Human Annotators. To evaluate the accuracy of the annotation model on natural expressions, we compare them to human judgments of faces in a held-out test set from the dataset introduced by  [12] . We use Pearson's Correlation Coefficient, which varies from -1 (anticorrelated) to 1 (correlated). The results are in Figure  7a  for each expression. We see that the accuracy levels of our model annotations exceed those of single rater human annotations on natural expressions compared with the average human rating in general. Correlations are shown across different demographics groups in Figure  7b . Importantly, the prediction correlations for our model were similar for faces of different ethnicities, genders, and age groups.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Baseline Model",
      "text": "Our evoked expression prediction model is based on  [46] . Our goal is to produce performance baselines for EEV from on an existing model benchmarked on the LIRIS-ACCEDE test set. The LIRIS-ACCEDE dataset  [6, 7] , part of the MediaEval benchmark  [13] , provides a way to compare model performances for affective content analysis. The model that achieved top performance on the MediaEval benchmark task  [46]  uses a combination of pre-computed features across multiple modalities, gated recurrent units (GRUs)  [10] , and a mixture of experts (MOE). We use a similar architecture in our experiments. This architecture has also been studied by  [36]  and performed well on the YouTube8M dataset [3]. Figure  8  presents an overview.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Feature Extraction",
      "text": "Videos present information to viewers through multiple modalities and a multi-modal approach will be needed to understand viewer response  [49] . We leverage information from image, face, and audio modalities by extracting frame level features for each second in the videos.\n\nImage features. For the image features, we read the video frames at 1 Hz. We then feed the frames into the Inception V3 architecture  [47]  trained on ImageNet  [14] . We extract the ReLu activation in the last hidden layer, resulting in a 2048-D feature vector for the image input.\n\nFace features. For the face features, we use the same extracted image, but focus on the two largest faces in the image. The face features are extracted using an Inception- based model trained on faces  [40] . This process results in another 2,048-D feature vector based on the face. We pad the face feature vector with zeros if less than two faces are detected in the image.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Audio Features.",
      "text": "The audio feature extraction is based on the VGG-style model provided by AudioSet  [18]  trained on a preliminary version of YouTube8M [3]. In particular, we extract audio at 16kHz mono and followed the method from AudioSet to compute the log mel-spectrogram. The 96 × 64 log mel spectrogram is then fed into the VGG-style model, which outputs 128-D embeddings.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Model Architecture",
      "text": "Temporal model. Each feature extracted above (image, face, audio) is fed into its own subnetwork consisting of GRUs. We use GRUs in order to take into account the temporal characteristics of video. The features are extracted at 1 Hz and fed into their respective GRU with a sequence length of 60. The outputs of the final state from each GRU are then concatenated and the fused vector is fed into the regression model.\n\nRegression model. We use context gating  [36]  to weigh the input features before feeding the representation into an MOE model  [23] . Context gating introduces gating vectors based on sigmoid activations that are capable of capturing dependencies between input features (input gates) and output labels (output gates). The output then goes through another context gating transformation for the final prediction for each expression.\n\nImplementation Details. Our models are trained using the Adam optimizer  [27]  with a mini-batch size of 128. We use a learning rate of 0.0005. We use gradient clipping when training the network to mitigate potential exploding gradient problems. For the GRU, we apply dropout  [45]   0.3 in each layer. For the context gating implementation, we apply batch normalization  [21]  before the non-linear layer.\n\nThe models are implemented in TensorFlow.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Datasets",
      "text": "EEV dataset. On the EEV dataset, we perform regression of each expression at 1 Hz. This is consistent with the labels in the LIRIS-ACCEDE dataset. We measure performance using Pearson's Correlation Coefficient (correlation). We report the average correlation over expressions.\n\nLIRIS-ACCEDE dataset. This dataset is chosen because it is the largest dataset that focuses on a similar task to the EEV dataset -predicting the impact of movies on viewers.\n\nThe LIRIS-ACCEDE dataset  [6]  is annotated each second with self-reported valence and arousal values. Each of the two dimensions is a continuous value in [-1, 1]. We use the same metrics as the dataset competition  [13]  to measure performance: mean squared error (MSE) and Pearson's Correlation Coefficient (correlation). The LIRIS-ACCEDE dataset is divided into three parts during download. We report performance on the first part (14 movies) and the rest (40 movies) for training.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results On Eev",
      "text": "We summarize the results of the EEV dataset in Table  4 . From the summary table, we see that we can learn a positive correlation using all combination of modalities. This result demonstrates that information for predicting affective responses can be learned from the EEV dataset, despite challenges outlined in section 3. Our observations align with the statement from  [43] , in that affective response is not arbitrary and there is video content that can evoke consistent viewer responses.\n\nThe result for each expression is in Table  5 . The model is able to predict some expressions, such as amusement, concentration, and surprise, with higher correlation than others, such as anger, and triumph. Although our correlation is low for some expressions, we note that the correlation for each expression is positive, which suggests that information can be learned from the video content to predict each individual expression.\n\nFeature ablation. We use the same architecture described in Figure  8  for our ablation study and selectively remove input features to test performance for different feature combinations. From Table  4 , we see that Image+Face+Audio performs similar to Image+Audio (without face features). Additionally, face features alone has the lowest correlation in the single modality tests. These observations suggest that the face modality may be less informative for evoked expressions than image or audio features. Comparing the single modality tests, we see that audio features has the highest correlation. We can further observe that by dropping the audio features from Image+Face+Audio, we experience the biggest decrease in correlation. These results show that adding audio features is helpful if we only have image or face features.\n\nFrom Table  5 , we see that removing audio features from Image+Face+Audio lowers the correlation of almost all expressions. In particular, the biggest decreases in performance can be observed for concentration, contentment, and disappointment. In contrast, by removing only image features or only face features from the full set, only the correlation of some expressions are lowered. Since the faces are extracted from the image, it is very likely that there are redundant signals in the remaining face or image feature, so the correlations are not lowered as much. When we remove both image and face features and use audio features alone, the correlations are generally lower for all expressions.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Results On Liris-Accede",
      "text": "The results on the LIRIS-ACCEDE dataset are in Table 6. We experiment with training from scratch and finetuning from a checkpoint pre-trained on EEV. We use the full feature version of the reaction prediction model with Image+Face+Audio and change the final regression layer to 2 dimensions for valence and arousal. This is to test the ability of EEV to be used for transfer learning in similar Table  6 : Results of the baseline model on LIRIS-ACCEDE validation set, using all input features. The mean squared error (MSE) and Correlation metrics preceded with \"V\" corresponds to the valence metrics, while the metrics preceded with \"A\" corresponds to the arousal metrics. We show results without (first row) and with (second row) transfer learning from EEV.\n\ntasks.\n\nWe observe that the transfer learning model has lower MSE and higher correlation than the model trained from LIRIS only. The improvements to valence metrics are larger than the improvements to arousal. This suggests that pretraining on the EEV data is helpful in improving performance for related tasks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "We introduce the EEV dataset, a large-scale video dataset for studying evoked viewer facial expressions from video content. EEV is larger and more diverse than previous video datasets for studying viewer reactions. Baseline model performance for predicting evoked expressions shows that while affective information can be learned from video content, there remains a lot of potential for improvement. We hope that the EEV dataset will be useful in developing novel models for affective computing and video analysis. The frame-level evoked expression labels released with EEV can be used to train complex models. Furthermore, we have shown the potential for EEV to be used for transfer learning on related tasks, such as viewer reaction prediction using valence and arousal.\n\nIn addition to training new models, the evoked expression labels in EEV can be used to search for themes and videos corresponding to specific expressions. This is useful for finding videos to evoke facial expressions for experiments in psychology and affective computing. Additionally, understanding the distribution of evoked expressions in different themes can help video recommendation systems better cater content to users. Video content creators can also benefit by studying affective stimuli in videos. We hope our evoked expression labels enable further explorations in these directions.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The EEV dataset contains publicly available content videos alongside detected viewer facial expressions while",
      "page": 1
    },
    {
      "caption": "Figure 2: To the best of our knowledge,",
      "page": 2
    },
    {
      "caption": "Figure 2: Video themes in EEV annotated using an auto-",
      "page": 2
    },
    {
      "caption": "Figure 2: , enabling affective content to be studied across",
      "page": 3
    },
    {
      "caption": "Figure 3: and there are a",
      "page": 3
    },
    {
      "caption": "Figure 3: The distribution of video lengths in the EEV",
      "page": 4
    },
    {
      "caption": "Figure 4: The correlation matrix between the evoked ex-",
      "page": 4
    },
    {
      "caption": "Figure 5: We observe that the",
      "page": 4
    },
    {
      "caption": "Figure 5: Distribution of all expressions in EEV. The dotted line represents the 80 percentile of the distribution.",
      "page": 5
    },
    {
      "caption": "Figure 2: Evoked Expressions and Themes.",
      "page": 5
    },
    {
      "caption": "Figure 6: To start, we ﬁrst compile a list of public reaction videos",
      "page": 5
    },
    {
      "caption": "Figure 6: ). The reaction",
      "page": 5
    },
    {
      "caption": "Figure 6: ). Then, the evoked expressions from the reaction",
      "page": 5
    },
    {
      "caption": "Figure 6: An overview of the data collection process for EEV.",
      "page": 6
    },
    {
      "caption": "Figure 7: Total prediction correlation of the facial expression recognition model on the dataset in [12], across the sixteen",
      "page": 7
    },
    {
      "caption": "Figure 7: b. Importantly, the",
      "page": 7
    },
    {
      "caption": "Figure 8: presents an overview.",
      "page": 7
    },
    {
      "caption": "Figure 8: Baseline model architecture with input modalities",
      "page": 8
    },
    {
      "caption": "Figure 8: for our ablation study and selectively remove",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Time (seconds)"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Image": "",
          "Column_3": "",
          "Inception\n(ImageNet)": "Inception\n(ImageNet)",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "GRU (512\nunits, 2 layer)",
          "Column_9": ""
        },
        {
          "Column_1": "",
          "Image": "56 x 256 x 3\nFace",
          "Column_3": "",
          "Inception\n(ImageNet)": "",
          "Column_5": "2048-D (60 s sequence)",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": ""
        },
        {
          "Column_1": "",
          "Image": "",
          "Column_3": "",
          "Inception\n(ImageNet)": "Inception\n(Faces)",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "GRU (256\nunits, 2 layer)",
          "Column_9": ""
        },
        {
          "Column_1": "",
          "Image": "H x W x 3\nAudio",
          "Column_3": "",
          "Inception\n(ImageNet)": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": ""
        },
        {
          "Column_1": "16kHz\naudio",
          "Image": "",
          "Column_3": "",
          "Inception\n(ImageNet)": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Google i/o 2013 -semantic video annotations in the youtube topics api: Theory and applications",
      "year": "2013",
      "venue": "Google i/o 2013 -semantic video annotations in the youtube topics api: Theory and applications"
    },
    {
      "citation_id": "2",
      "title": "Youtube-8m: A large-scale video classification benchmark",
      "authors": [
        "Sami Abu-El-Haija",
        "Nisarg Kothari",
        "Joonseok Lee",
        "Paul Natsev",
        "George Toderici",
        "Balakrishnan Varadarajan",
        "Sudheendra Vijayanarasimhan"
      ],
      "year": "2008",
      "venue": "Youtube-8m: A large-scale video classification benchmark"
    },
    {
      "citation_id": "3",
      "title": "A comprehensive study on mid-level representation and ensemble learning for emotional analysis of video material",
      "authors": [
        "Esra Acar",
        "Frank Hopfgartner",
        "Sahin Albayrak"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "4",
      "title": "Affective video content analysis: A multidisciplinary insight",
      "authors": [
        "Yoann Baveye",
        "Christel Chamaret",
        "Emmanuel Dellandréa",
        "Liming Chen"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Deep learning vs. kernel methods: Performance for emotion prediction in videos",
      "authors": [
        "Yoann Baveye",
        "Emmanuel Dellandréa",
        "Christel Chamaret",
        "Liming Chen"
      ],
      "year": "2008",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "6",
      "title": "Liris-accede: A video database for affective content analysis",
      "authors": [
        "Yoann Baveye",
        "Emmanuel Dellandrea",
        "Christel Chamaret",
        "Liming Chen"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition based on high-resolution eeg recordings and reconstructed brain sources",
      "authors": [
        "Hanna Becker",
        "Julien Fleureau",
        "Philippe Guillotel",
        "Fabrice Wendling",
        "Isabelle Merlet",
        "Laurent Albera"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "Joao Carreira",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "Kyunghyun Cho",
        "Bart Van Merriënboer",
        "Caglar Gulcehre",
        "Dzmitry Bahdanau",
        "Fethi Bougares",
        "Holger Schwenk",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "arxiv": "arXiv:1406.1078"
    },
    {
      "citation_id": "10",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "Alan Cowen",
        "Dacher Keltner"
      ],
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "11",
      "title": "What the face displays: Mapping 28 emotions conveyed by naturalistic expression",
      "authors": [
        "Alan Cowen",
        "Dacher Keltner"
      ],
      "year": "2007",
      "venue": "American Psychologist"
    },
    {
      "citation_id": "12",
      "title": "The mediaeval 2018 emotional impact of movies task",
      "authors": [
        "Emmanuel Dellandréa",
        "Liming Chen",
        "Yoann Baveye",
        "Mats Viktor Sjöberg",
        "Christel Chamaret"
      ],
      "year": "2008",
      "venue": "MediaEval 2018 Multimedia Benchmark Workshop Working Notes Proceedings of the MediaEval 2018 Workshop"
    },
    {
      "citation_id": "13",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "14",
      "title": "Factorized variational autoencoders for modeling audience reactions to movies",
      "authors": [
        "Zhiwei Deng",
        "Rajitha Navarathna",
        "Peter Carr",
        "Stephan Mandt",
        "Yisong Yue",
        "Iain Matthews",
        "Greg Mori"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "The humaine database: Addressing the collection and annotation of naturalistic and induced emotional data",
      "authors": [
        "Ellen Douglas-Cowie",
        "Roddy Cowie",
        "Ian Sneddon",
        "Cate Cox",
        "Orla Lowry",
        "Margaret Mcrorie",
        "Jean-Claude Martin",
        "Laurence Devillers",
        "Sarkis Abrilian",
        "Anton Batliner"
      ],
      "year": "2007",
      "venue": "International conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "16",
      "title": "Manual for the facial action coding system",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Manual for the facial action coding system"
    },
    {
      "citation_id": "17",
      "title": "Audio set: An ontology and humanlabeled dataset for audio events",
      "authors": [
        "Jort F Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Wade Jansen",
        "R Channing Lawrence",
        "Manoj Moore",
        "Marvin Plakal",
        "Ritter"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "18",
      "title": "Affective video content representation and modeling",
      "authors": [
        "Alan Hanjalic",
        "Li-Qun Xu"
      ],
      "year": "2005",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "19",
      "title": "Multimedia stimuli databases usage patterns: a survey report",
      "authors": [
        "Marko Horvat",
        "Siniša Popović",
        "Cosić"
      ],
      "year": "2013",
      "venue": "2013 36th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)"
    },
    {
      "citation_id": "20",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "Sergey Ioffe",
        "Christian Szegedy"
      ],
      "year": "2015",
      "venue": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "arxiv": "arXiv:1502.03167"
    },
    {
      "citation_id": "21",
      "title": "Predicting emotions in user-generated videos",
      "authors": [
        "Yu-Gang Jiang",
        "Baohan Xu",
        "Xiangyang Xue"
      ],
      "year": "2014",
      "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "22",
      "title": "Hierarchical mixtures of experts and the em algorithm",
      "authors": [
        "I Michael",
        "Robert Jordan",
        "Jacobs"
      ],
      "year": "1994",
      "venue": "Neural computation"
    },
    {
      "citation_id": "23",
      "title": "Combining modality specific deep neural networks for emotion recognition in video",
      "authors": [
        "Samira Kahou",
        "Christopher Pal",
        "Xavier Bouthillier",
        "Pierre Froumenty",
        "C ¸aglar Gülc ¸ehre",
        "Roland Memisevic",
        "Pascal Vincent",
        "Aaron Courville",
        "Yoshua Bengio",
        "Raul Ferrari"
      ],
      "year": "2013",
      "venue": "Proceedings of the 15th ACM on International conference on multimodal interaction"
    },
    {
      "citation_id": "24",
      "title": "Large-scale video classification with convolutional neural networks",
      "authors": [
        "Andrej Karpathy",
        "George Toderici",
        "Sanketh Shetty",
        "Thomas Leung",
        "Rahul Sukthankar",
        "Li Fei-Fei"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Assessment of emotional experience through facial expression",
      "authors": [
        "Karim Kassam"
      ],
      "year": "2011",
      "venue": "Assessment of emotional experience through facial expression"
    },
    {
      "citation_id": "26",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "27",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2012",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "28",
      "title": "Distinctive image features from scaleinvariant keypoints",
      "authors": [
        "David G Lowe"
      ],
      "year": "2004",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "29",
      "title": "Thuhcsi in mediaeval 2018 emotional impact of movies task",
      "authors": [
        "Ye Ma",
        "Xihao Liang",
        "Mingxing Xu"
      ],
      "year": "2018",
      "venue": "Thuhcsi in mediaeval 2018 emotional impact of movies task"
    },
    {
      "citation_id": "30",
      "title": "Automatic analysis of facial actions: A survey",
      "authors": [
        "Brais Martinez",
        "Michel Valstar",
        "Bihan Jiang",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Predicting ad liking and purchase intent: Large-scale analysis of facial responses to ads",
      "authors": [
        "Daniel Mcduff",
        "Rana Kaliouby",
        "Jeffrey Cohn",
        "Rosalind Picard"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Predicting online media effectiveness based on smile responses gathered over the internet",
      "authors": [
        "Daniel Mcduff",
        "Rana Kaliouby",
        "David Demirdjian",
        "Rosalind Picard"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "33",
      "title": "Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected",
      "authors": [
        "Daniel Mcduff",
        "Rana Kaliouby",
        "Thibaud Senechal",
        "May Amr",
        "Jeffrey Cohn",
        "Rosalind Picard"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "34",
      "title": "Large-scale affective content analysis: Combining media content features and facial reactions",
      "authors": [
        "Daniel Mcduff",
        "Mohammad Soleymani"
      ],
      "year": "2017",
      "venue": "2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)"
    },
    {
      "citation_id": "35",
      "title": "Learnable pooling with context gating for video classification",
      "authors": [
        "Antoine Miech",
        "Ivan Laptev",
        "Josef Sivic"
      ],
      "year": "2017",
      "venue": "Learnable pooling with context gating for video classification",
      "arxiv": "arXiv:1706.06905"
    },
    {
      "citation_id": "36",
      "title": "Moments in time dataset: one million videos for event understanding",
      "authors": [
        "Mathew Monfort",
        "Alex Andonian",
        "Bolei Zhou",
        "Kandan Ramakrishnan",
        "Sarah Bargal",
        "Yan Yan",
        "Lisa Brown",
        "Quanfu Fan",
        "Dan Gutfreund",
        "Carl Vondrick"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "37",
      "title": "Towards an intelligent framework for multimodal affective data analysis",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Amir Hussain",
        "Guang-Bin Huang"
      ],
      "year": "2015",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "38",
      "title": "Assessing the effectiveness of a large database of emotion-eliciting films: A new tool for emotion researchers",
      "authors": [
        "Alexandre Schaefer",
        "Frédéric Nils",
        "Xavier Sanchez",
        "Pierre Philippot"
      ],
      "year": "2010",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "39",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "Florian Schroff",
        "Dmitry Kalenichenko",
        "James Philbin"
      ],
      "year": "2015",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "40",
      "title": "Analysis of eeg signals and facial expressions for continuous emotion detection",
      "authors": [
        "Mohammad Soleymani",
        "Sadjad Asghari-Esfeden",
        "Yun Fu",
        "Maja Pantic"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "A bayesian framework for video affective representation",
      "authors": [
        "Mohammad Soleymani",
        "J Joep",
        "Guillaume Kierkels",
        "Thierry Chanel",
        "Pun"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "42",
      "title": "Corpus development for affective video indexing",
      "authors": [
        "Mohammad Soleymani",
        "Martha Larson",
        "Thierry Pun",
        "Alan Hanjalic"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "43",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "Mohammad Soleymani",
        "Jeroen Lichtenauer",
        "Maja Thierry Pun",
        "Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "Nitish Srivastava",
        "Geoffrey Hinton",
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Ruslan Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "45",
      "title": "Gla in mediaeval 2018 emotional impact of movies task",
      "authors": [
        "Jennifer J Sun",
        "Ting Liu",
        "Gautam Prasad"
      ],
      "year": "2008",
      "venue": "Gla in mediaeval 2018 emotional impact of movies task",
      "arxiv": "arXiv:1911.12361"
    },
    {
      "citation_id": "46",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "Christian Szegedy",
        "Vincent Vanhoucke",
        "Sergey Ioffe",
        "Jon Shlens",
        "Zbigniew Wojna"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "47",
      "title": "A compact embedding for facial expression similarity",
      "authors": [
        "Raviteja Vemulapalli",
        "Aseem Agarwala"
      ],
      "year": "2006",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "48",
      "title": "Affective understanding in film",
      "authors": [
        "Hee Lin",
        "Loong-Fah Cheong"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on circuits and systems for video technology"
    },
    {
      "citation_id": "49",
      "title": "Video affective content analysis: a survey of state-of-the-art methods",
      "authors": [
        "Shangfei Wang",
        "Qiang Ji"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "Utilizing affective analysis for efficient movie browsing",
      "authors": [
        "Shiliang Zhang",
        "Qi Tian",
        "Qingming Huang",
        "Wen Gao",
        "Shipeng Li"
      ],
      "year": "2009",
      "venue": "2009 16th IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "51",
      "title": "Video classification and recommendation based on affective analysis of viewers",
      "authors": [
        "Sicheng Zhao",
        "Hongxun Yao",
        "Xiaoshuai Sun"
      ],
      "year": "2013",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "52",
      "title": "Cognimuse: A multimodal video database annotated with saliency, events, semantics and emotion with application to summarization",
      "authors": [
        "Athanasia Zlatintsi",
        "Petros Koutras",
        "Georgios Evangelopoulos",
        "Nikolaos Malandrakis",
        "Niki Efthymiou",
        "Katerina Pastra"
      ],
      "year": "2017",
      "venue": "EURASIP Journal on Image and Video Processing"
    }
  ]
}