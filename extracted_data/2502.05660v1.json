{
  "paper_id": "2502.05660v1",
  "title": "Evaluating Vision-Language Models For Emotion Recognition",
  "published": "2025-02-08T18:25:31Z",
  "authors": [
    "Sree Bhattacharyya",
    "James Z. Wang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Large Vision-Language Models (VLMs) have achieved unprecedented success in several objective multimodal reasoning tasks. However, to further enhance their capabilities of empathetic and effective communication with humans, improving how VLMs process and understand emotions is crucial. Despite significant research attention on improving affective understanding, there is a lack of detailed evaluations of VLMs for emotion-related tasks, which can potentially help inform downstream finetuning efforts. In this work, we present the first comprehensive evaluation of VLMs for recognizing evoked emotions from images. We create a benchmark for the task of evoked emotion recognition and study the performance of VLMs for this task, from perspectives of correctness and robustness. Through several experiments, we demonstrate important factors that emotion recognition performance depends on, and also characterize the various errors made by VLMs in the process. Finally, we pinpoint potential causes for errors through a human evaluation study. We use our experimental results to inform recommendations for the future of emotion research in the context of VLMs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Equipping Artificial Intelligence (AI) systems with the capability to understand emotions is important for sensitive and effective interaction with human users in diverse applications  (Kołakowska et al., 2014; Zhao et al., 2018; Yang et al., 2021; Wang et al., 2023a) . This has been approached in the past through development of deep architectures, including multimodal and context-aware methods suited for specific downstream applications  (Lee et al., 2019; Mittal et al., 2020; Hoang et al., 2021) . The advent of Large Language Models (LLMs), however, has brought about a significant shift in focus. LLMs are now adapted or tuned to achieve what task-specific deep learning models were employed for. As a first step in understanding the inherent capabilities of popular large general-purpose models, before adapting them for specific tasks, LLMs have been evaluated through multi-faceted benchmarking experiments. This ranges from evaluating LLMs in objective  (Hendrycks et al., 2021; Lu et al., 2022)  and subjective task settings  (Ziems et al., 2022; Khandelwal et al., 2024; Fung et al., 2024) .\n\nStudies exploring emotions in the context of LLMs span both benchmarking and tuning efforts  (Xie et al., 2024; Xenos et al., 2024; Etesam et al., 2024) . Several works focus on evaluating text-only language models for emotional capabilities  (Liu et al., 2024c; Wang et al., 2023b)  or the use of emotional stimuli to enhance the performance of LLMs in other tasks  (Li et al., 2023; LI et al., 2024) . A few recent works also venture beyond the single modality of text, to approximate the human process of emotion perception more closely. Such works focus primarily on tuning large Vision-language models (VLMs)  (Xie et al., 2024; Xenos et al., 2024; Etesam et al., 2024) . However, most of the recent explorations concentrate either on specific datasets and models or directly target resource-intensive instruction tuning without highlighting the specific need to do so. While they present impressive results on overall quantitative metrics, there remains a notable lack of comprehensive and critical evaluation studies to illuminate the precise capabilities, weaknesses, and vulnerabilities of large models when performing emotion recognition in a multimodal setting.\n\nTo address this gap, in this paper, we present an extensive evaluation of popular VLMs for emotion recognition. We analyze their performance from lenses of accuracy and robustness, while also characterizing the causes for errors made by them. We investigate the specific task of evoked emotion recognition, because of (a) its widespread practical relevance in domains such as social interactions arXiv:2502.05660v1 [cs.CV] 8 Feb 2025  (Wieser et al., 2012; Jyoti and Rao, 2016; Awal et al., 2021) , online e-commerce  (Sánchez-Núñez et al., 2020) , artistic content creation and recommendation  (Wang et al., 2023a), etc., and, (b)  the non-trivial nature of the task, involving simultaneous multimodal and affective understanding to use implicit affective cues to predict exact, detailed emotions  (Wang et al., 2023a) , which is different from application-oriented tasks where the emotion information is atleast partially present with the model  (Deng et al., 2023; Li et al., 2024) . In evaluating VLMs for evoked emotion recognition, we specifically ask the following research questions:\n\n• RQ1: How well do VLMs recognize evoked emotions given images and a textual prompt?\n\n• RQ2: How robust are the models to minor and major variations in the prompts?\n\n• RQ3: What are the types of errors seen in the VLM responses and why do they occur?\n\nWe first compile existing image-based emotion datasets to create an Evoked Emotion benchmark of challenging difficulty, EVE. Using EVE, we evaluate 7 popular VLMs on the task of evoked emotion recognition. Beyond presenting metrics of correctness, in our analysis, we delve deep into additional aspects such as preference exhibited by models towards certain sentiments. We design 8 different settings to study the robustness of models to perturbations in prompts. These include shuffling the order of emotion labels in prompts, openvocabulary classification, adopting emotional perspectives, and using self-reasoning mechanisms. Finally, we create a formal framework to analyze mistakes made by VLMs and conduct a human study to localize the causes of such mistakes.\n\nOur key findings show that at the current state, VLMs are inept at predicting emotions evoked by images. We show that VLMs are significantly sensitive to the order in which class labels are presented in the prompts, and perform poorly when no labels are presented. We find that prompting VLMs to adopt an emotional persona has a drastic negative impact on their performance. We also observe that self-reasoning mechanisms help in the case of certain models. This is especially applicable for mechanisms that involve breaking the emotion recognition task down into more tractable sub-components (eg., captioning + reasoning). Finally, through our human study, we elucidate that factors leading to the poor VLM performance pertain not only to the model capabilities but also depend on the data used and task difficulty. We use our findings to further discuss important considerations to improve the emotion perception capability of VLMs. 1  .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Methods studying emotions using LLMs have included using theories grounded in psychology to develop evaluation metrics  (Wang et al., 2023b; Regan et al., 2024) , generating explanations given suitable image-emotion pairs  (Deng et al., 2023) . Efforts have also been made in the direction of finetuning LLMs like LLaMA  (Touvron et al., 2023) , BLOOM  (Workshop et al., 2022)  to create experts on emotional understanding, through instruction tuning  (Liu et al., 2024c) . Training-free enhancement methods have been approached to create emotionally conditioned generations for downstream tasks like image captioning or generating a news headline  (Li et al., 2024) .\n\nFew recent works also study emotions with multimodal language models. A recent method proposes visual instruction tuning to improve the performance of open models in evoked emotion prediction  (Xie et al., 2024) , using a resource-intensive method of generating synthetic data and fine-tuning models. Another recent effort evaluates Vision-Language Models (VLMs) for expressed emotion recognition, but includes only a single dataset, and depends on auxiliary models to complete intermediate tasks for the VLMs being evaluated  (Etesam et al., 2024) . Vision-language models have also been employed to generate additional contextual information which is used subsequently for training a Q-Former-based module for expressed emotion prediction  (Xenos et al., 2024) . Despite these promising recent research efforts in the area of emotional understanding with VLMs, to the best of our knowledge, the capabilities of advanced Vision-Language Models in evoked emotion recognition have thus far not been comprehensively analyzed.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Evaluation Data",
      "text": "We leverage popular, existing, evoked emotion recognition datasets to create EVE, an Evoked Emotion benchmark for our analysis. This includes EmoSet  (Yang et al., 2023) , FI  (You et al., 2016) , Abstract, ArtPhoto  (Machajdik and Hanbury, 2010) and Emotion6 (Peng et al., 2015) . The selection of the datasets ensures a diverse range of image types in the benchmark, ranging from images of humans, nature, objects in natural or artistically photographed settings, to images of paintings without any recognizable objects (eg., in Abstract). Emotion6 uses 7 discrete emotion classes, while all other datasets follow Mikel's 8-class emotion model  (Mikels et al., 2005) . The total number of samples in Abstract, ArtPhoto, and Emotion6 are under 2000, and we include the entire datasets for the evaluation. For the larger EmoSet and FI datasets, which contain 118000 and 23184 samples respectively, we downsample them each to contain about 2900 samples, retaining only the most challenging samples, as described below. This is done primarily to limit the time and resource consumption when evaluating closed-source models like GPT. Besides, the large size of these datasets is crucial only when training data-hungry deep learning architectures, and not when evaluating models.\n\nTo obtain the downsampled sets, a pre-trained ViT model  (Dosovitskiy et al., 2020)  is first finetuned using the entire EmoSet and FI datasets. This achieves weighted F1 scores of 0.91 and 0.53 respectively. For all predictions by the ViT model, the prediction probability is then obtained. This is used to choose moderate to difficult samples, to create initial candidates for the final evaluation sets. The initial candidates for EmoSet and FI are denoted as C e and C f respectively. The samples incorrectly classified by the fine-tuned model (most difficult) are automatically included in C e and C f . Then, we consider correctly predicted instances, where the probability of prediction is lower than a certain threshold. This probability threshold is chosen empirically to be 0.8, based on the prediction probability distribution over each dataset. Thus, the candidate sets C e and C f contain incorrectly classified samples, and samples predicted correctly with probability values less than 0.8. Intuitively, the former group of images represents the most difficult category, while the latter group consists of instances that are of intermediate difficulty. Finally, we subsample randomly from these candidate sets to create EmoSet-Hard and FI-Hard, retaining the original emotion class distributions. We include a more detailed account of the subsampling process in the Appendix (A.1), including a manual analysis of the higher difficulty level of samples in EmoSet-Hard and FI-Hard.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experimental Setup",
      "text": "We evaluate open-source models LLaVA (7B, 13B)  (Liu et al., 2024b) , LLaVA-Next (Vicuna 7B, 13B, Mistral 7B)  (Liu et al., 2024a) , and Qwen-VL  (Bai et al., 2023)  along with GPT4-omni  (Achiam et al., 2023) , in a zero-shot manner on the created benchmark. The task precisely requires the models to predict what emotion might be elicited from an individual when they are exposed to the visual stimuli of each image sample in the datasets. We categorize our main experiments into two primary settings: (a) simple multimodal classification, where each model is prompted to generate a single-word emotion prediction, and (b) experiments studying model robustness, where several minor and major perturbations in the prompts are introduced to study differences in model performances.\n\nPreliminaries for the task. For a single iteration of the evaluation process, the inputs are an image I, a prompt P describing emotion labels (words) for k discrete emotion classes, C = {c 0 , c 1 , ..., c k }, where C represents the set of all emotion labels. Model M , with parameters θ M , performs the classification operation M (•) on these inputs, generating a response containing the predicted evoked emotion. The responses are parsed and string-matched with the ground truth class labels, and weighted F1 scores are calculated.\n\nEmotion Properties Analyzed. We leverage properties of the fine-grained emotion classes in the data to provide a formal framework for our analysis. The fine-grained emotion class labels can be more broadly classified to belong to either positive or negative sentiment categories (Refer B.2). We define \"Sentiment Bias\" using this categorization to help reveal insightful trends in the model performances. We define a model's positive sentiment bias as its exhibited preference towards predicting a true negative sentiment sample to a positive sentiment class, and vice versa. Formally, given model M , for a single image sample, given the ground truth label class l and model predicted class c, and the sets of positive and negative emotions S P and S N respectively, we define the positive and negative sentiment bias as:\n\n(1)\n\nUsing this framework for analysis, we now describe our experiments and key results. Our first and simplest evaluation scheme, M c (•), denoting simple classification, involves prompting the models to choose a single emotion word from the list of labels provided in the prompt. Formally, each model generates:\n\nwhere j ∈ {0, ..., k}.\n\nFrom the results described in Table  1 , we note that the performance is determined not only by the model used but also by the content of the dataset on which it is evaluated. GPT4-o consistently outperforms most open models and even rivals the performance of fine-tuned models on certain datasets  (Xie et al., 2024; Xu et al., 2022)  like Emotion6. De-spite that, along with all other models, it falls short on the Abstract dataset, which contains images of abstract paintings without any human figures or objects. Further, open-source models LLaVA and LLaVA-Next outperform GPT4-o specifically on the FI dataset. Although some models perform comparably to fine-tuned or trained architectures on some datasets, overall, the zero-shot performance of VLMs in emotion recognition still largely lags behind models created specifically for this task.\n\nWe also look at the broader sentiment categories that the data samples belong to 2  . In Table  2 , we report the average F1 scores achieved by each model on each overarching sentiment category. For all models other than GPT4-o, the difference in performance on positive and negative sentiments is marginal. Models from the the LLaVA family perform slightly better on positive emotions, while Qwen-VL and GPT4-o are better on negative emotions. GPT4-o, despite showing the largest different between the two sentiment categories, has the highest individual F1 score for both sentiments.\n\nDiving deeper, we calculate the sentiment bias exhibited by the models (Fig.  1 ). We observe that models prefer positive sentiments over negative sentiments with a higher probability. This shows that when not fine-tuned specifically for emotionrelated tasks, and provided with emotion class labels, all of the models naturally exhibit a higher tendency to generate predictions of positive sentiments.\n\nIn our subsequent experiments, we aim to understand whether the model performance, along with the exhibited biases, is dependent on the specific format of prompts and responses.\n\n6 Robustness: How robust are VLMs to changes in emotion-related prompts?\n\n[RQ2]\n\nWe experiment with four types of changes to study the sensitivity of models: (a) shuffling the order of class labels in the prompts, (b) providing no class labels, (c) adopting an optimistic or a pessimistic persona, and (d) using three different self-reasoning mechanisms. This is mainly to understand whether the models get easily affected by the order or absence of class labels, gauge whether assuming a differing perspective improves or deteriorates the model performance and understand the effect of reasoning strategies which have been shown to be helpful in wide-ranging tasks  (Wei et al., 2022; Li et al., 2024) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Variation 1: Shuffled Emotion Order",
      "text": "The experiments in the previous section present the emotion class labels in alphabetical order in the prompts. In this section, we explore whether listing any one category of emotions (positive or negative) first, within the prompt, has an impact on the emotion recognition capability of the models. For example, with Mikel's 8-class model  (Mikels et al., 2005) , presenting positive emotions first in the prompt would mean adhering to the following order: amusement, awe, contentment, excitement and anger, disgust, fear, sadness. Fig.  2 (a) reports the weighted F1 scores for all models, averaged across datasets and model sizes. For all models, including the negative emotion labels first leads to lower performance. For LLaVA and LLaVA-Next, prompts that have positive emotion labels first show a slight performance improvement. Listing negative emotions first, on the other hand, leads to lower performance for all models, other than GPT4-o, which remains unaffected. We further unveil the precise impact of the shuffled order of emotion labels on sentiment bias. As shown in Fig.  2(b ), for all models other than GPT4-o, positive sentiment bias generally increases when either emotion class is presented first. Conversely, negative bias generally decreases with both kinds of shuffled order of emotions, except for LLaVA-Next. Thus, overall, most open-source models deteriorate when negative emotions are presented first, while their positive bias is increased when emotions are grouped according to sentiment categories.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Variation 2: Providing No Target Labels",
      "text": "For experiments in this section, we provide no explicit emotion class labels in the prompt to choose from. The models are free to respond using a single emotion word that does not necessarily belong to the datasets' label set. We use semantic similarity scores generated using SBERT  (Reimers and Gurevych, 2019)  to assign the predictions to the class with the most semantically similar label. As our task involves fine-grained emotion recognition, we further consider whether the freeform predictions by the models are specific enough. Using the original class labels from the datasets, C = {c 1 , c 2 , ...., c k }, we calculate:\n\nIntuitively, it denotes the maximum possible similarity between two distinct, fine-grained emotion classes. Thus, for each free-form prediction to be sufficiently specific, its similarity to the correctly assigned label class should be greater than the maximum similarity between two distinct classes. Given the set of all open-vocabulary model predictions O, and the ground truth labels L, we calculate the frequency with which each model makes adequately fine-grained predictions as follows:\n\nwhere E denotes the event of o i being assigned to class l i . Fig.  3  shows the F1 scores for each model, across datasets, with the frequency of fine-grained predictions depicted through the numbers above the bars. All models fare significantly better when provided with labels in the prompts than when openvocabulary prediction is required. Note that this is true even when the final classification is done using only maximum semantic similarity, which is a more relaxed criteria than requiring an exact string match with the provided labels. Further, LLaVA on average makes fine-grained predictions more often than all other models. GPT4-o uses specific emotion words the least often, implying that to make it suitable for use in fine-grained emotion prediction tasks, the inclusion of target labels is indispensable. Additionally, we compute the sentiment bias scores (Fig.  4 ), and find that the earlier trend is reversed for all models other than GPT4-o, when compared to classification with explicit target labels in the prompts (Fig.  1 ). This also shows, that With Labels Without Labels\n\nFigure  3 : The weighted F1 score with and without precise target labels in the prompts. The numbers in brown represent the percentage of fine-grained predictions made. when the predictions are not anchored using predefined class labels in the prompts, most models have a naturally higher likelihood of choosing negative emotion words over positive emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Variation 3: Adding A Persona",
      "text": "Approaching robustness from another angle, we explore whether urging the models to adopt a sentiment-related perspective (positive or negative) holds any influence. Specifically, we study whether adding an optimistic persona biases the model to choosing positive emotions more frequently, and vice versa, besides affecting the overall perfor-mance. We plot the average F1 score for each model, under different assumed personas in Fig.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "5(A).",
      "text": "All models perform poorly when adopting either a positive or negative persona. The degradation in performance is the most stark for Qwen-VL, and the least for GPT-4o. Across all the models and datasets, the performance drop when adopting a negative persona is significantly more than when adopting a positive persona. We show changes in the sentiment bias to be a primary reason for the poorer performance, as demonstrated through Fig.  5 (b) and Fig.  5(c ). It can be noted from Fig.  5 (b), that adopting a positive persona sharply increases the positive bias, which, on the other hand, is diminished by using a negative persona.\n\nSimilarly, as seen in Fig.  5 (c), negative bias increases sharply when adopting a negative persona, leading to models classifying nearly all samples to negative emotion classes (most frequently \"sadness\"). In contrast to the change in positive bias, negative bias is only marginally reduced when using a positive persona. Thus, all models show extreme vulnerability to the inclusion of a sentimental perspective. This could potentially make models susceptible to exploitation, for inducing severe bias in emotion-related tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Variation 4: Reasoning-Based Prompting Mechanisms",
      "text": "Adapting prompting methods like Chain-of-Thought  (Wei et al., 2022) , we explore whether prompting the models to self-reason with their generation impacts the performance. Specifically, we use three different evaluation mechanisms. In the first mechanism, the model generates an explana- tion for its emotion prediction simultaneously. The second mechanism uses three steps of contextual reasoning prior to prediction. The first two steps involve attending to the foreground and background objects in the images to predict emotions evoked by them individually. The third step requires reasoning about whether these two emotions are compatible, to decide the final prediction. Our last mechanism involves captioning the provided image, followed by reasoning using the caption to predict evoked emotion.\n\nThe aggregated F1 scores are presented in Fig.  6 . Contextual reasoning helps only GPT4-o among all models, highlighting the inability of most other models to capture relevant background context from images accurately. By analyzing specific responses, we also observe that LLaVA and LLaVA-Next struggle with the multi-step response format required for contextual reasoning. Captioningbased reasoning shows relatively higher gains with LLaVA and LLaVA-Next. This further underscores that these models underperform when reasoning over multiple modalities (image and text) simultaneously, compared to when reasoning only over text (captions of images). Overall, the models remain relatively robust to variations in the prompting mechanism, and only show slight improvements in some cases.\n\nFrom the results of our robustness experiments, it can be concluded that most models show significant variance with respect to prompt perturbations. However, the extent of such variance is largely determined by the type of perturbation. Designing models that are robust to such variations is thus an important area for further inquiry.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Analyzing Mistakes By Models [Rq3]",
      "text": "To characterize errors made by each model, we define three types of errors, proceeding from broad (or blatant) to fine-grained (subtle) mistakes:\n\n• Error Category (EC) I -Incorrect Sentiment:\n\nThe case where the ground truth and predicted label belong to different sentiment categories (eg., \"sadness\" and \"amusement\").\n\n• Error Category (EC) II -Correct Sentiment, Incorrect Arousal: The case where the ground truth and predicted label belong to the same sentiment but the different arousal or intensity category (eg., \"sadness\" and \"anger\"). Arousal, in the dimensional Valence-Arousal-Dominance (VAD) model of emotions, refers to the agitation level of a person, or the intensity of the emotion felt.\n\n• Error Category (EC) III -Correct Sentiment, Correct Arousal, Incorrect Prediction:\n\nThe case where the ground truth and predicted label belong to the same sentiment and arousal/intensity category, but are not the same fine-grained class (eg., \"fear\" and \"anger\").\n\nWe hypothesize that blatant errors in EC I can be attributed to the model's inability to reason about affect. However, the more nuanced errors (II and III) could be caused by subjective interpretation of closely related, distinct emotions. We conduct a manual evaluation study to explore this further, annotating about 500 error samples. Each annotation denotes whether a human rater agrees more with the model-predicted emotion label, with the original ground truth label from the dataset, with both emotion labels, or with neither.\n\nWe plot the human agreement percentage in Fig.  7 . The plot for EC I shows that errors in this category are indeed genuine errors by the models, as human annotations consistently agree more often with dataset ground truth.\n\nFor EC II, although agreement with ground truth still dominates, there is a significant increase in agreement with both model predictions and ground truth, proving that some of the errors in this more fine-grained category can be attributed to the subjectivity of emotion perception.\n\nFinally, for EC III, most of the model predictions, that do not match with dataset labels at a fine-grained level, may not be entirely incorrect, since they are preferred more often than the dataset ground truth. We also observe specific examples where the dataset ground truth incorrectly reflects the expressed emotion, while the model predictions accurately capture the evoked emotion. The socalled errors in EC III can thus be attributed to noisy ground truth from the datasets, rather than the capability of VLMs. This unveils the issue of unreliable ground truth labels in existing emotion datasets. It can also be noted that for all the error categories, the proportion of human annotations agreeing with neither the ground truth nor model prediction (labeled as \"Neither\") remains relatively small and constant. This reflects that either of the two emotion labels (ground truth or model predicted) or both were found to be plausible in most cases, showing that human preference of either category (as measured by agreement of annotations) is clear and trustworthy. We additionally show in the Appendix (B.4.4), that the Abstract data subset, on which models perform most poorly, is the most reliable dataset in terms of human agreement.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "We arrive at answers to our initial research questions through all of our experiments:\n\n• RQ1: VLMs are not adept at zero-shot multimodal emotion recognition, and often exhibit significant biases towards certain emotions.\n\n• RQ2: VLMs are sensitive to prompt changes. The performance depends largely on the way target labels are presented, the format of prompting and response, and whether VLMs adopt a sentimental perspective.\n\n• RQ3: VLMs make a combination of broad and fine-grained errors. Many deviations from a dataset's ground truth can also be attributed to ambiguous or unreliable original labels. This is especially applicable for the most finegrained errors.\n\nThe need for improvement in model capabilities could be approached through a deeper investigation of the internal model representations, the methods used for aligning models to the tuning data, etc. However, such interventions would require for the instruction-tuning or fine-tuning data to be noise-free. To make the datasets reliable, while accommodating the inherent subjectivity of the task, datasets could be created with explanations for annotations, emotion distributions or multiple labels instead of discrete single class labels. Further, the research community could benefit from availing detailed information on datasets, such as, the testretest reliability data  (Kim et al., 2018) , duration of exposure to emotion stimuli for each subject  (Lu et al., 2017) , etc. Further, there remains a strong need to distinguish between evoked and expressed emotions. Many current datasets are curated by querying images online using emotional keywords  (Yang et al., 2023) , which is susceptible to collecting images merely related to the keyword, and not necessarily evoking that exact emotion.\n\nThrough our experiments, human study and analysis, we hope to have highlighted that all aspects of VLMs' emotion recognition pipeline, specifically the data used and modeling, are in need of critical analysis and measures for improvements. Through this work, we also hope to inspire broader evaluation and benchmarking efforts to improve emotional reasoning in VLMs, extending to complementary areas of emotion understanding and generation, to help achieve the broader goal of making AI systems more empathetic, safe and useful.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Limitations",
      "text": "Although the current state of the study aims to be the first comprehensive evaluation of VLMs for evoked emotion recognition, there remains scope to include more models. With greater resources, there opens up the possibility of evaluating entire datasets and comparing the same with the model performances on the harder subsets included in our benchmark.\n\nThe current evaluation also includes only fewshot performances of the models, while the opportunity to fine-tune smaller models on the same datasets, particularly the difficult data subsets, remains open.\n\nFurther, the datasets currently included are shown to have ambiguous instances, which stem both from innate subjectivity of emotions and noise.\n\nAlthough we discuss useful measures to reduce make datasets more reliable, the possibility of ambiguous interpretations of emotions is a major challenge in affective computing. It continues to be an active area of research.\n\nAs most of the images are sourced from the internet, we also acknowledge the possibility of some of the images being included in the training data of the models evaluated. However, for a closed model like GPT4-o, it is not possible to verify the same.\n\nThe current benchmark and evaluation also address the specific task of evoked emotion recognition and could be extended to include other tasks in emotion recognition, as well as generation, to constitute a comprehensive benchmark for emotional understanding.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ethical Considerations",
      "text": "We depend on existing emotion datasets to create our benchmark. We acknowledge that the possibility of offensive images being present in the datasets cannot be ruled out. Although we manually analyze several instances from the datasets, we do not manually check the precise visual content in all of the images. Besides, though the datasets used do not contain any private identifiable information, a large number of images include humans, revealing their faces and gestures. We implore against the misuse of that information and will ensure dissemination of the dataset only for verifiably legitimate and valid purposes of research. As some of the datasets were also created many years ago, it is possible that they may not satisfy the required bar of ethical review in place at present. Ensuring that they do comply with the required standards of reproducibility and reliability can in itself be an important area of research. Finally, we only evaluate how well the models mimic trends it has learned through the multimodal data used for training, and do not claim that they possess any real, human-like, \"understanding\" of emotions.\n\nIn a larger perspective, our research aims to help create emotionally sensitive VLMs. We acknowledge that depending on the deployment of VLMs, emotional information could potentially be used for manipulating human behavior, such as using positive emotions to advertise products. Although the end result of such deployment is largely determined by the executive forces controlling the use of large models, we advocate strongly responsible usage of our research, and similar research endeavors.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A Sampling For Benchmark",
      "text": "In this section, we present additional details about the process adopted for creating the FI-Hard and EmoSet-Hard data subsets. This includes details of implementation, followed by examples of the varying difficulty levels targeted to be included through the benchmark generation process.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.1 Subsampling Data Based On Fine-Tuning",
      "text": "We first present specific implementation details of the fine-tuning process. We use the base size of ViT  (Dosovitskiy et al., 2020) , pretrained on ImageNet-21K  (Ridnik et al., 2021)  and fine-tuned on Ima-geNet 2012  (Russakovsky et al., 2015) . For finetuning on EmoSet, we add 3 linear layers, each followed by dropout layers (p=0.2) and a non-linearity of ReLU  (Nair and Hinton, 2010) . Training is carried out for 30 epochs, creating an 80:20 split into training and holdout sets. As the number of samples in FI is significantly smaller, we use the model pre-trained on EmoSet as the starting point for finetuning on FI. Making the final classification layer trainable, we update the weights of the pretrained model, based on the FI dataset. The training for FI follows a similar 80:20 split of training and unseen validation data, and is carried out for 30 epochs. In both cases, the models are optimized with Stochastic Gradient Descent, using an initial learning rate of 0.05, momentum of 0.9, and weight decay set to 0.00005. Along with SGD, Cosine Annealing scheduler is used. The objective is simply minimizing the multi-class Cross-Entropy Loss. The models are fine-tuned on single A40 GPUs with 4 cores. The total time taken for fine-tuning EmoSet and FI was around 5 hours and 3 hours respectively.\n\nOnce the model is fine-tuned on the entire EmoSet and FI datasets, as described in the main body, the prediction probabilities are used to further filter out the most obvious or easy samples. The probability values for the correctly classified samples in EmoSet are in the range [0.31, 1.0], and for FI are within in [0.32, 1.0], with most probability values lying above 0.9. We choose the threshold of 0.8 for both EmoSet and FI, keeping the value close to the average of the range, but slightly  higher, to account for the higher frequency of probability values greater than 0.9. Thus, the final data subsets contain samples that are either incorrectly predicted by the fine-tuned model, or are predicted correctly with probability less than 0.8. Intuitively, it includes examples that are harder to classify, contain less obvious expressions of emotion, or can potentially belong to multiple emotion classes. The final numbers of samples in each data subset is described in Table  3 . In the final subsamples, we also retain the original emotion class distribution of each dataset, as represented through Fig.  8 . As we do not train or fine-tune any models, the varied class distribution is not detrimental to our analysis. Further, to account for the unequal class distribution, we report the weighted F1 scores for all analysis.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "A.2 Manual Analysis Of Difficulty Of Images",
      "text": "We present examples from EmoSet-Hard and FI-Hard to demonstrate the qualitative difference in difficulty in predicting evoked emotions. Note that the examples may contain images that evoke strong negative emotions in the viewer. As seen in Fig.  10  and Fig.  11 , for each emotion category, the first 3 samples from the left are included in the final datasets, as they are either predicted correctly with a probability below 0.8 or are predicted incorrectly by the fine-tuned ViT models.\n\nConsider the examples from EmoSet-Hard described in Fig.  10 . From the examples for Amusement, the image with children is classified with the highest probability of belonging to this emotion class, followed by the image showing toys. The image of the squirrel, although predicted to belong to the Amusement class, is done so with a significantly lower probability. This hints at the bias within the dataset that leads models to associate certain elements in the image (children, toys, amusement parks, etc.) to the emotion class of Amusement. Thus, images with relatively uncommon elements, which may or may not be commonly associated with the Amusement emotion class, are included in the EmoSet-Hard set. Another example of this can be seen in the images shown for Anger, Disgust and Fear classes, where images that are more colorful or show toys or small children are classified into the Amusement category, disregarding the deeper context within the images. Further, as seen in the incorrectly classified example from the category of Awe, the facial expressions of the children in the image lead the image to be misclassified to belong to Sadness. Thus, instances with relatively more uncommon elements are included in the EmoSet-Hard set based on our strategy.\n\nThe examples from FI-Hard, as shown in Fig.  11  also testify to more difficult samples being chosen. The set includes images containing visual elements that can easily be correlated with certain emotion classes, but originally belong to different emotion categories. For instance, the misclassified images shown under Disgust and Fear categories contain toys, or colorfully dressed people. They Sadness Figure  11 : Examples from the created FI-Hard dataset. Similar to EmoSet-Hard, for Contentment and Sadness, no instances are found that are predicted correctly with a probability less than 0.8. For all other categories, the two leftmost examples describe instances that are correctly predicted, but with a probability less than 0.8. The next example shows an image predicted incorrectly. Finally, the rightmost example for all categories show the correctly predicted samples, which have probability of prediction higher than 0.8. are included in the FI-Hard dataset as potentially difficult instances to predict.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "B Main Experiments",
      "text": "In this section, we provide additional details for all of our experiments. This includes details of implementation such as the resources, time or specific prompts used, and supplemental results.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "B.1 Implementation Details",
      "text": "To evaluate all of the open models, we use Huggingface 3  . GPT4-o is evaluated using the OpenAI API  4  . The open models are loaded in their full sizes, and run using GPUs (A40 with four cores).\n\nThe maximum number of tokens to be generated is capped at 160, and is sufficient for all experiments.\n\nThe time taken for the evaluation is influenced by the evaluation format, with the format of contextual reasoning (Section 6.4) taking the longest time, owing to the higher number of tokens required to be generated as output. The results reported are obtained through single runs of each type of prompt, owing to the significant computational and monetary resources required for using the models.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "B.2 Emotion Properties Analyzed",
      "text": "We provide a categorization of the fine-grained emotion classes into broader positive and negative sentiment categories in Table  4 . Note that we do this only for the emotion categories belonging to the popular 8-class emotion model  (Mikels et al., 2005) , as we consider only the constituent datasets adhering to this model of classification for the finegrained analysis.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "B.3 Prompts Used",
      "text": "We include the exact prompts included in this section, in the Figures 12  13, 14, 15, 16, 17, 18, 19, and 20 . We use a specific template format in the prompts, with the first line of each prompt being the following: \"Imagine you are like a human, capable of feeling emotions, and an image is shown to you.\". We include this specifically to bypass content moderation policies in some models, that were otherwise leading the models to abstain from responding for some image samples. Although there was no overtly offensive or obscene content in the datasets we rely on, a large number of samples depict extreme (negative) emotions. We observed by experimenting with and without this specific starting line, that providing this warning helped in obtaining responses for most of the image samples.\n\nAlso, in the current stage of our study, we include only zero-shot prompting strategies for evaluation. At the time of conducting experiments, some of the models included in the evaluation framework were incapable of reasoning over multiple visual inputs. Thus, providing other models with visual few-shot examples would give them an unfair edge. However, we do experiment with few-shot examples in textual form (results not included in this paper) for a small subset of the data. Precisely, we provide a caption-like description of images, along with the corresponding emotion evoked. We observe that this leads to further confused responses for models like LLaVA, and thus avoid using any few-shot examples for our large-scale evaluation experiments.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "B.4 Additional Results",
      "text": "",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "B.4.1 Fine-Grained Class-Wise Performance",
      "text": "We present some additional results concerning the fine-grained performance of models. Table  5  shows the average F1 scores achieved by each model family on the fine-grained emotion classes. The results are calculated by averaging scores on the EmoSet-Hard, FI-Hard, Abstract and ArtPhoto subsets of our benchmark, as they follow the 8-class classification of emotions. Interestingly, all model families, apart from Qwen-VL, consistently achieve the highest individual F1 score on the fine-grained category of disgust. Further, all models other than GPT4-o show the worst performance on the category of Anger. This is also in line with results presented in Section 5, where GPT4-o is seen to perform significantly better on negative emotion categories. In Table  6 , we also present an aggregate of the F1 scores, by grouping emotions based on the sentiment (positive or negative) and arousal (high or low) categories.\n\nWe also show the class-wise Precision and Recall in Table  7  and 8             precision scores for Anger are consistently the highest, while it is also the category with the worst F1 score for most models. In contrast to that, the recall scores for Anger are consistently the lowest, showing that a high number of false negatives affects the overall performance of models on this category the most. The recall scores are the highest for the Disgust category (except for Qwen-VL), which is also the class where models achieve the highest F1 scores. Overall, a complementary relationship can be seen for the precision and recall scores for most categories, and can be investigated deeply for further analyses and improvements in future work.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "B.4.2 Predicting Emotions Without Any Labels",
      "text": "We study the capability of models to make finegrained, distinct emotion predictions in further detail, in addition to the results presented in Section 6.2. Recall that for all open-vocabulary prediction experiments, we calculate the semantic similarity of the model prediction with all emotion label classes, and assign the prediction to the class with the highest similarity. We now try to understand whether the model predicts an emotion that is truly closest semantically to a single emotion class, or it predicts a generic emotion word that could be considered almost as similar to multiple other emotion classes. In other words, we consider whether the maximum similarity score is significantly different from the second-largest similarity score between a given model prediction and the original emotion classes. Formally, given a model prediction o i , and the set of original class labels C, we first calculate the maximum similarity to assign the prediction to a particular label class:\n\nUsing this, we assign o i to the label class as follows:",
      "page_start": 21,
      "page_end": 21
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Positive and Negative Bias demonstrated by",
      "page": 4
    },
    {
      "caption": "Figure 1: ). We observe that",
      "page": 4
    },
    {
      "caption": "Figure 2: (a) reports the weighted F1 scores for all",
      "page": 5
    },
    {
      "caption": "Figure 2: (b), for all models other than GPT4-o, pos-",
      "page": 5
    },
    {
      "caption": "Figure 3: shows the F1 scores for each model,",
      "page": 5
    },
    {
      "caption": "Figure 4: ), and find that the earlier trend is",
      "page": 5
    },
    {
      "caption": "Figure 1: ). This also shows, that",
      "page": 5
    },
    {
      "caption": "Figure 2: (a): The weighted F1 score for each model, averaged across datasets. The different bars represent the",
      "page": 6
    },
    {
      "caption": "Figure 3: The weighted F1 score with and without",
      "page": 6
    },
    {
      "caption": "Figure 4: Sentiment bias for responses generated with-",
      "page": 6
    },
    {
      "caption": "Figure 5: (b) and Fig. 5(c). It can be noted from",
      "page": 6
    },
    {
      "caption": "Figure 5: (b), that adopting a positive persona sharply",
      "page": 6
    },
    {
      "caption": "Figure 5: (c), negative bias in-",
      "page": 6
    },
    {
      "caption": "Figure 5: Fig. (a): Weighted F1 score for each model, averaged across all datasets considered. The score drops",
      "page": 7
    },
    {
      "caption": "Figure 6: Weighted F1 scores, averaged across all",
      "page": 7
    },
    {
      "caption": "Figure 6: Contextual reasoning helps only GPT4-o among",
      "page": 7
    },
    {
      "caption": "Figure 7: Human agreement with model predictions (blue), ground truth from dataset (orange), both labels (green),",
      "page": 8
    },
    {
      "caption": "Figure 7: The plot for EC I shows that errors in this cate-",
      "page": 8
    },
    {
      "caption": "Figure 8: The distribution of different emotion classes in the final evaluation sets considered. The numbers of",
      "page": 13
    },
    {
      "caption": "Figure 9: The distribution of different emotion classes in the final evaluation sets considered, grouped according to",
      "page": 13
    },
    {
      "caption": "Figure 10: and Fig. 11, for each emotion category, the first",
      "page": 13
    },
    {
      "caption": "Figure 10: From the examples for Amuse-",
      "page": 13
    },
    {
      "caption": "Figure 11: also testify to more difficult samples being cho-",
      "page": 13
    },
    {
      "caption": "Figure 10: Examples from the created EmoSet-Hard dataset. For Contentment and Excitement, no instances are",
      "page": 14
    },
    {
      "caption": "Figure 11: Examples from the created FI-Hard dataset. Similar to EmoSet-Hard, for Contentment and Sadness, no",
      "page": 15
    },
    {
      "caption": "Figure 12: The prompt Simple Multimodal Classification",
      "page": 17
    },
    {
      "caption": "Figure 13: The prompt for shuffled order of emotions with positive emotions first.",
      "page": 17
    },
    {
      "caption": "Figure 14: The prompt for shuffled order of emotions with negative emotions first.",
      "page": 17
    },
    {
      "caption": "Figure 15: The prompt for open-vocabulary emotion prediction.",
      "page": 18
    },
    {
      "caption": "Figure 16: The prompt for adopting positive persona.",
      "page": 18
    },
    {
      "caption": "Figure 17: The prompt for adopting negative persona.",
      "page": 18
    },
    {
      "caption": "Figure 18: The prompt Explanation-based Reasoning",
      "page": 19
    },
    {
      "caption": "Figure 19: The prompt for Contextual Reasoning",
      "page": 19
    },
    {
      "caption": "Figure 20: The prompt Caption-Based Reasoning",
      "page": 19
    },
    {
      "caption": "Figure 21: Average Difference between the most similar",
      "page": 20
    },
    {
      "caption": "Figure 21: LLaVA (particularly LLaVA 13B)",
      "page": 21
    },
    {
      "caption": "Figure 3: on the percentage of",
      "page": 21
    },
    {
      "caption": "Figure 22: The frequency of agreement with model",
      "page": 23
    },
    {
      "caption": "Figure 22: For all datasets, on average, human an-",
      "page": 23
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , we note Inoursubsequentexperiments,weaimtounder-",
      "data": [
        {
          "0.52": "0.044",
          "0.19": "0.15",
          "0.24": "0.11",
          "0.25": "0.094"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "65"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.24": "0.41",
          "0.16": "0.59",
          "0.51": "0.12"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Qwen-VL\nLLaVA\nLLaVA-NEXT",
          "Column_2": "",
          "Column_3": "",
          "Column_4": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.6 egnahC\n0.4\n0.2 saiB\n0.0\nevitisoP\n0.2\n0.4 Positive Persona\nNegative Persona\nQwen-VL LLaVA LLaVA-Next GPT4-o": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "Positive Persona",
          "Column_6": "",
          "Column_7": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "I II\nFigure7: Humanagreementwithmodelpredictions(blue),groundtruthfromdataset\nandneitherlabel(red)fordifferentmodelsanderrorcategories(fromLefttoRight: E\nand arousal/intensity category, but are not thecapabilityofVLMs\nthe same fine-grained class (eg., \"fear\" and unreliablegroundtruth\n\"anger\"). datasets. Itcanalsobe\ncategories, the proporti\nWehypothesizethatblatanterrorsinECIcanbe agreeingwithneitherth\nattributedtothemodel’sinabilitytoreasonabout prediction(labeledas\"N\naffect. However,themorenuancederrors(IIand smallandconstant. Thi\nIII) could be caused by subjective interpretation two emotion labels (gr\nofcloselyrelated,distinctemotions. Weconduct dicted)orbothwerefou\na manual evaluation study to explore this further, cases,showingthathum\nannotatingabout500errorsamples. Eachannota- egory(asmeasuredby\ntion denotes whether a human rater agrees more isclearandtrustworthy\nwith the model-predictedemotion label, with the theAppendix(B.4.4),th\noriginalgroundtruthlabelfromthedataset, with onwhichmodelsperfor": "",
          "I II": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "III"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Amusement\nPre P d r i o c b t a i b o i n l : i t A y m : u s 0 e . m 7 e 5 nt Pr P e r d o i b c a t b i i o l n i : t A y m : u 0 s . e 3 m 8 ent Prediction: Sadness Pr P e r d o i b c a t b i i o l n i : t y A m > u s 0 e . m 8 ent": "",
          "Column_2": "Anger\nPrediction: Anger Prediction: Anger Prediction: Amusement Prediction: Anger\nProbability: 0.76 Probability: 0.41 Probability > 0.8",
          "Column_3": ""
        },
        {
          "Amusement\nPre P d r i o c b t a i b o i n l : i t A y m : u s 0 e . m 7 e 5 nt Pr P e r d o i b c a t b i i o l n i : t A y m : u 0 s . e 3 m 8 ent Prediction: Sadness Pr P e r d o i b c a t b i i o l n i : t y A m > u s 0 e . m 8 ent": "Awe\nP P r r o e b d a i b c i t l i i o t n y : : A 0 w . e 75 P P r r o e b d a i b c i t l i i o t n y : : A 0 w . e 36 Prediction: Sadness P P r r o e b d a i b c i t l i i o t n y : > A w 0 e .8",
          "Column_2": "",
          "Column_3": ""
        },
        {
          "Amusement\nPre P d r i o c b t a i b o i n l : i t A y m : u s 0 e . m 7 e 5 nt Pr P e r d o i b c a t b i i o l n i : t A y m : u 0 s . e 3 m 8 ent Prediction: Sadness Pr P e r d o i b c a t b i i o l n i : t y A m > u s 0 e . m 8 ent": "Disgust\nPrediction: Disgust Prediction: Disgust Prediction: Amusement Prediction: Disgust\nProbability: 0.74 Probability: 0.36 Probability > 0.8",
          "Column_2": "",
          "Column_3": ""
        },
        {
          "Amusement\nPre P d r i o c b t a i b o i n l : i t A y m : u s 0 e . m 7 e 5 nt Pr P e r d o i b c a t b i i o l n i : t A y m : u 0 s . e 3 m 8 ent Prediction: Sadness Pr P e r d o i b c a t b i i o l n i : t y A m > u s 0 e . m 8 ent": "Fear\nP P r r o e b d a i b c i t l i i o t n y : : F 0 e . a 7 r 5 P P r r e o d b i a c b t i i l o i n t : y : F 0 e . a 3 r 6 Prediction: Amusement P P r r o e b d a i b c i t l i i o t n y : > F e 0 a . r 8",
          "Column_2": "",
          "Column_3": ""
        },
        {
          "Amusement\nPre P d r i o c b t a i b o i n l : i t A y m : u s 0 e . m 7 e 5 nt Pr P e r d o i b c a t b i i o l n i : t A y m : u 0 s . e 3 m 8 ent Prediction: Sadness Pr P e r d o i b c a t b i i o l n i : t y A m > u s 0 e . m 8 ent": "Sadness\nP P r r e o d b i a c b t i i l o i n t : y : S a 0 d . n 7 e 5 ss P P r r e o d b i a c b t i i l o i n t : y S : a d 0 n . e 3 s 4 s Prediction: Anger P P r r e o d b i a c b t i i l o i n t : y S > a d 0 n . e 8 ss",
          "Column_2": "",
          "Column_3": ""
        },
        {
          "Amusement\nPre P d r i o c b t a i b o i n l : i t A y m : u s 0 e . m 7 e 5 nt Pr P e r d o i b c a t b i i o l n i : t A y m : u 0 s . e 3 m 8 ent Prediction: Sadness Pr P e r d o i b c a t b i i o l n i : t y A m > u s 0 e . m 8 ent": "Contentment Excitement\nPrediction: Sadness Prediction: Contentment Prediction: Fear Prediction: Excitement\nProbability > 0.8 Probability > 0.8",
          "Column_2": "",
          "Column_3": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Amusement\nPrediction: Amusement Prediction: Amusement Prediction: Excitement Prediction: Amusement\nProbability: 0.77 Probability: 0.38 Probability > 0.8": "Anger\nPrediction: Anger Prediction: Anger Prediction: Sadness Prediction: Anger\nProbability: 0.78 Probability: 0.49 Probability > 0.8",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": ""
        },
        {
          "Amusement\nPrediction: Amusement Prediction: Amusement Prediction: Excitement Prediction: Amusement\nProbability: 0.77 Probability: 0.38 Probability > 0.8": "",
          "Column_2": "Awe\nPrediction: Awe Prediction: Awe Prediction: Sadness Prediction: Awe\nProbability: 0.78 Probability: 0.41 Probability > 0.8",
          "Column_3": "",
          "Column_4": "",
          "Column_5": ""
        },
        {
          "Amusement\nPrediction: Amusement Prediction: Amusement Prediction: Excitement Prediction: Amusement\nProbability: 0.77 Probability: 0.38 Probability > 0.8": "",
          "Column_2": "",
          "Column_3": "Disgust\nPrediction: Disgust Prediction: Disgust Prediction: Amusement Prediction: Disgust\nProbability: 0.69 Probability: 0.44 Probability > 0.8",
          "Column_4": "",
          "Column_5": ""
        },
        {
          "Amusement\nPrediction: Amusement Prediction: Amusement Prediction: Excitement Prediction: Amusement\nProbability: 0.77 Probability: 0.38 Probability > 0.8": "",
          "Column_2": "",
          "Column_3": "Excitement\nPrediction: Excitement Prediction: Excitement Prediction: Fear Prediction: Excitement\nProbability: 0.73 Probability: 0.53 Probability > 0.8\nFear\nP P r r o e b d a i b c i t l i i o t n y : : F 0 e . a 6 r 7 P P r r o e b d a i b c i t l i i o t n y : : F 0 e . a 4 r 9 Prediction: Amusement P P r r o e b d a i b c i t l i i o t n y : > F e 0 a . r 8",
          "Column_4": "",
          "Column_5": ""
        },
        {
          "Amusement\nPrediction: Amusement Prediction: Amusement Prediction: Excitement Prediction: Amusement\nProbability: 0.77 Probability: 0.38 Probability > 0.8": "",
          "Column_2": "",
          "Column_3": "Contentment Sadness\nPrediction: Sadness Prediction: Contentment Prediction: Amusement Prediction: Sadness\nProbability > 0.8 Probability > 0.8",
          "Column_4": "",
          "Column_5": ""
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.63": "0.47"
        },
        {
          "0.63": "0.50"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.09": "0.06"
        },
        {
          "0.09": "0.22"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.51": "0.26"
        },
        {
          "0.51": "0.30"
        },
        {
          "0.51": "0.37"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.37": "0.37"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table 9: , we first present",
      "data": [
        {
          "0.92": "0.97"
        },
        {
          "0.92": "0.95"
        },
        {
          "0.92": "0.85"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table 9: ) and accompany that with",
      "data": [
        {
          "0.23": "0.15"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table 9: , we first present",
      "data": [
        {
          "0.23": "0.05"
        },
        {
          "0.23": "0.03"
        },
        {
          "0.23": "0.12"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table 9: , we first present",
      "data": [
        {
          "0.62": "0.78"
        },
        {
          "0.62": "0.64"
        }
      ],
      "page": 21
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Shyamal Anadkat, et al. 2023. Gpt-4 Technical Report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "venue": "Shyamal Anadkat, et al. 2023. Gpt-4 Technical Report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "Angrybert: Joint Learning Target And Emotion For Hate Speech Detection",
      "authors": [
        "Md Rabiul Awal",
        "Rui Cao",
        "Ka-Wei Lee",
        "Sandra Mitrović"
      ],
      "year": "2021",
      "venue": "Pacific-Asia conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "3",
      "title": "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Shusheng Yang",
        "Shijie Wang",
        "Sinan Tan",
        "Peng Wang",
        "Junyang Lin",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities",
      "arxiv": "arXiv:2308.12966"
    },
    {
      "citation_id": "4",
      "title": "Socratis: Are Large Multimodal Models Emotionally Aware? arXiv preprint",
      "authors": [
        "Katherine Deng",
        "Arijit Ray",
        "Reuben Tan",
        "Saadia Gabriel",
        "Bryan Plummer",
        "Kate Saenko"
      ],
      "year": "2023",
      "venue": "Socratis: Are Large Multimodal Models Emotionally Aware? arXiv preprint",
      "arxiv": "arXiv:2308.16741"
    },
    {
      "citation_id": "5",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations"
    },
    {
      "citation_id": "6",
      "title": "Contextual Emotion Recognition Using Large Vision Language Models",
      "authors": [
        "Yasaman Etesam",
        "Özge Nilay Yalçın",
        "Chuxuan Zhang",
        "Angelica Lim"
      ],
      "year": "2024",
      "venue": "Contextual Emotion Recognition Using Large Vision Language Models",
      "arxiv": "arXiv:2405.08992"
    },
    {
      "citation_id": "7",
      "title": "Massively Multi-cultural Knowledge Acquisition & Lm Benchmarking",
      "authors": [
        "Yi Fung",
        "Ruining Zhao",
        "Jae Doo",
        "Chenkai Sun",
        "Heng Ji"
      ],
      "year": "2024",
      "venue": "Massively Multi-cultural Knowledge Acquisition & Lm Benchmarking",
      "arxiv": "arXiv:2402.09369"
    },
    {
      "citation_id": "8",
      "title": "Measuring Massive Multitask Language Understanding",
      "authors": [
        "Dan Hendrycks",
        "Collin Burns",
        "Steven Basart",
        "Andy Zou",
        "Mantas Mazeika",
        "Dawn Song",
        "Jacob Steinhardt"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "9",
      "title": "Context-aware Emotion Recognition Based On Visual Relationship Detection",
      "authors": [
        "Manh-Hung Hoang",
        "Soo-Hyung Kim",
        "Hyung-Jeong Yang",
        "Guee-Sang Lee"
      ],
      "year": "2021",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2021.3091169"
    },
    {
      "citation_id": "10",
      "title": "A Survey On Sentiment Analysis And Opinion Mining",
      "authors": [
        "Seema Rao"
      ],
      "year": "2016",
      "venue": "Proceedings of the International Conference on Advances in Information Communication Technology & Computing, AICTC '16",
      "doi": "10.1145/2979779.2979832"
    },
    {
      "citation_id": "11",
      "title": "Do Moral Judgment And Reasoning Capability Of Llms Change With Language? A Study Using The Multilingual Defining Issues Test",
      "authors": [
        "Aditi Khandelwal",
        "Utkarsh Agarwal",
        "Kumar Tanmay",
        "Monojit Choudhury"
      ],
      "year": "2024",
      "venue": "Do Moral Judgment And Reasoning Capability Of Llms Change With Language? A Study Using The Multilingual Defining Issues Test",
      "arxiv": "arXiv:2402.02135"
    },
    {
      "citation_id": "12",
      "title": "Development And Validation Of Image Stimuli For Emotion Elicitation (isee): A Novel Affective Pictorial System With Testretest Repeatability",
      "authors": [
        "Hanjoo Kim",
        "Xin Lu",
        "Michael Costa",
        "Baris Kandemir",
        "Reginald Adams",
        "Jia Li",
        "James Wang",
        "Michelle Newman"
      ],
      "year": "2018",
      "venue": "Psychiatry Research"
    },
    {
      "citation_id": "13",
      "title": "Emotion Recognition And Its Applications. Human-computer systems interaction: Backgrounds and applications 3",
      "authors": [
        "Agata Kołakowska",
        "Agnieszka Landowska",
        "Mariusz Szwoch",
        "Wioleta Szwoch",
        "Michal Wrobel"
      ],
      "year": "2014",
      "venue": "Emotion Recognition And Its Applications. Human-computer systems interaction: Backgrounds and applications 3"
    },
    {
      "citation_id": "14",
      "title": "Context-aware Emotion Recognition Networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "15",
      "title": "Large Language Models Understand And Can Be Enhanced By Emotional Stimuli",
      "authors": [
        "Cheng Li",
        "Jindong Wang",
        "Yixuan Zhang",
        "Kaijie Zhu",
        "Wenxin Hou",
        "Jianxun Lian",
        "Fang Luo",
        "Qiang Yang",
        "Xing Xie"
      ],
      "year": "2023",
      "venue": "Large Language Models Understand And Can Be Enhanced By Emotional Stimuli",
      "arxiv": "arXiv:2307.11760"
    },
    {
      "citation_id": "16",
      "title": "Qiang Yang, and Xing Xie. 2024. The Good, The Bad, And Why: Unveiling Emotions In Generative Ai",
      "authors": [
        "L Cheng",
        "Jindong Wang",
        "Yixuan Zhang",
        "Kaijie Zhu",
        "Xinyi Wang",
        "Wenxin Hou",
        "Jianxun Lian",
        "Fang Luo"
      ],
      "venue": "Forty-first International Conference on Machine Learning"
    },
    {
      "citation_id": "17",
      "title": "Enhancing Emotional Generation Capability of Large Language Models via Emotional Chain-of-Thought",
      "authors": [
        "Zaijing Li",
        "Gongwei Chen",
        "Rui Shao",
        "Dongmei Jiang",
        "Liqiang Nie"
      ],
      "year": "2024",
      "venue": "Enhancing Emotional Generation Capability of Large Language Models via Emotional Chain-of-Thought",
      "arxiv": "arXiv:2401.06836"
    },
    {
      "citation_id": "18",
      "title": "LLaVA-NeXT: Improved Reasoning, OCR, and World Knowledge",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Yuheng Li",
        "Bo Li",
        "Yuanhan Zhang",
        "Sheng Shen",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "LLaVA-NeXT: Improved Reasoning, OCR, and World Knowledge"
    },
    {
      "citation_id": "19",
      "title": "The 38th Conference on Neural Information Processing Systems (NeurIPS)",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "The 38th Conference on Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "20",
      "title": "Emollms: A Series Of Emotional Large Language Models And Annotation Tools For Comprehensive Affective Analysis",
      "authors": [
        "Zhiwei Liu",
        "Kailai Yang",
        "Tianlin Zhang",
        "Qianqian Xie",
        "Zeping Yu",
        "Sophia Ananiadou"
      ],
      "year": "2024",
      "venue": "Emollms: A Series Of Emotional Large Language Models And Annotation Tools For Comprehensive Affective Analysis",
      "arxiv": "arXiv:2401.08508"
    },
    {
      "citation_id": "21",
      "title": "Learn To Explain: Multimodal Reasoning Via Thought Chains For Science Question Answering",
      "authors": [
        "Pan Lu",
        "Swaroop Mishra",
        "Tony Xia",
        "Liang Qiu",
        "Kai-Wei Chang",
        "Song-Chun Zhu",
        "Oyvind Tafjord",
        "Peter Clark",
        "Ashwin Kalyan"
      ],
      "year": "2022",
      "venue": "The 36th Conference on Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "22",
      "title": "An Investigation Into Three Visual Characteristics Of Complex Scenes That Evoke Human Emotion",
      "authors": [
        "Xin Lu",
        "Reginald Adams",
        "Jia Li",
        "Michelle Newman",
        "James Wang"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "23",
      "title": "Affective Image Classification using Features Inspired by Psychology and Art Theory",
      "authors": [
        "Jana Machajdik",
        "Allan Hanbury"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "24",
      "title": "Emotional Category Data on Images from the International Affective Picture System",
      "authors": [
        "Barbara Joseph A Mikels",
        "Fredrickson",
        "Casey Gregory R Larkin",
        "Sam Lindberg",
        "Patricia Maglio",
        "Reuter-Lorenz"
      ],
      "year": "2005",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "25",
      "title": "Emoticon: Context-aware Multimodal Emotion Recognition Using Frege's Principle",
      "authors": [
        "Trisha Mittal",
        "Pooja Guhan",
        "Uttaran Bhattacharya",
        "Rohan Chandra",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "26",
      "title": "Rectified Linear Units Improve Restricted Boltzmann Machines",
      "authors": [
        "Vinod Nair",
        "Geoffrey Hinton"
      ],
      "year": "2010",
      "venue": "Proceedings of the 27th International Conference on Machine Learning"
    },
    {
      "citation_id": "27",
      "title": "A Mixed Bag of Emotions: Model, Predict, and Transfer Emotion Distributions",
      "authors": [
        "Kuan-Chuan",
        "Tsuhan Peng",
        "Amir Chen",
        "Andrew Sadovnik",
        "Gallagher"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
      "doi": "10.1109/CVPR.2015.7298687"
    },
    {
      "citation_id": "28",
      "title": "",
      "authors": [
        "Ciaran Regan",
        "Nanami Iwahashi",
        "Shogo Tanaka",
        "Mizuki Oka"
      ],
      "year": "2024",
      "venue": "",
      "doi": "10.48550/arXiv.2402.04232",
      "arxiv": "arXiv:2402.04232"
    },
    {
      "citation_id": "29",
      "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics"
    },
    {
      "citation_id": "30",
      "title": "Imagenet-21k Pretraining For The Masses",
      "authors": [
        "Tal Ridnik",
        "Emanuel Ben-Baruch",
        "Asaf Noy",
        "Lihi Zelnik-Manor"
      ],
      "year": "2021",
      "venue": "The 35th Conference on Neural Information Processing Systems (NeurIPS)(Datasets and Benchmarks Track)"
    },
    {
      "citation_id": "31",
      "title": "Imagenet Large Scale Visual Recognition Challenge",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang",
        "Andrej Karpathy",
        "Aditya Khosla",
        "Michael Bernstein"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "32",
      "title": "Opinion Mining, Sentiment Analysis And Emotion Understanding In Advertising: A Bibliometric Analysis",
      "authors": [
        "Pablo Sánchez-Núñez",
        "Manuel Cobo",
        "Carlos De",
        "Las Heras-Pedrosa",
        "Ignacio Peláez",
        "Enrique Herrera-Viedma"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.3009482"
    },
    {
      "citation_id": "33",
      "title": "Faisal Azhar, et al. 2023. Llama: Open And Efficient Foundation Language Models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro"
      ],
      "venue": "Faisal Azhar, et al. 2023. Llama: Open And Efficient Foundation Language Models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "34",
      "title": "2023a. Unlocking the Emotional World of Visual Media: An Overview of the Science, Research, and Impact of Understanding Emotion",
      "authors": [
        "James Wang",
        "Sicheng Zhao",
        "Chenyan Wu",
        "Reginald Adams",
        "Michelle Newman",
        "Tal Shafir",
        "Rachelle Tsachor"
      ],
      "venue": "Proceedings of the IEEE",
      "doi": "10.1109/JPROC.2023.3273517"
    },
    {
      "citation_id": "35",
      "title": "2023b. Emotional Intelligence of Large Language Models",
      "authors": [
        "Xuena Wang",
        "Xueting Li",
        "Zi Yin",
        "Yue Wu",
        "Jia Liu"
      ],
      "venue": "Journal of Pacific Rim Psychology"
    },
    {
      "citation_id": "36",
      "title": "Chain-of-thought Prompting Elicits Reasoning In Large Language Models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Fei Xia",
        "Ed Chi",
        "V Quoc",
        "Denny Le",
        "Zhou"
      ],
      "year": "2022",
      "venue": "The 36th Conference on Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "37",
      "title": "Reduced Early Visual Emotion Discrimination As An Index Of Diminished Emotion Processing In Parkinson's Disease?evidence From Event-related Brain Potentials",
      "authors": [
        "Matthias Wieser",
        "Elisabeth Klupp",
        "Peter Weyers",
        "Paul Pauli",
        "David Weise",
        "Daniel Zeller",
        "Joseph Classen",
        "Andreas Mühlberger"
      ],
      "year": "2012",
      "venue": "Cortex"
    },
    {
      "citation_id": "38",
      "title": "Bloom: A 176b-parameter Open-access Multilingual Language Model",
      "authors": [
        "Bigscience Workshop",
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Ilić",
        "Roman Hesslow",
        "Alexandra Castagné",
        "François Sasha Luccioni",
        "Yvon"
      ],
      "year": "2022",
      "venue": "Bloom: A 176b-parameter Open-access Multilingual Language Model",
      "arxiv": "arXiv:2211.05100"
    },
    {
      "citation_id": "39",
      "title": "Ioanna Ntinou, Ioannis Patras, and Georgios Tzimiropoulos. 2024. Vllms Provide Better Context For Emotion Understanding Through Common Sense Reasoning",
      "authors": [
        "Alexandros Xenos",
        "Niki Foteinopoulou"
      ],
      "venue": "Ioanna Ntinou, Ioannis Patras, and Georgios Tzimiropoulos. 2024. Vllms Provide Better Context For Emotion Understanding Through Common Sense Reasoning",
      "arxiv": "arXiv:2404.07078"
    },
    {
      "citation_id": "40",
      "title": "Emovit: Revolutionizing Emotion Insights With Visual Instruction Tuning",
      "authors": [
        "Hongxia Xie",
        "Chu-Jun Peng",
        "Yu-Wen Tseng",
        "Hung-Jen Chen",
        "Chan-Feng Hsu",
        "Hong-Han Shuai",
        "Wen-Huang Cheng"
      ],
      "year": "2024",
      "venue": "Emovit: Revolutionizing Emotion Insights With Visual Instruction Tuning",
      "arxiv": "arXiv:2404.16670"
    },
    {
      "citation_id": "41",
      "title": "Mdan: Multi-level Dependent Attention Network For Visual Emotion Analysis",
      "authors": [
        "Liwen Xu",
        "Zhengtao Wang",
        "Bin Wu",
        "Simon Lui"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "42",
      "title": "EmoSet: A Large-Scale Visual Emotion Dataset with Rich Attributes",
      "authors": [
        "Jingyuan Yang",
        "Qirui Huang",
        "Tingting Ding",
        "Dani Lischinski",
        "Danny Cohen-Or",
        "Hui Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "43",
      "title": "Yuxuan Ding, and Xinbo Gao. 2021. Stimuli-aware Visual Emotion Analysis",
      "authors": [
        "Jingyuan Yang",
        "Jie Li",
        "Xiumei Wang"
      ],
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "44",
      "title": "Building a Large Scale Dataset for Image Emotion Recognition: The Fine Print and the Benchmark",
      "authors": [
        "Quanzeng You",
        "Jiebo Luo",
        "Jin Hailin",
        "Jianchao Yang"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "45",
      "title": "Predicting Personalized Image Emotion Perceptions In Social Networks",
      "authors": [
        "Sicheng Zhao",
        "Hongxun Yao",
        "Yue Gao"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2016.2628787"
    },
    {
      "citation_id": "46",
      "title": "The moral integrity corpus: A benchmark for ethical dialogue systems",
      "authors": [
        "Caleb Ziems",
        "Jane Yu",
        "Yi-Chia Wang",
        "Alon Halevy",
        "Diyi Yang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.261"
    },
    {
      "citation_id": "47",
      "title": "EmoSet-Hard FI-Hard Abstract ArtPhoto Emotion",
      "year": "1980",
      "venue": "EmoSet-Hard FI-Hard Abstract ArtPhoto Emotion"
    }
  ]
}