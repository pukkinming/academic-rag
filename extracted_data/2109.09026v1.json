{
  "paper_id": "2109.09026v1",
  "title": "Hybrid Data Augmentation And Deep Attention-Based Dilated Convolutional-Recurrent Neural Networks For Speech Emotion Recognition",
  "published": "2021-09-18T23:13:44Z",
  "authors": [
    "Nhat Truong Pham",
    "Duc Ngoc Minh Dang",
    "Sy Dzung Nguyen"
  ],
  "keywords": [
    "Speech emotion recognition",
    "WaveGAN",
    "Spec-GAN",
    "Pitch shifting",
    "Time shifting"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) has been one of the significant tasks in Human-Computer Interaction (HCI) applications. However, it is hard to choose the optimal features and deal with imbalance labeled data. In this article, we investigate hybrid data augmentation (HDA) methods to generate and balance data based on traditional and generative adversarial networks (GAN) methods. To evaluate the effectiveness of HDA methods, a deep learning framework namely (ADCRNN) is designed by integrating deep dilated convolutional-recurrent neural networks with an attention mechanism. Besides, we choose 3D log Mel-spectrogram (MelSpec) features as the inputs for the deep learning framework. Furthermore, we reconfigure a loss function by combining a softmax loss and a center loss to classify the emotions. For validating our proposed methods, we use the EmoDB dataset that consists of several emotions with imbalanced samples. Experimental results prove that the proposed methods achieve better accuracy than the state-of-the-art methods on the EmoDB with 87.12% and 88.47% for the traditional and GANbased methods, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "S PEECH emotion recognition has a significant role in a lot of applications like e-learning, healthcare, HCI, customer relationship management (CRM), robotics, and video games  [1] -  [3] . In the past decades, SER has been one of the hot research topics in the speech processing areas. Most of previous works tried to use different type of features for the SER, such as pitch, energy, zero-crossing rate (ZCR), formants, root mean square error (RMSE), prosodic  [4] -  [10] , Mel-frequency cepstrum coefficients (MFCC)  [11] -  [13] , linear predictive coding (LPC) and log frequency power coefficients (LFPC)  [14] . Besides, the researcher tried to reach a variety of classification schemes to classify or discriminate the emotions like hidden Markov model (HMM), Gaussian mixture model (GMM), support vector machine (SVM), k-nearest neighbors (K-NN), and Bayesian logistic regression  [4] -  [16] . Thanks to the development of deep learning, deep neural networks (DNN) have been used to automatically extract features for the SER instead of handcrafting  [17] -  [19] . Practically, deep convolutional neural networks (CNN) and long short-term memory (LSTM) have been successfully used to extract and exploit the time-frequency domain-based features from spectrograms  [20] -  [23] . The other studies applied an attention mechanism to the SER to obtain the most utterance features that represent the corresponding emotion  [24] -  [26] . However, using the hierarchical and complex deep learning model requires higher computational resources and a large labeled dataset to get better accuracy. Unfortunately, it takes a lot of time and cost to collect and annotate the dataset because each utterance might consist of multiple or ambiguous emotions. Therefore, either traditional or advanced data augmentation methods have been applied to generate and synthesize randomly the training data samples  [27] -  [31] . These studies only focused on using either GAN from feature space or adding noise/oversampling technique to generate and balance data samples, however, they are limited with sequence model and data space approaches.\n\nIn this article, we propose the HDA methods that combine traditional and GAN-based methods to generate and balance the labeled dataset. Then, the 3D log MelSpec low-level features are extracted as the inputs for the deep dilated convolutional-recurrent neural networks (DCRNN). The deep DCRNN learn and extract the high-level representations that are then fed into an attention layer to exploit the utterance-level features. Finally, we combine the softmax loss and center loss to classify the emotions from speech. We use the EmoDB for both the HDA state and emotion recognition state. The main contributions of this article are listed below:\n\n• We utilize the HDA methods that consist of time shifting, pitch shifting, WaveGAN, and SpecGAN to generate and balance samples on the EmoDB and IEMOCAP dataset. • After that, we implement the deep ADCRNN to learn and extract utterance-level features from the generated 3D log MelSpec low-level features. • Then, the loss function is reconfigured by combining the softmax and center loss to classify the emotional speech from both original and augmented data.   [32] . Lamiaa Abdel-Hamid used prosodic, spectral and wavelet features that consist of the pitch, intensity, formants, MFCC, long-term average spectrum (LTAS), and wavelet to investigate for the SER  [33] . Atalay et al. compared the feature selection techniques with MFCC features that include autoencoder, Chi-Square, and relief-F for emotion recognition in voice  [34] . Chen et al. proposed a two-layer fuzzy multiple random forests (TLFMRF) algorithm to classify the emotion from extracted features that fuse from personalize and nonpersonalized features and separate into emotional classes by fuzzy C-means clustering technique  [35] . Huang et al. proposed a feature extraction method upon wavelet packet (WP) filterbank for the SER that outperforms the MFCC features and can be used for 2D facial emotion recognition (FER) and audio-visual bimodal emotion recognition system  [36] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Deep Learning For Ser",
      "text": "Over the last decade, with the development of neural networks and deep learning, deep CNN and LSTM have been employed to extract features from the spectrogram representations of raw audio and classify emotions for the SER systems. Zhang et al. investigated the deep CNN to extract 3D log MelSpec features, then designed a discriminant temporal pyramid matching (DTPM) strategy to concatenate the learned segment-level features, and used SVM classifier to recognize the emotions  [21] . Tzirakis et al. proposed an end-to-end multimodal that consists of CNN to extract speech features and a deep residual network of 50 layers (ResNet50) to extract visual features, then fed into two LSTM layers to extract the important features for the SER  [37] . Zhao et al. designed 1D and 2D CNN with the LSTM for the SER that not only overcomes the shortcoming of the CNN and the LSTM but also takes advantage of the strength of them  [22] . Sajjad et al. proposed a method upon radial basis function network (RBFN) to clustering the key sequence segment, then all selected sequences are converted into spectrograms to extract features by CNN and learn the temporal information for classifying the emotions by bidirectional LSTM  [38] . Yao et al. investigated a fusion of 3 classifiers upon multi-task learning that consists of MelSpec combined with CNN (MS-CNN), low-level descriptors combined with recurrent neural networks (LLD-RNN), and hight-level statistical functions combined with deep neural network (HSF-DNN) for the SER  [39] . Meng et al. proposed a novel architecture for the SER using dilated CNN with residual block and bidirectional LSTM based on attention mechanism (ADRNN)  [40] . The ADRNN extracts the features and learn representation from 3D log MelSpec and then classifies the emotions using the loss function that applies the center loss together with the softmax loss.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Attention Mechanism For Ser",
      "text": "Since not all features equally contributed to recognizing the emotion from speech, recent studies have employed an attention mechanism for SER. Meng, Chen, and Xie et al. employed the attention-based LSTM to learn the relevant highlevel features representing for emotion states  [24] ,  [40, 41]    [44] . Ho et al. used the self-attention for RNN to exploit the context for each time step, then used the multihead attention to fuse all representatives for predicting the emotions  [45] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Adversarial Data Augmentation For Ser",
      "text": "To deal with imbalanced data and reduce overfitting, researchers have been used the data augmentation method to generate or synthesize data samples. Huang et al. proposed the data augmentation method for the training data by replacing the source data samples with the shorter overlapping samples extracted from them  [46] . Park et al. proposed a SpecAugment method for speech recognition  [47] . SpecAugment includes features warping, frequency masking, and time masking that are applied to the inputs of a neural network. Rebai et al.\n\nproposed a new DNN architecture taking advantage of both data augmentation and ensemble approaches to improve the accuracy of emotion recognition  [48] . In recent years, GANbased techniques have been developed to improve the accuracy of emotion recognition as a data augmentation method. Sahu et al. used the applications of GAN to synthesizing features vectors for the SER that enhances the performance of classification  [49] . Yi et al. proposed an adversarial data augmentation network (ADAN) that includes an autoencoder feature selection, a GAN, and an auxiliary classifier to improve the SER  [50] . The ADAN using the Wasserstein divergence instead of cross-entropy loss for training the GAN to generate feature vectors in both the original feature space and the latent space. Bao and Vu et al. investigated a method upon Cycle consistent adversarial networks (CycleGAN) that transfers the feature vectors from a large speech corpus without labeled into synthetic features of emotion styles to improve classification performance  [51] . Eskimez and Chatziagapi et al. proposed a GAN method upon CNN to generate the spectrograms for training the SER model  [52, 53] .\n\nThis study is motivated by the WaveGAN and SpecGAN in  [54] , the works in  [55, 56] , the deep learning architecture for 3D log MelSpec in  [40] , and the loss function for speech emotion recognition combined contrastive-center (CT-C) loss with softmax loss proposed by Pham et al.  [57] . We combine these motivated approaches to conduct our work in the following aspects:\n\n• First, we apply and implement the WaveGAN, SpecGAN, pitch shifting, and time shifting as HDA methods to generate and synthesize training dataset. • Second, the ADRNN is modified by removing all batch normalization (BN), then we also use a fully connected layer (FCN) with 64 units to obtain the reconfigured loss function.\n\n• Third, we apply different loss functions, such as the softmax loss, the reconfigured softmax loss + center loss, the softmax loss + the center loss in  [40] , and the CT-C loss + softmax loss in  [57]  to validate the proposed method. We run several experiments to compare these loss functions with each other and with the previous works.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Methodologies",
      "text": "Since the distribution of emotions in almost benchmark datasets and natural speech signals are not balance and lack of data. In this section, we utilize a baseline architecture for the SER system that deals with these problems and improves the recognition rate of emotions. The baseline architecture is shown in Fig.  1  consisting of six main blocks: Hybrid Data Augmentation block, 3D log MelSpec Generator block, DCRNN block, Attention block, Center block, and Softmax block. Table  I  shows the notations and their corresponding description used in this study. In the baseline architecture, we design the deep ADCRNN to learn and extract high-level representations from 3D log MelSpec low-level features for the SER. The deep ADCRNN is based on the ADRNN, but we modify it a bit by removing all BN layers after the dilated CNN layers. To overcome the challenging in RNN, such as complex dependencies, vanishing, and exploding gradients, we proposed dilated LSTMs to replace the BiLSTM. Furthermore, we use an FCN layer with 64 units to compute the center loss before the down-sampling shape to E classes to compute the softmax loss. This work is quite different from the ADRNN because the center loss and the softmax loss in the ADRNN are computed after downsampling shape to E classes. The baseline architecture in Fig.  1  is designed as follows:\n\n• First, the speech signals are augmented to generate and balance data by the hybrid data augmentation. • Second, we use a CNN layer to perform on 3D log MelSpec low-level features extracted by the 3D log MelSpec generator. • Third, we add 3 dilated CNN layers with residual block to extract temporal features. • Next, all feature maps are the input for the bidirectional LSTM to learn sequential features. • Then, we add an attention layer to exploit the utterancelevel features from sequential features. • Finally, a loss function is used to classify the emotion by combining the softmax loss and center loss for the SER. The structure of this section is organized as follows: The hybrid data augmentation methods with two approaches are presented in Subsection III-A; the 3D log MelSpec extraction is described in Subsection III-B; and in Subsection III-C, we present a deep learning framework included the deep DCRNN architecture, attention layer, and the loss function to construct the baseline architecture for the SER system.\n\nA. Hybrid Data Augmentation (HDA) Methods 1) Traditional Approaches: a) Time Shifting: Given a signal ω(t), we can shift the wave of the signal forward or backward by adding or subtracting a finite time τ , respectively. The output χ(t) after shifting is defined as follows:\n\nwhere τ = sr/100 and the sr is the sampling rate of the signal in this study. As using the time shifting, the signal is only shifted the position forward or backward without changing its amplitude. However, in this study, we not only want to shift the signal along its time but also want to roll it. Fig.  2  describes the examples of the time shifting and rolling in detail.\n\nb) Pitch Shifting: Pitch shifting is the efficient algorithm proposed by Lent in  [55] . This algorithm was based on the time stretching and resampling methods  [56] . In this study, pitch shifting is presented as in Fig.  3 .\n\nGiven the number of half-steps nhs and the number of bins nbins in each octave, the time stretching is obtained by computing the time stretching ratio S ratio as follows:\n\nThe resampling is obtained by computing the resampling ratio R ratio as follows:\n\nwhere T sr and S sr are the sampling rate of the target signal and the sampling rate of the source signal, respectively. If the R ratio > 1, then the pitch shifting signal is sped up,    II . In which, z, S, n ch, and the L out are the uniform distribution of 100 dimensions, the length of slice, the number of channels, and the linear output of the WaveGAN, respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B) Brief Introduction Of Specgan:",
      "text": "SpecGAN is also proposed in  [54]  that generates semiinvertible spectrograms to reconstruct frequency-domain audio or waveform. The SpecGAN is designed as follows:\n\n• First, the frequency-domain audio is converted to the spectrograms by obtaining the short-time Fourier transform (STFT) with the length of windows of 16 ms, the overlap between the successive windows of 8 ms, and the fast Fourier transform (FFT) size of 128. The spectrograms are scaled logarithmically to get better alignment.\n\n• Next, the spectrograms are clipped to three standard deviations and normalized to [-1, 1] scale. • Then, the DCGAN is applied to train and generate the spectrograms.\n\n• Finally, the Griffin-Lim algorithm  [60]  is employed with 16 iterations to convert the generated spectrograms to audio samples and estimate the phase. The parameters of the SpecGAN are described in detail in Table  III .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. 3D Log Melspec Extraction",
      "text": "In this study, we choose the 3D log MelSpec that are lowlevel features as the inputs for the deep ADCRNN model. The 3D log MelSpec low-level features consist of static, deltas, and delta-deltas coefficients are obtained as follows:\n\n• First, the audio samples are converted to the MelSpec by performing the STFT with the length of windows of 25 ms, the overlap between the successive windows of 10 ms, the number of filterbanks of 40, the frame rate of 16 kHz, resulting in 512 frequency bins (corresponding the FFT size of 512) with linear spaces from 300 Hz to 8 kHz. • Next, the static coefficient is obtained by scaling logarithmically the MelSpec. • Then, the deltas coefficient is obtained by computing the derivative of the static coefficient. • Finally, the delta-deltas coefficient is obtained by computing the derivative of the deltas coefficient.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Deep Learning Framework 1) Deep Adcrnn Architecture:",
      "text": "With the extracted 3D log MelSpec low-level features, the deep ADCRNN is used to learn and extract the high-level representation. The deep ADCRNN consists of 1 normal CNN layer, one max-pooling layer, 3 dilated CNN layers with skip dilated CNN connection, 1 linear layer, and 2 dilated LSTM layers. The first CNN layer has 3 × 3 kernel size, 128 feature maps, stride of 1, and valid padding. Each dilated CNN layer has 256 feature maps with 3×3 kernel size, and same padding. The dilation rate is set to 2 for the dilated CNN while it is set as list of (1, 2) for the dilated LSTM in this study. We only add the max-pooling layer after the first CNN layer to downsample feature maps. The max-pooling layer has 2 × 4 kernel size, 128 feature maps, stride of 2 × 4, and valid padding. To reduce the parameters effectively, we add a linear layer with 512 output units before fitting all feature maps into the dilated LSTMs. Each LSTM cell has 512 units and then we can obtain 512-dimensional sequential high-level representations. We also adopt a BN layer after the linear layer to improve the performance of the training process. The deep DCRNN architecture is shown in Fig.  5 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "2) Attention-Based Layer:",
      "text": "After extracting the high-level representations, an attentionbased layer is added to exploit the utterance-level features for the SER because all sequential high-level representations do not contribute equally to represent the emotions from speech. In this study, the attention layer for the bidirectional LSTM is defined as follows:\n\nwhere a τ is the attention output, h τ = -→ h τ ; ←h τ denotes the hidden state of the bidirectional LSTM output at time step τ , T is the total time-stamps, and the α τ is the normalized attention weight computed as below:\n\nwhere (•) denotes the element-wise product and the Z is the trainable weights. Finally, we add the FCN layer with 64 output units that are used to compute the center loss and help the softmax loss to easier map the utterance-level features into E different emotional classes as E spaces. Only one dropout is applied after the FCN layer. Especially, this work is quite different from the previous work in  [40]  because the center loss is computed with 64 units instead of from the FCN with E units corresponding E classes. The center loss and the softmax loss are defined to compute the loss function are presented in detail in the Subsubsection III-C3.\n\n3) Loss Function:\n\nIn this study, for the classification task, we combine the softmax loss and center loss as loss function to classify the  emotion from speech and update weights during the training cycle. Because we want to both separate the features and discriminate them to recognize the emotions from speech. Therefore, we try to maximize the distance between the classes by the softmax loss and minimize the distance within-class by the center loss to optimize the training process. The softmax loss or softmax cross-entropy loss is used to classify the features and it is widely applied in multiple classification problems. It is defined as below:\n\nwhere the L SM is the softmax loss and the bs is the batch size or the number of samples in mini-batch. The center loss that computes the distance between the features and their corresponding class centroids is defined as follows:\n\nwhere the L CT is the center loss and the C yn is the centroid of class that the n-th sample belongs to.\n\nThe loss function is defined by combing the softmax loss and center loss as in Eq. 8.\n\nwhere the L T is the total loss function and the ∈ (0, 1) is the factor to balance between the center and softmax losses.\n\nIf the = 0, the loss function becomes the softmax loss.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experimental Results And Comparison",
      "text": "A. Dataset 1) EmoDB dataset: In this study, the Berlin Database of Emotional Speech (EmoDB)  [61]  is used to implement the HDA methods and recognize the emotion from speech. The EmoDB consists of 535 audio data samples recorded by speaking the sentences in different emotions like happiness, sadness, anger, neutral, fear, disgust, and boredom. The speakers include 5 males and 5 females in the ages in a range of  [25, 32] . The original database is recorded in 44.1 kHz and then resampled to 16 kHz. The EmoDB is visualized in detail in Fig.  6 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Experimental Setup",
      "text": "The proposed model is trained on a single NVIDIA Geforce GTX 1050Ti 4 GB VRAM and 16 GB RAM with the TensorFlow framework. Our deep ADCRNN model is trained using Adam optimizer with learning rate 1e -4 , batch size of 16, and the probability of every unit keep in dropout layer of 0.5. To get the robustness of training results, we also use 5-fold cross-validation to train the model.\n\nThe loss functions that are applied to train the classifiers in this study are surveyed as follows:\n\n• L f 1 : Our reconfigured softmax loss and center loss; • L f 2 : Only the softmax loss;\n\n• L f 3 : The softmax loss and center loss in  [40] ;\n\n• L f 4 : The softmax loss and CT-C loss in  [57] . The HDA methods are applied to generate data for the bored, disgust, fear, happy, neutral, and sad emotions. We do experiments using L f 1 with 5 cases as follows:\n\n• Without using the HDA methods;\n\n• Using the time shifting;\n\n• Using the pitch shifting; • Using the WaveGAN;\n\n• Using the SpecGAN. The Griffin-Lim algorithm has been developed in the Librosa library is used to reconstruct the audio samples. For 3D log MelSpec extraction, we use the framework  [62]  to extract the static, deltas, and delta-deltas coefficients of the log MelSpec.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Results",
      "text": "1) Experiment without using the HDA Methods: Fig.  7  shows the waveform and the corresponding log MelSpec feature of the origin happy emotion on the EmoDB dataset.\n\nThe confusion matrix of the experiment using the original dataset is shown in Fig.  8  that presents the predicted and ground truth emotions. The An, Bo, Di, F e, Ha, Sa and N e represent the angry, bored, disgust, fear, happy, sad, and neutral emotions, respectively. The accuracy of the bored, happy, disgust and sad emotions of our proposed model are better accurate than the ADRNN at 100.00%, 78.57%, 87.50%,  2) Experiment using the Time Shifting: Fig.  9  shows the waveform and the corresponding log MelSpec feature of the time shifting happy emotion on the EmoDB dataset. Fig.  10  describes the predicted and ground truth emotions of the experiment using time shifting. In the case of comparing with the ADRNN, our proposed model with time shifting augmentation achieve better accuracy in the disgust, happy, and sad emotions at 100.00%, 81.82%, and 100.00%, respectively. The other emotions are a little less accurate than the ADRNN because of fewer data. On the other hand, the proposed model on the time shifting dataset is better than the original one in  3) Experiment using the Pitch Shifting: Fig.  11  shows the waveform and the corresponding log MelSpec feature of the pitch shifting happy emotion on the EmoDB dataset.\n\nThe confusion matrix of the predicted and ground truth emotion of the experiment using the pitch shifting on the EmoDB is shown in Fig.  12 . Using the pitch shifting, our proposed model achieves higher performance than the ADRNN of 1.83%, 1.11%, 5.63%, 24.18%, and 0.57% notable improvement in the angry, bored, disgust, happy, and sad emotions. In the case of comparing with the original dataset, the model using the pitch shifting is better accurate in the angry, disgust, happy, and neutral emotions at 94.74%, 88.24%, 83.33%, and 90.48%, respectively.  The confusion matrix of the emotion recognition rate for the experiment using WaveGAN on the EmoDB dataset is shown in Fig.  14 . In the angry, disgust, happy, and sad emotions, our proposed model could gain more improved accuracy than the ADRNN at 100.00%, 95.65%, 96.30%, and 100.00%, respectively. Especially, the accuracy in the happy emotion of the proposed model achieves higher than the ADRNN of 37.15% notable improvement. Compare with the original dataset, the model using the WaveGAN augmentation is better accurate in the angry, disgust, fear, happy, and neutral emotions at 100%, 95.65%, 83.33%, 96.30%, and 91.89%, respectively. The accuracy in the sad emotion is equal while another is less than the experiment using the original dataset. Fig.  16  shows the predicted and ground truth emotions of the experiment using SpecGAN on the EmoDB dataset in the confusion matrix. In the disgust, fear, happiness, and sad emotions, the proposed model using the SpecGAN achieves better accuracy at 90.91%, 100.00%, 70.00%, and 95.83% while the others are a little less than the ADRNN. In terms of comparing with the original data, the model using the SpecGAN gains more accuracy in the disgust, fear, and neutral emotions at 90.91%, 100.00%, and 90.24% while the other emotions are less accurate. Especially, the accuracy in the fear emotion is better than the experiment using the original dataset of 20.00% notable improvement. Fig.  16:  The experiment using the SpecGAN on the EmoDB.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Comparison",
      "text": "Table  IV  shows the comparison of different loss functions with their corresponding mean and standard deviation of accuracy using the augmented dataset by the WaveGAN. The model that combines the deep DCRNN architecture (Block 3) with attention (Block 4) and L f 4 achieves the highest accuracy at 91.90%. It proves that using the attention layer can gain higher accuracy than without using it while combining the softmax loss with the CT-C loss can achieve the highest accuracy. Besides, it also proves that the reconfigured loss function L f 1 outperforms the loss function L f 3 with 1% notable improvement. Therefore, using 64 units to compute the center loss is more optimal than E ones, where E is the number of emotional states. The models using the HDA methods are better accurate than the original dataset. Table  V  shows the comparison of the experimental results with their corresponding mean and standard deviation of accuracy using the reconfigured loss function L f 1 . In which, the experiment using the WaveGAN achieves the highest accuracy at 88.47%. With the GAN-based approach, the model using the WaveGAN is better than the SpecGAN while the model using pitch shifting is better than the time shifting in the traditional approach. The comparison of the proposed method with the previous works is shown in Table  VI . Our deep learning framework with the loss functions L f 1 and L f 4 outperform the ACRNN in  [24]  and the ADRNN in  [40]  at 88.60% and 91.90%, respectively. It proves that using the HDA methods with the proposed deep learning framework not only deals with the imbalanced and lack of data but also improves the recognition rate of emotional states. Simultaneously, it also proves that using the deep ADCRNN and the reconfigured loss function is better accuracy than the ADRNN with 3.21% notable improvement.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this article, the HDA methods that combine both traditional and GAN-based approaches are proposed to generate and balance data for the SER. Besides, the deep ADCRNN is implemented to learn and extract the utterance-level features from 3D log MelSpec low-level ones. Furthermore, the loss function combining the softmax and center losses is investigated to improve the accuracy of emotion recognition. Experimental results prove that the HDA methods for speech emotion recognition can achieve better accuracy than the state-of-the-art methods in case of dealing with limited and imbalanced data.\n\nAlthough the proposed hybrid augmentation methods and deep neural networks for the SER in this article achieves better accuracy and performance in terms of imbalance and lack of data, there are still a lot of aspects that can be improved and dived into research. In the future, we will investigate the multi-features fusion and multi-modal to exploit the robust and optimal features for the SER. Besides, we will also employ keyword spotting for the SER to integrate into the real-time systems.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: consisting of six main blocks: Hybrid",
      "page": 3
    },
    {
      "caption": "Figure 1: is designed as follows:",
      "page": 3
    },
    {
      "caption": "Figure 2: describes the",
      "page": 3
    },
    {
      "caption": "Figure 3: Given the number of half-steps nhs and the number of",
      "page": 3
    },
    {
      "caption": "Figure 1: The whole network architecture.",
      "page": 4
    },
    {
      "caption": "Figure 2: Examples of the time shifting and rolling.",
      "page": 4
    },
    {
      "caption": "Figure 3: The process of the pitch shifting.",
      "page": 4
    },
    {
      "caption": "Figure 4: describes the examples of",
      "page": 4
    },
    {
      "caption": "Figure 4: Examples of the pitch shifting.",
      "page": 4
    },
    {
      "caption": "Figure 5: Fig. 5: The deep DCRNN architecture.",
      "page": 5
    },
    {
      "caption": "Figure 6: The detailed distribution of the EmoDB dataset.",
      "page": 7
    },
    {
      "caption": "Figure 7: shows the waveform and the corresponding log",
      "page": 7
    },
    {
      "caption": "Figure 8: that presents the predicted and",
      "page": 7
    },
    {
      "caption": "Figure 7: Visualization of the waveform and the corresponding",
      "page": 7
    },
    {
      "caption": "Figure 8: The experiment without using the HDA methods on",
      "page": 7
    },
    {
      "caption": "Figure 9: shows the waveform and the corresponding log",
      "page": 7
    },
    {
      "caption": "Figure 10: describes the predicted and ground truth emotions of",
      "page": 7
    },
    {
      "caption": "Figure 9: Visualization of the waveform and the corresponding",
      "page": 8
    },
    {
      "caption": "Figure 10: The experiment using the time shifting on the EmoDB.",
      "page": 8
    },
    {
      "caption": "Figure 11: shows the waveform and the corresponding log",
      "page": 8
    },
    {
      "caption": "Figure 12: Using the pitch shifting, our pro-",
      "page": 8
    },
    {
      "caption": "Figure 11: Visualization of the waveform and the corresponding",
      "page": 8
    },
    {
      "caption": "Figure 12: The experiment using the pitch shifting on the",
      "page": 8
    },
    {
      "caption": "Figure 13: shows the waveform and the corresponding log",
      "page": 8
    },
    {
      "caption": "Figure 14: In the angry, disgust, happy, and sad",
      "page": 8
    },
    {
      "caption": "Figure 13: Visualization of the waveform and the corresponding",
      "page": 9
    },
    {
      "caption": "Figure 14: The experiment using the WaveGAN on the EmoDB.",
      "page": 9
    },
    {
      "caption": "Figure 15: shows the waveform and the corresponding log",
      "page": 9
    },
    {
      "caption": "Figure 16: shows the predicted and ground truth emotions of",
      "page": 9
    },
    {
      "caption": "Figure 15: Visualization of the waveform and the corresponding",
      "page": 9
    },
    {
      "caption": "Figure 16: The experiment using the SpecGAN on the EmoDB.",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Generator": "Operation",
          "Discriminator": "Operation"
        },
        {
          "Generator": "Input z ∼Uniform(-1,1)",
          "Discriminator": "Input x or G(z)"
        },
        {
          "Generator": "Dense",
          "Discriminator": "Conv1D 1 (S=4)"
        },
        {
          "Generator": "Reshape",
          "Discriminator": "LeakyReLU 1 (a=0.2)"
        },
        {
          "Generator": "ReLU 1",
          "Discriminator": "Phase Shufﬂe 1 (Bs=2)"
        },
        {
          "Generator": "Transpose Conv1D 1 (S=4)",
          "Discriminator": "Conv1D 2 (S=4)"
        },
        {
          "Generator": "ReLU 2",
          "Discriminator": "LeakyReLU 2 (a=0.2)"
        },
        {
          "Generator": "Transpose Conv1D 2 (S=4)",
          "Discriminator": "Phase Shufﬂe 2 (Bs=2)"
        },
        {
          "Generator": "ReLU 3",
          "Discriminator": "Conv1D 3 (S=4)"
        },
        {
          "Generator": "Transpose Conv1D 3 (S=4)",
          "Discriminator": "LeakyReLU 3 (a=0.2)"
        },
        {
          "Generator": "ReLU 4",
          "Discriminator": "Phase Shufﬂe 3 (Bs=2)"
        },
        {
          "Generator": "Transpose Conv1D 4 (S=4)",
          "Discriminator": "Conv1D 4 (S=4)"
        },
        {
          "Generator": "ReLU 5",
          "Discriminator": "LeakyReLU 4 (a=0.2)"
        },
        {
          "Generator": "Transpose Conv1D 5 (S=4)",
          "Discriminator": "Phase Shufﬂe 4 (Bs=2)"
        },
        {
          "Generator": "Tanh",
          "Discriminator": "Conv1D 5 (S=4)"
        },
        {
          "Generator": "",
          "Discriminator": "LeakyReLU 5 (a=0.2)"
        },
        {
          "Generator": "",
          "Discriminator": "Reshape"
        },
        {
          "Generator": "",
          "Discriminator": "Dense"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Generator": "Operation",
          "Discriminator": "Operation"
        },
        {
          "Generator": "Input z ∼Uniform(-1,1)",
          "Discriminator": "Input x or G(z)"
        },
        {
          "Generator": "Dense",
          "Discriminator": "Conv2D 1 (S=2)"
        },
        {
          "Generator": "Reshape",
          "Discriminator": "LeakyReLU 1 (a=0.2)"
        },
        {
          "Generator": "ReLU 1",
          "Discriminator": "Conv2D 2 (S=2)"
        },
        {
          "Generator": "Transpose Conv2D 1 (S=2)",
          "Discriminator": "LeakyReLU 2 (a=0.2)"
        },
        {
          "Generator": "ReLU 2",
          "Discriminator": "Conv2D 3 (S=2)"
        },
        {
          "Generator": "Transpose Conv2D 2 (S=2)",
          "Discriminator": "LeakyReLU 3 (a=0.2)"
        },
        {
          "Generator": "ReLU 3",
          "Discriminator": "Conv2D 4 (S=2)"
        },
        {
          "Generator": "Transpose Conv2D 3 (S=2)",
          "Discriminator": "LeakyReLU 4 (a=0.2)"
        },
        {
          "Generator": "ReLU 4",
          "Discriminator": "Conv2D 5 (S=2)"
        },
        {
          "Generator": "Transpose Conv2D 4 (S=2)",
          "Discriminator": "LeakyReLU 5 (a=0.2)"
        },
        {
          "Generator": "ReLU 5",
          "Discriminator": "Reshape"
        },
        {
          "Generator": "Transpose Conv2D 5 (S=2)",
          "Discriminator": "Dense"
        },
        {
          "Generator": "Tanh",
          "Discriminator": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "1\n2\n3\n4\n5",
          "Method": "Block 2 + Block 3 + Lf 1\nBlock 2 + Block 3 + Block 4 + LF 2\nBlock 2 + Block 3 + Block 4 + Lf 1\nBlock 2 + Block 3 + Block 4 + Lf 3\nBlock 2 + Block 3 + Block 4 + Lf 4",
          "Accuracy (%)": "67.74 ± 5.89\n84.87 ± 2.84\n88.60 ± 2.98\n87.57 ± 2.52\n91.90 ± 0.86"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Case": "1\n2\n3\n4\n5",
          "Method": "Without using the HDA methods\nUsing the time shifting\nUsing the pitch shifting\nUsing the WaveGAN\nUsing the SpecGAN",
          "Accuracy (%)": "85.66 ± 1.85\n86.84 ± 2.15\n87.12 ± 0.84\n88.47 ± 2.76\n87.32 ± 1.23"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A study of speech emotion recognition and its application to mobile services",
      "authors": [
        "W.-J Yoon",
        "Y.-H Cho",
        "K.-S Park"
      ],
      "year": "2007",
      "venue": "International Conference on Ubiquitous Intelligence and Computing"
    },
    {
      "citation_id": "2",
      "title": "Application of speech emotion recognition in intelligent household robot",
      "authors": [
        "X Huahu",
        "G Jue",
        "Y Jian"
      ],
      "year": "2010",
      "venue": "2010 International Conference on Artificial Intelligence and Computational Intelligence"
    },
    {
      "citation_id": "3",
      "title": "A real-time speech emotion recognition system and its application in online learning",
      "authors": [
        "L Cen",
        "F Wu",
        "Z Yu",
        "F Hu"
      ],
      "year": "2016",
      "venue": "Emotions, technology, design, and learning"
    },
    {
      "citation_id": "4",
      "title": "Speaker-sensitive emotion recognition via ranking: Studies on acted and spontaneous speech",
      "authors": [
        "H Cao",
        "R Verma",
        "A Nenkova"
      ],
      "year": "2015",
      "venue": "Computer speech & language"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition: Features and classification models",
      "authors": [
        "L Chen",
        "X Mao",
        "Y Xue",
        "L Cheng"
      ],
      "year": "2012",
      "venue": "Digital signal processing"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information and semantic labels",
      "authors": [
        "C.-H Wu",
        "W.-B Liang"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition using a hierarchical binary decision tree approach",
      "authors": [
        "C.-C Lee",
        "E Mower",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition and affective computing on vocal social media",
      "authors": [
        "W Dai",
        "D Han",
        "Y Dai",
        "D Xu"
      ],
      "year": "2015",
      "venue": "Information & Management"
    },
    {
      "citation_id": "9",
      "title": "Shape-based modeling of the fundamental frequency contour for emotion detection in speech",
      "authors": [
        "J Arias",
        "C Busso",
        "N Yoma"
      ],
      "year": "2014",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "10",
      "title": "Primitives-based evaluation and estimation of emotions in speech",
      "authors": [
        "M Grimm",
        "K Kroschel",
        "E Mower",
        "S Narayanan"
      ],
      "year": "2007",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition using gaussian mixture vector autoregressive models",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "13",
      "title": "Spoken emotion recognition using hierarchical classifiers",
      "authors": [
        "E Albornoz",
        "D Milone",
        "H Rufiner"
      ],
      "year": "2011",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "14",
      "title": "Segment-based emotion recognition from continuous mandarin chinese speech",
      "authors": [
        "J.-H Yeh",
        "T.-L Pao",
        "C.-Y Lin",
        "Y.-W Tsai",
        "Y.-T Chen"
      ],
      "year": "2011",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "15",
      "title": "Hybrid deep neural networkhidden markov model (dnn-hmm) based speech emotion recognition",
      "authors": [
        "L Li",
        "Y Zhao",
        "D Jiang",
        "Y Zhang",
        "F Wang",
        "I Gonzalez",
        "E Valentin",
        "H Sahli"
      ],
      "year": "2013",
      "venue": "2013 Humaine association conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "16",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "T Falk",
        "W.-Y Chan"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Fifteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "18",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Feature fusion methods research based on deep belief networks for speech emotion recognition under noise condition",
      "authors": [
        "Y Huang",
        "K Tian",
        "A Wu",
        "G Zhang"
      ],
      "year": "2019",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "20",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "21",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "23",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "24",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "25",
      "title": "Mutual correlation attentive factors in dyadic fusion networks for speech emotion recognition",
      "authors": [
        "Y Gu",
        "X Lyu",
        "W Sun",
        "W Li",
        "S Chen",
        "X Li",
        "I Marsic"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition using 3d convolutions and attention-based sliding recurrent networks with auditory front-ends",
      "authors": [
        "Z Peng",
        "X Li",
        "Z Zhu",
        "M Unoki",
        "J Dang",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "27",
      "title": "Dataset augmentation in feature space",
      "authors": [
        "T Devries",
        "G Taylor"
      ],
      "year": "2017",
      "venue": "Dataset augmentation in feature space",
      "arxiv": "arXiv:1702.05538"
    },
    {
      "citation_id": "28",
      "title": "Multi-conditioning and data augmentation using generative noise model for speech emotion recognition in noisy conditions",
      "authors": [
        "U Tiwari",
        "M Soni",
        "R Chakraborty",
        "A Panda",
        "S Kopparapu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Investigation of multilingual and mixed-lingual emotion recognition using enhanced cues with data augmentation",
      "authors": [
        "S Lalitha",
        "D Gupta",
        "M Zakariah",
        "Y Alotaibi"
      ],
      "year": "2020",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "30",
      "title": "Data augmentation using healthy speech for dysarthric speech recognition",
      "authors": [
        "B Vachhani",
        "C Bhat",
        "S Kopparapu"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "31",
      "title": "Data augmentation using generative adversarial networks for robust speech recognition",
      "authors": [
        "Y Qian",
        "H Hu",
        "T Tan"
      ],
      "year": "2019",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "32",
      "title": "Feature extraction algorithms to improve the speech emotion recognition rate",
      "authors": [
        "A Koduru",
        "H Valiveti",
        "A Budati"
      ],
      "year": "2020",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "33",
      "title": "Egyptian arabic speech emotion recognition using prosodic, spectral and wavelet features",
      "authors": [
        "L Abdel-Hamid"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "34",
      "title": "Comparison of feature selection methods in voice based emotion recognition systems",
      "authors": [
        "T Atalay",
        "D Ayata",
        "Y Yaslan"
      ],
      "year": "2018",
      "venue": "2018 26th Signal Processing and Communications Applications Conference (SIU)"
    },
    {
      "citation_id": "35",
      "title": "Two-layer fuzzy multiple random forest for speech emotion recognition in human-robot interaction",
      "authors": [
        "L Chen",
        "W Su",
        "Y Feng",
        "M Wu",
        "J She",
        "K Hirota"
      ],
      "year": "2020",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "36",
      "title": "Extraction of adaptive wavelet packet filter-bank-based acoustic feature for speech emotion recognition",
      "authors": [
        "Y Huang",
        "A Wu",
        "G Zhang",
        "Y Li"
      ],
      "year": "2015",
      "venue": "IET Signal Processing"
    },
    {
      "citation_id": "37",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "Clustering-based speech emotion recognition by incorporating learned features and deep bilstm",
      "authors": [
        "M Sajjad",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "39",
      "title": "Speech emotion recognition using fusion of three multi-task learning-based classifiers: Hsf-dnn, ms-cnn and lld-rnn",
      "authors": [
        "Z Yao",
        "Z Wang",
        "W Liu",
        "Y Liu",
        "J Pan"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "40",
      "title": "Speech emotion recognition from 3d log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "41",
      "title": "Attentionbased dense lstm for speech emotion recognition",
      "authors": [
        "Y Xie",
        "R Liang",
        "Z Liang",
        "L Zhao"
      ],
      "year": "2019",
      "venue": "IEICE TRANSACTIONS on Information and Systems"
    },
    {
      "citation_id": "42",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "S Yoon",
        "S Byun",
        "S Dey",
        "K Jung"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "43",
      "title": "Selfattention for speech emotion recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "INTER-SPEECH"
    },
    {
      "citation_id": "44",
      "title": "Exploring deep spectrum representations via attention-based recurrent and convolutional neural networks for speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Z Bao",
        "Y Zhao",
        "Z Zhang",
        "N Cummins",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "45",
      "title": "Multimodal approach of speech emotion recognition using multi-level multi-head fusion attention-based recurrent neural network",
      "authors": [
        "N.-H Ho",
        "H.-J Yang",
        "S.-H Kim",
        "G Lee"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "46",
      "title": "Multimodal continuous emotion recognition with data augmentation using recurrent neural networks",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian",
        "M Niu",
        "M Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "47",
      "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C.-C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "48",
      "title": "Improving speech recognition using data augmentation and acoustic model fusion",
      "authors": [
        "I Rebai",
        "Y Benayed",
        "W Mahdi",
        "J.-P Lorré"
      ],
      "year": "2017",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "49",
      "title": "On enhancing speech emotion recognition using generative adversarial networks",
      "authors": [
        "S Sahu",
        "R Gupta",
        "C Espy-Wilson"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "50",
      "title": "Improving speech emotion recognition with adversarial data augmentation network",
      "authors": [
        "L Yi",
        "M Mak"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "51",
      "title": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition",
      "authors": [
        "F Bao",
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition"
    },
    {
      "citation_id": "52",
      "title": "Data augmentation using gans for speech emotion recognition",
      "authors": [
        "A Chatziagapi",
        "G Paraskevopoulos",
        "D Sgouropoulos",
        "G Pantazopoulos",
        "M Nikandrou",
        "T Giannakopoulos",
        "A Katsamanis",
        "A Potamianos",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Data augmentation using gans for speech emotion recognition"
    },
    {
      "citation_id": "53",
      "title": "Gan-based data generation for speech emotion recognition",
      "authors": [
        "S Eskimez",
        "D Dimitriadis",
        "R Gmyr",
        "K Kumanati"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "54",
      "title": "Adversarial audio synthesis",
      "authors": [
        "C Donahue",
        "J Mcauley",
        "M Puckette"
      ],
      "year": "2019",
      "venue": "7th International Conference on Learning Representations, ICLR 2019"
    },
    {
      "citation_id": "55",
      "title": "An efficient method for pitch shifting digitally sampled sounds",
      "authors": [
        "K Lent"
      ],
      "year": "1989",
      "venue": "Computer Music Journal"
    },
    {
      "citation_id": "56",
      "title": "Real-time pitchshifting of musical signals by a time-varying factor using normalized filtered correlation time-scale modification (NFC-TSM)",
      "authors": [
        "A Haghparast",
        "H Penttinen",
        "V Välimäki"
      ],
      "year": "2007",
      "venue": "Proceedings of the International Conference on Digital Audio Effects (DAFx)"
    },
    {
      "citation_id": "57",
      "title": "A method upon deep learning for speech emotion recognition",
      "authors": [
        "N Pham",
        "D Dang",
        "S Nguyen"
      ],
      "year": "2020",
      "venue": "Journal of Advanced Engineering and Computation"
    },
    {
      "citation_id": "58",
      "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "authors": [
        "A Radford",
        "L Metz",
        "S Chintala"
      ],
      "year": "2016",
      "venue": "4th International Conference on Learning Representations, ICLR 2016"
    },
    {
      "citation_id": "59",
      "title": "Improved training of wasserstein gans",
      "authors": [
        "I Gulrajani",
        "F Ahmed",
        "M Arjovsky",
        "V Dumoulin",
        "A Courville"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg"
    },
    {
      "citation_id": "60",
      "title": "Signal estimation from modified short-time fourier transform",
      "authors": [
        "D Griffin",
        "J Lim"
      ],
      "year": "1984",
      "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "61",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A database of German emotional speech"
    },
    {
      "citation_id": "62",
      "title": "jameslyons/python speech features: release v0.6.1",
      "authors": [
        "J Lyons",
        ".-B Wang",
        "H Gianluca",
        "E Shteingart",
        "Y Mavrinac",
        "W Gaurkar",
        "S Watcharawisetkul",
        "L Birch",
        "J Zhihe",
        "J Hölzl",
        "H Lesinskis",
        "C Almér",
        "A Lord",
        "Stark"
      ],
      "year": "2020",
      "venue": "jameslyons/python speech features: release v0.6.1"
    }
  ]
}