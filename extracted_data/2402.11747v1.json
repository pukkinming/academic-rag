{
  "paper_id": "2402.11747v1",
  "title": "Parameter Efficient Finetuning For Speech Emotion Recognition And Domain Adaptation",
  "published": "2024-02-19T00:21:07Z",
  "authors": [
    "Nineli Lashkarashvili",
    "Wen Wu",
    "Guangzhi Sun",
    "Philip C. Woodland"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Parameter Efficient Finetuning",
    "Domain Adaptation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Foundation models have shown superior performance for speech emotion recognition (SER). However, given the limited data in emotion corpora, finetuning all parameters of large pre-trained models for SER can be both resource-intensive and susceptible to overfitting. This paper investigates parameter-efficient finetuning (PEFT) for SER. Various PEFT adaptors are systematically studied for both classification of discrete emotion categories and prediction of dimensional emotional attributes. The results demonstrate that the combination of PEFT methods surpasses full finetuning with a significant reduction in the number of trainable parameters. Furthermore, a two-stage adaptation strategy is proposed to adapt models trained on acted emotion data, which is more readily available, to make the model more adept at capturing natural emotional expressions. Both intra-and cross-corpus experiments validate the efficacy of the proposed approach in enhancing the performance on both the source and target domains.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion is a complex process influencing human interaction and cognitive processes  [1, 2] . Speech emotion recognition (SER) enables systems to perceive and respond to emotional cues in spoken language and plays a crucial role in human-computer interaction. SER has garnered growing interest in recent years  [3] [4] [5] , yet the data scarcity remains a critical challenge for its advancement. Two key concerns regarding emotion datasets are their limited corpus size and lack of naturalness. Most emotional databases only contain a few hours of recordings  [6] [7] [8] [9] [10]  with many acquiring emotional speech by participants reading sentences in designated emotional states  [6, 7, 10, 11] . These acted portrayals tend to reflect more standardised behaviours rather than the nuanced and ambiguous emotional expressions encountered in everyday interactions.\n\nThe development of foundation models has paved the way to a new paradigm, enabling the customisation of general-purpose models for specific downstream tasks. Foundation models are large machine learning models trained on a very large quantity of data usually by self-supervised learning. Transferring knowledge from pretrained foundation models can help alleviate the data sparsity issue of downstream tasks  [12] [13] [14] . However, finetuning the whole foundation model is resource-consuming and can lead to overfitting and Nineli Lashkarashvili is funded by DeepMind and Cambridge Trust. This work has been performed using resources provided by the Cambridge Tier-2 system operated by the University of Cambridge Research Computing Service (www.hpc.cam.ac.uk) funded by EPSRC Tier-2 capital grant EP/T022159/1. catastrophic forgetting. Parameter-efficient fine-tuning (PEFT) algorithms have been proposed to help mitigate these issues. PEFT approaches only fine-tune a small number of (extra/internal) model parameters while keeping most pre-trained foundation model parameters frozen. PEFT has been shown to be successful for natural language processing tasks  [15] [16] [17]  and also some speech tasks  [18, 19] . However, the efficacy of PEFT methods for SER, including their effectiveness across discrete emotion classes and dimensional emotion attributes hasn't been extensively explored.\n\nThis paper provides a comprehensive exploration of PEFT methods for SER. Emotion states can be defined either by discrete emotion classes (i.e., happy, sad, angry, neutral)  [20]  or dimensional attributes (i.e., valence, arousal, dominance)  [21] . Various PEFT methods, along with combinations thereof, are first employed to investigate both emotion classification and emotion attribute prediction, which aims to ascertain whether the methods exhibit consistent patterns of performance across both tasks.\n\nFurthermore, this paper investigates emotion domain adaptation via PEFT. SER datasets containing acted emotion data are more prevalent since acted emotion data are relatively easier to collect than natural emotion data. Therefore, the paper examines whether a model trained on acted emotion data can generalise to natural emotion. A two-stage adaption framework is developed for the adaptation of models trained on acted emotion data to the domain of natural emotions using the PEFT approach. Both intra-and cross-corpus adaptation are studied and results suggest that prediction of emotion in natural settings benefits from such adaptation.\n\nThe rest of the paper is organised as follows. Section 2 introduces PEFT adaptors and the proposed approach. The experimental setup and results are presented in Section 3 and Section 4 respectively, followed by conclusions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Structure",
      "text": "The backbone structure follows an upstream-downstream framework  [22] . Both wav2vec 2.0  [23]   1  and HuBERT  [24]   2  are used as upstream architectures. Both models consist of a CNN feature extractor and Transformer encoder parts, with wav2vec 2.0 containing 12 transformer blocks with a hidden dimension of 768 and 8 attention heads, and HuBERT comprising 24 transformer blocks with a hidden dimension of 1024 and 16 attention heads. The downstream model is constructed by two fully connected layers following the SUPERB benchmark setup  [25] . For emotion attribute prediction valence, arousal and dominance are predicted together.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Peft Adaptors",
      "text": "We first investigate the use of different PEFT adaptors and their combinations for SER. In the upstream model, PEFT modules are inserted in the Transformer encoders as shown in Fig.  1 . The following adaptors are examined. The Bottleneck Adaptor (BA)  [16]  is applied after the feedforward layer of the Transformer encoder  [25] . It consists of a down-scaling layer which transforms a d-dimensional feature into m-dimensional (m ≪ d) (FF Down), a non-linear operation and an up-scaling layer which projects the m-dimensional feature back to d-dimensional (FF Up). A residual connection is added around the BA block.\n\nLow-Rank Adaptation (LoRA)  [15]  adds trainable low-rank decomposition matrices to the weight matrices in the multi-head attention.\n\nwhere Wq ∈ R d×k is the original weight matrix for the query which is frozen during fine-tuning, W B q ∈ R d×r and W A q ∈ R r×k , r ≪ min(d, k) are the decomposition matrices which are updated during fine-tuning. Similar decompositions apply to weight matrices of the keys and values.\n\nThe Weighted Sum (WS)  [25]  adaptor assigns learnable weights to the outputs of each Transformer block and returns the weighted sum for downstream modules. The scalar weights are passed to a softmax function to weight the importance of hidden states, which can also be treated as a PEFT method  [18] .\n\nIn addition, this work proposes the Weight-Gating (WG) adaptor which assigns a learnable gating vector g ∈ R d to each hidden state of dimension d which is passed to a sigmoid activation function.\n\nwhere h denotes the hidden states, σ(•) stands for sigmoid fuction, and ⊙ is element-wise product. denotes that PEFT module is updated while that it is frozen. The figure depicts the scenario when BA and LoRA weights from stage 1 are not updated.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Domain Adaptation From Acted Emotion To Natural Emotion",
      "text": "Acted (simulated) emotion data is relatively easier to collect compared to natural emotion and acted speech databases are thus more prevalent than natural emotion databases. Utterances in acted speech databases usually contain professional or semi-professional actor recordings who are instructed to express designated emotions. Sometimes, the participant reads the same scripted sentences in different emotions  [7, [9] [10] [11]  which may not convey real-life emotions adequately, and can be exaggerated. Models trained on acted emotion data may not generalise well to real-world applications. Therefore, we investigate adapting models trained on acted emotion data to natural emotion data using PEFT. As shown in Fig.  2 , the pipeline contains two stages. In the first stage, the pre-trained upstream model is fine-tuned on acted emotion data using a combination of PEFT adaptors, which aims to leverage generic speech knowledge to SER. In the second stage, the SER model trained on acted emotion data is further fine-tuned on natural emotion data. Part of the PEFT adaptors are also frozen in the second stage and whether the adaptation can improve the performance in the target domain while avoiding catastrophic forgetting in the source domain is of interest. The WG and WS adaptors are always updated during training, reflecting their role in weighting the significance of hidden states, and they should align with the specific task at hand.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "IEMOCAP  [6]  consists of 5 sessions. Each session contains scripted (acted) and improvised dialogues between two speakers. In the improvised part actors were asked to think of any past memory and express themselves for a certain emotion. The IEMOCAP creators believe that such a setup will convey emotions closer to real life. Both categorical labels and dimensional (valence-arousaldominance) are provided. Following prior work  [25] , four emotion classes for categorical emotion prediction were used: happy (merged with excited), sad, angry, and neutral, resulting in 5,531 utterances. For dimensional emotion descriptor prediction, the entire dataset comprising 10,039 data samples were used. A leave-one-sessionout 5-fold cross-validation (5-cv) setup was used and the average scores are reported for classification and regression tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation Details",
      "text": "The system is implemented in PyTorch. Models were trained for 20 epochs using Adam optimiser with an initial learning rate of 5×10 for all experiments except finetuning (5 × 10 -5 ) and a linear learning rate scheduler. The best models were saved based on validation set performance and then used to evaluate on the test set. Following  [25] , discrete emotion classification was evaluated by accuracy (Acc) and emotion attributes prediction by the concordance correlation coefficient (CCC):\n\nwhere ρ is the Pearson correlation coefficient between true label (t) and model prediction (p). σ 2 a , µa and σ 2 p , µp are the variances and means for annotation and predictions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Results",
      "text": "PEFT adaptors were compared to the pre-trained foundation model without any finetuning (PT) and fully fine-tuning (FT) where the CNN feature extractor is frozen and all transformer blocks are updated.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Efficient Finetuning For Ser",
      "text": "Table  1  presents the outcome of applying LoRA, BA, WS, and WG PEFT methods and their combinations to both categorical and dimensional emotion prediction tasks for the wav2vec 2.0 and HuBERT models. Notably, all PEFT methods, except for WG in HuBERT, exhibit superior performance compared to the pre-trained baseline. Augmenting the combination of BA, LoRA, and WS with WG yields an overall performance boost that surpasses fine-tuning (FT) while using 2-3% of the total trainable parameters for both models. In terms of classification tasks, wav2vec 2.0 and HuBERT yield similar performance for different PEFT methods. HuBERT stands out particularly in categorical label prediction, achieving results not far from the state-of-the-art of 74.2%  [26]  without relying on external features, using only raw audio data.\n\nHowever, a noteworthy observation is that the combination of LoRA and BA falls short of the best-performing standalone methods. One plausible explanation could be over-parameterisation, which may impede model generalisation and increase complexity, potentially leading to overfitting the training data. WS and WG seem to serve as corrective measures, potentially compensating for lost discriminative information and effectively modulating crucial features. Consequently, they exhibit the highest accuracies for each model.\n\nThe outcomes for dimensional emotional attribute prediction deviate from those of categorical classification. This difference can be attributed to the inherent dissimilarities between these tasks. Estimating continuous emotional attributes is intrinsically more challenging, and unlike categorical label prediction, the combination of BA and LoRA does not lead to overparameterisation. Instead, this combination proves sufficient to capture the intricate relationships between the acoustic features and continuous emotional dimensions. For wav2vec 2.0 the addition of WS and WG was advantageous for arousal scores. This could be attributed to the contribution of WS and WG which both appear to enhance the performance compared to PT for wav2vec 2.0. Conversely, for HuBERT, this phenomenon doesn't hold and the performance declines when WS and WG are introduced. The decrease in performance could be due to overmodulation, where the added complexity of these PEFT methods result in reduced performance. It's worth noting that LoRA proves effective for dominance prediction, while BA excels in valence and arousal prediction. The results suggest that due to the increased task difficulty, PEFT modules tend to prioritize the enhancement of a specific emotion dimension. Overall, the combination of PEFT methods demonstrated superior performance compared to FT. Furthermore, PEFT for both wav2vec 2.0 and HuBERT models for dimensional emotional attribute prediction surpassed existing state-of-theart CCC scores for valence (0.582), arousal (0.667) and dominance (0.545)  [27]  with significantly fewer trainable parameters.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Adaptation From Acted Emotion To Natural Emotion",
      "text": "This section investigates the generalisability of a model trained on acted emotion data to natural emotion. The combination of BA, LoRA, WS and WG was used for stage 1 adapting since it produces the best performance in Table  1 . Both intra-and cross-corpus adaptation were investigated. Fold 1 (the model trained on sessions 2-5 and tested on session 1) was used for IEMOCAP.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Intra-Corpus Adaptation",
      "text": "IEMOCAP contains scripted dialogues (IEMsc) with acted emotion and improvised (IEMim) which contain more natural emotional expression. We first investigated adapting a model trained on scripted data to improvised emotion.\n\nThe first two rows of Table  2  show the results of directly finetuning pre-trained model on scripted data and improvised data respectively. It can be seen that achieving high accuracy on scripted emotional state prediction is challenging for the wav2vec 2.0 model, while HuBERT exhibits better generalisation and decent performance across different data segments. HuBERT representations are more powerful which was also evident in PEFT SER. The last four rows present the results of stage 2 adaptation. Overall, adapting a model trained on scripted data using improvised data enhances SER performance for natural emotions. There's a consistent improvement in target domain performance for both wav2vec 2.0 and HuBERT models. The accuracy of improvised data surpasses the baseline achieved by direct fine-tuning on improvised data, indicating that the model benefits from leveraging knowledge from acted emotion. Adapting a HuBERT model trained on scripted data to the improvised dataset results in a performance boost of about 6% on the improvised set. Freezing a portion of PEFT modules has a minimal or even beneficial impact on the target task's performance, notably enhancing prediction quality on the source domain. This phenomenon is particularly pronounced in the case of HuBERT, where fine-tuning only WS and WG modules yields comparable performance to fine-tuning all PEFT modules and avoids forgetting of source task.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Cross-Corpus Adaptation",
      "text": "The use of an external ESD corpus was investigated for cross-corpus adaptation. In ESD  [11]  speakers were directed to read each sentence for all designated emotion categories. For ESD, English utterances corresponding to the four emotion classes in IEMOCAP and original train/evaluation/test splits were used. Table  3  presents the results of adapting from the ESD dataset to the IEMOCAP improvised set. Underlined values in the table highlight that a model trained on one corpus faces challenges generalising to the other. However, it's worth noting that HuBERT, which was only trained on the ESD data, exhibits reasonable accuracy on the improvised set. During the stage 2 adaptation process, when selected adaptors were frozen, a trade-off is observed between pre-serving knowledge from the source domain and effectively adapting to the target domain. Both models exhibit the best improvised accuracy when LoRA is frozen and and their performance across different methods is quite similar. Presumably, LoRA captures important features and freezing it allows valuable knowledge to be retained and doesn't hinder generalisation. The results suggest that freezing part of the PEFT modules similar to intra-corpus adaptation is beneficial in preserving source task accuracy. Unlike intra-corpus adaptation, a large decline in performance is observed for the ESD dataset after adaptation, possibly due to the fact that emotions in ESD follow a more paradigmatic pattern.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "This work investigates PEFT for speech emotion recognition. Results show that PEFT can achieve comparable performance to full fine-tuning while requiring far fewer updated parameters. In both categorical and dimensional emotional descriptor prediction tasks, the combination of various PEFT adaptors results in enhanced system performance, demonstrating comparable or even superior outcomes to the state-of-the-art models, all while significantly reducing trainable parameters and without relying on external features beyond raw audio data. Furthermore, a two-stage pipeline is introduced to adapt models trained on acted emotion to better handle natural emotions using PEFT. The results from both intra-and cross-corpus experiments highlight the effectiveness of this adaptation strategy, showcasing performance enhancements in both source and target domains. Freezing BA or LoRA modules was found helpful in avoiding catastrophic forgetting of the source domain. This approach not only facilitates the retention of the source task but also offers the prospect of achieving performance levels equivalent to, if not surpassing, a model trained directly on natural emotion.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A wav2vec 2.0 / HuBERT Transformer layer with various",
      "page": 2
    },
    {
      "caption": "Figure 1: The following",
      "page": 2
    },
    {
      "caption": "Figure 2: Proposed two-stage domain adaptation from acted to natural",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "catastrophic forgetting.\nParameter-efficient fine-tuning (PEFT) al-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "gorithms have been proposed to help mitigate these issues.\nPEFT"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "approaches only fine-tune a small number of (extra/internal) model"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "parameters while keeping most pre-trained foundation model param-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "eters frozen. PEFT has been shown to be successful for natural lan-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "guage processing tasks [15–17] and also some speech tasks [18, 19]."
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "However, the efficacy of PEFT methods for SER, including their ef-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "fectiveness across discrete emotion classes and dimensional emotion"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "attributes hasn’t been extensively explored."
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "This paper provides a comprehensive exploration of PEFT meth-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "ods for SER. Emotion states can be defined either by discrete emo-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "tion classes\n(i.e., happy,\nsad,\nangry, neutral)\n[20] or dimensional"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "attributes\n(i.e., valence, arousal, dominance)\n[21].\nVarious PEFT"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "methods, along with combinations thereof, are first employed to in-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "vestigate both emotion classification and emotion attribute predic-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "tion, which aims to ascertain whether the methods exhibit consistent"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "patterns of performance across both tasks."
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "Furthermore, this paper investigates emotion domain adaptation"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "via PEFT. SER datasets containing acted emotion data are more"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "prevalent\nsince acted emotion data are relatively easier\nto collect"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "than natural emotion data. Therefore, the paper examines whether a"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "model\ntrained on acted emotion data can generalise to natural emo-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "tion. A two-stage adaption framework is developed for the adapta-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "tion of models trained on acted emotion data to the domain of natu-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "ral emotions using the PEFT approach. Both intra- and cross-corpus"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "adaptation are studied and results suggest that prediction of emotion"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "in natural settings benefits from such adaptation."
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "The rest of\nthe paper\nis organised as follows.\nSection 2 intro-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "duces PEFT adaptors and the proposed approach. The experimental"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "setup and results are presented in Section 3 and Section 4 respec-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "tively, followed by conclusions."
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "2. METHODOLOGY"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "2.1. Model structure"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "The backbone\nstructure\nfollows\nan upstream-downstream frame-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "work [22]. Both wav2vec 2.0 [23]1 and HuBERT [24]2 are used as"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "upstream architectures. Both models consist of a CNN feature ex-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "tractor and Transformer encoder parts, with wav2vec 2.0 containing"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "12 transformer blocks with a hidden dimension of 768 and 8 atten-"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "tion heads, and HuBERT comprising 24 transformer blocks with a"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "hidden dimension of 1024 and 16 attention heads. The downstream"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "model\nis constructed by two fully connected layers\nfollowing the"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "SUPERB benchmark setup [25].\nFor emotion attribute prediction"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "valence, arousal and dominance are predicted together."
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": ""
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "1https://huggingface.co/facebook/wav2vec2-base"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "2facebook/hubert-large-ll60k"
        },
        {
          "Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.": "© IEEE 2024"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "Model\nLoRA\nActed\nBA"
        },
        {
          "FeedForward\nLoRA\nFF Down": "Add & Norm",
          "Natural\nBA\nLoRA": "WG\nWS"
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "WG\nWS"
        },
        {
          "FeedForward\nLoRA\nFF Down": "WB",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "LoRA\nAttention",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "Fig. 2. Proposed two-stage domain adaptation from acted to natural"
        },
        {
          "FeedForward\nLoRA\nFF Down": "WA",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "Q\nK\nV",
          "Natural\nBA\nLoRA": "emotion prediction.\ndenotes that PEFT module is updated while"
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "that\nit\nis frozen. The figure depicts the scenario when BA and"
        },
        {
          "FeedForward\nLoRA\nFF Down": "LoRA\nLoRA\nWq\nLoRA Wk\nWv",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "LoRA weights from stage 1 are not updated."
        },
        {
          "FeedForward\nLoRA\nFF Down": "Hidden States",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "2.3. Domain adaptation from acted emotion to natural emotion"
        },
        {
          "FeedForward\nLoRA\nFF Down": "WG",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "weight vector",
          "Natural\nBA\nLoRA": "Acted (simulated) emotion data is relatively easier\nto collect com-"
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "pared to natural emotion and acted speech databases are thus more"
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "prevalent than natural emotion databases. Utterances in acted speech"
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "databases\nusually\ncontain\nprofessional\nor\nsemi-professional\nac-"
        },
        {
          "FeedForward\nLoRA\nFF Down": "Fig. 1. A wav2vec 2.0 / HuBERT Transformer\nlayer with various",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "tor\nrecordings who are instructed to express designated emotions."
        },
        {
          "FeedForward\nLoRA\nFF Down": "PEFT adaptors",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "Sometimes,\nthe participant\nreads\nthe\nsame\nscripted sentences\nin"
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "different emotions [7, 9–11] which may not convey real-life emo-"
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "tions adequately, and can be exaggerated. Models trained on acted"
        },
        {
          "FeedForward\nLoRA\nFF Down": "2.2. PEFT adaptors",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "emotion data may not generalise well\nto real-world applications."
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "Therefore, we investigate adapting models\ntrained on acted emo-"
        },
        {
          "FeedForward\nLoRA\nFF Down": "We first investigate the use of different PEFT adaptors and their com-",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "tion data to natural emotion data using PEFT. As shown in Fig. 2,"
        },
        {
          "FeedForward\nLoRA\nFF Down": "binations for SER.\nIn the upstream model, PEFT modules are in-",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "the pipeline contains two stages.\nIn the first stage,\nthe pre-trained"
        },
        {
          "FeedForward\nLoRA\nFF Down": "serted in the Transformer encoders as shown in Fig. 1. The following",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "upstream model\nis fine-tuned on acted emotion data using a com-"
        },
        {
          "FeedForward\nLoRA\nFF Down": "adaptors are examined.",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "bination of PEFT adaptors, which aims to leverage generic speech"
        },
        {
          "FeedForward\nLoRA\nFF Down": "The Bottleneck Adaptor\n(BA)\n[16]\nis applied after\nthe feed-",
          "Natural\nBA\nLoRA": "knowledge to SER. In the second stage,\nthe SER model\ntrained on"
        },
        {
          "FeedForward\nLoRA\nFF Down": "forward layer of\nthe Transformer encoder\n[25].\nIt\nconsists of a",
          "Natural\nBA\nLoRA": "acted emotion data is\nfurther fine-tuned on natural emotion data."
        },
        {
          "FeedForward\nLoRA\nFF Down": "down-scaling layer which transforms a d-dimensional\nfeature into",
          "Natural\nBA\nLoRA": "Part of\nthe PEFT adaptors are also frozen in the second stage and"
        },
        {
          "FeedForward\nLoRA\nFF Down": "m-dimensional (m ≪ d) (FF Down), a non-linear operation and an",
          "Natural\nBA\nLoRA": "whether\nthe adaptation can improve the performance in the target"
        },
        {
          "FeedForward\nLoRA\nFF Down": "up-scaling layer which projects the m-dimensional feature back to",
          "Natural\nBA\nLoRA": "domain while avoiding catastrophic forgetting in the source domain"
        },
        {
          "FeedForward\nLoRA\nFF Down": "d-dimensional (FF Up). A residual connection is added around the",
          "Natural\nBA\nLoRA": "is of interest. The WG and WS adaptors are always updated during"
        },
        {
          "FeedForward\nLoRA\nFF Down": "BA block.",
          "Natural\nBA\nLoRA": "training, reflecting their role in weighting the significance of hidden"
        },
        {
          "FeedForward\nLoRA\nFF Down": "Low-Rank Adaptation (LoRA) [15] adds trainable low-rank de-",
          "Natural\nBA\nLoRA": "states, and they should align with the specific task at hand."
        },
        {
          "FeedForward\nLoRA\nFF Down": "composition matrices to the weight matrices in the multi-head atten-",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "tion.",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "3. EXPERIMENTAL SETUP"
        },
        {
          "FeedForward\nLoRA\nFF Down": "W ′\n(1)\nq = Wq + ∆W = Wq + W B\nq W A",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "3.1. Datasets"
        },
        {
          "FeedForward\nLoRA\nFF Down": "where Wq ∈ Rd×k is the original weight matrix for the query which",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "is frozen during fine-tuning, W B\n∈ Rd×r and W A\n∈ Rr×k, r ≪",
          "Natural\nBA\nLoRA": "IEMOCAP [6] consists of 5 sessions. Each session contains scripted"
        },
        {
          "FeedForward\nLoRA\nFF Down": "min(d, k) are the decomposition matrices which are updated during",
          "Natural\nBA\nLoRA": "(acted)\nand improvised dialogues between two speakers.\nIn the"
        },
        {
          "FeedForward\nLoRA\nFF Down": "fine-tuning. Similar decompositions apply to weight matrices of the",
          "Natural\nBA\nLoRA": "improvised part\nactors were\nasked to think of\nany past memory"
        },
        {
          "FeedForward\nLoRA\nFF Down": "keys and values.",
          "Natural\nBA\nLoRA": "and express\nthemselves\nfor\na\ncertain emotion.\nThe\nIEMOCAP"
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "creators believe that\nsuch a setup will convey emotions closer\nto"
        },
        {
          "FeedForward\nLoRA\nFF Down": "The Weighted Sum (WS) [25] adaptor assigns learnable weights",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "real\nlife. Both categorical\nlabels and dimensional (valence-arousal-"
        },
        {
          "FeedForward\nLoRA\nFF Down": "to the outputs of each Transformer block and returns the weighted",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "dominance) are provided. Following prior work [25], four emotion"
        },
        {
          "FeedForward\nLoRA\nFF Down": "sum for downstream modules. The scalar weights are passed to a",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "classes for categorical emotion prediction were used: happy (merged"
        },
        {
          "FeedForward\nLoRA\nFF Down": "softmax function to weight\nthe importance of hidden states, which",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "with excited), sad, angry, and neutral, resulting in 5,531 utterances."
        },
        {
          "FeedForward\nLoRA\nFF Down": "can also be treated as a PEFT method [18].",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "For dimensional emotion descriptor prediction,\nthe entire dataset"
        },
        {
          "FeedForward\nLoRA\nFF Down": "In addition, this work proposes the Weight-Gating (WG) adaptor",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "comprising 10,039 data samples were used. A leave-one-session-"
        },
        {
          "FeedForward\nLoRA\nFF Down": "which assigns a learnable gating vector g ∈ Rd to each hidden state",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "out 5-fold cross-validation (5-cv) setup was used and the average"
        },
        {
          "FeedForward\nLoRA\nFF Down": "of dimension d which is passed to a sigmoid activation function.",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "scores are reported for classification and regression tasks."
        },
        {
          "FeedForward\nLoRA\nFF Down": "h′ = σ(g) ⊙ h\n(2)",
          "Natural\nBA\nLoRA": ""
        },
        {
          "FeedForward\nLoRA\nFF Down": "",
          "Natural\nBA\nLoRA": "3.2.\nImplementation details"
        },
        {
          "FeedForward\nLoRA\nFF Down": "where h denotes the hidden states, σ(·) stands for sigmoid fuction,",
          "Natural\nBA\nLoRA": "The system is implemented in PyTorch. Models were trained for 20"
        },
        {
          "FeedForward\nLoRA\nFF Down": "and ⊙ is element-wise product.",
          "Natural\nBA\nLoRA": "epochs using Adam optimiser with an initial learning rate of 5×10−4"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: IEMOCAP 5-cv mean accuracy and CCC scores for categorical and dimensional emotion prediction. ‘✓’ indicates the use of",
      "data": [
        {
          "wav2vec 2.0BASE": "CCCV",
          "HuBERTLARGE": "CCCV"
        },
        {
          "wav2vec 2.0BASE": "0.630",
          "HuBERTLARGE": "0.664"
        },
        {
          "wav2vec 2.0BASE": "0.599",
          "HuBERTLARGE": "0.579"
        },
        {
          "wav2vec 2.0BASE": "0.428",
          "HuBERTLARGE": "0.514"
        },
        {
          "wav2vec 2.0BASE": "0.375",
          "HuBERTLARGE": "0.351"
        },
        {
          "wav2vec 2.0BASE": "0.644",
          "HuBERTLARGE": "0.678"
        },
        {
          "wav2vec 2.0BASE": "0.596",
          "HuBERTLARGE": "0.665"
        },
        {
          "wav2vec 2.0BASE": "0.571",
          "HuBERTLARGE": "0.651"
        },
        {
          "wav2vec 2.0BASE": "0.639",
          "HuBERTLARGE": "0.652"
        },
        {
          "wav2vec 2.0BASE": "0.356",
          "HuBERTLARGE": "0.458"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: IEMOCAP 5-cv mean accuracy and CCC scores for categorical and dimensional emotion prediction. ‘✓’ indicates the use of",
      "data": [
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "FT\n90 M\n66.30\n0.639\n0.702",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "0.505\n311 M\n68.53\n0.652\n0.715\n0.527"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "PT\n0\n51.02\n0.356\n0.635",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "0.495\n0\n60.09\n0.458\n0.554\n0.477"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "Table 1.\nIEMOCAP 5-cv mean accuracy and CCC scores for categorical and dimensional emotion prediction.",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "‘✓’\nindicates the use of"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "the PEFT adaptor during training. # param denotes trainable parameters excluding downstream layers and BA, LoRA, WS, WG represent",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "Bottleneck, Low-Rank Adaptation, Weighted Sum, and Weight-Gating adaptors, respectively.",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "for all experiments except finetuning (5 × 10−5) and a linear learn-",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "The outcomes for dimensional emotional attribute prediction de-"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "ing rate scheduler. The best models were saved based on validation",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "viate from those of categorical classification. This difference can be"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "set performance and then used to evaluate on the test set. Follow-",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "attributed to the inherent dissimilarities between these tasks.\nEs-"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "ing [25], discrete emotion classification was evaluated by accuracy",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "timating continuous emotional attributes is intrinsically more chal-"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "(Acc) and emotion attributes prediction by the concordance correla-",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "lenging, and unlike categorical\nlabel prediction,\nthe combination of"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "tion coefficient (CCC):",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "BA and LoRA does not\nlead to overparameterisation.\nInstead,\nthis"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "combination proves sufficient\nto capture the intricate relationships"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "2ρσaσp",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "CCC =\n(3)",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "between the acoustic features and continuous emotional dimensions."
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "σ2\na + σ2\np + (µa − µp)2",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "For wav2vec 2.0 the addition of WS and WG was advantageous for"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "arousal scores. This could be attributed to the contribution of WS"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "where ρ is the Pearson correlation coefficient between true label (t)",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "and WG which both appear to enhance the performance compared"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "and model prediction (p). σ2\na, µa and σ2\np, µp are the variances and",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "to PT for wav2vec 2.0. Conversely, for HuBERT,\nthis phenomenon"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "means for annotation and predictions.",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "doesn’t hold and the performance declines when WS and WG are"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "introduced. The decrease in performance could be due to overmodu-"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "4. RESULTS",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "lation, where the added complexity of these PEFT methods result in"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "reduced performance.\nIt’s worth noting that LoRA proves effective"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "PEFT adaptors were compared to the pre-trained foundation model",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "for dominance prediction, while BA excels in valence and arousal"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "without any finetuning (PT) and fully fine-tuning (FT) where the",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "prediction.\nThe results suggest\nthat due to the increased task dif-"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "CNN feature extractor is frozen and all\ntransformer blocks are up-",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "ficulty, PEFT modules tend to prioritize the enhancement of a spe-"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "dated.",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "cific emotion dimension. Overall,\nthe combination of PEFT meth-"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "ods demonstrated superior performance compared to FT. Further-"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "4.1. Efficient finetuning for SER",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "more, PEFT for both wav2vec 2.0 and HuBERT models for dimen-"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "sional emotional attribute prediction surpassed existing state-of-the-"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "Table 1 presents\nthe outcome of\napplying LoRA, BA, WS,\nand",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "art CCC scores for valence (0.582), arousal (0.667) and dominance"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "WG PEFT methods\nand\ntheir\ncombinations\nto\nboth\ncategorical",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "(0.545) [27] with significantly fewer trainable parameters."
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "and dimensional emotion prediction tasks for the wav2vec 2.0 and",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "HuBERT models. Notably, all PEFT methods, except\nfor WG in",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "4.2. Adaptation from acted emotion to natural emotion"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "HuBERT, exhibit superior performance compared to the pre-trained",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "baseline. Augmenting the combination of BA, LoRA, and WS with",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "This section investigates the generalisability of a model\ntrained on"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "WG yields an overall performance boost\nthat surpasses fine-tuning",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "acted emotion data to natural emotion.\nThe combination of BA,"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "(FT) while using 2-3% of\nthe total\ntrainable parameters\nfor both",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "LoRA, WS and WG was used for stage 1 adapting since it produces"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "models.\nIn terms of classification tasks, wav2vec 2.0 and HuBERT",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "the best performance in Table 1. Both intra- and cross-corpus adap-"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "yield similar performance for different PEFT methods.\nHuBERT",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "tation were investigated. Fold 1 (the model\ntrained on sessions 2-5"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "stands out particularly in categorical\nlabel prediction, achieving re-",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "and tested on session 1) was used for IEMOCAP."
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "sults not far from the state-of-the-art of 74.2% [26] without relying",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "on external features, using only raw audio data.",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "4.2.1.\nIntra-corpus adaptation"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "However, a noteworthy observation is that\nthe combination of",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": ""
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "LoRA and BA falls short of the best-performing standalone methods.",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "IEMOCAP contains scripted dialogues (IEMsc) with acted emotion"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "One plausible explanation could be over-parameterisation, which",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "and improvised (IEMim) which contain more natural emotional ex-"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "may impede model generalisation and increase complexity, poten-",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "pression. We first\ninvestigated adapting a model\ntrained on scripted"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "tially leading to overfitting the training data. WS and WG seem to",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "data to improvised emotion."
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "serve as corrective measures, potentially compensating for lost dis-",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "The first\ntwo rows of Table 2 show the results of directly fine-"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "criminative information and effectively modulating crucial features.",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "tuning\npre-trained model\non\nscripted\ndata\nand\nimprovised\ndata"
        },
        {
          "✓\n✓\n✓\n✓\n67.00\n2.5 M\n0.571\n0.708": "Consequently, they exhibit the highest accuracies for each model.",
          "71.88\n0.516\n6.7 M\n0.651\n0.701\n0.521": "respectively. It can be seen that achieving high accuracy on scripted"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: presents the results of adapting from the ESD dataset experiments highlight the effectiveness of this adaptation strategy,",
      "data": [
        {
          "wav2vec 2.0BASE": "IEMsc\nIEMim",
          "HuBERTLARGE": "IEMsc\nIEMim"
        },
        {
          "wav2vec 2.0BASE": "",
          "HuBERTLARGE": "71.63"
        },
        {
          "wav2vec 2.0BASE": "",
          "HuBERTLARGE": ""
        },
        {
          "wav2vec 2.0BASE": "",
          "HuBERTLARGE": ""
        },
        {
          "wav2vec 2.0BASE": "",
          "HuBERTLARGE": ""
        },
        {
          "wav2vec 2.0BASE": "",
          "HuBERTLARGE": ""
        },
        {
          "wav2vec 2.0BASE": "",
          "HuBERTLARGE": ""
        },
        {
          "wav2vec 2.0BASE": "",
          "HuBERTLARGE": ""
        },
        {
          "wav2vec 2.0BASE": "",
          "HuBERTLARGE": ""
        },
        {
          "wav2vec 2.0BASE": "55.32\n68.91",
          "HuBERTLARGE": "63.83\n74.66"
        },
        {
          "wav2vec 2.0BASE": "",
          "HuBERTLARGE": ""
        },
        {
          "wav2vec 2.0BASE": "58.87",
          "HuBERTLARGE": ""
        },
        {
          "wav2vec 2.0BASE": "",
          "HuBERTLARGE": ""
        },
        {
          "wav2vec 2.0BASE": "70.63",
          "HuBERTLARGE": "74.66"
        },
        {
          "wav2vec 2.0BASE": "",
          "HuBERTLARGE": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: presents the results of adapting from the ESD dataset experiments highlight the effectiveness of this adaptation strategy,",
      "data": [
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": ""
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": "wav2vec 2.0BASE"
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "Source",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": "IEMim"
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "PT",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": "33.01"
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": ""
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": ""
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "PT",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": "67.56"
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": ""
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "ESD",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": "64.10"
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": ""
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "ESD",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": "70.63"
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": ""
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": ""
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "ESD",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": "67.56"
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": ""
        },
        {
          "Intra-corpus IEMOCAP scripted (IEMsc) to improvised (IEMim) adaptation.": "ESD",
          "‘✓’ denotes that PEFT module is updated and ‘∗’": "69.28"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: presents the results of adapting from the ESD dataset experiments highlight the effectiveness of this adaptation strategy,",
      "data": [
        {
          "Table 3. Cross-corpus ESD to IEMOCAP improvised (IEMim) adaptation. ‘✓’ denotes that PEFT module is updated and ‘∗’ that it is frozen.": ""
        },
        {
          "Table 3. Cross-corpus ESD to IEMOCAP improvised (IEMim) adaptation. ‘✓’ denotes that PEFT module is updated and ‘∗’ that it is frozen.": "serving knowledge from the source domain and effectively adapting"
        },
        {
          "Table 3. Cross-corpus ESD to IEMOCAP improvised (IEMim) adaptation. ‘✓’ denotes that PEFT module is updated and ‘∗’ that it is frozen.": "to the target domain. Both models exhibit the best improvised accu-"
        },
        {
          "Table 3. Cross-corpus ESD to IEMOCAP improvised (IEMim) adaptation. ‘✓’ denotes that PEFT module is updated and ‘∗’ that it is frozen.": "racy when LoRA is frozen and and their performance across different"
        },
        {
          "Table 3. Cross-corpus ESD to IEMOCAP improvised (IEMim) adaptation. ‘✓’ denotes that PEFT module is updated and ‘∗’ that it is frozen.": "methods is quite similar. Presumably, LoRA captures important fea-"
        },
        {
          "Table 3. Cross-corpus ESD to IEMOCAP improvised (IEMim) adaptation. ‘✓’ denotes that PEFT module is updated and ‘∗’ that it is frozen.": "tures and freezing it allows valuable knowledge to be retained and"
        },
        {
          "Table 3. Cross-corpus ESD to IEMOCAP improvised (IEMim) adaptation. ‘✓’ denotes that PEFT module is updated and ‘∗’ that it is frozen.": "doesn’t hinder generalisation. The results suggest\nthat freezing part"
        },
        {
          "Table 3. Cross-corpus ESD to IEMOCAP improvised (IEMim) adaptation. ‘✓’ denotes that PEFT module is updated and ‘∗’ that it is frozen.": "of the PEFT modules similar to intra-corpus adaptation is beneficial"
        },
        {
          "Table 3. Cross-corpus ESD to IEMOCAP improvised (IEMim) adaptation. ‘✓’ denotes that PEFT module is updated and ‘∗’ that it is frozen.": "in preserving source task accuracy. Unlike intra-corpus adaptation,"
        },
        {
          "Table 3. Cross-corpus ESD to IEMOCAP improvised (IEMim) adaptation. ‘✓’ denotes that PEFT module is updated and ‘∗’ that it is frozen.": "a large decline in performance is observed for the ESD dataset after"
        },
        {
          "Table 3. Cross-corpus ESD to IEMOCAP improvised (IEMim) adaptation. ‘✓’ denotes that PEFT module is updated and ‘∗’ that it is frozen.": "adaptation, possibly due to the fact\nthat emotions in ESD follow a"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen, “Lora:"
        },
        {
          "6. REFERENCES": "[1] Gerben A Van Kleef, Arik Cheshin, Agneta H Fischer, and",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Low-rank adaptation of large language models,” arXiv preprint"
        },
        {
          "6. REFERENCES": "Iris K Schneider, “The social nature of emotions,” Frontiers in",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "arXiv:2106.09685, 2021."
        },
        {
          "6. REFERENCES": "psychology, vol. 7, pp. 896, 2016.",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "[16] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna"
        },
        {
          "6. REFERENCES": "[2] Chai M Tyng, Hafeez U Amin, Mohamad NM Saad,\nand",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona"
        },
        {
          "6. REFERENCES": "Aamir S Malik,\n“The influences of emotion on learning and",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Attariyan,\nand Sylvain Gelly,\n“Parameter-efficient\ntransfer"
        },
        {
          "6. REFERENCES": "memory,” Frontiers in psychology, p. 1454, 2017.",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "learning for nlp,”\nin International Conference on Machine"
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Learning. PMLR, 2019, pp. 2790–2799."
        },
        {
          "6. REFERENCES": "[3] Y. Kim, H. Lee, and E. M. Provost,\n“Deep learning for\nro-",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "[17] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg, “Bitfit:"
        },
        {
          "6. REFERENCES": "bust feature generation in audiovisual emotion recognition,” in",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Simple parameter-efficient fine-tuning for\ntransformer-based"
        },
        {
          "6. REFERENCES": "Proc. ICASSP, Vancouver, 2013.",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "masked language-models,”\narXiv preprint arXiv:2106.10199,"
        },
        {
          "6. REFERENCES": "[4]\nS. Poria, E. Cambria, R. Bajpai, and A. Hussain, “A review of",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "2021."
        },
        {
          "6. REFERENCES": "affective computing:\nFrom unimodal analysis to multimodal",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "[18] Zih-Ching\nChen,\nChin-Lun\nFu,\nChih-Ying\nLiu,\nShang-"
        },
        {
          "6. REFERENCES": "fusion,” Information Fusion, vol. 37, pp. 98–125, 2017.",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Wen Daniel Li, and Hung-yi Lee,\n“Exploring efficient-tuning"
        },
        {
          "6. REFERENCES": "[5] Wen Wu, Chao Zhang,\nand Philip C Woodland,\n“Emotion",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "methods in self-supervised speech models,” in 2022 IEEE Spo-"
        },
        {
          "6. REFERENCES": "recognition by fusing time synchronous and time asynchronous",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "ken Language Technology Workshop (SLT), 2023, pp. 1120–"
        },
        {
          "6. REFERENCES": "representations,” in Proc. ICASSP, Online, 2021.",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "1127."
        },
        {
          "6. REFERENCES": "[6] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "[19] Yingting Li, Ambuj Mehrish, Rishabh Bhardwaj, Navonil Ma-"
        },
        {
          "6. REFERENCES": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "jumder, Bo Cheng, Shuai Zhao, Amir Zadeh, Rada Mihalcea,"
        },
        {
          "6. REFERENCES": "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“IEMO-",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "and Soujanya Poria,\n“Evaluating parameter-efficient\ntransfer"
        },
        {
          "6. REFERENCES": "CAP: Interactive emotional dyadic motion capture database,”",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "learning approaches on sure benchmark for speech understand-"
        },
        {
          "6. REFERENCES": "Language Resources and Evaluation, vol. 42, pp. 335–359,",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "ing,” in Proc. ICASSP, Rhodes, Greece, 2023."
        },
        {
          "6. REFERENCES": "2008.",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "[20] Robert Plutchik,\n“A general psychoevolutionary theory of"
        },
        {
          "6. REFERENCES": "[7]\nSteven R Livingstone\nand\nFrank A Russo,\n“The Ryer-",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "emotion,” in Theories of emotion, pp. 3–33. Elsevier, 1980."
        },
        {
          "6. REFERENCES": "son Audio-Visual Database of Emotional Speech and Song",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "[21] Albert Mehrabian,\n“Basic dimensions for a general psycho-"
        },
        {
          "6. REFERENCES": "(RAVDESS): A dynamic, multimodal set of\nfacial and vocal",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "logical\ntheory:\nImplications\nfor personality,\nsocial, environ-"
        },
        {
          "6. REFERENCES": "expressions in North American English,”\nPloS one, vol. 13,",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "mental, and developmental studies, vol. 2,” Cambridge, MA:"
        },
        {
          "6. REFERENCES": "no. 5, pp. e0196391, 2018.",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Oelgeschlager, Gunn & Hain, 1980."
        },
        {
          "6. REFERENCES": "[8] Olivier Martin, Irene Kotsia, Benoit Macq, and Ioannis Pitas,",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "[22] Abdelrahman Mohamed,\nHung-yi\nLee,\nLasse\nBorgholt,"
        },
        {
          "6. REFERENCES": "“The\neNTERFACE’05 audio-visual\nemotion database,”\nin",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Jakob D Havtorn, Joakim Edin, Christian Igel, Katrin Kirch-"
        },
        {
          "6. REFERENCES": "22nd International Conference on Data Engineering Work-",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "hoff, Shang-Wen Li, Karen Livescu, Lars Maaløe, et al., “Self-"
        },
        {
          "6. REFERENCES": "shops (ICDEW’06). IEEE, 2006, pp. 8–8.",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "IEEE\nsupervised speech representation learning: A review,”"
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Journal of Selected Topics in Signal Processing, 2022."
        },
        {
          "6. REFERENCES": "[9] Houwei Cao, David G Cooper, Michael K Keutmann, Ruben C",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "Gur, Ani Nenkova, and Ragini Verma,\n“CREMA-D: Crowd-",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "[23] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand"
        },
        {
          "6. REFERENCES": "sourced emotional multimodal actors dataset,” IEEE Transac-",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Michael Auli, “wav2vec 2.0: A framework for self-supervised"
        },
        {
          "6. REFERENCES": "tions on Affective Computing, vol. 5, no. 4, pp. 377–390, 2014.",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "learning of speech representations,” Advances in neural infor-"
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "mation processing systems, vol. 33, pp. 12449–12460, 2020."
        },
        {
          "6. REFERENCES": "[10]\nFelix Burkhardt, Astrid Paeschke, Miriam Rolfes, Walter F",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "Sendlmeier, Benjamin Weiss, et al.,\n“A database of German",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "[24] Wei-Ning Hsu,\nBenjamin Bolte,\nYao-Hung Hubert\nTsai,"
        },
        {
          "6. REFERENCES": "emotional speech.,”\nin Interspeech, 2005, vol. 5, pp. 1517–",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman"
        },
        {
          "6. REFERENCES": "1520.",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Mohamed,\n“HuBERT: Self-supervised speech representation"
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "IEEE/ACM\nlearning by masked prediction of hidden units,”"
        },
        {
          "6. REFERENCES": "[11] Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li,\n“Emo-",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Transactions on Audio, Speech, and Language Processing, vol."
        },
        {
          "6. REFERENCES": "tional voice conversion: Theory, databases and ESD,” Speech",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "29, pp. 3451–3460, 2021."
        },
        {
          "6. REFERENCES": "Communication, vol. 137, pp. 1–18, 2022.",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "[25]\nShu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff"
        },
        {
          "6. REFERENCES": "[12] Yu Zhang, Daniel S Park, Wei Han,\nJames Qin, Anmol Gu-",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Lai, Kushal Lakhotia, Yist Y Lin, Andy T Liu,\nJiatong Shi,"
        },
        {
          "6. REFERENCES": "lati, Joel Shor, Aren Jansen, Yuanzhong Xu, Yanping Huang,",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "Xuankai Chang, Guan-Ting Lin, et al.,\n“SUPERB: Speech"
        },
        {
          "6. REFERENCES": "Shibo Wang, et al.,\n“BigSSL: Exploring the frontier of large-",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "processing universal performance benchmark,” arXiv preprint"
        },
        {
          "6. REFERENCES": "scale semi-supervised learning for automatic speech recogni-",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "arXiv:2105.01051, 2021."
        },
        {
          "6. REFERENCES": "tion,”\nIEEE Journal of Selected Topics in Signal Processing,",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "[26]\nItai Gat, Hagai Aronowitz, Weizhong Zhu, Edmilson Morais,"
        },
        {
          "6. REFERENCES": "vol. 16, no. 6, pp. 1519–1532, 2022.",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "and Ron Hoory,\n“Speaker normalization for self-supervised"
        },
        {
          "6. REFERENCES": "[13] Edmilson Morais,\nRon Hoory, Weizhong\nZhu,\nItai Gat,",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "speech emotion recognition,”\nin Proc.\nICASSP, Singapore,"
        },
        {
          "6. REFERENCES": "Matheus Damasceno, and Hagai Aronowitz, “Speech emotion",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "2022."
        },
        {
          "6. REFERENCES": "recognition using self-supervised features,”\nin Proc. ICASSP,",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "[27]\nSundararajan Srinivasan, Zhaocheng Huang, and Katrin Kirch-"
        },
        {
          "6. REFERENCES": "Singapore, 2022.",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "hoff,\n“Representation learning through cross-modal condi-"
        },
        {
          "6. REFERENCES": "[14] Wen Wu, Chao Zhang,\nand Philip C Woodland,\n“Self-",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "tional teacher-student training for speech emotion recognition,”"
        },
        {
          "6. REFERENCES": "supervised representations in speech-based depression detec-",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": "in Proc. ICASSP, Singapore, 2022."
        },
        {
          "6. REFERENCES": "tion,” in Proc. ICASSP, Rhodes, Greece, 2023.",
          "[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "The social nature of emotions",
      "authors": [
        "Gerben A Van Kleef",
        "Arik Cheshin",
        "Agneta Fischer",
        "Iris Schneider"
      ],
      "year": "2016",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "3",
      "title": "The influences of emotion on learning and memory",
      "authors": [
        "Chai M Tyng",
        "Mohamad Hafeez U Amin",
        "Aamir S Saad",
        "Malik"
      ],
      "year": "2017",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "4",
      "title": "Deep learning for robust feature generation in audiovisual emotion recognition",
      "authors": [
        "Y Kim",
        "H Lee",
        "E Provost"
      ],
      "year": "2013",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "5",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Philip Woodland"
      ],
      "year": "2021",
      "venue": "Proc. ICASSP, Online"
    },
    {
      "citation_id": "7",
      "title": "IEMO-CAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "8",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "9",
      "title": "The eNTERFACE'05 audio-visual emotion database",
      "authors": [
        "Olivier Martin",
        "Irene Kotsia",
        "Benoit Macq",
        "Ioannis Pitas"
      ],
      "year": "2006",
      "venue": "22nd International Conference on Data Engineering Workshops (ICDEW'06)"
    },
    {
      "citation_id": "10",
      "title": "CREMA-D: Crowdsourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Ruben Michael K Keutmann",
        "Ani Gur",
        "Ragini Nenkova",
        "Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "A database of German emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "A database of German emotional speech"
    },
    {
      "citation_id": "12",
      "title": "Emotional voice conversion: Theory, databases and ESD",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "13",
      "title": "BigSSL: Exploring the frontier of largescale semi-supervised learning for automatic speech recognition",
      "authors": [
        "Yu Zhang",
        "S Daniel",
        "Wei Park",
        "James Han",
        "Anmol Qin",
        "Joel Gulati",
        "Aren Shor",
        "Yuanzhong Jansen",
        "Yanping Xu",
        "Shibo Huang",
        "Wang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "Edmilson Morais",
        "Ron Hoory",
        "Weizhong Zhu",
        "Itai Gat",
        "Matheus Damasceno",
        "Hagai Aronowitz"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Selfsupervised representations in speech-based depression detection",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Philip Woodland"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "16",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Yelong Hu",
        "Phillip Shen",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "arxiv": "arXiv:2106.09685"
    },
    {
      "citation_id": "17",
      "title": "Parameter-efficient transfer learning for nlp",
      "authors": [
        "Neil Houlsby",
        "Andrei Giurgiu",
        "Stanislaw Jastrzebski",
        "Bruna Morrone",
        "Quentin De Laroussilhe",
        "Andrea Gesmundo",
        "Mona Attariyan",
        "Sylvain Gelly"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "18",
      "title": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
      "authors": [
        "Elad Ben Zaken",
        "Shauli Ravfogel",
        "Yoav Goldberg"
      ],
      "year": "2021",
      "venue": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
      "arxiv": "arXiv:2106.10199"
    },
    {
      "citation_id": "19",
      "title": "Exploring efficient-tuning methods in self-supervised speech models",
      "authors": [
        "Zih-Ching Chen",
        "Chin-Lun Fu",
        "Chih-Ying Liu",
        "Shang-Wen Daniel",
        "Hung-Yi Lee"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "20",
      "title": "Evaluating parameter-efficient transfer learning approaches on sure benchmark for speech understanding",
      "authors": [
        "Yingting Li",
        "Ambuj Mehrish",
        "Rishabh Bhardwaj",
        "Navonil Majumder",
        "Bo Cheng",
        "Shuai Zhao",
        "Amir Zadeh",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "21",
      "title": "A general psychoevolutionary theory of emotion",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1980",
      "venue": "Theories of emotion"
    },
    {
      "citation_id": "22",
      "title": "Basic dimensions for a general psychological theory: Implications for personality, social, environmental, and developmental studies",
      "authors": [
        "Albert Mehrabian"
      ],
      "year": "1980",
      "venue": "Basic dimensions for a general psychological theory: Implications for personality, social, environmental, and developmental studies"
    },
    {
      "citation_id": "23",
      "title": "Selfsupervised speech representation learning: A review",
      "authors": [
        "Abdelrahman Mohamed",
        "Hung-Yi Lee",
        "Lasse Borgholt",
        "Jakob Havtorn",
        "Joakim Edin",
        "Christian Igel",
        "Katrin Kirchhoff",
        "Shang-Wen",
        "Karen Li",
        "Lars Livescu",
        "Maaløe"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "25",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "26",
      "title": "SUPERB: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Andy Yist Y Lin",
        "Jiatong Liu",
        "Xuankai Shi",
        "Guan-Ting Chang",
        "Lin"
      ],
      "year": "2021",
      "venue": "SUPERB: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "27",
      "title": "Speaker normalization for self-supervised speech emotion recognition",
      "authors": [
        "Itai Gat",
        "Hagai Aronowitz",
        "Weizhong Zhu",
        "Edmilson Morais",
        "Ron Hoory"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "28",
      "title": "Representation learning through cross-modal conditional teacher-student training for speech emotion recognition",
      "authors": [
        "Sundararajan Srinivasan",
        "Zhaocheng Huang",
        "Katrin Kirchhoff"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    }
  ]
}