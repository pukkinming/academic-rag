{
  "paper_id": "2204.05281v1",
  "title": "Physically Disentangled Representations",
  "published": "2022-04-11T17:36:40Z",
  "authors": [
    "Tzofi Klinghoffer",
    "Kushagra Tiwary",
    "Arkadiusz Balata",
    "Vivek Sharma",
    "Ramesh Raskar"
  ],
  "keywords": [
    "Representation Learning",
    "Unsupervised Learning",
    "3D Understanding",
    "Inverse Rendering",
    "Inverse Graphics",
    "Face Analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "State-of-the-art methods in generative representation learning yield semantic disentanglement, but typically do not consider physical scene parameters, such as geometry, albedo, lighting, or camera. We posit that inverse rendering, a way to reverse the rendering process to recover scene parameters from an image, can also be used to learn physically disentangled representations of scenes without supervision. In this paper, we show the utility of inverse rendering in learning representations that yield improved accuracy on downstream clustering, linear classification, and segmentation tasks with the help of our novel Leave-One-Out, Cycle Contrastive loss (LOOCC), which improves disentanglement of scene parameters and robustness to out-of-distribution lighting and viewpoints. We perform a comparison of our method with other generative representation learning methods across a variety of downstream tasks, including face attribute classification, emotion recognition, identification, face segmentation, and car classification. Our physically disentangled representations yield higher accuracy than semantically disentangled alternatives across all tasks and by as much as 18%. We hope that this work will motivate future research in applying advances in inverse rendering and 3D understanding to representation learning. Code is made available here.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Unsupervised representation learning is a long-standing goal in the computer vision community. While representation learning has become widely popular in recent years as a way to mitigate dependencies on labeled data, learned features are typically said to capture semantic properties, not specific physical properties, such as geometry, albedo, lighting, or camera view. Existing representation learning methods can broadly be categorized as either generative or discriminative. Generative models rely on image generation or reconstruction as a signal for self-supervision, whereas discriminative models seek to predict known attributes of images as an auxiliary task. In the case of generative representation learning methods, such as as variational autoencoders (VAEs)  [17]  and generative Fig.  1 : Overview: (a) we aim to achieve physical disentanglement using a differentiable renderer in the loop during training, as opposed to (b) other generative representation learning methods, such as  [3, 7, 14] , that learn semantically disentangled features.\n\nadversarial networks (GANs)  [10] , learned features have also been shown to be semantically disentangled  [2, 3] . Using a physical model, such as a renderer, to capture and disentangle the features of physical properties is less studied, but could be highly impactful. Motivated by the human ability to understand the world under different lighting conditions and viewpoints, we propose a method to learn and disentangle the features of physical properties using inverse rendering.\n\nInverse rendering provides a way for physical properties, also known as physical scene parameters, such as geometry, albedo, lighting, and camera view, to be predicted from images, and has rapidly advanced in recent years due to the availability of differentiable renderers. Whereas rendering, commonly used in graphics, synthesizes images from scene parameters, inverse rendering decomposes images into scene parameters. Because a differentiable renderer is used in training, features of complex physical parameters that are modeled by the renderer can be explicitly learned. By leveraging available priors, synthetic data, or implicit cues, such as symmetry, inverse rendering can be learned without supervision, and thus we show can be used for representation learning.\n\nIn this paper, we leverage existing inverse rendering methods to learn physically disentangled representations that have utility across a wide-range of downstream tasks. We first introduce a general framework for representation learning using inverse rendering that can be adapted to fit existing inverse renderers. Since disentangling scene parameters without supervision is ill-posed, we then propose a novel loss term called Leave-One-Out, Cycle Contrastive loss (LOOCC) to improve disentanglement in the feature space. LOOCC applies contrastive learning to the features of images and their physically augmented counterparts, generated by a renderer. By generating augmented images with only a single changed scene parameter, we can then enforce that features of other scene parameters remain unchanged. Finally, we evaluate the learned representations on clustering, linear classification, and segmentation tasks. We show improved performance over strong baselines on all tasks, including face attribute classification, emotion recognition, identification, face segmentation, and car classification. Lastly, we discuss how including a renderer in learning can improve  (1)  interpretability by attributing predictions to physical scene parameters, and (2) robustness to physical phenomena, such as novel lighting and views, which remains a challenge in deep learning  [26, 27] . In summary, we make the following contributions:\n\n-A general framework for learning physically disentangled feature representations through inverse rendering without supervision. Representations learned with our framework can effectively be used across many downstream tasks. -A novel objective called Leave-One-Out, Cycle Contrastive loss (LOOCC) that helps disentangle the features of physical scene parameters such that they are more useful for downstream tasks. -Detailed empirical results showing the benefit of our learned features for downstream clustering, linear classification, and segmentation tasks, including face attribute classification, emotion recognition, identification, face segmentation, and car classification. -Discussions on the improvements the above contributions make to interpretability and robustness to out-of-distribution lighting and camera views.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Representation Learning",
      "text": "Visual representation learning techniques aim to extract meaningful features from unlabeled images. Once models have been trained on the unlabeled data, learned representations can be transferred to downstream supervised learning tasks or used for applications such as image editing or synthesis.\n\nDiscriminative Methods: Popular discriminative methods for representation learning include self-supervised learning and contrastive learning. Self-supervised learning methods use auxiliary tasks, such as predicting image rotation, to learn features from unlabeled data  [8, 9, 19, 30] . Contrastive learning methods, such as SimCLR  [6] , minimize the mutual information between multiple views of an image, while retaining task-relevant features  [43] . Typically, image views are created using standard data augmentation, rather than physical augmentations, such as changes to lighting or camera view, as done in our work. Learning disentangled features with discriminative representation learning methods remains an open problem  [4] . We use a renderer to disentangle features and create physical augmentations that are used in our contrastive loss.\n\nGenerative Methods: VAEs  [3, 17, 28, 45]  and GANs  [2, 29]  are commonly used for generative representation learning. Methods such as  [52]  utilize semantic disentanglement in the StyleGAN  [13, 14]  latent space to perform image editing.\n\nOther methods, such as Retrieve in Style (RIS)  [7] , further disentangle the Style-GAN2 latent code and use it for tasks such as image retrieval. Few papers have explored physical disentanglement in generative models.  [20]  shows that the VAE latent space can be disentangled into graphics codes, such as pose, light, texture or shape, but do not utilize a renderer.  [49]  trains an encoder to learn disentangled scene parameters by using reinforcement learning with a non-differentiable graphics engine on synthetic data, while  [48]  extend this to real images of simple scenes. We use a differentiable renderer and demonstrate our method on images in the wild across many tasks. 3D Representations Recent work has begun to explore how 3D information can be leveraged to improve the quality of learned representations.  [12]  proposes the use of an RGB-D dataset to render scenes from multiple views, using each view as an augmentation for a contrastive loss, and shows improved performance on downstream object detection and segmentation.  [18]  uses a scene representation network (SRN)  [39] , an implicit representation of a 3D scene, as a feature extractor for semi-supervised segmentation, training a segmentation classifier on top of the SRN features. Our work is motivated by these papers, and introduces a new way to learn physical representations that are useful for downstream tasks.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Inverse Rendering",
      "text": "Neural inverse renderers use neural networks to predict scene parameters from images and differentiable renderers to perform image reconstruction. Wu et al.\n\n[51] accomplish unsupervised inverse rendering by leveraging symmetry in objects to create second views, but are thus constrained to highly symmetric objects such as human faces, cat faces, cars, etc.  [50]  builds on this method by introducing a self-supervised discriminator to improve disentanglement of albedo in regions of high specularity, while  [47]  extends  [51]  to non-symmetrical objects in the wild by including a geometry-prior in the form of a pre-trained depth network or structure from motion.  [24, 35]  both achieve inverse rendering of indoor scenes by first pre-training their models on synthetic data with ground truth for each scene parameter.  [53]  performs inverse rendering of outdoor scenes by utilizing multi-view stereo and a albedo consistency loss that enforces predicted albedo is consistent under different lighting. Our method utilizes the inverse rendering pipeline from  [51]  for representation learning from unlabeled single views, assessing performance on downstream tasks, rather than image reconstruction.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Proposed Method",
      "text": "In this section, we present our proposed method for learning representations with physical disentanglement through the use of inverse rendering. First, we describe a general framework for representation learning through inverse rendering, and then we describe our proposed Leave-One-Out, Cycle Contrastive loss (LOOCC).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Learning Representations With Inverse Rendering",
      "text": "We use inverse rendering as a mechanism to learn feature representations that are disentangled with regard to the physical scene parameters modeled by the renderer. Many existing inverse rendering methods use a shared encoder and Fig.  2 : Our proposed method. Encoders are pre-trained for each physical scene parameter using an inverse rendering objective with a renderer in the loop. Learning is constrained by both a reconstruction loss and our novel Leave-One-Out, Cycle Contrastive (LOOCC) loss (illustrated in orange), requiring no supervision. LOOCC improves disentanglement between scene parameters by re-rendering images with new lighting or view, and enforcing that the features for other scene parameters remain similar. After training, the encoders can be used as feature extractors for downstream tasks.\n\nseparate decoders to estimate scene parameters  [24, 35] . We instead propose the use of separate encoders per scene parameter to explicitly disentangle the scene parameter features. While this architectural modification can easily be implemented in methods that use a shared encoder, we instead utilize the work of Wu et al.  [51] , which proposes a highly modular pipeline consisting of separate encoders and decoders for each scene parameter.\n\nNeural Inverse Renderer: The trainable inverse renderer is composed of a set of encoders E θ , which each take an image x ∈ R H×W ×3 to a physically disentangled feature, geometry, albedo, light, or camera. The output of each encoder is used to train a decoder D θ to predict the corresponding scene parameter. Predicted light values (1 × 4) embed ambient and diffuse intensity, pitch, and yaw, while camera values embed camera rotation and translation in x, y, and z (1 × 6). Both the encoders and decoders are parameterized by a neural network.\n\nDifferentiable Renderer (NMR): While E θ predicts physically disentangled features Z x , their explicit counterparts, scene parameters S px , are predicted by each decoder D θ . The scene parameters are then fed into a differentiable renderer (NMR)  [15] , R N M R . The NMR is responsible for constraining the encoderdecoders by reconstructing the input image x.\n\nRepresentations: Features are extracted from the last conv layer of each encoder and stacked for use on downstream tasks. The stack can contain the features for all scene parameters (geometry, albedo, lighting, and camera view) or a subset. By including a subset, robustness to the omitted features can be improved, as decision-making no longer relies on the now omitted features, as discussed in Section 5. In this paper, we only utilize geometry and albedo features for downstream tasks because they capture the most information about the scene. We also observe that, surprisingly, light and camera features are useful on their own for many downstream tasks, as reported in the Appendix. In our method, we pre-train the model solely on the inverse rendering task without consideration of which downstream task(s) the features will be used for. Future work could harness the proposed framework to jointly train a learned representation with the target task in mind.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Leave-One-Out, Cycle Contrastive Loss",
      "text": "Disentangling scene parameters without supervision is a key challenge in inverse rendering. We propose a novel Leave-One-Out, Cycle Contrastive loss (LOOCC) to improve disentanglement as shown in Figure  1 . Our method consists of physical augmentation, cyclic encoding, and contrastive learning.\n\nPhysical Augmentation: In addition to reconstructing x, our method generates an augmented image of the scene by randomly perturbing a predicted scene parameter, S p . Since the light and camera parameters are represented as four and six dimensional vectors, respectively, they can be perturbed by sampling a uniform distribution bounded by the desired range of each value. We randomly perturb light or camera while keeping other parameters the same, and use R N M R to render the augmented image. We define a function f that takes as input estimated scene parameters, S px for a given input image x, and randomly selects and perturbs either the light or camera parameter. In the below equation, we follow the example from Figure  2  and let f perturb the camera parameter.\n\nCyclic Encoding: We leverage the observation that if we reconstruct an augmented image x recon aug from S px aug , it should differ only by one scene parameter.\n\nThe augmented image can then cycle back through E θ , generating a set of augmented features, Z xaug , that should be the same as the features of x, except for the features of the perturbed parameter. Following from the above example where the camera parameter is perturbed, we get Z U xaug which are the features of the unchanged scene parameters from the augmented image.\n\nLeave-One-Out Contrastive Loss: We leave out the features of the single perturbed scene parameter and use the contrastive loss proposed by  [6]  to enforce that the rest of the features in x and x recon aug are similar, and thus that perturbing one scene parameter does not impact the features for the rest. Our intuition is that by leaving out one set of features, we allow these features in x and x recon aug to be pushed apart, while the contrastive loss pulls the rest of the features together. Following the example from Figure  2  where camera view augmentation is shown, we arrive at the following equation for the normalized temperature-scaled cross entropy (contrastive) loss. We denote Z U\n\nx to be all the features of the unchanged scene parameters from x, leaving out the features that were perturbed in x recon aug .\n\nIn Equation  6 , sim measures cosine similarity of two feature vectors, N is the minibatch size of the input, and τ is the temperature parameter.\n\nTotal Loss: We define the total loss as a weighted sum of the reconstruction and LOOCC loss terms. In our work, we utilize the reconstruction loss proposed by Wu et al.  [51] , but any reconstruction loss can be used.\n\nWhile it is clear that modifying the lighting of a scene should not impact geometry, albedo, or camera, the impact of modifying the camera is less clear. While the geometry and albedo of a scene rendered from the perspective of two camera viewpoints will be different, we note that the encoder-decoders are tasked with recovering the canonical geometry and albedo. Thus, regardless of the camera view, the outputs of the encoder-decoders will be consistent, and we can safely enforce that changes in camera view do not result in changes in geometry or albedo. Finally, while the methods presented in this paper focus on augmenting light and camera view, they can also be extended to augment other scene parameters, assuming a sufficient sampling mechanism for perturbations. Since the current architecture uses autoencoders to estimate geometry and albedo, there is no explicit sampling method, but this could be overcome by using a VAE or network flow model to estimate geometry and albedo.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we first introduce our datasets and evaluation metrics, followed by a description of our implementation. We then present the results of using the features learned by our method for many downstream tasks. We compare three versions of our method -No LOOCC, LOOCC with light augmentations (LOOCC-L), and LOOCC with light and view augmentations (LOOCC-LV).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Datasets",
      "text": "Training: We train two sets of models with our proposed method. Our face models are trained on the UTK Face dataset  [56] , containing 23,708 images, and we use an 80/10/10 split for train, validation, and test. Our car models are trained on the ShapeNet  [5]  cars dataset rendered by  [51] , consisting of 28k train images, 7k validation images, and 7k test images. Testing: We draw upon multiple datasets to test the performance of our method across different downstream tasks. We use the CelebA dataset  [25]  for face attribute classification (Section 4.4), Buffy the Vampire Slayer (Buffy)  [1]  and Big Bang Theory (BBT)  [41]  for identification (Section 4.5), the Real-world Affective Faces Database (RAF-DB)  [22, 23]  for emotion recognition (Section 4.6), the CelebA Mask dataset  [21]  for face segmentation (Section 4.7), and a subset of the ShapeNet cars test dataset mentioned above for car classification (Section 4.8). Each test dataset is described in more detail in its corresponding section.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "We evaluate our method on clustering, linear classification, and segmentation tasks. For clustering, we report hierarchical agglomerative clustering (HAC) as commonly used in past work in representation learning  [37, 38, 42, 54, 55]  with minimum variance ward linkage  [46] . To measure clustering performance, we report clustering accuracy (also known as weighted cluster purity) and F1-score. F1-score is computed using a weighted average. Clustering accuracy is computed   1 : Clustering results for face classification tasks. Despite having a smaller latent space than the baselines, our method outperforms them, especially with the use of our novel Leave-One-Out, Cycle Contrastive loss (LOOCC). LOOCC-L indicates light augmentation and -LV indicates light and view augmentation. Tasks include identification (BBT, Buffy), emotion recognition (RAF-DB), and attribute classification (CelebA). by assigning the most common ground truth label for a cluster to all points in the cluster, as defined below.\n\nIn Equation  8 , N is the number of samples, n c is the number of samples in cluster c, and p c is cluster purity, measured as the fraction of the largest number of samples from the same class to n c . We set C as the number of classes. For linear classification, we report accuracy using a threshold of 50%. For segmentation, we report mean intersection over union (mIoU) and pixel accuracy.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Implementation Details",
      "text": "Our implementation was developed in PyTorch 1.8.0  [31] . Both our method and the VQ-VAE  [45]  baselines are trained using the ADAM optimizer  [16]  with an initial learning rate of 1 × 10 -3 , and early stopping is used for both to prevent overfitting, with a patience of ten epochs. Models are trained and tested using 64 × 64 sized images. When the LOOCC loss is employed with our method, we use a temperature of 0.5 for Equation  6 , and we set α to 0.01 and β to 1.00 for Equation  7 . For light augmentation, we randomly perturb ambient light intensity, diffuse light intensity, and lighting direction by sampling uniform distributions between (-0.5, 0.5), (-0.5, 0.5), and (-45°, 45°), respectively, where the lighting direction is sampled for both pitch and yaw. For camera augmentation, we randomly perturb pitch and yaw by sampling uniform distributions between (-22.5°, 22.5°) and (-45°, 45°), respectively. All sampled values are added to the corresponding predicted parameter for the image. Unless otherwise stated, only geometry and albedo features are used for downstream tasks.\n\nOur inverse rendering model has a total of 29,441,536 trainable parameters, with 12,979,648 for the encoder-decoders embedding the geometry and albedo features. Each scene parameter is represented by a 1 × 256 feature embedding, resulting in a 1 × 512 feature embedding for geometry and albedo together. We train the VQ-VAE with a latent dimension (D) of 416 and a latent dimension space (K) of 512, yielding 19,199,491 trainable parameters, and 416 × 25 × 25 (260k) sized feature embeddings. We also trained the VQ-VAE model with multiple parameters and found the above parameters yielded the strongest baseline.\n\nFor all face classification tasks, we also compare our model against the stateof-the-art StyleGAN2  [14] , with  [44]  to invert images into the GAN latent space, and Retrieve In Style  [7]  (which uses a StyleGAN2 generator, again with  [44]  for inversion). These models are pre-trained on the large-scale Flickr-Faces-HQ Dataset (FFHQ), containing 70,000 training images (nearly 3x the number of training images used by our model), and are much larger at 297,506,764 trainable parameters in the generator alone (over 10x larger than our model, not including the discriminator). The feature embeddings of these models are also much larger than ours (9216 and 8 × 9088, respectively, where the 8 channels in the RIS embedding encode features for nose, eyes, mouth, hair, background, cheeks, neck, and clothes). For fair comparison to our model, we test each baseline with 64×64 sized images, but also observe that our model is competitive with baselines even when they are tested with 256 × 256 images, as reported in the Appendix.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Face Attribute Classification",
      "text": "Details: We use the representations learned by our method for face attribute classification, evaluated with both clustering and linear classification. Our test dataset consists of 6,000 randomly selected images from CelebA. For clustering, we extract the features from our model and run clustering 40 times for each of the binary attributes in CelebA, reporting the average. For linear classification, we train an MLP projection head (i.e. linear classifier) on top of the pre-trained model being evaluated, following  [6] . We limit the labeled training samples to 100, 500, or 1000, and evaluate with the pre-trained model either frozen or finetuned, reporting each. Each classifier model is trained for 100 epochs.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Method",
      "text": "# Train Acc (Finetune) Acc (Frozen) Supervised ResNet-18  [11]  100 0.8186 -VQ-VAE (NeuRIPS '17)  [45]   Results: Results for clustering and linear classification are reported in Table  1  and Table  2 , respectively. Across both forms of evaluation, our model outperforms the baselines, even as we vary the number of training samples for linear classification. Interestingly, fine-tuning our model seems to provide a consistent benefit, while fine-tuning the baselines does not. In analyzing the linear model, we observed that the two attributes that our method improved accuracy on most were high cheek bone and mouth slightly open, where we expect geometric features captured in our model to be most beneficial.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Face Identification",
      "text": "Details: We demonstrate the utility of our learned representations on face identification, specifically on the challenging task of video face clustering. We use Buffy season 5, episode 2 and BBT season 1, episode 1 as prepared by  [37] . Both datasets contain extremely challenging out-of-distribution lighting and viewpoints, and thus test the robustness of our model. As in prior work  [36, 37] , clustering is done on a per-track basis by averaging the features of each frame in the track. Our Buffy dataset contains 568 tracks and six identities (Xander, Buffy, Dawn, Anya, Willow, Giles), and our BBT dataset contains 644 tracks and five identities (Howard, Leonard, Penny, Raj, Sheldon).\n\nResults: Our models outperform all baselines for clustering accuracy and F1 on both the Buffy and BBT datasets. Incorporation of the LOOCC loss is especially helpful since it improves disentanglement and thus robustness to lighting and viewpoints, which are challenging in both datasets. For BBT, the LOOCC loss with only light augmentations is best, whereas, for Buffy, the best clustering accuracy and F1 are achieved by the LOOCC model with light and view augmentations. During analysis, we observed that our models also have higher normalized mutual information (NMI) than all baselines on both Buffy and BBT. We achieve 43% and 23% on Buffy and BBT, respectively, whereas the best baseline methods, VQ-VAE and RIS, achieve 13% and 18%, respectively.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Details: We use clustering to perform emotion recognition using our learned representations. Our test dataset contains 3,068 images from RAF-DB, each containing one of the following seven emotions: surprise, fear, disgust, happiness, sadness, anger, and neutral.\n\nResults: Our method yields the strongest performance for emotion recognition over all metrics. We also observe that the LOOCC loss again yields improved performance over the no LOOCC alternative.   3 : U-net face segmentation results on the CelebA Mask dataset containing 19 classes. We report the mIoU, top 10 mIoU, and pixel accuracy, and compare the randomly initialized U-net with our method, which uses a pre-trained geometry or albedo U-net encoder and randomly initialized decoder. We also report performance when both the geometry and albedo encoders are jointly used as the U-net encoder. We allow the pre-trained encoders to be fine-tuned, except in the last row.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Face Segmentation",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Details:",
      "text": "We test our learned features on segmentation using 24,127 training images and 2,885 test images from CelebA Mask, all downsampled to 64 × 64. Segmentation is done over 19 classes, containing instances such as eyes, mouth, etc. We leverage the U-net model  [32]  to perform segmentation and compare the supervised U-net with a U-net containing our pre-trained encoder, for either geometry or albedo. For pre-training, we replace and train E θ in our model with U-net encoders. We also report accuracy when both the geometry and albedo encoders are jointly used in U-net by stacking the features used for each skip connection, and experiment with both fine-tuned and frozen encoders. All segmentation models are trained for 20 epochs. Results are reported in Table  3 .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Results:",
      "text": "We observe that pre-training the U-net encoder with our method yields improvements in segmentation, both for mIoU and pixel accuracy. When both geometry and albedo encoders are jointly used and kept frozen, our model still outperforms supervised U-net in mIoU, despite fewer trainable parameters.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Method",
      "text": "Cluster Accuracy F1-Score VQ-VAE (NeuRIPS '17)  [45]  0",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Shapenet Car Classification",
      "text": "Details: We utilize a subset of the ShapeNet car test data rendered by  [51]  to evaluate car classification. For each image, we extract the car name from the ShapeNet metadata. Since ShapeNet contains a diverse set of cars, we limit our test set to five classes, using keyword matching to gather labels. The five classes are police car, ambulance, limousine, jeep, and Ferrari. The final test set contains 1000 images, each rendered with random lighting and viewpoint.\n\nResults: We compare our proposed method with VQ-VAE. Despite the VQ-VAE model having a much larger latent space, our method yields higher clustering accuracy and F1. As with results on face data, we observe that the inclusion of the LOOCC loss significantly improves performance, with light and view augmentation yielding the best performance.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Discussion",
      "text": "Our proposed method introduces inverse rendering as a framework for representation learning, and shows the utility of physically disentangled representations on many downstream tasks. Although the majority of our benchmarked tasks are on face datasets, the introduced methods will scale across domains as inverse rendering continues to advance. While testing our model, we also discovered that the learned features are useful regardless of clustering method, and include results on FINCH  [33] , which discovers a hierarchy of partitions without being given the number of clusters, in the Appendix. Beyond improved accuracy on downstream tasks, we posit that our framework has two other main advantages over existing methods: improved interpretability and improved robustness to physical phenomena that are both modeled by the renderer and disentangled. Applications to Interpretability: Interpretability of deep learning models is a concern in many domains. While gradient-based attribution methods are a common way to gain high-level insight into model predictions, the resulting saliency maps often lack the granularity needed for sufficient model understanding. Such saliency maps highlight which pixels were most important for a model prediction. While this is helpful, it can be unclear why a certain pixel or group of pixels were important -color, geometry, texture, etc. could all be reasons why a pixel might have been important. Thus, we observe that incorporating physical models, such as renderers, into learning, as done in our proposed method, provides an avenue for improved interpretability. By leveraging existing gradientbased attribution methods  [34, 40] , we can determine how much the features of Fig.  4 : Interpretability of our model using Grad-CAM  [34]  on the features of each scene parameter, with % contribution of each feature computed as the normalized sum of absolute integrated gradients (IG)  [40]  for the feature:\n\n, where z j is the N -dimensional feature for scene parameter j. Blue indicates higher attribution.\n\neach physical scene parameter contributed to a prediction, and generate corresponding saliency maps for each feature, as shown in Figure  4 . As differentiable renderers become more realistic and are able to model more scene parameters, interpretability can continue to be improved when renderers are used in training.\n\nDisentanglement & Robustness We observe that our methods improve physical disentanglement and robustness. The learned features are predictive of scene parameters, which themselves are disentangled, as shown in  [51] , and can be rendered to form an image. We compute Pearson's correlation coefficient (PCC) between each combination of the four learned physical features, across multiple datasets, both with and without our LOOCC loss. Not only do results indicate low correlation between features, but also that LOOCC further reduces correlation. Without LOOCC, our method yields a mean PCC of 0.28 and 0.26 on the Buffy and BBT datasets, respectively, whereas, with LOOCC, mean PCC is 0.18 on both datasets. Disentanglement is also supported empirically by our model's robustness to novel lighting and camera views. Our method, which omits camera and light features for testing, yields the biggest improvement over other methods when evaluated on Buffy and BBT, which contain novel lighting and views. More work is needed to study whether including a renderer in training and disentangling physical features can sufficiently mitigate the susceptibility of deep learning models to misclassifications under novel lighting and views  [26, 27] .\n\nLimitations & Future Work: A limitation of our work is the inverse renderer used, which constrains us to symmetrical objects and does not account for specular effects. Future work can build on our method to show the utility of inverse rendering and physical disentanglement for perception of more complex scenes and larger images. In addition, our proposed LOOCC loss samples light and view only, but could sample geometry and albedo if these parameters were modeled probabilistically. Finally, by using a differentiable renderer that models other scene parameters, such as haze, atmospheric conditions, etc, robustness to a variety of physical phenomenon may be improved using our method.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "We present a framework for disentangling representations with regard to physical scene parameters, such as geometry, albedo, light, and camera view, using inverse rendering. We demonstrate improved performance over strong baselines with our learned features across both face and non-face images, and with clustering, linear classification, and segmentation tasks. We also introduce a novel objective called Leave-One-Out, Cycle Contrastive loss (LOOCC) that leads to improved downstream performance. LOOCC helps to disentangle scene parameters by contrasting images with their physically augmented counterparts, generated by a renderer. Finally, we discuss the implications of using differentiable rendering as part of representation learning on areas such as interpretability and robustness to physical phenomena, including novel lighting and camera views.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Acknowledgements",
      "text": "This research was supported by the SMART Contract IARPA Grant #2021-20111000004. The authors would also like to thank Shangzhe Wu and Ayush Chopra for valuable conversations related to this research.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview: (a) we aim to achieve physical disentanglement using a differentiable",
      "page": 2
    },
    {
      "caption": "Figure 2: Our proposed method. Encoders are pre-trained for each physical scene param-",
      "page": 5
    },
    {
      "caption": "Figure 1: Our method consists of phys-",
      "page": 6
    },
    {
      "caption": "Figure 2: and let f perturb the camera parameter.",
      "page": 6
    },
    {
      "caption": "Figure 2: where camera view augmentation is shown,",
      "page": 7
    },
    {
      "caption": "Figure 3: We demonstrate the utility of our method across a variety of tasks and chal-",
      "page": 8
    },
    {
      "caption": "Figure 4: Interpretability of our model using Grad-CAM [34] on the features of each",
      "page": 14
    },
    {
      "caption": "Figure 4: As differentiable",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Clustering results for face classification tasks. Despite having a smaller latent",
      "page": 9
    },
    {
      "caption": "Table 2: Linear classification accuracy for CelebA attribute classification with varying",
      "page": 11
    },
    {
      "caption": "Table 1: and Table 2, respectively. Across both forms of evaluation, our model outper-",
      "page": 11
    },
    {
      "caption": "Table 3: U-net face segmentation results on the CelebA Mask dataset containing",
      "page": 12
    },
    {
      "caption": "Table 3: Results: We observe that pre-training the U-net encoder with our method yields",
      "page": 12
    },
    {
      "caption": "Table 4: Clustering results for ShapeNet car classification. Images contains either a",
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Semi-supervised Learning with Constraints for Person Identification in Multimedia Data",
      "authors": [
        "M Bäuml",
        "M Tapaswi",
        "R Stiefelhagen"
      ],
      "year": "2013",
      "venue": "CVPR"
    },
    {
      "citation_id": "2",
      "title": "State-of-the-art in the architecture",
      "authors": [
        "A Bermano",
        "R Gal",
        "Y Alaluf",
        "R Mokady",
        "Y Nitzan",
        "O Tov",
        "O Patashnik",
        "D Cohen-Or"
      ],
      "year": "2022",
      "venue": "State-of-the-art in the architecture"
    },
    {
      "citation_id": "3",
      "title": "Understanding disentangling in β-vae",
      "authors": [
        "C Burgess",
        "I Higgins",
        "A Pal",
        "L Matthey",
        "N Watters",
        "G Desjardins",
        "A Lerchner"
      ],
      "year": "2018",
      "venue": "Understanding disentangling in β-vae",
      "arxiv": "arXiv:1804.03599"
    },
    {
      "citation_id": "4",
      "title": "Unsupervised disentanglement without autoencoding: Pitfalls and future directions",
      "authors": [
        "A Burns",
        "A Sarna",
        "D Krishnan",
        "A Maschinot"
      ],
      "year": "2021",
      "venue": "Unsupervised disentanglement without autoencoding: Pitfalls and future directions",
      "arxiv": "arXiv:2108.06613"
    },
    {
      "citation_id": "5",
      "title": "Shapenet: An information-rich 3d model repository",
      "authors": [
        "A Chang",
        "T Funkhouser",
        "L Guibas",
        "P Hanrahan",
        "Q Huang",
        "Z Li",
        "S Savarese",
        "M Savva",
        "S Song",
        "H Su"
      ],
      "year": "2015",
      "venue": "Shapenet: An information-rich 3d model repository",
      "arxiv": "arXiv:1512.03012"
    },
    {
      "citation_id": "6",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "7",
      "title": "Retrieve in style: Unsupervised facial feature transfer and retrieval",
      "authors": [
        "M Chong",
        "W Chu",
        "A Kumar",
        "D Forsyth"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "8",
      "title": "Unsupervised visual representation learning by context prediction",
      "authors": [
        "C Doersch",
        "A Gupta",
        "A Efros"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "9",
      "title": "Multi-task self-supervised visual learning",
      "authors": [
        "C Doersch",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "10",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "11",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "12",
      "title": "Pri3d: Can 3d priors help 2d representation learning",
      "authors": [
        "J Hou",
        "S Xie",
        "B Graham",
        "A Dai",
        "M Nießner"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "13",
      "title": "A style-based generator architecture for generative adversarial networks",
      "authors": [
        "T Karras",
        "S Laine",
        "T Aila"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "14",
      "title": "Analyzing and improving the image quality of stylegan",
      "authors": [
        "T Karras",
        "S Laine",
        "M Aittala",
        "J Hellsten",
        "J Lehtinen",
        "T Aila"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "15",
      "title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
      "authors": [
        "H Kato",
        "Y Ushiku",
        "T Harada"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "16",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "17",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "18",
      "title": "Semantic implicit neural scene representations with semi-supervised training",
      "authors": [
        "A Kohli",
        "V Sitzmann",
        "G Wetzstein"
      ],
      "year": "2020",
      "venue": "2020 International Conference on 3D Vision (3DV)"
    },
    {
      "citation_id": "19",
      "title": "Unsupervised representation learning by predicting image rotations",
      "authors": [
        "N Komodakis",
        "S Gidaris"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations (ICLR"
    },
    {
      "citation_id": "20",
      "title": "Deep convolutional inverse graphics network",
      "authors": [
        "T Kulkarni",
        "W Whitney",
        "P Kohli",
        "J Tenenbaum"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Maskgan: Towards diverse and interactive facial image manipulation",
      "authors": [
        "C Lee",
        "Z Liu",
        "L Wu",
        "P Luo"
      ],
      "year": "2020",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "22",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "23",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "24",
      "title": "Inverse rendering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from a single image",
      "authors": [
        "Z Li",
        "M Shafiei",
        "R Ramamoorthi",
        "K Sunkavalli",
        "M Chandraker"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Deep learning face attributes in the wild",
      "authors": [
        "Z Liu",
        "P Luo",
        "X Wang",
        "X Tang"
      ],
      "year": "2015",
      "venue": "Proceedings of International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "26",
      "title": "When and how do cnns generalize to out-of-distribution categoryviewpoint combinations",
      "authors": [
        "S Madan",
        "T Henry",
        "J Dozier",
        "H Ho",
        "N Bhandari",
        "T Sasaki",
        "F Durand",
        "H Pfister",
        "X Boix"
      ],
      "year": "2020",
      "venue": "When and how do cnns generalize to out-of-distribution categoryviewpoint combinations",
      "arxiv": "arXiv:2007.08032"
    },
    {
      "citation_id": "27",
      "title": "Small in-distribution changes in 3d perspective and lighting fool both cnns and transformers",
      "authors": [
        "S Madan",
        "T Sasaki",
        "T Li",
        "X Boix",
        "H Pfister"
      ],
      "year": "2021",
      "venue": "Small in-distribution changes in 3d perspective and lighting fool both cnns and transformers",
      "arxiv": "arXiv:2106.16198"
    },
    {
      "citation_id": "28",
      "title": "Disentangling disentanglement in variational autoencoders",
      "authors": [
        "E Mathieu",
        "T Rainforth",
        "N Siddharth",
        "Y Teh"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "29",
      "title": "Semi-supervised stylegan for disentanglement learning",
      "authors": [
        "W Nie",
        "T Karras",
        "A Garg",
        "S Debnath",
        "A Patney",
        "A Patel",
        "A Anandkumar"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "30",
      "title": "Unsupervised learning of visual representations by solving jigsaw puzzles",
      "authors": [
        "M Noroozi",
        "P Favaro"
      ],
      "year": "2016",
      "venue": "Unsupervised learning of visual representations by solving jigsaw puzzles"
    },
    {
      "citation_id": "31",
      "title": "Pytorch: An imperative style, highperformance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing tems"
    },
    {
      "citation_id": "32",
      "title": "U-net: Convolutional networks for biomedical image segmentation",
      "authors": [
        "O Ronneberger",
        "P Fischer",
        "T Brox"
      ],
      "year": "2015",
      "venue": "International Conference on Medical image computing and computer-assisted intervention"
    },
    {
      "citation_id": "33",
      "title": "Efficient parameter-free clustering using first neighbor relations",
      "authors": [
        "S Sarfraz",
        "V Sharma",
        "R Stiefelhagen"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "34",
      "title": "Gradcam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "Gradcam: Visual explanations from deep networks via gradient-based localization"
    },
    {
      "citation_id": "35",
      "title": "Neural inverse rendering of an indoor scene from a single image",
      "authors": [
        "S Sengupta",
        "J Gu",
        "K Kim",
        "G Liu",
        "D Jacobs",
        "J Kautz"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "36",
      "title": "A simple and effective technique for face clustering in tv series",
      "authors": [
        "V Sharma",
        "M Sarfraz",
        "R Stiefelhagen"
      ],
      "year": "2017",
      "venue": "CVPR: Brave New Motion Representations Workshop"
    },
    {
      "citation_id": "37",
      "title": "Self-supervised learning of face representations for video face clustering",
      "authors": [
        "V Sharma",
        "M Tapaswi",
        "M Sarfraz",
        "R Stiefelhagen"
      ],
      "year": "2019",
      "venue": "14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "38",
      "title": "Clustering based contrastive learning for improving face representations",
      "authors": [
        "V Sharma",
        "M Tapaswi",
        "M Sarfraz",
        "R Stiefelhagen"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "39",
      "title": "Scene representation networks: Continuous 3d-structure-aware neural scene representations. Advances in Neural Information Processing Systems",
      "authors": [
        "V Sitzmann",
        "M Zollhöfer",
        "G Wetzstein"
      ],
      "year": "2019",
      "venue": "Scene representation networks: Continuous 3d-structure-aware neural scene representations. Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "40",
      "title": "Axiomatic attribution for deep networks",
      "authors": [
        "M Sundararajan",
        "A Taly",
        "Q Yan"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "41",
      "title": "Probabilistic Person Identification in TV-Series",
      "authors": [
        "M Tapaswi",
        "M Bäuml",
        "R Stiefelhagen"
      ],
      "year": "2012",
      "venue": "Probabilistic Person Identification in TV-Series"
    },
    {
      "citation_id": "42",
      "title": "Total cluster: A person agnostic clustering method for broadcast videos",
      "authors": [
        "M Tapaswi",
        "O Parkhi",
        "E Rahtu",
        "E Sommerlade",
        "R Stiefelhagen",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Indian Conference on Computer Vision Graphics and Image Processing"
    },
    {
      "citation_id": "43",
      "title": "What makes for good views for contrastive learning?",
      "authors": [
        "Y Tian",
        "C Sun",
        "B Poole",
        "D Krishnan",
        "C Schmid",
        "P Isola"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "44",
      "title": "Designing an encoder for stylegan image manipulation",
      "authors": [
        "O Tov",
        "Y Alaluf",
        "Y Nitzan",
        "O Patashnik",
        "D Cohen-Or"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "45",
      "title": "Neural discrete representation learning",
      "authors": [
        "A Van Den Oord",
        "O Vinyals"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "46",
      "title": "Hierarchical grouping to optimize an objective function",
      "authors": [
        "J Ward"
      ],
      "year": "1963",
      "venue": "Journal of the American statistical association"
    },
    {
      "citation_id": "47",
      "title": "De-rendering 3d objects in the wild",
      "authors": [
        "F Wimbauer",
        "S Wu",
        "C Rupprecht"
      ],
      "year": "2022",
      "venue": "De-rendering 3d objects in the wild",
      "arxiv": "arXiv:2201.02279"
    },
    {
      "citation_id": "48",
      "title": "Learning to see physics via visual de-animation",
      "authors": [
        "J Wu",
        "E Lu",
        "P Kohli",
        "B Freeman",
        "J Tenenbaum"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "49",
      "title": "Neural scene de-rendering",
      "authors": [
        "J Wu",
        "J Tenenbaum",
        "P Kohli"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "50",
      "title": "De-rendering the world's revolutionary artefacts",
      "authors": [
        "S Wu",
        "A Makadia",
        "J Wu",
        "N Snavely",
        "R Tucker",
        "A Kanazawa"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "51",
      "title": "Unsupervised learning of probably symmetric deformable 3d objects from images in the wild",
      "authors": [
        "S Wu",
        "C Rupprecht",
        "A Vedaldi"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "52",
      "title": "Gan inversion: A survey",
      "authors": [
        "W Xia",
        "Y Zhang",
        "Y Yang",
        "J Xue",
        "B Zhou",
        "M Yang"
      ],
      "year": "2021",
      "venue": "Gan inversion: A survey",
      "arxiv": "arXiv:2101.05278"
    },
    {
      "citation_id": "53",
      "title": "Inverserendernet: Learning single image inverse rendering",
      "authors": [
        "Y Yu",
        "W Smith"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "54",
      "title": "Deep metric learning with improved triplet loss for face clustering in videos",
      "authors": [
        "S Zhang",
        "Y Gong",
        "J Wang"
      ],
      "year": "2016",
      "venue": "Pacific Rim Conference on Multimedia"
    },
    {
      "citation_id": "55",
      "title": "Joint face representation adaptation and clustering in videos",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2016",
      "venue": "Joint face representation adaptation and clustering in videos"
    },
    {
      "citation_id": "56",
      "title": "Age progression/regression by conditional adversarial autoencoder",
      "authors": [
        "Z Zhang",
        "Y Song",
        "H Qi"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    }
  ]
}