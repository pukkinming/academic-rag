{
  "paper_id": "2108.04605v1",
  "title": "A Novel Markovian Framework For Integrating Absolute And Relative Ordinal Emotion Information",
  "published": "2021-08-10T11:24:36Z",
  "authors": [
    "Jingyao Wu",
    "Ting Dang",
    "Vidhyasaharan Sethu",
    "Eliathamby Ambikairajah"
  ],
  "keywords": [
    "Speech emotion recognition",
    "emotion ranks",
    "emotion dynamics",
    "Markov model",
    "ordinal classification",
    "ordinal data",
    "preference learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Fig. 1. Two 1-second clips of video and speech recording with attached arousal labels from two utterances. (a) The corresponding annotated interval labels; (b) ROLs indexed at each time instant; and (c) AOLs denoted by different shades of blue indicating three levels: low, medium and high.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "PEECH is one of the most natural form of human communication and a key modality through with emotions are expressed. Consequently, speech emotion recognition (SER) has received increasing interest within the speech processing and Human-Computer Interaction research communities  [1] [2] [3] [4] . Within affective computing systems, emotions are typically represented using either categorical labels or dimensional labels  [4] [5] [6] . In the former case, emotions are represented as discrete categories using nominal labels (e.g., happy, sad, angry, etc.)  [4] [5] [6] ; and in the latter case, emotion are described using interval scales along affective dimensions such as arousal (activated vs deactivated) and valence (pleasant vs. unpleasant)  [7] [8] [9] . Additionally, the interval labels can also be used to assign scalar values for arousal and valence at each time step, leading to a continuous time series emotion labels  [10, 11] . Both nominal and interval labelling systems have been extensively utilised and investigated. However, the challenge of low inter-rater agreement, i.e. variability in labels reflecting variations in perception among different annotators, remains  [11, 12] . This in turn introduces ambiguity in the labels used to train SER systems and increases uncertainty in the predictions  [12] .\n\n'distance' between the elements. Additionally, research in psychology has shown that people are better at discriminating among options than they are at assigning definitive values for what they perceived  [13] . Consequently, several recent studies have advocated that ordinal labelling of emotions is better aligned with human perception, evidenced by lower inter-rater variability  [14] [15] [16] [17] [18] .\n\nWhile the idea of using ordinal labels is appealing, their use in affective computing systems is fairly recent and the term has been used to refer to two related but distinct types of labels in the emotion recognition literature. In this manuscript, we introduce the terms absolute ordinal labels (AOL) and relative ordinal labels (ROL) to distinguish between these two types of ordinal labels which describe different aspects of emotions. Typically AOLs are obtained by asking annotators to select an element from a finite ordinal scale (e.g., {low arousal, medium arousal, high arousal})  [19] , whereas ROLs are obtained by asking annotators to rank the segments within a speech segment from the lowest arousal (or valence) to the highest arousal (or valence)  [11] . As illustrated in Fig.  1 (c ), AOLs denoted by different shades of blue take values from an 'ordered class', with each label providing an absolute and easily interpretable indication of the affective dimensions at that time (e.g., low arousal). ROLs, denoted by the position of red dots in (b), indicate the rank of that segment with respect to all others within the utterance. These ranks retain the pairwise order relationship between all pairs of segments as inferred from the interval labels shown in (a). ROLs provide a relative label indicating the affect at each time step with respect to other time steps, encapsulating a notion of how emotion changes but lacking an indication of an absolute level.\n\nBoth AOLs and ROLs are ordinal but are not equivalent and convey complementary information. Consider two speech segments that are only assigned AOLs -both labelled 'low arousal', even if annotators can determine which of the two corresponds to a lower arousal, the labelling scheme does not encode this. On the other hand, if the two segments are assigned ROLs -for instance, 'A corresponds to lower aroused than B', clearly, there is no indication of the absolute level of arousal and this label cannot distinguish between a scenario where A corresponds to low arousal level and B corresponds to a medium arousal level (Clip 1 in Fig.  1 ), and one where A corresponds to medium arousal level and B to a high arousal level (Clip 2 in Fig.  1 ). Current literature in affective computing typically utilises one or another, with both scenarios referred to as 'ordinal labels'  [11, 20] .\n\nIn this paper, we propose a framework that integrates both the absolute and relative ordinal information. We assume that AOLs are more readily interpretable and consequently the predictions of an emotion recognition system should be absolute ordinal quantities. Simultaneously we also recognise that ROLs are better aligned with the types of judgements humans are better at making, and consequently ROL should inform the training and outputs of the emotion prediction system. In the proposed framework, the AOLs are represented as states of a finite state Markov model, with the state transition prob-abilities predicted by a system trained using ROLs. The overall model, which we refer to as the dynamic ordinal Markov model (DOMM), thus incorporates both information about the emotion at a given time (state probabilities) as well as the change in emotion at a given time (state transition probabilities).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ordinal Emotion Prediction Systems",
      "text": "In recent years, there has been growing interest in ordinal regression techniques for affective computing and these approaches have shown some advantages in utilising and modelling the ordinal nature of the labels  [11] . In this section we summarise recent research on developing both relative and absolute ordinal emotion prediction systems. It should be noted that the terms 'absolute ordinal' and 'relative ordinal' are not used in the literature and the distinction is introduced in this paper.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Relative Ordinal Prediction System",
      "text": "Preference learning (PL), a popular framework for retrieval tasks, refers to a set of ordinal regression models that has become an appealing approach in affective computing. It ranks the samples in ascending/descending order of an emotional attribute. Cao et.al  [21]  describe the use of rankers for categorical emotion representation with different rankers trained for each emotion category. For instance, a 'happy' ranker trained to treat sample labelled 'happy' with a higher preference than a sample labelled 'sad'. For ranking along emotional attributes or dimensions such as arousal and valence, RankSVMs (a popular PL framework) have been adopted in a number of studies  [20, [22] [23] [24] [25] . Deep neural network (DNN) based emotional PL algorithms such as RankNet  [20]  and RBF-ListNet  [23]  have also drawn increasing attention within the field.\n\nApart from preference learning methods, ordinal regression tasks have also been framed as binary classification problems, by decomposing the ordinal variables into an ensemble of pairs of classes in a staircase-like manner, leading to the Ordinal Binary Decomposition Approach  [26] . Finally, naÃ¯ve approaches which involve treating ordinal labels as interval or nominal labels (and ignoring their ordinal nature) have also been investigated. For instance, a conventional regression model can be trained by treating the relative ordinal labels [ğ‘Ÿ 1 , â€¦ , ğ‘Ÿ ğ‘ ] as ğ‘ different interval values [1, 2, â€¦ , ğ‘›, â€¦ , ğ‘] (e.g., assign the rank order as real numerical values, ğ‘Ÿ ğ‘› = ğ‘› )  [26, 27] . However, as stated in  [26, 28]  the regressor will be more sensitive to the converted values rather than the original rank order, which might degrade the performance of the regression models.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Absolute Ordinal Prediction System",
      "text": "AOLs have been adopted widely in current emotion recognition studies, but they are generally treated as nominal labels and the underlying ordinality is ignored. For instance, in a few different studies  [29] [30] [31] [32] , affect labels in terms of low, medium and high arousal are utilised, but naÃ¯ve nominal classifiers, such as support vector machine (SVM), are employed  [30] . An alternative approach is the threshold method that assumes a continuous latent variable represents the emotional state, with appropriate thresholds dividing the range of the latent variable into a few ordered ranges corresponding to absolute ordinal labels. The prediction model then learns a mapping function between the input features and the continuous hidden latent variable, which is then converted to an absolute ordinal label. Several modelling methods can be utilised in this approach, such as cumulative link models  [33] , and proportional odds models  [34] .\n\nAmongst truly ordinal prediction systems, the Ordinal Multi-class SVM (OMSVM) which was first proposed by Kim and Ahn  [35]  for credit ratings and has since been widely used in the area of financial and market analysis  [36, 37] , appears to be a very promising candidate. Additionally, a suitable ordinal binary decomposition approach can also be employed to model AOLs. Compared to the naÃ¯ve approach and threshold methods, the binary decomposition method is more suitable since it can capture ordinal information without defining distances between ordered classes. Deep learning may also be a valid solutions to ordinal binary decomposition, but it can suffer with generalisation, especially for small dataset  [35, 38] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Proposed Dynamic Ordinal Markov Model",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Combining Absolute And Relative Ordinal Information",
      "text": "The This idea is illustrated in Fig.  2 , with the position of the blue circles indicating the AOL with ğ´ 0 , ğ´ 1 , and ğ´ 2 denoting low (L), medium (M) and high (H) arousal states respectively; and the values within the circles denoting the ROL (rank within a set of 100 time steps). The number on each arrow given by Î” shows the change in ROL between two consecutive instances. It is clear that the value of Î” carries information about the dynamics of the AOL, i.e., any change in ğ´ ğ‘– . For example, the chances of erroneously predicting the AOL at time ğ‘¡ 4 as low or medium instead of high would be greatly reduced if the automatic system took into account that over the ğ‘¡ 3â†’4 transition, ROL change from rank 48 to 87 (i.e., âˆ† = 39).\n\nAn overview of the proposed Dynamic Ordinal Markov Model (DOMM) based emotion prediction system is shown in Fig.  3 . It comprises two subsystems which operate in parallel, one to predict AOLs and one to predict ROLs from the input features. The AOL prediction subsystem is designed to estimate state posteriors, ğ‘ƒ(ğ›½ ğ‘¡ |ğ’™ ğ‘¡ ), given the input features, ğ’™ ğ‘¡ , at each time step ğ‘¡. The possible states of the model, ğ›½, correspond to the finite set of possible AOLs. In all the systems presented in this paper, there are possible states -'low', 'medium' and 'high' for arousal or valence.\n\nThe output of the ROL prediction subsystem is a rank, ğ›¼ ğ‘¡ , at each time step ğ‘¡; from which state transition probabilities, ğ‘ƒ(ğ›½ ğ‘¡ |ğ›½ ğ‘¡-1 , Î”ğ›¼ ğ‘¡ ) , are inferred based on the rank difference between consecutive time steps, Î”ğ›¼ ğ‘¡ = ğ›¼ ğ‘¡ -ğ›¼ ğ‘¡-1 as per Bayes theorem:\n\nwhere ğ‘ƒ(âˆ†ğ›¼ ğ‘¡ | ğ›½ ğ‘¡-1 , ğ›½ ğ‘¡ ) represents the probability of rank (ROL) difference âˆ†ğ›¼ ğ‘¡ given the state (AOL) at previous time step, ğ›½ ğ‘¡-1 , and the current state, ğ›½ ğ‘¡ ; ğ‘ƒ(âˆ†ğ›¼ ğ‘¡ | ğ›½ ğ‘¡-1 ) denotes the probability of a change in rank (ROL) of âˆ†ğ›¼ ğ‘¡ occurring given previous state was ğ›½ ğ‘¡-1 ; and ğ‘ƒ(ğ›½ ğ‘¡ | ğ›½ ğ‘¡-1 ) denotes the probability of transitioning from state ğ›½ ğ‘¡-1 to state ğ›½ ğ‘¡ . Models for all three probabilities on the righthand side of (1) can be inferred from labelled training data.\n\nTo model ğ‘ƒ(âˆ†ğ›¼ ğ‘¡ | ğ›½ ğ‘¡-1 ), first all instances of ROL pairs [ ğ›¼ ğ‘¡ , ğ›¼ ğ‘¡-1 ] with initial state ğ›½ ğ‘¡-1 = ğ¿ from the training data are selected and the corresponding set of Î”ğ›¼ ğ‘¡ values are computed. Kernel density estimation (KDE)  [40, 41]   Models for ğ‘ƒ(Î”ğ›¼ ğ‘¡ |ğ›½ ğ‘¡-1 , ğ›½ ğ‘¡ ) can also be obtained by the same approach, by partitioning the dataset into 9 subsets corresponding to all combinations of ğ›½ ğ‘¡-1 , ğ›½ ğ‘¡ âˆˆ {ğ¿, ğ‘€, ğ»} Ã— {ğ¿, ğ‘€, ğ»} , following by KDE. An example is illustrated in Fig.  4   Lastly, the probability ğ‘ƒ( ğ›½ ğ‘¡ | ğ›½ ğ‘¡-1 ) is calculated as:\n\nwhere, ğ‘–, ğ‘— âˆˆ {ğ¿, ğ‘€, ğ»}; ğ‘ ğ‘– denotes the number of instances of state (AOL) ğ‘– in the training data set; and ğ‘ ğ‘–â†’ğ‘— denotes the number of instances of state ğ‘– followed by state ğ‘— in the training set.\n\nTogether, the state posteriors, ğ‘ƒ(ğ›½ ğ‘¡ |ğ’™ ğ‘¡ ), and the state transition probabilities, ğ‘ƒ(ğ›½ ğ‘¡ |ğ›½ ğ‘¡-1 , Î”ğ›¼ ğ‘¡ ) , describe a 'dynamic' ordinal Markov model (DOMM). Here the term 'ordinal' refers to the fact that the model represents ordinal labels and the term 'dynamic' refers to the fact that both the state posterior probabilities and the state transition probabilities are time-varying, with both dependent on the input features, ğ’™ ğ‘¡ , at time ğ‘¡. Note that Î”ğ›¼ ğ‘¡ is estimated from ğ’™ ğ‘¡ by the ROL prediction subsystem and hence the state transition probabilities are also dynamically inferred from the input features. Given a sequence of input features, {ğ’™ ğ‘¡ ; ğ‘¡ 1 â‰¤ ğ‘¡ â‰¤ ğ‘¡ ğ‘ }, the DOMM generates a state lattice and the most probable sequence of states (ab-  The projection of points on the vector indicates ranks  [45] . For instance, for hyperplane w âƒ—âƒ—âƒ— 1 , the rank is 1â‰»2â‰»3â‰»4, while for w âƒ—âƒ—âƒ— 2 the rank is 2â‰»3â‰»1â‰»4. solute ordinal labels) can be inferred using the Viterbi algorithm  [42] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Subsystem Descriptions",
      "text": "Any appropriate machine learning model can be used as the AOL and ROL prediction subsystems in the proposed framework. In the experiments reported in this paper, we use an Ordinal Multiclass Support Vector Machine (OMSVM) as the AOL prediction subsystem and a RankSVM based model as the ROL prediction subsystem. They have both been shown to be robust to overfitting and their mathematical framework is well understood  [43, 44] . Furthermore, both explicitly model the inherent ordinal nature of the labels  [35, 45] . An overview of both subsystems is provided below.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Aol Prediction Subsystem: Omsvm",
      "text": "OMSVM  [35, 38]  is a variation of the conventional Multiclass SVM (MSVM) that makes use of the additional structure imparted to the data by the ordinal nature of the labels  [46] [47] [48] [49] . Instead of treating the ordinal labels as independent classes, as would be the case with conventional MSVM systems, OMSVM first applies an ordinal pairwise partitioning (OPP) technique that divide the ordinal labels into groups as OneVsNext: O 1 &O 2 , O 2 &O 3 â€¦, and then uses a series of SVMs for each group. During the prediction (test) phase, the test data is passing through the SVM classifiers in order, either along the forward or backward direction, until a class is predicted, at which point the remaining SVMs will not be used. To convert the outputs of OMSVM into state posteriors, we fit a sigmoid function to the outputs as suggested in  [50] :\n\nwhere ğ‘¦ denotes the SVM output; and ğ‘ and ğ‘ refer to sigmoid function parameters which are determined during training as outlined in  [50] . For further details, readers are referred to  [46, 50] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Rol Prediction Subsystem: Ranksvm",
      "text": "Conventional SVM classifiers aims to find a hyperplane ğ‘¤ âƒ—âƒ— that maximally separates two classes, and RankSVMs  [45]  extend this idea to identify a hyperplane that performs so-called preference comparisons, such that projections of the data on that hyperplane still preserve the original rank orders. This idea is illustrated in Fig.  5 , where it can be seen that the optimal hyperplane would be ğ‘¤ 1 (instead of ğ‘¤ 2 ) since the projections onto the hyperplane corresponding to ranks (ROLs) 1 â‰» 2 â‰» 3 â‰» 4 along the hyperplane (where 'â‰»' denotes 'preference') still preserve its order.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Database Description",
      "text": "None of the publicly available speech corpora come with absolute and relative ordinal labels. Consequently, in this work we use the well-established RECOLA  [51]  and the IEMOCAP  [52]  databases and convert the interval labels to AOL and ROL.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Recola",
      "text": "The Remote Collaborative and Affective Interactions (RECOLA) dataset  [51]  is a widely used multimodal corpus containing both audio and video modalities. It consists of 23 dyadic interactions from 46 participants including 27 females and 19 males. The data used in the experiments reported in this paper corresponds to that used in the AVEC challenge 2016  [53, 54] , which consists of 9 utterances each in the training and development sets (with no overlap) with each utterance having a duration of 5 minutes. Results on the development set are reported as test set labels have not been publicly released. Each utterance is also annotated by 6 raters with continuous arousal and valence ratings between -1 to 1 at a sampling interval of 40ms. Delay compensation is applied to compensate the human perception delays in the labels as suggested in  [55]   with 4 seconds for arousal and 2 seconds for valence. Additionally, the labels are smoothed by averaging within 1sec windows, with 50% overlap between windows. This is in line with the suggested use of 1-3 second windows to capture the trend in the interval labels  [56] . In total, each utterance (5 minute duration) comprised of 615 and 617 windows for arousal and valence.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Interval To Aol Conversion:",
      "text": "Absolute ordinal labels can be obtained by thresholding the interval labels into three ordinal levels -low, medium, and high. However, choosing these thresholds are a challenge and no commonly agreed upon thresholds exist  [29, 57] . Therefore, a range of potentially suitable thresholds were analysed. Additionally, the conversion is carried out individually for each annotator and the final consensus AOL is inferred via majority vote among the 6 individual AOLs (one per annotator).\n\nGiven the label value ğ‘¦ Ì… ğ‘¡ representing the average arousal/valence intensity within each window and thresholds ğœƒ 1 and ğœƒ 2 , the AOL is taken as Low, if ğ‘¦ Ì… ğ‘¡ â‰¤ ğœƒ 1 ; Medium, if ğœƒ 1 < ğ‘¦ Ì… ğ‘¡ â‰¤ ğœƒ 2 ; and High, if ğ‘¦ Ì… ğ‘¡ > ğœƒ 2 . To determine suitable values for ğœƒ 1 and ğœƒ 2 , measures of label balance and inter-rater agreement after conversion are analysed. Interrater agreement represents consensus level among the converted AOL across multiple annotators. It is quantified as the ratio of the number of frames where more than half of the raters agree on the same AOL. A reasonably high ratio means a higher inter-rater agreement.\n\nThere will be a trade-off between the label balance and level of agreement. For instance, thresholding all annotations into a single AOL leads to perfect agreement but also a highly skewed and uninformative label distribution. Whereas, equally spacing the thresholds is likely to lead to less distinguishable labels that are unlikely to be optimal except in the case of uniform distribution of interval labels, at unrealistic expectation -extreme affect labels are less frequent.\n\nFinally, as arousal annotations were observed to be reasonably symmetrically distributed over [-1, 1], the thresholds ğœƒ ğ‘1 and ğœƒ ğ‘2 for arousal were also set to be symmetric, with ğœƒ ğ‘2 = -ğœƒ ğ‘1 . A range of different values of ğœƒ ğ‘2 , within [0.08, 0.2] with a step size 0.02, was analysed. This region was selected based on the observation that the median arousal value from every annotator fell in the range [-0.067, 0.118] and we wanted to ensure that the 'medium' label always covers the median values. Both the label balance measure and the inter-rater agreement for the chosen range of thresholds are shown in Fig.  6(a) .\n\nIt was also observed that valence labels in RECOLA were skewed towards positive values and consequently the two thresholds, ğœƒ ğ‘£1 and ğœƒ ğ‘£2 , were assumed to be symmetrical about 0.085 and instead analysed over the range ğœƒ ğ‘£2 âˆˆ [0.13, 0.21] with ğœƒ ğ‘£1 = 0.17 -ğœƒ ğ‘£2 . These ranges were also chosen so as to ensure the median label value from each annotator was covered by the 'medium' label.\n\nThe label balance measure and inter-rater agreement variations across the range of ğœƒ ğ‘£2 is depicted in Fig.  6(b ). In Fig.  6 , the. x-axis denotes the upper threshold ğœƒ ğ‘2 and ğœƒ ğ‘£2 values, the left y-axis denotes the label balance and right y-axis denotes the inter-rater agreement levels. Label balance is plotted as the average ğ›¾ across all annotators (i.e., ğ›¾Ì… ) and represented by a blue dashed line, and the inter-rater agreement level is indicated by the solid red line.\n\nIt can be seen from Fig.  6 (a) that ğ›¾Ì… tends to converge to the minima within the range ğœƒ ğ‘2 âˆˆ [0.14, 0.18], suggesting this would lead to the most balanced label distribution. The agreement level within this range is between 0.54 to 0.6 which is relatively high as well. It should also be noted that the higher agreement level outside this range for ğœƒ ğ‘2 mostly likely arises from imbalanced labels -i.e., a large number of instances are assigned to one label. Similar finding were observed for valence, and threshold val-  ues ranging around ğœƒ ğ‘£2 âˆˆ [0.16, 0.18] were found to be suitable. Finally, a different set of AOLs for different thresholds within the range ğœƒ ğ‘2 âˆˆ [0.12, 0.18] for arousal; and ğœƒ ğ‘£2 âˆˆ [0.14, 0.18] for valence with step size 0.01 were inferred and all experiments were repeated using each set and the mean and standard deviation across them are reported in this paper.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Interval To Rol Conversion",
      "text": "ROLs are similarly inferred from smoothed interval labels, obtained by first windowing the interval labels with 1 sec windows (with 50% overlap) and computing the mean within each window. A rank sequence of ROLs is then obtained by performing pairwise comparisons for each individual annotator, and global ROLs were inferred using the Qualitative Agreement (QA) method across all annotators  [58] . Within each utterance, a matrix of pairwise comparisons amongst all windows for each individual annotator is first collected as shown in Fig.  7 . A consensus matrix is then obtained via majority vote among matrices from all annotators and the final rank sequence of ROLs is obtained from this consensus matrix.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iemocap",
      "text": "The Interactive Emotional dyadic MOtion CAPture database (IEMOCAP)  [52]  contains 12 hours of audio-visual recordings of 5 dyadic sessions from 5 pairs of actors. Both scripted dialogs and improvised dialogs are collected, and each dialog is segmented into speaker turns. In total the corpus contains 10,039 turns with an average duration of 4.5 s. IEMOCAP is annotated at a turn level that each turn is attached with both a categorical emotion label (i.e., happiness, anger, sadness, neutral state and frustration) and primitive based interval annotations (valence, activation, and dominance) on a 5-point scale (i.e., 1 -low/negative, 5 -high/positive). However, only the interval labels along arousal and valence within the range  [1, 5]  are considered in this work and the experiments on IEMOCAP are carried out at the turn-level with the same leave-one-speaker-out cross validation as in  [29] .\n\nA number of different conversion schemes has been employed to convert interval labels in IEMOCAP to AOLs, but they lack a consistent set of thresholds  [29, 30, [59] [60] [61] . Therefore, we adopted the idea of clustering the labels in the dataset to identify suitable decision thresholds  [59, 61] .\n\nUsing K-means clustering, this led to the following thresholds for arousal: ğ¿ğ‘œğ‘¤ âˆˆ [1, 2.5), ğ‘€ğ‘’ğ‘‘ğ‘–ğ‘¢ğ‘š âˆˆ [2.5, 3.5), and ğ»ğ‘–ğ‘”â„ âˆˆ  [3.5, 5] ; and these for valence: ğ¿ğ‘œğ‘¤ âˆˆ [1, 2.8), ğ‘€ğ‘’ğ‘‘ğ‘–ğ‘¢ğ‘š âˆˆ [2.8, 4), and ğ»ğ‘–ğ‘”â„ âˆˆ  [4, 5] . Interval to ROL conversion in the IEMOCAP data was carried out in a similar manner to that in RECOLA, but based on pairwise comparisons between speaker turns instead of 1 second windows  [20, 22] .\n\nThe label conversions threshold adopted for both the IEMOCAP and RECOLA datasets are summarized in Table 1 and Table  2  shows the label distributions obtained after conversion for the training and test sets. Specifically, all the experimental results reported in this paper using the IEMOCAP database, the experiments are carried outwith leave-one-speaker-out cross validation resulting in 10-fold cross validation. Consequently, the mean and standard deviation of number of turns used in training and testing set across the 10 folds are reported in Table  2 . Simiarly, all experiments on the RECOLA database, was conducted for different threshold settings, which results in different numbers for each class. The mean and standard deviation of samples across all thresholds is reported.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiment Settings",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Features",
      "text": "The 88-dimensioanl extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS), a relatively standard feature set used in affective computing to simplify benchmarking, was adopted for all the experiments reported in this paper  [62, 63] . The feature set comprises of arithmetic mean and coefficient of variation functionals applied to 18 low-level descriptors (LLDs) extracted from the minimalistic acoustic parameter set along with another 8 functionals applied to pitch and loudness. Additional 7 LLDs are extracted from the extension parameter set with 4 statistics over the unvoiced segments, 6 temporal features, and 26 additional cepstral parameters and dynamic parameter  [62] . The features were extracted using the OpenSMILE toolkit  [64]  and for additional details about eGeMAPS, readers are referred to  [62] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "System Parameters Settings",
      "text": "As previously mentioned, the AOL prediction subsystem in the proposed framework was implemented as an ordinal multiclass SVM (OMSVM) in the reported experiments. This OMSVM implementation used the Classifica-tionECOC MATLAB toolbox (an error correction output code multi-class classifier)  [65] . The state posterior probabilities were then computed using the FitPosterior function  [50] .\n\nThe RankSVM model used in the ROL prediction subsystem was implemented using the toolkit referred to in  [66] . It is trained using primal Newton method, which is known to be fast,  [67, 68]  and the maximum Newton step was set to be 20 as in  [66] . Finally, both the OMSVM and RankSVM models utilised linear kernels and both used ğ‘ = 1 Ã— 10 -4 as suggested in  [30, 69] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Aol Prediction Evaluation Metrics",
      "text": "Unweighted Average Recall (UAR) is a standard metric used to quantify performance in nominal classification tasks  [29, 30] , and has also been utilised to report 'classification accuracy' even predicting AOLs  [35] . We report UAR as one measure to allow comparison with existing literature. However, we also note that UAR does not take into account ordinality in the labels. For instance, incorrectly predicting 'Low arousal' as 'Medium arousal' or 'High arousal' both carry the same penalty although obviously the latter is a bigger error. Therefore, to take the ordinal nature of AOLs into consideration, we also report the weighted Cohen's Kappa (WK) coefficient, ğ‘˜ ğ‘¤ , which is used to measure the consistency between two AOL sequences. The coefficient, ğ‘˜ ğ‘¤ , indicates the level of agreement between two different label sequences (predictions Vs ground truth) as given by (  4 ), with ğ‘˜ ğ‘¤ = 1 indicating perfect agreement and ğ‘˜ ğ‘¤ = 0 indicating only chance agreement  [70] . ğ‘— of the predictions with ğ‘› .ğ‘— referred to the number of AOLs equal to ğ‘—. N denotes the total number of AOLs and ğ‘¤ ğ‘–ğ‘— is the element of ğ‘– ğ‘¡â„ row and ğ‘— ğ‘¡â„ column of matrix ğ‘Š defined in (5).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Rol Prediction Evaluation Metrics",
      "text": "To quantify the performance of the ROL prediction subsystem we adopt measure called precision at k (P@k), which is widely used in the rank-based retrieval tasks and in preference learning tasks  [20, 22] . It is a measurement of precision in retrieving top or bottom k% of the samples. It is calculated by first dividing ground truth labels into high and low groups following the rule that ROLs larger than median is collected in the high group and vice versa as shown in top section of Fig.  8 . The prediction is counted as a success if the top ğ‘˜% of the predictions belong to high group and the bottom ğ‘˜% samples of the predictions are retrieved from the low group as depicted in Fig.  8 . The other commonly used metric for quantifying mismatch between two ranks is Kendell's Tau  [71] . It ranges from -1 to 1 indicating the range from completely antithesis to perfect matched, with 0 indicating the two ranks are independent  [72] . Kendall's Tau, ğœ, is given as:\n\nwhere ğ‘‡ refers to total number of comparisons given by ğ‘‡ = ğ‘›(ğ‘›-1)",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "2",
      "text": ", with n referring to the highest rank index.\n\nC denotes the number of concordant pairs and D denotes the number of discordant pairs. It should be noted that both P@k and ğœ are used to evaluate a single sequence of ROLs. In the reported experiments, P@k and ğœ are first calculated for each utterance and the mean over all the utterances in the test set is then reported. Prior to testing the proposed DOMM based prediction system, we quantify the performance of the AOL and ROL prediction susbsystems individually based on the metrics outlined in the previous section. Experimental results on the IEMOCAP database are reported in terms of the mean and standard deviation computed across each fold of the 10-fold cross validation as discussed in section 4. Results on the RECOLA database are reported in terms of means and standard deviations of the performance metrics determined on the different AOL sets obtained using the identified range of thresholds (ğœƒ ğ‘2 and ğœƒ ğ‘£2 ) for the interval label to AOL conversions (refer section 4.1).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Omsvm Performance",
      "text": "The performance of the OMSVM based AOL prediction subsystem evaluated on both IEMOCAP and REOCLA in terms of UAR are reported in Table  3 . On the IEMOCAP database, the OMSVM system is compared to an HMM based baseline  [29] . While the UAR of the OMSVM system is lower than that of the baseline, it is reasonably high and significantly higher than chance. Consequently, we expect the OMSVM system to constitute a reasonable AOL prediction subsystem to evaluate the proposed framework. Finally, it should be noted that the baseline system results were reported with a different AOL conversion scheme.\n\nNo suitable baseline systems that allow for a direct comparison on the RECOLA database could be identified since most published literature using RECOLA deals with interval label prediction. The closest reported approaches focus on binary classification tasks of Low/High arousal/valence  [31, 32] . Thus, the binary classification  [31]  was adopted as a reference system (refer Table  3 ). The performance of the OMSVM subsystem is lower than that of reference system, but the OMSVM system's target is one of three AOLs while the reference system is solving a twoclass problem.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ranksvm Perforamce",
      "text": "The performance of the RankSVM based ROL prediction subsystem is quantified in terms of both Kendall's Tau and P@k. Fig.  9  shows P@k for k = 10%, 20%, 30%, 40% and 50%. The solid lines correspond to arousal and dashed lines to valence with the red and blue colours representing performance on IEMOCAP and RECOLA respectively. On the IEMOCAP dataset, the RankSVM system achieves a P@k of around 95% and 67% for arousal and valence at k=10% which is better than the 85% and 63% reported in  [20] .\n\nAdditionally, recognising that P@k does not reflect the exact ranking order within the samples but only indicates the performance of retrieving top and bottom k% of samples, we also report Kendall's Tau in Table  4 . The performance of the RankSVM system for valence prediction is uniformly poorer than arousal rank prediction and while this is consistent with reported results on interval labels as well when using audio inputs  [54, 73] , the Kendall's Tau value for valence rank prediction in IEMOCAP is close to chance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Evaluating The Proposed Domm Framework",
      "text": "The DOMM framework was developed to integrate complementary information from independent AOL and ROL prediction subsystems. Noting that the proposed approach ultimately predicts AOLs, UAR and weighted Cohen's kappa (ğ‘˜ ğ‘¤ ) are employed as performance metrics. Also recall that UAR does not take the ordinal nature of the labels into account, while ğ‘˜ ğ‘¤ does. The proposed system, denoted as DOMM_RS, is compared to the basic OMSVM system that does not make use of any relative ordinal information. In addition, we also report the per-Fig.  9 . P@k on RECOLA and IEMOCAP at k values: P@10, P@20, P@30, P@40 and P@50.    5 . It can be seen from these results that consistently, across both databases and across both arousal and valence, the DOMM_RS outperforms the OMSVM system and is in turn outperformed by the DOMM_GT system. The consistent trends strongly support the idea that absolute and relative ordinal labels contain complementary information and jointly modelling them has clear benefits. The performance discrepancy between DOMM_RS and DOMM_GT can be reasonably explained by the performance of the RankSVM based ROL prediction subsystem reported in Table  4 . Additionally, it is worth noting that arousal AOLs are somewhat unbalanced for IEMOCAP as can be seen from Table  2 . The relatively lower number of high and low arousal state could have the effect of increasing the transition probability into the medium arousal state and consequently a higher chance of incorrectly predicting medium  [29] . Table  5  also includes the reported performance of the HMM baseline  [29] , however, as previously mentioned this performance was estimated with different AOLs and data partitions.\n\nFinally, in addition to the summary results in Table  5 , we also provide comparisons between the three systems for each individual arousal and valence thresholds (ğœƒ ğ‘2 and ğœƒ ğ‘£2 ) employed in our interval label to AOL conversion in Fig.  10  and 11 . The red bar on the boxplots indicates the median values across different thresholds and each dot represents performance measured with each threshold. The dashed lines in the boxplots connect dots corresponding to the same threshold and is provided for ease of comparison of systems at the same threshold. It can be seen that the trends observable in Table  5  and even more evident with the additional detail in these figures and specifically DOMM_GT outperforms DOMM_RS, which in turn outperforms OMSVM across every all measures and when using any of the thresholds for interval to AOL conversion.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In the growing body of literature on ordinal emotion prediction, the term \"ordinal emotion labels\" has been used to refer to two related but different notions. In this paper, we explicitly distinguish between them and introduce the terminology absolute ordinal label (AOL) and relative ordinal label (ROL), conveying the different aspect of ordinality they emobody. Followig this, we establish that the complementary nature of the information encoded in these two different ordinal labelling schemes can be integrated in a joint model of emotion dynamics. Specifically, this novel framework, referred to as the dynamic ordinal Markov model (DOMM), integrates time varying absolute and relative ordinal information.\n\nA realizable and computationally inexpensive, speechbased emotion prediction based on the proposed framework, making use of OMSVM and RankSVM based subsystems for AOL and ROL predictions was implemented to validate the proposed framework. The emotion prediction results obtained across a range of different configurations clearly indicates that integrating relative ordinal information can significantly improve absolute ordinal emotion prediction. The novel system is compared to both, a baseline OMSVM system that only models AOLs; and a version of the novel system that uses oracle ROLs (instead of predicted ROLs) to serve as an indication of the upper bound on the predictive capabilities of the dynamic original Markov model. These experimental comparisons were carried out on both IEMOCAP and RECOLA datasets, and in terms of both UAR and weigthed Kappa. Consistently, in all comparisons, the joint model outperformed AOL only prediction. Finally, the comparison with the version using oracle ROLs suggest that improving the RankSVM based subsystem can lead to further gains in prediction accuracy.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Two 1-second clips of video and speech recording with",
      "page": 1
    },
    {
      "caption": "Figure 1: (c), AOLs denoted by different shades of blue take val-",
      "page": 2
    },
    {
      "caption": "Figure 1: ), and one where A corre-",
      "page": 2
    },
    {
      "caption": "Figure 1: ). Current literature in affective com-",
      "page": 2
    },
    {
      "caption": "Figure 2: A graphical representation of the complementary nature of",
      "page": 3
    },
    {
      "caption": "Figure 2: , with the position of the",
      "page": 3
    },
    {
      "caption": "Figure 3: It comprises two subsystems which oper-",
      "page": 3
    },
    {
      "caption": "Figure 4: with: Fig. 4(a) showing the histogram",
      "page": 3
    },
    {
      "caption": "Figure 4: (b) showing the histogram for high-low (H-",
      "page": 3
    },
    {
      "caption": "Figure 4: (c) showing the models for",
      "page": 4
    },
    {
      "caption": "Figure 3: Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-",
      "page": 4
    },
    {
      "caption": "Figure 5: Points in feature space are ranked by two weight vectors.",
      "page": 5
    },
    {
      "caption": "Figure 5: , where it can",
      "page": 5
    },
    {
      "caption": "Figure 4: Example of histograms and PDF distributions of different",
      "page": 5
    },
    {
      "caption": "Figure 6: , the. x-axis denotes the upper threshold ğœƒğ‘2",
      "page": 6
    },
    {
      "caption": "Figure 6: (a) that ğ›¾Ì… tends to converge to",
      "page": 6
    },
    {
      "caption": "Figure 6: Key characteristics (label balence & agreement) of con-",
      "page": 6
    },
    {
      "caption": "Figure 7: Illustration of QA method [58]. (a) Individual comparison",
      "page": 7
    },
    {
      "caption": "Figure 8: The prediction is count-",
      "page": 8
    },
    {
      "caption": "Figure 8: The other commonly used metric for quantifying mis-",
      "page": 8
    },
    {
      "caption": "Figure 8: A graphical representation of P@k. The test labels are",
      "page": 8
    },
    {
      "caption": "Figure 9: shows P@k for k = 10%, 20%, 30%, 40%",
      "page": 9
    },
    {
      "caption": "Figure 9: P@k on RECOLA and IEMOCAP at k values: P@10, P@20",
      "page": 9
    },
    {
      "caption": "Figure 10: and 11. The red bar on the boxplots indi-",
      "page": 10
    },
    {
      "caption": "Figure 10: Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on arousal of RECOLA.",
      "page": 10
    },
    {
      "caption": "Figure 11: Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Jingyao Wu, Student Member, IEEE, Ting Dang, Member, IEEE, Vidhyasaharan Sethu, Member,": "IEEE, and Eliathamby Ambikairajah, Senior Member, IEEE"
        },
        {
          "Jingyao Wu, Student Member, IEEE, Ting Dang, Member, IEEE, Vidhyasaharan Sethu, Member,": "Abstractâ€”There is growing interest in affective computing for the representation and prediction of emotions along ordinal"
        },
        {
          "Jingyao Wu, Student Member, IEEE, Ting Dang, Member, IEEE, Vidhyasaharan Sethu, Member,": "scales. However, the term ordinal emotion label has been used to refer to both absolute notions such as low or high arousal, as"
        },
        {
          "Jingyao Wu, Student Member, IEEE, Ting Dang, Member, IEEE, Vidhyasaharan Sethu, Member,": "well as relation notions such as arousal is higher at one instance compared to another. In this paper, we introduce the"
        },
        {
          "Jingyao Wu, Student Member, IEEE, Ting Dang, Member, IEEE, Vidhyasaharan Sethu, Member,": "terminology absolute and relative ordinal labels to make this distinction clear and investigate both with a view to integrate them"
        },
        {
          "Jingyao Wu, Student Member, IEEE, Ting Dang, Member, IEEE, Vidhyasaharan Sethu, Member,": "and exploit their complementary nature. We propose a Markovian framework referred to as Dynamic Ordinal Markov Model"
        },
        {
          "Jingyao Wu, Student Member, IEEE, Ting Dang, Member, IEEE, Vidhyasaharan Sethu, Member,": "(DOMM) that makes use of both absolute and relative ordinal information, to improve speech based ordinal emotion prediction."
        },
        {
          "Jingyao Wu, Student Member, IEEE, Ting Dang, Member, IEEE, Vidhyasaharan Sethu, Member,": "Finally, the proposed framework is validated on two speech corpora commonly used in affective computing, the RECOLA and"
        },
        {
          "Jingyao Wu, Student Member, IEEE, Ting Dang, Member, IEEE, Vidhyasaharan Sethu, Member,": "the IEMOCAP databases, across a range of system configurations. The results consistently indicate that integrating relative"
        },
        {
          "Jingyao Wu, Student Member, IEEE, Ting Dang, Member, IEEE, Vidhyasaharan Sethu, Member,": "ordinal information improves absolute ordinal emotion prediction."
        },
        {
          "Jingyao Wu, Student Member, IEEE, Ting Dang, Member, IEEE, Vidhyasaharan Sethu, Member,": "Index Termsâ€”Speech emotion recognition, emotion ranks, emotion dynamics, Markov model, ordinal classification, ordinal"
        },
        {
          "Jingyao Wu, Student Member, IEEE, Ting Dang, Member, IEEE, Vidhyasaharan Sethu, Member,": "data, preference learning"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "A  third  framework  for  labelling  which  is  drawing  in-"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "creasing  attention  in  SER  research  is  referred  to  ordinal"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "labelling  scheme,  whereby  dimensions  such  as  arousal"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "and  valence  are  still  used  but  instead  of  interval  scales,"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "ordinal  scales  are  used  [11,  13].  An  ordinal  scale  is  one"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "which  defines  an  order  between  elements  on  the  scale,"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "but no notion of distance is defined between the elements"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "[13]. For instance, on a scale of low, medium and high, a"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "definite order exists but it is not meaningful to discuss the"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "Fig.  1.    Two  1-second  clips  of  video  and  speech  recording  with"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "attached  arousal  labels  from  two  utterances.  (a)  The  correspond-"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "ing  annotated  interval  labels;  (b)  ROLs  indexed  at  each  time  in-"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "stant; and (c) AOLs denoted by different shades of blue  indicating"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   â—†   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "three levels: low, medium and high."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "psychology has shown that people are better at discrimi-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "overall  model,  which  we  refer  to  as  the  dynamic  ordinal"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "nating  among  options  than  they  are  at  assigning  defini-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "Markov  model  (DOMM),  thus  incorporates  both  infor-"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "tive  values  for  what  they  perceived  [13].  Consequently,",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "mation about the emotion at a given time (state probabili-"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "several  recent  studies  have  advocated  that  ordinal  label-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "ties)  as  well  as  the  change  in  emotion  at  a  given  time"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "ling of emotions is better aligned with human perception,",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "(state transition probabilities)."
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "evidenced by lower inter-rater variability [14-18].",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "While  the  idea  of  using  ordinal  labels  is  appealing,",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "2  ORDINAL EMOTION PREDICTION SYSTEMS"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "their  use  in  affective  computing  systems  is  fairly  recent",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "and  the  term  has  been  used  to  refer  to  two  related  but",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "In recent years, there has been growing interest in ordinal"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "distinct  types  of  labels  in  the  emotion  recognition  litera-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "regression  techniques  for  affective  computing  and  these"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "ture. In this manuscript, we introduce the terms absolute",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "approaches have shown some advantages in utilising and"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "ordinal  labels  (AOL)  and  relative  ordinal  labels  (ROL)  to",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "modelling the ordinal nature of the labels [11]. In this sec-"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "distinguish  between  these  two \ntypes  of  ordinal \nlabels",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "tion  we  summarise  recent  research  on  developing  both"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "which  describe  different  aspects  of  emotions.  Typically",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "relative and absolute ordinal emotion prediction systems."
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "AOLs  are  obtained  by asking  annotators  to  select  an  ele-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "It  should  be  noted  that  the  terms  â€˜absolute  ordinalâ€™  and"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "ment from a finite ordinal scale (e.g., {low arousal, medi-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "â€˜relative  ordinalâ€™  are  not  used  in  the  literature  and  the"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "um  arousal,  high  arousal})  [19],  whereas  ROLs  are  ob-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "distinction is introduced in this paper."
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "tained by asking annotators to rank the segments within a",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "2.1 Relative Ordinal Prediction System"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "speech  segment  from  the  lowest  arousal  (or  valence)  to",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "Preference \nlearning  (PL),  a  popular  framework  for  re-"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "the highest arousal (or valence) [11]. As illustrated in Fig.",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "trieval  tasks,  refers  to  a  set  of  ordinal  regression  models"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "1 (c), AOLs denoted by different shades of blue take val-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "that has become an appealing approach in affective com-"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "ues from an â€˜ordered classâ€™, with each label providing an",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "puting. It ranks the samples in ascending/descending  or-"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "absolute  and  easily  interpretable  indication  of  the  affec-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "der  of  an  emotional  attribute.  Cao  et.al  [21]  describe  the"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "tive  dimensions  at  that  time  (e.g.,  low  arousal).  ROLs,",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "use of rankers for categorical emotion representation with"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "denoted  by  the  position  of  red  dots  in  (b),  indicate  the",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "different  rankers  trained  for  each  emotion  category.  For"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "rank of that segment with respect to all others within the",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "instance, a â€˜happyâ€™ ranker trained to treat sample labelled"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "utterance. These ranks retain the pairwise order relation-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "â€˜happyâ€™  with  a  higher  preference  than  a  sample  labelled"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "ship  between  all  pairs  of  segments  as  inferred  from  the",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "â€˜sadâ€™.  For  ranking  along  emotional  attributes  or  dimen-"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "interval labels shown in (a). ROLs provide a relative label",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "sions such as arousal and valence, RankSVMs (a popular"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "indicating the affect at each time step with respect to oth-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "PL framework) have been adopted in a number of studies"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "er  time  steps,  encapsulating  a  notion  of  how  emotion",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "[20, 22-25]. Deep neural network (DNN) based emotional"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "changes but lacking an indication of an absolute level.",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "PL algorithms such as RankNet [20] and RBF-ListNet [23]"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "Both  AOLs  and  ROLs  are  ordinal  but  are  not  equiva-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "have also drawn increasing attention within the field."
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "lent  and  convey  complementary  information.  Consider",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "Apart  from  preference  learning  methods,  ordinal  re-"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "two speech segments that are only assigned AOLs â€“ both",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "gression tasks have also been framed as binary classifica-"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "labelled  â€˜low  arousalâ€™,  even  if  annotators  can  determine",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "tion problems, by decomposing the ordinal variables into"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "which  of  the  two  corresponds  to  a  lower  arousal,  the  la-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "an ensemble of pairs of classes in a staircase-like manner,"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "belling scheme does not encode this. On the other hand, if",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "leading  to  the  Ordinal  Binary  Decomposition  Approach"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "the  two  segments  are  assigned  ROLs  â€“  for  instance,  â€˜A",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "[26]. Finally, naÃ¯ve approaches  which involve treating or-"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "corresponds to lower aroused than Bâ€™, clearly, there is no",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "dinal  labels  as  interval  or  nominal  labels  (and  ignoring"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "indication  of  the  absolute  level  of  arousal  and  this  label",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "their  ordinal  nature)  have  also  been  investigated.  For  in-"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "cannot  distinguish  between  a  scenario  where  A  corre-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "stance, a conventional regression model can be trained by"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "sponds to low arousal level and B corresponds to a medi-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "treating the relative ordinal labels [ğ‘Ÿ1, â€¦ , ğ‘Ÿğ‘] as ğ‘ different"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "um arousal level (Clip 1 in Fig. 1), and one where A corre-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "interval values [1, 2, â€¦ , ğ‘›, â€¦ , ğ‘] (e.g., assign the rank order"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "sponds  to  medium  arousal  level  and  B  to  a  high  arousal",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "as  real  numerical  values, ğ‘Ÿğ‘› = ğ‘› )  [26,  27].  However,  as"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "level (Clip 2 in Fig. 1). Current literature in affective com-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "stated  in  [26,  28]  the  regressor  will  be  more  sensitive  to"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "puting typically utilises one or another, with both scenar-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "the  converted  values  rather  than  the  original  rank  order,"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "ios referred to as â€˜ordinal labelsâ€™ [11, 20].",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "which  might  degrade  the  performance  of  the  regression"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "In this paper, we propose a framework that integrates",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "models."
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "both  the  absolute  and  relative  ordinal  information.  We",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "assume  that  AOLs  are  more  readily \ninterpretable  and",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "2.2 Absolute Ordinal Prediction System"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "consequently  the  predictions  of  an  emotion  recognition",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "AOLs  have  been  adopted  widely \nin  current  emotion"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "system  should  be  absolute  ordinal  quantities.  Simultane-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "recognition  studies,  but \nthey  are  generally \ntreated  as"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "ously we also recognise that ROLs are better aligned with",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "nominal  labels  and  the  underlying  ordinality  is  ignored."
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "the  types  of  judgements  humans  are  better  at  making,",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "For  instance,  in  a  few  different  studies  [29-32],  affect  la-"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "and  consequently  ROL  should  inform  the  training  and",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "bels  in  terms  of  low,  medium  and  high  arousal  are  uti-"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "outputs  of  the  emotion  prediction  system.  In  the  pro-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "lised, but naÃ¯ve nominal classifiers, such as support vector"
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "posed framework, the AOLs are represented as states of a",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": "machine (SVM), are employed [30]."
        },
        {
          "â€˜distanceâ€™ between the elements. Additionally, research in": "finite state Markov model, with the state transition prob-",
          "abilities  predicted  by  a  system  trained  using  ROLs.  The": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "insides the circles, and the arrows depicting ROL changes between"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "consecutive  time  steps.  The  red  circles  represent  the  potential"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "AOLs  at  time ğ‘¡4 but  would  be  ruled  out  given  the Î” over  the ğ‘¡3â†’4"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "transition."
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "took \ninto  account \nthat  over \nthe  ğ‘¡3â†’4  transition,  ROL"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "change from rank 48 to 87 (i.e., âˆ† = 39)."
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "An  overview  of  the  proposed  Dynamic  Ordinal  Mar-"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "kov  Model  (DOMM)  based  emotion  prediction  system  is"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "shown in Fig. 3. It comprises two subsystems which oper-"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "ate  in  parallel,  one  to  predict  AOLs  and  one  to  predict"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "ROLs  from  the  input  features.  The  AOL  prediction  sub-"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "system  is  designed  to  estimate  state  posteriors, ğ‘ƒ(ğ›½ğ‘¡|ğ’™ğ‘¡),"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "given  the  input  features, ğ’™ğ‘¡,  at  each  time step ğ‘¡.  The  pos-"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "sible states of the model, ğ›½, correspond to the finite set of"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "possible AOLs. In all the systems presented in this paper,"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "there  are  possible  states  â€“  â€˜lowâ€™,  â€˜mediumâ€™  and  â€˜highâ€™  for"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "arousal or valence."
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "The output of the ROL prediction subsystem is a rank,"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "ğ›¼ğ‘¡, at each time step ğ‘¡; from which state transition proba-"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "inferred  based  on  the  rank \nbilities,  ğ‘ƒ(ğ›½ğ‘¡|ğ›½ğ‘¡âˆ’1, Î”ğ›¼ğ‘¡) ,  are"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "difference  between  consecutive \ntime  steps,  Î”ğ›¼ğ‘¡ = ğ›¼ğ‘¡ âˆ’"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "ğ›¼ğ‘¡âˆ’1 as per Bayes theorem:"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "ğ‘ƒ(âˆ†ğ›¼ğ‘¡ |ğ›½ğ‘¡âˆ’1,  ğ›½ğ‘¡) ğ‘ƒ( ğ›½ğ‘¡ | ğ›½ğ‘¡âˆ’1)"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "(1)"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "ğ‘ƒ (ğ›½ğ‘¡ | ğ›½"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "ğ‘¡âˆ’1\nğ‘ƒ(âˆ†ğ›¼ğ‘¡ | ğ›½ğ‘¡âˆ’1)"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "where ğ‘ƒ(âˆ†ğ›¼ğ‘¡ | ğ›½ğ‘¡âˆ’1, ğ›½ğ‘¡) represents  the  probability  of  rank"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "(ROL)  difference âˆ†ğ›¼ğ‘¡  given  the  state  (AOL)  at  previous"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "time  step, ğ›½ğ‘¡âˆ’1,  and  the  current  state, ğ›½ğ‘¡; ğ‘ƒ(âˆ†ğ›¼ğ‘¡ | ğ›½ğ‘¡âˆ’1) de-"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "notes  the  probability  of  a  change  in  rank  (ROL)  of   âˆ†ğ›¼ğ‘¡"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "occurring  given  previous  state  was ğ›½ğ‘¡âˆ’1;  and ğ‘ƒ(ğ›½ğ‘¡| ğ›½ğ‘¡âˆ’1)"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "denotes the probability of transitioning from state ğ›½ğ‘¡âˆ’1 to"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "state ğ›½ğ‘¡.  Models  for  all  three  probabilities  on  the  right-"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "hand  side  of  (1)  can  be  inferred  from  labelled  training"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "data."
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "To  model ğ‘ƒ(âˆ†ğ›¼ğ‘¡ | ğ›½ğ‘¡âˆ’1),  first  all  instances  of  ROL  pairs"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "the \ntraining \n[ ğ›¼ğ‘¡,  ğ›¼ğ‘¡âˆ’1]  with \ninitial  state   ğ›½ğ‘¡âˆ’1 = ğ¿  from"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "data are selected and the corresponding set of Î”ğ›¼ğ‘¡ values"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "are computed. Kernel density estimation (KDE) [40, 41] is"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "then  carried  out  to  obtain  a  model  for ğ‘ƒ(Î”ğ›¼ğ‘¡|ğ›½ğ‘¡âˆ’1 = ğ¿)."
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "Similarly \nmodels \nfor \n and \nğ‘ƒ(Î”ğ›¼ğ‘¡|ğ›½ğ‘¡âˆ’1 = ğ‘€)"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "ğ‘ƒ(Î”ğ›¼ğ‘¡|ğ›½ğ‘¡âˆ’1 = ğ») are obtained."
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "Models  for ğ‘ƒ(Î”ğ›¼ğ‘¡|ğ›½ğ‘¡âˆ’1, ğ›½ğ‘¡) can  also  be  obtained  by  the"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "same approach, by partitioning the dataset into 9 subsets"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "corresponding \nto \nall \ncombinations \nof \nğ›½ğ‘¡âˆ’1, ğ›½ğ‘¡ âˆˆ"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "{ğ¿, ğ‘€, ğ»} Ã— {ğ¿, ğ‘€, ğ»} ,  following  by  KDE.  An  example \nis"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "illustrated in Fig. 4 with: Fig. 4(a) showing the histogram"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "of âˆ†ğ›¼ğ‘¡ for  low-low  (L-L)  transitions  (from  the  RECOLA"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": "dataset); Fig. 4(b) showing the histogram for high-low (H-"
        },
        {
          "AOLs  at  different  time  steps  with  ROLs  indicated  by  the  number": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "An alternative approach is the threshold method that": "assumes  a  continuous \nlatent  variable \nrepresents \nthe"
        },
        {
          "An alternative approach is the threshold method that": "emotional  state,  with  appropriate  thresholds  dividing"
        },
        {
          "An alternative approach is the threshold method that": "the range of the latent variable into a few ordered ranges"
        },
        {
          "An alternative approach is the threshold method that": "corresponding to absolute ordinal labels. The prediction"
        },
        {
          "An alternative approach is the threshold method that": "model  then  learns  a  mapping  function  between  the  in-"
        },
        {
          "An alternative approach is the threshold method that": "put  features  and  the  continuous  hidden  latent  variable,"
        },
        {
          "An alternative approach is the threshold method that": "which  is  then  converted  to  an  absolute  ordinal  label."
        },
        {
          "An alternative approach is the threshold method that": "Several  modelling  methods  can  be  utilised  in  this  ap-"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "proach,  such  as  cumulative  link  models  [33],  and  pro-"
        },
        {
          "An alternative approach is the threshold method that": "portional odds models [34]."
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "Amongst  truly  ordinal  prediction  systems,  the  Ordi-"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "nal  Multi-class  SVM  (OMSVM)  which  was  first  pro-"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "posed  by  Kim  and  Ahn  [35]  for  credit  ratings  and  has"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "since been widely used in the area of financial and mar-"
        },
        {
          "An alternative approach is the threshold method that": "ket  analysis  [36,  37],    appears  to  be  a  very  promising"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "candidate. Additionally, a suitable ordinal binary decom-"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "position approach can also be  employed to model AOLs."
        },
        {
          "An alternative approach is the threshold method that": "Compared to the naÃ¯ve approach and threshold methods,"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "the binary decomposition method is more suitable since it"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "can  capture  ordinal \ninformation  without  defining  dis-"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "tances  between  ordered  classes.  Deep  learning  may  also"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "be a valid solutions to ordinal binary decomposition, but"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "it  can  suffer  with  generalisation,  especially  for  small  da-"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "taset [35, 38]."
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "3  PROPOSED DYNAMIC ORDINAL MARKOV MODEL"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "3.1 Combining Absolute and Relative Ordinal"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "Information"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "The  task  of  integrating  two  aspects  of  ordinal  emotion"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "labels with AOL reflecting the emotional state (static) and"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "ROL  indicating  change  in  emotional  state  (dynamic)  can"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "be intuitively associated with a Markovian framework. A"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "Markov  model  (MM)  is  a  stochastic  model  describing  a"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "sequence of states, where the probability of occurrence of"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "future states depends only on the current state [39]. In the"
        },
        {
          "An alternative approach is the threshold method that": "context  of  ordinal  emotion  labels,  AOL  and  ROL  can  be"
        },
        {
          "An alternative approach is the threshold method that": "viewed as informing the state and transition probabilities"
        },
        {
          "An alternative approach is the threshold method that": "of  a  MM.  For  instance,  it  is  reasonable  to  expect  that  a"
        },
        {
          "An alternative approach is the threshold method that": "positive change in the ROL (increase in rank) would indi-"
        },
        {
          "An alternative approach is the threshold method that": "cate  high  probabilities  for  transitions  from  a  â€˜lowâ€™  arous-"
        },
        {
          "An alternative approach is the threshold method that": "al/valence  state  to  a \nâ€˜mediumâ€™  arousal/valence  state  as"
        },
        {
          "An alternative approach is the threshold method that": "well as â€˜mediumâ€™ to â€˜highâ€™, while simultaneously indicat-"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "ing  a  low  probability  for  other  transitions  (like  â€˜highâ€™  to"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "â€˜mediumâ€™, â€˜highâ€™ to â€˜lowâ€™, etc.). Similarly, a large negative"
        },
        {
          "An alternative approach is the threshold method that": "change  in  ROL  would  indicate  a  high  probability  for  a"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "â€˜Highâ€™  to  â€˜Lowâ€™  transition  and  a  low  probability  for  all"
        },
        {
          "An alternative approach is the threshold method that": "other transitions."
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "This idea is illustrated in Fig. 2, with the position of the"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "blue circles indicating the AOL with ğ´0, ğ´1, and ğ´2 denot-"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "ing  low  (L),  medium  (M)  and  high  (H)  arousal  states  re-"
        },
        {
          "An alternative approach is the threshold method that": "spectively; and the values within the circles denoting the"
        },
        {
          "An alternative approach is the threshold method that": "ROL (rank within a set of 100 time steps). The number on"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "each arrow given by Î” shows the change in ROL between"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "two  consecutive  instances.  It  is  clear  that  the  value  of Î”"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "carries  information  about  the  dynamics  of  the  AOL,  i.e.,"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "any change in ğ´ğ‘–. For example, the chances of erroneously"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "predicting  the  AOL  at  time ğ‘¡4 as  low  or  medium  instead"
        },
        {
          "An alternative approach is the threshold method that": ""
        },
        {
          "An alternative approach is the threshold method that": "of high would be greatly reduced if the automatic system"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "dle with blue circles representing the system state ğ›½ğ‘¡ = {ğ¿, ğ‘€, ğ»} at time ğ‘¡ and green arrows denoting transition probabilities ğ‘ƒ(ğ›½ğ‘¡ | ğ›½ğ‘¡âˆ’1, âˆ†ğ›¼ğ‘¡)."
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "Both of these parameters change with time and inferred based on the AOL and ROL prediction systems, making the model dynamic. A repre-"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "sentation of the state lattice is given at the bottom, where the y axis ğ´ğ‘–, ğ‘– = [0,1,2] represents the AOLs comprising of low(L), medium (M) and"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "high (H) states, and the x axis represents the time step from ğ‘¡1 to ğ‘¡ğ‘. Blue dots corresponding to the state posterior probabilities calculated"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "from AOL  prediction system at each time instant. The transition probabilities between each state between each pair of time frames (green"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "arrow). Red line indicating the â€˜best pathâ€™ after Viterbi decoding which gives the final predicted AOL sequence."
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "L) \ntransitions;  and  Fig.  4(c)  showing \nthe  models \nfor"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "ğ‘ƒ(âˆ†ğ›¼ğ‘¡ | ğ›½ğ‘¡âˆ’1 = ğ¿, ğ›½ğ‘¡ = ğ¿)  and  ğ‘ƒ(âˆ†ğ›¼ğ‘¡ | ğ›½ğ‘¡âˆ’1 = ğ», ğ›½ğ‘¡ = ğ¿)  ob-"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "tained via KDE. It can be seen that âˆ†ğ›¼ğ‘¡ is much more like-"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "ly to take small positive or negative values given L-L tran-"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "sitions, while it is much more likely to take large negative"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "values given H-L transitions."
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "Lastly, the probability ğ‘ƒ( ğ›½ğ‘¡ | ğ›½ğ‘¡âˆ’1) is calculated as:"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "ğ‘ğ‘–â†’ğ‘—"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "(2)"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "ğ‘ƒ( ğ›½ğ‘¡ = ğ‘— | ğ›½ğ‘¡âˆ’1 = ğ‘–) ="
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": ""
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "ğ‘ğ‘–"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": ""
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "where, ğ‘–, ğ‘— âˆˆ {ğ¿, ğ‘€, ğ»};  ğ‘ğ‘– denotes the number of instances"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": ""
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "of state (AOL) ğ‘– in the training data set; and ğ‘ğ‘–â†’ğ‘— denotes"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "the  number  of  instances  of  state ğ‘– followed  by  state ğ‘— in"
        },
        {
          "Fig. 3.  Overview of the proposed DOMM system architecture. A graphical representation of the Markov model diagram is shown in the mid-": "the training set."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "the rank is 2â‰»3â‰»1â‰»4."
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "1"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "(3) \nğ‘ƒ( ğ›½ğ‘¡ = ğ‘–|ğ‘¦) ="
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "1 + ğ‘’ğ‘¥ğ‘(ğ‘ğ‘¦ + ğ‘)"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "where ğ‘¦ denotes  the  SVM  output;  and ğ‘  and ğ‘  refer  to"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "sigmoid  function  parameters  which  are  determined  dur-"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "ing training as outlined in [50]. For further details, readers"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "are referred to [46, 50]."
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "3.2.2 ROL Prediction Subsystem: RankSVM"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "Conventional SVM classifiers aims to find a hyperplane ğ‘¤âƒ—âƒ—"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "that maximally separates two classes, and RankSVMs [45]"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "extend  this  idea  to  identify  a  hyperplane  that  performs"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "so-called preference comparisons, such that projections of"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "the  data  on  that  hyperplane  still  preserve  the  original"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "rank orders. This idea is illustrated in Fig. 5, where it can"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "be seen that the optimal hyperplane would be ğ‘¤1 (instead"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "of ğ‘¤2)  since  the  projections  onto  the  hyperplane  corre-"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "sponding to ranks (ROLs) 1 â‰» 2 â‰» 3 â‰» 4 along the hyper-"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "plane  (where  â€˜â‰»â€™  denotes  â€˜preferenceâ€™)  still  preserve  its"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "order."
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "4  DATABASE DESCRIPTION"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "None of the publicly available speech corpora come with"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "absolute and relative ordinal labels. Consequently, in this"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "work  we    use  the  well-established  RECOLA  [51]  and  the"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "IEMOCAP  [52]  databases  and  convert  the  interval  labels"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "to AOL and ROL."
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "4.1 RECOLA"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "The  Remote  Collaborative \nand  Affective \nInteractions"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "(RECOLA)  dataset  [51] is a widely used multimodal cor-"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "pus  containing  both  audio  and  video  modalities.  It  con-"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "sists of 23 dyadic interactions from 46 participants includ-"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "ing 27 females and 19 males. The data used in the experi-"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "ments reported in this paper corresponds to that used in"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "the AVEC  challenge 2016 [53, 54], which consists of 9 ut-"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "terances  each  in  the training  and  development  sets  (with"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "no  overlap)  with  each  utterance  having  a  duration  of  5"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "minutes.  Results  on  the  development  set  are  reported  as"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "test set labels have not been publicly released. Each utter-"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "ance is also annotated by 6 raters with continuous arousal"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "and valence ratings between -1 to 1 at a sampling interval"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "of 40ms."
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "Delay  compensation  is  applied  to  compensate  the  hu-"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": "man  perception  delays  in  the  labels  as  suggested  in  [55]"
        },
        {
          "instance,  for  hyperplane wâƒ—âƒ—âƒ— 1,  the  rank  is  1â‰»2â‰»3â‰»4,  while  for wâƒ—âƒ—âƒ— 2": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "ditionally,  the  labels  are  smoothed  by  averaging  within"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "1sec windows, with 50% overlap between windows. This"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "is in line with the suggested use of 1-3 second windows to"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "capture the trend in the interval labels [56]. In total, each"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "utterance  (5  minute  duration)  comprised  of  615  and  617"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "windows for arousal and valence."
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "4.1.1 Interval to AOL Conversion:"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "Absolute  ordinal  labels  can  be  obtained  by  thresholding"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "the interval labels into three ordinal levels - low, medium,"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "and high. However, choosing these thresholds are a chal-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "lenge and no commonly agreed upon thresholds exist [29,"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "57].  Therefore,  a  range  of  potentially  suitable  thresholds"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "were analysed. Additionally, the conversion is carried out"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "individually  for  each  annotator  and  the  final  consensus"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "AOL is inferred via majority vote among the 6 individual"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "AOLs (one per annotator)."
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "Given the label value ğ‘¦Ì…ğ‘¡representing the average arous-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "al/valence \nintensity  within  each  window  and \nthresh-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "olds ğœƒ1and ğœƒ2, the AOL is taken as Low, if  ğ‘¦Ì…ğ‘¡ â‰¤   ğœƒ1; Medi-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "um,  if ğœƒ1 < ğ‘¦Ì…ğ‘¡ â‰¤   ğœƒ2;  and  High,  if ğ‘¦Ì…ğ‘¡ >   ğœƒ2.  To  determine"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "suitable  values  for ğœƒ1 and ğœƒ2,  measures  of  label  balance"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "and  inter-rater  agreement  after  conversion  are  analysed."
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "Label balance (ğ›¾) is computed based on the distribution of"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "three classes occurred within training set. First, the differ-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "ence  in  relative  frequency  between  the  most  and  least"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "frequent AOLs within training dataset is computed as ğ›¾ ="
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "|ğ‘ğ‘šâˆ’ğ‘ğ‘™\n|,  where ğ‘ğ‘š and ğ‘ğ‘™ represent  number  of  most  and"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "ğ‘"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "least frequent AOLs respectively, and  N is the total num-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "ber  of  AOLs  within  the  training  set.  A  large ğ›¾ indicates"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "highly imbalanced label distribution and vice versa. Inter-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "rater  agreement  represents  consensus \nlevel  among  the"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "converted AOL across multiple annotators. It is quantified"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "as the ratio of the number of frames where more than half"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "of  the  raters  agree  on  the  same  AOL.  A  reasonably  high"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "ratio means a higher inter-rater agreement."
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "There will be a trade-off between the label balance and"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "level of agreement. For  instance, thresholding all annota-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "tions  into  a  single  AOL  leads  to  perfect  agreement  but"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "also a highly skewed and uninformative label distribution."
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "Whereas,  equally  spacing  the  thresholds  is  likely  to  lead"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "to  less  distinguishable  labels  that  are unlikely  to  be  opti-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "mal except in the case of uniform distribution of interval"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "labels, at unrealistic expectation â€“ extreme affect labels are"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "less frequent."
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "Finally, as arousal annotations were observed to be rea-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "sonably symmetrically distributed over [-1, 1], the thresh-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "olds ğœƒğ‘1 and ğœƒğ‘2 for arousal were also set to be symmetric,"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "with ğœƒğ‘2 =   âˆ’ ğœƒğ‘1. A range of different values of ğœƒğ‘2, with-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "in [0.08, 0.2] with  a  step  size  0.02,  was  analysed.  This  re-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "gion was selected based on the observation that the medi-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "an  arousal  value  from  every  annotator  fell  in  the  range"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "[âˆ’0.067, 0.118] and  we  wanted  to  ensure  that  the  â€˜medi-"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "umâ€™ label always covers the median values. Both the label"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "balance  measure  and  the  inter-rater  agreement  for  the"
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "chosen range of thresholds are shown in Fig. 6(a)."
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": ""
        },
        {
          "with 4 seconds for arousal and 2 seconds for valence. Ad-": "It  was  also  observed  that  valence  labels  in  RECOLA"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: suitable. Finally, a different set of AOLs for different INTERVAL TO ABSOLUTE ORDINAL LABEL CONVERSIONS.",
      "data": [
        {
          "the IEMOCAP database, the experiments are carried out-": "with  leave-one-speaker-out  cross  validation  resulting  in"
        },
        {
          "the IEMOCAP database, the experiments are carried out-": "10-fold  cross  validation.  Consequently, \nthe  mean  and"
        },
        {
          "the IEMOCAP database, the experiments are carried out-": "standard  deviation  of  number  of  turns  used  in  training"
        },
        {
          "the IEMOCAP database, the experiments are carried out-": ""
        },
        {
          "the IEMOCAP database, the experiments are carried out-": "and testing set across the 10 folds are reported in Table 2."
        },
        {
          "the IEMOCAP database, the experiments are carried out-": ""
        },
        {
          "the IEMOCAP database, the experiments are carried out-": "Simiarly,  all  experiments  on  the  RECOLA  database,  was"
        },
        {
          "the IEMOCAP database, the experiments are carried out-": ""
        },
        {
          "the IEMOCAP database, the experiments are carried out-": "conducted  for  different  threshold  settings,  which  results"
        },
        {
          "the IEMOCAP database, the experiments are carried out-": "in different numbers for each class. The mean and stand-"
        },
        {
          "the IEMOCAP database, the experiments are carried out-": ""
        },
        {
          "the IEMOCAP database, the experiments are carried out-": "ard deviation of samples across all thresholds is reported."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: suitable. Finally, a different set of AOLs for different INTERVAL TO ABSOLUTE ORDINAL LABEL CONVERSIONS.",
      "data": [
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "suitable.  Finally,  a  different  set  of  AOLs  for  different",
          "TABLE 1": "INTERVAL TO ABSOLUTE ORDINAL LABEL CONVERSIONS."
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "thresholds  within  the  range ğœƒğ‘2 âˆˆ [0.12,  0.18]  for  arousal;",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "Arousal"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "and ğœƒğ‘£2 âˆˆ [0.14,  0.18]  for  valence with  step  size  0.01  were",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "inferred and all experiments were repeated using each set",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "Low \nMedium \nHigh"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "and  the  mean  and  standard  deviation  across  them  are",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "IEMOCAP \n[1, 2.5) \n[2.5, 3.5) \n[3.5, 5]"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "reported in this paper.",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "RECOLA \n[-1, ğœƒa1] \n(ğœƒa1, ğœƒa2)  \n[ğœƒa2, 1]"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "4.1.2 Interval to ROL Conversion",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "ğœƒğ‘1 âˆˆ [-0.12, -0.18]  ğœƒğ‘2 âˆˆ [0.12, 0.18]"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "ROLs are similarly inferred from smoothed interval labels,",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "obtained by first windowing the interval labels with 1 sec",
          "TABLE 1": "Valence"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "windows  (with  50%  overlap)  and  computing  the  mean",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "Low \nMedium \nHigh"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "within  each  window.  A  rank  sequence  of  ROLs  is  then",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "IEMOCAP \n[1, 2.8] \n(2.8, 4) \n[4, 5]"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "obtained  by  performing  pairwise  comparisons  for  each",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "RECOLA \n[-1, ğœƒv1] \n(ğœƒv1, ğœƒv2)  \n[ğœƒv2, 1]"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "individual  annotator,  and  global  ROLs  were  inferred  us-",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "ing  the  Qualitative  Agreement  (QA)  method  across  all",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "ğœƒğ‘£1 âˆˆ [-0.03, 0.01]  ğœƒğ‘£2 âˆˆ [0.14, 0.18]"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "annotators  [58].  Within  each  utterance,  a  matrix  of  pair-",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "wise comparisons amongst all windows for each individ-",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "TABLE 2"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "ual  annotator  is  first  collected  as shown  in  Fig. 7.  A  con-",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "ABSOLUTE ORDINAL LABELS DISTRIBUTION"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "sensus  matrix  is  then  obtained  via  majority  vote  among",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "matrices  from  all  annotators  and  the  final  rank  sequence",
          "TABLE 1": "IEMOCAP"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "of ROLs is obtained from this consensus matrix.",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "Low \nMedium \nHigh"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "4.2 IEMOCAP",
          "TABLE 1": "Training set \nArousal \n932Â±42 \n2180Â±40 \n1319Â±51"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "The  Interactive  Emotional  dyadic  MOtion  CAPture  data-",
          "TABLE 1": "Valence \n1310Â±26 \n1534Â±52 \n1580Â±34"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "base  (IEMOCAP)  [52]  contains  12  hours  of  audio-visual",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "Testing set \nArousal \n103Â±42 \n242Â±40 \n146Â±51"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "recordings  of  5  dyadic  sessions  from  5  pairs  of  actors.",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "valence \n149Â±26 \n170Â±52 \n176Â±34"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "Both scripted dialogs and improvised dialogs are collect-",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "ed,  and  each  dialog  is  segmented  into  speaker  turns.  In",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "RECOLA"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "total  the  corpus  contains  10,039  turns  with  an  average",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "Low \nMedium \nHigh"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "duration  of  4.5  s.  IEMOCAP  is  annotated  at  a  turn  level",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "that each turn is attached with both a categorical emotion",
          "TABLE 1": "Training set \nArousal \n1555Â±274 \n1730Â±948 \n2250Â±674"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "label  (i.e.,  happiness,  anger,  sadness,  neutral  state  and",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "Valence \n1814Â±454 \n2002Â±190 \n1754Â±327"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "frustration) and primitive based interval annotations (va-",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "Testing set \nArousal \n2424Â±255 \n1335Â±736 \n1775Â±483"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "lence, activation, and dominance) on a 5-point scale (i.e., 1",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "Valence \n2175Â±428 \n1868Â±162 \n1528Â±372"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "-low/negative,  5  â€“  high/positive).    However,  only  the  in-",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "terval  labels  along  arousal  and  valence  within  the  range",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "[1,5] are considered in this work and the experiments on",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "Using  K-means \nclustering, \nthis \nled \nto \nthe \nfollowing"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "IEMOCAP are carried out at the turn-level with the same",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "thresholds for arousal: ğ¿ğ‘œğ‘¤ âˆˆ [1, 2.5), ğ‘€ğ‘’ğ‘‘ğ‘–ğ‘¢ğ‘š âˆˆ [2.5, 3.5),"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "leave-one-speaker-out cross validation as in [29].",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "and ğ»ğ‘–ğ‘”â„ âˆˆ [3.5, 5];  and  these  for  valence: ğ¿ğ‘œğ‘¤ âˆˆ [1, 2.8),"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "A  number  of  different  conversion  schemes  has  been",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "ğ‘€ğ‘’ğ‘‘ğ‘–ğ‘¢ğ‘š âˆˆ [2.8, 4), and ğ»ğ‘–ğ‘”â„ âˆˆ [4, 5]. Interval to ROL con-"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "employed to convert interval labels in IEMOCAP to AOLs,",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "version in the IEMOCAP data was carried out in a similar"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "but they lack a consistent set of thresholds [29, 30, 59-61].",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "manner  to  that  in  RECOLA,  but  based  on  pairwise  com-"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "Therefore, we adopted the idea of clustering the labels in",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "parisons  between speaker  turns  instead  of 1 second  win-"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "the dataset to identify suitable decision thresholds [59, 61].",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "dows [20, 22]."
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "The  label  conversions  threshold  adopted  for  both  the"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "IEMOCAP and RECOLA datasets are summarized in Ta-"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "ble  1  and  Table  2  shows  the  label  distributions  obtained"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "after conversion for the training and test sets. Specifically,"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "all  the  experimental  results  reported  in  this  paper  using"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "the IEMOCAP database, the experiments are carried out-"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "with  leave-one-speaker-out  cross  validation  resulting  in"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "10-fold  cross  validation.  Consequently, \nthe  mean  and"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "standard  deviation  of  number  of  turns  used  in  training"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "Fig. 7. Illustration of QA method [58]. (a) Individual comparison",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "and testing set across the 10 folds are reported in Table 2."
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "matrix  obtained \nfrom \ninterval \nlabels \nfor  one  rater.  Up-arrow",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "Simiarly,  all  experiments  on  the  RECOLA  database,  was"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "indicates  an  increase;  down-arrow  indicates  a  decrease  and",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "equal denotes tie. (b) Consensus matrix obtained by aggregat-",
          "TABLE 1": "conducted  for  different  threshold  settings,  which  results"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "ing individual matrix collected from multiple raters using majori-",
          "TABLE 1": "in different numbers for each class. The mean and stand-"
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "ty votes.",
          "TABLE 1": ""
        },
        {
          "ues  ranging  around ğœƒğ‘£2 âˆˆ [0.16,  0.18]  were  found  to  be": "",
          "TABLE 1": "ard deviation of samples across all thresholds is reported."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "5.1 Features"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "The \n88-dimensioanl \nextended \nGeneva  Minimalistic"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "Acoustic Parameter Set (eGeMAPS), a relatively standard"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "feature \nset  used \nin  affective \ncomputing \nto \nsimplify"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "benchmarking,  was  adopted  for  all  the  experiments  re-"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "ported in this paper [62, 63]. The feature set comprises of"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "arithmetic  mean  and  coefficient  of  variation  functionals"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "applied to 18 low-level descriptors (LLDs) extracted from"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "the minimalistic acoustic parameter set along with anoth-"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "er 8 functionals applied to pitch and loudness. Additional"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "7  LLDs  are  extracted  from  the  extension  parameter  set"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "with  4  statistics  over  the  unvoiced  segments,  6  temporal"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "features,  and  26  additional  cepstral  parameters  and  dy-"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "namic  parameter  [62].  The  features  were  extracted  using"
        },
        {
          "5  EXPERIMENT SETTINGS": "the  OpenSMILE  toolkit  [64]  and  for  additional  details"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "about eGeMAPS, readers are referred to [62]."
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "5.2 System Parameters Settings"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "As previously mentioned, the AOL prediction subsystem"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "in the proposed framework was implemented as an ordi-"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "nal  multiclass  SVM  (OMSVM) \nin  the  reported  experi-"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "ments.  This  OMSVM  implementation  used  the  Classifica-"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "tionECOC  MATLAB  toolbox  (an  error  correction  output"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "code multi-class classifier) [65]. The state posterior proba-"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "bilities were then computed using the FitPosterior function"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "[50]."
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "The RankSVM model used in the ROL prediction sub-"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "system  was  implemented  using  the  toolkit  referred  to  in"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "[66].  It  is  trained  using primal  Newton  method,  which  is"
        },
        {
          "5  EXPERIMENT SETTINGS": "known to be fast, [67, 68] and the maximum Newton step"
        },
        {
          "5  EXPERIMENT SETTINGS": "was set to be 20 as in [66].  Finally, both the OMSVM and"
        },
        {
          "5  EXPERIMENT SETTINGS": "RankSVM  models  utilised  linear  kernels  and  both  used"
        },
        {
          "5  EXPERIMENT SETTINGS": "ğ‘ = 1 Ã— 10âˆ’4 as suggested in [30, 69]."
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "5.3 Evaluation Metrics"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "5.3.1 AOL Prediction Evaluation Metrics"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "Unweighted  Average  Recall  (UAR)  is  a  standard  metric"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "used  to  quantify  performance  in  nominal  classification"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "tasks [29, 30], and has also been utilised to report â€˜classifi-"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "cation  accuracyâ€™  even  predicting  AOLs  [35].  We  report"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "UAR  as  one  measure  to  allow  comparison  with  existing"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "literature. However, we also note that UAR does not take"
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "into  account  ordinality  in  the  labels.  For  instance,  incor-"
        },
        {
          "5  EXPERIMENT SETTINGS": "rectly  predicting  â€˜Low  arousalâ€™  as  â€˜Medium  arousalâ€™  or"
        },
        {
          "5  EXPERIMENT SETTINGS": "â€˜High  arousalâ€™  both  carry  the  same  penalty  although  ob-"
        },
        {
          "5  EXPERIMENT SETTINGS": "viously  the  latter  is  a  bigger  error.  Therefore,  to  take  the"
        },
        {
          "5  EXPERIMENT SETTINGS": "ordinal nature of AOLs into consideration, we also report"
        },
        {
          "5  EXPERIMENT SETTINGS": "the  weighted  Cohenâ€™s  Kappa  (WK)  coefficient, ğ‘˜ğ‘¤,  which"
        },
        {
          "5  EXPERIMENT SETTINGS": "is  used  to  measure  the  consistency  between  two  AOL"
        },
        {
          "5  EXPERIMENT SETTINGS": "indicates \nthe \nlevel  of \nsequences.  The  coefficient,  ğ‘˜ğ‘¤ ,"
        },
        {
          "5  EXPERIMENT SETTINGS": "agreement \nbetween \ntwo \ndifferent \nlabel \nsequences"
        },
        {
          "5  EXPERIMENT SETTINGS": "(predictions Vs ground truth) as given by (4), with ğ‘˜ğ‘¤ = 1"
        },
        {
          "5  EXPERIMENT SETTINGS": "indicating  perfect  agreement  and   ğ‘˜ğ‘¤ = 0 indicating  only"
        },
        {
          "5  EXPERIMENT SETTINGS": "chance agreement [70]."
        },
        {
          "5  EXPERIMENT SETTINGS": ""
        },
        {
          "5  EXPERIMENT SETTINGS": "3ğ‘–\n3ğ‘—\n3ğ‘–\n3ğ‘—\nâˆ‘\nâˆ‘\nâˆ‘\nâˆ‘\nâˆ’\nğ‘¤ğ‘–ğ‘— ğ‘ğ‘–ğ‘— \nğ‘¤ğ‘–ğ‘— ğ‘ğ‘–.ğ‘.ğ‘— \n=1\n=1\n=1\n=1"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: On the IEMOCAP TABL E 4",
      "data": [
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "Prior  to  testing  the  proposed  DOMM  based  prediction"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "system,  we  quantify  the  performance  of  the  AOL  and"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "ROL  prediction  susbsystems  individually  based  on  the"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "metrics  outlined \nin  the  previous  section.  Experimental"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "results  on  the  IEMOCAP  database  are  reported  in  terms"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "of  the  mean  and  standard  deviation  computed  across"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "each  fold  of  the  10-fold  cross  validation  as  discussed  in"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "section  4.  Results  on  the  RECOLA  database  are  reported"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "in terms of means and standard deviations of the perfor-"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "mance  metrics  determined  on  the  different  AOL  sets  ob-"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "tained  using  the  identified  range  of  thresholds  (ğœƒğ‘2 and"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "ğœƒğ‘£2)  for  the  interval  label  to  AOL  conversions  (refer  sec-"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "tion 4.1)."
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "6.1 OMSVM Performance"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "The  performance  of  the  OMSVM  based  AOL  prediction"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "subsystem evaluated on both IEMOCAP and REOCLA in"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "terms of UAR are  reported in Table  3. On the IEMOCAP"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "database,  the  OMSVM  system  is  compared  to  an  HMM"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "based  baseline  [29].  While  the  UAR  of  the  OMSVM  sys-"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "tem is lower than that of the baseline, it is reasonably high"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "and  significantly  higher  than  chance.  Consequently,  we"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "expect  the  OMSVM  system  to  constitute  a  reasonable"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "AOL  prediction \nsubsystem \nto  evaluate \nthe  proposed"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "framework.  Finally,  it  should  be  noted  that  the  baseline"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "system  results  were  reported  with  a  different  AOL  con-"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "version scheme."
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "No  suitable  baseline  systems  that  allow  for  a  direct"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "comparison on the RECOLA database could be identified"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "since most published literature using RECOLA deals with"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "interval label prediction. The closest reported approaches"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "focus  on  binary  classification  tasks  of  Low/High  arous-"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "al/valence  [31, 32]. Thus, the binary classification [31] was"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "adopted as a reference system (refer Table 3). The perfor-"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "mance  of  the  OMSVM  subsystem  is  lower  than  that  of"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "reference  system,  but  the  OMSVM  systemâ€™s  target  is  one"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "of three AOLs while the reference system is solving a two-"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "class problem."
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "TABLE 3"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "UNWEIGHTED AVERAGE RECALL (UAR) AND WEIGHTED"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "KAPPPA (ğ‘˜ğ‘¤) MEASUREMENT OF OMSVM SUBSYSTEM."
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "IEMOCAP"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "Arousal \nValence"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "UAR (%) \nUAR (%) \nğ‘˜ğ‘¤ \nğ‘˜ğ‘¤"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "OMSVM \n53.1Â±3.79 \n0.279Â±0.06 \n46.3Â±4.67 \n0.190Â±0.06"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "HMM [29] \n61.9Â±4.88 \n- \n49.9Â±3.63 \n-"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "RECOLA"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "Arousal \nValence"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "UAR (%) \nUAR (%) \nğ‘˜ğ‘¤ \nğ‘˜ğ‘¤"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "OMSVM \n55.3Â±2.08 \n0.443Â±0.05 \n36.4Â±1.77 \n0.079Â±0.03"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "Binary Classi-"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "60.7 \n- \n52.3 \n-"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": "fication [31]"
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        },
        {
          "6  EXPERIMENTAL RESULTS AND DISCUSSION": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 5: them has clear benefits.",
      "data": [
        {
          "TABLE 5": "UNWEIGHTED AVERAGE RECALL (UAR) AND WEIGHTED",
          "them has clear benefits.": "The performance discrepancy between DOMM_RS and"
        },
        {
          "TABLE 5": "KAPPA (ğ‘˜ğ‘¤)   MEASUREMENT IN TERMS OF MEAN AND STAND-",
          "them has clear benefits.": "DOMM_GT  can  be  reasonably  explained  by  the  perfor-"
        },
        {
          "TABLE 5": "ARD DEVIATION OF 10-FOLD CROSS-VALIDATION ON IE-",
          "them has clear benefits.": "mance of the RankSVM based ROL prediction subsystem"
        },
        {
          "TABLE 5": "MOCAP, AND DIFFERENT THRESHOLDS ON RECOLA.",
          "them has clear benefits.": "reported  in  Table  4.  Additionally,  it  is  worth  noting  that"
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "arousal AOLs are somewhat unbalanced for IEMOCAP as"
        },
        {
          "TABLE 5": "IEMOCAP",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "can be seen from Table 2. The relatively lower number of"
        },
        {
          "TABLE 5": "Arousal \nValence",
          "them has clear benefits.": "high  and  low  arousal  state  could  have  the  effect  of  in-"
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "creasing \nthe \ntransition  probability \ninto \nthe  medium"
        },
        {
          "TABLE 5": "UAR (%) \nUAR (%) \nğ‘˜ğ‘¤ \nğ‘˜ğ‘¤",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "arousal  state  and  consequently  a  higher  chance  of  incor-"
        },
        {
          "TABLE 5": "OMSVM \n53.1Â±3.79 \n0.279Â±0.09 \n46.3Â±4.67 \n0.190Â±0.06",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "rectly  predicting  medium  [29].  Table  5  also  includes  the"
        },
        {
          "TABLE 5": "DOMM_RS \n53.2Â±4.98 \n0.252Â±0.11 \n51.1Â±4.94 \n0.232Â±0.07",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "reported performance of the HMM baseline [29], however,"
        },
        {
          "TABLE 5": "DOMM_GT \n60.9Â±5.10 \n0.363Â±0.12 \n59.2Â±7.13 \n0.358Â±0.10",
          "them has clear benefits.": "as previously mentioned this performance was estimated"
        },
        {
          "TABLE 5": "HMM [36] \n61.9Â±4.88 \n- \n49.9Â±3.63 \n-",
          "them has clear benefits.": "with different AOLs and data partitions."
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "Finally,  in  addition  to  the  summary  results  in  Table  5,"
        },
        {
          "TABLE 5": "RECOLA",
          "them has clear benefits.": "we  also  provide  comparisons  between  the  three  systems"
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "for  each  individual  arousal  and  valence  thresholds  (ğœƒğ‘2"
        },
        {
          "TABLE 5": "Arousal \nValence",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "and ğœƒğ‘£2)  employed  in  our  interval  label  to  AOL  conver-"
        },
        {
          "TABLE 5": "UAR (%) \nUAR (%) \nğ‘˜ğ‘¤ \nğ‘˜ğ‘¤",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "sion  in  Fig.  10  and  11.  The  red  bar  on  the  boxplots  indi-"
        },
        {
          "TABLE 5": "OMSVM \n55.3Â±2.08 \n0.443Â±0.05 \n36.4Â±1.77 \n0.079Â±0.03",
          "them has clear benefits.": "cates  the  median  values  across  different  thresholds  and"
        },
        {
          "TABLE 5": "DOMM_RS \n55.9Â±2.14 \n0.462Â±0.07 \n38.4Â±2.07 \n0.113Â±0.04",
          "them has clear benefits.": "each  dot  represents  performance  measured  with  each"
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "threshold.  The  dashed  lines  in  the  boxplots  connect  dots"
        },
        {
          "TABLE 5": "DOMM_GT \n57.7Â±2.52 \n0.493Â±0.06 \n40.8Â±1.88 \n0.168Â±0.03",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "corresponding to the same  threshold and is provided for"
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "ease  of  comparison  of  systems  at  the  same  threshold.  It"
        },
        {
          "TABLE 5": "formance of a version of the DOMM framework that uses",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "can be seen that the trends observable in Table 5 and even"
        },
        {
          "TABLE 5": "rank difference (Î”ğ›¼ğ‘¡) from the ground truth ROLs instead",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "more  evident  with  the  additional  detail  in  these  figures"
        },
        {
          "TABLE 5": "of the ROLs predicted by the RankSVM system. This sys-",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "and \nspecifically  DOMM_GT  outperforms  DOMM_RS,"
        },
        {
          "TABLE 5": "tem,  denoted  as  DOMM_GT,  serves  as  an  indication  of",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "which \nin \nturn  outperforms  OMSVM  across  every  all"
        },
        {
          "TABLE 5": "the  upper  bound  of  what  is  possible  when  integrating",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "measures and when using any of the thresholds for inter-"
        },
        {
          "TABLE 5": "relative \nand \nabsolute \nordinal \ninformation  with \nthe",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "val to AOL conversion."
        },
        {
          "TABLE 5": "DOMM framework. The results from both IEMOCAP and",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "RECOLA are reported in Table 5. It can be seen from these",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "results that consistently, across both databases and across",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "7  CONCLUSION"
        },
        {
          "TABLE 5": "both  arousal  and  valence,  the  DOMM_RS  outperforms",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "In the growing body of literature on ordinal emotion pre-"
        },
        {
          "TABLE 5": "the  OMSVM  system  and  is  in  turn  outperformed  by  the",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "diction,  the  term  â€œordinal  emotion  labelsâ€  has  been  used"
        },
        {
          "TABLE 5": "DOMM_GT  system.  The  consistent  trends  strongly  sup-",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "to refer to two related but different notions. In this paper,"
        },
        {
          "TABLE 5": "port the idea that absolute and relative ordinal labels con-",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "we explicitly distinguish between them and introduce the"
        },
        {
          "TABLE 5": "tain  complementary \ninformation  and \njointly  modelling",
          "them has clear benefits.": ""
        },
        {
          "TABLE 5": "",
          "them has clear benefits.": "terminology  absolute  ordinal \nlabel  (AOL)  and  relative"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "ordinal label (ROL), conveying the different aspect of or-"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "dinality  they  emobody.  Followig  this,  we  establish  that"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "the complementary nature of the information encoded in"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "these two different ordinal labelling schemes can be inte-"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "grated in a joint model of emotion dynamics.  Specifically,"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "this novel framework, referred to as the dynamic ordinal"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "Markov model (DOMM), integrates time varying absolute"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "and relative ordinal information."
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "A realizable and computationally inexpensive, speech-"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "based  emotion  prediction  based  on  the  proposed  frame-"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "work,  making  use  of  OMSVM  and  RankSVM  based  sub-"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "systems  for  AOL  and  ROL  predictions  was  implemented"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "to validate the proposed framework. The emotion predic-"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "tion results obtained across a range of different configura-"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "tions  clearly \nindicates \nthat \nintegrating  relative  ordinal"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "information  can  significantly \nimprove  absolute  ordinal"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "emotion  prediction.  The  novel  system \nis  compared  to"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "both, a baseline OMSVM system that only models AOLs;"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "and  a  version  of  the  novel  system  that  uses  oracle  ROLs"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "(instead  of  predicted  ROLs)  to  serve  as  an  indication  of"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "the upper bound on the predictive capabilities of the dy-"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "namic  original  Markov  model.  These  experimental  com-"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "parisons  were \ncarried \nout \non \nboth \nIEMOCAP \nand"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "RECOLA  datasets,  and \nin \nterms  of  both  UAR  and"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "weigthed  Kappa.  Consistently, \nin  all  comparisons, \nthe"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "joint  model  outperformed  AOL  only  prediction.  Finally,"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "the  comparison  with  the  version  using  oracle  ROLs  sug-"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "gest  that  improving  the  RankSVM  based  subsystem  can"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "lead to further gains in prediction accuracy."
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": ""
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": ""
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "REFERENCES"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": ""
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "[1] \nV\n. Sethu, J. Epps, and E. Ambikairajah, \"Speech based emotion"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "recognition,\"  in  Speech  and  Audio  Processing  for  Coding,  En-"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "hancement and Recognition: Springer, 2015, pp. 197-228."
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "[2] \nG.  Shashidhar,  K.  Koolagudi,  and  R.  Sreenivasa, \n\"Emotion"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "recognition from speech: a review,\" Springer Science+ Business"
        },
        {
          "Fig. 11.  Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA.": "Media, vol. 15, pp. 99-117, 2012."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "fective Computing  and Intelligent  Interaction  (ACII),  2017,  pp.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "odds models,\" vol. 56, no. 4, pp. 928-942, 2012."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "248-255: IEEE.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[35]  K.-j.  Kim  and  H.  Ahn,  \"A  corporate  credit  rating  model  using"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[17]  N.  Stewart,  G.  D.  Brown,  and  N.  Chater,  \"Absolute  identifica-",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "multi-class  support  vector  machines  with  an  ordinal  pairwise"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "tion by relative judgment,\" vol. 112, no. 4, p. 881, 2005.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "partitioning approach,\" Computers & Operations Research, vol."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[18]  B.  Seymour  and  S.  M.  McClure,  \"Anchors,  scales  and  the  rela-",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "39, no. 8, pp. 1800-1811, 2012."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "tive  coding  of  value  in  the  brain,\"  vol.  18,  no.  2,  pp.  173-178,",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[36]  H. Ahn and K.-J. Kim, \"Corporate credit rating using multiclass"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "2008.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "classification models with order information,\" vol. 5, no. 12, pp."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[19]  G. N. Yannakakis and H. P. MartÃ­nez, \"Ratings are overrated!,\"",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "1783-1788, 2011."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "Frontiers in ICT, vol. 2, p. 13, 2015.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[37]  J. Kwon, K. Choi, and Y. Suh, \"Double Ensemble Approaches to"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[20]  S. Parthasarathy,  R.  Lotfian, and C.  Busso,  \"Ranking  emotional",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "Predicting Firms' Credit Rating,\" in PACIS, 2013, p. 158: Jeju Is-"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "attributes  with  deep  neural  networks,\"  in  2017  IEEE  interna-",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "land."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "tional  conference  on  acoustics,  speech  and  signal  processing",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[38]  L. Cao, L. K. Guan, and Z. Jingqing, \"Bond rating using support"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "(ICASSP), 2017, pp. 4995-4999: IEEE.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "vector  machine,\"  Intelligent  Data  Analysis,  vol.  10,  no.  3,  pp."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[21]  H. Cao, R. Verma, and A. Nenkova, \"Speaker-sensitive emotion",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "285-296, 2006."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "recognition  via  ranking:  Studies  on  acted  and  spontaneous",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[39]  L. R. Rabiner, \"A tutorial on hidden Markov models and select-"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "speech,\" vol. 29, no. 1, pp. 186-202, 2015.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "ed  applications  in  speech  recognition,\"  vol.  77,  no.  2,  pp.  257-"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[22]  R. Lotfian and C. Busso, \"Practical considerations on the use of",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "286, 1989."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "preference learning for ranking emotional speech,\" in 2016 IEEE",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[40]  A. W. Bowman and A. Azzalini, Applied smoothing techniques"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "International Conference  on  Acoustics,  Speech and  Signal  Pro-",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "for data analysis: the kernel approach with S-Plus illustrations."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "cessing (ICASSP), 2016, pp. 5205-5209: IEEE.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "OUP Oxford, 1997."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[23]  Y.-H.  Yang  and  H.  H.  Chen,  \"Ranking-based  emotion  recogni-",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[41]  H.  Peter  D,  \"Kernel  estimation  of  a  distribution  function,\"  vol."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "tion for music organization and retrieval,\" IEEE Transactions on",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "14, no. 3, pp. 605-620, 1985."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "Audio,  Speech,  and  Language  Processing,  vol.  19,  no.  4,  pp.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[42]  G. D. Forney, \"The viterbi algorithm,\" vol. 61, no. 3, pp. 268-278,"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "762-774, 2011.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "1973."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[24]  D.  Melhart,  K.  Sfikas,  G.  Giannakakis,  and  G.  Y.  A.  Liapis,  \"A",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[43]  N. Fuhr, \"Optimum polynomial retrieval functions based on the"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "study  on  affect  model  validity:  Nominal  vs  ordinal  labels,\"  in",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "probability  ranking  principle,\"  ACM  Transactions  on \nInfor-"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "Workshop  on  Artificial  Intelligence  in  Affective  Computing,",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "mation Systems (TOIS), vol. 7, no. 3, pp. 183-204, 1989."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "2020, pp. 27-34: PMLR.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[44]  C.  Cortes  and  V.  Vapnik,  \"Support-vector  networks,\"  Machine"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[25]  H. P. Martinez, G. N. Yannakakis, and J. Hallam, \"Donâ€™t classify",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "learning, vol. 20, no. 3, pp. 273-297, 1995."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "ratings  of  affect;  rank  them!,\"  IEEE  transactions  on  affective",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[45]  T.  Joachims,  \"Optimizing  search  engines  using  clickthrough"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "computing, vol. 5, no. 3, pp. 314-326, 2014.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "data,\" in Proceedings of the eighth ACM SIGKDD international"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[26]  P.  A.  Gutierrez,  M.  Perez-Ortiz,  J.  Sanchez-Monedero,  F.  Fer-",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "conference on Knowledge discovery and data mining, 2002, pp."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "nandez-Navarro,  and  C.  Hervas-Martinez,  \"Ordinal  regression",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "133-142: ACM."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "methods: survey and experimental study,\" IEEE Transactions on",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[46]  M. Gonen, A. G. Tanugur, and E. Alpaydin, \"Multiclass posteri-"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "Knowledge  and  Data  Engineering,  vol.  28,  no.  1,  pp.  127-146,",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "or probability support vector machines,\" vol. 19, no. 1, pp. 130-"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "2016.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "139, 2008."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[27]  I.  H.  Witten,  E.  Frank,  M.  A.  Hall,  and  C.  J.  Pal,  Data  Mining:",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[47]  C.-W.  Hsu  and  C.-J.  Lin,  \"A  comparison  of  methods  for  mul-"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "Practical  machine \nlearning \ntools \nand \ntechniques.  Morgan",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "ticlass  support  vector  machines,\"  IEEE  transactions  on  Neural"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "Kaufmann, 2016.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "Networks, vol. 13, no. 2, pp. 415-425, 2002."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[28]  J.  SÃ¡nchez-Monedero,  P.  A.  GutiÃ©rrez,  P.  TiÅˆo,  and  C.  J.  N.  c.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[48]  A.  J.  Smola  and  B.  SchÃ¶lkopf,  \"A  tutorial  on  support  vector"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "HervÃ¡s-MartÃ­nez,  \"Exploitation  of  pairwise  class  distances  for",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "regression,\" Statistics and computing, vol. 14, no. 3, pp. 199-222,"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "ordinal classification,\" vol. 25, no. 9, pp. 2450-2485, 2013.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "2004."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[29] A. Metallinou, M. Wollmer, A. Katsamanis, F. Eyben, B. Schuller,",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[49]  J. A. Suykens and J. Vandewalle, \"Least squares support vector"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "and  S.  Narayanan,  \"Context-sensitive \nlearning  for  enhanced",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "machine classifiers,\" vol. 9, no. 3, pp. 293-300, 1999."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "audiovisual  emotion  classification,\"  vol.  3,  no.  2,  pp.  184-198,",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[50]  J. Platt, \"Probabilistic outputs for support vector machines and"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "2012.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "comparisons to regularized likelihood methods,\" vol. 10, no. 3,"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[30]  J. C. Kim and M. A. Clements, \"Multimodal affect classification",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "pp. 61-74, 1999."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "at various temporal lengths,\" vol. 6, no. 4, pp. 371-384, 2015.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[51]  F.  Ringeval,  A.  Sonderegger,  J.  Sauer,  and  D.  Lalanne,  \"Intro-"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[31]  M.  Neumann,  \"Cross-lingual  and  multilingual  speech  emotion",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "ducing the RECOLA multimodal corpus of remote collaborative"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "recognition  on  english  and  french,\"  in  2018  IEEE  International",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "and affective interactions,\" in 2013 10th IEEE international con-"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "Conference \non  Acoustics, \nSpeech \nand \nSignal \nProcessing",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "ference and workshops on automatic face and gesture recogni-"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "(ICASSP), 2018, pp. 5769-5773: IEEE.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "tion (FG), 2013, pp. 1-8: IEEE."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[32]  Z. Zhang, F. Ringeval, B. Dong, E. Coutinho, E. Marchi, and B.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[52]  C.  Busso  et  al.,  \"IEMOCAP:  Interactive  emotional  dyadic  mo-"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "SchÃ¼ller,  \"Enhanced  semi-supervised  learning  for  multimodal",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "tion capture database,\" Language resources and evaluation, vol."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "emotion recognition,\" in 2016 IEEE International Conference on",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "42, no. 4, p. 335, 2008."
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "Acoustics,  Speech  and  Signal  Processing  (ICASSP),  2016,  pp.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "[53]  M.  Valstar  et  al.,  \"Avec  2016:  Depression,  mood,  and  emotion"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "5185-5189: IEEE.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "recognition workshop and challenge,\" in Proceedings of the 6th"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[33]  A. Agresti, Categorical data analysis. John Wiley & Sons, 2003.",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "international  workshop  on  audio/visual  emotion  challenge,"
        },
        {
          "of  emotions,\"  in  2017  Seventh  International  Conference  on  Af-": "[34]  J. Verwaeren, W. Waegeman, and B. De Baets, \"Learning partial",
          "ordinal \nclass  memberships  with  kernel-based  proportional": "2016, pp. 3-10."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "SPEECH, 2016, pp. 3598-3602.",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "speech  and  signal  processing  (ICASSP),  2017,  pp.  2367-2371:"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "[57]  M. Neumann and N. T. Vu, \"Improving speech emotion recog-",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "IEEE."
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "nition with unsupervised representation learning on unlabeled",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "Jingyao  WU  received the BE (Hons) degree  in"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "speech,\" in ICASSP 2019-2019 IEEE International Conference on",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "engineering  from  the  University  of  New  South"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "Acoustics,  Speech  and  Signal  Processing  (ICASSP),  2019,  pp.",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "Wales \n(UNSW),  Sydney,  Australia \nin  2020,"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "7390-7394: IEEE.",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "where she is currently working towards the PhD"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "[58]  S. Parthasarathy and C. Busso, \"Preference-learning with quali-",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "degree in signal processing. Her research inter-"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "ests \ninclude  emotion \nrecognition,  multimodal"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "tative  agreement  for  sentence  level  emotional  annotations,\"  in",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "signal  processing  and  machine learning.  She  is"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "Proc. Interspeech, 2018, pp. 252-256.",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "a Student Member of the IEEE and SPS."
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "[59]  O. Verkholyak, D. Fedotov, H. Kaya, Y. Zhang, and A. Karpov,",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "\"Hierarchical Two-level modelling of emotional states in spoken",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "dialog  systems,\"  in  ICASSP  2019-2019  IEEE  International  Con-",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "received \nthe  BE  degree  and \nthe \nTing  Dang"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "ference  on  Acoustics,  Speech  and  Signal  Processing  (ICASSP),",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "MEngSc \ndegree \nin \nsignal \nprocessing \nfrom"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "2019, pp. 6700-6704: IEEE.",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "Northwestern Polytechnical  University, Shaanxi,"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "[60]  W.  Han,  T.  Jiang,  Y.  Li,  B.  Schuller,  and  H.  Ruan,  \"Ordinal",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "China, \nin  2012  and  2015, \nrespectively.  She"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "received the PhD  degree  from  the  University  of"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "Learning  for  Emotion  Recognition  in  Customer  Service  Calls,\"",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "New  South  Wales  (UNSW),  Sydney,  Australia,"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "in  ICASSP 2020-2020  IEEE  International Conference  on Acous-",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "in 2018. She is currently a postdoctoral research"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "tics,  Speech  and  Signal  Processing  (ICASSP),  2020,  pp.  6494-",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "associate  in  Department  of  Computer  Science,"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "6498: IEEE.",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "University  of  Cambridge,  UK.  Her  primary  re-"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "search \ninterests \ninclude  affective  computing,"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "[61]  C.-C.  Lee,  C.  Busso,  S.  Lee,  and  S.  S.  Narayanan,  \"Modeling",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "speech  processing,  audio-based  health  diagnosis,  and  machine"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "mutual  influence  of  interlocutor  emotion  states  in  dyadic  spo-",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "learning techniques. She is a Member of the IEEE, SPS and ISCA."
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "ken  interactions,\"  in  Tenth  Annual  Conference  of  the  Interna-",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "tional Speech Communication Association, 2009.",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "Vidhyasaharan Sethu received his B.E. degree"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "[62]  F. Eyben et al., \"The Geneva minimalistic acoustic parameter set",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "from  Anna  University,  Chennai,  India,  in  2005,"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "his  M.Eng.Sc.  degree  in  signal  processing,  and"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "(GeMAPS)  for  voice  research  and  affective  computing,\"  IEEE",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "his  Ph.D.  degree  in  speech  signal  processing"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "Transactions on Affective Computing, vol. 7, no. 2, pp. 190-202,",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "from \nthe  University \nof  New  South  Wales"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "2016.",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "(UNSW),  Sydney,  Australia,  in  2006  and  2010,"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "[63]  L. Tian, J. Moore, and C. Lai, \"Recognizing emotions in spoken",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "respectively.  He \nis  a  senior \nlecturer  at \nthe"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "School  of  Electrical  Engineering  and  Telecom-"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "dialogue  with  hierarchically \nfused  acoustic  and \nlexical \nfea-",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "munications,  UNSW,  Sydney,  2052,  Australia."
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "tures,\"  in  2016  IEEE  Spoken  Language  Technology  Workshop",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "From  2010  to  2013,  he  was  a  postdoctoral  fel-"
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "(SLT), 2016, pp. 565-572: IEEE.",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": ""
        },
        {
          "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-": "",
          "speech,\"  in  2017  IEEE  international  conference  on  acoustics,": "low  at  the  Speech  Processing  Research  Group,  UNSW.  He  has"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "lenge bridging across audio, video, and physiological data,\" in",
          "vol. 70, no. 4, p. 213, 1968.": "[71]  A.  R.  Gilpin, \n\"Table \nfor \nconversion  of  Kendall's  Tau \nto"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "Proceedings of the 5th international workshop on audio/visual",
          "vol. 70, no. 4, p. 213, 1968.": "Spearman's  Rho  within  the  context  of  measures  of  magnitude"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "emotion challenge, 2015, pp. 3-8.",
          "vol. 70, no. 4, p. 213, 1968.": "of  effect \nfor  meta-analysis,\"  Educational  and  psychological"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[55]  Z. Huang et al., \"An investigation of annotation delay compen-",
          "vol. 70, no. 4, p. 213, 1968.": "measurement, vol. 53, no. 1, pp. 87-92, 1993."
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "sation and output-associative fusion for multimodal continuous",
          "vol. 70, no. 4, p. 213, 1968.": "[72]  I.  StatSoft,  \"Electronic  statistics  textbook,\"  Tulsa,  OK:  StatSoft,"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "emotion  prediction,\"  in  Proceedings  of  the  5th  International",
          "vol. 70, no. 4, p. 213, 1968.": "2013."
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "Workshop on Audio/Visual Emotion Challenge, 2015, pp. 41-48.",
          "vol. 70, no. 4, p. 213, 1968.": "[73]  J. Han, Z. Zhang, F. Ringeval, and B. Schuller, \"Reconstruction-"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[56]  S.  Parthasarathy  and  C.  Busso,  \"Defining  Emotionally  Salient",
          "vol. 70, no. 4, p. 213, 1968.": "error-based \nlearning \nfor  continuous  emotion  recognition \nin"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "Regions  Using  Qualitative  Agreement  Method,\" \nin \nINTER-",
          "vol. 70, no. 4, p. 213, 1968.": "speech,\"  in  2017  IEEE  international  conference  on  acoustics,"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "SPEECH, 2016, pp. 3598-3602.",
          "vol. 70, no. 4, p. 213, 1968.": "speech  and  signal  processing  (ICASSP),  2017,  pp.  2367-2371:"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[57]  M. Neumann and N. T. Vu, \"Improving speech emotion recog-",
          "vol. 70, no. 4, p. 213, 1968.": "IEEE."
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "nition with unsupervised representation learning on unlabeled",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "Jingyao  WU  received the BE (Hons) degree  in"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "speech,\" in ICASSP 2019-2019 IEEE International Conference on",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "engineering  from  the  University  of  New  South"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "Acoustics,  Speech  and  Signal  Processing  (ICASSP),  2019,  pp.",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "Wales \n(UNSW),  Sydney,  Australia \nin  2020,"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "7390-7394: IEEE.",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "where she is currently working towards the PhD"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[58]  S. Parthasarathy and C. Busso, \"Preference-learning with quali-",
          "vol. 70, no. 4, p. 213, 1968.": "degree in signal processing. Her research inter-"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "ests \ninclude  emotion \nrecognition,  multimodal"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "tative  agreement  for  sentence  level  emotional  annotations,\"  in",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "signal  processing  and  machine learning.  She  is"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "Proc. Interspeech, 2018, pp. 252-256.",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "a Student Member of the IEEE and SPS."
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[59]  O. Verkholyak, D. Fedotov, H. Kaya, Y. Zhang, and A. Karpov,",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "\"Hierarchical Two-level modelling of emotional states in spoken",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "dialog  systems,\"  in  ICASSP  2019-2019  IEEE  International  Con-",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "received \nthe  BE  degree  and \nthe \nTing  Dang"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "ference  on  Acoustics,  Speech  and  Signal  Processing  (ICASSP),",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "MEngSc \ndegree \nin \nsignal \nprocessing \nfrom"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "2019, pp. 6700-6704: IEEE.",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "Northwestern Polytechnical  University, Shaanxi,"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[60]  W.  Han,  T.  Jiang,  Y.  Li,  B.  Schuller,  and  H.  Ruan,  \"Ordinal",
          "vol. 70, no. 4, p. 213, 1968.": "China, \nin  2012  and  2015, \nrespectively.  She"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "received the PhD  degree  from  the  University  of"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "Learning  for  Emotion  Recognition  in  Customer  Service  Calls,\"",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "New  South  Wales  (UNSW),  Sydney,  Australia,"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "in  ICASSP 2020-2020  IEEE  International Conference  on Acous-",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "in 2018. She is currently a postdoctoral research"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "tics,  Speech  and  Signal  Processing  (ICASSP),  2020,  pp.  6494-",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "associate  in  Department  of  Computer  Science,"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "6498: IEEE.",
          "vol. 70, no. 4, p. 213, 1968.": "University  of  Cambridge,  UK.  Her  primary  re-"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "search \ninterests \ninclude  affective  computing,"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[61]  C.-C.  Lee,  C.  Busso,  S.  Lee,  and  S.  S.  Narayanan,  \"Modeling",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "speech  processing,  audio-based  health  diagnosis,  and  machine"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "mutual  influence  of  interlocutor  emotion  states  in  dyadic  spo-",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "learning techniques. She is a Member of the IEEE, SPS and ISCA."
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "ken  interactions,\"  in  Tenth  Annual  Conference  of  the  Interna-",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "tional Speech Communication Association, 2009.",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "Vidhyasaharan Sethu received his B.E. degree"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[62]  F. Eyben et al., \"The Geneva minimalistic acoustic parameter set",
          "vol. 70, no. 4, p. 213, 1968.": "from  Anna  University,  Chennai,  India,  in  2005,"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "his  M.Eng.Sc.  degree  in  signal  processing,  and"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "(GeMAPS)  for  voice  research  and  affective  computing,\"  IEEE",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "his  Ph.D.  degree  in  speech  signal  processing"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "Transactions on Affective Computing, vol. 7, no. 2, pp. 190-202,",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "from \nthe  University \nof  New  South  Wales"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "2016.",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "(UNSW),  Sydney,  Australia,  in  2006  and  2010,"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[63]  L. Tian, J. Moore, and C. Lai, \"Recognizing emotions in spoken",
          "vol. 70, no. 4, p. 213, 1968.": "respectively.  He \nis  a  senior \nlecturer  at \nthe"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "School  of  Electrical  Engineering  and  Telecom-"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "dialogue  with  hierarchically \nfused  acoustic  and \nlexical \nfea-",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "munications,  UNSW,  Sydney,  2052,  Australia."
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "tures,\"  in  2016  IEEE  Spoken  Language  Technology  Workshop",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "From  2010  to  2013,  he  was  a  postdoctoral  fel-"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "(SLT), 2016, pp. 565-572: IEEE.",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "low  at  the  Speech  Processing  Research  Group,  UNSW.  He  has"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[64]  F. Eyben, M. WÃ¶llmer, and B. Schuller, \"Opensmile: the munich",
          "vol. 70, no. 4, p. 213, 1968.": "coauthored approximately 100 publications and serves on the edito-"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "rial board of Computer Speech and Language. His research interests"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "versatile  and  fast  open-source  audio  feature  extractor,\"  in  Pro-",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "include  the  application  of  machine  learning  to  speech  processing"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "ceedings  of  the  18th  ACM  international  conference  on  Multi-",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "and  affective  computing,  speaker \nrecognition,  and  computational"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "media, 2010, pp. 1459-1462: ACM.",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "paralinguistics. He is a Member of IEEE."
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[65]  S.  Escalera,  O.  Pujol,  and  P.  Radeva,  \"Separability  of  ternary",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "Eliathamby  Ambikairajah  has  a  B.Sc.  (Eng.)"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "codes for sparse designs of error-correcting output codes,\" vol.",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "(Hons.) degree from University of Sri Lanka and"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "30, no. 3, pp. 285-297, 2009.",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "his Ph.D. degree from Keele University, UK. He"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[66]  O. Chapelle and S. S. Keerthi, \"Efficient algorithms for ranking",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "was  the  Acting  Deputy  Vice-Chancellor  Enter-"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "with SVMs,\" vol. 13, no. 3, pp. 201-215, 2010.",
          "vol. 70, no. 4, p. 213, 1968.": "prise  in  2020  at  the  University  of  New  South"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "Wales, Sydney Australia after previously serving"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[67]  S. S. Keerthi and D. DeCoste, \"A modified finite Newton meth-",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "as the Head of School of Electrical Engineering"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "od for fast solution of large scale linear SVMs,\" vol. 6, no. Mar,",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "and  Telecommunications, \nfrom  2009 \nto  2019."
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "pp. 341-361, 2005.",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "His  previous  career  appointments  also  include"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[68]  O.  J. N.  c. Chapelle,  \"Training  a  support  vector  machine  in the",
          "vol. 70, no. 4, p. 213, 1968.": "Head  of  Electronic  Engineering  and  also  Dean"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "of Engineering at the Athlone Institute of Technology in Ireland from"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "primal,\" vol. 19, no. 5, pp. 1155-1178, 2007.",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "1982  to  1999.  He  was  an  associate  editor  of  IEEE  Transactions  on"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[69]  R.-E.  Fan,  K.-W.  Chang,  C.-J.  Hsieh,  X.-R.  Wang,  and  C.-J.  Lin,",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "Education from 2012 to 2019. His research interests include speaker"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "\"LIBLINEAR: A library for large linear classification,\" vol. 9, pp.",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "",
          "vol. 70, no. 4, p. 213, 1968.": "and  language  recognition,  emotion  detection,  machine  learning  and"
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "1871-1874, 2008.",
          "vol. 70, no. 4, p. 213, 1968.": "cochlear modeling. He is a Senior Member of IEEE."
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "[70]  J. Cohen, \"Weighted kappa: Nominal scale agreement provision",
          "vol. 70, no. 4, p. 213, 1968.": ""
        },
        {
          "[54]  F. Ringeval et al., \"Av+ ec 2015: The first affect recognition chal-": "for scaled disagreement or partial credit,\" Psychological bulletin,",
          "vol. 70, no. 4, p. 213, 1968.": ""
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech based emotion recognition",
      "authors": [
        "V Sethu",
        "J Epps",
        "E Ambikairajah"
      ],
      "year": "2015",
      "venue": "Speech and Audio Processing for Coding, Enhancement and Recognition"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition from speech: a review",
      "authors": [
        "G Shashidhar",
        "K Koolagudi",
        "R Sreenivasa"
      ],
      "year": "2012",
      "venue": "Springer Science+ Business Media"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie"
      ],
      "year": "2001",
      "venue": "Emotion recognition in human-computer interaction"
    },
    {
      "citation_id": "4",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "Conscious emotional experience emerges as a function of multilevel, appraisaldriven response synchronization",
      "authors": [
        "D Grandjean",
        "D Sander",
        "K Scherer"
      ],
      "year": "2008",
      "venue": "Consciousness and cognition"
    },
    {
      "citation_id": "6",
      "title": "Challenges in real-life emotion annotation and machine learning based detection",
      "authors": [
        "L Devillers",
        "L Vidrascu",
        "L Lamel"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "7",
      "title": "Emotions: A general psychoevolutionary theory",
      "authors": [
        "R Plutchik"
      ],
      "year": "1984",
      "venue": "Emotions: A general psychoevolutionary theory"
    },
    {
      "citation_id": "8",
      "title": "Describing the emotional states that are expressed in speech",
      "authors": [
        "R Cowie",
        "R Cornelius"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "9",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "10",
      "title": "Issues in data labelling",
      "authors": [
        "R Cowie",
        "C Cox",
        "J.-C Martin",
        "A Batliner",
        "D Heylen",
        "K Karpouzis"
      ],
      "year": "2011",
      "venue": "Emotion-oriented systems"
    },
    {
      "citation_id": "11",
      "title": "The ordinal nature of emotions: An emerging approach",
      "authors": [
        "G Yannakakis",
        "R Cowie",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Annotation and processing of continuous emotional attributes: Challenges and opportunities",
      "authors": [
        "A Metallinou",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "13",
      "title": "The magical number seven, plus or minus two: Some limits on our capacity for processing information",
      "authors": [
        "G Miller"
      ],
      "year": "1956",
      "venue": "The magical number seven, plus or minus two: Some limits on our capacity for processing information"
    },
    {
      "citation_id": "14",
      "title": "Adaptation-level theory: an experimental and systematic approach to behavior",
      "authors": [
        "H Helson"
      ],
      "year": "1964",
      "venue": "Adaptation-level theory: an experimental and systematic approach to behavior"
    },
    {
      "citation_id": "15",
      "title": "Adaptation level and the affective appraisal of environments",
      "authors": [
        "J Russell",
        "U Lanius"
      ],
      "year": "1984",
      "venue": "Adaptation level and the affective appraisal of environments"
    },
    {
      "citation_id": "16",
      "title": "The ordinal nature Fig. 11. Unweighted average recall (UAR) and weighted kappa (WK) evaluation of different thresholds on valence of RECOLA. of emotions",
      "authors": [
        "G Yannakakis",
        "R Cowie",
        "C Busso"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "17",
      "title": "Absolute identification by relative judgment",
      "authors": [
        "N Stewart",
        "G Brown",
        "N Chater"
      ],
      "year": "2005",
      "venue": "Absolute identification by relative judgment"
    },
    {
      "citation_id": "18",
      "title": "Anchors, scales and the relative coding of value in the brain",
      "authors": [
        "B Seymour",
        "S Mcclure"
      ],
      "year": "2008",
      "venue": "Anchors, scales and the relative coding of value in the brain"
    },
    {
      "citation_id": "19",
      "title": "Ratings are overrated!",
      "authors": [
        "G Yannakakis",
        "H MartÃ­nez"
      ],
      "year": "2015",
      "venue": "Frontiers in ICT"
    },
    {
      "citation_id": "20",
      "title": "Ranking emotional attributes with deep neural networks",
      "authors": [
        "S Parthasarathy",
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "21",
      "title": "Speaker-sensitive emotion recognition via ranking: Studies on acted and spontaneous speech",
      "authors": [
        "H Cao",
        "R Verma",
        "A Nenkova"
      ],
      "year": "2015",
      "venue": "Speaker-sensitive emotion recognition via ranking: Studies on acted and spontaneous speech"
    },
    {
      "citation_id": "22",
      "title": "Practical considerations on the use of preference learning for ranking emotional speech",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Ranking-based emotion recognition for music organization and retrieval",
      "authors": [
        "Y.-H Yang",
        "H Chen"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "24",
      "title": "A study on affect model validity: Nominal vs ordinal labels",
      "authors": [
        "D Melhart",
        "K Sfikas",
        "G Giannakakis",
        "G Liapis"
      ],
      "year": "2020",
      "venue": "Workshop on Artificial Intelligence in Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Don't classify ratings of affect; rank them!",
      "authors": [
        "H Martinez",
        "G Yannakakis",
        "J Hallam"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "26",
      "title": "Ordinal regression methods: survey and experimental study",
      "authors": [
        "P Gutierrez",
        "M Perez-Ortiz",
        "J Sanchez-Monedero",
        "F Fernandez-Navarro",
        "C Hervas-Martinez"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "27",
      "title": "Data Mining: Practical machine learning tools and techniques",
      "authors": [
        "I Witten",
        "E Frank",
        "M Hall"
      ],
      "year": "2016",
      "venue": "Data Mining: Practical machine learning tools and techniques"
    },
    {
      "citation_id": "28",
      "title": "Exploitation of pairwise class distances for ordinal classification",
      "authors": [
        "J SÃ¡nchez-Monedero",
        "P GutiÃ©rrez",
        "P TiÅˆo",
        "C HervÃ¡s-MartÃ­nez"
      ],
      "year": "2013",
      "venue": "Exploitation of pairwise class distances for ordinal classification"
    },
    {
      "citation_id": "29",
      "title": "Context-sensitive learning for enhanced audiovisual emotion classification",
      "authors": [
        "A Metallinou",
        "M Wollmer",
        "A Katsamanis",
        "F Eyben",
        "B Schuller",
        "S Narayanan"
      ],
      "year": "2012",
      "venue": "Context-sensitive learning for enhanced audiovisual emotion classification"
    },
    {
      "citation_id": "30",
      "title": "Multimodal affect classification at various temporal lengths",
      "authors": [
        "J Kim",
        "M Clements"
      ],
      "year": "2015",
      "venue": "Multimodal affect classification at various temporal lengths"
    },
    {
      "citation_id": "31",
      "title": "Cross-lingual and multilingual speech emotion recognition on english and french",
      "authors": [
        "M Neumann"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Enhanced semi-supervised learning for multimodal emotion recognition",
      "authors": [
        "Z Zhang",
        "F Ringeval",
        "B Dong",
        "E Coutinho",
        "E Marchi",
        "B SchÃ¼ller"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "Categorical data analysis",
      "authors": [
        "A Agresti"
      ],
      "year": "2003",
      "venue": "Categorical data analysis"
    },
    {
      "citation_id": "34",
      "title": "Learning partial ordinal class memberships with kernel-based proportional odds models",
      "authors": [
        "J Verwaeren",
        "W Waegeman",
        "B Baets"
      ],
      "year": "2012",
      "venue": "Learning partial ordinal class memberships with kernel-based proportional odds models"
    },
    {
      "citation_id": "35",
      "title": "A corporate credit rating model using multi-class support vector machines with an ordinal pairwise partitioning approach",
      "authors": [
        "K.-J Kim",
        "H Ahn"
      ],
      "year": "2012",
      "venue": "Computers & Operations Research"
    },
    {
      "citation_id": "36",
      "title": "Corporate credit rating using multiclass classification models with order information",
      "authors": [
        "H Ahn",
        "K.-J Kim"
      ],
      "year": "2011",
      "venue": "Corporate credit rating using multiclass classification models with order information"
    },
    {
      "citation_id": "37",
      "title": "Double Ensemble Approaches to Predicting Firms' Credit Rating",
      "authors": [
        "J Kwon",
        "K Choi",
        "Y Suh"
      ],
      "year": "2013",
      "venue": "PACIS"
    },
    {
      "citation_id": "38",
      "title": "Bond rating using support vector machine",
      "authors": [
        "L Cao",
        "L Guan",
        "Z Jingqing"
      ],
      "year": "2006",
      "venue": "Intelligent Data Analysis"
    },
    {
      "citation_id": "39",
      "title": "A tutorial on hidden Markov models and selected applications in speech recognition",
      "authors": [
        "L Rabiner"
      ],
      "year": "1989",
      "venue": "A tutorial on hidden Markov models and selected applications in speech recognition"
    },
    {
      "citation_id": "40",
      "title": "Applied smoothing techniques for data analysis: the kernel approach with S-Plus illustrations",
      "authors": [
        "A Bowman",
        "A Azzalini"
      ],
      "year": "1997",
      "venue": "Applied smoothing techniques for data analysis: the kernel approach with S-Plus illustrations"
    },
    {
      "citation_id": "41",
      "title": "Kernel estimation of a distribution function",
      "authors": [
        "H Peter"
      ],
      "year": "1985",
      "venue": "Kernel estimation of a distribution function"
    },
    {
      "citation_id": "42",
      "title": "The viterbi algorithm",
      "authors": [
        "G Forney"
      ],
      "year": "1973",
      "venue": "The viterbi algorithm"
    },
    {
      "citation_id": "43",
      "title": "Optimum polynomial retrieval functions based on the probability ranking principle",
      "authors": [
        "N Fuhr"
      ],
      "year": "1989",
      "venue": "ACM Transactions on Information Systems (TOIS)"
    },
    {
      "citation_id": "44",
      "title": "Support-vector networks",
      "authors": [
        "C Cortes",
        "V Vapnik"
      ],
      "year": "1995",
      "venue": "Machine learning"
    },
    {
      "citation_id": "45",
      "title": "Optimizing search engines using clickthrough data",
      "authors": [
        "T Joachims"
      ],
      "year": "2002",
      "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining"
    },
    {
      "citation_id": "46",
      "title": "Multiclass posterior probability support vector machines",
      "authors": [
        "M Gonen",
        "A Tanugur",
        "E Alpaydin"
      ],
      "year": "2008",
      "venue": "Multiclass posterior probability support vector machines"
    },
    {
      "citation_id": "47",
      "title": "A comparison of methods for multiclass support vector machines",
      "authors": [
        "C.-W Hsu",
        "C.-J Lin"
      ],
      "year": "2002",
      "venue": "IEEE transactions on Neural Networks"
    },
    {
      "citation_id": "48",
      "title": "A tutorial on support vector regression",
      "authors": [
        "A Smola",
        "B SchÃ¶lkopf"
      ],
      "year": "2004",
      "venue": "Statistics and computing"
    },
    {
      "citation_id": "49",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "J Suykens",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "Least squares support vector machine classifiers"
    },
    {
      "citation_id": "50",
      "title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods",
      "authors": [
        "J Platt"
      ],
      "year": "1999",
      "venue": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods"
    },
    {
      "citation_id": "51",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "52",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "53",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th international workshop on audio/visual emotion challenge"
    },
    {
      "citation_id": "54",
      "title": "Av+ ec 2015: The first affect recognition challenge bridging across audio, video, and physiological data",
      "year": "2015",
      "venue": "Proceedings of the 5th international workshop on audio/visual emotion challenge"
    },
    {
      "citation_id": "55",
      "title": "An investigation of annotation delay compensation and output-associative fusion for multimodal continuous emotion prediction",
      "authors": [
        "Z Huang"
      ],
      "year": "2015",
      "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "56",
      "title": "Defining Emotionally Salient Regions Using Qualitative Agreement Method",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2016",
      "venue": "INTER-SPEECH"
    },
    {
      "citation_id": "57",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "Preference-learning with qualitative agreement for sentence level emotional annotations",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "59",
      "title": "Hierarchical Two-level modelling of emotional states in spoken dialog systems",
      "authors": [
        "O Verkholyak",
        "D Fedotov",
        "H Kaya",
        "Y Zhang",
        "A Karpov"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "60",
      "title": "Ordinal Learning for Emotion Recognition in Customer Service Calls",
      "authors": [
        "W Han",
        "T Jiang",
        "Y Li",
        "B Schuller",
        "H Ruan"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "61",
      "title": "Modeling mutual influence of interlocutor emotion states in dyadic spoken interactions",
      "authors": [
        "C.-C Lee",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "Tenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "62",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "F Eyben"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "63",
      "title": "Recognizing emotions in spoken dialogue with hierarchically fused acoustic lexical features",
      "authors": [
        "L Tian",
        "J Moore",
        "C Lai"
      ],
      "year": "2016",
      "venue": "2016 IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "64",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M WÃ¶llmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "65",
      "title": "Separability of ternary codes for sparse designs of error-correcting output codes",
      "authors": [
        "S Escalera",
        "O Pujol",
        "P Radeva"
      ],
      "year": "2009",
      "venue": "Separability of ternary codes for sparse designs of error-correcting output codes"
    },
    {
      "citation_id": "66",
      "title": "Efficient algorithms for ranking with SVMs",
      "authors": [
        "O Chapelle",
        "S Keerthi"
      ],
      "year": "2010",
      "venue": "Efficient algorithms for ranking with SVMs"
    },
    {
      "citation_id": "67",
      "title": "A modified finite Newton method for fast solution of large scale linear SVMs",
      "authors": [
        "S Keerthi",
        "D Decoste"
      ],
      "year": "2005",
      "venue": "A modified finite Newton method for fast solution of large scale linear SVMs"
    },
    {
      "citation_id": "68",
      "title": "Training a support vector machine in the primal",
      "authors": [
        "O Chapelle"
      ],
      "year": "2007",
      "venue": "Training a support vector machine in the primal"
    },
    {
      "citation_id": "69",
      "title": "LIBLINEAR: A library for large linear classification",
      "authors": [
        "R.-E Fan",
        "K.-W Chang",
        "C.-J Hsieh",
        "X.-R Wang",
        "C.-J Lin"
      ],
      "year": "2008",
      "venue": "LIBLINEAR: A library for large linear classification"
    },
    {
      "citation_id": "70",
      "title": "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit",
      "authors": [
        "J Cohen"
      ],
      "year": "1968",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "71",
      "title": "Table for conversion of Kendall's Tau to Spearman's Rho within the context of measures of magnitude of effect for meta-analysis",
      "authors": [
        "A Gilpin"
      ],
      "year": "1993",
      "venue": "Educational and psychological measurement"
    },
    {
      "citation_id": "72",
      "title": "Electronic statistics textbook",
      "authors": [
        "I Statsoft"
      ],
      "year": "2013",
      "venue": "Electronic statistics textbook"
    },
    {
      "citation_id": "73",
      "title": "Reconstructionerror-based learning for continuous emotion recognition in speech",
      "authors": [
        "J Han",
        "Z Zhang",
        "F Ringeval",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing"
    }
  ]
}