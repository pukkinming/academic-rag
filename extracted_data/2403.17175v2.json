{
  "paper_id": "2403.17175v2",
  "title": "Engagement Measurement Based On Facial Landmarks And Spatial-Temporal Graph Convolutional Networks",
  "published": "2024-03-25T20:43:23Z",
  "authors": [
    "Ali Abedi",
    "Shehroz S. Khan"
  ],
  "keywords": [
    "Engagement Measurement",
    "Graph Convolutional Network",
    "Ordinal Classification",
    "Transfer Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Engagement in virtual learning is crucial for a variety of factors including student satisfaction, performance, and compliance with learning programs, but measuring it is a challenging task. There is therefore considerable interest in utilizing artificial intelligence and affective computing to measure engagement in natural settings as well as on a large scale. This paper introduces a novel, privacy-preserving method for engagement measurement from videos. It uses facial landmarks, which carry no personally identifiable information, extracted from videos via the MediaPipe deep learning solution. The extracted facial landmarks are fed to Spatial-Temporal Graph Convolutional Networks (ST-GCNs) to output the engagement level of the student in the video. To integrate the ordinal nature of the engagement variable into the training process, ST-GCNs undergo training in a novel ordinal learning framework based on transfer learning. Experimental results on two video student engagement measurement datasets show the superiority of the proposed method compared to previous methods with improved state-of-the-art on the En-gageNet dataset with a 3.1% improvement in four-class engagement level classification accuracy and on the Online Student Engagement dataset with a 1.5% improvement in binary engagement classification accuracy. Gradient-weighted Class Activation Mapping (Grad-CAM) was applied to the developed ST-GCNs to interpret the engagement measurements obtained by the proposed method in both the spatial and temporal domains. The relatively lightweight and fast ST-GCN and its integration with the real-time MediaPipe make the proposed approach capable of being deployed on virtual learning platforms and measuring engagement in real-time.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Engagement is key in education, blending students' attention and interest within a learning context  [1] . It not only stems from existing interests but also fosters new ones through sustained attention  [2] , essential for content comprehension and engagement development  [1] . However, measuring and upholding engagement is challenging, and demands significant effort from educators. The advent of remote sensing, Artificial Intelligence (AI), and affective computing offers new avenues for accurately measuring engagement across various learning environments, including virtual learning platforms. Technologies such as facial expression recognition  [3]  and eye gaze tracking  [4]  enable more precise monitoring and enhancement of student engagement. This paper explores the development of AI algorithms for automated engagement measurement in virtual learning, taking into account the definition of engagement in educational psychology and its measurement methodologies.\n\nFredricks et al.  [5]  defined engagement through affective, behavioral, and cognitive components. Affective engagement relates to students' emotions and attitudes towards their tasks, including interest or feelings during a learning class  [6] . Behavioral engagement refers to active participation, such as focusing on a computer screen or not being distracted by a phone  [7] . Cognitive engagement deals with a student's dedication to learning and tackling challenges, affecting outcomes such as recall and understanding  [1] . Booth et al.  [1]  identified that engagement is measured through various signals such as facial expressions, eye movements, posture, heart rate, brain activity, audio, and digital interactions. Cameras, prevalent in devices for online learning, make video a key data modality for AI-based engagement measurement  [8, 9] . Therefore, AI techniques predominantly focus on video to measure student engagement  [8] [9] [10] .\n\nAI-driven engagement measurement methods are divided into end-to-end and feature-based approaches. While feature-based methods generally outperform end-to-end approaches, which process raw videos, they require extensive identification of effective features through trial and error  [8] [9] [10] . Extracting multimodal features from videos with multiple computer vision algorithms or neural networks  [11, 12] , and their subsequent analysis by deep neural networks to measure engagement, render these methods computationally demanding  [8, 13] . Such computational requirements limit their use on local devices and necessitate the transfer of privacy-sensitive video data to cloud servers for engagement measurement. In contrast, extracting low-dimensional landmark information, such as facial and hand landmarks, not only provides more compact data but also captures essential geometric features for affect and behavior analysis, including engagement, without personal identifiers  [14] [15] [16] . Previous feature-based engagement measurement approaches extracted features such as head pose, facial Action Units (AUs), iris and gaze features, and affect and emotion features, which are indicators of the behavioral and affective components of engagement  [8] [9] [10] . The literature has demonstrated the success of facial landmarks in capturing these aforementioned features  [3, [17] [18] [19] [20] . A model such as Spatial-Temporal Graph Convolutional Networks (ST-GCNs)  [21] , capable of analyzing spatial-temporal facial landmarks and learning these aforementioned features inherently, can reduce the need for raw facial videos. An approach based on facial landmarks prioritizes privacy and reduces computational demands, making it more practical for real-time engagement measurement.\n\nIn this paper, an alternative course away from conventional end-to-end and feature-based approaches is charted, and a novel engagement measurement technique using facial landmarks extracted from videos is presented. The proposed method is characterized by its privacy-preserving nature and computational efficiency. This work makes the following contributions:\n\n-This marks the first instance in video-based engagement measurement  [8, 9, 13, [22] [23] [24] [25]  where facial landmarks, as the single data modality extracted from videos, are analyzed through ST-GCNs  [21]  to infer the engagement level in the video; -To integrate the ordinal nature of the engagement variable into the training process, ST-GCNs undergo training in a novel ordinal learning framework utilizing transfer learning; -Extensive experiments conducted on two video-based engagement measurement datasets demonstrate the superiority of the proposed method over previous methods, achieving an improved state-of-the-art in engagement level classification accuracy. For explainability, Gradient-weighted Class Activation Mapping (Grad-CAM) is applied to the developed ST-GCNs to interpret the engagement measurements obtained by the proposed method in both spatial and temporal domains.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "The literature review in this paper serves two main purposes: discussing past video-based engagement measurement techniques and examining the use of graph convolutional networks for facial expression and affect analysis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Engagement Measurement",
      "text": "Karimah et al.  [8]  conducted a systematic review on measuring student engagement in virtual learning environments, revealing a focus on affective and behavioral components of engagement. Most methods utilize datasets annotated by external observers  [10]  to train both feature-based and end-to-end models.\n\nIn the following, some of the relevant feature-based and end-to-end works on video-based engagement measurement are discussed.\n\nIn the domain of end-to-end engagement measurement techniques, deep neural networks analyze consecutive raw video frames to output the engagement level of the student in the video. These methods do not employ the extraction of handcrafted features from the videos; instead, the network is adept at autonomously learning to extract the most useful features directly from the videos, utilizing consecutive convolutional layers. The deep neural networks implemented in these end-to-end approaches include networks capable of video analysis, such as 3D Convolutional Neural Networks (3D CNNs) and Video Transformers  [26] [27] [28] , as well as combinations of 2D CNNs with sequential neural networks such as Long Short-Term Memory (LSTM) and Temporal Convolutional Network (TCN)  [26, 27, 29, 30] .\n\nThe process of measuring engagement through feature-based techniques involves two stages. Initially, behavioral and affective features are extracted from video frames, relying on either domain-specific knowledge or pre-trained models for facial embedding extraction. OpenFace  [31]  is notably effective for its comprehensive feature extraction capabilities, including AUs, eye movements, gaze direction, and head positioning, and is widely applied in engagement measurement  [12, 13, 32, 33] . Examples of facial embedding include the Masked Autoencoder for facial video Representation LearnINg (MARLIN)  [11] , utilized by Singh et al.  [12] , and the Emotion Face Alignment Network (EmoFAN)  [19] , used by Abedi et al.  [13] . Subsequently, to analyze these extracted features and infer engagement, various machine learning and deep learning models are employed, such as Bag-of-Words (BoW)  [32] , Recurrent Neural Network (RNN) variations  [24] , Temporal Convolutional Networks (TCNs)  [13, 33] , Transformers  [12, 34] , and ensemble models  [35] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Graph-Based Facial Affect And Expression Analysis",
      "text": "Liu et al.  [3]  conducted a comprehensive review of the literature on graph-based methods for facial affect analysis. These methods typically take an image or a sequence of images as input and produce an affect classification or regression as output. Based on their review, Liu et al.  [3]  proposed a pipeline for graph-based facial affect analysis, which includes (i) face preprocessing, (ii) graph-based affective representation, and (iii) graph-based relational reasoning. Preprocessing involves steps such as face detection and registration. Graph-based affective representation involves defining the structure of the graph, i.e., nodes and edges. The graph structure can be spatial or spatiotemporal depending on whether the input data is still images or videos. The graph structure can be at the landmark level, region level, or AU level, with nodes representing facial landmarks, facial regions of interest, or facial AUs, respectively. In the relational reasoning step, the edges, nodes, their interrelations, and temporal dynamics are analyzed through relational reasoning machine-learning or deep-learning models to make inferences regarding affect. Models used to analyze graph data include dynamic Bayesian networks, RNNs, CNNs, fully-connected neural networks, and nontemporal and temporal graph neural networks  [3] .\n\nZhou et al.  [36]  proposed a facial expression recognition method based on spatiotemporal landmark-level and region-level graphs. The intra-frame graph is formed by the connections among thirty-four facial landmarks located around the eyes, lips, and cheeks. The definition of these intra-frame connections was done manually. Inter-frame connections were established by linking each node in one frame to its corresponding node in the following frame. Two parallel ST-GCNs with analogous structures were trained; one on the nodes' x-and y-coordinates, and another on their histogram of orientation features. ST-GCNs' outputs were concatenated and processed by a fully connected network to identify facial expressions. This method's disadvantages include independent training of the two ST-GCNs rather than joint learning, and manual definition of nodes and edges.\n\nWei et al.  [15]  presented a graph-based method for micro-expression recognition in video. The method included a dual-stream ST-GCN, focusing on the x and y coordinates of facial landmarks and the distance and angles between adjacent facial landmarks. An AU-specific loss function was incorporated into the neural network's training process in order to incorporate the association between AUs and micro-expressions. Their methodology employed three different sets of facial landmarks as graph nodes, comprising sets with 14, 31, and Dlib's  [37]  68 facial landmarks. Notably, the set with 68 landmarks was the only one to include landmarks along the jawline. The experiments demonstrated the best results for micro-expression recognition when 14 facial landmark sets were used.\n\nLeong et al.  [25]  introduced a method employing spatial-temporal graph attention networks to analyze facial landmarks and head poses, aimed at identifying academic emotions. The facial landmarks used were those around the eyebrow, eye, nose, and mouth excluding those that outline the face's outer shape, with the reasoning being they lack correlation with affective states. A notable limitation of this approach is its reliance on multiple deep neural networks for extracting features, i.e., facial landmarks and head pose. The method achieved lower academic emotion detection accuracy when compared to prior feature-based methods  [13] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discussion",
      "text": "As reviewed in this section, while some methods have used facial landmarks and ST-GCNs to recognize facial affect and expression in videos, there has been no exploration of the use of these methods to measure engagement. The fundamental differences between engagement and facial affect and expression make their measurement different. First, engagement is a multi-component state comprising behavioral, affective, and cognitive components. To illustrate, key indicators of behavioral engagement, such as being off-task or on-task, are determined by head pose, eye gaze, and blink rate. These indicators, and therefore engagement, cannot be effectively measured using methods designed solely for facial affect analysis. Second, engagement is not a constant state; it varies over time and should be measured at specific time resolutions where it remains stable and quantifiable. An ideal measurement duration for engagement is between ten and forty seconds, which is longer than the time resolution for facial affect analysis, which sometimes occurs at the frame level. Third, engagement measurement could involve recognizing an ordinal variable that indicates levels of engagement, as opposed to facial expression recognition, which identifies categorical variables without inherent order.\n\nExisting engagement measurement approaches face limitations due to the necessity of employing multiple deep neural networks for the extraction and analysis of multi-modal features. Coupled with the significant differences between facial affect analysis and engagement measurement as outlined above, these limitations underscore a gap in the field. In response, we introduce a straightforward yet effective method for engagement measurement. This method is lightweight and fast, preserving privacy while also demonstrating improvements over current methodologies across two video-based engagement measurement datasets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Method",
      "text": "The input to the proposed method is a video sample of a student seated in front of a laptop or PC camera during a virtual learning session. The sequences of facial landmarks extracted from consecutive video frames through MediaPipe  [14, 20]  are analyzed by ST-GCN  [16, 21]  to output the engagement level of the student in the video.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Graph-Based Representation",
      "text": "The MediaPipe deep learning solution  [14] , a framework that is both real-time and cross-platform, is employed for the extraction of facial landmarks from video. Incorporated within MediaPipe, Attention Mesh  [20]  is adept at detecting 468 3D facial landmarks throughout the face and an extra 10 landmarks for the iris. However, not all 478 landmarks are employed in the proposed engagement measurement method. Consistent with existing studies  [3] , only 68 of the 3D facial landmarks, which match those identified by the Dlib framework  [37] , in addition to the 10 3D iris landmarks, making a total of 78 landmarks, are utilized.\n\nThe 3D facial landmarks encapsulate crucial spatial-temporal information pertinent to head pose  [17] , AUs  [18] , eye gaze  [20] , and affect  [3, 19] -key features identified in prior engagement measurement studies  [8-10, 13, 22-24, 26, 27, 33] . Consequently, there is no necessity for extracting additional handcrafted features from the video frames.\n\nThe N 3D facial landmarks extracted from T consecutive video frames are utilized to construct a spatiotemporal graph G = (V, E). In this graph, the set of nodes V = {v ti |t = 1, . . . , T, i = 1, . . . , N } encompasses all the facial landmarks in a sequence. To construct G, first, the facial landmarks within one frame are connected with edges according to a connectivity structure based on Delaunay triangulation  [38]  which is consistent with true facial muscle distribution and uniform for different subjects  [3] . Then each landmark will be connected to the same landmark in the consecutive frame.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Graph-Based Reasoning",
      "text": "Based on the spatiotemporal graph of facial landmarks G = (V, E) constructed above, an adjacency matrix A is defined as an N ×N matrix where the element at position (i, j) is set to 1 if there is an edge connecting the i th and j th landmarks, and set to 0 otherwise. An identity matrix I, of the same dimensions as A, is created to represent self-connections. A spatial-temporal graph, as the basic element of an ST-GCN layer, is implemented as follows  [21] .\n\nwhere\n\nM is a learnable weight matrix that enables scaling the contributions of a node's feature to its neighboring nodes  [21] . The input feature map, denoted f in , is the raw coordinates of facial landmarks for the first layer of ST-GCN, and it represents the outputs from previous layers in subsequent layers of ST-GCN. The dimensionality of f in is (C, N, T ), where C\n\nis the number of channels, for example, 3 in the initial input to the network corresponding to the x, y, and z coordinates of the facial landmarks. In each layer of ST-GCN, initially, the spatial (intra-frame) convolution is applied to f in based on the weight matrix W spatial , utilizing a standard 2D convolution with a kernel size of 1 × 1. Subsequently, the resulting tensor is multiplied by the normalized adjacency matrix Λ -1 2 ((A+I)⊙M )Λ -1 2 across the spatial dimension. Afterward, the temporal (inter-frame) convolution, based on the weight matrix W temporal , is applied to the tensor output from the spatial convolution. This convolution is a standard 2D convolution with a kernel size of 1 × Γ , where Γ signifies the temporal kernel size.\n\nFollowing an adequate number of ST-GCN layers, specifically three in this work, that perform spatial-temporal graph convolutions as outlined above, the resulting tensor undergoes 2D average pooling. The final output of the network is generated by a final 2D convolution. This convolution employs a kernel size of 1 × 1 and features an output channel dimensionality equal to the number of classes K, i.e., the number of engagement levels to be measured. An explanation of the detailed architecture of the ST-GCNs for specific datasets can be found in subsection 4.2.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ordinal Engagement Classification Through Transfer Learning",
      "text": "The model described above, with a final layer comprising K output channels, tackles the engagement measurement problem as a categorical K-class classification problem without taking into account the ordinal nature of the engagement variable  [10, 39, 40] . The model could harness the ordinality of the engagement variable to enhance its inferences. Drawing inspiration from  [13, 41] , a novel ordinal learning framework based on transfer learning is introduced as follows.\n\nTraining phase-The original K-level ordinal labels, y = 0, 1, . . . , K -1, in the training set are converted into K -1 binary labels y i as follows: if y > i, then y i = 1; otherwise, y i = 0, for i = 0, 1, . . . , K -2. Subsequently, K -1 binary classifiers are trained with the training set and the K -1 binary label sets described above. The training of binary classifiers is based on transfer learning, which proceeds as follows: Initially, a network is trained on the dataset with the original K-class labels, employing a regular final layer with K output channels. After training, the ST-GCN layers of this network are frozen, and the final layer is removed. To this frozen network, K -1 separate untrained 2D convolution layers with a single output channel each are added, resulting in K -1 new networks. Each of these networks consists of a frozen sequence of ST-GCN layers followed by an untrained 2D convolution layer. These K -1 new networks are then trained on the entire dataset using the K -1 binary label sets described above. During this phase, only the final 2D convolution layers are subjected to training. Inference phase-For the ordinal classification of a test sample, the sample is initially input into the pre-trained (and frozen) sequence of ST-GCN layers, followed by a 2D average pooling layer. The tensor obtained from this process is then input into K -1 pre-trained final 2D convolution layers, each yielding a probability estimate for the test sample being in the binary class y t = y i , where i = 0, 1, . . . , K -2. Subsequently, these K -1 binary probability estimates are transformed into a single multi-class probability of the sample belonging to class y = 0, 1, . . . , K -1, as follows  [41] .\n\nDespite the increased training time for the ordinal model within the aforementioned ordinal learning framework, the final count of parameters in the ordinal model remains nearly identical to that of the original non-ordinal model.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Experiments",
      "text": "This section evaluates the performance of the proposed method relative to existing methods in video-based engagement measurement. It reports and discusses the results of multi-class and binary classification of engagement across two datasets. Based on the engagement measurement problem at hand, several evaluation metrics are employed. In the context of multi-class engagement level classification, metrics such as accuracy and confusion matrix are reported. For binary engagement classification, accuracy, the Area Under the Curve of the Receiver Operating Characteristic (AUC-ROC), and the Area Under the Curve of the Precision and Recall curve (AUC-PR) are utilized. In addition, the number of parameters of the models used, memory consumption, and inference time of the proposed method are compared to those of previous methods.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Datasets",
      "text": "Experiments on two large video-based engagement measurement datasets were conducted, each presenting unique challenges that further enabled the validation of the proposed method. EngageNet: The EngageNet dataset  [12] , recognized as the largest dataset for student engagement measurement, includes video recordings of 127 subjects participating in virtual learning sessions. Each video sample has a duration of 10 seconds, with a frame rate of 30 fps and a resolution of 1280 × 720 pixels. The subjects' video recordings were annotated as four ordinal levels of engagement: Not-Engaged, Barely-Engaged, Engaged, and Highly-Engaged. The dataset was divided into 7983, 1071, and 2257 samples for training, validation, and testing, respectively, using a subject-independent data split approach  [12] . However, only the training and validation sets were made available by the dataset creators and were utilized for the training and validation of predictive models in the experiments presented in this paper. The distribution of samples in the four aforementioned classes of engagement in the training and validation sets are 1550, 1035, 1658, and 3740, and 132, 97, 273, and 569, respectively. Online SE: The Online SE dataset  [42] , comprises videos of six students participating in online courses via the Zoom platform. These recordings span 10 seconds each, with a frame rate of 24 fps and a resolution of 220 × 155 pixels. The videos were annotated as either Not-Engaged or Engaged. The dataset was segmented into 3190, 1660, and 1290 samples for training, validation, and testing, respectively. The distribution of samples in the two aforementioned classes of engagement in the training, validation, and test sets is 570 and 2620, 580 and 1080, and 570 and 720, respectively.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Experimental Setting",
      "text": "The sole information extracted from video frames is facial landmarks, which are analyzed by ST-GCN to determine the engagement level of the student in the video. Drawing inspiration from the pioneering works on body-joints-based action analysis  [21, 43] , the proposed ST-GCN for facial-landmarks-based engagement measurement is structured as follows. The input facial landmarks are first processed through a batch normalization layer, followed by three consecutive ST-GCN layers with 64, 128, and 256 output channels, respectively. Residual connections are incorporated in the last two ST-GCN layers. A dropout rate of 0.1 is applied to each ST-GCN layer. The temporal kernel size in the ST-GCN layers is selected to be 9. Subsequent to the ST-GCN layers, an average pooling layer is utilized, and its resulting tensor is directed into a 2D convolutional layer with 256 input channels and a number of output channels corresponding to the number of classes. A Softmax activation function then computes the probability estimates. In cases where engagement measurement is framed as a binary classification task, the terminal 2D convolution layer is configured with a single output channel, substituting Softmax with a Sigmoid function. The Sigmoid function is also employed for the individual binary classifiers within the ordinal learning framework detailed in subsection 3.3. The models are trained using the Adam optimizer with mini-batches of size 16 and an initial learning rate of 0.001 for 300 epochs. The learning rate is decayed by a factor of 0.1 every 100 epochs.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Results",
      "text": "Comparison to Previous Methods Table  1  presents the comparative results of two settings of the proposed method, a regular non-ordinal classifier and an ordinal classifier, with previous methods on the validation set of the EngageNet dataset  [12] . The engagement measurement in EngageNet  [12]  is a four-class classification problem and the accuracy is reported as the evaluation metric. The previous methods in Table  1  include state-of-the-art end-to-end methods, including the combination of ResNet-50 with TCN  [27]  and the combination of EfficientNet B7 with LSTM and bidirectional LSTM  [30] , followed by stateof-the-art feature-based methods. The previous feature-based methods in Table  1  used different combinations of OpenFace's eye gaze, head pose, and AU features  [31]  along with MARLIN's facial embedding features  [11] . These features were classified by LSTM, CNN-LSTM, TCN, and Transformer. Refer to  [12]  for more details. The results of the feature-based method proposed by Vedernikov et al.  [34]  are also reported, where the aforementioned features are classified using a Transformer-based neural network called the Tensor-Convolution and Convolution-Transformer Network (TCCT-Net).\n\nTable  1 : Classification accuracy of engagement levels on the validation set of the EngageNet dataset  [12] : comparison of state-of-the-art end-to-end methods and feature-based methods with various feature sets and classification models against two configurations of the proposed method -facial landmarks analyzed by ST-GCN and ordinal ST-GCN. Bolded values denote the best results.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ref.",
      "text": "Features Model Accuracy  [27]  End to End Model ResNet + TCN 0.5472  [30]  End to End Model EfficientNet + LSTM 0.5757  [30]  End to End Model EfficientNet + Bi-LSTM 0.5894  [12]  Gaze LSTM 0.6125  [12]  Head Pose LSTM 0.6760  [12]  AU LSTM 0.6303  [12]  Gaze + Head Pose LSTM 0.6769  [12]  Gaze + Head Pose + AU LSTM 0.6704  [12]  Gaze CNN-LSTM 0.6060  [12]  Head Pose CNN-LSTM 0.6732  [12]  AU CNN-LSTM 0.6172  [12]  Gaze + AU CNN-LSTM 0.6275  [12]  Head Pose + AU CNN-LSTM 0.6751  [12]  Gaze + Head Pose + AU CNN-LSTM 0.6751  [12]  Gaze TCN 0.6256  [12]  Head Pose TCN 0.6611  [12]  AU TCN 0.6293  [12]  Gaze + Head Pose + AU TCN 0.6779  [12]  Gaze Transformer 0.5545  [12]  Gaze + Head Pose Transformer 0.6445  [12]  Gaze + Head Pose + AU Transformer 0.6910  [12]  Gaze + Head Pose + AU + MARLIN Transformer 0.6849  [34]  Gaze TCCT-Net 0.6433  [34]  Head Pose TCCT-Net 0.6891  [34]  AU TCCT-Net 0.6629  [34]  Gaze + Head Pose TCCT-Net 0.6564  [34]  Gaze + Head Pose + AU TCCT-Net 0.6713 Ours Facial Landmarks ST-GCN 0.6937 Ours Facial Landmarks Ordinal ST-GCN 0.7124\n\nDespite the abundant data samples in the EngageNet dataset  [12]  available for training complex neural networks such as ResNet + TCN  [27]  and Effi-cientNet B7 + bidirectional LSTM  [30] , their performance is inferior to that of feature-based methods. This highlights the necessity of extracting hand-crafted features or facial landmarks from videos and building classifiers on top of them.\n\nIn the single feature configurations of previous methods in Table  1 , head pose achieves better results compared to AUs, which is better than eye gaze. While head pose and eye gaze are indicators of behavioral engagement  [7, 13] , AUs, which are associated with facial expressions and affect  [19] , are indicators of affective engagement. Combining these three features is always beneficial since engagement is a multi-component variable that can be measured by affective and behavioral indicators when the only available data is video  [13] . Among classification models, utilizing more advanced models improves accuracy; the Transformer is better than TCN, which is better than CNN-LSTM and LSTM; however, it comes at the cost of increased computational complexity. For single features, the Transfomer-based TCCT-Net  [34]  outperforms other classifiers; however, for multiple feature sets, the vanilla Transformer  [12]  outperforms the others.\n\nThe proposed ST-GCN in Table  1 , which relies solely on facial landmarks without requiring raw facial videos or multiple hand-crafted features, outperforms previous methods. Moreover, making the proposed method ordinal further improves the state-of-the-art by 3.1% compared to eye gaze, head pose, AUs, and MARLIN features  [11]  with the Transformer  [12] . Tables  2a  and 2b  depict the confusion matrices of the ordinal and non-ordinal configurations of the proposed method in Table  1 . As shown, incorporating ordinality significantly increases the number of correctly classified samples in the first three classes and results in a 2.7% improvement in accuracy compared to its non-ordinal counterpart.\n\nTable  2 : Confusion matrices of the proposed method with (a) non-ordinal and (b) ordinal ST-GCN on the validation set of the EngageNet dataset  [12] . Table  3 : Classification accuracy of engagement levels on the validation set of the EngageNet dataset  [12]  for different variants of the proposed method. The feature extraction step in  [34]  and  [12]  involves running multiple deeplearning models in OpenFace  [31]  to capture eye gaze, head pose, and AUs, as well as another complex network for MARLIN feature embeddings  [11] . While these feature extraction networks are complex for real-time use, the proposed method relies on facial landmarks extracted using the real-time MediaPipe  [14, 20] . Considering only the classification models, the number of parameters in EfficientNet B7 + LSTM  [30] , ResNet + TCN  [27] , Transformer  [12] , the proposed nonordinal ST-GCN, and ordinal ST-GCN are 82,681,812, 24,639,236, 1,063,108, 861,688, and 861,431, respectively. The memory consumption for EfficientNet B7 + LSTM  [30] , ResNet + TCN  [27] , Transformer  [12] , and the proposed ordinal ST-GCN are 2268.78, 790.35, 178.00, and 180.96 megabytes, respectively. The inference time for classifying a data sample using EfficientNet B7 + LSTM  [30] , ResNet + TCN  [27] , Transformer  [12] , and the proposed ordinal ST-GCN are 348, 51, 10, and 0.8 milliseconds, respectively. This indicates the efficiency of the proposed method, which, while being lightweight and fast, also improves the state-of-the-art.\n\nVariants of the Proposed Method Table  3  displays the results of different variants of the proposed method on the validation set of the EngageNet  [12]  dataset. In the first two variants, the x, y, and z coordinates of facial landmarks are converted into multivariate time series and analyzed by an LSTM and TCN. The LSTM includes four unidirectional layers with 256 neurons in hidden units and is followed by a 256 × 4 fully connected layer. The parameters of the TCN are as follows: the number of layers, number of filters, kernel size, and dropout rate are 8, 64, 8, and 0.05, respectively. While their results are acceptable and better than most of the earlier methods in Table  1 , they cannot outperform ST-GCN, the last two rows of Table  1 . The fact that the accuracy of the LSTM and TCN in Table  3  is higher than those in Table  1  signifies the efficiency of facial landmarks for engagement measurement. In the third variant, the z coordinates of facial landmarks are disregarded, and the decrease in the accuracy of the non-ordinal ST-GCN indicates the importance of the z coordinates for engagement measurement. Temporal kernel sizes other than 9 in the fourth and fifth variants have a negative impact on the results of the non-ordinal ST-GCN. When engagement measurement is performed using every 2, 4, 8, and 16 frames, instead of every frame, there is a slight decrease in the accuracy of the non-ordinal ST-GCN. However, this is a trade-off between accuracy and computation since reducing the frame rate corresponds to a reduction in computation. The last row of Table  3  shows the results of the ordinal ST-GCN when 21 hand landmarks extracted using MediaPipe  [14]  were added to the facial landmarks. The lower accuracy compared to using only facial landmarks is due to two factors. Firstly, there is 80% missingness in the hand landmarks due to the absence of hands in the videos. Secondly, it indicates that facial landmarks alone are sufficient for engagement measurement, capturing both behavioral and emotional indicators of engagement.\n\nResults on the Online SE Dataset Table  4  presents the results of the proposed method in comparison to previous methods on the test set of the Online SE dataset  [42] . The earlier methods listed in Table  4  are feature-based, extracting affective and behavioral features from video frames and performing binary classification of the features using TCN in  [33] , TCN in  [13] , LSTM with attention in  [44] , and BoW in  [32] . Given that engagement measurement in the Online SE dataset  [42]  is posed as a binary classification problem, implementing the ordinal version of the proposed method is not required. The proposed method, employing facial landmarks with ST-GCN, attains the highest accuracy and AUC PR. Interpretation of Results Fig.  1  displays the interpretation of engagement measurements taken using the proposed method by applying Grad-CAM  [43]  to the ordinal ST-GCN trained on the training set of the EngageNet dataset. For visualization purposes, three exemplary frames (out of 300) from the beginning, middle, and end of three data samples annotated as Not-Engaged in the validation set of the EngageNet dataset are shown in Fig.  1  (a)-(c). The facial landmarks extracted through MediaPipe are overlaid on the frames, where the color map of the facial landmarks, from blue to red, depicts the class activation map values of the last ST-GCN layer in the trained ST-GCN. The facial landmarks associated with the target class of Not-Engaged at certain frames are colored towards red. In Fig.  1  (a), during the beginning and middle of the video, the first two exemplary frames show the student engaged, with lower class activation map values. At the end of the video, when the student is not looking at the camera (computer screen) and is looking elsewhere, landmarks on the iris, eye, and jawline show higher values, resulting in the model classifying the sample as Not-Engaged. In Fig.  1  (b), the student is not paying attention and is playing with their phone. Jawline, eye, iris, and eyebrow facial landmarks with red colors have higher class activation map values, resulting in the model classifying the sample as Not-Engaged. In Fig.  1  (c), throughout the entire video sample, the head pose of the student is normal and perpendicular to the camera. However, the eyes are almost closed, indicating sleepiness and low arousal. This is detected by the higher class activation map values on the eye and iris landmarks, corresponding to the intensity of AU number 45, which indicates how closed the eyes are. This resulted in the model classifying the sample as Not-Engaged.",
      "page_start": 11,
      "page_end": 13
    },
    {
      "section_name": "Discussion",
      "text": "Our research led to the development of a novel deep-learning framework for student engagement measurement. In the proposed framework, sequences of facial landmarks are extracted from consecutive video frames and analyzed by ordinal ST-GCNs to make inferences regarding the engagement level of the student in the video. The successful application of our model to the EngageNet  [12]  and Online SE  [42]  datasets not only confirms its efficacy but also establishes a new standard in engagement level classification accuracy, outperforming previous methods. As the sole input information to the developed engagement measurement models, the 3D facial landmarks contain information about head pose  [17] , AUs  [18] , eye gaze  [20] , and affect  [3, 19] , which are the key indicators of behavioral and affective engagement. The relatively lightweight and fast ST-GCN and its integration with real-time MediaPipe  [14]  make the proposed framework capable of being deployed on virtual learning platforms and measuring engagement in real-time. The proposed method is privacy-preserving and does not require access to personally identifiable raw video data for engagement measurement. In a real-world deployment, the cross-platform MediaPipe solution  [14] , running on a web, mobile, or desktop application, extracts facial landmarks from video data on users' local devices. These non-identifiable facial landmarks are then transferred to a cloud, where they are analyzed by ST-GCNs to measure engagement. The interpretability feature of the proposed method, enabled through Grad-CAM, facilitates understanding which facial landmarks, corresponding to behavioral and affective indicators of engagement, contribute to certain levels of engagement. It also helps identify the specific timestamps at which these contributions occur. This provides instructors with additional information to take necessary actions and promote student engagement. A limitation of the proposed method is its reliance on the quality of facial landmarks detected by MediaPipe. In the context of engagement measurement in virtual learning sessions, an occluded or absent face, and consequently non-detected facial landmarks, correspond to lower levels of engagement or disengagement. Our developed ST-GCN was able to correctly classify most samples with occluded or absent faces, i.e., no facial landmarks, as Not-Engaged. To improve the performance of the proposed method, the following direction could be investigated: analyzing facial landmarks with more advanced ST-GCNs, which are equipped with attention mechanisms and trained through contrastive learning techniques and applying augmentation techniques to video data before facial landmark extraction  [45]  or to facial landmark data to improve the generalizability of ST-GCNs.",
      "page_start": 13,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: displays the interpretation of engagement",
      "page": 13
    },
    {
      "caption": "Figure 1: (a)-(c). The facial",
      "page": 13
    },
    {
      "caption": "Figure 1: (a), during the beginning and middle of the video,",
      "page": 13
    },
    {
      "caption": "Figure 1: (b), the student is not paying attention and is playing",
      "page": 13
    },
    {
      "caption": "Figure 1: (c), throughout the entire video sample, the",
      "page": 13
    },
    {
      "caption": "Figure 1: Interpretation of engagement measurements taken using the proposed",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "{ali.abedi,shehroz.khan}@uhn.ca"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "learning is crucial for a variety of fac-\nAbstract. Engagement in virtual"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "tors\nincluding student\nsatisfaction, performance, and compliance with"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "learning programs, but measuring it is a challenging task. There is there-"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "fore considerable interest in utilizing artificial\nintelligence and affective"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "computing to measure engagement\nin natural\nsettings as well as on a"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "large scale. This paper introduces a novel, privacy-preserving method for"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "engagement measurement\nfrom videos.\nIt uses\nfacial\nlandmarks, which"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "carry no personally identifiable information, extracted from videos via"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "the MediaPipe deep learning solution. The extracted facial\nlandmarks"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "are fed to Spatial-Temporal Graph Convolutional Networks (ST-GCNs)"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "to output the engagement level of the student in the video. To integrate"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "the ordinal nature of the engagement variable into the training process,"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "ST-GCNs undergo training in a novel ordinal\nlearning framework based"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "on transfer learning. Experimental results on two video student engage-"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "ment measurement datasets show the superiority of the proposed method"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "compared to previous methods with improved state-of-the-art on the En-"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "gageNet dataset with a 3.1% improvement in four-class engagement level"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "classification accuracy and on the Online Student Engagement dataset"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "with a 1.5% improvement in binary engagement classification accuracy."
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "Gradient-weighted Class Activation Mapping (Grad-CAM) was applied"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "to the developed ST-GCNs to interpret\nthe engagement measurements"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "obtained by the proposed method in both the spatial and temporal do-"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "mains. The relatively lightweight and fast ST-GCN and its integration"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "with the real-time MediaPipe make the proposed approach capable of"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "being deployed on virtual\nlearning platforms and measuring engagement"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "in real-time."
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "Keywords: Engagement Measurement · Graph Convolutional Network"
        },
        {
          "Institute of Biomedical Engineering, University of Toronto, Toronto, Canada": "· Ordinal Classification · Transfer Learning."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nAli Abedi and Shehroz S. Khan": "new ones\nthrough sustained attention [2], essential\nfor content comprehension"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "and engagement development\n[1]. However, measuring and upholding engage-"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "ment is challenging, and demands significant effort from educators. The advent"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "of\nremote\nsensing, Artificial\nIntelligence\n(AI), and affective\ncomputing offers"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "new avenues for accurately measuring engagement across various learning envi-"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "ronments,\nincluding virtual\nlearning platforms. Technologies\nsuch as\nfacial ex-"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "pression recognition [3] and eye gaze tracking [4] enable more precise monitoring"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "and enhancement of student engagement. This paper explores the development"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "of AI algorithms\nfor automated engagement measurement\nin virtual\nlearning,"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "taking into account the definition of engagement in educational psychology and"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "its measurement methodologies."
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "Fredricks\net al.\n[5] defined engagement\nthrough affective, behavioral, and"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "cognitive components. Affective engagement\nrelates\nto students’ emotions and"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "attitudes\ntowards\ntheir\ntasks,\nincluding interest or\nfeelings during a learning"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "class [6]. Behavioral engagement refers to active participation, such as focusing"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "on a computer\nscreen or not being distracted by a phone\n[7]. Cognitive\nen-"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "gagement deals with a student’s dedication to learning and tackling challenges,"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "affecting outcomes such as recall and understanding [1]. Booth et al. [1] identified"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "that engagement is measured through various signals such as facial expressions,"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "eye movements, posture, heart\nrate, brain activity, audio, and digital\ninterac-"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "tions. Cameras, prevalent in devices for online learning, make video a key data"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "modality for AI-based engagement measurement [8, 9]. Therefore, AI techniques"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "predominantly focus on video to measure student engagement [8–10]."
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "AI-driven engagement measurement methods are divided into end-to-end and"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "feature-based approaches. While\nfeature-based methods generally outperform"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "end-to-end approaches, which process\nraw videos,\nthey require extensive iden-"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "tification of effective features through trial and error\n[8–10]. Extracting multi-"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "modal\nfeatures from videos with multiple computer vision algorithms or neural"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "networks [11,12], and their subsequent analysis by deep neural networks to mea-"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "sure engagement, render these methods computationally demanding [8,13]. Such"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "computational requirements limit their use on local devices and necessitate the"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "transfer of privacy-sensitive video data to cloud servers for engagement measure-"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "ment. In contrast, extracting low-dimensional\nlandmark information, such as fa-"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "cial and hand landmarks, not only provides more compact data but also captures"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "essential geometric features for affect and behavior analysis,\nincluding engage-"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "ment, without personal\nidentifiers\n[14–16]. Previous\nfeature-based engagement"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "measurement approaches\nextracted features\nsuch as head pose,\nfacial Action"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "Units (AUs),\niris and gaze features, and affect and emotion features, which are"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "indicators of the behavioral and affective components of engagement [8–10]. The"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "literature has demonstrated the success of\nfacial\nlandmarks in capturing these"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "aforementioned features\n[3, 17–20]. A model\nsuch as Spatial-Temporal Graph"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "Convolutional Networks (ST-GCNs) [21], capable of analyzing spatial-temporal"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "facial\nlandmarks and learning these aforementioned features inherently, can re-"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "duce\nthe need for\nraw facial videos. An approach based on facial\nlandmarks"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "prioritizes privacy and reduces computational demands, making it more practi-"
        },
        {
          "2\nAli Abedi and Shehroz S. Khan": "cal\nfor real-time engagement measurement."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length\n3": "In this paper, an alternative course away from conventional end-to-end and"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "feature-based approaches is charted, and a novel engagement measurement tech-"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "nique using facial\nlandmarks extracted from videos is presented. The proposed"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "method is characterized by its privacy-preserving nature and computational ef-"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "ficiency. This work makes the following contributions:"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "– This marks\nthe first\ninstance in video-based engagement measurement\n[8,"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "9, 13, 22–25] where facial\nlandmarks, as\nthe single data modality extracted"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "from videos, are analyzed through ST-GCNs\n[21]\nto infer\nthe engagement"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "level\nin the video;"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "– To integrate the ordinal nature of the engagement variable into the training"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "process, ST-GCNs undergo training in a novel ordinal\nlearning framework"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "utilizing transfer learning;"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "– Extensive experiments conducted on two video-based engagement measure-"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "ment datasets demonstrate the superiority of the proposed method over pre-"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "vious methods, achieving an improved state-of-the-art\nin engagement\nlevel"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "classification accuracy. For explainability, Gradient-weighted Class Activa-"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "tion Mapping (Grad-CAM) is applied to the developed ST-GCNs to inter-"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "pret\nthe\nengagement measurements obtained by the proposed method in"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "both spatial and temporal domains."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nAli Abedi and Shehroz S. Khan": "The process of measuring engagement through feature-based techniques in-"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "volves two stages. Initially, behavioral and affective features are extracted from"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "video frames, relying on either domain-specific knowledge or pre-trained models"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "for facial embedding extraction. OpenFace [31]\nis notably effective for its com-"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "prehensive feature extraction capabilities,\nincluding AUs, eye movements, gaze"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "direction, and head positioning, and is widely applied in engagement measure-"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "ment [12, 13, 32, 33]. Examples of\nfacial embedding include the Masked Autoen-"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "coder for facial video Representation LearnINg (MARLIN) [11], utilized by Singh"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "et al.\n[12], and the Emotion Face Alignment Network (EmoFAN) [19], used by"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "Abedi et al.\n[13]. Subsequently, to analyze these extracted features and infer en-"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "gagement, various machine learning and deep learning models are employed, such"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "as Bag-of-Words (BoW) [32], Recurrent Neural Network (RNN) variations [24],"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "Temporal Convolutional Networks\n(TCNs)\n[13, 33], Transformers\n[12, 34], and"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "ensemble models [35]."
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "2.2\nGraph-based Facial Affect and Expression Analysis"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "Liu et al.\n[3] conducted a comprehensive review of the literature on graph-based"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "methods for facial affect analysis. These methods typically take an image or a"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "sequence of\nimages as input and produce an affect classification or regression as"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "output. Based on their review, Liu et al.\n[3] proposed a pipeline for graph-based"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "facial affect analysis, which includes (i) face preprocessing, (ii) graph-based af-"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "fective representation, and (iii) graph-based relational reasoning. Preprocessing"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "involves steps such as face detection and registration. Graph-based affective rep-"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "resentation involves defining the structure of\nthe graph,\ni.e., nodes and edges."
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "The graph structure can be spatial or spatiotemporal depending on whether the"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "input data is\nstill\nimages or videos. The graph structure can be at\nthe land-"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "mark level, region level, or AU level, with nodes representing facial\nlandmarks,"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "facial regions of\ninterest, or facial AUs, respectively. In the relational reasoning"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "step, the edges, nodes, their interrelations, and temporal dynamics are analyzed"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "through relational reasoning machine-learning or deep-learning models to make"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "inferences regarding affect. Models used to analyze graph data include dynamic"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "Bayesian networks, RNNs, CNNs,\nfully-connected neural networks, and non-"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "temporal and temporal graph neural networks [3]."
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "Zhou et al.\n[36] proposed a facial expression recognition method based on"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "spatiotemporal\nlandmark-level and region-level graphs. The intra-frame graph is"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "formed by the connections among thirty-four facial landmarks located around the"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "eyes,\nlips, and cheeks. The definition of these intra-frame connections was done"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "manually. Inter-frame connections were established by linking each node in one"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "frame to its corresponding node in the following frame. Two parallel ST-GCNs"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "with analogous structures were trained; one on the nodes’ x- and y-coordinates,"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "and another on their histogram of orientation features. ST-GCNs’ outputs were"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "concatenated and processed by a fully connected network to identify facial ex-"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "pressions. This method’s disadvantages include independent training of the two"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "ST-GCNs rather than joint learning, and manual definition of nodes and edges."
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "Wei et al.\n[15] presented a graph-based method for micro-expression recog-"
        },
        {
          "4\nAli Abedi and Shehroz S. Khan": "nition in video. The method included a dual-stream ST-GCN, focusing on the x"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length\n5": "and y coordinates of\nfacial\nlandmarks and the distance and angles between ad-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "jacent facial\nlandmarks. An AU-specific loss function was incorporated into the"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "neural network’s training process in order to incorporate the association between"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "AUs and micro-expressions. Their methodology employed three different sets of"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "facial\nlandmarks as graph nodes, comprising sets with 14, 31, and Dlib’s [37] 68"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "facial landmarks. Notably, the set with 68 landmarks was the only one to include"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "landmarks along the jawline. The experiments demonstrated the best results for"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "micro-expression recognition when 14 facial\nlandmark sets were used."
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "Leong et al.\n[25]\nintroduced a method employing spatial-temporal graph at-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "tention networks\nto analyze facial\nlandmarks and head poses, aimed at\niden-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "tifying academic\nemotions. The\nfacial\nlandmarks used were\nthose around the"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "eyebrow,\neye, nose, and mouth excluding those\nthat outline\nthe\nface’s outer"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "shape, with the reasoning being they lack correlation with affective states. A"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "notable limitation of\nthis approach is its reliance on multiple deep neural net-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "works for extracting features,\ni.e.,\nfacial\nlandmarks and head pose. The method"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "achieved lower academic emotion detection accuracy when compared to prior"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "feature-based methods [13]."
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "2.3\nDiscussion"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "As reviewed in this section, while some methods have used facial\nlandmarks and"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "ST-GCNs to recognize facial affect and expression in videos, there has been no"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "exploration of the use of these methods to measure engagement. The fundamen-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "tal differences between engagement and facial affect and expression make their"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "measurement different. First, engagement\nis a multi-component state compris-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "ing behavioral, affective, and cognitive components. To illustrate, key indicators"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "of behavioral engagement, such as being off-task or on-task, are determined by"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "head pose,\neye gaze, and blink rate. These\nindicators, and therefore\nengage-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "ment, cannot be effectively measured using methods designed solely for\nfacial"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "affect analysis. Second, engagement is not a constant state;\nit varies over time"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "and should be measured at specific time resolutions where it remains stable and"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "quantifiable. An ideal measurement duration for engagement is between ten and"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "forty seconds, which is\nlonger\nthan the time resolution for\nfacial affect analy-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "sis, which sometimes occurs at the frame level. Third, engagement measurement"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "could involve recognizing an ordinal variable that indicates levels of engagement,"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "as opposed to facial expression recognition, which identifies categorical variables"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "without inherent order."
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "Existing engagement measurement approaches face limitations due to the ne-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "cessity of employing multiple deep neural networks for the extraction and anal-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "ysis of multi-modal\nfeatures. Coupled with the significant differences between"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "facial affect analysis and engagement measurement as outlined above, these lim-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "itations underscore a gap in the field. In response, we introduce a straightforward"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "yet effective method for engagement measurement. This method is lightweight"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "and fast, preserving privacy while also demonstrating improvements over current"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "methodologies across two video-based engagement measurement datasets."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nAli Abedi and Shehroz S. Khan": "3\nMethod"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "The input to the proposed method is a video sample of a student seated in front"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "of a laptop or PC camera during a virtual learning session. The sequences of facial"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "landmarks extracted from consecutive video frames through MediaPipe [14, 20]"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "are analyzed by ST-GCN [16, 21] to output the engagement level of the student"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "in the video."
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "3.1\nGraph-based Representation"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "The MediaPipe deep learning solution [14], a framework that is both real-time"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "and cross-platform, is employed for the extraction of facial landmarks from video."
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "Incorporated within MediaPipe, Attention Mesh [20]\nis adept at detecting 468"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "3D facial\nlandmarks\nthroughout\nthe\nface and an extra 10 landmarks\nfor\nthe"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "iris. However, not all 478 landmarks are employed in the proposed engagement"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "measurement method. Consistent with existing studies\n[3], only 68 of\nthe 3D"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "facial\nlandmarks, which match those identified by the Dlib framework [37],\nin"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "addition to the 10 3D iris landmarks, making a total of 78 landmarks, are utilized."
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "The 3D facial\nlandmarks\nencapsulate\ncrucial\nspatial-temporal\ninformation"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "pertinent to head pose [17], AUs [18], eye gaze [20], and affect [3,19]—key features"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "identified in prior engagement measurement studies [8–10, 13, 22–24, 26, 27, 33]."
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "Consequently, there is no necessity for extracting additional handcrafted features"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "from the video frames."
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "The N\n3D facial\nlandmarks extracted from T consecutive video frames are"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "utilized to construct a spatiotemporal graph G = (V, E). In this graph, the set of"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "landmarks\nnodes V = {vti|t = 1, . . . , T, i = 1, . . . , N } encompasses all the facial"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "in a sequence. To construct G, first, the facial\nlandmarks within one frame are"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "connected with edges according to a connectivity structure based on Delaunay"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "triangulation [38] which is consistent with true facial muscle distribution and"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "uniform for different subjects [3]. Then each landmark will be connected to the"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "same landmark in the consecutive frame."
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "3.2\nGraph-based Reasoning"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "Based on the spatiotemporal graph of\nfacial\nlandmarks G = (V, E) constructed"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "above, an adjacency matrix A is defined as an N ×N matrix where the element at"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "position (i, j) is set to 1 if there is an edge connecting the ith and jth landmarks,"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "and set\nto 0 otherwise. An identity matrix I, of\nthe\nsame dimensions as A,"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "is created to represent self-connections. A spatial-temporal graph, as the basic"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "element of an ST-GCN layer,\nis implemented as follows [21]."
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "(cid:17)\n(cid:16)"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "2 ((A + I) ⊙ M )Λ− 1\n(1)\nΛ− 1\n2 finWspatial\nWtemporal\nfout ="
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "where Λii = (cid:80)"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "j(Aij + I ij). M is a learnable weight matrix that enables scaling"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "the contributions of a node’s\nfeature to its neighboring nodes\n[21]. The input"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "is\nthe\nraw coordinates of\nfacial\nlandmarks\nfor\nthe\nfeature map, denoted fin,"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "first\nlayer of ST-GCN, and it\nrepresents\nthe outputs\nfrom previous\nlayers\nin"
        },
        {
          "6\nAli Abedi and Shehroz S. Khan": "subsequent layers of ST-GCN. The dimensionality of fin is (C, N, T ), where C"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length\n7": "is\nthe number of channels,\nfor example, 3 in the initial\ninput\nto the network"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "corresponding to the x, y, and z coordinates of\nthe facial\nlandmarks.\nIn each"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "layer of ST-GCN,\ninitially,\nthe spatial\n(intra-frame) convolution is applied to"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "fin based on the weight matrix Wspatial, utilizing a standard 2D convolution"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "with a kernel size of 1 × 1. Subsequently, the resulting tensor is multiplied by the"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "2 ((A+I)⊙M )Λ− 1\n2 across the spatial dimension.\nnormalized adjacency matrix Λ− 1"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "Afterward, the temporal (inter-frame) convolution, based on the weight matrix"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "is applied to the\ntensor output\nfrom the\nspatial\nconvolution. This\nWtemporal,"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "convolution is a standard 2D convolution with a kernel size of 1 × Γ , where Γ"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "signifies the temporal kernel size."
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "Following an adequate number of ST-GCN layers,\nspecifically three in this"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "work, that perform spatial-temporal graph convolutions as outlined above, the"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "resulting tensor undergoes 2D average pooling. The final output of the network"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "is generated by a final 2D convolution. This convolution employs a kernel size"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "of 1 × 1 and features an output channel dimensionality equal to the number of"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "classes K,\ni.e., the number of engagement levels to be measured. An explanation"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "of the detailed architecture of the ST-GCNs for specific datasets can be found"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "in subsection 4.2."
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "3.3\nOrdinal Engagement classification through Transfer Learning"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "The model described above, with a final\nlayer comprising K output channels,"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "tackles the engagement measurement problem as a categorical K-class classifica-"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "tion problem without taking into account the ordinal nature of the engagement"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "variable [10, 39, 40]. The model could harness the ordinality of the engagement"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "variable\nto enhance\nits\ninferences. Drawing inspiration from [13, 41], a novel"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "ordinal\nlearning framework based on transfer learning is introduced as follows."
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "Training phase– The original K-level ordinal\nlabels, y = 0, 1, . . . , K − 1,\nin the"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "if y > i, then\ntraining set are converted into K − 1 binary labels yi as follows:"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "for\ni = 0, 1, . . . , K − 2. Subsequently, K − 1 binary\nyi = 1; otherwise, yi = 0,"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "classifiers are\ntrained with the\ntraining set and the K − 1 binary label\nsets"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "described above. The training of binary classifiers is based on transfer learning,"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "which proceeds as follows: Initially, a network is trained on the dataset with the"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "original K-class labels, employing a regular final\nlayer with K output channels."
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "After training, the ST-GCN layers of this network are frozen, and the final layer is"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "removed. To this frozen network, K −1 separate untrained 2D convolution layers"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "with a single output channel each are added, resulting in K − 1 new networks."
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "Each of these networks consists of a frozen sequence of ST-GCN layers followed"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "by an untrained 2D convolution layer. These K −1 new networks are then trained"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "on the entire dataset using the K − 1 binary label sets described above. During"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "this phase, only the final 2D convolution layers are subjected to training."
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "Inference phase– For\nthe ordinal classification of a test\nsample,\nthe sample"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "is initially input into the pre-trained (and frozen) sequence of ST-GCN layers,"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "followed by a 2D average pooling layer. The tensor obtained from this process"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "is then input into K − 1 pre-trained final 2D convolution layers, each yielding a"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "probability estimate for the test sample being in the binary class yt = yi, where"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "i = 0, 1, . . . , K − 2. Subsequently, these K − 1 binary probability estimates are"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8\nAli Abedi and Shehroz S. Khan": "transformed into a single multi-class probability of the sample belonging to class"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "y = 0, 1, . . . , K − 1, as follows [41]."
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "if k = 0,\n1 − p(yt ≥ 0),"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": " \n(2)\nif 0 < k < K − 1,\np(yt = k) =\np(yt > k − 1) − p(yt ≥ k),"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "if k = K − 1.\np(yt > K − 2),"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "Despite the increased training time for the ordinal model within the afore-"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "mentioned ordinal\nlearning framework, the final count of parameters in the or-"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "dinal model remains nearly identical to that of the original non-ordinal model."
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "4\nExperiments"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "This section evaluates the performance of the proposed method relative to exist-"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "ing methods in video-based engagement measurement.\nIt reports and discusses"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "the\nresults of multi-class and binary classification of\nengagement across\ntwo"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "datasets. Based on the engagement measurement problem at hand, several eval-"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "uation metrics are employed. In the context of multi-class engagement level clas-"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "sification, metrics such as accuracy and confusion matrix are reported. For binary"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "engagement classification, accuracy, the Area Under the Curve of the Receiver"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "Operating Characteristic (AUC-ROC), and the Area Under\nthe Curve of\nthe"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "Precision and Recall curve (AUC-PR) are utilized.\nIn addition, the number of"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "parameters of the models used, memory consumption, and inference time of the"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "proposed method are compared to those of previous methods."
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "4.1\nDatasets"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "Experiments on two large video-based engagement measurement datasets were"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "conducted, each presenting unique challenges that further enabled the validation"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "of the proposed method."
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "EngageNet: The EngageNet dataset\n[12],\nrecognized as\nthe\nlargest dataset"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "for student engagement measurement,\nincludes video recordings of 127 subjects"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "participating in virtual\nlearning sessions. Each video sample has a duration of"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "10 seconds, with a frame rate of 30 fps and a resolution of 1280 × 720 pixels. The"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "subjects’ video recordings were annotated as four ordinal\nlevels of engagement:"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "Not-Engaged, Barely-Engaged, Engaged, and Highly-Engaged. The dataset was"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "divided into 7983, 1071, and 2257 samples for training, validation, and testing,"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "respectively, using a subject-independent data split approach [12]. However, only"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "the\ntraining and validation sets were made available by the dataset\ncreators"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "and were utilized for\nthe\ntraining and validation of predictive models\nin the"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "experiments presented in this paper. The distribution of\nsamples\nin the\nfour"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "aforementioned classes of\nengagement\nin the\ntraining and validation sets are"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "1550, 1035, 1658, and 3740, and 132, 97, 273, and 569, respectively."
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "Online SE: The Online SE dataset [42], comprises videos of six students par-"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "ticipating in online courses via the Zoom platform. These recordings\nspan 10"
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "seconds each, with a frame rate of 24 fps and a resolution of 220 × 155 pixels."
        },
        {
          "8\nAli Abedi and Shehroz S. Khan": "The videos were annotated as either Not-Engaged or Engaged. The dataset was"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: include state-of-the-art end-to-end methods,",
      "data": [
        {
          "Title Suppressed Due to Excessive Length\n9": "segmented into 3190, 1660, and 1290 samples for training, validation, and test-"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "ing, respectively. The distribution of samples in the two aforementioned classes"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "of engagement in the training, validation, and test sets is 570 and 2620, 580 and"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "1080, and 570 and 720, respectively."
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "4.2\nExperimental Setting"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "The\nsole\ninformation extracted from video frames\nis\nfacial\nlandmarks, which"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "are analyzed by ST-GCN to determine the engagement\nlevel of\nthe student\nin"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "the video. Drawing inspiration from the pioneering works on body-joints-based"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "action analysis\n[21, 43],\nthe proposed ST-GCN for\nfacial-landmarks-based en-"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "gagement measurement is structured as follows. The input facial\nlandmarks are"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "first processed through a batch normalization layer,\nfollowed by three consecu-"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "tive ST-GCN layers with 64, 128, and 256 output channels, respectively. Residual"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "connections are incorporated in the last two ST-GCN layers. A dropout rate of"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "0.1 is applied to each ST-GCN layer. The temporal kernel size in the ST-GCN"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "layers is selected to be 9. Subsequent to the ST-GCN layers, an average pooling"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "layer is utilized, and its resulting tensor is directed into a 2D convolutional\nlayer"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "with 256 input channels and a number of output channels corresponding to the"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "number of classes. A Softmax activation function then computes the probability"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "estimates. In cases where engagement measurement is framed as a binary classifi-"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "cation task, the terminal 2D convolution layer is configured with a single output"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "channel,\nsubstituting Softmax with a Sigmoid function. The Sigmoid function"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "is also employed for the individual binary classifiers within the ordinal\nlearning"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "framework detailed in subsection 3.3. The models are trained using the Adam"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "optimizer with mini-batches of size 16 and an initial\nlearning rate of 0.001 for"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "300 epochs. The learning rate is decayed by a factor of 0.1 every 100 epochs."
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "4.3\nExperimental Results"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "Comparison to Previous Methods Table 1 presents the comparative results"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "of two settings of the proposed method, a regular non-ordinal classifier and an"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "ordinal classifier, with previous methods on the validation set of the EngageNet"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "dataset\n[12]. The\nengagement measurement\nin EngageNet\n[12]\nis a four-class"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "classification problem and the accuracy is\nreported as\nthe\nevaluation metric."
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "The previous methods in Table 1 include state-of-the-art end-to-end methods,"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "including the\ncombination of ResNet-50 with TCN [27] and the\ncombination"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "of EfficientNet B7 with LSTM and bidirectional LSTM [30],\nfollowed by state-"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "of-the-art feature-based methods. The previous feature-based methods in Table"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "1 used different combinations of OpenFace’s eye gaze, head pose, and AU fea-"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "tures [31] along with MARLIN’s facial embedding features [11]. These features"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "were classified by LSTM, CNN-LSTM, TCN, and Transformer. Refer to [12]\nfor"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "more details. The results of the feature-based method proposed by Vedernikov"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "et al.\n[34] are also reported, where\nthe aforementioned features are\nclassified"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "using a Transformer-based neural network called the Tensor-Convolution and"
        },
        {
          "Title Suppressed Due to Excessive Length\n9": "Convolution-Transformer Network (TCCT-Net)."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 1: Classification accuracy of engagement levels on the validation set of",
      "data": [
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "and feature-based methods with various"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "against two configurations of the proposed method -"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "by ST-GCN and ordinal ST-GCN. Bolded values denote the best results."
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "Ref."
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[27]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[30]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[30]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[12]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[34]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[34]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[34]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[34]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "[34]"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "Ours"
        },
        {
          "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods": "Ours"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 1: Classification accuracy of engagement levels on the validation set of",
      "data": [
        {
          "10": "Table 1: Classification accuracy of engagement",
          "Ali Abedi and Shehroz S. Khan": ""
        },
        {
          "10": "the EngageNet dataset [12]: comparison of state-of-the-art end-to-end methods",
          "Ali Abedi and Shehroz S. Khan": ""
        },
        {
          "10": "and feature-based methods with various",
          "Ali Abedi and Shehroz S. Khan": ""
        },
        {
          "10": "against two configurations of the proposed method -",
          "Ali Abedi and Shehroz S. Khan": ""
        },
        {
          "10": "by ST-GCN and ordinal ST-GCN. Bolded values denote the best results.",
          "Ali Abedi and Shehroz S. Khan": ""
        },
        {
          "10": "Ref.",
          "Ali Abedi and Shehroz S. Khan": "Features"
        },
        {
          "10": "[27]",
          "Ali Abedi and Shehroz S. Khan": "End to End Model"
        },
        {
          "10": "[30]",
          "Ali Abedi and Shehroz S. Khan": "End to End Model"
        },
        {
          "10": "[30]",
          "Ali Abedi and Shehroz S. Khan": "End to End Model"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Gaze"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Head Pose"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "AU"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Gaze + Head Pose"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Gaze + Head Pose + AU"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Gaze"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Head Pose"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "AU"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Gaze + AU"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Head Pose + AU"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Gaze + Head Pose + AU"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Gaze"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Head Pose"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "AU"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Gaze + Head Pose + AU"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Gaze"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Gaze + Head Pose"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Gaze + Head Pose + AU"
        },
        {
          "10": "[12]",
          "Ali Abedi and Shehroz S. Khan": "Gaze + Head Pose + AU + MARLIN Transformer"
        },
        {
          "10": "[34]",
          "Ali Abedi and Shehroz S. Khan": "Gaze"
        },
        {
          "10": "[34]",
          "Ali Abedi and Shehroz S. Khan": "Head Pose"
        },
        {
          "10": "[34]",
          "Ali Abedi and Shehroz S. Khan": "AU"
        },
        {
          "10": "[34]",
          "Ali Abedi and Shehroz S. Khan": "Gaze + Head Pose"
        },
        {
          "10": "[34]",
          "Ali Abedi and Shehroz S. Khan": "Gaze + Head Pose + AU"
        },
        {
          "10": "Ours",
          "Ali Abedi and Shehroz S. Khan": "Facial Landmarks"
        },
        {
          "10": "Ours",
          "Ali Abedi and Shehroz S. Khan": "Facial Landmarks"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 1: , which relies solely on facial landmarks",
      "data": [
        {
          "Title Suppressed Due to Excessive Length\n11": "Transformer is better than TCN, which is better than CNN-LSTM and LSTM;"
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "however,\nit comes at\nthe cost of\nincreased computational complexity. For\nsin-"
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "gle features, the Transfomer-based TCCT-Net [34] outperforms other classifiers;"
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "however,\nfor multiple feature sets, the vanilla Transformer [12] outperforms the"
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "others."
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "The proposed ST-GCN in Table 1, which relies\nsolely on facial\nlandmarks"
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "without\nrequiring raw facial videos or multiple hand-crafted features, outper-"
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "forms previous methods. Moreover, making the proposed method ordinal further"
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "improves the state-of-the-art by 3.1% compared to eye gaze, head pose, AUs, and"
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "MARLIN features [11] with the Transformer [12]. Tables 2a and 2b depict the"
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "confusion matrices of the ordinal and non-ordinal configurations of the proposed"
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "method in Table 1. As shown, incorporating ordinality significantly increases the"
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "number of correctly classified samples in the first three classes and results in a"
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "2.7% improvement in accuracy compared to its non-ordinal counterpart."
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "Table 2: Confusion matrices of the proposed method with (a) non-ordinal and"
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "(b) ordinal ST-GCN on the validation set of the EngageNet dataset [12]."
        },
        {
          "Title Suppressed Due to Excessive Length\n11": "(b) Ordinal ST-GCN\n(a) Non-ordinal ST-GCN"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 1: , which relies solely on facial landmarks",
      "data": [
        {
          "Table 3: Classification accuracy of engagement levels on the validation set of the": "EngageNet dataset [12]\nfor different variants of the proposed method."
        },
        {
          "Table 3: Classification accuracy of engagement levels on the validation set of the": "Variant\nAccuracy"
        },
        {
          "Table 3: Classification accuracy of engagement levels on the validation set of the": "ST-GCN is replaced with LSTM\n0.6847"
        },
        {
          "Table 3: Classification accuracy of engagement levels on the validation set of the": "ST-GCN is replaced with TCN\n0.6813"
        },
        {
          "Table 3: Classification accuracy of engagement levels on the validation set of the": "joints are used\n0.6655\nOnly x and y coordinates of"
        },
        {
          "Table 3: Classification accuracy of engagement levels on the validation set of the": "A temporal kernel of 3 is used\n0.6748"
        },
        {
          "Table 3: Classification accuracy of engagement levels on the validation set of the": "A temporal kernel of 15 is used\n0.6841"
        },
        {
          "Table 3: Classification accuracy of engagement levels on the validation set of the": "Every 2 frames are used\n0.6813"
        },
        {
          "Table 3: Classification accuracy of engagement levels on the validation set of the": "Every 4 frames are used\n0.6907"
        },
        {
          "Table 3: Classification accuracy of engagement levels on the validation set of the": "Every 8 frames are used\n0.6907"
        },
        {
          "Table 3: Classification accuracy of engagement levels on the validation set of the": "Every 16 frames are used\n0.6841"
        },
        {
          "Table 3: Classification accuracy of engagement levels on the validation set of the": "Hand landmarks are added\n0.6956"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 3: displays the results of different",
      "data": [
        {
          "12\nAli Abedi and Shehroz S. Khan": "861,688, and 861,431, respectively. The memory consumption for EfficientNet B7"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "+ LSTM [30], ResNet + TCN [27], Transformer [12], and the proposed ordinal"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "ST-GCN are 2268.78, 790.35, 178.00, and 180.96 megabytes,\nrespectively. The"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "inference time for classifying a data sample using EfficientNet B7 + LSTM [30],"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "ResNet + TCN [27], Transformer\n[12], and the proposed ordinal ST-GCN are"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "348, 51, 10, and 0.8 milliseconds,\nrespectively. This\nindicates\nthe efficiency of"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "the proposed method, which, while being lightweight and fast, also improves the"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "state-of-the-art."
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "Variants of the Proposed Method Table 3 displays the results of different"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "variants of\nthe proposed method on the validation set of\nthe EngageNet\n[12]"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "dataset. In the first two variants, the x, y, and z coordinates of facial\nlandmarks"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "are converted into multivariate time series and analyzed by an LSTM and TCN."
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "The LSTM includes four unidirectional\nlayers with 256 neurons in hidden units"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "and is followed by a 256 × 4 fully connected layer. The parameters of the TCN"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "are as follows: the number of\nlayers, number of filters, kernel size, and dropout"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "rate are 8, 64, 8, and 0.05, respectively. While their results are acceptable and"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "better than most of the earlier methods in Table 1, they cannot outperform ST-"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "GCN,\nthe last\ntwo rows of Table 1. The fact\nthat\nthe accuracy of\nthe LSTM"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "and TCN in Table 3 is higher\nthan those\nin Table 1 signifies\nthe\nefficiency"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "of\nfacial\nlandmarks\nfor\nengagement measurement.\nIn the\nthird variant,\nthe z"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "coordinates of facial landmarks are disregarded, and the decrease in the accuracy"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "of\nthe non-ordinal ST-GCN indicates\nthe importance of\nthe z coordinates\nfor"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "engagement measurement. Temporal kernel sizes other than 9 in the fourth and"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "fifth variants have a negative impact on the results of the non-ordinal ST-GCN."
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "When engagement measurement is performed using every 2, 4, 8, and 16 frames,"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "instead of every frame, there is a slight decrease in the accuracy of the non-ordinal"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "ST-GCN. However, this is a trade-off between accuracy and computation since"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "reducing the frame rate corresponds to a reduction in computation. The last row"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "of Table 3 shows the results of\nthe ordinal ST-GCN when 21 hand landmarks"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "extracted using MediaPipe [14] were added to the facial\nlandmarks. The lower"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "accuracy compared to using only facial\nlandmarks is due to two factors. Firstly,"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "there is 80% missingness in the hand landmarks due to the absence of hands in"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "the videos. Secondly,\nit\nindicates\nthat\nfacial\nlandmarks alone are sufficient\nfor"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "engagement measurement, capturing both behavioral and emotional\nindicators"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "of engagement."
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "Results on the Online SE Dataset Table 4 presents the results of the pro-"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "posed method in comparison to previous methods on the test set of the Online"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "SE dataset [42]. The earlier methods listed in Table 4 are feature-based, extract-"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "ing affective and behavioral\nfeatures from video frames and performing binary"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "classification of\nthe features using TCN in [33], TCN in [13], LSTM with at-"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "tention in [44], and BoW in [32]. Given that engagement measurement\nin the"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "Online SE dataset [42]\nis posed as a binary classification problem,\nimplement-"
        },
        {
          "12\nAli Abedi and Shehroz S. Khan": "ing the ordinal version of\nthe proposed method is not\nrequired. The proposed"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length": "",
          "13": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "13": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "13": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "13": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "13": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "AUC ROC",
          "13": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "0.8764",
          "13": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "0.8710",
          "13": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "0.8890",
          "13": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "0.8926",
          "13": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "0.8806",
          "13": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "13": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "13": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14\nAli Abedi and Shehroz S. Khan": "time \n \n \n \n \n \n \n \n \n(a) \n(b) \n(c)"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "Fig. 1:\nInterpretation of\nengagement measurements\ntaken using the proposed"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "method by applying Gradient-weighted Class Activation Mapping (Grad-CAM)"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "to the Spatial-Temporal Graph Convolutional Network (ST-GCN). Please refer"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "to the last paragraph of subsection 4.3 for further details."
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "in engagement level classification accuracy, outperforming previous methods. As"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "the sole input\ninformation to the developed engagement measurement models,"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "the 3D facial\nlandmarks\ncontain information about head pose\n[17], AUs\n[18],"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "eye gaze [20], and affect [3, 19], which are the key indicators of behavioral and"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "affective engagement. The relatively lightweight and fast ST-GCN and its\nin-"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "tegration with real-time MediaPipe [14] make the proposed framework capable"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "of being deployed on virtual\nlearning platforms and measuring engagement\nin"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "real-time. The proposed method is privacy-preserving and does not require ac-"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "cess to personally identifiable raw video data for engagement measurement. In a"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "real-world deployment, the cross-platform MediaPipe solution [14], running on a"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "web, mobile, or desktop application, extracts facial landmarks from video data on"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "users’\nlocal devices. These non-identifiable facial\nlandmarks are then transferred"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "to a cloud, where they are analyzed by ST-GCNs to measure engagement. The"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "interpretability feature of the proposed method, enabled through Grad-CAM, fa-"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "cilitates understanding which facial\nlandmarks, corresponding to behavioral and"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "affective indicators of engagement, contribute to certain levels of engagement. It"
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "also helps identify the specific timestamps at which these contributions occur."
        },
        {
          "14\nAli Abedi and Shehroz S. Khan": "This provides instructors with additional\ninformation to take necessary actions"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length\n15": "and promote student engagement. A limitation of the proposed method is its re-"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "liance on the quality of\nfacial\nlandmarks detected by MediaPipe. In the context"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "of engagement measurement in virtual\nlearning sessions, an occluded or absent"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "face, and consequently non-detected facial\nlandmarks, correspond to lower levels"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "of engagement or disengagement. Our developed ST-GCN was able to correctly"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "classify most samples with occluded or absent faces,\ni.e., no facial\nlandmarks, as"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "Not-Engaged. To improve the performance of the proposed method, the following"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "direction could be investigated: analyzing facial\nlandmarks with more advanced"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "ST-GCNs, which are equipped with attention mechanisms and trained through"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "contrastive learning techniques and applying augmentation techniques to video"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "data before facial landmark extraction [45] or to facial landmark data to improve"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "the generalizability of ST-GCNs."
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "Acknowledgment The authors express their sincere thanks to the Multimodal"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "Perception Lab at the International\nInstitute of\nInformation Technology, Ban-"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "galore, India, for their generosity in providing the Online SE dataset, which was"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "instrumental\nin the execution of our experiments."
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "This\nresearch was\nfunded by the Natural Sciences and Engineering Research"
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "Council of Canada."
        },
        {
          "Title Suppressed Due to Excessive Length\n15": "References"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "1. B. M. Booth, N. Bosch, and S. K. D’Mello,\n“Engagement detection and its appli-"
        },
        {
          "References": "cations in learning: A tutorial and selective review,” Proceedings of\nthe IEEE, vol."
        },
        {
          "References": "111, no. 10, pp. 1398–1422, 2023."
        },
        {
          "References": "2. S. Hidi and K. A. Renninger,\n“The\nfour-phase model of\ninterest development,”"
        },
        {
          "References": "Educational psychologist, vol. 41, no. 2, pp. 111–127, 2006."
        },
        {
          "References": "3. Y. Liu, X. Zhang, Y. Li, J. Zhou, X. Li, and G. Zhao,\n“Graph-based facial affect"
        },
        {
          "References": "analysis: A review,”\nIEEE Transactions on Affective Computing, 2022."
        },
        {
          "References": "4. E. Wood, T. Baltrusaitis, X. Zhang, Y. Sugano, P. Robinson, and A. Bulling,"
        },
        {
          "References": "“Rendering of eyes for eye-shape registration and gaze estimation,”\nin Proceedings"
        },
        {
          "References": "of\nthe IEEE international conference on computer vision, 2015, pp. 3756–3764."
        },
        {
          "References": "5.\nJ. A. Fredricks, P. C. Blumenfeld, and A. H. Paris, “School engagement: Potential"
        },
        {
          "References": "of the concept, state of the evidence,” Review of educational research, vol. 74, no. 1,"
        },
        {
          "References": "pp. 59–109, 2004."
        },
        {
          "References": "6. S. D’Mello and A. Graesser, “Dynamics of affective states during complex learning,”"
        },
        {
          "References": "Learning and Instruction, vol. 22, no. 2, pp. 145–157, 2012."
        },
        {
          "References": "7.\nJ. Ocumpaugh, “Baker rodrigo ocumpaugh monitoring protocol (bromp) 2.0 tech-"
        },
        {
          "References": "nical and training manual,” New York, NY and Manila, Philippines: Teachers Col-"
        },
        {
          "References": "lege, Columbia University and Ateneo Laboratory for the Learning Sciences, vol. 60,"
        },
        {
          "References": "2015."
        },
        {
          "References": "8. S. N. Karimah and S. Hasegawa,\n“Automatic engagement estimation in smart ed-"
        },
        {
          "References": "ucation/learning settings: a systematic review of engagement definitions, datasets,"
        },
        {
          "References": "and methods,” Smart Learning Environments, vol. 9, no. 1, pp. 1–48, 2022."
        },
        {
          "References": "9. M. Dewan, M. Murshed, and F. Lin,\n“Engagement detection in online learning: a"
        },
        {
          "References": "review,” Smart Learning Environments, vol. 6, no. 1, pp. 1–20, 2019."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "16\nAli Abedi and Shehroz S. Khan": "10. S. S. Khan, A. Abedi, and T. Colella,\n“Inconsistencies\nin measuring student en-"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "gagement in virtual\nlearning–a critical review,” arXiv preprint arXiv:2208.04548,"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "2022."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "11. Z. Cai, S. Ghosh, K. Stefanov, A. Dhall, J. Cai, H. Rezatofighi, R. Haffari, and"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "M. Hayat,\n“Marlin: Masked autoencoder for facial video representation learning,”"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "in Proceedings of\nthe\nIEEE/CVF Conference on Computer Vision and Pattern"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "Recognition, 2023, pp. 1493–1504."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "12. M. Singh, X. Hoque, D. Zeng, Y. Wang, K. Ikeda, and A. Dhall,\n“Do i have your"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "attention: A large scale engagement prediction dataset and baselines,”\nin Proceed-"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "ings of the 25th International Conference on Multimodal Interaction, ser. ICMI ’23."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "New York, NY, USA: Association for Computing Machinery, 2023, p. 174–182."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "13. A. Abedi and S. S. Khan,\n“Affect-driven ordinal engagement measurement\nfrom"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "video,” Multimedia Tools and Applications, pp. 1–20, 2023."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "14. C. Lugaresi, J. Tang, H. Nash, C. McClanahan, E. Uboweja, M. Hays, F. Zhang,"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "“Mediapipe: A framework for building\nC.-L. Chang, M. G. Yong, J. Lee et al.,"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "perception pipelines,” arXiv preprint arXiv:1906.08172, 2019."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "15.\nJ. Wei, W. Peng, G. Lu, Y. Li, J. Yan, and G. Zhao, “Geometric graph representa-"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "tion with learnable graph structure and adaptive au constraint for micro-expression"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "recognition,”\nIEEE Transactions on Affective Computing, 2023."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "16. K. Zheng, J. Wu, J. Zhang, and C. Guo,\n“A skeleton-based rehabilitation exer-"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "cise assessment\nsystem with rotation invariance,”\nIEEE Transactions on Neural"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "Systems and Rehabilitation Engineering, vol. 31, pp. 2612–2621, 2023."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "17. S. Malek and S. Rossi, “Head pose estimation using facial-landmarks classification"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "for children rehabilitation games,” Pattern Recognition Letters, vol. 152, pp. 406–"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "412, 2021."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "18. G. M. Jacob and B. Stenger,\n“Facial action unit detection with transformers,”"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "in Proceedings of\nthe\nIEEE/CVF Conference on Computer Vision and Pattern"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "Recognition, 2021, pp. 7680–7689."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "19. A. Toisoul, J. Kossaifi, A. Bulat, G. Tzimiropoulos, and M. Pantic, “Estimation of"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "continuous valence and arousal\nlevels from faces in naturalistic conditions,” Nature"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "Machine Intelligence, vol. 3, no. 1, pp. 42–50, 2021."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "20.\nI. Grishchenko, A. Ablavatski, Y. Kartynnik, K. Raveendran, and M. Grundmann,"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "“Attention mesh: High-fidelity face mesh prediction in real-time,” arXiv preprint"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "arXiv:2006.10962, 2020."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "21. S. Yan, Y. Xiong, and D. Lin,\n“Spatial\ntemporal graph convolutional networks"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "for skeleton-based action recognition,”\nin Proceedings of\nthe AAAI conference on"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "artificial\nintelligence, vol. 32, no. 1, 2018."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "22. X. Ma, M. Xu, Y. Dong, and Z. Sun,\n“Automatic student engagement\nin online"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "learning environment based on neural\nturing machine,”\nInternational Journal of"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "Information and Education Technology, vol. 11, no. 3, pp. 107–111, 2021."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "23. O. Copur, M. Nakıp, S. Scardapane, and J. Slowack,\n“Engagement detection with"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "multi-task training in e-learning environments,”\nin International Conference on"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "Springer, 2022, pp. 411–422.\nImage Analysis and Processing."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "24. A. Abedi\nand S. S. Khan,\n“Detecting disengagement\nin virtual\nlearning\nas\nan"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "anomaly using temporal convolutional network autoencoder,” Signal,\nImage and"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "Video Processing, pp. 1–9, 2023."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "25. H. L. Fwa,\n“Fine-grained detection of academic\nemotions with spatial\ntemporal"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "graph attention networks using facial\nlandmarks,” 2022."
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "26. A. Gupta, A. D’Cunha, K. Awasthi, and V. Balasubramanian,\n“Daisee: Towards"
        },
        {
          "16\nAli Abedi and Shehroz S. Khan": "user engagement recognition in the wild,” arXiv preprint arXiv:1609.01885, 2016."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length\n17": "27. A. Abedi and S. S. Khan, “Improving state-of-the-art in detecting student engage-"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "ment with resnet and tcn hybrid network,”\nin 2021 18th Conference on Robots and"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "IEEE, 2021, pp. 151–157.\nVision (CRV)."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "28. X. Ai, V. S. Sheng, and C. Li,\n“Class-attention video transformer for engagement"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "intensity prediction,” arXiv preprint arXiv:2208.07216, 2022."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "29.\nJ. Liao, Y. Liang, and J. Pan, “Deep facial spatiotemporal network for engagement"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "prediction in online learning,” Applied Intelligence, vol. 51, no. 10, pp. 6609–6621,"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "2021."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "30. T. Selim, I. Elkabani, and M. A. Abdou,\n“Students engagement level detection in"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "online e-learning using hybrid efficientnetb7 together with tcn,\nlstm, and bi-lstm,”"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "IEEE Access, vol. 10, pp. 99 573–99 583, 2022."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "31. T. Baltrusaitis, A. Zadeh, Y. C. Lim, and L.-P. Morency, “Openface 2.0: Facial be-"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "havior analysis toolkit,”\nin 2018 13th IEEE international conference on automatic"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "IEEE, 2018, pp. 59–66.\nface & gesture recognition (FG 2018)."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "32. A. Abedi, C. Thomas, D. B. Jayagopi, and S. S. Khan,\n“Bag of\nstates: A non-"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "sequential\napproach\nto\nvideo-based\nengagement measurement,”\narXiv\npreprint"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "arXiv:2301.06730, 2023."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "33. C. Thomas, N. Nair, and D. B. Jayagopi,\n“Predicting engagement\nintensity in"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "the wild using temporal convolutional network,”\nin Proceedings of\nthe 20th ACM"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "International Conference on Multimodal Interaction, 2018, pp. 604–610."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "34. A. Vedernikov, P. Kumar, H. Chen, T. Seppänen, and X. Li, “Tcct-net: Two-stream"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "network architecture for\nfast and efficient engagement estimation via behavioral"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "feature signals,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "and Pattern Recognition, 2024, pp. 4723–4732."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "35. X. Tian, B. P. Nunes, Y. Liu, and R. Manrique,\n“Predicting student engagement"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "using sequential ensemble model,”\nIEEE Transactions on Learning Technologies,"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "2023."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "36.\nJ. Zhou, X. Zhang, Y. Liu, and X. Lan, “Facial expression recognition using spatial-"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "temporal\nsemantic graph network,”\nin 2020 IEEE International Conference on"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "Image Processing (ICIP), 2020, pp. 1961–1965."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "37. D. King,\n“Dlib c++ library,” http://dlib.net/, 2024."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "38. Y. Liu, X. Zhang, Y. Lin, and H. Wang,\n“Facial expression recognition via deep"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "action units graph network based on psychological mechanism,” IEEE Transactions"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "on Cognitive and Developmental Systems, vol. 12, no. 2, pp. 311–322, 2019."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "39. G. N. Yannakakis, R. Cowie, and C. Busso,\n“The ordinal nature of emotions: An"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "emerging approach,”\nIEEE Transactions on Affective Computing, vol. 12, no. 1,"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "pp. 16–35, 2018."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "40.\nJ. Whitehill, Z. Serpell, Y.-C. Lin, A. Foster, and J. R. Movellan,\n“The faces of"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "engagement: Automatic recognition of student engagementfrom facial expressions,”"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "IEEE Transactions on Affective Computing, vol. 5, no. 1, pp. 86–98, 2014."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "41. E. Frank and M. Hall,\n“A simple approach to ordinal classification,”\nin Machine"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "Learning: ECML 2001: 12th European Conference on Machine Learning Freiburg,"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "Springer, 2001, pp. 145–156.\nGermany, September 5–7, 2001 Proceedings 12."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "42. C. Thomas, K. P. Sarma, S. S. Gajula, and D. B. Jayagopi,\n“Automatic predic-"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "tion of presentation style and student engagement\nfrom videos,” Computers and"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "Education: Artificial Intelligence, p. 100079, 2022."
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "43. K. Zheng, J. Wu, J. Zhang, and C. Guo,\n“A skeleton-based rehabilitation exer-"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "cise assessment\nsystem with rotation invariance,”\nIEEE Transactions on Neural"
        },
        {
          "Title Suppressed Due to Excessive Length\n17": "Systems and Rehabilitation Engineering, 2023."
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "18\nAli Abedi and Shehroz S. Khan": "44. X. Chen, L. Niu, A. Veeraraghavan, and A. Sabharwal,"
        },
        {
          "18\nAli Abedi and Shehroz S. Khan": "mation of gameplay engagement"
        },
        {
          "18\nAli Abedi and Shehroz S. Khan": "Transactions on Affective Computing, 2019."
        },
        {
          "18\nAli Abedi and Shehroz S. Khan": "45. A. Abedi, M. Malmirian,\nand S. S. Khan,\n“Cross-modal"
        },
        {
          "18\nAli Abedi and Shehroz S. Khan": "augmentation\nfor\nrehabilitation\nexercise\nquality"
        },
        {
          "18\nAli Abedi and Shehroz S. Khan": "arXiv:2306.09546, 2023."
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Engagement detection and its applications in learning: A tutorial and selective review",
      "authors": [
        "B Booth",
        "N Bosch",
        "S D'mello"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "2",
      "title": "The four-phase model of interest development",
      "authors": [
        "S Hidi",
        "K Renninger"
      ],
      "year": "2006",
      "venue": "Educational psychologist"
    },
    {
      "citation_id": "3",
      "title": "Graph-based facial affect analysis: A review",
      "authors": [
        "Y Liu",
        "X Zhang",
        "Y Li",
        "J Zhou",
        "X Li",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Rendering of eyes for eye-shape registration and gaze estimation",
      "authors": [
        "E Wood",
        "T Baltrusaitis",
        "X Zhang",
        "Y Sugano",
        "P Robinson",
        "A Bulling"
      ],
      "year": "2015",
      "venue": "Proceedings"
    },
    {
      "citation_id": "5",
      "title": "School engagement: Potential of the concept, state of the evidence",
      "authors": [
        "J Fredricks",
        "P Blumenfeld",
        "A Paris"
      ],
      "year": "2004",
      "venue": "Review of educational research"
    },
    {
      "citation_id": "6",
      "title": "Dynamics of affective states during complex learning",
      "authors": [
        "S Mello",
        "A Graesser"
      ],
      "year": "2012",
      "venue": "Learning and Instruction"
    },
    {
      "citation_id": "7",
      "title": "Baker rodrigo ocumpaugh monitoring protocol (bromp) 2.0 technical and training manual",
      "authors": [
        "J Ocumpaugh"
      ],
      "year": "2015",
      "venue": "Baker rodrigo ocumpaugh monitoring protocol (bromp) 2.0 technical and training manual"
    },
    {
      "citation_id": "8",
      "title": "Automatic engagement estimation in smart education/learning settings: a systematic review of engagement definitions, datasets, and methods",
      "authors": [
        "S Karimah",
        "S Hasegawa"
      ],
      "year": "2022",
      "venue": "Smart Learning Environments"
    },
    {
      "citation_id": "9",
      "title": "Engagement detection in online learning: a review",
      "authors": [
        "M Dewan",
        "M Murshed",
        "F Lin"
      ],
      "year": "2019",
      "venue": "Smart Learning Environments"
    },
    {
      "citation_id": "10",
      "title": "Inconsistencies in measuring student engagement in virtual learning-a critical review",
      "authors": [
        "S Khan",
        "A Abedi",
        "T Colella"
      ],
      "year": "2022",
      "venue": "Inconsistencies in measuring student engagement in virtual learning-a critical review",
      "arxiv": "arXiv:2208.04548"
    },
    {
      "citation_id": "11",
      "title": "Marlin: Masked autoencoder for facial video representation learning",
      "authors": [
        "Z Cai",
        "S Ghosh",
        "K Stefanov",
        "A Dhall",
        "J Cai",
        "H Rezatofighi",
        "R Haffari",
        "M Hayat"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Do i have your attention: A large scale engagement prediction dataset and baselines",
      "authors": [
        "M Singh",
        "X Hoque",
        "D Zeng",
        "Y Wang",
        "K Ikeda",
        "A Dhall"
      ],
      "year": "2023",
      "venue": "Proceedings of the 25th International Conference on Multimodal Interaction, ser. ICMI '23"
    },
    {
      "citation_id": "13",
      "title": "Affect-driven ordinal engagement measurement from video",
      "authors": [
        "A Abedi",
        "S Khan"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "14",
      "title": "Mediapipe: A framework for building perception pipelines",
      "authors": [
        "C Lugaresi",
        "J Tang",
        "H Nash",
        "C Mcclanahan",
        "E Uboweja",
        "M Hays",
        "F Zhang",
        "C.-L Chang",
        "M Yong",
        "J Lee"
      ],
      "year": "2019",
      "venue": "Mediapipe: A framework for building perception pipelines",
      "arxiv": "arXiv:1906.08172"
    },
    {
      "citation_id": "15",
      "title": "Geometric graph representation with learnable graph structure and adaptive au constraint for micro-expression recognition",
      "authors": [
        "J Wei",
        "W Peng",
        "G Lu",
        "Y Li",
        "J Yan",
        "G Zhao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "A skeleton-based rehabilitation exercise assessment system with rotation invariance",
      "authors": [
        "K Zheng",
        "J Wu",
        "J Zhang",
        "C Guo"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "17",
      "title": "Head pose estimation using facial-landmarks classification for children rehabilitation games",
      "authors": [
        "S Malek",
        "S Rossi"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "18",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "G Jacob",
        "B Stenger"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "A Toisoul",
        "J Kossaifi",
        "A Bulat",
        "G Tzimiropoulos",
        "M Pantic"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Attention mesh: High-fidelity face mesh prediction in real-time",
      "authors": [
        "I Grishchenko",
        "A Ablavatski",
        "Y Kartynnik",
        "K Raveendran",
        "M Grundmann"
      ],
      "year": "2020",
      "venue": "Attention mesh: High-fidelity face mesh prediction in real-time",
      "arxiv": "arXiv:2006.10962"
    },
    {
      "citation_id": "21",
      "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "S Yan",
        "Y Xiong",
        "D Lin"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "22",
      "title": "Automatic student engagement in online learning environment based on neural turing machine",
      "authors": [
        "X Ma",
        "M Xu",
        "Y Dong",
        "Z Sun"
      ],
      "year": "2021",
      "venue": "International Journal of Information and Education Technology"
    },
    {
      "citation_id": "23",
      "title": "Engagement detection with multi-task training in e-learning environments",
      "authors": [
        "O Copur",
        "M Nakıp",
        "S Scardapane",
        "J Slowack"
      ],
      "year": "2022",
      "venue": "International Conference on Image Analysis and Processing"
    },
    {
      "citation_id": "24",
      "title": "Detecting disengagement in virtual learning as an anomaly using temporal convolutional network autoencoder",
      "authors": [
        "A Abedi",
        "S Khan"
      ],
      "year": "2023",
      "venue": "Signal, Image and Video Processing"
    },
    {
      "citation_id": "25",
      "title": "Fine-grained detection of academic emotions with spatial temporal graph attention networks using facial landmarks",
      "authors": [
        "H Fwa"
      ],
      "year": "2022",
      "venue": "Fine-grained detection of academic emotions with spatial temporal graph attention networks using facial landmarks"
    },
    {
      "citation_id": "26",
      "title": "Daisee: Towards user engagement recognition in the wild",
      "authors": [
        "A Gupta",
        "A D'cunha",
        "K Awasthi",
        "V Balasubramanian"
      ],
      "year": "2016",
      "venue": "Daisee: Towards user engagement recognition in the wild",
      "arxiv": "arXiv:1609.01885"
    },
    {
      "citation_id": "27",
      "title": "Improving state-of-the-art in detecting student engagement with resnet and tcn hybrid network",
      "authors": [
        "A Abedi",
        "S Khan"
      ],
      "year": "2021",
      "venue": "2021 18th Conference on Robots and Vision (CRV)"
    },
    {
      "citation_id": "28",
      "title": "Class-attention video transformer for engagement intensity prediction",
      "authors": [
        "X Ai",
        "V Sheng",
        "C Li"
      ],
      "year": "2022",
      "venue": "Class-attention video transformer for engagement intensity prediction",
      "arxiv": "arXiv:2208.07216"
    },
    {
      "citation_id": "29",
      "title": "Deep facial spatiotemporal network for engagement prediction in online learning",
      "authors": [
        "J Liao",
        "Y Liang",
        "J Pan"
      ],
      "year": "2021",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "30",
      "title": "Students engagement level detection in online e-learning using hybrid efficientnetb7 together with tcn, lstm, and bi-lstm",
      "authors": [
        "T Selim",
        "I Elkabani",
        "M Abdou"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "31",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "32",
      "title": "Bag of states: A nonsequential approach to video-based engagement measurement",
      "authors": [
        "A Abedi",
        "C Thomas",
        "D Jayagopi",
        "S Khan"
      ],
      "year": "2023",
      "venue": "Bag of states: A nonsequential approach to video-based engagement measurement",
      "arxiv": "arXiv:2301.06730"
    },
    {
      "citation_id": "33",
      "title": "Predicting engagement intensity in the wild using temporal convolutional network",
      "authors": [
        "C Thomas",
        "N Nair",
        "D Jayagopi"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "34",
      "title": "Tcct-net: Two-stream network architecture for fast and efficient engagement estimation via behavioral feature signals",
      "authors": [
        "A Vedernikov",
        "P Kumar",
        "H Chen",
        "T Seppänen",
        "X Li"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "35",
      "title": "Predicting student engagement using sequential ensemble model",
      "authors": [
        "X Tian",
        "B Nunes",
        "Y Liu",
        "R Manrique"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Learning Technologies"
    },
    {
      "citation_id": "36",
      "title": "Facial expression recognition using spatialtemporal semantic graph network",
      "authors": [
        "J Zhou",
        "X Zhang",
        "Y Liu",
        "X Lan"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "37",
      "title": "Dlib c++ library",
      "authors": [
        "D King"
      ],
      "year": "2024",
      "venue": "Dlib c++ library"
    },
    {
      "citation_id": "38",
      "title": "Facial expression recognition via deep action units graph network based on psychological mechanism",
      "authors": [
        "Y Liu",
        "X Zhang",
        "Y Lin",
        "H Wang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "39",
      "title": "The ordinal nature of emotions: An emerging approach",
      "authors": [
        "G Yannakakis",
        "R Cowie",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "The faces of engagement: Automatic recognition of student engagementfrom facial expressions",
      "authors": [
        "J Whitehill",
        "Z Serpell",
        "Y.-C Lin",
        "A Foster",
        "J Movellan"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "A simple approach to ordinal classification",
      "authors": [
        "E Frank",
        "M Hall"
      ],
      "year": "2001",
      "venue": "Machine Learning: ECML 2001: 12th European Conference on Machine Learning Freiburg"
    },
    {
      "citation_id": "42",
      "title": "Automatic prediction of presentation style and student engagement from videos",
      "authors": [
        "C Thomas",
        "K Sarma",
        "S Gajula",
        "D Jayagopi"
      ],
      "year": "2022",
      "venue": "Computers and Education: Artificial Intelligence"
    },
    {
      "citation_id": "43",
      "title": "A skeleton-based rehabilitation exercise assessment system with rotation invariance",
      "authors": [
        "K Zheng",
        "J Wu",
        "J Zhang",
        "C Guo"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "44",
      "title": "Faceengage: robust estimation of gameplay engagement from user-contributed (youtube) videos",
      "authors": [
        "X Chen",
        "L Niu",
        "A Veeraraghavan",
        "A Sabharwal"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Cross-modal video to body-joints augmentation for rehabilitation exercise quality assessment",
      "authors": [
        "A Abedi",
        "M Malmirian",
        "S Khan"
      ],
      "year": "2023",
      "venue": "Cross-modal video to body-joints augmentation for rehabilitation exercise quality assessment",
      "arxiv": "arXiv:2306.09546"
    }
  ]
}