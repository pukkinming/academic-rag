{
  "paper_id": "2004.14198v2",
  "title": "Multimodal Routing: Improving Local And Global Interpretability Of Multimodal Language Analysis",
  "published": "2020-04-29T13:42:22Z",
  "authors": [
    "Yao-Hung Hubert Tsai",
    "Martin Q. Ma",
    "Muqiao Yang",
    "Ruslan Salakhutdinov",
    "Louis-Philippe Morency"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The human language can be expressed through multiple sources of information known as modalities, including tones of voice, facial gestures, and spoken language. Recent multimodal learning with strong performances on human-centric tasks such as sentiment analysis and emotion recognition are often blackbox, with very limited interpretability. In this paper we propose Multimodal Routing, which dynamically adjusts weights between input modalities and output representations differently for each input sample. Multimodal routing can identify relative importance of both individual modalities and cross-modality features. Moreover, the weight assignment by routing allows us to interpret modalityprediction relationships not only globally (i.e. general trends over the whole dataset), but also locally for each single input sample, meanwhile keeping competitive performance compared to state-of-the-art methods. * indicates equal contribution. Code is available at https://github.com/martinmamql/ multimodal_routing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The human language contains multimodal cues, including textual (e.g., spoken or written words), visual (e.g., body gestures), and acoustic (e.g., voice tones) modalities. It acts as a medium for human communication and has been advanced in areas spanning affect recognition  (Busso et al., 2008) , media description  (Lin et al., 2014) , and multimedia information retrieval  (Abu-El-Haija et al., 2016) . Modeling multimodal sources requires to understand the relative importance of not only each single modality (defined as unimodal explanatory features) but also the interactions (defined as bimodal or trimodal explanatory features)  (Büchel et al., 1998) . Recent work  (Liu et al., 2018; Williams et al., 2018; Ortega et al., 2019)  proposed methods to fuse information across modalities and yielded superior performance, but these models are often black-box with very limited interpretability. Interpretability matters. It allows us to identify crucial explanatory features for predictions. Such interpretability knowledge could be used to provide insights into multimodal learning, improve the model design, or debug a dataset. This inerpretability is useful at two levels: the global and the local level. The global interpretation reflects the general (averaged) trends of explanatory feature importance over the whole dataset. The local interpretation is arguably harder but can give a highresolution insight of feature importance specifically depending on each individual samples during training and inference. These two levels of interpretability should provide us an understanding of unimodal, bimodal and trimodal explanatory features.  In this paper we address both local and global interpretability of unimodal, bimodal and trimodal explanatory featuress by presenting Multimodal Routing. In human multimodal language, such routing dynamically changes weights between modalities and output labels for each sample as shown in Fig.  1 . The most significant contribution of Multimodal Routing is its ability to establish local weights dynamically for each input sample between modality features and the labels during training and inference, thus providing local interpretation for each sample.\n\nOur experiments focus on two tasks of sentiment analysis and emotion recognition tasks using two benchmark multimodal language datasets, IEMO-CAP  (Busso et al., 2008)  and CMU-MOSEI  (Zadeh et al., 2018) . We first study how our model compares with the state-of-the-art methods on these tasks. More importantly we provide local interpretation by qualitatively analyzing adjusted local weights for each sample. Then we also analyze the global interpretation using statistical techniques to reveal crucial features for prediction on average. Such interpretation of different resolutions strengthens our understanding of multimodal language learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Multimodal language learning is based on the fact that human integrates multiple sources such as acoustic, textual, and visual information to learn language  (McGurk and MacDonald, 1976; Ngiam et al., 2011; Baltrušaitis et al., 2018) . Recent ad-vances in modeling multimodal language using deep neural networks are not interpretable  (Wang et al., 2019; Tsai et al., 2019a) . Linear method like the Generalized Additive Models (GAMs)  (Hastie, 2017)  do not offer local interpretability. Even though we could use post hoc (interpret predictions given an arbitrary model) methods such as LIME  (Ribeiro et al., 2016) , SHAP (Lundberg and Lee, 2017), and L2X  (Chen et al., 2018)  to interpret these black-box models, these interpretation methods are designed to detect the contributions only from unimodal features but not bimodal or trimodal explanatory features. It is shown that in human communication, modality interactions are more important than individual modalities  (Engle, 1998) .\n\nTwo recent methods, Graph-MFN  (Zadeh et al., 2018)  and Multimodal Factorized Model (MFM)  (Tsai et al., 2019b) , attempted to interpret relationships between modality interactions and learning for human language. Nonetheless, Graph-MFN did not separate the contributions among unimodal and multimodal explanatory features, and MFM only provided the analysis on trimodal interaction feature. Both of them cannot interpret how both single modality and modality interactions contribute to final prediction at the same time.\n\nOur method is inspired and related to Capsule Networks  (Sabour et al., 2017; Hinton et al., 2018) , which performs routing between layers of capsules. Each capsule is a group of neurons that encapsulates spatial information as well as the probability of an object being present. On the other hand, our method performs routing between multimodal features (i.e., unimodal, bimodal, and trimodal explanatory features) and concepts of the model's decision.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "The proposed Multimodal Routing contains three stages shown in Fig.  2 : encoding, routing, and prediction. The encoding stage encodes raw inputs (speech, text, and visual data) to unimodal, bimodal, and trimodal features. The routing stage contains a routing mechanism  (Sabour et al., 2017; Hinton et al., 2018) , which 1) updates some hidden representations and 2) adjusts local weights between each feature and each hidden representation by pairwise similarity. Following previous work  (Mao et al., 2019) , we call the hidden representations \"concepts\", and each of them is associated to specific a prediction label (in our case sentiment or an emotion). Finally, the prediction stage takes the inferred concepts to perform model prediction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Routing",
      "text": "We use v(isual), a(coustic), and t(ext) to denote the three commonly considered modalities in human multimodal language. Let x = {x a , x v , x t } represent the multimodal input. x a ∈ R Ta×da is an audio stream with time length T a and feature dimension d a (at each time step). Similarly, x v ∈ R Tv×dv is the visual stream and x t ∈ R Tt×dt is the text stream. In our paper, we consider multiclass or multilabel prediction tasks for the multimodal language modeling, in which we use y ∈ R J to denote the ground truth label with J being the number of classes or labels, and ŷ to represent the model's prediction. Our goal is to find the relative importance of the contributions from unimodal (e.g., x a itself), bimodal (e.g., the interaction between x a and x v ), and trimodal features (e.g., the interaction between x a , x v , and x t ) to the model prediction ŷ.\n\nEncoding Stage. The encoding stage encodes multimodal inputs {x a , x v , x t } into explanatory features. We use f i ∈ R d f to denote the features with i ∈ {a, v, t} being unimodal, i ∈ {av, vt, ta} being bimodal, and i ∈ {avt} being trimodal interactions. d f is the dimension of the feature. To be specific, f a = F a (x a ; θ), f av = F av (x a , x v ; θ), and f avt = F avt (x a , x v , x t ; θ) with θ as the parameters of the encoding functions and F as the encoding functions. Multimodal Transformer (MulT)  (Tsai et al., 2019a ) is adopted as the design of the encoding functions F i . Here the trimodal function F avt encodes sequences from three modalities into a unified representation, F av encodes acoustic and visual modalities, and F a encodes acoustic input. Next, p i ∈ [0, 1] is a scalar representing how each feature f i is activated in the model. Similar to f i , we also use MulT to encode p i from the input x i . That is, p a = P a (x a ; θ ), p av = P av (x a , x v , θ ), and p avt = P avt (x a , x v , x t , θ ) with θ as the parameters of MulT and P i as corresponding encoding functions (details in the Supplementary).\n\nRouting Stage. The goal of routing is to infer interpretable hidden representations (termed here as \"concepts\") for each output label. The first step of routing is to initialize the concepts with equal weights, where all explanatory features are as important. Then the core part of routing is an iterative process which will enforce for each explanatory feature to be assigned to only one output representations (a.k.a the \"concepts\"; in reality it is a soft assignment) that shows high similarity with a concept. Formally each concept c j ∈ R dc is represented as a one-dimensional vector of dimension d c . Linear weights r ij , which we term routing coefficient, are defined between each concept c j and explanatory factor f i .\n\nThe first half of routing, which we call routing adjustment, is about finding new assignment (i.e. the routing coefficients) between the input features and the newly learned concepts by taking a softmax of the dot product over all concepts, thus only the features showing high similarity of a concept (sentiment or an emotion in our case) will be assigned close to 1, instead of having all features assigned to all concepts. This will help local interpretability because we can always distinguish important explanatory features from non-important ones. The second half of routing, which we call concept update, is to update concepts by linearly aggregating the new input features weighted by the routing coefficients, so that it is local to each input samples.\n\n-Routing adjustment. We define the routing coefficient r ij ∈ [0, 1] by measuring the similarity 1\n\nProcedure 1 Multimodal Routing 1: procedure ROUTING({fi}, {pi}, {Wij}) 2:\n\nConcepts are initialized with uniform weights 3:\n\nfor t iterations do /* Routing Adjustment */ 4: for all feature i and concept j: sij ← (fiWij) cj 5:\n\nfor all feature i: rij ← exp sij / j exp s ij /* Concepts Update */ 6: for all concept j: cj ← i pirij(fiWij) return {cj} between f i W ij and c j :\n\nWe note that r ij is normalized over all concepts c j . Hence, it is a coefficient which takes high value only when f i is in agreement with c j but not with c j , where j = j.\n\n-Concept update. After obtaining p i from encoding stage, we update concepts c j using weighted average as follows: updates the concepts based on the routing weights by summing input features f i projected by weight matrix W ij to the space of the jth concept\n\nc j is now essentially a linear aggregation from\n\nWe summarize the routing procedure in Procedure 1, which returns concepts (c j ) given explanatory features (f i ), local weights (W ij ) and p i . First, we initialize the concepts with uniform weights. Then, we iteratively perform adjustment on routing coefficients (r ij ) and concept updates. Finally, we return the updated concepts.\n\nPrediction Stage. The prediction stage takes the inferred concepts to make predictions ŷ. Here, we apply linear transformations to concept c j to obtain the logits. Specifically, the jth logit is formulated as\n\nwhere o j ∈ R dc and is the weight of the linear transformation for the jth concept. Then, the Softmax (for multi-class task) or Sigmoid (for multi-label task) function is applied on the logits to obtain the prediction ŷ.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Interpretability",
      "text": "In this section, we provide the framework of locally interpreting relative importance of unimodal, bimodal, and trimodal explanatory features to model prediction given different samples, by interpreting the routing coefficients r ij , which represents the weight assignment between feature f i and concept c j . We also provide methods to globally interpret the model across the whole dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Local Interpretation",
      "text": "The goal of local interpretation is trying to understand how the importance of modality and modality interaction features change, given different multimodal samples. In eq. (  3 ), a decision logit considers an addition of the contributions from the unimodal {f a , f v , f t }, bimodal {f av , f vt , f ta }, and trimodal f avt explanatory features. The particular contribution from the feature f i to the jth concept is represented by\n\nIt takes large value when 1) p i of the feature f i is large; 2) the agreement r ij is high (the feature f i is in agreement with concept c j and is not in agreement with c j , where j = j); and 3) the dot product o j (f i W ij ) is large. Intuitively, any of the three scenarios requires high similarity between a modality feature and a concept vector which represents a specific sentiment or emotion. Note that p i , r ij and f i are the covariates and o j and W ij are the parameters in the model. Since different input samples yield distinct p i and r ij , we can locally interpret p i and r ij as the effects of the modality feature f i contributing to the jth logit of the model, which is roughly a confidence of predicting jth sentiment or emotion. We will show examples of local interpretations in the Interpretation Analysis section.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Global Interpretation",
      "text": "To globally interpret Multimodal Routing, we analyze r ij , the average values of routing coefficients r ij s over the entire dataset. Since eq. (  3 ) considers a linear effect from f i , p i and r ij to logit j , r ij represents the average assignment from feature f i to the jth logit. Instead of reporting the values for r ij , we provide a statistical interpretation on r ij using confidence intervals to provide a range of possible plausible coefficients with probability guarantees. Similar tests on p i and p i r ij are provided in Supplementary Materials. Here we choose confidence intervals over p-values because they provide much richer information  (Ranstam, 2012; Du Prel et al., 2009) . Suppose we have n data with the corre-\n\nIf n is large enough and r ij has finite mean and finite variance (it suffices since r ij ∈ [0, 1] is bounded), according to Central Limit Theorem, r ij (i.e., mean of r ij ) follows a Normal distribution:\n\nwhere µ is the true mean of r ij and s 2 n is the sample variance in r ij . Using eq. (  4 ), we can provide a confidence interval for r ij . We follow 95% confidence in our analysis.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we first provide details of experiments we perform and comparison between our proposed model and state-of-the-art (SOTA) method, as well as baseline models. We include interpretability analysis in the next section.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "We perform experiments on two publicly available benchmarks for human multimodal affect recognition: CMU-MOSEI  (Zadeh et al., 2018)  and IEMO-CAP  (Busso et al., 2008) . CMU-MOSEI  (Zadeh et al., 2018)  contains 23, 454 movie review video clips taken from YouTube. For each clip, there are two tasks: sentiment prediction (multiclass classification) and emotion recognition (multilabel classification). For the sentiment prediction task, each sample is labeled by an integer score in the range [-3, 3], indicating highly negative sentiment (-3) to highly positive sentiment (+3). We use some metrics as in prior work  (Zadeh et al., 2018) : seven class accuracy (Acc 7 : seven class classification in Z ∈ [-3, 3]), binary accuracy (Acc 2 : two-class classification in {-1, +1}), and F1 score of predictions. For the emotion recognition task, each sample is labeled by one or more emotions from {Happy, Sad, Angry, Fear, Disgust, Surprise}. We report the metrics  (Zadeh et al., 2018) : six-class accuracy (multilabel accuracy of predicting six emotion labels) and F1 score.\n\nIEMOCAP consists of 10K video clips for human emotion analysis. Each clip is evaluated and then assigned (possibly more than one) labels of emotions, making it a multilabel learning task. Following prior work and insight  (Tsai et al., 2019a; Tripathi et al., 2018; Jack et al., 2014) , we report on four emotions (happy, sad, angry, and neutral), with metrics four-class accuracy and F1 score.\n\nFor both datasets, we use the extracted features from a public SDK https://github.com/ A2Zadeh/CMU-MultimodalSDK, whose features are extracted from textual (GloVe word embedding  (Pennington et al., 2014) ), visual (Facet (iMotions, 2019)), and acoustic (COVAREP  (Degottex et al., 2014) ) modalities. The acoustic and vision features are processed to be aligned with the words (i.e., text features). We present results using this wordaligned setting in this paper, but ours can work on unaligned multimodal language sequences. The train, valid and test set split are following previous work  (Wang et al., 2019; Tsai et al., 2019a) .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study And Baseline Models",
      "text": "We provide two ablation studies for interpretable methods as baselines: The first is based on Generalized Additive Model (GAM)  (Hastie, 2017)  which directly sums over unimodal, bimodal, and trimodal features and then applies a linear transformation to obtain a prediction. This is equivalent to only using weight p i and no routing coefficients. The second is our denoted as Multimodal Routing * , which performs only one routing iteration (by setting t = 1 in Procedure 1) and does not iteratively adjust the routing and update the concepts.\n\nWe also choose other non-interpretable methods that achieved state-of-the-art to compare the performance of our approach to: Early Fusion LSTM (EF-LSTM), Late Fusion LSTM (LF-LSTM) (Hochreiter and Schmidhuber, 1997), Recurrent Attended Variation Embedding Network (RAVEN)  (Wang et al., 2019) , and Multimodal Transformer  (Tsai et al., 2019a) .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results And Discussions",
      "text": "We trained our model on 1 RTX 2080 GPU. We use 7 layers in the Multimodal Transformer, and choose the batch size as 32. The model is trained with initial learning rate of 10 -4 and Adam optimizer.\n\nCMU-MOSEI sentiment. Table  1  summarizes the results on this dataset. We first compare all the interpretable methods. We see that Multimodal Routing enjoys performance improvement over both GAM  (Hastie, 2017) , a linear model on encoded features, and Multimodal Routing * , a noniterative feed-forward net with same parameters as Multimodal Routing. The improvement suggests the proposed iterative routing can obtain a more robust prediction by dynamically associating the features and the concepts of the model's predictions. Next, when comparing to the non- interpretable methods, Multimodal Routing outperforms EF-LSTM, LF-LSTM and RAVEN models and performs competitively when compared with MulT  (Tsai et al., 2019a) . It is good to see that our method can competitive performance with the added advantage of local and global interpretability (see analysis in the later section). The configuration of our model is in the supplementary file.\n\nCMU-MOSEI emotion. We report the results in Table  2 . We do not report RAVEN  (Wang et al., 2019)  and MulT  (Tsai et al., 2019a)  since they did not report CMU-MOSEI results. Compared with all the baselines, Multimodal Routing performs again competitively on most of the results metrics. We note that the distribution of labels is skewed (e.g., there are disproportionately very few samples labeled as \"surprise\"). Hence, this skewness somehow results in the fact that all models end up predicting not \"surprise\", thus the same accuracy for \"surprise\" across all different approaches. IEMOCAP emotion. When looking at the IEMOCAP results in Table  1 , we see similar trends with CMU-MOSEI sentiment and CMU-MOSEI emotion, that multimodal routing achieves perfor-mance close to the SOTA method. We see a performance drop in the emotion \"happy\", but our model outperforms the SOTA method for the emotion \"angry\".",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Interpretation Analysis",
      "text": "In this section, we revisit our initial research question: how to locally identify the importance or contribution of unimodal features and the bimodal or trimodal interactions? We provide examples in this section on how multimodal routing can be used to see the variation of contributions. We first present the local interpretation and then the global interpretation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Local Interpretation Analysis",
      "text": "We show how our model makes decisions locally for each specific input sample by looking at inferred coefficients p i r ij . Different samples create different p i and r ij , and their product represents how each feature vector contributes to final prediction locally, thus providing local interpretability. We provide such interpretability analysis on examples from CMU-MOSEI sentiment prediction and emotion recognition, and illustrate them in Fig.  3 . For sentiment prediction, we show samples with true labels neutral (0), most negative sentiment (-3), and most positive (+3) sentiment score. For emotion recognition, we illustrate examples with true label \"happy\", \"sad\", and \"disgust\" emotions. A color leaning towards red in the rightmost spectrum stands for a high association, while a color leaning towards blue suggests a low association.\n\nIn the upper-left example in Fig.  3 , a speaker is introducing movie Sweeny Todd. He says the movie is a musical and suggests those who dislike musicals not to see the movie. Since he has no personal judgment on whether he personally likes or dislikes the movie, his sentiment is classified as neutral (0), although the text modality (i.e., transcript) contains a \"don't\". In the vision modality (i.e., videos), he frowns when he mentions this movie is musical, but we cannot conclude his sentiment to be neutral by only looking at the visual modality. By looking at both vision and text together (their interaction), the confidence in neutral is high. The model gives the text-vision interaction feature a high value of p i r ij to suggest it highly contributes to the prediction, which confirms our reasoning above.\n\nSimilarly, for the bottom-left example, the speaker is sharing her experience on how to audition for a Broadway show. She talks about a very detailed and successful experience of herself and describes \"love\" in her audition monologue, which is present in the text. Also, she has a dramatic smile and a happy tone. We believe all modalities play a role in the prediction. As a result, the trimodal interaction feature contributes significantly to the prediction of happiness, according to our model.\n\nNotably, by looking at the six examples overall, we could see each individual sample bears a different pattern of feature importance, even when the sentiment is the same. This is a good debuging and interpretation tool. For global interpretation, all these samples will be averaged giving more of a general trend.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Global Interpretation Analysis",
      "text": "Here we analyze the global interpretation of Multimodal Routing. Given the averaged routing coefficients r ij generated and aggregated locally from samples, we want to know the overall connection between each modality or modality interaction and each concept across the whole dataset.    (bottom) . We bold the values that have the largest mean in each emotion and are significantly larger than a uniform routing (1/J = 1/7 = 0.143).\n\nthese routing coefficients we will compare them to uniform weighting, i.e., 1 J where J is the number of concepts. To perform such analysis, we provide confidence intervals of each r ij . If this interval is outside of 1 J , we can interpret it as a distinguishably significant feature. See Supplementary for similar analysis performed on p i r ij and p i .\n\nFirst we provide confidence intervals of r ij sampled from CMU-MOSEI sentiment. We compare our confidence intervals with the value 1 J . From top part of Table  3 , we can see that our model relies identified language modality for neutral sentiment predictions; acoustic modality for extremely negative predictions (row r a column -3); and textacoustic bimodal interaction for extremely positive predictions (row r ta column 3). Similarly, we analyze r ij sampled from CMU-MOSEI emotion (bottom part of Table  3 ). We can see that our model identified the text modality for predicting emotion fear (row r t column Fear, the same indexing for later cases), the acoustic modality for predicting emotion disgust, the text-acoustic interaction for predicting emotion surprise, and the acoustic-visual interaction for predicting emotion angry. For emotion happy and sad, either trimodal interaction has the most significant connection, or the routing is not significantly different among modalities.\n\nInterestingly, these results echo previous research. In both sentiment and emotion cases, acoustic features are crucial for predicting negative sen-timent or emotions. This well aligns with research results in behavior science  (Lima et al., 2013) . Furthermore, (Livingstone and Russo, 2018) showed that the intensity of emotion angry is stronger in acoustic-visual than in either acoustic or visual modality in human speech.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we presented Multimodal Routing to identify the contributions from unimodal, bimodal and trimodal explanatory features to predictions in a locally manner. For each specific input, our method dynamically associates an explanatory feature with a prediction if the feature explains the prediction well. Then, we interpret our approach by analyzing the routing coefficients, showing great variation of feature importance in different samples. We also conduct global interpretation over the whole datasets, and show that the acoustic features are crucial for predicting negative sentiment or emotions, and the acoustic-visual interactions are crucial for predicting emotion angry. These observations align with prior work in psychological research. The advantage of both local and global interpretation is achieved without much loss of performance compared to the SOTA methods. We believe that this work sheds light on the advantages of understanding human behaviors from a multimodal perspective, and makes a step towards introducing more interpretable multimodal language models. .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Encoding P I From Input",
      "text": "In practice, we use the same MulT to encode f i and p i simultaneously. We design MulT to have an output dimension d f + 1. A sigmoid function is applied to the last dimension of the output. For this output, the first d f dimensions refers to f i and the last dimension refers to p i .\n\n.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Training Details And Hyper-Parameters",
      "text": "Our model is trained using the Adam (Kingma and Ba, 2014) optimizer with a batch size of 32. The learning rate is 1e-4 for CMU-MOSEI Sentiment and IEMOCAP, and 1e-5 for CMU-MOSEI emotion. We apply a dropout  (Srivastava et al., 2014)  of 0.5 during training.\n\nFor the encoding stage, we use MulT  (Tsai et al., 2019a)  as feature extractor. After the encoder producing unimodal, bimodal, and trimodal features, we performs linear transformation for each feature, and output feature vectors with dimension d f = 64.\n\nWe perform two iterations of routing between features and concepts with dimension d c = 64 where d c is the dimension of concepts. All experiments use the same hyper-parameter configuration in this paper.  .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Remarks On Cmu-Mosei Sentiment",
      "text": "Our model poses the problem as classification and predicts only integer labels, so we don't provide mean average error and correlation metrics.\n\n.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Remarks On Cmu-Mosei Emotion",
      "text": "Due to the introduction of concepts in our model, we transform the CMU-MOSEI emotion recognition task from a regression problem (every emotion has a score in [0, 3] indicating how strong the evidence of that emotion is) to a classification problem. For each sample with six emotion scores, we label all emotions with scores greater than zero to be present in the sample. Then a data sample would have a multiclass label.\n\n.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Global Interpretation Result",
      "text": "We analyze global interpretation of both CMU-MOSEI sentiment and emotion task.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Cmu-Mosei Sentiment",
      "text": "The analysis of the routing coefficients r ij is included in the main paper. We then analyze p i (table 6) and the products p i r ij (table 4). Same as analysis in the main paper, our model relies on acoustic modality for extremely negative predictions (row r a column -3) and textacoustic bimodal interaction for extremely positive predictions (row r ta column 3). The sentiment that is neutral or less extreme are predicted by contribu-tions from many different modalities / interactions. The activation table shows high activation value (> 0.8) for most modality / interactions except p vl .\n\nCMU-MOSEI Emotion Same as above, we analyze p i (Table  7 ) and the product p i r ij (Table  5 ).The result is very similar to that of r ij . The activation table shows high activation value (> 0.8) for most modality / interactions except p vl , same as CMU-MOSEI sentiment. We see strong connections between audio-visual interactions and angry, text modality and fear, audio modality and disgust, and text-audio interactions and surprise. The activation table shows high activation value (> 0.8) for most modality / interactions except p vl as well.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of Multimodal Routing, where",
      "page": 1
    },
    {
      "caption": "Figure 2: Overview of Multimodal Routing, which contains encoding, routing, and prediction stages. We consider",
      "page": 2
    },
    {
      "caption": "Figure 1: The most signiﬁcant contribution of",
      "page": 2
    },
    {
      "caption": "Figure 2: encoding, routing, and",
      "page": 3
    },
    {
      "caption": "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples",
      "page": 7
    },
    {
      "caption": "Figure 3: For sentiment prediction, we show samples with",
      "page": 7
    },
    {
      "caption": "Figure 3: , a speaker is",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "",
          "yielded superior performance, but these models are": "often black-box with very limited interpretability."
        },
        {
          "Abstract": "The human language can be expressed through",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "multiple\nsources\nof\ninformation\nknown\nas",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "modalities, including tones of voice, facial ges-",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "tures,\nand spoken language.\nRecent multi-",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "modal\nlearning with strong performances on",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "human-centric tasks such as sentiment analy-",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "sis and emotion recognition are often black-",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "box, with\nvery\nlimited\ninterpretability.\nIn",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "this\npaper we\npropose Multimodal Routing,",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "which dynamically adjusts weights between in-",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "put modalities and output\nrepresentations dif-",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "ferently for each input\nsample. Multimodal",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "routing\ncan\nidentify\nrelative\nimportance\nof",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "both individual modalities and cross-modality",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "features.\nMoreover,\nthe weight\nassignment",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "by\nrouting\nallows\nus\nto\ninterpret modality-",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "prediction relationships not only globally (i.e.",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "general trends over the whole dataset), but also",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "locally for\neach single\ninput\nsample, mean-",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "",
          "yielded superior performance, but these models are": "Figure 1: An example of Multimodal Routing, where"
        },
        {
          "Abstract": "while keeping competitive performance com-",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "",
          "yielded superior performance, but these models are": "the weights between visual,\ntextual, and visual-textual"
        },
        {
          "Abstract": "pared to state-of-the-art methods.",
          "yielded superior performance, but these models are": ""
        },
        {
          "Abstract": "",
          "yielded superior performance, but these models are": "explanatory features and concepts of emotions (happy"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "pared to state-of-the-art methods."
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "1\nIntroduction"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "The human language contains multimodal cues, in-"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "cluding textual (e.g., spoken or written words), vi-"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "sual (e.g., body gestures), and acoustic (e.g., voice"
        },
        {
          "while keeping competitive performance com-": "tones) modalities.\nIt acts as a medium for human"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "communication and has been advanced in areas"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "spanning affect\nrecognition (Busso et al., 2008),"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "media description (Lin et al., 2014), and multi-"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "media information retrieval (Abu-El-Haija et al.,"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "2016). Modeling multimodal sources requires to"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "understand the relative importance of not only each"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "single modality (deﬁned as unimodal explanatory"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "features) but also the interactions (deﬁned as bi-"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "modal or trimodal explanatory features) (B¨uchel"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "et al., 1998).\nRecent work\n(Liu et al., 2018;"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "Williams et al., 2018; Ortega et al., 2019) proposed"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "methods to fuse information across modalities and"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "∗"
        },
        {
          "while keeping competitive performance com-": "indicates\nequal\ncontribution.\nCode\nis\navail-"
        },
        {
          "while keeping competitive performance com-": ""
        },
        {
          "while keeping competitive performance com-": "https://github.com/martinmamql/\nable\nat"
        },
        {
          "while keeping competitive performance com-": "multimodal_routing."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(Prediction)": "unimodal bimodal"
        },
        {
          "(Prediction)": "Routing Stage"
        },
        {
          "(Prediction)": "Encoding Stage\nPrediction Stage"
        },
        {
          "(Prediction)": "(iterates between routing adjustment and concepts update)"
        },
        {
          "(Prediction)": "Figure 2: Overview of Multimodal Routing, which contains encoding, routing, and prediction stages. We consider"
        },
        {
          "(Prediction)": "only two input modalities\nin this example.\nThe encoding stage computes unimodal and bimodal explanatory"
        },
        {
          "(Prediction)": "features with the inputs from different modalities.\nThe routing stage iteratively performs concepts update and"
        },
        {
          "(Prediction)": "routing adjustment. The prediction stage decodes the concepts to the model’s prediction. The routing associates"
        },
        {
          "(Prediction)": "the text and the visual-text features with negative sentiment in the left example, and the vision and the visual-text"
        },
        {
          "(Prediction)": "features with positive sentiment in the right example before making predictions."
        },
        {
          "(Prediction)": "In this paper we address both local and global\nvances\nin modeling multimodal\nlanguage using"
        },
        {
          "(Prediction)": "interpretability of unimodal, bimodal and trimodal\ndeep neural networks are not interpretable\n(Wang"
        },
        {
          "(Prediction)": "explanatory featuress by presenting Multimodal\net al., 2019; Tsai et al., 2019a). Linear method like"
        },
        {
          "(Prediction)": "Routing. In human multimodal language, such rout-\nthe Generalized Additive Models (GAMs) (Hastie,"
        },
        {
          "(Prediction)": "ing dynamically changes weights between modali-\n2017) do not offer\nlocal\ninterpretability.\nEven"
        },
        {
          "(Prediction)": "ties and output\nlabels for each sample as shown\nthough we could use post hoc (interpret predic-"
        },
        {
          "(Prediction)": "in Fig. 1.\nThe most\nsigniﬁcant contribution of\ntions given an arbitrary model) methods such as"
        },
        {
          "(Prediction)": "Multimodal Routing is its ability to establish local\nLIME (Ribeiro et al., 2016), SHAP (Lundberg and"
        },
        {
          "(Prediction)": "weights dynamically for each input sample between\nLee, 2017), and L2X (Chen et al., 2018) to inter-"
        },
        {
          "(Prediction)": "modality features and the labels during training and\npret\nthese black-box models,\nthese interpretation"
        },
        {
          "(Prediction)": "inference,\nthus providing local\ninterpretation for\nmethods are designed to detect\nthe contributions"
        },
        {
          "(Prediction)": "each sample.\nonly from unimodal features but not bimodal or"
        },
        {
          "(Prediction)": "Our experiments focus on two tasks of sentiment\ntrimodal explanatory features.\nIt is shown that in"
        },
        {
          "(Prediction)": "analysis and emotion recognition tasks using two\nhuman communication, modality interactions are"
        },
        {
          "(Prediction)": "benchmark multimodal language datasets, IEMO-\nmore important than individual modalities (Engle,"
        },
        {
          "(Prediction)": "CAP (Busso et al., 2008) and CMU-MOSEI (Zadeh\n1998)."
        },
        {
          "(Prediction)": "et al., 2018). We ﬁrst study how our model com-\nTwo\nrecent methods,\nGraph-MFN (Zadeh"
        },
        {
          "(Prediction)": "pares with the state-of-the-art methods on these\net al., 2018) and Multimodal Factorized Model"
        },
        {
          "(Prediction)": "tasks. More importantly we provide local\ninter-\n(MFM) (Tsai et al., 2019b), attempted to interpret"
        },
        {
          "(Prediction)": "pretation by qualitatively analyzing adjusted local\nrelationships between modality interactions and"
        },
        {
          "(Prediction)": "weights for each sample. Then we also analyze the\nlearning for human language. Nonetheless, Graph-"
        },
        {
          "(Prediction)": "global\ninterpretation using statistical\ntechniques\nMFN did not separate the contributions among uni-"
        },
        {
          "(Prediction)": "to reveal crucial\nfeatures for prediction on aver-\nmodal and multimodal explanatory features, and"
        },
        {
          "(Prediction)": "age.\nSuch interpretation of different\nresolutions\nMFM only provided the analysis on trimodal inter-"
        },
        {
          "(Prediction)": "strengthens our understanding of multimodal lan-\naction feature. Both of them cannot interpret how"
        },
        {
          "(Prediction)": "guage learning.\nboth single modality and modality interactions con-"
        },
        {
          "(Prediction)": "tribute to ﬁnal prediction at the same time."
        },
        {
          "(Prediction)": "2\nRelated Work"
        },
        {
          "(Prediction)": "Our method is inspired and related to Capsule"
        },
        {
          "(Prediction)": "Networks (Sabour et al., 2017; Hinton et al., 2018),\nMultimodal language learning is based on the fact"
        },
        {
          "(Prediction)": "that human integrates multiple sources\nsuch as\nwhich performs routing between layers of capsules."
        },
        {
          "(Prediction)": "acoustic,\ntextual, and visual\ninformation to learn\nEach capsule is a group of neurons that encapsu-"
        },
        {
          "(Prediction)": "language (McGurk and MacDonald, 1976; Ngiam\nlates spatial information as well as the probability"
        },
        {
          "(Prediction)": "et al., 2011; Baltruˇsaitis et al., 2018). Recent ad-\nof an object being present.\nOn the other hand,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "our method performs routing between multimodal": "features (i.e., unimodal, bimodal, and trimodal ex-",
          "et al., 2019a) is adopted as the design of the en-": "coding functions Fi. Here the trimodal function"
        },
        {
          "our method performs routing between multimodal": "planatory features) and concepts of\nthe model’s",
          "et al., 2019a) is adopted as the design of the en-": "Favt encodes sequences from three modalities into"
        },
        {
          "our method performs routing between multimodal": "decision.",
          "et al., 2019a) is adopted as the design of the en-": "a uniﬁed representation, Fav encodes acoustic and"
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "visual modalities, and Fa encodes acoustic input."
        },
        {
          "our method performs routing between multimodal": "3\nMethod",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "Next, pi ∈ [0, 1] is a scalar representing how each"
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "feature fi\nis activated in the model. Similar to fi,"
        },
        {
          "our method performs routing between multimodal": "The proposed Multimodal Routing contains three",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "we also use MulT to encode pi from the input xi."
        },
        {
          "our method performs routing between multimodal": "stages shown in Fig. 2:\nencoding,\nrouting, and",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "That\nis, pa = Pa(xa; θ(cid:48)), pav = Pav(xa, xv, θ(cid:48)),"
        },
        {
          "our method performs routing between multimodal": "prediction.\nThe encoding stage encodes raw in-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "and pavt = Pavt(xa, xv, xt, θ(cid:48)) with θ(cid:48) as the pa-"
        },
        {
          "our method performs routing between multimodal": "puts (speech,\ntext, and visual data)\nto unimodal,",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "rameters of MulT and Pi as corresponding encod-"
        },
        {
          "our method performs routing between multimodal": "bimodal, and trimodal features. The routing stage",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "ing functions (details in the Supplementary)."
        },
        {
          "our method performs routing between multimodal": "contains a routing mechanism (Sabour et al., 2017;",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "Hinton et al., 2018), which 1) updates some hid-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "Routing Stage. The goal of routing is to infer in-"
        },
        {
          "our method performs routing between multimodal": "den representations and 2) adjusts local weights",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "terpretable hidden representations (termed here as"
        },
        {
          "our method performs routing between multimodal": "between each feature and each hidden represen-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "“concepts”) for each output\nlabel. The ﬁrst step"
        },
        {
          "our method performs routing between multimodal": "tation by pairwise similarity. Following previous",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "of routing is to initialize the concepts with equal"
        },
        {
          "our method performs routing between multimodal": "work (Mao et al., 2019), we call\nthe hidden rep-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "weights, where all explanatory features are as im-"
        },
        {
          "our method performs routing between multimodal": "resentations “concepts”, and each of\nthem is as-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "portant. Then the core part of routing is an iterative"
        },
        {
          "our method performs routing between multimodal": "sociated to speciﬁc a prediction label (in our case",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "process which will enforce for each explanatory"
        },
        {
          "our method performs routing between multimodal": "sentiment or an emotion). Finally,\nthe prediction",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "feature to be assigned to only one output\nrepre-"
        },
        {
          "our method performs routing between multimodal": "stage takes the inferred concepts to perform model",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "sentations (a.k.a the “concepts”;\nin reality it\nis a"
        },
        {
          "our method performs routing between multimodal": "prediction.",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "soft assignment) that shows high similarity with a"
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "is repre-\nconcept. Formally each concept cj ∈ Rdc"
        },
        {
          "our method performs routing between multimodal": "3.1\nMultimodal Routing",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "sented as a one-dimensional vector of dimension"
        },
        {
          "our method performs routing between multimodal": "We use v(isual), a(coustic), and t(ext) to denote the",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "dc. Linear weights rij, which we term routing co-"
        },
        {
          "our method performs routing between multimodal": "three commonly considered modalities in human",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "efﬁcient, are deﬁned between each concept cj and"
        },
        {
          "our method performs routing between multimodal": "multimodal language. Let x = {xa, xv, xt} repre-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "explanatory factor fi."
        },
        {
          "our method performs routing between multimodal": "sent the multimodal input. xa ∈ RTa×da is an au-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "The ﬁrst half of routing, which we call routing"
        },
        {
          "our method performs routing between multimodal": "dio stream with time length Ta and feature dimen-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "adjustment,\nis about ﬁnding new assignment (i.e."
        },
        {
          "our method performs routing between multimodal": "sion da (at each time step). Similarly, xv ∈ RTv×dv",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "the routing coefﬁcients) between the input features"
        },
        {
          "our method performs routing between multimodal": "is the text\nis the visual stream and xt ∈ RTt×dt",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "and the newly learned concepts by taking a soft-"
        },
        {
          "our method performs routing between multimodal": "stream.\nIn our paper, we consider multiclass or",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "max of the dot product over all concepts, thus only"
        },
        {
          "our method performs routing between multimodal": "multilabel prediction tasks for the multimodal lan-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "the features showing high similarity of a concept"
        },
        {
          "our method performs routing between multimodal": "guage modeling, in which we use y ∈ RJ to denote",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "(sentiment or an emotion in our case) will be as-"
        },
        {
          "our method performs routing between multimodal": "the ground truth label with J being the number of",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "signed close to 1,\ninstead of having all\nfeatures"
        },
        {
          "our method performs routing between multimodal": "classes or labels, and ˆy to represent\nthe model’s",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "assigned to all concepts. This will help local\nin-"
        },
        {
          "our method performs routing between multimodal": "prediction. Our goal is to ﬁnd the relative impor-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "terpretability because we can always distinguish"
        },
        {
          "our method performs routing between multimodal": "tance of the contributions from unimodal (e.g., xa",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "important explanatory features from non-important"
        },
        {
          "our method performs routing between multimodal": "itself), bimodal (e.g.,\nthe interaction between xa",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "ones. The second half of routing, which we call"
        },
        {
          "our method performs routing between multimodal": "and xv), and trimodal features (e.g., the interaction",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "concept update, is to update concepts by linearly"
        },
        {
          "our method performs routing between multimodal": "between xa, xv, and xt) to the model prediction ˆy.",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "aggregating the new input features weighted by the"
        },
        {
          "our method performs routing between multimodal": "Encoding Stage. The encoding stage encodes mul-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "routing coefﬁcients, so that it is local to each input"
        },
        {
          "our method performs routing between multimodal": "timodal\ninputs {xa, xv, xt} into explanatory fea-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "samples."
        },
        {
          "our method performs routing between multimodal": "to denote the features with\ntures. We use fi ∈ Rdf",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "- Routing adjustment. We deﬁne the routing coef-"
        },
        {
          "our method performs routing between multimodal": "i ∈ {a, v, t} being unimodal, i ∈ {av, vt, ta} be-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "ﬁcient rij ∈ [0, 1] by measuring the similarity 1"
        },
        {
          "our method performs routing between multimodal": "ing bimodal, and i ∈ {avt} being trimodal interac-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "tions. df is the dimension of the feature. To be spe-",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "ciﬁc, fa = Fa(xa; θ), fav = Fav(xa, xv; θ), and",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "1We use dot-product as\nthe similarity measurement as"
        },
        {
          "our method performs routing between multimodal": "favt = Favt(xa, xv, xt; θ) with θ as the parameters",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "in prior work (Sabour et al., 2017). Note that routing only"
        },
        {
          "our method performs routing between multimodal": "of the encoding functions and F as the encoding",
          "et al., 2019a) is adopted as the design of the en-": ""
        },
        {
          "our method performs routing between multimodal": "",
          "et al., 2019a) is adopted as the design of the en-": "changes rij, not Wij. Another choice can be the probability"
        },
        {
          "our method performs routing between multimodal": "functions. Multimodal Transformer (MulT) (Tsai",
          "et al., 2019a) is adopted as the design of the en-": "of a ﬁt under a Gaussian distribution (Hinton et al., 2018)."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Procedure 1 Multimodal Routing": "1: procedure ROUTING({fi}, {pi}, {Wij})",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "In this section, we provide the framework of locally"
        },
        {
          "Procedure 1 Multimodal Routing": "2:\nConcepts are initialized with uniform weights",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "interpreting relative importance of unimodal, bi-"
        },
        {
          "Procedure 1 Multimodal Routing": "3:\nfor t iterations do",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "/* Routing Adjustment */",
          "3.2\nInterpretability": "modal, and trimodal explanatory features to model"
        },
        {
          "Procedure 1 Multimodal Routing": "4:\nfor all feature i and concept j: sij ← (fiWij)(cid:62)cj",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "prediction given different samples, by interpreting"
        },
        {
          "Procedure 1 Multimodal Routing": "(cid:1)/ (cid:80)",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "5:\nj(cid:48) exp(cid:0)sij(cid:48)",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "/* Concepts Update */",
          "3.2\nInterpretability": "the routing coefﬁcients rij, which represents the"
        },
        {
          "Procedure 1 Multimodal Routing": "6:\nfor all concept j: cj ← (cid:80)\ni pirij(fiWij)",
          "3.2\nInterpretability": "weight assignment between feature fi and concept"
        },
        {
          "Procedure 1 Multimodal Routing": "return {cj}",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "cj. We also provide methods to globally interpret"
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "the model across the whole dataset."
        },
        {
          "Procedure 1 Multimodal Routing": "between fiWij and cj:",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "3.2.1\nLocal Interpretation"
        },
        {
          "Procedure 1 Multimodal Routing": "exp((cid:104)fiWij, cj(cid:105))",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "(1)\nrij =",
          "3.2\nInterpretability": "The goal of local interpretation is trying to under-"
        },
        {
          "Procedure 1 Multimodal Routing": "(cid:11)",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "(cid:80)\nj(cid:48) exp((cid:10)fiWij(cid:48), cj(cid:48)",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "stand how the importance of modality and modality"
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "interaction features change, given different multi-"
        },
        {
          "Procedure 1 Multimodal Routing": "We note that rij\nis normalized over all concepts cj.",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "modal samples.\nIn eq.\n(3), a decision logit con-"
        },
        {
          "Procedure 1 Multimodal Routing": "Hence,\nit\nis a coefﬁcient which takes high value",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "siders an addition of\nthe contributions from the"
        },
        {
          "Procedure 1 Multimodal Routing": "only when fi\nis in agreement with cj but not with",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "unimodal {fa, fv, ft}, bimodal {fav, fvt, fta}, and"
        },
        {
          "Procedure 1 Multimodal Routing": "(cid:54)= j.\ncj(cid:48), where j(cid:48)",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "trimodal favt explanatory features. The particular"
        },
        {
          "Procedure 1 Multimodal Routing": "from encod-\n- Concept update. After obtaining pi",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "to the jth concept\ncontribution from the feature fi"
        },
        {
          "Procedure 1 Multimodal Routing": "ing stage, we update concepts cj using weighted",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "It\ntakes large\nis represented by pirijo(cid:62)"
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "j (fiWij)."
        },
        {
          "Procedure 1 Multimodal Routing": "average as follows: updates the concepts based on",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "is large; 2) the\nvalue when 1) pi of the feature fi"
        },
        {
          "Procedure 1 Multimodal Routing": "the routing weights by summing input features fi",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "agreement rij is high (the feature fi is in agreement"
        },
        {
          "Procedure 1 Multimodal Routing": "to the space of the\nprojected by weight matrix Wij",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "with concept cj and is not\nin agreement with cj(cid:48),"
        },
        {
          "Procedure 1 Multimodal Routing": "jth concept",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "where j(cid:48)\n(cid:54)= j); and 3) the dot product o(cid:62)"
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "j (fiWij)"
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "is large.\nIntuitively, any of the three scenarios re-"
        },
        {
          "Procedure 1 Multimodal Routing": "(2)\ncj =\npirij(fiWij)",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "(cid:88) i",
          "3.2\nInterpretability": "quires high similarity between a modality feature"
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "and a concept vector which represents a speciﬁc"
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "sentiment or emotion. Note that pi, rij and fi are"
        },
        {
          "Procedure 1 Multimodal Routing": "is now essentially a linear aggregation from\ncj",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "the covariates and oj and Wij are the parameters in"
        },
        {
          "Procedure 1 Multimodal Routing": "(fiWij) with weights pirij.",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "the model. Since different input samples yield dis-"
        },
        {
          "Procedure 1 Multimodal Routing": "We summarize the routing procedure in Proce-",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "tinct pi and rij, we can locally interpret pi and rij"
        },
        {
          "Procedure 1 Multimodal Routing": "dure\n1, which returns concepts (cj) given explana-",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "as the effects of the modality feature fi contribut-"
        },
        {
          "Procedure 1 Multimodal Routing": "tory features (fi), local weights (Wij) and pi. First,",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "ing to the jth logit of the model, which is roughly a"
        },
        {
          "Procedure 1 Multimodal Routing": "we initialize the concepts with uniform weights.",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "conﬁdence of predicting jth sentiment or emotion."
        },
        {
          "Procedure 1 Multimodal Routing": "Then, we iteratively perform adjustment on routing",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "We will show examples of local interpretations in"
        },
        {
          "Procedure 1 Multimodal Routing": "coefﬁcients (rij) and concept updates. Finally, we",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "the Interpretation Analysis section."
        },
        {
          "Procedure 1 Multimodal Routing": "return the updated concepts.",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "3.2.2\nGlobal Interpretation"
        },
        {
          "Procedure 1 Multimodal Routing": "Prediction Stage. The prediction stage takes the",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "inferred concepts to make predictions ˆy. Here, we",
          "3.2\nInterpretability": "To globally interpret Multimodal Routing, we ana-"
        },
        {
          "Procedure 1 Multimodal Routing": "to obtain\napply linear transformations to concept cj",
          "3.2\nInterpretability": "lyze rij, the average values of routing coefﬁcients"
        },
        {
          "Procedure 1 Multimodal Routing": "the logits. Speciﬁcally, the jth logit is formulated",
          "3.2\nInterpretability": "rijs over the entire dataset. Since eq. (3) considers"
        },
        {
          "Procedure 1 Multimodal Routing": "as",
          "3.2\nInterpretability": "rep-\na linear effect from fi, pi and rij\nto logitj, rij"
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "to\nresents the average assignment from feature fi"
        },
        {
          "Procedure 1 Multimodal Routing": "logitj = o(cid:62)\nj cj",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "the jth logit. Instead of reporting the values for rij,"
        },
        {
          "Procedure 1 Multimodal Routing": "(3)",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "we provide a statistical interpretation on rij using"
        },
        {
          "Procedure 1 Multimodal Routing": "(cid:88) i\n=\npirijo(cid:62)\nj (fiWij)",
          "3.2\nInterpretability": ""
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "conﬁdence intervals to provide a range of possible"
        },
        {
          "Procedure 1 Multimodal Routing": "",
          "3.2\nInterpretability": "plausible coefﬁcients with probability guarantees."
        },
        {
          "Procedure 1 Multimodal Routing": "where oj ∈ Rdc and is the weight of the linear trans-",
          "3.2\nInterpretability": "Similar tests on pi and pirij are provided in Sup-"
        },
        {
          "Procedure 1 Multimodal Routing": "formation for the jth concept. Then, the Softmax",
          "3.2\nInterpretability": "plementary Materials. Here we choose conﬁdence"
        },
        {
          "Procedure 1 Multimodal Routing": "(for multi-class task) or Sigmoid (for multi-label",
          "3.2\nInterpretability": "intervals over p-values because they provide much"
        },
        {
          "Procedure 1 Multimodal Routing": "task) function is applied on the logits to obtain the",
          "3.2\nInterpretability": "richer information (Ranstam, 2012; Du Prel et al.,"
        },
        {
          "Procedure 1 Multimodal Routing": "prediction ˆy.",
          "3.2\nInterpretability": "2009).\nSuppose we have n data with the corre-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "enough and rij has ﬁnite mean and ﬁnite variance",
          "For both datasets, we use\nthe\nextracted fea-": "tures from a public SDK https://github.com/"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "(it sufﬁces since rij ∈ [0, 1] is bounded), according",
          "For both datasets, we use\nthe\nextracted fea-": "A2Zadeh/CMU-MultimodalSDK, whose features are"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "to Central Limit Theorem, rij\n(i.e., mean of rij)",
          "For both datasets, we use\nthe\nextracted fea-": "extracted from textual\n(GloVe word embedding"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "follows a Normal distribution:",
          "For both datasets, we use\nthe\nextracted fea-": "(Pennington et al., 2014)), visual (Facet (iMotions,"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "",
          "For both datasets, we use\nthe\nextracted fea-": "2019)), and acoustic (COVAREP (Degottex et al.,"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "(cid:19)\n(cid:18)",
          "For both datasets, we use\nthe\nextracted fea-": ""
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "s2",
          "For both datasets, we use\nthe\nextracted fea-": ""
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": ",\nµ,",
          "For both datasets, we use\nthe\nextracted fea-": "2014)) modalities. The acoustic and vision features"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "n n\n(4)\nrij ∼ N",
          "For both datasets, we use\nthe\nextracted fea-": ""
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "",
          "For both datasets, we use\nthe\nextracted fea-": "are processed to be aligned with the words (i.e.,"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "",
          "For both datasets, we use\nthe\nextracted fea-": "text features). We present results using this word-"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "where µ is the true mean of rij and s2\nn is the sample",
          "For both datasets, we use\nthe\nextracted fea-": ""
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "",
          "For both datasets, we use\nthe\nextracted fea-": "aligned setting in this paper, but ours can work on"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "variance in rij. Using eq. (4), we can provide a con-",
          "For both datasets, we use\nthe\nextracted fea-": ""
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "",
          "For both datasets, we use\nthe\nextracted fea-": "unaligned multimodal\nlanguage sequences. The"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "ﬁdence interval for rij. We follow 95% conﬁdence",
          "For both datasets, we use\nthe\nextracted fea-": ""
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "",
          "For both datasets, we use\nthe\nextracted fea-": "train, valid and test set split are following previous"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "in our analysis.",
          "For both datasets, we use\nthe\nextracted fea-": ""
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "",
          "For both datasets, we use\nthe\nextracted fea-": "work (Wang et al., 2019; Tsai et al., 2019a)."
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "4\nExperiments",
          "For both datasets, we use\nthe\nextracted fea-": ""
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "",
          "For both datasets, we use\nthe\nextracted fea-": "4.2\nAblation Study and Baseline Models"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "In this section, we ﬁrst provide details of experi-",
          "For both datasets, we use\nthe\nextracted fea-": "We provide two ablation studies for interpretable"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "ments we perform and comparison between our pro-",
          "For both datasets, we use\nthe\nextracted fea-": "methods as baselines: The ﬁrst\nis based on Gen-"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "posed model and state-of-the-art (SOTA) method,",
          "For both datasets, we use\nthe\nextracted fea-": "eralized Additive Model\n(GAM)\n(Hastie, 2017)"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "as well\nas baseline models. We\ninclude\ninter-",
          "For both datasets, we use\nthe\nextracted fea-": "which directly sums over unimodal, bimodal, and"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "pretability analysis in the next section.",
          "For both datasets, we use\nthe\nextracted fea-": "trimodal features and then applies a linear transfor-"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "",
          "For both datasets, we use\nthe\nextracted fea-": "mation to obtain a prediction. This is equivalent"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "4.1\nDatasets",
          "For both datasets, we use\nthe\nextracted fea-": ""
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "",
          "For both datasets, we use\nthe\nextracted fea-": "to only using weight pi and no routing coefﬁcients."
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "We perform experiments on two publicly available",
          "For both datasets, we use\nthe\nextracted fea-": "The second is our denoted as Multimodal Routing∗,"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "benchmarks for human multimodal affect recogni-",
          "For both datasets, we use\nthe\nextracted fea-": "which performs only one routing iteration (by set-"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "tion: CMU-MOSEI (Zadeh et al., 2018) and IEMO-",
          "For both datasets, we use\nthe\nextracted fea-": "ting t = 1 in Procedure 1) and does not iteratively"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "CAP (Busso et al., 2008). CMU-MOSEI (Zadeh",
          "For both datasets, we use\nthe\nextracted fea-": "adjust the routing and update the concepts."
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "et al., 2018) contains 23, 454 movie review video",
          "For both datasets, we use\nthe\nextracted fea-": "We also choose other non-interpretable methods"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "clips taken from YouTube. For each clip, there are",
          "For both datasets, we use\nthe\nextracted fea-": "that achieved state-of-the-art to compare the perfor-"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "two tasks: sentiment prediction (multiclass classi-",
          "For both datasets, we use\nthe\nextracted fea-": "mance of our approach to: Early Fusion LSTM (EF-"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "ﬁcation) and emotion recognition (multilabel clas-",
          "For both datasets, we use\nthe\nextracted fea-": "LSTM), Late Fusion LSTM (LF-LSTM) (Hochre-"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "siﬁcation). For the sentiment prediction task, each",
          "For both datasets, we use\nthe\nextracted fea-": "iter and Schmidhuber, 1997), Recurrent Attended"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "sample is labeled by an integer score in the range",
          "For both datasets, we use\nthe\nextracted fea-": "Variation Embedding Network (RAVEN) (Wang"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "[−3, 3], indicating highly negative sentiment (−3)",
          "For both datasets, we use\nthe\nextracted fea-": "et al., 2019), and Multimodal Transformer (Tsai"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "to highly positive sentiment (+3). We use some",
          "For both datasets, we use\nthe\nextracted fea-": "et al., 2019a)."
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "metrics as in prior work (Zadeh et al., 2018): seven",
          "For both datasets, we use\nthe\nextracted fea-": ""
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "",
          "For both datasets, we use\nthe\nextracted fea-": "4.3\nResults and Discussions"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "class accuracy (Acc7: seven class classiﬁcation in",
          "For both datasets, we use\nthe\nextracted fea-": ""
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "two-class\nZ ∈ [−3, 3]), binary accuracy (Acc2:",
          "For both datasets, we use\nthe\nextracted fea-": "We trained our model on 1 RTX 2080 GPU. We use"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "classiﬁcation in {−1, +1}), and F1 score of pre-",
          "For both datasets, we use\nthe\nextracted fea-": "7 layers in the Multimodal Transformer, and choose"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "dictions. For the emotion recognition task, each",
          "For both datasets, we use\nthe\nextracted fea-": "the batch size as 32. The model\nis trained with"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "sample is labeled by one or more emotions from",
          "For both datasets, we use\nthe\nextracted fea-": "initial learning rate of 10−4 and Adam optimizer."
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "{Happy, Sad, Angry, Fear, Disgust, Surprise}. We",
          "For both datasets, we use\nthe\nextracted fea-": "CMU-MOSEI sentiment. Table 1 summarizes"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "report the metrics (Zadeh et al., 2018): six-class ac-",
          "For both datasets, we use\nthe\nextracted fea-": "the results on this dataset. We ﬁrst compare all the"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "curacy (multilabel accuracy of predicting six emo-",
          "For both datasets, we use\nthe\nextracted fea-": "interpretable methods. We see that Multimodal"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "tion labels) and F1 score.",
          "For both datasets, we use\nthe\nextracted fea-": "Routing enjoys performance improvement over"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "IEMOCAP consists of 10K video clips for hu-",
          "For both datasets, we use\nthe\nextracted fea-": "both GAM (Hastie, 2017), a linear model on en-"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "man emotion analysis. Each clip is evaluated and",
          "For both datasets, we use\nthe\nextracted fea-": "coded features, and Multimodal Routing∗, a non-"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "then assigned (possibly more than one) labels of",
          "For both datasets, we use\nthe\nextracted fea-": "iterative feed-forward net with same parameters"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "emotions, making it a multilabel learning task. Fol-",
          "For both datasets, we use\nthe\nextracted fea-": "as Multimodal Routing.\nThe improvement sug-"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "lowing prior work and insight (Tsai et al., 2019a;",
          "For both datasets, we use\nthe\nextracted fea-": "gests the proposed iterative routing can obtain a"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "Tripathi et al., 2018; Jack et al., 2014), we report",
          "For both datasets, we use\nthe\nextracted fea-": "more robust prediction by dynamically associat-"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "on four emotions (happy, sad, angry, and neutral),",
          "For both datasets, we use\nthe\nextracted fea-": "ing the features and the concepts of the model’s"
        },
        {
          "sponding rij = {rij,1, rij,2, · · · rij,n}. If n is large": "with metrics four-class accuracy and F1 score.",
          "For both datasets, we use\nthe\nextracted fea-": "predictions. Next, when comparing to the non-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CMU-MOSEI Sentiment": "-",
          "IEMOCAP Emotion": "Sad"
        },
        {
          "CMU-MOSEI Sentiment": "Acc2",
          "IEMOCAP Emotion": "F1"
        },
        {
          "CMU-MOSEI Sentiment": "",
          "IEMOCAP Emotion": ""
        },
        {
          "CMU-MOSEI Sentiment": "78.2",
          "IEMOCAP Emotion": "80.5"
        },
        {
          "CMU-MOSEI Sentiment": "80.6",
          "IEMOCAP Emotion": "81.7"
        },
        {
          "CMU-MOSEI Sentiment": "79.1",
          "IEMOCAP Emotion": "83.1"
        },
        {
          "CMU-MOSEI Sentiment": "82.5",
          "IEMOCAP Emotion": "86.0"
        },
        {
          "CMU-MOSEI Sentiment": "",
          "IEMOCAP Emotion": ""
        },
        {
          "CMU-MOSEI Sentiment": "79.5",
          "IEMOCAP Emotion": "82.4"
        },
        {
          "CMU-MOSEI Sentiment": "81.2",
          "IEMOCAP Emotion": "83.2"
        },
        {
          "CMU-MOSEI Sentiment": "81.7",
          "IEMOCAP Emotion": "85.2"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "denotes our method without": "2019a). We make our results bold if it is SOTA or close to SOTA (≤ 1%).",
          "iterative routing. Our": "",
          "results are better or close to the state-of-the-art": "",
          "(Tsai et al.,": ""
        },
        {
          "denotes our method without": "",
          "iterative routing. Our": "",
          "results are better or close to the state-of-the-art": "",
          "(Tsai et al.,": ""
        },
        {
          "denotes our method without": "Models",
          "iterative routing. Our": "Happy",
          "results are better or close to the state-of-the-art": "",
          "(Tsai et al.,": ""
        },
        {
          "denotes our method without": "",
          "iterative routing. Our": "F1",
          "results are better or close to the state-of-the-art": "Acc",
          "(Tsai et al.,": "F1"
        },
        {
          "denotes our method without": "",
          "iterative routing. Our": "",
          "results are better or close to the state-of-the-art": "",
          "(Tsai et al.,": ""
        },
        {
          "denotes our method without": "EF-LSTM",
          "iterative routing. Our": "68.6",
          "results are better or close to the state-of-the-art": "76.5",
          "(Tsai et al.,": "87.8"
        },
        {
          "denotes our method without": "LF-LSTM",
          "iterative routing. Our": "68.0",
          "results are better or close to the state-of-the-art": "76.1",
          "(Tsai et al.,": "87.8"
        },
        {
          "denotes our method without": "MulT (Tsai et al., 2019a)",
          "iterative routing. Our": "71.8",
          "results are better or close to the state-of-the-art": "77.6",
          "(Tsai et al.,": "87.8"
        },
        {
          "denotes our method without": "",
          "iterative routing. Our": "",
          "results are better or close to the state-of-the-art": "",
          "(Tsai et al.,": ""
        },
        {
          "denotes our method without": "GAM (Hastie, 2017)",
          "iterative routing. Our": "69.6",
          "results are better or close to the state-of-the-art": "77.5",
          "(Tsai et al.,": "87.8"
        },
        {
          "denotes our method without": "Multimodal Routing∗",
          "iterative routing. Our": "69.3",
          "results are better or close to the state-of-the-art": "77.5",
          "(Tsai et al.,": "87.8"
        },
        {
          "denotes our method without": "Multimodal Routing",
          "iterative routing. Our": "69.4",
          "results are better or close to the state-of-the-art": "77.6",
          "(Tsai et al.,": "87.8"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "We make our results bold if it is the best or close to the best (≤ 1%).": "interpretable methods, Multimodal Routing outper-"
        },
        {
          "We make our results bold if it is the best or close to the best (≤ 1%).": "forms EF-LSTM, LF-LSTM and RAVEN models"
        },
        {
          "We make our results bold if it is the best or close to the best (≤ 1%).": "and performs competitively when compared with"
        },
        {
          "We make our results bold if it is the best or close to the best (≤ 1%).": "MulT (Tsai et al., 2019a).\nIt\nis good to see that"
        },
        {
          "We make our results bold if it is the best or close to the best (≤ 1%).": "our method can competitive performance with the"
        },
        {
          "We make our results bold if it is the best or close to the best (≤ 1%).": "added advantage of local and global interpretability"
        },
        {
          "We make our results bold if it is the best or close to the best (≤ 1%).": "(see analysis in the later section). The conﬁguration"
        },
        {
          "We make our results bold if it is the best or close to the best (≤ 1%).": ""
        },
        {
          "We make our results bold if it is the best or close to the best (≤ 1%).": "of our model is in the supplementary ﬁle."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "from CMU-MOSEI sentiment prediction task;\nthe bottom row contains three examples from CMU-MOSEI emo-"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "tion recognition task. pirij represents the contribution from the explanatory features i (unimodal/bimodal/trimodal"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "interaction features) to the prediction logitj (see eq. 3). In these examples, j is chosen to be the ground truth label."
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "emotion recognition, and illustrate them in Fig. 3.\nSimilarly,\nfor\nthe\nbottom-left\nexample,\nthe"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "For sentiment prediction, we show samples with\nspeaker\nis sharing her experience on how to au-"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "true labels neutral\n(0), most negative sentiment\ndition for a Broadway show. She talks about a very"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "(−3), and most positive (+3) sentiment score. For\ndetailed and successful experience of herself and"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "emotion recognition, we illustrate examples with\ndescribes “love” in her audition monologue, which"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "true label “happy”, “sad”, and “disgust” emotions.\nis present in the text. Also, she has a dramatic smile"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "A color leaning towards red in the rightmost spec-\nand a happy tone. We believe all modalities play"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "trum stands for a high association, while a color\na role in the prediction. As a result,\nthe trimodal"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "leaning towards blue suggests a low association.\ninteraction feature contributes signiﬁcantly to the"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "prediction of happiness, according to our model."
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "In the upper-left example in Fig. 3, a speaker is"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "Notably, by looking at the six examples overall,"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "introducing movie Sweeny Todd. He says the movie"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "we could see each individual sample bears a differ-"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "is a musical and suggests those who dislike musi-"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "ent pattern of feature importance, even when the"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "cals not to see the movie. Since he has no personal"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "sentiment is the same. This is a good debuging and"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "judgment on whether he personally likes or dislikes"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "interpretation tool.\nFor global\ninterpretation, all"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "the movie, his sentiment is classiﬁed as neutral (0),"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "these samples will be averaged giving more of a"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "although the text modality (i.e., transcript) contains"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "general trend."
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "a “don’t”. In the vision modality (i.e., videos), he"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "frowns when he mentions this movie is musical, but"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "5.2\nGlobal Interpretation Analysis"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "we cannot conclude his sentiment to be neutral by"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "only looking at the visual modality. By looking at\nHere we analyze the global interpretation of Multi-"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "both vision and text together (their interaction), the\nmodal Routing. Given the averaged routing coef-"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "conﬁdence in neutral is high. The model gives the\nﬁcients rij generated and aggregated locally from"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "samples, we want to know the overall connection\ntext-vision interaction feature a high value of pirij"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "to suggest\nit highly contributes to the prediction,\nbetween each modality or modality interaction and"
        },
        {
          "Figure 3: Local interpretation (qualitative results) for Multimodal Routing. The upper row contains three examples": "which conﬁrms our reasoning above.\neach concept across the whole dataset. To evaluate"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: , we can see that our model and trimodal explanatory features to predictions",
      "data": [
        {
          "Sentiment": "0"
        },
        {
          "Sentiment": "(0.194, 0.328)"
        },
        {
          "Sentiment": "(0.051, 0.079)"
        },
        {
          "Sentiment": "(0.161, 0.178)"
        },
        {
          "Sentiment": "(0.011, 0.045)"
        },
        {
          "Sentiment": "(0.060, 0.093)"
        },
        {
          "Sentiment": "(0.158, 0.172)"
        },
        {
          "Sentiment": "(0.149, 0.178)"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: , we can see that our model and trimodal explanatory features to predictions",
      "data": [
        {
          "rtav": "",
          "(0.112, 0.143)": "",
          "(0.062, 0.093)": "",
          "(0.119, 0.149)\n(0.149, 0.178)": "Emotions",
          "(0.100, 0.149)": "",
          "(0.213, 0.322)": "",
          "(0.064, 0.094)": ""
        },
        {
          "rtav": "",
          "(0.112, 0.143)": "Happy",
          "(0.062, 0.093)": "Sad",
          "(0.119, 0.149)\n(0.149, 0.178)": "Angry",
          "(0.100, 0.149)": "Fear",
          "(0.213, 0.322)": "Disgust",
          "(0.064, 0.094)": "Surprise"
        },
        {
          "rtav": "rt",
          "(0.112, 0.143)": "(0.114, 0.171)",
          "(0.062, 0.093)": "(0.078, 0.115)",
          "(0.119, 0.149)\n(0.149, 0.178)": "(0.093, 0.170)",
          "(0.100, 0.149)": "(0.382, 0.577)",
          "(0.213, 0.322)": "(0.099, 0.141)",
          "(0.064, 0.094)": "(0.026, 0.120)"
        },
        {
          "rtav": "ra",
          "(0.112, 0.143)": "(0.107, 0.171)",
          "(0.062, 0.093)": "(0.095, 0.116)",
          "(0.119, 0.149)\n(0.149, 0.178)": "(0.104, 0.149)",
          "(0.100, 0.149)": "(0.119, 0.160)",
          "(0.213, 0.322)": "(0.285, 0.431)",
          "(0.064, 0.094)": "(0.092, 0.117)"
        },
        {
          "rtav": "rv",
          "(0.112, 0.143)": "(0.139, 0.164)",
          "(0.062, 0.093)": "(0.143, 0.168)",
          "(0.119, 0.149)\n(0.149, 0.178)": "(0.225, 0.259)",
          "(0.100, 0.149)": "(0.159, 0.182)",
          "(0.213, 0.322)": "(0.141, 0.155)",
          "(0.064, 0.094)": "(0.123, 0.136)"
        },
        {
          "rtav": "rta",
          "(0.112, 0.143)": "(0.117, 0.158)",
          "(0.062, 0.093)": "(0.039, 0.059)",
          "(0.119, 0.149)\n(0.149, 0.178)": "(0.104, 0.143)",
          "(0.100, 0.149)": "(0.055, 0.079)",
          "(0.213, 0.322)": "(0.055, 0.082)",
          "(0.064, 0.094)": "(0.462, 0.615)"
        },
        {
          "rtav": "rav",
          "(0.112, 0.143)": "(0.102, 0.136)",
          "(0.062, 0.093)": "(0.054, 0.074)",
          "(0.119, 0.149)\n(0.149, 0.178)": "(0.358, 0.482)",
          "(0.100, 0.149)": "(0.219, 0.261)",
          "(0.213, 0.322)": "(0.043, 0.072)",
          "(0.064, 0.094)": "(0.092, 0.107)"
        },
        {
          "rtav": "rvt",
          "(0.112, 0.143)": "(0.173, 0.215)",
          "(0.062, 0.093)": "(0.075, 0.099)",
          "(0.119, 0.149)\n(0.149, 0.178)": "(0.212, 0.241)",
          "(0.100, 0.149)": "(0.180, 0.196)",
          "(0.213, 0.322)": "(0.134, 0.150)",
          "(0.064, 0.094)": "(0.132, 0.142)"
        },
        {
          "rtav": "rtav",
          "(0.112, 0.143)": "(0.182, 0.225)",
          "(0.062, 0.093)": "(0.146, 0.197)",
          "(0.119, 0.149)\n(0.149, 0.178)": "(0.146, 0.183)",
          "(0.100, 0.149)": "(0.158, 0.209)",
          "(0.213, 0.322)": "(0.151, 0.176)",
          "(0.064, 0.094)": "(0.101, 0.116)"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: , we can see that our model and trimodal explanatory features to predictions",
      "data": [
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "(0.173, 0.215)\n(0.075, 0.099)\n(0.212, 0.241)\n(0.180, 0.196)\n(0.134, 0.150)\n(0.132, 0.142)\nrvt"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "(0.182, 0.225)\n(0.146, 0.197)\n(0.146, 0.183)\n(0.158, 0.209)\n(0.151, 0.176)\n(0.101, 0.116)\nrtav"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "Table 3: Global\ninterpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of rij, sampled"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "from CMU-MOSEI sentiment task (top) and emotion task (bottom). We bold the values that have the largest mean"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "in each emotion and are signiﬁcantly larger than a uniform routing (1/J = 1/7 = 0.143)."
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "these routing coefﬁcients we will compare them to\ntiment or emotions. This well aligns with research"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "1J\nuniform weighting, i.e.,\nwhere J is the number\nresults in behavior science (Lima et al., 2013). Fur-"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "of concepts. To perform such analysis, we provide\nthermore, (Livingstone and Russo, 2018) showed"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "that\nthe intensity of emotion angry is stronger in\nconﬁdence intervals of each rij. If this interval is"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "outside of 1"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "J , we can interpret it as a distinguishably"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "signiﬁcant feature. See Supplementary for similar\nmodality in human speech."
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "analysis performed on pirij and pi."
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "6\nConclusion"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "First we provide conﬁdence intervals of rij sam-"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "In this paper, we presented Multimodal Routing to\npled from CMU-MOSEI sentiment. We compare"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "1J\nidentify the contributions from unimodal, bimodal\nour conﬁdence intervals with the value\n. From"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "and trimodal explanatory features to predictions\ntop part of Table 3, we can see that our model"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "in a locally manner. For each speciﬁc input, our\nrelies identiﬁed language modality for neutral sen-"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "method dynamically associates an explanatory fea-\ntiment predictions; acoustic modality for extremely"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "ture with a prediction if the feature explains the pre-\nnegative predictions (row ra column -3); and text-"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "diction well. Then, we interpret our approach by\nacoustic bimodal interaction for extremely positive"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "analyzing the routing coefﬁcients, showing great\npredictions (row rta column 3). Similarly, we ana-"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "variation of\nfeature importance in different sam-\nlyze rij sampled from CMU-MOSEI emotion (bot-"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "ples. We also conduct global\ninterpretation over\ntom part of Table 3). We can see that our model"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "the whole datasets, and show that the acoustic fea-\nidentiﬁed the text modality for predicting emotion"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "tures are crucial for predicting negative sentiment\nthe same indexing for\nfear (row rt column Fear,"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "or emotions, and the acoustic-visual\ninteractions\nlater cases),\nthe acoustic modality for predicting"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "are crucial for predicting emotion angry. These ob-\nemotion disgust,\nthe text-acoustic interaction for"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "servations align with prior work in psychological\npredicting emotion surprise, and the acoustic-visual"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "research. The advantage of both local and global\ninteraction for predicting emotion angry. For emo-"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "interpretation is achieved without much loss of per-\ntion happy and sad, either trimodal interaction has"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "formance compared to the SOTA methods. We be-\nthe most signiﬁcant connection, or the routing is"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "lieve that this work sheds light on the advantages of\nnot signiﬁcantly different among modalities."
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "understanding human behaviors from a multimodal"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "Interestingly,\nthese\nresults\necho previous\nre-"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "perspective, and makes a step towards introducing"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "search. In both sentiment and emotion cases, acous-"
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "more interpretable multimodal language models."
        },
        {
          "(0.358, 0.482)\n(0.102, 0.136)\n(0.054, 0.074)\n(0.219, 0.261)\n(0.043, 0.072)\n(0.092, 0.107)\nrav": "tic features are crucial for predicting negative sen-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Neural\nLong\nshort-term memory.\ncomputation,"
        },
        {
          "Acknowledgements": "This work was supported in part by the DARPA",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "9(8):1735–1780."
        },
        {
          "Acknowledgements": "grants FA875018C0150 HR00111990016, NSF",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "iMotions. 2019.\nimotions: Unpack human behavior."
        },
        {
          "Acknowledgements": "IIS1763562, NSF Awards #1750439 #1722822, Na-",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Rachael E Jack, Oliver GB Garrod,\nand Philippe G"
        },
        {
          "Acknowledgements": "tional Institutes of Health, and Apple. We would",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Schyns. 2014. Dynamic facial expressions of emo-"
        },
        {
          "Acknowledgements": "also like to acknowledge NVIDIA’s GPU support.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "tion transmit an evolving hierarchy of signals over"
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "time. Current biology, 24(2):187–192."
        },
        {
          "Acknowledgements": "References",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Diederik P Kingma and Jimmy Ba. 2014.\nAdam: A"
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "arXiv preprint\nmethod for stochastic optimization."
        },
        {
          "Acknowledgements": "Sami Abu-El-Haija, Nisarg Kothari,\nJoonseok Lee,",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "arXiv:1412.6980."
        },
        {
          "Acknowledgements": "Paul\nNatsev,\nGeorge\nToderici,\nBalakrishnan",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "Varadarajan,\nand\nSudheendra\nVijayanarasimhan.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "C´esar F Lima, S˜ao Lu´ıs Castro, and Sophie K Scott."
        },
        {
          "Acknowledgements": "2016. Youtube-8m: A large-scale video classiﬁca-",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "2013. When voices get emotional: a corpus of non-"
        },
        {
          "Acknowledgements": "tion benchmark. arXiv preprint arXiv:1609.08675.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "verbal vocalizations for research on emotion process-"
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "ing. Behavior research methods, 45(4):1234–1245."
        },
        {
          "Acknowledgements": "Tadas\nBaltruˇsaitis,\nChaitanya\nAhuja,\nand\nLouis-",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James"
        },
        {
          "Acknowledgements": "Philippe Morency. 2018. Multimodal machine learn-",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar,"
        },
        {
          "Acknowledgements": "IEEE transac-\ning:\nA survey\nand\ntaxonomy.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "and C Lawrence Zitnick. 2014.\nMicrosoft\ncoco:"
        },
        {
          "Acknowledgements": "tions on pattern analysis and machine intelligence,",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Common objects\nin context.\nIn European confer-"
        },
        {
          "Acknowledgements": "41(2):423–443.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "ence on computer vision, pages 740–755. Springer."
        },
        {
          "Acknowledgements": "Christian B¨uchel, Cathy Price, and Karl Friston. 1998.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-"
        },
        {
          "Acknowledgements": "A multimodal\nlanguage region in the ventral visual",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "narasimhan, Paul Pu Liang, Amir Zadeh, and Louis-"
        },
        {
          "Acknowledgements": "pathway. Nature, 394(6690):274.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Philippe Morency. 2018.\nEfﬁcient\nlow-rank multi-"
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "arXiv\nmodal fusion with modality-speciﬁc factors."
        },
        {
          "Acknowledgements": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "preprint arXiv:1806.00064."
        },
        {
          "Acknowledgements": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJean-",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "nette N Chang,\nSungbok Lee,\nand\nShrikanth\nS",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Steven R Livingstone and Frank A Russo. 2018. The"
        },
        {
          "Acknowledgements": "Narayanan. 2008.\nIemocap:\nInteractive emotional",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "ryerson audio-visual database of emotional speech"
        },
        {
          "Acknowledgements": "Language\nre-\ndyadic motion\ncapture\ndatabase.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "and song (ravdess): A dynamic, multimodal set of"
        },
        {
          "Acknowledgements": "sources and evaluation, 42(4):335.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "facial and vocal expressions in north american en-"
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "glish. PloS one, 13(5)."
        },
        {
          "Acknowledgements": "Jianbo Chen,\nLe Song, Martin\nJ Wainwright,\nand",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "Michael\nI Jordan. 2018.\nLearning to explain: An",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Scott M Lundberg and Su-In Lee. 2017.\nA uniﬁed"
        },
        {
          "Acknowledgements": "information-theoretic perspective on model interpre-",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "approach to interpreting model predictions.\nIn Ad-"
        },
        {
          "Acknowledgements": "tation. arXiv preprint arXiv:1802.07814.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "vances\nin Neural\nInformation Processing Systems,"
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "pages 4765–4774."
        },
        {
          "Acknowledgements": "Gilles Degottex, John Kane, Thomas Drugman, Tuomo",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "Raitio, and Stefan Scherer. 2014.\nCovarep—a col-",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B"
        },
        {
          "Acknowledgements": "laborative voice analysis repository for speech tech-",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Tenenbaum,\nand\nJiajun Wu.\n2019.\nThe\nneuro-"
        },
        {
          "Acknowledgements": "2014\nieee\ninternational\nconference\nnologies.\nIn",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "symbolic\nconcept\nlearner:\nInterpreting\nscenes,"
        },
        {
          "Acknowledgements": "on acoustics, speech and signal processing (icassp),",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "words,\nand\nsentences\nfrom natural\nsupervision."
        },
        {
          "Acknowledgements": "pages 960–964. IEEE.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "arXiv preprint arXiv:1904.12584."
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Harry McGurk and John MacDonald. 1976. Hearing"
        },
        {
          "Acknowledgements": "Jean-Baptist Du Prel, Gerhard Hommel, Bernd R¨ohrig,",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "lips and seeing voices. Nature, 264(5588):746–748."
        },
        {
          "Acknowledgements": "and Maria Blettner. 2009.\nConﬁdence interval or",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "p-value?:\npart 4 of\na\nseries on evaluation of\nsci-",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Jiquan Ngiam, Aditya Khosla, Mingyu Kim,\nJuhan"
        },
        {
          "Acknowledgements": "¨",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "Deutsches\nArzteblatt\nInterna-\nentiﬁc publications.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Nam, Honglak Lee, and Andrew Y Ng. 2011. Mul-"
        },
        {
          "Acknowledgements": "tional, 106(19):335.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "timodal deep learning."
        },
        {
          "Acknowledgements": "Randi A Engle. 1998. Not channels but composite sig-",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Juan\nDS\nOrtega,\nMohammed\nSenoussaoui,\nEric"
        },
        {
          "Acknowledgements": "nals: Speech, gesture, diagrams and object demon-",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Granger, Marco Pedersoli,\nPatrick Cardinal,\nand"
        },
        {
          "Acknowledgements": "strations are integrated in multimodal explanations.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Alessandro L Koerich. 2019.\nMultimodal\nfusion"
        },
        {
          "Acknowledgements": "the twentieth annual conference\nIn Proceedings of",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "with deep neural networks for audio-video emotion"
        },
        {
          "Acknowledgements": "of the cognitive science society, pages 321–326.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": ""
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "recognition. arXiv preprint arXiv:1907.03196."
        },
        {
          "Acknowledgements": "Trevor J Hastie. 2017. Generalized additive models.\nIn",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Jeffrey Pennington, Richard Socher, and Christopher"
        },
        {
          "Acknowledgements": "Statistical models in S, pages 249–307. Routledge.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "Manning. 2014. Glove: Global vectors for word rep-"
        },
        {
          "Acknowledgements": "",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "the 2014 conference\nresentation.\nIn Proceedings of"
        },
        {
          "Acknowledgements": "Geoffrey E Hinton, Sara Sabour, and Nicholas Frosst.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "on empirical methods in natural\nlanguage process-"
        },
        {
          "Acknowledgements": "2018. Matrix capsules with em routing.",
          "Sepp\nHochreiter\nand\nJ¨urgen\nSchmidhuber.\n1997.": "ing (EMNLP), pages 1532–1543."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Advances in neural\ninformation processing systems,": "pages 3856–3866."
        },
        {
          "Advances in neural\ninformation processing systems,": ""
        },
        {
          "Advances in neural\ninformation processing systems,": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,"
        },
        {
          "Advances in neural\ninformation processing systems,": "Ilya\nSutskever,\nand Ruslan\nSalakhutdinov.\n2014."
        },
        {
          "Advances in neural\ninformation processing systems,": ""
        },
        {
          "Advances in neural\ninformation processing systems,": "Dropout:\na simple way to prevent neural networks"
        },
        {
          "Advances in neural\ninformation processing systems,": ""
        },
        {
          "Advances in neural\ninformation processing systems,": "The journal of machine learning\nfrom overﬁtting."
        },
        {
          "Advances in neural\ninformation processing systems,": ""
        },
        {
          "Advances in neural\ninformation processing systems,": "research, 15(1):1929–1958."
        },
        {
          "Advances in neural\ninformation processing systems,": ""
        },
        {
          "Advances in neural\ninformation processing systems,": ""
        },
        {
          "Advances in neural\ninformation processing systems,": ""
        },
        {
          "Advances in neural\ninformation processing systems,": "Samarth Tripathi,\nSarthak Tripathi,\nand Homayoon"
        },
        {
          "Advances in neural\ninformation processing systems,": ""
        },
        {
          "Advances in neural\ninformation processing systems,": "Beigi. 2018. Multi-modal emotion recognition on"
        },
        {
          "Advances in neural\ninformation processing systems,": "iemocap dataset using deep learning. arXiv preprint"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "and conﬁdence intervals a better alternative.",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "(0.98, 0.995)\npt"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "(0.991, 0.992)\npa"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "Marco\nTulio\nRibeiro,\nSameer\nSingh,\nand\nCarlos",
          "Conﬁdence Interval": "(0.807, 0.880)\npv"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "(0.948, 0.965)\npta"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "Guestrin. 2016. Why should i\ntrust you?: Explain-",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "(0.968, 0.969)\npav"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "ing the predictions of any classiﬁer.\nIn Proceed-",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "(0.588, 0.764)\npvt"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "ings of\nthe 22nd ACM SIGKDD international con-",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "(0.908, 0.949)\nptav"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "ference on knowledge discovery and data mining,",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "pages 1135–1144. ACM.",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "Table 6: Global interpretation (quantitative results) for"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "Multimodal Routing. Conﬁdence interval of pi, sam-"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "Sara Sabour, Nicholas Frosst,\nand Geoffrey E Hin-",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "pled from CMU-MOSEI sentiment task."
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "ton. 2017. Dynamic routing between capsules.\nIn",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "Advances in neural\ninformation processing systems,",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "pages 3856–3866.",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "Conﬁdence Interval"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "Ilya\nSutskever,\nand Ruslan\nSalakhutdinov.\n2014.",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "(0.980, 0.999)\npt"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "Dropout:\na simple way to prevent neural networks",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "(0.991, 0.992)\npa"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "The journal of machine learning\nfrom overﬁtting.",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "(0.816, 0.894)\npv"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "research, 15(1):1929–1958.",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "(0.935, 0.963)\npta"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "(0.967, 0.968)\npav"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "(0.635, 0.771)\npvt"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "Samarth Tripathi,\nSarthak Tripathi,\nand Homayoon",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "(0.913, 0.946)\nptav"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "Beigi. 2018. Multi-modal emotion recognition on",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "iemocap dataset using deep learning. arXiv preprint",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "arXiv:1804.05788.",
          "Conﬁdence Interval": "Table 7: Global interpretation (quantitative results) for"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "Multimodal Routing. Conﬁdence interval of pi, sam-"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,",
          "Conﬁdence Interval": "pled from CMU-MOSEI emotion task."
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "J Zico Kolter, Louis-Philippe Morency, and Ruslan",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "Salakhutdinov. 2019a. Multimodal\ntransformer\nfor",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "arXiv\nunaligned multimodal\nlanguage sequences.",
          "Conﬁdence Interval": ".1\nEncoding pi from input"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "preprint arXiv:1906.00295.",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "In practice, we use the same MulT to encode fi"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "and pi simultaneously. We design MulT to have an"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh,",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "Louis-Philippe Morency, and Ruslan Salakhutdinov.",
          "Conﬁdence Interval": "output dimension df + 1. A sigmoid function is"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "2019b. Learning factorized multimodal representa-",
          "Conﬁdence Interval": "applied to the last dimension of the output. For this"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "tions.\nIn International Conference on Representa-",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "output, the ﬁrst df dimensions refers to fi and the"
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "tion Learning.",
          "Conﬁdence Interval": ""
        },
        {
          "Jonas Ranstam. 2012. Why the p-value culture is bad": "",
          "Conﬁdence Interval": "last dimension refers to pi."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 7: ) and the product p r (Table",
      "data": [
        {
          "Sentiment": "0"
        },
        {
          "Sentiment": "(0.128, 0.206)"
        },
        {
          "Sentiment": "(0.040, 0.069)"
        },
        {
          "Sentiment": "(0.067, 0.089)"
        },
        {
          "Sentiment": "(0.037, 0.092)"
        },
        {
          "Sentiment": "(0.030, 0.058)"
        },
        {
          "Sentiment": "(0.076, 0.098)"
        },
        {
          "Sentiment": "(0.113, 0.164)"
        },
        {
          "Sentiment": ""
        },
        {
          "Sentiment": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 7: ) and the product p r (Table",
      "data": [
        {
          "Table 4: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": ""
        },
        {
          "Table 4: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": ""
        },
        {
          "Table 4: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": ""
        },
        {
          "Table 4: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "ptrt"
        },
        {
          "Table 4: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "para"
        },
        {
          "Table 4: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "pvrv"
        },
        {
          "Table 4: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "ptarta"
        },
        {
          "Table 4: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "pavrav"
        },
        {
          "Table 4: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "pvtrvt"
        },
        {
          "Table 4: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "ptavrtav"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 7: ) and the product p r (Table",
      "data": [
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "from CMU-MOSEI emotion task."
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": ".3\nRemarks on CMU-MOSEI Sentiment"
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": ""
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "Our model poses the problem as classiﬁcation and"
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": ""
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "predicts only integer labels, so we don’t provide"
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "mean average error and correlation metrics."
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": ""
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": ""
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": ".4\nRemarks on CMU-MOSEI Emotion"
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": ""
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "Due to the introduction of concepts in our model,"
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "we transform the CMU-MOSEI emotion recogni-"
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "tion task from a regression problem (every emotion"
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "has a score in [0, 3] indicating how strong the evi-"
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "dence of that emotion is) to a classiﬁcation problem."
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "For each sample with six emotion scores, we label"
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "all emotions with scores greater\nthan zero to be"
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "present in the sample. Then a data sample would"
        },
        {
          "Table 5: Global interpretation (quantitative results) for Multimodal Routing. Conﬁdence Interval of pirij, sampled": "have a multiclass label."
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Youtube-8m: A large-scale video classification benchmark",
      "authors": [
        "Sami Abu-El-Haija",
        "Nisarg Kothari",
        "Joonseok Lee",
        "Paul Natsev",
        "George Toderici",
        "Balakrishnan Varadarajan",
        "Sudheendra Vijayanarasimhan"
      ],
      "year": "2016",
      "venue": "Youtube-8m: A large-scale video classification benchmark",
      "arxiv": "arXiv:1609.08675"
    },
    {
      "citation_id": "2",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "3",
      "title": "A multimodal language region in the ventral visual pathway",
      "authors": [
        "Christian Büchel",
        "Cathy Price",
        "Karl Friston"
      ],
      "year": "1998",
      "venue": "Nature"
    },
    {
      "citation_id": "4",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "5",
      "title": "Learning to explain: An information-theoretic perspective on model interpretation",
      "authors": [
        "Jianbo Chen",
        "Le Song",
        "Martin Wainwright",
        "Michael Jordan"
      ],
      "year": "2018",
      "venue": "Learning to explain: An information-theoretic perspective on model interpretation",
      "arxiv": "arXiv:1802.07814"
    },
    {
      "citation_id": "6",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "2014 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "7",
      "title": "Confidence interval or p-value?: part 4 of a series on evaluation of scientific publications",
      "authors": [
        "Jean-Baptist Du Prel",
        "Gerhard Hommel",
        "Bernd Röhrig",
        "Maria Blettner"
      ],
      "year": "2009",
      "venue": "Deutsches Ärzteblatt International"
    },
    {
      "citation_id": "8",
      "title": "Not channels but composite signals: Speech, gesture, diagrams and object demonstrations are integrated in multimodal explanations",
      "authors": [
        "Randi Engle"
      ],
      "year": "1998",
      "venue": "Proceedings of the twentieth annual conference of the cognitive science society"
    },
    {
      "citation_id": "9",
      "title": "Generalized additive models",
      "authors": [
        "Trevor Hastie"
      ],
      "year": "2017",
      "venue": "Statistical models in S"
    },
    {
      "citation_id": "10",
      "title": "Matrix capsules with em routing. Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory",
      "authors": [
        "Geoffrey Hinton",
        "Sara Sabour",
        "Nicholas Frosst"
      ],
      "year": "2018",
      "venue": "Neural computation"
    },
    {
      "citation_id": "11",
      "title": "imotions: Unpack human behavior",
      "year": "2019",
      "venue": "imotions: Unpack human behavior"
    },
    {
      "citation_id": "12",
      "title": "Dynamic facial expressions of emotion transmit an evolving hierarchy of signals over time",
      "authors": [
        "Rachael Jack",
        "Oliver Garrod",
        "Philippe Schyns"
      ],
      "year": "2014",
      "venue": "Current biology"
    },
    {
      "citation_id": "13",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "14",
      "title": "When voices get emotional: a corpus of nonverbal vocalizations for research on emotion processing",
      "authors": [
        "São Luís César F Lima",
        "Sophie Castro",
        "Scott"
      ],
      "year": "2013",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "15",
      "title": "Microsoft coco: Common objects in context",
      "authors": [
        "Tsung-Yi Lin",
        "Michael Maire",
        "Serge Belongie",
        "James Hays",
        "Pietro Perona",
        "Deva Ramanan",
        "Piotr Dollár",
        "C Lawrence"
      ],
      "year": "2014",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "16",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Efficient low-rank multimodal fusion with modality-specific factors",
      "arxiv": "arXiv:1806.00064"
    },
    {
      "citation_id": "17",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "18",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "M Scott",
        "Su-In Lundberg",
        "Lee"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "19",
      "title": "The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
      "authors": [
        "Jiayuan Mao",
        "Chuang Gan",
        "Pushmeet Kohli",
        "Joshua Tenenbaum",
        "Jiajun Wu"
      ],
      "year": "2019",
      "venue": "The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
      "arxiv": "arXiv:1904.12584"
    },
    {
      "citation_id": "20",
      "title": "Hearing lips and seeing voices",
      "authors": [
        "Harry Mcgurk",
        "John Macdonald"
      ],
      "year": "1976",
      "venue": "Nature"
    },
    {
      "citation_id": "21",
      "title": "Multimodal deep learning",
      "authors": [
        "Jiquan Ngiam",
        "Aditya Khosla",
        "Mingyu Kim",
        "Juhan Nam",
        "Honglak Lee",
        "Andrew Ng"
      ],
      "year": "2011",
      "venue": "Multimodal deep learning"
    },
    {
      "citation_id": "22",
      "title": "Multimodal fusion with deep neural networks for audio-video emotion recognition",
      "authors": [
        "Juan Ds Ortega",
        "Mohammed Senoussaoui",
        "Eric Granger",
        "Marco Pedersoli",
        "Patrick Cardinal",
        "Alessandro Koerich"
      ],
      "year": "2019",
      "venue": "Multimodal fusion with deep neural networks for audio-video emotion recognition",
      "arxiv": "arXiv:1907.03196"
    },
    {
      "citation_id": "23",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "24",
      "title": "Why the p-value culture is bad and confidence intervals a better alternative",
      "authors": [
        "Jonas Ranstam"
      ],
      "year": "2012",
      "venue": "Why the p-value culture is bad and confidence intervals a better alternative"
    },
    {
      "citation_id": "25",
      "title": "Why should i trust you?: Explaining the predictions of any classifier",
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "26",
      "title": "Dynamic routing between capsules",
      "authors": [
        "Sara Sabour",
        "Nicholas Frosst",
        "Geoffrey Hinton"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "27",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "Nitish Srivastava",
        "Geoffrey Hinton",
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Ruslan Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "28",
      "title": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "authors": [
        "Samarth Tripathi",
        "Sarthak Tripathi",
        "Homayoon Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "arxiv": "arXiv:1804.05788"
    },
    {
      "citation_id": "29",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Multimodal transformer for unaligned multimodal language sequences",
      "arxiv": "arXiv:1906.00295"
    },
    {
      "citation_id": "30",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "International Conference on Representation Learning"
    },
    {
      "citation_id": "31",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Recognizing emotions in video using multimodal dnn feature fusion",
      "authors": [
        "Jennifer Williams",
        "Steven Kleinegesse"
      ],
      "year": "2018",
      "venue": "Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)"
    },
    {
      "citation_id": "33",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    }
  ]
}