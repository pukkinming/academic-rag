{
  "paper_id": "2505.23236v1",
  "title": "Towards Llm-Empowered Fine-Grained Speech Descriptors For Explainable Emotion Recognition",
  "published": "2025-05-29T08:36:28Z",
  "authors": [
    "Youjun Chen",
    "Xurong Xie",
    "Haoning Xu",
    "Mengzhe Geng",
    "Guinan Li",
    "Chengxi Deng",
    "Huimeng Wang",
    "Shujie Hu",
    "Xunying Liu"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Speech emotion descriptors",
    "LLM",
    "VAE",
    "Information Bottleneck"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents a novel end-to-end LLM-empowered explainable speech emotion recognition (SER) approach. Finegrained speech emotion descriptor (SED) features, e.g., pitch, tone and emphasis, are disentangled from HuBERT SSL representations via alternating LLM fine-tuning to joint SER-SED prediction and ASR tasks. VAE compressed HuBERT features derived via Information Bottleneck (IB) are used to adjust feature granularity. Experiments on the IEMOCAP and MELD benchmarks demonstrate that our approach consistently outperforms comparable LLaMA-based SER baselines, including those using either (a) alternating multi-task fine-tuning alone or (b) feature disentanglement only. Statistically significant increase of SER unweighted accuracy by up to 4.0% and 3.7% absolute (5.4% and 6.6% relative) are obtained. More importantly, emotion descriptors offer further explainability for SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is fundamental to humancomputer interaction. Despite decades of promising advancements, most research  [1] [2] [3] [4]  has primarily focused on classifying speech into single, discrete emotion categories. However, such approaches oversimplify the complexity of human emotions, as single-label outputs fail to capture the nuanced interplay of linguistic content and paralinguistic cues. Consequently, traditional SER systems lack the granularity and explainability necessary for a deeper and more human-like understanding of emotional expression  [5] .\n\nIn recent years, substantial efforts have been directed toward improving the explainability of the SER models by probing linguistic content-and emotion descriptor-related intermediate representations and separately decoding them. These include: 1) disentangling task-oriented intermediate features from compact encodings  [6] [7] [8] [9] [10] [11] , e.g. SSL representations; and 2) modeling them individually based on the input audios and transcripts generated by ASR models  [12] [13] [14] [15] [16] [17] . More recent research has focused on large language models (LLMs), demonstrating their ability to effectively interpret both linguistic content and emotion descriptors in SER tasks even as many approaches continue to rely on single-word labels for entire utterances  [18] [19] [20] [21] [22] . Furthermore, LLMs have been employed to generate natural language descriptions, providing more precise and accessible insights into fine-grained emotion descriptors  [5, [23] [24] [25] [26] .\n\nHowever, these prior studies suffer from the following limitations: a) Lack of explainable SER methods using both feature disentanglement and fine-grained emotion descriptors. While some methods disentangle content-and emotion descriptorrelated representations  [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17]  or associate emotion-related emotion descriptors with predicted emotion labels  [5, [23] [24] [25] [26] , none integrates these approaches to improve human understanding of emotion cues; b) Lack of an end-to-end explainable SER architecture capable of simultaneously generating transcript, emotion descriptors and emotion labels directly from input speech. Instead, some methods rely on ASR models to transcribe input speech into text before performing SER  [12] [13] [14] [15] [16] [17] , making the SER performance inherently dependent on the efficacy of the selected ASR models; and c) Lack of exploration of the tight connection between fine-grained emotion descriptors prediction and SER performance. While some recent studies have focused on capturing fine-grained emotion descriptors, their quantitative impact on SER performance remains underexplored  [5, [23] [24] [25] [26] .\n\nTo this end, this paper presents a novel end-to-end LLMempowered explainable SER approach. Fine-grained emotion descriptor features are disentangled from HuBERT SSL representations via alternating LLM fine-tuning to joint SER-SED prediction and content transcription based ASR tasks. VAE compressed HuBERT features derived via IB are used to adjust the trade-off between accuracy and model complexity.\n\nExperiments on the benchmark IEMOCAP and MELD datasets suggest our approach consistently outperforms the comparable LLaMA SER baselines constructed using either a) alternating multi-task fine-tuning alone; or b) features disentanglement only. Statistically significant increases of SER unweighted accuracy by up to 4.0% and 3.7% absolute (5.4% and 6.6% relative) are obtained. More importantly, the predicted emotion descriptors offer further explainability for SER.\n\nThe main contributions of our work are summarized below: 1) To the best of our knowledge, this paper presents the first work to use both feature disentanglement and fine-grained speech emotion descriptors for explainable SER. In contrast, previous research paid attention to either feature disentanglement  [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17]  or fine-grained emotion descriptor prediction  [5, [23] [24] [25] [26]  alone.\n\n2) This paper introduces a novel end-to-end LLM-based explainable SER architecture that simultaneously generates transcript, emotion descriptors and emotion labels directly from input speech. In contrast, prior research employs external ASR models to generate transcripts before performing SER  [12] [13] [14] [15] [16] [17] , which may lead to low SER performance affected by ASR errors.\n\n3) This paper investigates the tight connection between fine-grained SED prediction and SER performance. In contrast, prior research focused on generating fine-grained and precise emotion descriptors alone without further quantita-Figure  1 : Illustration of the proposed LLM-empowered explainable speech emotion recognition approach and prompt template.\n\ntive research on the importance of fine-grained SED prediction for SER performance  [5, [23] [24] [25] [26] . Our code is available at https://github.com/SEUJames23/explainable-emotionrecognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Disentanglement",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Alternating Multi-Task Fine-Tuning",
      "text": "A parameter-efficient fine-tuning method known as low-rank adaptation, or LoRA  [27] , is widely employed to fine-tune the LLM-based models. However, extensively fine-tuning the model using the LoRA method on a large amount of promptrepetitive speech-text data may cause the model to overfit on specific speech tasks and fail to address the remaining instructions  [20] . To avoid this problem, we adopt an alternating multitask fine-tuning strategy during training. More details on leveraging alternating multi-task fine-tuning to disentangle contentand emotion descriptor-related representations can be found in Sec. 3.5.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Information Bottleneck",
      "text": "IB  [28]  aims to learn a compressed representation Z of the input X while preserving relevant information about the output Y. The whole process can be interpreted as maximizing the mutual information I(Z, Y) between Y and Z, and minimizing the mutual information I(Z, X) between X and Z simultaneously. The mathematical formula of IB loss is shown in Eq. (  1 ).\n\nHowever, the exact computation of LIB is intractable. To address this, a variational approximate estimate of IB (VIB)  [29]  has been introduced. Specifically, for each training example (x, y), the IB loss is upper bounded by:\n\nwhere, q(y|z) is a variational approximation to p(y|z). p0(z), a specified prior distribution for latent representation Z, usually takes standard normal distribution. And p(z|x) denotes an estimate of the posterior probability of z. β ≥ 0 controls the balance between compression and prediction.\n\nIn the VIB loss function, the first term, the task loss, encourages the encoder to preserve information relevant to the label, while the second term, the KL divergence, pushes it to discard as much information as possible. Eq.(  2 ) takes the same form of variational auto-encoder (VAE) style Bayesian learning  [30] [31] [32] , with the reparameterization trick used in the optimization process.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "System Architecture",
      "text": "As is illustrated in Fig.  1 , the architecture of our approach consists of a HuBERT encoder, two feature disentanglement blocks, two feature adapters and an LLM decoder. Here, we denote the HuBERT representations, intermediate representations, and target outputs as X, Z (Z Con for content-related, Z Des for emotion descriptor-related), and Y, respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Hubert Encoder",
      "text": "We use the encoder of the self-supervised learning (SSL) model HuBERT-large 1    [33]  to encode the input audio. Since the different hidden layer outputs of the SSL models contain the latent representations for various tasks  [33, 34] , we use all the hidden layers' outputs as X. We keep the HuBERT encoder's parameters frozen throughout the entire training process.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Content And Descriptor Features Disentanglement",
      "text": "Based on IB, we construct content and emotion descriptor disentanglement blocks consisting of weighted sum layers, content and emotion descriptor encoders, and VAE to probe the compact intermediate representations. Specifically, during training, weighted sum layers first learn the weights to extract the most relevant information from the SSL representations x based on the downstream tasks. After being passed through the corresponding encoders which comprise a two-layer perceptron with GELU activation between the linear layers, x are then input to two linear layers to yield the mean and standard deviation vectors, i.e., µ(x) and σ(x), of the posterior distribution p(z|x). We simulate Gaussian samples of z which are then fed into the LLM decoder. During inference, z = µ(x) is used instead of sampling from p(z|x).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Content And Descriptor Feature Adapters",
      "text": "To enable the LLM decoder to comprehend the content-and emotion descriptor-related information, we incorporate trainable adapters mapping the latent representations into the embedding space of the LLM. Following  [35] , after downsampling Z, the corresponding adapters produce the textual content and emotion descriptor embeddings S (S Con for content and S Des for emotion descriptor) with a two-layer perceptron with ReLU activation between the linear layers. The above process can be formalized as Eq. (  3 ). S = Linear(ReLU(Linear(DownSample(Z)))) (3)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Large Language Model",
      "text": "We use the state-of-the-art Llama-3.1-8B-Instruct3 2  [36]  as the LLM decoder, working as logq(y|z) in VIB. The prompt template Prompt(•) is shown in Fig.  1  (bottom middle, in light blue), where S Con and S Des are filled into the position corresponding to ⟨Content⟩ and ⟨Speech descriptor⟩, respectively. Then the entire sequence Prompt(S) is input into the LLM decoder to autoregressively generate the text response\n\nThe LLM loss LLLM of the decoder based on the cross-entropy loss can be regarded as the task loss in VIB loss (Eq. 2), which is calculated by:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Alternating Fine-Tuning On Asr And Ser-Sed Tasks",
      "text": "As shown in Fig.  2 , we split the whole training process into two stages on the two sequential downstream tasks. In the first stage, the content encoder and adapter are trainable, while the emotion descriptor encoder and adapter are frozen with emotion descriptor tokens set to zero. Via LLM fine-tuning to speech content transcription based ASR task, content-related information disentangled from HuBERT SSL representations is preserved as much as possible. Following the first stage, the content encoder and adapter are frozen, while the emotion descriptor encoder and adapter are trainable in the second stage. Since sufficient content-related information has been retained in the first stage, the emotion descriptor encoder only focuses on disentangling SED-related information as a complementary in the joint SER-SED prediction and speech content transcription based ASR tasks. Note that we do not need to follow a two-stage process to obtain the content and emotion descriptor prediction during inference.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "We use publicly available SED prediction and SER datasets in the training and evaluation stages.\n\nTraining. SpeechCraft  [37]  annotates the utterances from four well-known corpora in a unified and fine-grained manner, which mainly considers the emotion descriptors such as pitch, energy, speed, age, gender, tone and emphasis. We select the largescale subset GigaSpeech-m  [38]  among them as the training dataset, which includes above 670k audio clips with a total duration reaching 739.91 hours. Additionally, utterances regarded as happy, sad, angry and neutral accounted for the vast majority of these data, with a few surprised, disgusted and fearful ones. Evaluation. Following  [25] , we conduct zero-shot evaluations on the SED prediction performance of the proposed method on the Mandarin and English speech emotion caption datasets EMOSEC  [39] , which consists of 15 male and 15 female speakers and 45 039 utterances in total. Furthermore, we explore the SER performance of the proposed method on the two widelyused datasets, i.e., IEMOCAP  [40]  and MELD  [41] . To be consistent and comparable with previous works  [42, 43]  on IEMO-CAP, we merge \"excited\" with \"happy\" to better balance the size of each emotion class, resulting in four classes. And we 2 https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct conduct leave-one-session-out 5-fold cross-validation on this data  3  . As for MELD, we just follow its original split setup.\n\nFigure  2 : Illustration of two stages of the whole training process on the two sequential downstream tasks with the alternating multi-task fine-tuning strategy.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Baseline Systems",
      "text": "We compare the proposed method with the following baselines: HuBERT+LLM-SER+SED+ASR: LLaMA SER system finetuned to joint SER-SED prediction and ASR tasks using standard HuBERT features.\n\nHuBERT+Feature Disentanglement(FD)+LLM-SER+ASR:\n\nLLaMA SER system fine-tuned to joint SER and ASR tasks using disentangled HuBERT representations.\n\nHuBERT+Feature Disentanglement(FD)+LLM-SER+SED:\n\nLLaMA SER system fine-tuned to joint SER and SED prediction tasks using disentangled HuBERT representations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Metrics Sed Prediction Task.",
      "text": "To evaluate the accuracy of the SED prediction, we adopt the same supervised metrics in  [5] , containing standard natural language generation metrics BLEU @4(B@4), M ET EOR(M ), ROU GE l (R), CIDEr(C), and SP ICE(S)  4  . B@4 focuses on the appearance frequency of emotion clues and is used to evaluate the emotional consistency and fine-grainedness of the generated SED. Compared with B@4, M considers synonyms more, and R pays more attention to the sufficiency and faithfulness of output. C and S compute the accuracy of SED using human consensus. SER and ASR tasks. We adopt the commonly used metrics unweighted accuracy (UA) and word error rate (WER) for SER and ASR tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training Details",
      "text": "As mentioned in Sec.3.5, we adopt the alternating supervised fine-tuning strategy, with the first and second stages separately conducted for 4 epochs. For the first stage, we set the batch size to 8 and the constant learning rate to 2e -4 with the first 3% of steps for warmup. For the second stage, with the same batch size and warmup ratio as the first stage, we adopt a cosine learning rate scheduler and set the peak learning rate to 2e -5 . Additionally, the β in the IB regularization iterates from 1e 0 to 1e -4 , which will be further discussed in Sec. 4.5.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Main Results",
      "text": "SED prediction performance comparison with baselines and state-of-the-art methods is shown in Tab. 1. The proposed system fine-tuned to joint SER-SED prediction and ASR tasks with feature disentanglement (Sys. 3) consistently outperforms the baseline systems (Sys. 1,2) on the SED prediction performance.\n\nTable  1 : SED prediction performance comparison with baselines and state-of-the-art methods on the EMOSEC dataset. \"P\" stands for the results of methods from public papers. \" ‡\" represents statistically significant (Paired Single-tailed t TEST  [44] , p=0.05) score improvement over Sys. 1.   1) , respectively. Specifically, the highest B@4 and M demonstrate feature disentanglement can preserve more SED-related information, enabling the LLM decoder to generate more finegrained SED, while the highest R, C and S scores reveal the accuracy of the predicted SED. Additionally, the best-performing system (Sys. 3) can achieve higher evaluation scores than the state-of-the-art SED methods (Sys. P1-P4), which also shows the efficacy of our approach. SER performance comparison with baselines and state-ofthe-art methods is shown in Tab. 2. The same trend can be found in the SER task along with ASR. The proposed system fine-tuned to joint SER-SED prediction and ASR tasks with feature disentanglement (Sys. 7) consistently outperforms the baselines (Sys.  [4] [5] [6] . The highest statistically significant improvement of unweighted accuracy can reach 4.0% and 3.7% absolute (5.4% and 6.6%) on IEMOCAP and MELD (Sys.7 vs. Tab. 3 illustrates that removing any of these techniques, or modifying the parameter settings all lead to performance degradation, where Sys. 4 is the best performing system in this paper (also as Sys. 3 in Tab. 1 and Sys. 7 in Tab. 2). We can see that: 1) The predicted emotion descriptors offer further explainability for SER with improvements of unweighted accuracy by 7.0% and 6.8% absolute (9.9% and 12.8% relative) on IEMOCAP and MELD (Sys. 4 vs. Sys. 3), respectively. 2) Speech content information also contributes to some extent to SER (Sys. 4 vs. Sys. 1), although it is not as effective as emotion descriptors.\n\n3) The value of β has an effect on SED prediction and SER performance (Sys.4-8), since β controls the trade-off between removing redundant information (high β) and preserving task-related information (low β) from the HuBERT SSL representations. When β falls within the range of 1e -3 to e -1 (Sys.4,6,7), the SER performance remains relatively stable. 4) When β set to 1 (Sys. 5), IB regularization degrades into a standard VAE. Compared to other systems with IB (Sys. 4,6-8), there is a significant decrease in the performance of Sys. 5, proving IB is effective for this task. 5) The alternating SFT method can efficiently mitigate overfitting in ASR tasks to improve SED prediction and SER performance (Sys. 4 vs. Sys. 2).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "This paper presents a novel end-to-end LLM-empowered explainable SER approach. Fine-grained SED features are disentangled from HuBERT SSL representations via alternating LLM fine-tuning to joint SER-SED prediction and ASR tasks. VAE compressed HuBERT features derived via IB are used to adjust feature granularity. Experiments on the benchmark IEMOCAP and MELD datasets suggest our approach consistently outperforms the comparable LLaMA SER baselines constructed using either a) alternating tasks fine-tuning alone; or b) features disentanglement only. Statistically significant increases of SER unweighted accuracy by up to 4.0% and 3.7% absolute (5.4% and 6.6% relative) are obtained. Future research will focus on exploring the differences in fine-grained emotion descriptors when different people express the same emotion.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the proposed LLM-empowered explainable speech emotion recognition approach and prompt template.",
      "page": 2
    },
    {
      "caption": "Figure 1: , the architecture of our approach",
      "page": 2
    },
    {
      "caption": "Figure 1: (bottom middle, in light",
      "page": 3
    },
    {
      "caption": "Figure 2: , we split the whole training process into two",
      "page": 3
    },
    {
      "caption": "Figure 2: Illustration of two stages of the whole training pro-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 1: SED prediction performance comparison with base- Table3:AblationstudiesontheimportanceofβintheIBregu-",
      "data": [
        {
          "ID": "",
          "Experimental Setup": "β",
          "SED Prediction Accuracy": "EMOSEC",
          "SER & ASR Accuracy": "IEMOCAP"
        },
        {
          "ID": "",
          "Experimental Setup": "",
          "SED Prediction Accuracy": "",
          "SER & ASR Accuracy": ""
        },
        {
          "ID": "1\n2\n3\n4\n5",
          "Experimental Setup": "e−2\ne0\n6 e−1\n7 e−3\n8 e−4",
          "SED Prediction Accuracy": "10.2",
          "SER & ASR Accuracy": "22.0 30.1 19.2 8.8 75.5∗"
        },
        {
          "ID": "",
          "Experimental Setup": "",
          "SED Prediction Accuracy": "8.0\n17.9 26.7 16.3 5.1",
          "SER & ASR Accuracy": "76.8"
        },
        {
          "ID": "",
          "Experimental Setup": "",
          "SED Prediction Accuracy": "/",
          "SER & ASR Accuracy": "70.6∗"
        },
        {
          "ID": "",
          "Experimental Setup": "",
          "SED Prediction Accuracy": "10.3\n22.1 30.4 19.3 9.0",
          "SER & ASR Accuracy": "77.6"
        },
        {
          "ID": "",
          "Experimental Setup": "",
          "SED Prediction Accuracy": "7.8",
          "SER & ASR Accuracy": "16.9 25.2 12.4 6.7 72.8∗"
        },
        {
          "ID": "",
          "Experimental Setup": "",
          "SED Prediction Accuracy": "9.8\n21.5 29.6 18.9 8.6",
          "SER & ASR Accuracy": "77.3"
        },
        {
          "ID": "",
          "Experimental Setup": "",
          "SED Prediction Accuracy": "10.4\n21.9 30.3 19.4 8.9",
          "SER & ASR Accuracy": "77.4"
        },
        {
          "ID": "",
          "Experimental Setup": "",
          "SED Prediction Accuracy": "10.0\n21.7 29.6 18.6 8.5",
          "SER & ASR Accuracy": "77.1"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion neural transducer for fine-grained speech emotion recognition",
      "authors": [
        "S Shen"
      ],
      "year": "2024",
      "venue": "ICASSP"
    },
    {
      "citation_id": "3",
      "title": "Generalization of self-supervised learningbased representations for cross-domain speech emotion recognition",
      "authors": [
        "A Naini"
      ],
      "year": "2024",
      "venue": "ICASSP"
    },
    {
      "citation_id": "4",
      "title": "Noise-robust speech emotion recognition using shared self-supervised representations with integrated speech enhancement",
      "authors": [
        "J.-T Tzeng"
      ],
      "year": "2025",
      "venue": "ICASSP"
    },
    {
      "citation_id": "5",
      "title": "Emix: a data augmentation method for speech emotion recognition",
      "authors": [
        "A Dang"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Secap: Speech emotion captioning with large language model",
      "authors": [
        "Y Xu"
      ],
      "year": "2024",
      "venue": "AAAI"
    },
    {
      "citation_id": "7",
      "title": "Disentanglement network: Disentangle the emotional features from acoustic features for speech emotion recognition",
      "authors": [
        "Z Yuan"
      ],
      "year": "2024",
      "venue": "ICASSP"
    },
    {
      "citation_id": "8",
      "title": "Disentangling textual and acoustic features of neural speech representations",
      "authors": [
        "H Mohebbi"
      ],
      "year": "2024",
      "venue": "Disentangling textual and acoustic features of neural speech representations"
    },
    {
      "citation_id": "9",
      "title": "Variational information bottleneck for effective lowresource audio classification",
      "authors": [
        "S Si"
      ],
      "year": "2021",
      "venue": "Variational information bottleneck for effective lowresource audio classification"
    },
    {
      "citation_id": "10",
      "title": "Multi-modal emotion recognition using multiple acoustic features and dual cross-modal transformer",
      "authors": [
        "Y Wu"
      ],
      "year": "2024",
      "venue": "ICASSP"
    },
    {
      "citation_id": "11",
      "title": "Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition",
      "authors": [
        "Z Ma"
      ],
      "year": "2024",
      "venue": "ICASSP"
    },
    {
      "citation_id": "12",
      "title": "Variational information bottleneck based regularization for speaker recognition",
      "authors": [
        "D Wang"
      ],
      "year": "2021",
      "venue": "Variational information bottleneck based regularization for speaker recognition"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "B Schuller"
      ],
      "year": "2004",
      "venue": "ICASSP"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition with multi-level acoustic and semantic information extraction and interaction",
      "authors": [
        "Y Gao"
      ],
      "year": "2024",
      "venue": "Speech emotion recognition with multi-level acoustic and semantic information extraction and interaction"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition using decomposed speech via multi-task learning",
      "authors": [
        "J.-H Hsu"
      ],
      "year": "2023",
      "venue": "Speech emotion recognition using decomposed speech via multi-task learning"
    },
    {
      "citation_id": "16",
      "title": "Multimodal emotion recognition with high-level speech and text features",
      "authors": [
        "M Makiuchi"
      ],
      "year": "2021",
      "venue": "Multimodal emotion recognition with high-level speech and text features"
    },
    {
      "citation_id": "17",
      "title": "Frontend attributes disentanglement for speech emotion recognition",
      "authors": [
        "Y.-X Xi"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Q Jin"
      ],
      "year": "2015",
      "venue": "ICASSP"
    },
    {
      "citation_id": "19",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "C Tang"
      ],
      "year": "2023",
      "venue": "Salmonn: Towards generic hearing abilities for large language models"
    },
    {
      "citation_id": "20",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Y Chu"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models"
    },
    {
      "citation_id": "21",
      "title": "Wavllm: Towards robust and adaptive speech large language model",
      "authors": [
        "S Hu"
      ],
      "year": "2024",
      "venue": "EMNLP"
    },
    {
      "citation_id": "22",
      "title": "Enhancing multimodal emotion recognition through asr error compensation and llm fine-tuning",
      "authors": [
        "J Kyung"
      ],
      "year": "2024",
      "venue": "Enhancing multimodal emotion recognition through asr error compensation and llm fine-tuning"
    },
    {
      "citation_id": "23",
      "title": "Large language model based generative error correction: A challenge and baselines for speech recognition, speaker tagging, and emotion recognition",
      "authors": [
        "C.-H Yang"
      ],
      "year": "2024",
      "venue": "Large language model based generative error correction: A challenge and baselines for speech recognition, speaker tagging, and emotion recognition"
    },
    {
      "citation_id": "24",
      "title": "Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research",
      "authors": [
        "X Mei"
      ],
      "year": "2024",
      "venue": "TASLP"
    },
    {
      "citation_id": "25",
      "title": "Training audio captioning models without audio",
      "authors": [
        "S Deshmukh"
      ],
      "year": "2024",
      "venue": "ICASSP"
    },
    {
      "citation_id": "26",
      "title": "Aligncap: Aligning speech emotion captioning to human preferences",
      "authors": [
        "Z Liang"
      ],
      "year": "2024",
      "venue": "Aligncap: Aligning speech emotion captioning to human preferences"
    },
    {
      "citation_id": "27",
      "title": "Clap4emo: Chatgpt-assisted speech emotion retrieval with natural language supervision",
      "authors": [
        "W.-C Lin"
      ],
      "year": "2024",
      "venue": "ICASSP"
    },
    {
      "citation_id": "28",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E Hu"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models"
    },
    {
      "citation_id": "29",
      "title": "The information bottleneck method",
      "authors": [
        "N Tishby"
      ],
      "year": "2000",
      "venue": "The information bottleneck method"
    },
    {
      "citation_id": "30",
      "title": "Deep variational information bottleneck",
      "authors": [
        "A Alemi"
      ],
      "year": "2017",
      "venue": "ICLR"
    },
    {
      "citation_id": "31",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma"
      ],
      "year": "2014",
      "venue": "ICLR"
    },
    {
      "citation_id": "32",
      "title": "Bayesian learning for deep neural network adaptation",
      "authors": [
        "X Xie"
      ],
      "year": "2021",
      "venue": "TASLP"
    },
    {
      "citation_id": "33",
      "title": "Bayesian learning of lf-mmi trained time delay neural networks for speech recognition",
      "authors": [
        "S Hu"
      ],
      "year": "2021",
      "venue": "TASLP"
    },
    {
      "citation_id": "34",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W Hsu"
      ],
      "year": "2021",
      "venue": "TASLP"
    },
    {
      "citation_id": "35",
      "title": "Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert",
      "authors": [
        "H.-J Chang"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "36",
      "title": "Llama-omni: Seamless speech interaction with large language models",
      "authors": [
        "Q Fang"
      ],
      "year": "2024",
      "venue": "Llama-omni: Seamless speech interaction with large language models"
    },
    {
      "citation_id": "37",
      "title": "The llama 3 herd of models",
      "authors": [
        "A Dubey"
      ],
      "year": "2024",
      "venue": "The llama 3 herd of models"
    },
    {
      "citation_id": "38",
      "title": "Speechcraft: A fine-grained expressive speech dataset with natural language description",
      "authors": [
        "Z Jin"
      ],
      "year": "2024",
      "venue": "ACM MM"
    },
    {
      "citation_id": "39",
      "title": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio",
      "authors": [
        "G Chen"
      ],
      "year": "2021",
      "venue": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio"
    },
    {
      "citation_id": "40",
      "title": "Emosec: Emotion recognition from scene context",
      "authors": [
        "S Thuseethan"
      ],
      "year": "2022",
      "venue": "NEUROCOMPUTING"
    },
    {
      "citation_id": "41",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso"
      ],
      "year": "2008",
      "venue": "LANG RESOUR EVAL"
    },
    {
      "citation_id": "42",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations"
    },
    {
      "citation_id": "43",
      "title": "Emobox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark",
      "authors": [
        "Z Ma"
      ],
      "year": "2024",
      "venue": "Emobox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark"
    },
    {
      "citation_id": "44",
      "title": "Disentanglement network: Disentangle the emotional features from acoustic features for speech emotion recognition",
      "authors": [
        "Z Yuan"
      ],
      "year": "2024",
      "venue": "ICASSP"
    },
    {
      "citation_id": "45",
      "title": "How first-and second-language emotion words influence emotion perception in swedish-english bilinguals",
      "authors": [
        "M.-F Champoux-Larsson"
      ],
      "year": "2022",
      "venue": "BILING-LANG COGN"
    },
    {
      "citation_id": "46",
      "title": "Blsp-emo: Towards empathetic large speechlanguage models",
      "authors": [
        "C Wang"
      ],
      "year": "2024",
      "venue": "EMNLP"
    },
    {
      "citation_id": "47",
      "title": "Speech emotion recognition using decomposed speech via multi-task learning",
      "authors": [
        "J.-H Hsu"
      ],
      "year": "2023",
      "venue": "Speech emotion recognition using decomposed speech via multi-task learning"
    },
    {
      "citation_id": "48",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen"
      ],
      "year": "2022",
      "venue": "IEEE J. Sel. Top. Signal Process"
    },
    {
      "citation_id": "49",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski"
      ],
      "year": "2020",
      "venue": "Adv. Neural Inf. Process. Syst"
    }
  ]
}