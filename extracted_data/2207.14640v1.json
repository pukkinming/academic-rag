{
  "paper_id": "2207.14640v1",
  "title": "Emosens: Emotion Recognition Based On Sensor Data Analysis Using Lightgbm",
  "published": "2022-07-12T13:52:32Z",
  "authors": [
    "Gayathri S",
    "Akshat Anand",
    "Astha Vijayvargiya",
    "Pushpalatha M",
    "Vaishnavi Moorthy",
    "Sumit Kumar",
    "Harichandana B S S"
  ],
  "keywords": [
    "Smart Wearable",
    "affective computing",
    "machine learning",
    "emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Smart wearables have played an integral part in our day to day life.From recording ECG signals to analysing body fat composition,the smart wearables can do it all. The smart devices encompass various sensors which can be employed to derive meaningful information regarding the user's physical and psychological conditions.Our approach focuses on employing such sensors to identify and obtain the variations in the mood of a user at a given instance through the use of supervised machine learning techniques.The study examines the performance of various supervised learning models such as Decision Trees, Random Forests, XGBoost, LightGBM on the dataset. With our proposed model, we obtained a high recognition rate of 92.5% using XGBoost and LightGBM for 9 different emotion classes.By utilizing this, we aim to improvise and suggest methods to aid emotion recognition for better mental health analysis and mood monitoring.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Affective computing involves studying and developing intelligent systems that can identify, understand and imitate human emotions. It is a multidisciplinary field that spans across computer science, cognitive science and psychology. The field of affective computing was first discussed in the year 1995 in Rosalind Picard's work  [1]  which elaborated on the ability of computers to imitate human emotions by understanding the underlying expression. It is necessary to study the emotions of humans as they hold a huge significance in human interactions as suggested by Ekman  [2] . The fact that humans treat computers as other living creatures necessitates improving human-computer interaction.  [3] . This solidifies the idea that the interactions between humans and computers can be made naturalistic through the use of affective computing. In other words, affective computing helps in instilling human essence in artificially intelligent machines. Research in affective computing can be categorized into two: sentiment analysis and emotion recognition.Our objective is to focus on emotion recognition.\n\nEmotion recognition has always been a thriving topic for areas of research as it deals with human interaction which in fact helps understanding human emotional states better with use of engineering, psychology and cognitive science. Recognizing emotions through these modalities could be helpful in accurately understanding psychological health and humans with computer interaction without using external medical instruments. Emotion recognition has numerous applications. It can help monitor emotional states of humans in critical situations. Under clinical situations, it can be used to monitor psychological conditions of patients.In the entertainment/ video game industry, it can be used to recognize the emotions of users to a particular video, film clip or game. In the consumer service industry to improve marketing or enhance user experience by identifying the user's reaction to a product. Since these applications of emotion recognition are highly user-centered, it is only fair if the solution is easily accessible, low cost and computationally efficient. The proposed approach aims to integrate the vast availability of smart edge devices and advanced machine learning techniques to make emotion recognition at a user-centered level feasible.\n\nEmotions are complex and encompass changes in feelings, thoughts, behaviour, poise, body language, cognitive reactions. These changes can be recognized through various audio and visual cues. Most studies use these cues to identify the emotional states of a human at a given situation. However, facial expression and tonal variations can be subject to manipulation as humans tend to control emotions. Therefore, a better and more accurate indicator of emotional state would be physiological signals. The physiological signals are directly related to the autonomous nervous system (ANS) which is a part of the peripheral nervous system of the human body. The ANS sympathetic nerves are triggered when a human goes through a positive or negative experience. This affects the heart rate significantly. There are a number of physiological signals that are used to perform emotion recognition. A few popular ones include ECG(Electrocardiogram), EEG(Electroencephalogram), GSR(Galvanic Skin Response), EMG(Electromyography), HRV(Heart Rate Variability). Out of these, the ECG is most popularly used as it indicates emotions, less susceptible to noise and relatively easy to access.\n\nEverything a person feels or does has an immediate impact on their heart.ECG is essentially obtained by studying the continuous contractions and expanding of the heart. Hence, the heart rate conveys the variations in emotions of a person. The sympathetic nervous system is triggered due to variability in emotion which in turn enhances perception to internal and external stimuli. These changes can be detected in ECG. Apart from being indicative of the variations in emotions of a human, heart rate can be easily measured using the sensors in modern smart watches. Not only would this ensure a user-centered approach towards emotion recognition and monitoring, but is also cost-efficient. However, the acquisition of data and processing it for further use is not as easy it sounds as the signals and heart rates are subject to variations within the same subject (intra-subject) and between different subjects(inter-subject), apart from the noise due to change in posture, monitoring environment and devices used to record the signals  [31] .\n\nWith the complexity of data acquisition and processing being high, most datasets used for the task of emotion recognition have similar methods of data acquisition. A common observation is that most of the datasets collect data from a small number of volunteers who are stimulated using emotion inducing audio/video clips and the subsequent reaction in the required physiological signal signal that follows is recorded through an appropriate sensor. The emotion labelling is performed through Self Assessment Manikins (SAM)  [4] . Very commonly used datasets for the purpose of emotion recognition are DREAMER  [5] , MEPD  [6] , MAHNOB-HCI  [7] , K-Emocon  [8] , AMIGOS  [9] . The proposed work was performed with the DREAMER dataset. Appropriate preprocessing, feature extraction, training and validation methods were observed and performed on the DREAMER dataset to achieve the required results. The study contributes the following: a user-centered approach to develop an emotion recognition system using 1) Low-cost, non-invasive, wearable sensors 2) Classifying emotions using machine learning\n\nWe have explained in detail about the workflow for developing our emotion recognition system in the upcoming sections. The classification approaches that were considered include K-Nearest Neighbours, Linear SVM, RBF SVM, Decision Trees, Random Forests, XGBoost, LightGBM, AdaBoost.\n\nThe structure of the following sections are as follows: Section II contains the review of prior arts relevant to our proposed work. Section III goes over the dataset,the preprocessing techniques used. Section IV discusses emotion models and the target variable. Section V presents the classification models and their results. Section VI concludes the study and proposes future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Literature Review",
      "text": "Various physiological modalities are used across multiple studies for emotion recognition. ECG has been widely used to monitor and assess psychological and mental conditions  [10] . It can act as a powerful indicator of stress in the human body  [11]    [12] . As a result,the proposed work is interested in ECG-based emotion recognition, we only considered the ECG modality of the available datasets. Among the available datasets we considered AMIGOS, K-Emocon, MAHNOB-HCI, MEPD, DREAMER.\n\nAfter examining the above datasets and the features, we concluded that the DREAMER dataset fit our needs the best.\n\nEkman suggested that emotions are discrete and measurable  [13] . On that basis he summarized emotions into 6 elementary emotions including happy, anger, sad, fear, disgust and surprise  [14] . This proposed model is called the Discrete Emotion Model.The proposed study aims to identify discrete emotions through ECG signals. A study by Dissanayake et al.  [15]  recognized 6 emotional classes through HRV features obtained from ECG signal. An accuracy of 80% was achieved using Extra Tree Classifier with feature selection. KNN gave a 52% accuracy on a study by Jerrita et al.  [16]  on 30 subjects. Video clips were used for emotion elicitation. 5 class emotional classification on 25 subjects achieved an accuracy of 56.9% using SVM by Guo H. at al.  [17] . A study conducted by Zhang et al.  [18]  with 4 emotional classes into consideration provided an accuracy of 92%. The best results were provided by a KNN model after feature extraction. 6 classes of emotions were classified with an accuracy of 92.87% by Selvaraj et al.  [19] . Only non-linear features of ECG signal were considered. Fuzzy KNN performed the best. Study by Yang et al.  [20]  utilizes a Bayesian Network on ECG and EEG multimodal sensor data to classify 6 emotion classes with an accuracy of 98.06%.\n\nWe propose an approach in Fig.  1  that follows the traditional machine learning pipeline to improve the emotion recognition performance of 9 emotion classes instead of the 6 emotion classes that are considered in most of the prior works. We have achieved a benchmark accuracy of 92.5% for 9 emotion classes using our proposed approach. Our ECG signal based approach results are reported against the publicly available dataset DREAMER. The signals are acquired using low-cost, wearable sensors which represent the dependability of our proposed methodology in a real-world environment.  We looked at several datasets such as DREAMER, MPED, K-emocon out of which we found DREAMER as the perfect dataset to fit our needs. The features which attracted us to go with DREAMER dataset were\n\n1) The dataset focused on extensive affect recognition.\n\n2) It uses low cost, off-the-shelf devices to measure ECG and EEG signals.\n\n3) The results using the devices was on-par with medical grade devices in terms of affect(emotion) recognition. 4) It aims to integrate emotion recognition with daily life activities which was very well aligned with our research.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "A. Data Acquisition",
      "text": "The DREAMER dataset contains EEG and ECG signal data from 23 volunteers out of which 14 are male and 9 are female.The volunteers were aged between 22-33 years. The data was collected through audio and video stimuli. The study was conducted in a darkened room with a 45\" monitor to play the audio and video stimuli. 18 film clips suggested by Gabert-quillen et al.  [29]  that evoke different emotions from the spectator were played for 65-353 seconds, out of which reactions from the last 60 secs were taken into consideration to accommodate the change in emotions a person goes through. The 18 clips targeted 9 emotions, namely: amusement, excitement, happiness, calmness, anger, disgust, fear, sadness and surprise. Out of the EEG and ECG data obtained, we have chosen to focus on ECG signals. These features will be discussed in the upcoming sections.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Ecg Signal And Features",
      "text": "A SHIMMER wireless sensor was used to record the ECG signals at 256 Hz. It is shown in studies that ECG correlates with the emotional state of a human  [21]    [22] . The Heart rate and heart rate variability have been noted to have an association with the emotional state of a person. The spectral, temporal and frequency parameters of HRV are useful in detecting emotions. These features were derived using Pan-Tompkins QRS detection algorithm  [28] . The algorithm is used to detect the QRS complexes present in the ECG signal. These complexes can further be used to detect the R peaks. The Augsburg Biosignal Toolbox  [23]  was used to get the mean,median , standard deviation from the PQRST complexes. The BioSig Toolbox  [24]  was used to obtain the HRV features for further analysis. These features included RR intervals, RMSSD, pSD, LF,HF, LF/HF. The authors extracted 71 features from the ECG signal. Further,baseline normalization was performed to ensure well distributed data points.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Data Preprocessing And Feature Extraction",
      "text": "The dataset was made in the MATLAB environment, so it was in .mat format. In order to process it we use the Neurokit library. We converted the .mat file into .csv by loading all Neurokit dependencies. After conversion we got 34 features based on Heart Rate Variability. After thorough exploratory data analysis we found that video name wasn't an important feature and we decided to drop it.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Emotion Models",
      "text": "There are two types of emotional models: Discrete Emotional Model and Dimensional Emotional Model. The Discrete Emotional Model assumes that human beings go through a set of basic emotions that are recognizable through expressions and biological processes. These emotions are considered to be a category rather than a stand-alone emotional state. In the Discrete Emotional Model(DEM), emotions are categorized as happiness,fear, anger, disgust, sadness and surprise.\n\nThe Dimensional Emotional Model preconceives emotions experienced by humans by describing them in two dimensional or three dimensional planes. The model also uses valence and arousal to indicate the intensity of the emotion. It suggests that emotional states of humans are affected by a complex, intertwined neurophysiological network.There are various dimensional models suggested. A few popular ones include the Circumplex model  [25] , Vector model, Plutchik's model  [26] , Positive activation-negative activation model  [27] .\n\nThe dataset also contains valence and arousal values which are provided emphasis in the dimensional model. The dataset gathers valence and arousal scores on a scale 1 -5,pertaining to the intensity of the emotion experienced by the volunteer. Valence represents how positive or negative the reaction to the stimuli was. Arousal represents how calm or excited a human is. A valence score of 1 (low valence) indicates a",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Target Variable",
      "text": "Our target variable was the target emotion feature. The task is to classify the emotions into 9 different categories. The emotions under consideration are: Calmness, Surprise, Amusement, Fear, Excitement, Disgust, Happiness, Anger, Sadness. The classes are balanced. Our approach follows the discrete emotional model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Model Description & Hyperparameter Tuning",
      "text": "We used various machine learning models to categorize the emotions according to the ECG signal readings. The models that we used were: K-Nearest Neighbours, Linear SVM, RBF SVM, Decision Trees, Random Forests, XGBoost, LightGBM, AdaBoost. Apart from this we also used a Multilayer Perceptron Classifier with an adaptive learning rate.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Model Training",
      "text": "For training and validation, we have used GroupKfold validation in Scikit learn to ensure that overfitting is combated. GroupKFold ensures that the number of distinct groups is the same in each fold, thereby ensuring class balance. We have used 10 splits and the classification metrics were averaged for all of them at the end of the training procedure. A pipeline was created which first used min-max scaling on the dataset for normalization and then performed classification on the dataset using 11 different classifiers. Following this, the mean accuracy score,precision, recall, F-score and runtime were calculated for the 10 different splits of the dataset.\n\nWe performed hyperparameter tuning on all the machine learning models and observed that the accuracy increased for Random Forest, Decision Trees. The updated results of these can be found in Table  III .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vi. Results And Inferences",
      "text": "The proposed approach calculated the accuracy of multi emotion classification to evaluate the performance of these classifiers. We used 34 HRV features of the dataset. GroupK-Fold cross-validation was used and n splits was set to be 10. The average accuracy for these 10 splits were then identified and tabulated in Table  II . Further to improve the accuracy of the classifiers, we performed hyperparameter tuning on them and noticed that there was an improvement in performance of Decision Trees and Random Forests. The comparison of the accuracy for the hyperparameter tuned models is given in Table  III . Out of all the classifiers, XGBoost and LightGBM performed well to give an accuracy score of 92.5%. Decision Tree gave an accuracy of 92.5% after hyperparameter tuning. Random Forests gave an accuracy score of 92.2%. This signifies that the task of emotion recognition and classification can be performed efficiently using advanced machine learning classifiers such as XGBoost, LightGBM, Decision tree or Random Forests. All the 4 classifiers gave comparable results with a small average runtime.The training curves for these graphs indicate how the model's accuracy progressively increases for the validation set. Support Vector Machines (SVM), which was a popular choice in prior arts, did not give the expected results.In Fig.  5 . we can see the learning curve for SVM multiclass classifier.A reason for the low accuracy could be the number of classes.From Fig.  9 . we can observe that RBF SVM performs better than Linear SVM which is a clear indicator that the data is not linearly separable.This also implicitly addresses the overall underperformance of SVM for our proposed approach. Fig.  6 . indicates how decision tree performs on the training and validation sets. It can be seen that even though the model overfits on the training data, the validation accuracy seems to be increasing with the increasing number of training iterations. It shows that there is some generalization happening. Similarly, LightGBM in Fig.  8 .performs well on the validation set to give an accuracy of 92.5%. LightGBM is a gradient boosting framework based on decision trees which is fast and gives improved accuracy. It can handle large amounts of data, multiclass classification and requires low memory.\n\nThere was a 5 % jump in accuracy for Decision Tree after hyperparameter tuning and a 0.9 % increase in accuracy for random forest.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "The purpose of work was to indicate that emotion recognition can be efficiently performed through the amalgamation of low-cost, off the shelf edge devices and advanced machine learning algorithms with low computational resources. We have managed to achieve the same optimally.The results indicate that our method is effectual in emotion recognition. The applications of this study are vast. Mood monitoring is highly relevant to most professional, educational and medical   with the training data on device. Mobile devices have become computationally powerful and this can be harnessed to enable on-device training on user specific data, thereby ensuring privacy and personalization.Smart edge device manufacturing companies could tap into the potential of federated learning to enable privacy-oriented emotion monitoring at a large scale by enabling data collection through smart watches/bracelets which are eventually connected to a smart phone wherein the data can be used to train the shared model.The shared model would be trained using the collected data.Then the model is downloaded locally to the user's mobile devices. Smart watches are connected to and controlled through user applications in a mobile phone. These applications can be updated with the sensor data from the smart watch. The data collected can be used to train the model locally with the data specific to the user and the downloaded model.The approach would be data agnostic and user centered.Further,generating real-time data using the readily available smart edge devices to identify the effectiveness of our solution in real-time scenarios over long periods of time while experiencing stimuli would be explored.The applications of this would be vast and profound.",
      "page_start": 4,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: that follows the traditional",
      "page": 2
    },
    {
      "caption": "Figure 1: Proposed model",
      "page": 3
    },
    {
      "caption": "Figure 2: PQRST Complex for ECG [30]",
      "page": 3
    },
    {
      "caption": "Figure 3: Data Pre-processing Workﬂow",
      "page": 3
    },
    {
      "caption": "Figure 4: Circumplex model of emotions",
      "page": 4
    },
    {
      "caption": "Figure 5: we can see the learning curve",
      "page": 4
    },
    {
      "caption": "Figure 9: we can observe",
      "page": 4
    },
    {
      "caption": "Figure 6: indicates how decision",
      "page": 4
    },
    {
      "caption": "Figure 8: performs well on the validation set to give an accuracy of",
      "page": 4
    },
    {
      "caption": "Figure 5: SVM training curve",
      "page": 5
    },
    {
      "caption": "Figure 6: Decision Tree training curve",
      "page": 5
    },
    {
      "caption": "Figure 7: XGBoost Training curve",
      "page": 5
    },
    {
      "caption": "Figure 8: LightGBM Training Curve",
      "page": 5
    },
    {
      "caption": "Figure 9: Graphical Representation of Model Accuracy",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Paper": "Yang et al.\n[20]",
          "Model": "Bayesian Network",
          "Classes": "6",
          "Accuracy(%)": "98.6"
        },
        {
          "Paper": "Zhang et al.\n[18]",
          "Model": "KNN",
          "Classes": "4",
          "Accuracy(%)": "92"
        },
        {
          "Paper": "Selvaraj et al.\n[19]",
          "Model": "KNN",
          "Classes": "6",
          "Accuracy(%)": "92.87"
        },
        {
          "Paper": "Dissanayake et al.\n[15]",
          "Model": "Extra Tree Classiﬁer",
          "Classes": "6",
          "Accuracy(%)": "80"
        },
        {
          "Paper": "Jerrita et al.\n[16]",
          "Model": "KNN",
          "Classes": "6",
          "Accuracy(%)": "52"
        },
        {
          "Paper": "Guo H et al.\n[17]",
          "Model": "SVM",
          "Classes": "5",
          "Accuracy(%)": "56.9"
        },
        {
          "Paper": "Proposed Technique",
          "Model": "LightGBM",
          "Classes": "9",
          "Accuracy(%)": "92.5"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Nearest Neighbours",
          "Accuracy (%)": "43.33",
          "Precision": "46.17",
          "Recall": "43.3",
          "F-score": "40.16"
        },
        {
          "Model": "Linear SVM",
          "Accuracy (%)": "26.67",
          "Precision": "22.64",
          "Recall": "26.67",
          "F-score": "21.38"
        },
        {
          "Model": "RBF SVM",
          "Accuracy (%)": "55.28",
          "Precision": "65.84",
          "Recall": "55.28",
          "F-score": "54.19"
        },
        {
          "Model": "Gaussian Process",
          "Accuracy (%)": "36.11",
          "Precision": "28.04",
          "Recall": "36.11",
          "F-score": "30.79"
        },
        {
          "Model": "Decision Tree",
          "Accuracy (%)": "87.50",
          "Precision": "88.5",
          "Recall": "87.5",
          "F-score": "87.14"
        },
        {
          "Model": "Random Forest",
          "Accuracy (%)": "91.39",
          "Precision": "91.00",
          "Recall": "91.39",
          "F-score": "91.11"
        },
        {
          "Model": "Neural Net",
          "Accuracy (%)": "41.11",
          "Precision": "41.1",
          "Recall": "41.11",
          "F-score": "36.28"
        },
        {
          "Model": "AdaBoost",
          "Accuracy (%)": "22.2",
          "Precision": "11.57",
          "Recall": "22.22",
          "F-score": "10.85"
        },
        {
          "Model": "Naive Bayes",
          "Accuracy (%)": "25.0",
          "Precision": "20.99",
          "Recall": "25.0",
          "F-score": "19.45"
        },
        {
          "Model": "XGBClassiﬁer",
          "Accuracy (%)": "92.5",
          "Precision": "91.93",
          "Recall": "92.5",
          "F-score": "92.32"
        },
        {
          "Model": "LightGBM",
          "Accuracy (%)": "92.5",
          "Precision": "91.81",
          "Recall": "92.5",
          "F-score": "91.98"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "NeuroKit2: A Python toolbox for neurophysiological signal processing",
      "authors": [
        "D Makowski",
        "T Pham",
        "Z Lau",
        "J Brammer",
        "F Lespinasse",
        "H Pham",
        "C Schölzel",
        "S Chen"
      ],
      "year": "2021",
      "venue": "Behavior Research Methods",
      "doi": "10.3758/s13428-020-01516-y"
    },
    {
      "citation_id": "2",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "1995",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Facial signs of emotional experience",
      "authors": [
        "P Ekman",
        "W Freisen",
        "S Ancoli"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/h0077722"
    },
    {
      "citation_id": "4",
      "title": "Self-Assessment Manikin",
      "authors": [
        "Teah-Marie & Bynion",
        "Matthew Feldner"
      ],
      "year": "2017",
      "venue": "Encyclopedia of Personality and Individual Differences",
      "doi": "10.1007/978-3-319-28099-8"
    },
    {
      "citation_id": "5",
      "title": "Measuring emotion: The selfassessment manikin and the semantic differential",
      "authors": [
        "Margaret Bradley",
        "Peter Lang"
      ],
      "year": "1994",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry",
      "doi": "10.1016/0005-7916(94)90063-9"
    },
    {
      "citation_id": "6",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals from Wireless Low-cost Off-the-Shelf Devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/JBHI.2017.2688239"
    },
    {
      "citation_id": "7",
      "title": "MPED: A Multi-Modal Physiological Emotion Database for Discrete Emotion Recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2891579"
    },
    {
      "citation_id": "8",
      "title": "A Multimodal Database for Affect Recognition and Implicit Tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.25"
    },
    {
      "citation_id": "9",
      "title": "K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations",
      "authors": [
        "Cheul Park",
        "Young",
        "Narae Cha",
        "Soowon Kang",
        "Auk Kim",
        "Ahsan Khandoker",
        "Leontios Hadjileontiadis",
        "Alice Oh",
        "Yong Jeong",
        "Uichin Lee"
      ],
      "year": "2020",
      "venue": "K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations",
      "doi": "293.10.1038/s41597-020-00630-y"
    },
    {
      "citation_id": "10",
      "title": "AMIGOS: A Dataset for Affect, Personality and Mood Research on Individuals and Groups",
      "authors": [
        "J Miranda-Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2018.2884461"
    },
    {
      "citation_id": "11",
      "title": "A Sequential Procedure for Individual Identity Verification Using ECG",
      "authors": [
        "John Irvine",
        "Steven Israel"
      ],
      "year": "2009",
      "venue": "EURASIP Journal on Advances in Signal Processing",
      "doi": "10.1155/2009/243215"
    },
    {
      "citation_id": "12",
      "title": "Analysis of Electrocardiogram (ECG) Signals for Human Emotional Stress Classification",
      "authors": [
        "S Bong",
        "M Murugappan",
        "S Yaacob"
      ],
      "year": "2012",
      "venue": "Trends in Intelligent Robotics, Automation, and Manufacturing. IRAM 2012. Communications in Computer and Information Science",
      "doi": "10.1007/978-3-642-35197-622"
    },
    {
      "citation_id": "13",
      "title": "",
      "authors": [
        "P Ekman",
        "John Basic Emotions",
        "Wiley",
        "Ltd Sons"
      ],
      "year": "1999",
      "venue": ""
    },
    {
      "citation_id": "14",
      "title": "Waghurdekar Smart music player integrating facial emotion recognition and music mood recommendation",
      "authors": [
        "S Gilda",
        "H Zafar",
        "C Soni"
      ],
      "year": "2017",
      "venue": "Proc. 2017 Int. Conf. Wirel. Commun. Signal process. Networking, WiSPNET"
    },
    {
      "citation_id": "15",
      "title": "An ensemble learning approach for electrocardiogram sensor based human emotion recognition",
      "authors": [
        "T Dissanayake",
        "Y Rajapaksha",
        "R Ragel",
        "I Nawinne"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "16",
      "title": "Electrocardiogrambased emotion recognition system using empirical mode decomposition and discrete Fourier transform",
      "authors": [
        "S Jerritta",
        "M Murugappan",
        "K Wan",
        "S Yaacob"
      ],
      "year": "2014",
      "venue": "Expert Syst"
    },
    {
      "citation_id": "17",
      "title": "Heart Rate Variability Signal Features for Emotion Recognition by Using Principal Component Analysis and Support Vectors Machine",
      "authors": [
        "H Guo",
        "Y Huang",
        "C Lin",
        "J Chien",
        "K Haraikawa",
        "J Shieh"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 IEEE 16th International Conference on Bioinformatics and Bioengineering (BIBE)"
    },
    {
      "citation_id": "18",
      "title": "Research on emotion recognition based on ECG signal",
      "authors": [
        "Z Zhang",
        "X Wang",
        "P Li",
        "X Chen",
        "L Shao"
      ],
      "year": "1678",
      "venue": "J. Phys. Conf. Ser"
    },
    {
      "citation_id": "19",
      "title": "Classification of emotional states from electrocardiogram signals: A non-linear approach based on hurst",
      "authors": [
        "J Selvaraj",
        "M Murugappan",
        "K Wan",
        "S Yaacob"
      ],
      "year": "2013",
      "venue": "BioMed. Eng. Online"
    },
    {
      "citation_id": "20",
      "title": "A Convolution Neural Network Based Emotion Recognition System using Multimodal Physiological Signals",
      "authors": [
        "C Yang",
        "N Fahier",
        "W Li",
        "W Fang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 IEEE International Conference on Consumer Electronics"
    },
    {
      "citation_id": "21",
      "title": "Using Noninvasive Wearable Computers to Recognize Human Emotions from Physiological Signals",
      "authors": [
        "C Lisetti",
        "F Nasoz"
      ],
      "year": "2004",
      "venue": "EURASIP J. Adv. Signal Process",
      "doi": "10.1155/S1110865704406192"
    },
    {
      "citation_id": "22",
      "title": "Looking at pictures: Affective, facial, visceral, and behavioral reactions",
      "authors": [
        "P Lang",
        "M Greenwald",
        "M Bradley",
        "A Hamm"
      ],
      "year": "1993",
      "venue": "Psychophysiology",
      "doi": "10.1111/j.1469-8986.1993.tb03352.x"
    },
    {
      "citation_id": "23",
      "title": "From physiological signals to emotions: implementing and comparing selected methods for feature extraction and classification",
      "authors": [
        "J Wagner",
        "K Jonghwa",
        "E Andre"
      ],
      "year": "2005",
      "venue": "IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "24",
      "title": "Alois; BioSig: The Free and Open Source Software Library for Biomedical Signal Processing",
      "authors": [
        "Carmen Vidaurre",
        "Tilmann Sander",
        "Schlögl"
      ],
      "year": "2011",
      "venue": "Alois; BioSig: The Free and Open Source Software Library for Biomedical Signal Processing",
      "doi": "10.1155/2011/935364"
    },
    {
      "citation_id": "25",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/h0077714"
    },
    {
      "citation_id": "26",
      "title": "Emotion and Life: perspective from psychology biology and evolution",
      "authors": [
        "R Plutchik"
      ],
      "year": "2003",
      "venue": "Emotion and Life: perspective from psychology biology and evolution"
    },
    {
      "citation_id": "27",
      "title": "The two general activation systems of affect: Structural findings, evolutionary considerations, and psychobiological evidence",
      "authors": [
        "D Watson",
        "D Wiese",
        "J Vaidya",
        "A Tellegen"
      ],
      "year": "1999",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/0022-3514.76.5.820"
    },
    {
      "citation_id": "28",
      "title": "A Real-Time QRS Detection Algorithm",
      "authors": [
        "J Pan",
        "W Tompkins"
      ],
      "year": "1985",
      "venue": "IEEE Transactions on Biomedical Engineering",
      "doi": "10.1109/TBME.1985.325532"
    },
    {
      "citation_id": "29",
      "title": "Ratings for emotion film clips",
      "authors": [
        "Crystal Gabert-Quillen",
        "Ellen Bartolini",
        "Benjamin Swerdlow",
        "Charles Sanislow"
      ],
      "venue": "Ratings for emotion film clips",
      "doi": "10.3758/s13428-014-0500-0"
    },
    {
      "citation_id": "30",
      "title": "A Neural Network-Based ECG Classification Processor With Exploitation of Heartbeat Similarity -Scientific Figure on ResearchGate",
      "venue": "A Neural Network-Based ECG Classification Processor With Exploitation of Heartbeat Similarity -Scientific Figure on ResearchGate"
    }
  ]
}