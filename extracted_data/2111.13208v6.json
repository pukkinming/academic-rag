{
  "paper_id": "2111.13208v6",
  "title": "Evaluation Of Interpretability For Deep Learning Algorithms In Eeg Emotion Recognition: A Case Study In Autism",
  "published": "2021-11-25T18:28:29Z",
  "authors": [
    "Juan Manuel Mayor-Torres",
    "Sara Medina-DeVilliers",
    "Tessa Clarkson",
    "Matthew D. Lerner",
    "Giuseppe Riccardi"
  ],
  "keywords": [
    "Convolutional Neural Networks (CNN)",
    "Explainable AI (XAI)",
    "re-training",
    "RemOve-And-Retrain (ROAR)",
    "Electroencephalography (EEG)",
    "Autism Spectrum Disorder",
    "Autism",
    "XAI methods",
    "Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Current models on Explainable Artificial Intelligence (XAI) have shown an evident and quantified lack of reliability for measuring feature-relevance when statistically entangled features are proposed for training deep classifiers. There has been an increase in the application of Deep Learning in clinical trials to predict early diagnosis of neuro-developmental disorders, such as Autism Spectrum Disorder (ASD). However, the inclusion of more reliable saliency-maps to obtain more trustworthy and interpretable metrics using neural activity features is still insufficiently mature for practical applications in diagnostics or clinical trials. Moreover, in ASD research the inclusion of deep classifiers that use neural measures to predict viewed facial emotions is relatively unexplored. Therefore, in this study we propose the evaluation of a Convolutional Neural Network (CNN) for electroencephalography (EEG)-based facial emotion recognition decoding complemented with a novel RemOve-And-Retrain (ROAR) methodology to recover highly relevant features used in the classifier. Specifically, we compare well-known relevance maps such as Layer-Wise Relevance Propagation (LRP), PatternNet, Pattern-Attribution, and Smooth-Grad Squared. This study is the first to consolidate a more transparent feature-relevance calculation for a successful EEG-based facial emotion recognition using a within-subject-trained CNN in typically-developed and ASD individuals.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "D EEP Learning (DL) has led to large improvements in many domains: e.g. image recognition  [1] , automated translation  [2] , and object detection  [3] . In recent years, researchers have started to investigate DL for clinical application. For example: the use of DL for the diagnosis of complex neurodevelopmental disorders, such as, Parkinson  [4] , Rett  [5] , or Alzheimer  [6] . Other uses include facial emotion recognition (FER) in typically-developed (TD) individuals  [7] ,  [8] ,  [9] ,  [10]  and individuals with ASD  [11] . Despite this, most research on processing electroencephalography (EEG) data still relies on more traditional machine approaches such as Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA). This includes the work on ASD diagnoses  [12] ,  [13]  and FER  [14] ,  [15] ,  [16] ,  [17] . These methods were quite successful in multiple applications such as motor imagery decoding  [18]  and artifact removal  [19] . However, their accuracy is limited for more complex applications such as FER decoding.\n\nTraditional classifiers necessitate explicit feature extraction prior to classification in order to achieve high accuracy  [20] ,  [21] ,  [22] . In the aforementioned case of motor imagery decoding, features were generated using Common-Spatial Patterns (CSP) which is directly linked to neurophysiological processes  [23] . Unfortunately, in the case of FER decoding relevant features are not known or directly linked to neurophysiological data  [24] ,  [25] ,  [26] . This limitation motivates use of DL, such as CNNs, to process minimally preprocessed EEG data and recover features  [27] ,  [28] .\n\nTo our knowledge, our previous study is the first successful application of DL classifiers on neural activity to decode facial emotions in ASD populations  [11] . In this study, we constructed a 2D image from the EEG as an input by stacking the individual EEG channels vertically -columns represent time and rows represent different EEG channels so that a single EEG image can be processed directly by a CNN  [11] ,  [28] . One could think of these EEG images as scribble drawings made by a child who is learning to draw.\n\nEach scribble drawing represents a different encoded facial emotion in the neural activity or a single-trial. In other words, the CNN would need to find the best class separability using the features that are specific to each scribble drawing or facial emotion. What makes this task particularly difficult is that the representations are highly non-deterministic and noisy, just as the scribble drawings from a child. Consequently, understanding which features were used to classify facial emotions for typically developed (TD) controls or ASD participants performing FER  [11] ,  [29]  is difficult. This contributes to the CNN being an inaccessible black-box system, which can not be explained directly from the neural features or the DL models we commonly use  [30] .\n\nTo alleviate this problem, Explainable AI (XAI) methods (i.e, saliency-maps) have been introduced recently as a way to understand DL classifiers. Some well-know XAI methods, such as Grad-CAM  [31] , Grad-CAM++  [32] , Integrated Gra-dients (IG)  [33] , and Smooth-Grad  [34] , were developed for image classification and semantic segmentation  [3] ,  [25] . It is not clear how applicable these methods are to decoding emotions from EEG images  [35] ,  [36] . A key difference between image classification and EEG image decoding is, as mentioned before, the presence of noise. For these reasons it is not clear which XAI methods are best suited for recovering features of the decoded EEG image.\n\nIn this study, we utilize a CNN to classify emotions FER using neural activity in the form of an EEG image. Our previous work demonstrated that a CNN was able to decode neural activity during an FER task in TD and ASD individuals, suggesting intact FER encoding. The discrepancy between encoded FER and behavioral performance on the FER task in ASD suggests that impairments arise as a result of problems deploying properly encoding facial emotion information into behavioral responses  [11] ,  [29] . To better understand when and how FER is encoded from the neural activity in TD and ASD, we can use XAI methods to recover relevant features.\n\nTo recover what EEG features are necessary to obtain an accurate facial emotion classification, we must first understand which XAI methods are reliable. With this goal, we analyze the following methods: Layer-Wise Relevance Propagation (LRP)  [37] ,  [38] , PatternNet, Pattern-Attribution  [39] , and Smooth-Grad Squared  [40]  using an approach called RemOve-And Retrain (ROAR)  [41] . ROAR works by systematically removing features, indicated to be informative according to the XAI methods, one a time from the CNN and obtaining classifier accuracies without that features. If after their feature-removal the classifier cannot obtain a high categorization accuracy, then the feature identified by XAI methods is indeed informative, reliable and necessary for decoding. This approach allows us to recover which XAI methods are definitely reliable for classifying correct facial emotion using features from neural activity.\n\nThis study is the first to evaluate reliable XAI methods using ROAR, including EEG data from TD and ASD groups, and comparing the final CNN-FER metrics with the metrics associated with the FER (behavioral performance) task. This paper will be structured as follows:  (1)  In the first part of the methods section, we will describe the demographics of the TD and ASD samples, the EEG preprocessing methods used for artifact-removal and signal processing, and the CNN architecture and training. (2) In the second part of the methods section, we will describe ROAR and discuss the XAI methods we use in the ROAR evaluation. (3) In the results section, we will report the comparisons between the different XAI methods, and (4) finally, we will discuss our conclusions in the context of ASD, and Machine Learning (ML) research. This study will provide important contributions for evaluating XAI methods using ROAR, and its application for decoding FER in individuals with and without ASD  [42] ,  [43] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Materials And Methods",
      "text": "In the first subsection, we describe the participant samples and the corresponding demographics. In the second subsection, we describe that the FER task is completed while undergoing EEG. In the third subsection, we describe the EEG data pre-processing, artifact removal, and whitening using Zero-Component-Analysis (ZCA) procedures implemented prior to training the CNN. In the fourth subsection, we outline the ROAR methodology and the XAI methods for feature evaluation via feature-relevance calculation.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Participants",
      "text": "Eighty-eight participants (Age: 15.34±1.58 years), were included in the following analyses and taken from a larger study on emotional and social processing  [11] ,  [44] . Of these, 48 participants (29 male; Age: 15.39±1.55 years) were TD, and 40 participants (32 male; Age: 14.77±2.16 years) had a diagnosis of ASD. All participants with an ASD diagnosis were confirmed using the Autism Diagnostic Observation Schedule, Version 2.0 (ADOS-2; ASD severity Comparison Score ADOS-CS: TD 3.33±2.71, ASD 8.15±2.05)  [45]  and were considered high-functioning on the Kaufman Brief Intelligence Test-2 (KBIT-2). ASD participants had significantly elevated ADOS-CS compared to the TD group (p=0.038), but there was no significant difference in intellectual functioning (p=0.227).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Face Emotion Recognition (Fer)",
      "text": "Participants completed a web-based FER task while undergoing EEG  [29] ,  [46] . During this task; participants viewed emotional facial expressions from 48 children and adults' face photographs. These facial images were taken from the DANVA-2 image-set  [44] . We presented the emotional faces randomly with a cross-fixation of 200 ms, and a 2second length face presentation followed by an emotion labeling menu. The emotion labeling menu presented the face-photograph again with four basic emotion labels on the bottom (i.e., happy, sad, anger, and fear).\n\nFrom the DANVA-2 image-set, 24 faces showed adults portraying a particular emotion, and the other 24 showed children portraying the same emotion, resulting in six stimuli for each emotion and for each modality (i.e, children/adult). Each EEG trial is associated with a single facial emotion stimulus presented per subject. For this study, we consider the performances obtained from this task as a human behavioral performance quota -see section 2.6 for a more detailed description of these metrics. Each participant's behavioral performance was calculated after all 48 faces were presented. The behavioral performance metrics include error-rates, accuracies, and reaction-times per participant.\n\nEEG trials were sorted by facial emotion, combined and stored as a single EEGlab  [47]  structure for each emotion, resulting in 4 EEGlab structures (happy, sad, anger, and fear) per participant, comprising 12 EEG trials of both adults and children stimuli.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Eeg Recordings",
      "text": "EEG neural activity was recorded using a Brain Products 32channels Brain-Vision ActiCHAmp device with an original sample rate at 1KHz. Each 2D EEG image was then composed of 752 points on the time-domain and 30 channels in the spatial domain. EEG data were digitized at 16-bit resolution. The raw EEG signal was filtered with a notch filter at 60Hz with a half-power cut-off of 12db/Oct. Each active electrode was measured online with respect to a common mode-sense active electrode producing a monopolar (nondifferential) channel. The EEG data collection procedures of this study adhered to best practices for EEG data collection in ASD  [48] .\n\nEEG trials were segmented between -200-1500ms and downsampled to 500Hz to avoid classifier overfitting  [18] ,  [26] . Each EEG trial was re-referenced to the T9-T10 bilateral reference  [48] . After the re-reference process, we used the remaining 30 channels (i.e., FT9, F7, FC5, FP1, FZ, FP2, F4, F8, FC6, FT10, F4, F3, FC1, C3, FC1, FC2, C4, T7, CP5, CP1, Cz, CP2, P4, P8, CP6, T8, P7, P3, Pz, O1, O2, and Oz) to create an EEG image composed of the 30 channels × 752 time-points for each trial.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Eeg Pre-Processing",
      "text": "Each EEG trial was automatically cleaned using the following processes in sequence: 1) the Koethe's cleanraw Artifact Subspace Reconstruction (ASR) in Prep pipeline complemented by the Makoto's pipeline  [49]  for bad channels removal, and 2) the ADJUST EEGlab plugin for blinking and movement artifact removal using the 2 electro-occulogram (EOG) channels  [50] .\n\nThese processes were applied with the purpose of excluding corrupted or distorted channels with artifacts, signal dropout, and electrode malfunction before using AD-JUST. The ADJUST plugin uses spatial and temporal features such as temporal kurtosis, spatial average-difference, maximum epoch variance, and generic discontinuities for EEG spatial features to detect horizontal or vertical eye blinking artifacts from independent-components (ICs)  [51] . The resulting EEG artifact-free trials were baseline corrected -200 ms, prior to stimuli onset (0ms) using a linear detrending described in  [11] , then re-segmented for feature extraction to 0-1500ms.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Zca Whitening Transformation",
      "text": "The Zero Components Analysis (ZCA) is a whitening transformation used to normalize the images amplitude using a Zero Phase Mahalanobis Distance criterion  [52] , without changing the correlation between the feature domains in the resulting covariance matrix.\n\nThe artifact-free input EEG image is denoted as x in the following analyses. The covariance matrix associated to the input x, denoted as S x , is calculated following S x = V DV T where V is the eigen-vectors matrix of x and D is the diagonal matrix to construct the eigenvalue decomposition of x. Thus, the new whitened-image X zca is calculated following Equation 1 controlling the level of output contrast using zca . For our specific facial emotion decoding pipeline we use a low contrast of zca = 0.01.\n\nThe resulting X zca has the same size of input x. The new image X zca represents a non-rotated (i.e., zero-phase feature-space) whitened image to efficiently feed (CNN)based pipelines  [53] ,  [54] . We used this new ZCA image X zca for training the CNN classifier on a single-trial or image level. Following this approach we could obtain a better separability than only using x  [11] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cnn Architecture And Training",
      "text": "The complete pipeline for the proposed EEG-based facial emotion decoding on individuals with and without ASD we used here is described in  [11] , and it is described graphically in the supplementary material -Figure S.1. In this subsection we will describe the CNN architecture including: the classifier convolutional-pooling (conv-pool) blocks, parameters, dimensions, and training methods including: initialization, learning rates, and stopping criteria. Our motivation for using the (CNN)-based architecture was based on previous studies which used three normalized conv-pool blocks connected to a fully-connected (FC) layer and a final decision layer  [11] ,  [28] . These type-of networks are suitable CNN architectures for the specific amount of trials we include in this study and to avoid overfitting effects. As in Schirrmeister et al.  [28] , we constructed our CNN with three conv-pool blocks -going from high-tolow kernel dimensionality, and from low-to-high filters per layer. Specifically, we set the kernel dimensions per layer based on the size of X zca , in a rectangular 30 channels × 752 time-points image, to make the conv-pool blocks more rectangular than the typical CNN architectures used for image categorization  [1] ,  [25] .\n\nThe first conv-pool block was composed of a convolutional layer with a kernel-size of 100×10 and 32 filters, and a subsequent max-pooling layer with a size of 5 × 2 connected to an amplitude normalization layer. The second conv-pool block was composed of a convolutional layer with a kernelsize of 20 × 5, and a max-pooling layer with a size of 2 × 2 units connected to a second amplitude normalization layer. A third conv-pool block was composed of a convlayer with a size of 10 × 2, and a max-pooling layer with a size of 2 × 2 and 128 filters. No batch-level normalization was used. Each conv-pool block had a stride factor of 2 and non-zero-padding. Thus, the output size was half the size of the x and y dimensions -without adding any extra zeros in the image edges. The third max-pooling layer on the last conv-pool block was connected to a dense fullyconnected (FC) layer with 1024 sigmoid units, and this layer was connected to a softmax layer for computing the four classes probabilities associated with a particular emotion (i.e., happy, sad, anger, and fear). This last conv-pool block did not have a normalization layer connected before the softmax layer.\n\nThe training method was based on the Adam optimization  [55] . We set an initial learning-rate equaling 0.00001 with a linear weight decay of 0.000001 per iteration. To assure a faster convergence in the training process. we used the Glorot's initializer for the kernel weights and for all the convolutional and max-pool layers  [56] . For the biases initialization, we use an uniform random distribution across all the layers with µ = 0 and σ = 0.1. No random or heuristic search was used to set the initial learning rate or the decay rates. We used a dropout layer with p = 0.25 applied to the FC layer. All the conv-pool activation-functions were Rectified Linear Unit (ReLU) and trained with 4 size minibatches -changing the training indexes randomly on each iteration. A maximum of 500 iterations was set as part of the training process. We used the early-stopping criterion described in  [28]  and 72.5% of trials across all participants TABLE 1: Mean and std for Face Emotion Recogniton (FER) or human performance, and CNN (machine) performances metrics for the eighty-eight participants on this study . The results are computed averaging the Accuracy (Acc), precision (Pre), Recall (Re), and F1 score (F1) from all the confusion matrices constructed per participant. For ASD group comparing the metrics across FER and CNN modalities we found always significant differences p < 0.05*. fell into this early stop criterion requiring less than 200 iterations -73.31% TD and 74.65% ASD. The (CNN)-based pipeline performance was evaluated using a Leave-One-Trial-Out (LOTO) cross-validation for each participant. This means that the performance was measured iterating intra-subjectly across all the 48 EEG trials, per participant, using 47 for train and 1 for testing. The Accuracy (Acc), precision (Pre), Recall (Re), and F1 scores reported on Table  1  are calculated using a wrapped (macro) confusion matrix calculated for each participant obtained in the LOTO cross-validation. We used and reported the same metrics in our previous study  [11] . The evaluation of the metrics for the human behavioral performance on the FER task consists of the same approach explained aboveassuming each participant has an equal level of entropy as observed in the 47 trials used for training the CNN. These performance metrics using the LOTO cross validation provide a personalized neural representation for facial emotion decoding in individuals with and without ASD  [11] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Metrics",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Xai Methods",
      "text": "We used four XAI methods for estimating featureimportance levels on the trained CNN. We used the Smooth-Grad  [34]  method as a baseline and three other XAI saliencymaps: Smooth-Grad Squared  [40]  -a simple numerical variation of Smooth-Grad, PatternNet, Pattern-Attribution  [57] , and the Layer-Wise Relevance Propagation (LRP)  [37] . These XAI methods are included in the iNNvestigate software package  [58] , a Python module we used for evaluating each XAI method proposed in this study.\n\nFor the subsequent analyses we define a feature relevance-map based on the LRP model  [36] ,  [38] , and for all the XAI methods analyzed here as R 1 q . These relevance values are normalized in amplitude, limiting the propagated relevance between [-1, 1]. Thus, the R 1 q ≥0 values are considered relevant -or a positive contribution to successful facial emotion decoding. All the features associated with positive values represent a hit or an accurate CNN facial emotion decoding. On the other hand, the R 1 q <0 values are considered irrelevant or a negative contribution for a successful facial emotion decoding.\n\nTo obtain a final average relevance measure for each facial emotion, we average the resulting relevance map for each iteration -across the iterations of the LOTO -per subject-cross-validation  [11] . Figure  1  shows the average relevance maps for TD and ASD, and for all the XAI methods. In the following subsections, we describe the detailed models of each XAI method included in this study for statistical evaluation and ROAR.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Smooth-Grad And Smooth-Grad Squared",
      "text": "Smooth-Grad is a XAI method proposed by Smilkov et al.  [34] . In the basic form they start out by computing the gradient of logit (unit) i w.r.t. to the input, denoted as g(x) = ∂f (x)i ∂x . Typically this results in noise saliencymaps which can be mitigated by averaging the gradients of multiple noisy versions of the input, thereby improving the signal to noise ratio.\n\nThe hyper parameters for this method are the number of samples N we use, the mean -typically zero -and variance 2 of the Gaussian noise. From Equation  2 , it should be clear that this method can in principle be applied to any XAI method and not just the gradients. Recent studies  [35] ,  [41]  used a variation called Smooth-Grad squared. Here the gradients are squared before averaging as shown in Equation  3 .\n\nPreviously, this approach performed better than the original Smooth-Grad implementation for vision tasks  [40] ,  [41] .\n\nThe relevance maps for TD and ASD groups calculated using the Smooth-Grad and the Smooth-Grad Squared are shown from subfigure 1a to 1b -in Figure  1 . We consider SmoothGrad as a baseline because of its simplicity and its broad usage on other XAI studies  [35] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Layer-Wise Relevance Propagation",
      "text": "Layer-Wise Relevance Propagation (LRP)  [37] ,  [59]  is a family of XAI methods based on the Lebesgue energyconservation law  [60] . The quantity observed at the logit i is seen as the relevance for a certain class and in each layer of the network we assume that the same amount of relevance is present for this class. Equation 4 makes this explicit for a multiple-layer network. The relevance observed at the logit is equal to f (x) i . This is then distributed towards the input in a layerwise manner such that relevance is preserved through the layers. The relevance at neuron d in layer l is denoted as R l d in LRP.\n\nLRP methods have variations of this process that are optimized for specific tasks. In this study, we used LRP preset B (LRP-B) as implemented in the iNNvestigate toolbox. This LRP configuration makes use of the epsilon LRP rule for dense layers and the alpha-beta rule for convolutional layers.\n\nThe epsilon rule is based on the z-rule where the relevance is proportional to the weight multiplied neuron con- tribution denoted as\n\n. These contributions are then normalized as seen below.\n\nThis normalization can become problematic if the denumerator/denominator is (close to) zero. In this case, a very small value epsilon is added (or subtracted to keep the sign constant) to ensure numerical stability. For the convolutional layers the α-β rule is used.\n\nUsing this approach with the alpha-beta rule includes parameters α and β that controls how much weight is given to the positive relevance components (z + ij ), and negative relevance components (z - ij ). While the iNNvestigate package  [58]  includes many variants of the LRP rules as presets, we use preset B as described above with α = 2 and β = 1.\n\nPreset A with α = 1 and β = 0. was also evaluated, but was less significant that preset B on our data and therefore is not included  [11] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Patternnet And Pattern-Attribution",
      "text": "PatternNet and Pattern-Attribution XAI salience methods have previously been described by P.J Kindermans et al.  [57] . These methods can be seen as an extension of the LRP or Deep-Taylor Decomposition framework. PatternNet and Pattern-Attribution consider which parts of the input a neuron is invariant to and which parts it is trying to detect. By doing this, the explanation of a single artificial neuron without the non-linearity is consistent with the explanation of multivariate linear models in neuroimaging  [61] . PatterNet and Pattern-Attribution do still rely on layerwise propagation from LRP to combine individual neuron-wise explanations to whole network explanations.\n\nIn Pattern-Attribution and PatternNet XAI methods the input x to a neuron is assumed to be composed of an informative signal component and a non-informative distractor component from the output of that neuron y.\n\nHere can be seen as a noise source that minimizes the distractor so it is essentially ignored. PatternNet and Patter-Attribution operate under the assumption that the linear model explanation should not change if is zero. Therefore, the estimation of a s is necessary to propagate according to the signal component. PatternNet and Pattern-Attribution propose Equation  7 to find a set of new filter weights ω that are separated as much as possible from the network weights ωin order to maximize the signal component across the layers of the trained CNN following y l = ωT x l y l is the output of the layer l and x l the corresponding input associated with the filter weights ω.\n\nFor PatternNet and Pattern-Attribution the calculation of the distractor d assumes the following equivalences: y l = ωT x l and ωT d = 0. We obtain this latter equivalence deriving Equation 7 in terms of ω and optimizing. Thus, to compute a new signal estimator that isolates the network noise from we can define S a (x) = a d ωT x. This new estimator increases the correlation between x and s in a new term denoted as ρ. This correlation can be calculated with the estimation of the input distribution of x -denoted as u on each subsequent layer, and considering ωT d = 0 during the relevance propagation across all those layers after training. ρ is then defined in Equation  8 -assuming all the noise will be reflected in the estimated variance σ u,d = σ y .\n\nIn order to assure that ωT d = 0 will occur during the relevance propagation, the covariance between the distractor d and the layer output y must be zero or as close as possible to zero cov(d, y). In this manner, the new signal estimator S a (x), or for simplicity reasons a, can be obtained assuming that cov(x, y) = cov(S a (x), y). Thus, assuming that the learning rates are independent of the interaction between x and y on each layer, we can obtain cov(x, y) = acov(y, y)\n\nand the definitive signal estimator a is defined in Equation  9 .\n\nE is the expected value for any single or joint variable xy described in Equation  9 . The propagation of the estimator a is the main difference between PatternNet and Pattern-Attribution. Both methods use the same model described on Equation  9 to calculate the estimator per layer. However, PatternNet uses a similar propagation as the LRP Deep-Taylor model  [37] ,  [38]  based on the \"message passing\" modality -without propagating the estimator using information from ω. Pattern-Attribution method uses the numerical incidence of the filter weights ωT which are estimated through ω. This method propagates a part of ω using a product between the filter weights and the signal estimator ωT a, instead of only a  [57] . This last consideration of filter weights supports a more noisy relevance map calculated from Pattern-Attribution as we can see in Figures  1c  and 1d .\n\nThe top part of each subfigure in Figure  1  are the Average saliency-maps with the channel indexes on the y-axis, and the time-points in ms on the x-axis. The bottom part of each subfigure shows five topo-plots representing the average relevance on five different time ranges, such as 0-500, 250-750, 500-1000, 750-1250, and 1000-1500 ms.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Remove-And-Retrain (Roar) -Certainty Analysis",
      "text": "For evaluating the certainty and reliability of the XAI methods, we used the RemOve-And-RetrAin (ROAR) methodology  [41] . ROAR uses the average relevance map R 1 s for each participant group (i.e., TD and ASD) for each facial emotion as a feature importance indicator. ROAR weights the feature-importance directly from the input feature-space using the relevance values calculated from each XAI method and sorts the values from high-to-low relevance. The ROAR pipeline is reported in Figure S.2 in supplementary material. In ROAR, the average XAI relevance map, per group, R 1 s determines whether features are included in each new training process. Features are suppressed in order of high-to-low relevance. Feature removal is done based on an elementwise product between the average posthoc relevance map obtained from an XAI method and the input EEG image. The product dictates what features will be removed or accepted for the CNN re-training based on a relevance threshold.\n\nPrevious studies have used ROAR to assess the level of certainty of multiple XAI methods in a quantitative way  [64, 65] . These studies compare the level of accuracy detriment associated with the relevant features removal using random relevance patterns as a baseline. We used this same approach to evaluate and compare XAI methods using ROAR and random relevance baselines within each group. To do this, we set various relevance thresholds as a pixel/feature-removal rate r (r=0.1, 0.2, 0.5, 0.7, and 0.9) to generate a binarized-mask, R b , from the average relevancemap R 1 s using Equation  10 .\n\nThe relevance R 1 s is averaged across facial emotions R 1 s = (R happy +R sad +R angry +R f ear )/4 for normalization. The resulting normalized binary-mask dictates which pixels/features are admitted into the re-training. These binary masks have the same channels and time-points indexes and size of X zca . Different binary masks used in this analysis are reported and illustrated in the supplementary material in Figures S.  3",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "And S.4 Including Different Values Of R.",
      "text": "In the following subsection, we will introduce the random baselines we used to compare the final ROAR metrics, and check if the re-training performances are more interpretable using the XAI methods or the random patterns in the feature removal.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Roar Baselines",
      "text": "We include two basic distribution-based binary masks as random baselines for the ROAR evaluation. The first, most uninformed, baseline we used is a common random baseline based on an uniform distribution with a pixel/feature removal set to r=0.5.\n\nThe second baseline we used evaluated time-domain feature relevance per channel. In this baseline, two types of random slices are generated. The first slice is a random pattern based on a 47 × 1 slice covering 47 time-points and a single channel. This pattern allows all features in ROAR, while occluding time-domain slices with a size of 47 time-points × 1 channel. This retains a cohesive set of slices of approximately 20ms around important positive or negative EEG event-related potentials (ERPs) evoked during FER, such as N1, P3, and Late-Positive Potential (LPP)  [62] ,  [63] . The second slicing baseline is identical to the first, but does not the 47×1 slices randomly. Instead, it sorts relevance values based on a XAI method to dictate which feature will be removed. This pattern will be referred to as a method-related slice baseline in the following analyses. Binary-masks are reported in the supplementary material in Figure S.5. As a gold-standard, we expect that the feature removal associated with the more reliable XAI saliencymaps reduces more accuracy than the random baselines. We also expect that the accuracy detriment will be more plausible for higher r values in comparison with lower r values, thus assuring that those removed features are truly relevant.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "This results section is divided in three subsections 1) human behavioral FER and CNN performance results for both the TD and ASD groups 2) ROAR results comparing XAI salience methods relative to random baselines and each other within groups for interpretability of FER encoding, and 3) group differences in XAI relevance maps for each facial emotion or class.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Performances -Fer And Cnn",
      "text": "Table  1  includes the Accuracy (Acc), precision (Pre), Recall (Re), and F1 scores for human behavioral FER and CNN performances. The metrics are formally described in  [64] . Using one-way ANOVA, we found significant differences in Acc (F(1,87)=10.43, p=0.00144), pre (F(1,87)=6.31, p=0.0301), Re (F(1,87)=9.35, p=0.00561), and F1 (F(1,87)=8.66, p=0.0232), between FER and CNN metrics. For all metrics, CNN was more accurate than FER.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Xai Methods -Statistical Comparisons",
      "text": "The five time ranges mentioned above are used for adjusting the p-values of the statistical comparison reported below using a Bonferroni-Holm correction  [65] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Xai Saliency-Maps Comparison",
      "text": "Visual inspection of XAI saliency-maps in Figure  1 , shows similarities in the relevance distribution within the five timeranges mentioned above. To test these statistical similarities, we used the Kolmogorov-Smirnov test (KS-test)  [66] . Comparing the relevances obtained in the Smooth-Grad method and Smooth-Grad Squared method we found similarities across the five time ranges h=1, p≤ 0.001**.\n\nSimilarities were also observed in late-time ranges, such as 750-1250 and 1000-1500 ms, between PatternNet and LRP-B h=1, p≤0.028*, and between PatternNet and Pattern-Attribution, h =1, p≤0.043*. We attributed this to a more noisy relevance pattern obtained from Pattern-Attribution after the marginal values of the weights ωT can also mod- ulate the calculation of the estimator a (see section 2.7.3). There were no other statistical similarities between other XAI methods, h=0, p's>0.05. All these findings are in line with our expectations after training the CNN with this type of EEG image.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Differences Between Td And Asd Relevance Maps",
      "text": "There were no significant differences in the Smooth-Grad,Smooth-Grad average, Smooth-Grad Squared, Pat-ternNet, or Pattern-Attribution XAI salience maps for any specific facial emotion, or the average across facial emotions between TD and ASD groups. For Smooth-Grad we obtained F's(1,87)≤1.334, p's>0.05, across all emotions or any time-range (see section XAI methods). Similar differences were obtained for Smooth-Grad Squared F(1,87)≤0.129 with p>0.05, PatternNet F(1,87)≤0.883, p's>0.05, and Pattern-Attribution F(1,87)≤0.222, p's>0.05.\n\nThe main significant differences found between TD and ASD XAI salience maps are observed in the LRP-B method, specifically for negative emotions, such as Anger and Fear. This is consistent with the observable behavioral deficits in ASD performing FER. We found significant differences for Average in 0-500ms, (F(1,87)=7.889, p=0.0344), TD>ASD, and in 1000-1500ms, (F(1,87)=11.56, p=0.0033), TD<ASD. For Sad in 750-1250ms, (F(1,87)=8.491, p=0.0141), TD>ASD, and in 1000-1500ms, (F(1,87)=13.54, p=0.0005), TD>ASD. For anger in 0-500ms, F(1,87)=10.85, p=0.0095, TD>ASD, and in 1000-1500ms, (F(1,87)=9.667, p=0.0102), TD<ASD. For Fear in 0-500ms, (F(1,87)=23.47, p=7.6E-6), TD>ASD, in 500-1000ms, (F(1,87)=7.193, p=0.0263), TD<ASD, and in [750-1250]ms, (F(1,87)=9.313, p=0.0121), TD<ASD. All individual F and p values for each facial emotion, group, and XAI method are reported in the supplementary material in Table  S .1.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Differences Between Binary Masks",
      "text": "To evaluate the differences between binary masks obtained from all the trained TD and ASD CNNs for each facial emotion we used an One-way ANOVA and Bonferroni-Holm correction over the same five time-ranges. Figure  2a   shows the topo-plots related to the binary masks for the time range between 500-1000ms. In this time range we only found significant differences for r=0. In PatternNet we did not find any significant difference on this early time range. This must be related to the behavioral associated with the CNN, less noisy, and coarser relevance patterns associated with PatternNet.\n\nFigure  2b  shows the topo-plots related to the binary masks for the interval between 750-1250ms. There we found differences for r=0.2 and PatternNet (F(1,87)=6.22, p=0.021). We also found differences for r=0.5 in Smooth-Grad Squared (F(1,87)=3.51, p=0.048), Pattern-Attribution (F(1,87)=3.78, p=0.041), and the LRP-B preset (F(1,87)=3.81, p=0.043). These results suggest that in terms of the r values, the binary mask patterns are consistent. This also shows that binary masks are consistent across r in the late time-ranges -excepting r=0.5. To assure a more consistent comparison between the binary masks of TD and ASD, we evaluate them using the K-S test. For all the significant differences found in this analysis we can surmise that the binary masks all come from the same distribution -h=1, p<0.05. There were no significant differences between 0-500ms for any XAI method.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Accuracy Detriment Differences Across R Values",
      "text": "For comparing the accuracy detriment across the r values of 0, 0.1, 0.2, 0.35, 0.5, 0.7, 0.9, and 1, we used an oneway ANOVA with a subsequent Bonferroni-Holm correction across the r values. The comparison was evaluated and adjusted by grouping the accuracies obtained for each TD and ASD participant described in Table  1 . Figures  3a  and 3b  shows the accuracy detriment for all the XAI methods included in this study for TD and ASD respectively. Similarly, to illustrate the performance detriment obtained from the method-related 47×1 baseline we show the accuracy detriment in Figure  3c  and 3d for TD and ASD. Analyzing the differences between TD and ASD accuracies across the XAI methods we observed significant differences between ROAR-related accuracies and the random baselines. For Smooth-Grad we found significant differences in comparison with the random baseline for r=0.7 (F(1,95)=3.31, p=0.00155) and r=0.9 (F(1,95)=2.65, p=0.0224) in TD. For ASD we found differences in r=0.7 (F(1,79)=4.01, p=0.000331), and r=0.9 (F(179)=2.23, p=0.0338).\n\nAgain for Smooth-Grad we found differences in comparison with the method-related slices in r=0.7 (F(1,95)=10.58,p=1.45e-6), and in r=0.9 (F(1,95)=7.33, p=0.00023) for TD. For ASD we found differences in r=0.7 (F(1,79)=11.11, p=2.28E-7), and in r=0.9 (F(1,79)=7.45, p=0.000148). For this particular method, all the methodrelated slice baseline accuracies were lower in comparison with the SmoothGrad-related performance detriment. Other differences in the accuracy detriment are more observable in low values of r. For instance, for Smooth-Grad Squared and r=0.2 (F(1,95)=10.99, p=0.0000274) for TD, and r=0.2 (F(1,79)=12.45, p=0.0000345) for ASD. The accuracy detriments associated with the Smooth-Grad Squared method-related slice baseline are lower than the XAI method itself. However, we did not find significant differences for TD (F(1,95)≤2.58, p>0.1893) or ASD (F(1,79)≤2.88, p>0.1910) for r values different than 0.2. For PatternNet we only found significant differences in comparison with the method-related slices baseline. For r=0.7 the XAI method always shows a lower accuracy for TD (F(1,95)=13.38, p=0.0001178), and for ASD (F(1,79)=20.45, p=2.77E-7). In r=0.9 we found differences for TD (F(1,95)=3.42, p=0.0267), and for ASD (F(1,79)=2.99, p=0.0321). For other values of r we did not find differences after correction. For Pattern-Attribution we found differences in r=0.2 for TD (F(1,95)=8.35, p=0.00327), and for ASD (F(1,79)=7.91, p=0.00899). We found other differences where the method-related slice baseline is showing a lower accuracy than the XAI method itself, specifically in r=0.5 for TD (F(1,95)=10.12, p=0.000224) and ASD (F(1,79)=9.88, p=0.003367). For other r values we did not find any significant differences.\n\nComparing the ROAR accuracy detriments for the LRP-B preset method, we found significant differences in r=0.2 for TD (F(1,95)=3.56, p=0.00214), and for ASD (F(1,79)=3.66, p=0.00203). An important observation is that the accuracy detriment associated with the LRP-B method is only showing significant differences for ASD in r=0.7 (F(1,79)=3.91, p=0.000156). For other r values we did not find any significant difference -(F(1,95)≤1.99, p>0.2489).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "DL pipelines are effective  [11] ,  [27] ,  [28]  at decoding facial emotions, but it is unclear what neural information is more important in this process. To address this, we compared XAI methods to determine their trustworthiness in this application  [36] . We compared XAI saliency-maps (Figure  1 ) generated from LRP-B, PatternNet, and Pattern-Attribution methods and identified features that are consistent with known patterns observed in EEG during facial emotion decoding for individuals with and without ASD  [62] ,  [67] . Even though the previously-mentioned XAI methods identify features that are consistent with prior knowledge on EEG-based facial emotion decoding  [11] , we observed some quantitative differences between the associated relevance patterns. Because of these differences, it is not clear which method best represents the neural-network facial emotiondecoding process. This motivates the usage of ROAR and random and method-slices baseline for quantifying the level of relevance of each of these XAI methods.ROAR yields a reliable evaluation of what features of the EEG input are used to train the CNN for FER decoding. We observed that LRP, PatternNet and Pattern-Attribution identify the late time-ranges, after 500ms relative to the stimulus onset, as essential for correct facial emotion decoding, which is consistent with previous literature  [29] ,  [42] ,  [68] .\n\nThese analyses demonstrate that XAI methods can be applied to recover features necessary for neural encoding. However, this effect is only observed when less than 50% of the relevant features are removed, as we observed in Figures 8.a and 8.c. While the best performing method differs based on the exact setting, we recommend using the SmoothGrad-Squared approach, since it is the easiest to implement, before using more complicated methods.\n\nThe results discussed above have shown that the patterns found by the XAI methods are meaningful. For Pat-ternNet, Pattern-Attribution, Smooth-Grad Squared, and LRP-B when r=0.5 we see significant differences in timeranges later than 500 ms. These differences are consistent with the late activation neural component in ASD when performing FER  [46] . The three time-ranges that were essential for facial emotion decoding in ASD were centered at 250, 600, and 1100 ms. These ranges were not essential for facial emotion decoding in TD. Instead, in TD subjects only two time ranges were essential for facial encoding -centered at 250, and 1200 ms. This suggests that EEG facial emotion decoding is done differently between groups, which is consistent with other EEG studies  [9] ,  [29] . The success of the CNN as a fully-personalized classifier -based on a LOTO cross-validation per subject-is an important novelty including XAI methods evaluation. This evaluation shows that it is possible to decode emotions reliably from the EEG of ASD populations when they perform FER. This confirms that there is intact emotion information encoding in ASD from the EEG single trial/image level  [69] , understanding fully-personalized neural representations using XAI methods might be useful to develop more efficient datadriven ASD interventions  [70] .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "The certainty analysis employed in this study suggests that the correctly decoded emotions/trials on our proposed EEG-based CNN pipeline are associated with relevance patterns that show high-relevance values on late time-ranges. In time-ranges between 500-1500ms after the stimulus onset we find significant differences associated with the relevance patterns and the ROAR binary masks obtained from some XAI methods, such as, LRP-B, PatternNet and Pattern-Attribution. This effect is also observed when evaluating ROAR on those methods, thus suggesting that the EEG relevant features assessed with ROAR are particularly useful for obtaining a successful emotion recognition.\n\nThe differences in accuracy detriment observed after evaluating ROAR are important for supporting the differences observed between TD and ASD relevance maps. The more reliable XAI methods obtained after using ROAR and removing more than the 50% of the relevant features are precisely the LRP-B, PatternNet and Pattern-Attribution. These methods are the ones that show significant differences in the late-timing between TD and ASD. Specifically, those differences between TD and ASD observed in the more reliable XAI methods are consistent with the altered neural connectivity patterns observed when individuals with ASD process emotion from faces.\n\nThis study consolidates important findings in ASD and computational-neuroscience research. The ROAR evaluation can identify the more reliable and intuitively important features that can successfully decode an emotion from EEG activity. These distinguishable patterns are setting a more consistent and remarkable set of information that is precisely relevant for the emotion decoding. This represents a different and intact emotion information encoding in individuals with ASD that can be efficiently extracted by the CNN. These results can re-define the current stateof-the-art of facial emotion-decoding pipelines and XAI. This supports, CNN as a perceptual-based classifier, which overcomes the behavioral/neural emotion comprehension deficits observed in individuals with ASD.\n\nThis study is the first, quantitatively speaking, to employ ROAR for evaluating robust XAI methods on EEG-based facial emotion recognition. This study is also the first to use EEG features for evaluating the reliability and correctness of current state-of-the-art XAI methods, including EEG trials from ASD and non-ASD individuals.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the average",
      "page": 4
    },
    {
      "caption": "Figure 1: We consider",
      "page": 4
    },
    {
      "caption": "Figure 1: Average relevance-maps for Smooth-Grad Squared, PatternNet, Pattern-Attribution, and LRP-B ﬂat preset. The relevance-maps for TD are shown on the left,",
      "page": 5
    },
    {
      "caption": "Figure 1: are the Average",
      "page": 6
    },
    {
      "caption": "Figure 2: Topoplots examples for the binary masks on 500-1000ms and 750-1250ms time ranges. These topo-maps cover all the saliency methods analyzed in this study",
      "page": 8
    },
    {
      "caption": "Figure 2: b shows the topo-plots related to the binary",
      "page": 8
    },
    {
      "caption": "Figure 3: c and 3d for TD and ASD.",
      "page": 8
    },
    {
      "caption": "Figure 3: Average accuracies and detriments comparison between all the saliency maps evaluated in this study. On Figures 3a and 3b, and for the method-based slices",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Metrics/Groups": "",
          "FER": "Acc",
          "Column_3": "Pre",
          "Column_4": "Re",
          "Column_5": "F1",
          "CNN": "Acc",
          "Column_7": "Pre",
          "Column_8": "Re",
          "Column_9": "F1"
        },
        {
          "Metrics/Groups": "TD",
          "FER": "0.815±0.083",
          "Column_3": "0.808±0.079",
          "Column_4": "0.802±0.077",
          "Column_5": "0.807±0.079",
          "CNN": "0.860±0.213",
          "Column_7": "0.864±0.201",
          "Column_8": "0.860±0.204",
          "Column_9": "0.862±0.202"
        },
        {
          "Metrics/Groups": "ASD*",
          "FER": "0.776±0.093",
          "Column_3": "0.774±0.089",
          "Column_4": "0.768±0.088",
          "Column_5": "0.771±0.088",
          "CNN": "0.934±0.134",
          "Column_7": "0.935±0.132",
          "Column_8": "0.933±0.134",
          "Column_9": "0.934±0.132"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "2",
      "title": "Sequence to sequence learning with neural networks",
      "authors": [
        "I Sutskever",
        "O Vinyals",
        "Q Le"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "3",
      "title": "Mask r-cnn",
      "authors": [
        "K He",
        "G Gkioxari",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "4",
      "title": "Evaluation of source-wise missing data techniques for the prediction of parkinson's disease using smartphones",
      "authors": [
        "J Prince",
        "F Andreotti",
        "M Vos"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Multimodal hand stereotypies detection in rett syndrome treatment using deep belief neural networks",
      "authors": [
        "H O'leary",
        "J Mayor",
        "W Kaufmann",
        "M Sahin"
      ],
      "year": "2017",
      "venue": "2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "6",
      "title": "Multichannel sleep stage classification and transfer learning using convolutional neural networks",
      "authors": [
        "F Andreotti",
        "H Phan",
        "N Cooray",
        "C Lo",
        "M Hu",
        "M Vos"
      ],
      "year": "2018",
      "venue": "2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition using multimodal deep learning",
      "authors": [
        "W Liu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "8",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "9",
      "title": "Towards explaining deep learning networks to distinguish facial expressions of pain and emotions",
      "authors": [
        "K Weitz",
        "T Hassan",
        "U Schmid",
        "J Garbas"
      ],
      "year": "2018",
      "venue": "Forum Bildverarbeitung"
    },
    {
      "citation_id": "10",
      "title": "Estimating uncertainty in deep learning for reporting confidence to clinicians when segmenting nuclei image data",
      "authors": [
        "B Ghoshal",
        "A Tucker",
        "B Sanghera",
        "W Wong"
      ],
      "year": "2019",
      "venue": "2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)"
    },
    {
      "citation_id": "11",
      "title": "Facial emotions are accurately encoded in the neural signal of those with autism spectrum disorder: A deep learning approach",
      "authors": [
        "J Torres",
        "T Clarkson",
        "K Hauschild",
        "C Luhmann",
        "M Lerner",
        "G Riccardi"
      ],
      "year": "2021",
      "venue": "Biological Psychiatry: Cognitive Neuroscience and Neuroimaging"
    },
    {
      "citation_id": "12",
      "title": "Eeg complexity as a biomarker for autism spectrum disorder risk",
      "authors": [
        "W Bosl",
        "A Tierney",
        "H Tager-Flusberg",
        "C Nelson"
      ],
      "year": "2011",
      "venue": "BMC medicine"
    },
    {
      "citation_id": "13",
      "title": "Stimulus dependent neural oscillatory patterns show reliable statistical identification of autism spectrum disorder in a face perceptual decision task",
      "authors": [
        "J Castelhano",
        "P Tavares",
        "S Mouga",
        "G Oliveira",
        "M Castelo-Branco"
      ],
      "year": "2018",
      "venue": "Clinical Neurophysiology"
    },
    {
      "citation_id": "14",
      "title": "Feature extraction and selection for emotion recognition from eeg",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "16",
      "title": "Eeg analysis of facial affect recognition process of individuals with asd performance prediction leveraging social context",
      "authors": [
        "J Fan",
        "E Bekele",
        "Z Warren",
        "N Sarkar"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "17",
      "title": "Eeg-based affect and workload recognition in a virtual driving environment for asd intervention",
      "authors": [
        "J Fan",
        "J Wade",
        "A Key",
        "Z Warren",
        "N Sarkar"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "18",
      "title": "Single-trial analysis and classification of erp components-a tutorial",
      "authors": [
        "B Blankertz",
        "S Lemm",
        "M Treder",
        "S Haufe",
        "K.-R Üller"
      ],
      "year": "2011",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "19",
      "title": "Automatic classification of artifactual ica-components for artifact removal in eeg signals",
      "authors": [
        "I Winkler",
        "S Haufe",
        "M Tangermann"
      ],
      "year": "2011",
      "venue": "Behavioral and Brain Functions"
    },
    {
      "citation_id": "20",
      "title": "Classification of respiratory disturbances in rett syndrome patients using restricted boltzmann machine",
      "authors": [
        "H O'leary",
        "J Mayor",
        "C.-S Poon",
        "W Kaufmann",
        "M Sahin"
      ],
      "year": "2017",
      "venue": "2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "21",
      "title": "Eeg signals classification using linear and nonlinear discriminant methods",
      "authors": [
        "J Torres"
      ],
      "year": "2013",
      "venue": "El Hombre y la Máquina"
    },
    {
      "citation_id": "22",
      "title": "Eeg-based single trial classification emotion recognition: A comparative analysis in individuals with and without autism spectrum disorder",
      "authors": [
        "J Torres",
        "E Libsack",
        "T Clarkson",
        "C Keifer",
        "G Riccardi",
        "M Lerner"
      ],
      "year": "2018",
      "venue": "International Society for Autism Research"
    },
    {
      "citation_id": "23",
      "title": "On the use of convolutional neural networks and augmented csp features for multi-class motor imagery of eeg signals classification",
      "authors": [
        "H Yang",
        "S Sakhavi",
        "K Ang",
        "C Guan"
      ],
      "year": "2015",
      "venue": "2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "24",
      "title": "Filter bank common spatial pattern (fbcsp) in brain-computer interface",
      "authors": [
        "K Ang",
        "Z Chin",
        "H Zhang",
        "C Guan"
      ],
      "year": "2008",
      "venue": "2008 IEEE International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "25",
      "title": "Classifying different emotional states by means of eeg-based functional connectivity patterns",
      "authors": [
        "Y.-Y Lee",
        "S Hsieh"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "26",
      "title": "The bci competition 2003: progress and perspectives in detection and discrimination of eeg single trials",
      "authors": [
        "B Blankertz",
        "K.-R Muller",
        "G Curio",
        "T Vaughan",
        "G Schalk",
        "J Wolpaw",
        "A Schlogl",
        "C Neuper",
        "G Pfurtscheller",
        "T Hinterberger"
      ],
      "year": "2004",
      "venue": "IEEE transactions on biomedical engineering"
    },
    {
      "citation_id": "27",
      "title": "Enhanced error decoding from errorrelated potentials using convolutional neural networks",
      "authors": [
        "J Torres",
        "T Clarkson",
        "E Stepanov",
        "C Luhmann",
        "M Lerner",
        "G Riccardi"
      ],
      "year": "2018",
      "venue": "40th International Engineering in Medicine and Biology Conference"
    },
    {
      "citation_id": "28",
      "title": "Deep learning with convolutional neural networks for eeg decoding and visualization",
      "authors": [
        "R Schirrmeister",
        "J Springenberg",
        "L Fiederer",
        "M Glasstetter",
        "K Eggensperger",
        "M Tangermann",
        "F Hutter",
        "W Burgard",
        "T Ball"
      ],
      "year": "2017",
      "venue": "Human brain mapping"
    },
    {
      "citation_id": "29",
      "title": "Mechanisms of facial emotion recognition in autism spectrum disorders: insights from eye tracking and electroencephalography",
      "authors": [
        "M Black",
        "N Chen",
        "K Iyer",
        "O Lipp",
        "M Falkmer",
        "T Tan",
        "S Girdler"
      ],
      "year": "2017",
      "venue": "Neuroscience & Biobehavioral Reviews"
    },
    {
      "citation_id": "30",
      "title": "Toward efficient automation of interpretable machine learning",
      "authors": [
        "B Kovalerchuk",
        "N Neuhaus"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Big Data (Big Data)"
    },
    {
      "citation_id": "31",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "32",
      "title": "Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks",
      "authors": [
        "A Chattopadhay",
        "A Sarkar",
        "P Howlader",
        "V Balasubramanian"
      ],
      "year": "2018",
      "venue": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "33",
      "title": "Why are saliency maps noisy? cause of and solution to noisy saliency maps",
      "authors": [
        "B Kim",
        "J Seo",
        "S Jeon",
        "J Koo",
        "J Choe",
        "T Jeon"
      ],
      "year": "2019",
      "venue": "Why are saliency maps noisy? cause of and solution to noisy saliency maps",
      "arxiv": "arXiv:1902.04893"
    },
    {
      "citation_id": "34",
      "title": "Smoothgrad: removing noise by adding noise",
      "authors": [
        "D Smilkov",
        "N Thorat",
        "B Kim",
        "F Viégas",
        "M Wattenberg"
      ],
      "year": "2017",
      "venue": "Smoothgrad: removing noise by adding noise",
      "arxiv": "arXiv:1706.03825"
    },
    {
      "citation_id": "35",
      "title": "The (un) reliability of saliency methods",
      "authors": [
        "P.-J Kindermans",
        "S Hooker",
        "J Adebayo",
        "M Alber",
        "K Sch Ütt",
        "S Dähne",
        "D Erhan",
        "B Kim"
      ],
      "year": "2017",
      "venue": "The (un) reliability of saliency methods",
      "arxiv": "arXiv:1711.00867"
    },
    {
      "citation_id": "36",
      "title": "Toward interpretable machine learning: Transparent deep neural networks and beyond",
      "authors": [
        "W Samek",
        "G Montavon",
        "S Lapuschkin",
        "C Anders",
        "K.-R Üller"
      ],
      "year": "2020",
      "venue": "Toward interpretable machine learning: Transparent deep neural networks and beyond",
      "arxiv": "arXiv:2003.07631"
    },
    {
      "citation_id": "37",
      "title": "Layer-wise relevance propagation for deep neural network architectures",
      "authors": [
        "A Binder",
        "S Bach",
        "G Montavon",
        "W Samek"
      ],
      "year": "2016",
      "venue": "Information Science and Applications (ICISA)"
    },
    {
      "citation_id": "38",
      "title": "Methods for interpreting and understanding deep neural networks",
      "authors": [
        "G Montavon",
        "W Samek",
        "K.-R Üller"
      ],
      "year": "2018",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Learning how to explain neural networks: Patternnet and patternattribution",
      "authors": [
        "P.-J Kindermans",
        "K Sch Ütt",
        "M Alber",
        "K -R. M Üller",
        "D Erhan",
        "B Kim",
        "S Dähne"
      ],
      "year": "2017",
      "venue": "Learning how to explain neural networks: Patternnet and patternattribution",
      "arxiv": "arXiv:1705.05598"
    },
    {
      "citation_id": "40",
      "title": "Sanity checks for saliency maps",
      "authors": [
        "J Adebayo",
        "J Gilmer",
        "M Muelly",
        "I Goodfellow",
        "M Hardt",
        "B Kim"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "41",
      "title": "Evaluating feature importance estimates",
      "authors": [
        "S Hooker",
        "D Erhan",
        "P.-J Kindermans",
        "B Kim"
      ],
      "year": "2018",
      "venue": "Evaluating feature importance estimates",
      "arxiv": "arXiv:1806.10758"
    },
    {
      "citation_id": "42",
      "title": "Understanding the nature of face processing impairment in autism: insights from behavioral and electrophysiological studies",
      "authors": [
        "G Dawson",
        "S Webb",
        "J Mcpartland"
      ],
      "year": "2005",
      "venue": "Developmental neuropsychology"
    },
    {
      "citation_id": "43",
      "title": "Development of social brain circuitry in autism",
      "authors": [
        "G Dawson",
        "R Bernier"
      ],
      "year": "2007",
      "venue": "Human behavior, learning, and the developing brain: Atypical development"
    },
    {
      "citation_id": "44",
      "title": "Manual for the receptive tests of the diagnostic analysis of nonverbal accuracy 2",
      "authors": [
        "S Nowicki"
      ],
      "year": "2000",
      "venue": "Manual for the receptive tests of the diagnostic analysis of nonverbal accuracy 2"
    },
    {
      "citation_id": "45",
      "title": "Autism spectrum disorders",
      "authors": [
        "C Lord",
        "E Cook",
        "B Leventhal",
        "D Amaral"
      ],
      "year": "2000",
      "venue": "Neuron"
    },
    {
      "citation_id": "46",
      "title": "Neural correlates of face and object recognition in young children with autism spectrum disorder, developmental delay, and typical development",
      "authors": [
        "G Dawson",
        "L Carver",
        "A Meltzoff",
        "H Panagiotides",
        "J Mc-Partland",
        "S Webb"
      ],
      "year": "2002",
      "venue": "Child development"
    },
    {
      "citation_id": "47",
      "title": "Eeglab: an open source toolbox for analysis of single-trial eeg dynamics including independent component analysis",
      "authors": [
        "A Delorme",
        "S Makeig"
      ],
      "year": "2004",
      "venue": "Journal of neuroscience methods"
    },
    {
      "citation_id": "48",
      "title": "Guidelines and best practices for electrophysiological data collection, analysis and reporting in autism",
      "authors": [
        "S Webb",
        "R Bernier",
        "H Henderson",
        "M Johnson",
        "E Jones",
        "M Lerner",
        "J Mcpartland",
        "C Nelson",
        "D Rojas",
        "J Townsend"
      ],
      "year": "2015",
      "venue": "Journal of autism and developmental disorders"
    },
    {
      "citation_id": "49",
      "title": "Finding the optimal cross-subject eeg data alignment method for analysis and bci",
      "authors": [
        "N Bigdely-Shamlo",
        "G Ibagon",
        "C Kothe",
        "T Mullen"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)"
    },
    {
      "citation_id": "50",
      "title": "Adjust: An automatic eeg artifact detector based on the joint use of spatial and temporal features",
      "authors": [
        "A Mognon",
        "J Jovicich",
        "L Bruzzone",
        "M Buiatti"
      ],
      "year": "2011",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "51",
      "title": "Independent component analysis: algorithms and applications",
      "authors": [
        "A Hyvärinen",
        "E Oja"
      ],
      "year": "2000",
      "venue": "Neural networks"
    },
    {
      "citation_id": "52",
      "title": "Learning feature representations with k-means",
      "authors": [
        "A Coates",
        "A Ng"
      ],
      "year": "2012",
      "venue": "Neural networks: Tricks of the trade"
    },
    {
      "citation_id": "53",
      "title": "The effectiveness of technology-based intervention in improving emotion recognition through facial expression in people with autism spectrum disorder: a systematic review",
      "authors": [
        "C Lee",
        "S Lam",
        "S Tsang",
        "C Yuen",
        "C Ng"
      ],
      "year": "2018",
      "venue": "Review Journal of Autism and Developmental Disorders"
    },
    {
      "citation_id": "54",
      "title": "Adversarially occluded samples for person re-identification",
      "authors": [
        "H Huang",
        "D Li",
        "Z Zhang",
        "X Chen",
        "K Huang"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "55",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "56",
      "title": "Deep sparse rectifier neural networks",
      "authors": [
        "X Glorot",
        "A Bordes",
        "Y Bengio"
      ],
      "year": "2011",
      "venue": "Proceedings of the fourteenth international conference on artificial intelligence and statistics"
    },
    {
      "citation_id": "57",
      "title": "Patternnet and patternlrp-improving the interpretability of neural networks",
      "authors": [
        "P.-J Kindermans",
        "K Sch Ütt",
        "M Alber",
        "S Dähne"
      ],
      "year": "2017",
      "venue": "stat"
    },
    {
      "citation_id": "58",
      "title": "Kindermans, \"innvestigate neural networks!",
      "authors": [
        "M Alber",
        "S Lapuschkin",
        "P Seegerer",
        "M Hägele",
        "K Sch Ütt",
        "G Montavon",
        "W Samek",
        "K -R. M Üller",
        "S Dähne"
      ],
      "year": "2019",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "59",
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "authors": [
        "S Bach",
        "A Binder",
        "G Montavon",
        "F Klauschen",
        "W Samek"
      ],
      "year": "2015",
      "venue": "PloS one"
    },
    {
      "citation_id": "60",
      "title": "An existence result for scalar conservation laws using measure valued solutions",
      "authors": [
        "A Szepessy"
      ],
      "year": "1989",
      "venue": "Communications in Partial Differential Equations"
    },
    {
      "citation_id": "61",
      "title": "On the influence of high-pass filtering on ica-based artifact reduction in eeg-erp",
      "authors": [
        "I Winkler",
        "S Debener",
        "M Tangermann"
      ],
      "year": "2015",
      "venue": "2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "62",
      "title": "Face perception and learning in autism spectrum disorders",
      "authors": [
        "S Webb",
        "E Neuhaus",
        "S Faja"
      ],
      "year": "2017",
      "venue": "The Quarterly Journal of Experimental Psychology"
    },
    {
      "citation_id": "63",
      "title": "Where does eeg come from and what does it mean?",
      "authors": [
        "M Cohen"
      ],
      "year": "2017",
      "venue": "Trends in neurosciences"
    },
    {
      "citation_id": "64",
      "title": "Evaluation: from precision, recall and f-measure to roc, informedness, markedness and correlation",
      "authors": [
        "D Powers"
      ],
      "year": "2011",
      "venue": "Evaluation: from precision, recall and f-measure to roc, informedness, markedness and correlation"
    },
    {
      "citation_id": "65",
      "title": "Holm's sequential bonferroni procedure",
      "authors": [
        "H Abdi"
      ],
      "year": "2010",
      "venue": "Encyclopedia of research design"
    },
    {
      "citation_id": "66",
      "title": "Kolmogorov-smirnov test for life test data with hybrid censoring",
      "authors": [
        "B Banerjee",
        "B Pradhan"
      ],
      "year": "2018",
      "venue": "Communications in Statistics-Theory and Methods"
    },
    {
      "citation_id": "67",
      "title": "Developmental change in the erp responses to familiar faces in toddlers with autism spectrum disorders versus typical development",
      "authors": [
        "S Webb",
        "E Jones",
        "K Merkle",
        "K Venema",
        "J Greenson",
        "M Murias",
        "G Dawson"
      ],
      "year": "2011",
      "venue": "Child development"
    },
    {
      "citation_id": "68",
      "title": "An effective neurofeedback intervention to improve social interactions in children with autism spectrum disorder",
      "authors": [
        "E Friedrich",
        "A Sivanathan",
        "T Lim",
        "N Suttie",
        "S Louchart",
        "S Pillen",
        "J Pineda"
      ],
      "year": "2015",
      "venue": "Journal of autism and developmental disorders"
    },
    {
      "citation_id": "69",
      "title": "Interpretable sincnet-based deep learning for emotion recognition from eeg brain activity",
      "authors": [
        "J Mayor-Torres",
        "M Ravanelli",
        "S Medina-Devilliers",
        "M Lerner",
        "G Riccardi"
      ],
      "year": "2021",
      "venue": "Interpretable sincnet-based deep learning for emotion recognition from eeg brain activity",
      "arxiv": "arXiv:2107.10790"
    },
    {
      "citation_id": "70",
      "title": "Explainable artificial intelligence for neuroscience: Behavioral neurostimulation",
      "authors": [
        "J.-M Fellous",
        "G Sapiro",
        "A Rossi",
        "H Mayberg",
        "M Ferrante"
      ],
      "year": "2019",
      "venue": "Frontiers in neuroscience"
    }
  ]
}