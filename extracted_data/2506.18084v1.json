{
  "paper_id": "2506.18084v1",
  "title": "Tem 3 -Learning: Time-Efficient Multimodal Multi-Task Learning For Advanced Assistive Driving",
  "published": "2025-06-22T16:12:27Z",
  "authors": [
    "Wenzhuo Liu",
    "Yicheng Qiao",
    "Zhen Wang",
    "Qiannan Guo",
    "Zilong Chen",
    "Meihua Zhou",
    "Xinran Li",
    "Letian Wang",
    "Zhiwei Li",
    "Huaping Liu",
    "Wenshuo Wang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multi-task learning (MTL)   can advance assistive driving by exploring inter-task correlations through shared representations. However, existing methods face two critical limitations: single-modality constraints limiting comprehensive scene understanding and inefficient architectures impeding realtime deployment. This paper proposes TEM 3 -Learning (Time-Efficient Multimodal Multi-task Learning), a novel framework that jointly optimizes driver emotion recognition, driver behavior recognition, traffic context recognition, and vehicle behavior recognition through a two-stage architecture. The first component, the mamba-based multi-view temporal-spatial feature extraction subnetwork (MTS-Mamba), introduces a forward-backward temporal scanning mechanism and globallocal spatial attention to efficiently extract low-cost temporalspatial features from multi-view sequential images. The second component, the MTL-based gated multimodal feature integrator (MGMI), employs task-specific multi-gating modules to adaptively highlight the most relevant modality features for each task, effectively alleviating the negative transfer problem in MTL. Evaluation on the AIDE dataset, our proposed model achieves state-of-the-art accuracy across all four tasks, maintaining a lightweight architecture with fewer than 6 million parameters and delivering an impressive 142.32 FPS inference speed. Rigorous ablation studies further validate the effectiveness of the proposed framework and the independent contributions of each module. The code is available on https: //github.com/Wenzhuo-Liu/TEM3-Learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Advanced Driver Assistance Systems (ADAS) can improve driving safety by continuously monitoring the driver's state and traffic environment  [14] ,  [54] . However, existing research is mostly limited to the solution of single tasks, such as driver emotion/behavior, traffic environment recognition, without addressing the inherent interdependencies between these tasks  [38] . Studies show a significant coupling rela-  tionship between the drivers' state and the traffic environment  [36] ,  [51] . For example, drivers frequently adjust their behavior, such as changing lanes based on surrounding traffic conditions, while traffic congestion can induce driver anxiety.\n\nIntegrating driver state and traffic environment recognition into a unified multi-task learning (MTL) framework can provide a more holistic understanding of driving scenarios and enhance the safety performance of ADAS  [51] .\n\nCompared to single-task learning, MTL reduces overfitting by sharing features across tasks, thereby improving the performance of each individual task  [9] ,  [24] . In the context of ADAS, existing MTL models generally focus on related sub-tasks. For example, Wu proposed a model that jointly learns multiple traffic environment-related tasks, enabling simultaneous recognition of lane markings, drivable areas, and object detection in driving scenes  [47] . Similarly, Xing et al. focus on driver-related MTL to recognize the driver's emotions and behaviors  [48] . While these approaches have yielded performance improvements, they often struggle with negative transfer, where task performance deteriorates due to task-related conflicts or differences  [27] .\n\nMTL models tend to have more complex structures com-pared to single-task learning, as they must simultaneously handle multiple tasks  [47] ,  [52]  (Fig.  1  (a) and (c)). Many MTL methods rely on inputs from a single modality such as driving scene images  [6] ,  [8] . However, optimal task performance requires complementary multimodal inputs  [17] ,  [28] . For example, combining driver images with eye movement data can improve emotion recognition  [37] , while fusing image and point cloud data can enhance traffic object recognition  [1] ,  [19] . Unfortunately, many multimodal input algorithms still rely on independent feature extraction  [23] ,  [29] ,  [30] ,  [45] , which significantly increases model parameters, reduces inference speed, and compromises practical scalability (Fig.  1  (b)). Therefore, a key challenge lies in effectively leveraging multimodal information while optimizing individual task performance, enhancing task-level synergy, and maintaining high inference speed with a minimal parameter count.\n\nTo address these challenges, we propose a Time-efficient multimodal multi-task learning network that simultaneously performs four tasks -driver emotion recognition (DER), driver behavior recognition (DBR), traffic context recognition (TCR), and vehicle behavior recognition (VBR) -using multimodal data (Fig.  1 (d) ). This is achieved through two key components: the Mamba-based multi-view temporalspatial feature extraction subnetwork (MTS-Mamba) and the multi-task learning-based gated multimodal feature integrator (MGMI). MTS-Mamba introduces a forward-backward temporal scanning mechanism and a global-local spatial feature extraction strategy, enabling efficient extraction of temporal-spatial features from multi-view sequential images at low computational cost. This approach provides richer and more robust feature representations for subsequent multitask recognition. MGMI, inspired by previous work  [34] , incorporates a multi-gating mechanism that dynamically adjusts the weights of modality features based on taskspecific attention. This selective reinforcement of important features helps alleviate negative transfer. We validated the effectiveness of our method using the publicly available AIDE dataset. Experimental results show that our model outperforms previous methods on all four tasks, with fewer than 6M parameters and exceptionally fast inference speed, demonstrating its efficiency and practicality. Our contributions include:\n\n‚Ä¢ A time-efficient multimodal MTL framework that provides a new paradigm for multimodal MTL in ADAS. ‚Ä¢ The MTS-Mamba subnetwork, which effectively extracts temporal-spatial features from multi-view sequential images across multiple dimensions. ‚Ä¢ The MGMI mechanism, which adaptively adjusts attention to different modalities for each task, mitigating negative transfer and enhancing task-specific feature extraction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Multimodal Learning",
      "text": "ADAS systems that rely solely on single-modality data often struggle to handle the complex and dynamic challenges encountered during driving  [1] ,  [3] ,  [12] ,  [15] ,  [37] ,  [40] ,  [46] . Each modality offers unique advantages and limitations, and leveraging multiple modalities can provide a more comprehensive understanding of the driving environment. For example, combining multi-view driving scene images with LiDAR data enhances environmental perception  [26] ,  [49] . Similarly, integrating driver images and joint information can effectively capture facial expressions and behavioral states  [51] . As a result, many studies have increasingly turned to multimodal data to enhance task accuracy. For instance, Zhou et al.  [55]  used front-view driving scene images, driver images, and vehicle speed data for driver behavior recognition, while Liu et al.  [33]  combined camera images and LiDAR data for 3D object detection.\n\nHowever, multimodal learning in ADAS faces two key challenges. First, many existing models rely on independent feature extraction branches for each modality, even when processing multi-view images  [17] ,  [29] ,  [31] ,  [42] ,  [55] . This approach significantly increases the model's parameter count and overlooks potential intermodal interactions, leading to inefficient information use. Second, multimodal fusion methods are often limited to combining only a few modality features, restricting their generalization ability and scalability.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Multi-Task Learning",
      "text": "MTL improves model performance on each task by sharing features across tasks  [9] ,  [24] . The two predominant strategies for parameter sharing are hard and soft parameter sharing. In hard parameter sharing, most parameters are shared across tasks, with task-specific differentiation occurring only in the final layers  [4] ,  [10] ,  [25] . For example, Wu et al.  [47]  adopted this structure to simultaneously achieve traffic object detection, drivable area segmentation, and lane detection. While simple and efficient, this approach is prone to negative transfer when tasks exhibit substantial differences, limiting its generalizability. To mitigate this, soft parameter sharing has been proposed, where each task retains independent parameters but leverages shared features, thus preserving task independence and reducing task conflicts  [7] ,  [13] ,  [50] . For instance, Choi et al.  [8]  designed a taskadaptive attention generator to enable independent parameters for tasks like monocular 3D object detection, semantic segmentation, and dense depth estimation.\n\nAlthough soft parameter sharing alleviates task conflicts, two critical challenges remain. First, introducing taskspecific parameters increases the model's flexibility but significantly raises the parameter count, which can hinder realtime performance -a major concern in applications like ADAS, where real-time processing is crucial. Second, existing MTL models in ADAS are predominantly designed for single-modality inputs (e.g., driving scene images), limiting their applicability to multimodal scenarios and constraining the full potential of MTL-based solutions. The overall pipeline of TEM 3 -Learning. MTS-Mamba and 3D CNN are used to extract multimodal features from vehicle-exterior images, vehicle-interior images, and driver joints, respectively. The multitask learning-based gated multimodal feature integrator (MGMI) adaptively fuses these features, enabling multi-task recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "This section presents the overall structure and key modules of the proposed TEM 3 -Learning network. We first describe the overall architecture of the network and then detail two core modules: MTS-Mamba and MGMI.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Network Overview",
      "text": "To fully leverage the synergies between multimodal and multi-view data while balancing accuracy and efficiency in a MTL context, our proposed TEM 3 -Learning network (Fig.  2 ) consists of three core components: the multi-branch feature extraction layer, MGMI, and the multi-task recognition layer.\n\nThe first component, the multi-branch feature extraction layer, processes multimodal data through two modules: MTS-Mamba and 3D CNN. MTS-Mamba extracts deep temporal-spatial features from vehicle-exterior images (front, left, and right views) and vehicle-interior images (insideview, driver's facial and body images). The 3D CNN module focuses on extracting high-level semantic features from the driver's posture and gestures. These extracted features are then integrated by MGMI, which first uses a self-attention mechanism to obtain task-shared features, followed by a multi-gating mechanism to adaptively weight features based on task-specific attention, thus improving feature extraction and alleviating conflicts between tasks.\n\nDuring training, we use a cross-entropy loss function that integrates individual task losses to optimize the overall model performance. The total loss L total is computed as:\n\nwhere ≈∑r denotes the recognition results for task r, and y r denotes the corresponding ground truth. The number of tasks m is set to 4, corresponding to the four tasks: driver emotion recognition, driver behavior recognition, traffic context recognition, and vehicle behavior recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Mamba-Based Multi-View Temporal-Spatial Feature Extraction Subnetwork (Mts-Mamba)",
      "text": "Multi-view sequential images from both vehicle interiors and exteriors exhibit strong temporal-spatial correlations.\n\nFor instance, sequential vehicle-interior images effectively capture driver behavior (e.g., looking around, making calls), while multi-view vehicle-exterior images provide a comprehensive understanding of surrounding environments (e.g., pedestrians, vehicles, obstacles). Leveraging these temporalspatial features is crucial for enhancing ADAS performance in environmental perception and behavior recognition. However, in real-world driving scenarios -especially in complex scenarios like congested intersections where dynamic targets and complex movement patterns are prevalentmodeling these features becomes increasingly challenging. The difficulty arises from the drastic environmental changes and the need to balance modeling quality with real-time performance. CNNs offer good real-time performance but are limited by their local receptive field, which hampers the capture of global temporal-spatial features and, consequently, task recognition accuracy. On the other hand, attention-based methods, while capable of global modeling, suffer from high computational complexity, making them unsuitable for processing large volumes of multi-view sequential data in real-time ADAS applications. To address these challenges, we propose MTS-Mamba, which combines a dual-path temporal-spatial feature extraction structure with a State Space Model (SSM) based on Mamba  [16] . This approach efficiently captures multi-view sequential image features while maintaining low computational cost, ensuring a balance between recognition accuracy and real-time performance.\n\nBefore MTS-Mamba, we need to process multi-view sequential images. First, the sequential images of the j-th view are concatenated along the channel dimension, forming a sequence I j . Each I j is then processed through deep convolution and adaptive average pooling to extract initial features of identical dimensions. These features are integrated into a feature map F f ‚àà R C√óH√óW , where C, H, and W represent the channel count, height, and width, respectively. This feature map is subsequently input into the MTS-Mamba module for further processing.\n\nFor the input feature map F f , we first apply 1D convolution with GELU activation to enhance feature representation within the dual-path temporal-spatial feature extraction structure,i.e., global and local layers. The local and global layers employ forward and backward scanning, respectively, to extract bidirectional temporal features. Specifically, given that F f spans 16 consecutive frames, the local layer perform a forward scan where each channel of F f is linearly transformed by the state parameters B ‚Ä¢ C ‚ä§ of the statespace model, capturing forward temporal dependencies. In the global layer, the spatial order of F f is reversed during the backward scanning to capture temporal interdependencies from both directions. Then, it is linearly transformed with B ‚Ä¢ C ‚ä§ . During training, the shared state parameters B and C are updated via backpropagation to retain bidirectional temporal information. The SSM calculates the temporal feature weight W ssm as:",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Ô£± Ô£¥ Ô£≤",
      "text": "Ô£¥ Ô£≥\n\nwhere B (s+1) denotes the updated shared weight parameter, Œ∑ is the learning rate, œÉ(‚Ä¢) is the sigmoid function, A ‚àà R C√ón is the state transition matrix where n indicates the state dimension, d state and d dim are the state unit vector and channel unit vector, and D ‚àà R C is the bias vector.\n\nAfter capturing the bidirectional temporal features, the local layer uses 3√ó3 average pooling and linear projection to extract local spatial features F l , while the global layer uses adaptive average pooling and linear projection to capture global spatial features F g . This captures detailed features and global context from spatial information at different scales, providing more comprehensive and rich spatial feature representations. We then merge the multi-scale spatial features F l and F g and multiply them by the temporal feature weight information W ssm , combining temporal dynamics with spatial structure. A residual connection is then used to combine the merged features with the original input features F f , producing the final output feature F o :\n\nwhere Œ≥ is the scaling factor, LN denotes linear layer, and ‚äô denotes element-wise multiplication. In summary, MTS-Mamba's unique dual-path temporalspatial feature extraction structure effectively captures both multi-scale spatial features and bidirectional temporal dependencies. By introducing forward-backward scanning mechanisms, MTS-Mamba achieves a balance between accuracy in temporal-spatial feature modeling and computational efficiency, significantly enhancing real-time performance in ADAS applications.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Multi-Task Gated Multimodal Integrator (Mgmi)",
      "text": "We propose the Multi-task Gated Multimodal Integrator (MGMI), which introduces task-specific gating mechanisms to dynamically adjust the fusion weights of different modality features, enabling task-driven feature selection and alleviating task conflicts. Specifically, we first concatenate the features H 1 , H 2 ‚àà R C√óH√óW extracted by MTS-Mamba with the features H 3 from the 3D CNN module along the channel dimension to obtain the initial fused feature. This fused feature undergoes three separate convolution operations to produce query (Q), key (K), and value (V), respectively. The computation process is as follows:\n\nThe attention matrix is computed by performing a dot product between Q and K, and multiplying the resulting attention matrix by V to obtain the weighted attention scores, which are reshaped back to R C√óH√óW to yield the taskshared features.\n\nTo adapt to each task's specific focus, we design a multigating mechanism. The task-shared features are input into four task-specific gating units, one for each task, which perform convolution, BatchNorm, and Sigmoid operations to calculate attention weights for the multimodal features H 1 , H 2 , and H 3 . A weighted sum of these features is calculated to produce the task-specific feature F r , where r ‚àà {1, 2, 3, 4} represents each task. The task-specific feature F r is\n\nwhere œÉ i r is the i-th gating Sigmoid function for task r, and BN denotes the batch normalization operation. This design optimizes the feature fusion process for each task by dynamically adjusting the importance of each modality's features, allowing the model to extract task-specific features while retaining shared characteristics. By alleviating negative transfer across tasks, MGMI enhances the performance of each task within the multi-task learning framework. Once multimodal features are fused for each task, the multi-task recognition layer applies independent pooling for each task's feature F r and inputs them into their respective classifiers, producing the prediction ≈∑r for each task.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Dataset",
      "text": "The AIDE dataset is an open-source collection designed to advance ADAS research, consisting of 2,898 samples",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Data Preprocessing",
      "text": "The preprocessing pipeline follows three sequential steps: extraction of driver facial and body images, data synchronization, and data augmentation. First, driver facial and body regions are cropped using bounding box coordinates from inside-view images. The multimodal data (images and joint positions) is synchronized at 16 frames per second, ensuring temporal alignment of features across modalities. Each input sequence is formed from 16 consecutive frames to capture temporal dependencies. To further augment the dataset, multi-view images are subjected to random horizontal and vertical flips (50% probability), enhancing the diversity of the training set and improving model robustness.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Evaluation Metrics",
      "text": "To evaluate the model performance, we use the following key metrics: accuracy (Œ± acc ), mean accuracy (Œ≤ macc ), frames per second (FPS), and total parameter count (Params). Œ≤ macc represents the average accuracy across multiple tasks, providing an overall performance measure for multi-task learning. It is computed as:\n\nwhere and Œ± r acc represents the accuracy of the model on the r-th task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Experimental Details",
      "text": "All experiments were conducted on an NVIDIA L40S GPU with 48GB VRAM. Training was performed for 125 epochs, with stabilization of the outcomes guiding early stopping. The initial learning rate was set to 0.001 and dynamically adjusted: reduced to 0.0005 between epochs 25 and 50, and further decreased to 0.00005 after epoch 50. The batch size was 24, with Stochastic Gradient Descent (SGD) used as the optimization algorithm, featuring a momentum of 0.9 and weight decay of 0.0001 to ensure convergence while mitigating overfitting.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Comparison With Sota Models",
      "text": "We compared our model with state-of-the-art methods, following the experimental setup from  [51] , which involves independent feature extraction from multimodal data and multi-view images. Our model consistently outperformed these baseline models across all evaluation metrics, including task-specific Œ± acc , Œ≤ macc , FPS, and parameter count. With fewer than 6M parameters, our model improved Œ≤ macc by 3.48%-9.32% and achieved an inference speed of 142.32 FPS, significantly surpassing the real-time requirements of ADAS systems.\n\nThese performance improvements are attributed to the model's design optimizations. Specifically, the joint feature extraction across similar modalities reduces model complexity and enhances inter-modal interactions, while the adaptive weighting strategy addresses task-specific feature importance, mitigating negative transfer and improving scalability. These innovations enable our model to achieve superior results in terms of both performance and efficiency, demonstrating its practical applicability for ADAS.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "F. Ablation Experiment",
      "text": "We conducted a series of ablation experiments to evaluate the individual contributions of the MTS-Mamba and MGMI modules, as well as their key components, and to investigate the interactive effects between tasks and multimodal data.\n\n1) Ablation experiments on MTS-Mamba and MGMI: We designed three experimental configurations to assess the contributions of the MTS-Mamba and MGMI modules. In the first group, we replaced MTS-Mamba with a simple VGG16 network and MGMI with basic concatenation fusion. The results, shown in Table  II , demonstrate significant improvements in Œ≤ macc by 5.2%-12.04% when both MTS-Mamba and MGMI were used. Specifically, replacing VGG16 with MTS-Mamba improved Œ≤ macc by 7.66% and increased FPS by 52.89, while reducing parameters by over four times. Similarly, replacing concatenation fusion with MGMI resulted in a 5.2% increase in Œ≤ macc with only an additional 0.26M parameters. These findings highlight the effectiveness of both MTS-Mamba and MGMI in efficiently extracting temporalspatial features and mitigating task conflicts in multi-task learning (MTL).\n\nWe also evaluated the contribution of the forwardbackward temporal scanning mechanisms and the globallocal spatial feature extraction module in MTS-Mamba.  When either of these components was removed, Œ≤ macc dropped by 3.47%-3.82%, as shown in Table  III . This decline underscores the importance of the synergy between these components in extracting rich spatiotemporal features and synchronizing bidirectional temporal information, providing robust representations for multi-task recognition. Additionally, despite these performance gains, the model maintained its inference speed and did not introduce additional parameters, confirming the efficiency of the MTS-Mamba design.\n\n2) Ablation on Self-Attention and Multi-Gating Mechanisms in MGMI: We analyzed the role of the self-attention and multi-gating mechanisms in MGMI. Removing either of these components led to a noticeable decline in Œ≤ macc , especially when the multi-gating mechanism was excluded (Table  IV ). This result is expected, as the multi-gating mechanism dynamically adjusts the fusion weights of each modality's features according to the task's requirements, enabling selective emphasis on the most relevant modality  for each task. This adjustment alleviates feature conflicts in MTL and significantly improves the model's performance across multiple tasks. The experimental results reinforce the importance of both the self-attention and multi-gating mechanisms in enhancing feature fusion for multi-task learning.\n\n3) Ablation experiments between different tasks: To explore the advantages of MTL and the interactions between different tasks, we designed a series of ablation experiments. The four tasks were grouped into two dimensions: driver state recognition (DER and DBR) and traffic environment recogni-  tion (TCR and VRB). We performed two sets of experiments (Table  V ): we retained only the driver state recognition tasks (DER and DBR), excluding the traffic environment tasks (TCR and VBR). The results showed a drop in Œ± acc for DER and DBR by 1.86%-2.13%. In the second set, we retained only the traffic environment recognition tasks (TCR and VBR), excluding the driver state classification tasks (DER and DBR), which resulted in a more significant Œ± acc drop of 4.82%-5.86%. These results demonstrate that the tasks within the MTL framework benefit from significant synergies. Jointly learning both driver state and traffic environment recognition tasks enhances the model's overall accuracy and generalization capability. 4) Ablation experiments on multimodal data: We validate the independent contributions of each modality through ablation experiments by categorizing the modalities into three groups: vehicle-exterior images (front-view, left-view, rightview), vehicle-interior images (inside-view, driver's facial and body images), and joint data (posture and gesture). We trained the model with each data group separately, and the results are shown in Table  VI .\n\nThe results demonstrate that models trained with a single modality perform worse than the multimodal model, with a drop of 5.82% to 14.18% in Œ≤ macc . This confirms the crucial role of multimodal data in ADAS-related tasks. Further analysis reveals that different modalities benefit different tasks: vehicle-exterior images improve accuracy for TCR and VBR, while vehicle-interior images and joint data enhance the accuracy of DER and DBR tasks. This variation arises because each modality expresses different information-vehicle-exterior images reflect road conditions, while vehicle-interior images and joint data capture the driver's behavior and facial expressions. These findings validate the necessity of our MGMI design, which adaptively adjusts the weights of different modalities to alleviate negative transfer and improve task performance in MTL scenarios.",
      "page_start": 5,
      "page_end": 7
    },
    {
      "section_name": "V. Conclusion",
      "text": "This paper introduces a TEM 3 -Learning (Time-Efficient Multimodal Multi-Task Learning) framework designed to recognize driver emotion, behavior, traffic context, and vehicle behavior simultaneously. TEM 3 -Learning integrates two key components: MTS-Mamba, which efficiently captures temporal-spatial features from multi-view sequential images, and MGMI, which adaptively adjusts the weights of modality features for each task using a multi-gate mechanism. This design alleviates negative transfer between tasks, optimizing performance across multiple recognition tasks. Experimental results on the AIDE dataset demonstrate that TEM 3 -Learning achieves superior performance in all four recognition tasks, with an inference speed exceeding the baseline models, while maintaining fewer than 6 million parameters. These findings highlight the efficiency, scalability, and practical applicability of TEM 3 -Learning in real-time ADAS systems. TEM 3 -Learning and its core components offer a valuable contribution to multimodal multi-task learning in ADAS, paving the way for the development of more efficient and robust algorithms in this field.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison of four mainstream algorithm frameworks: (a) single-",
      "page": 1
    },
    {
      "caption": "Figure 1: (a) and (c)). Many",
      "page": 2
    },
    {
      "caption": "Figure 1: (b)). Therefore, a key challenge lies in effectively",
      "page": 2
    },
    {
      "caption": "Figure 1: (d)). This is achieved through two",
      "page": 2
    },
    {
      "caption": "Figure 2: The overall pipeline of TEM3-Learning. MTS-Mamba and",
      "page": 3
    },
    {
      "caption": "Figure 3: The structure diagram of MTS-Mamba, which includes the forward and backward scanning mechanisms, as well as the global-local spatial feature",
      "page": 4
    },
    {
      "caption": "Figure 4: Structure of the multi-task learning-based gated multimodal feature",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "‚Ä¶": "ùë¥ùüê\nùë¥ùíä\nùë¥ùüè\nùë¥ùíä+ùüè\nùë¥ùüè"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "‚Ä¶\nùë¨ùüè\nùë¨ùíä\nùë¨ùíä+ùüè"
        },
        {
          "‚Ä¶": "ùë¨ùüê"
        },
        {
          "‚Ä¶": "ùë¨ùüè"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "ùìï"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "ùë∂ùüè\nùë∂ùüè"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "(a)\n(b)"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "‚Ä¶"
        },
        {
          "‚Ä¶": "ùë¥ùüê\nùë¥ùíä\nùë¥ùüè\nùë¥ùüè\nùë¥ùíä+ùüè"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "‚Ä¶\nùë¨ùíã‚â™ùíä\nùë¨ùüè"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "ùë¨ùüè"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "ùìï"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "‚Ä¶\n‚Ä¶\nùë∂ùüê\nùë∂ùüè\nùë∂ùíï\nùë∂ùüê\nùë∂ùüè\nùë∂ùíï"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "(c)\n(d)"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": ": Feature Flow\n: Fusion Layer\n: Data Modality\nùìï\nùë¥ùíä"
        },
        {
          "‚Ä¶": ": Fusion Flow\n: Multi-Task Output"
        },
        {
          "‚Ä¶": "ùë∂ùíï\nùë¨ùíã‚â™ùíä : Feature Extractor"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "Fig. 1.\nComparison of four mainstream algorithm frameworks: (a) single-"
        },
        {
          "‚Ä¶": "modal single-task,\n(b) multimodal single-task,\n(c) single-modal multi-task,"
        },
        {
          "‚Ä¶": "and (d)\nthe proposed multimodal multi-task. As\nthe number of\ntasks and"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "modalities increases, model complexity rises significantly, making it crucial"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "to balance computational efficiency with accuracy in multimodal multi-task"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "learning."
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "tionship between the drivers‚Äô\nstate and the traffic environ-"
        },
        {
          "‚Ä¶": "ment\n[36],\n[51]. For example, drivers frequently adjust\ntheir"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "behavior, such as changing lanes based on surrounding traffic"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "conditions, while traffic congestion can induce driver anxiety."
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "Integrating driver\nstate and traffic environment\nrecognition"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "into\na\nunified multi-task\nlearning\n(MTL)\nframework\ncan"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "provide a more holistic understanding of driving scenarios"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "and enhance the safety performance of ADAS [51]."
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "Compared to single-task learning, MTL reduces overfitting"
        },
        {
          "‚Ä¶": "by\nsharing\nfeatures\nacross\ntasks,\nthereby\nimproving\nthe"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "performance of each individual\ntask [9], [24]. In the context"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "of ADAS, existing MTL models generally focus on related"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "sub-tasks. For example, Wu proposed a model\nthat\njointly"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "learns multiple\ntraffic\nenvironment-related\ntasks,\nenabling"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "simultaneous\nrecognition of\nlane markings, drivable\nareas,"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "and object detection in driving scenes [47]. Similarly, Xing"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "et al.\nfocus on driver-related MTL to recognize the driver‚Äôs"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "emotions and behaviors\n[48]. While these approaches have"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "yielded performance improvements,\nthey often struggle with"
        },
        {
          "‚Ä¶": "negative transfer, where task performance deteriorates due to"
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "task-related conflicts or differences [27]."
        },
        {
          "‚Ä¶": ""
        },
        {
          "‚Ä¶": "MTL models tend to have more complex structures com-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "handle multiple tasks\n[47],\n[52]\n(Fig. 1 (a) and (c)). Many",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "[46]. Each modality offers unique advantages and limitations,"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "MTL methods rely on inputs from a single modality such as",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "and leveraging multiple modalities can provide a more com-"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "driving scene images [6], [8]. However, optimal\ntask perfor-",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "prehensive\nunderstanding\nof\nthe\ndriving\nenvironment. For"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "mance requires complementary multimodal inputs [17], [28].",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "example, combining multi-view driving scene images with"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "For example, combining driver\nimages with eye movement",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "LiDAR data enhances environmental perception [26],\n[49]."
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "data\ncan\nimprove\nemotion\nrecognition\n[37], while\nfusing",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "Similarly, integrating driver images and joint information can"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "image and point cloud data can enhance traffic object recog-",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "effectively capture\nfacial\nexpressions\nand behavioral\nstates"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "nition [1], [19]. Unfortunately, many multimodal\ninput algo-",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "[51]. As\na\nresult, many\nstudies\nhave\nincreasingly\nturned"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "rithms still rely on independent feature extraction [23], [29],",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "to multimodal data to enhance task accuracy. For\ninstance,"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "[30],\n[45], which significantly increases model parameters,",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "Zhou\net\nal.\n[55]\nused\nfront-view driving\nscene\nimages,"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "reduces inference speed, and compromises practical scalabil-",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "driver\nimages,\nand vehicle\nspeed data\nfor driver behavior"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "ity (Fig. 1 (b)). Therefore, a key challenge lies in effectively",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "recognition, while Liu et al.\n[33] combined camera images"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "leveraging multimodal\ninformation while\noptimizing\nindi-",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "and LiDAR data for 3D object detection."
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "vidual\ntask performance, enhancing task-level\nsynergy, and",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "However, multimodal\nlearning in ADAS faces\ntwo key"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "maintaining high inference speed with a minimal parameter",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "challenges. First, many existing models rely on independent"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "count.",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "feature\nextraction\nbranches\nfor\neach modality,\neven when"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "To address these challenges, we propose a Time-efficient",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "processing multi-view images\n[17],\n[29],\n[31],\n[42],\n[55]."
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "multimodal multi-task learning network that simultaneously",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "This\napproach significantly increases\nthe model‚Äôs parame-"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "performs\nfour\ntasks ‚Äî driver\nemotion recognition (DER),",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "ter\ncount\nand\noverlooks\npotential\nintermodal\ninteractions,"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "driver behavior recognition (DBR), traffic context recognition",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "leading to inefficient\ninformation use. Second, multimodal"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "(TCR),\nand\nvehicle\nbehavior\nrecognition\n(VBR) ‚Äî using",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "fusion methods are often limited to combining only a few"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "multimodal data (Fig. 1 (d)). This\nis achieved through two",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "modality features,\nrestricting their generalization ability and"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "key\ncomponents:\nthe Mamba-based multi-view temporal-",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "scalability."
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "spatial feature extraction subnetwork (MTS-Mamba) and the",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "multi-task learning-based gated multimodal\nfeature integra-",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "tor\n(MGMI). MTS-Mamba\nintroduces\na\nforward-backward",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "B. Multi-task Learning"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "temporal\nscanning mechanism and\na\nglobal-local\nspatial",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "feature\nextraction strategy,\nenabling efficient\nextraction of",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "MTL improves model performance on each task by shar-"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "temporal-spatial\nfeatures from multi-view sequential\nimages",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "ing\nfeatures\nacross\ntasks\n[9],\n[24]. The\ntwo\npredominant"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "at low computational cost. This approach provides richer and",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "strategies for parameter sharing are hard and soft parameter"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "more\nrobust\nfeature\nrepresentations\nfor\nsubsequent multi-",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "sharing.\nIn\nhard\nparameter\nsharing, most\nparameters\nare"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "task recognition. MGMI,\ninspired by previous work [34],",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "shared across tasks, with task-specific differentiation occur-"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "incorporates\na multi-gating mechanism that\ndynamically",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "ring only in the final\nlayers\n[4],\n[10],\n[25]. For\nexample,"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "adjusts\nthe weights\nof modality\nfeatures\nbased\non\ntask-",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "Wu\net\nal.\n[47]\nadopted\nthis\nstructure\nto\nsimultaneously"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "specific attention. This selective reinforcement of\nimportant",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "achieve traffic object detection, drivable area segmentation,"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "features helps\nalleviate negative\ntransfer. We validated the",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "and lane detection. While simple and efficient,\nthis approach"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "effectiveness\nof\nour method\nusing\nthe\npublicly\navailable",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "is prone to negative transfer when tasks exhibit\nsubstantial"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "AIDE dataset. Experimental\nresults\nshow that\nour model",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "differences, limiting its generalizability. To mitigate this, soft"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "outperforms previous methods on all\nfour\ntasks, with fewer",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "parameter sharing has been proposed, where each task retains"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "than 6M parameters and exceptionally fast\ninference speed,",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "independent parameters but\nleverages\nshared features,\nthus"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "demonstrating its\nefficiency and practicality. Our\ncontribu-",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "preserving task independence and reducing task conflicts [7],"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "tions include:",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "[13],\n[50]. For\ninstance, Choi\net\nal.\n[8]\ndesigned\na\ntask-"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "‚Ä¢ A time-efficient multimodal MTL framework that pro-",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "adaptive\nattention generator\nto enable\nindependent param-"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "vides a new paradigm for multimodal MTL in ADAS.",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "eters for tasks like monocular 3D object detection, semantic"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "‚Ä¢ The MTS-Mamba\nsubnetwork, which\neffectively\nex-",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "segmentation, and dense depth estimation."
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "tracts temporal-spatial features from multi-view sequen-",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "tial\nimages across multiple dimensions.",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "Although\nsoft\nparameter\nsharing\nalleviates\ntask\ncon-"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "‚Ä¢ The MGMI mechanism, which adaptively adjusts\nat-",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "flicts,\ntwo critical challenges remain. First,\nintroducing task-"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "tention to different modalities for each task, mitigating",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "specific parameters increases the model‚Äôs flexibility but sig-"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "negative\ntransfer\nand\nenhancing\ntask-specific\nfeature",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "nificantly raises the parameter count, which can hinder real-"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "extraction.",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "time performance ‚Äî a major\nconcern in applications\nlike"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "ADAS, where real-time processing is crucial. Second, exist-"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "II. RELATED WORK",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "ing MTL models in ADAS are predominantly designed for"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "A. Multimodal Learning",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": ""
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "single-modality inputs (e.g., driving scene images),\nlimiting"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "ADAS systems\nthat\nrely solely on single-modality data",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "their applicability to multimodal scenarios and constraining"
        },
        {
          "pared to single-task learning,\nas\nthey must\nsimultaneously": "often struggle to handle the complex and dynamic challenges",
          "encountered during driving [1],\n[3],\n[12],\n[15],\n[37],\n[40],": "the full potential of MTL-based solutions."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Vehicle-\nMTS-": "Multi-task Recognition",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "exterior",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "Mamba",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "Images",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "capture driver behavior (e.g.,\nlooking around, making calls),"
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "while multi-view vehicle-exterior\nimages\nprovide\na\ncom-"
        },
        {
          "Vehicle-\nMTS-": "Multi-task Learning-",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "MTS-\nMamba\nDriver\nDriver\nEmotion\nBehavior\nVehicle-\nbased Gated Multimodal",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "prehensive understanding of surrounding environments (e.g.,"
        },
        {
          "Vehicle-\nMTS-": "interior",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "Feature Integrator",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "pedestrians, vehicles, obstacles). Leveraging these temporal-"
        },
        {
          "Vehicle-\nMTS-": "Images",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "spatial\nfeatures is crucial\nfor enhancing ADAS performance"
        },
        {
          "Vehicle-\nMTS-": "Vehicle \nTraffic \n3D CNN",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "in environmental perception and behavior recognition. How-"
        },
        {
          "Vehicle-\nMTS-": "Driver",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "Behavior\nContext",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "Joints",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "ever,\nin real-world driving scenarios ‚Äî especially in com-"
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "plex scenarios\nlike\ncongested intersections where dynamic"
        },
        {
          "Vehicle-\nMTS-": "Fig.\n2.\nThe\noverall\npipeline\nof\nTEM3-Learning. MTS-Mamba\nand",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "targets\nand\ncomplex movement\npatterns\nare\nprevalent ‚Äî"
        },
        {
          "Vehicle-\nMTS-": "3D CNN are\nused\nto\nextract multimodal\nfeatures\nfrom vehicle-exterior",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "modeling these\nfeatures becomes\nincreasingly challenging."
        },
        {
          "Vehicle-\nMTS-": "images, vehicle-interior\nimages, and driver\njoints,\nrespectively. The multi-",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "The difficulty arises from the drastic environmental changes"
        },
        {
          "Vehicle-\nMTS-": "task learning-based gated multimodal feature integrator (MGMI) adaptively",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "fuses these features, enabling multi-task recognition.",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "and\nthe\nneed\nto\nbalance modeling\nquality with\nreal-time"
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "performance. CNNs offer good real-time performance but"
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "are\nlimited\nby\ntheir\nlocal\nreceptive field, which\nhampers"
        },
        {
          "Vehicle-\nMTS-": "III. METHODOLOGY",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "the\ncapture\nof\nglobal\ntemporal-spatial\nfeatures\nand,\ncon-"
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "sequently,\ntask\nrecognition\naccuracy. On\nthe\nother\nhand,"
        },
        {
          "Vehicle-\nMTS-": "This section presents the overall structure and key modules",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "attention-based methods, while capable of global modeling,"
        },
        {
          "Vehicle-\nMTS-": "of\nthe proposed TEM3-Learning network. We first describe",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "suffer\nfrom high\ncomputational\ncomplexity, making\nthem"
        },
        {
          "Vehicle-\nMTS-": "the overall architecture of\nthe network and then detail\ntwo",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "unsuitable\nfor processing large volumes of multi-view se-"
        },
        {
          "Vehicle-\nMTS-": "core modules: MTS-Mamba and MGMI.",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "quential\ndata\nin\nreal-time ADAS applications. To\naddress"
        },
        {
          "Vehicle-\nMTS-": "A. Network Overview",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "these challenges, we propose MTS-Mamba, which combines"
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "a dual-path temporal-spatial feature extraction structure with"
        },
        {
          "Vehicle-\nMTS-": "To fully leverage the synergies between multimodal and",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "a State Space Model\n(SSM) based on Mamba\n[16]. This"
        },
        {
          "Vehicle-\nMTS-": "multi-view data while balancing accuracy and efficiency in a",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "approach\nefficiently\ncaptures multi-view sequential\nimage"
        },
        {
          "Vehicle-\nMTS-": "MTL context, our proposed TEM3-Learning network (Fig. 2)",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "features while maintaining low computational\ncost,\nensur-"
        },
        {
          "Vehicle-\nMTS-": "consists of\nthree core components:\nthe multi-branch feature",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "ing a balance between recognition accuracy and real-time"
        },
        {
          "Vehicle-\nMTS-": "extraction layer, MGMI, and the multi-task recognition layer.",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "performance."
        },
        {
          "Vehicle-\nMTS-": "The first component,\nthe multi-branch feature extraction",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "layer,\nprocesses multimodal\ndata\nthrough\ntwo modules:",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "Before MTS-Mamba, we need to process multi-view se-"
        },
        {
          "Vehicle-\nMTS-": "MTS-Mamba\nand\n3D CNN. MTS-Mamba\nextracts\ndeep",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "quential\nimages. First,\nthe sequential\nimages of the j-th view"
        },
        {
          "Vehicle-\nMTS-": "temporal-spatial features from vehicle-exterior images (front,",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "are\nconcatenated along the\nchannel dimension,\nforming a"
        },
        {
          "Vehicle-\nMTS-": "left,\nand\nright\nviews)\nand\nvehicle-interior\nimages\n(inside-",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "is then processed through deep convo-\nsequence Ij. Each Ij"
        },
        {
          "Vehicle-\nMTS-": "view, driver‚Äôs facial and body images). The 3D CNN module",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "lution and adaptive average pooling to extract\ninitial features"
        },
        {
          "Vehicle-\nMTS-": "focuses on extracting high-level semantic features from the",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "of\nidentical dimensions. These\nfeatures\nare\nintegrated into"
        },
        {
          "Vehicle-\nMTS-": "driver‚Äôs posture\nand gestures. These\nextracted features\nare",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "a\n‚àà RC√óH√óW , where C, H,\nand W\nfeature map Ff"
        },
        {
          "Vehicle-\nMTS-": "then integrated by MGMI, which first uses a self-attention",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "represent\nthe channel count, height, and width,\nrespectively."
        },
        {
          "Vehicle-\nMTS-": "mechanism to\nobtain\ntask-shared\nfeatures,\nfollowed\nby\na",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "This feature map is subsequently input into the MTS-Mamba"
        },
        {
          "Vehicle-\nMTS-": "multi-gating mechanism to adaptively weight\nfeatures based",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "module for\nfurther processing."
        },
        {
          "Vehicle-\nMTS-": "on task-specific attention,\nthus improving feature extraction",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "and alleviating conflicts between tasks.",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "For\nthe input\nfeature map Ff , we first apply 1D convo-"
        },
        {
          "Vehicle-\nMTS-": "During training, we use a cross-entropy loss function that",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "lution with GELU activation to enhance feature representa-"
        },
        {
          "Vehicle-\nMTS-": "integrates individual task losses to optimize the overall model",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "tion within the dual-path temporal-spatial\nfeature extraction"
        },
        {
          "Vehicle-\nMTS-": "performance. The total\nis computed as:\nloss Ltotal",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "structure,i.e., global and local\nlayers. The local and global"
        },
        {
          "Vehicle-\nMTS-": "(cid:16)\n(cid:17)",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "layers employ forward and backward scanning, respectively,"
        },
        {
          "Vehicle-\nMTS-": "m(cid:88) r\n,\nCrossEntropy\n(1)\nyr, yr\nLtotal =",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "to extract bidirectional\ntemporal features. Specifically, given"
        },
        {
          "Vehicle-\nMTS-": "=1",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "spans 16 consecutive\nframes,\nthe\nlocal\nlayer per-\nthat Ff"
        },
        {
          "Vehicle-\nMTS-": "where ÀÜyr denotes the recognition results for\ntask r, and yr",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "is\nlinearly\nform a forward scan where each channel of Ff"
        },
        {
          "Vehicle-\nMTS-": "denotes the corresponding ground truth. The number of tasks",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "transformed by the\nstate parameters B ¬∑ C‚ä§ of\nthe\nstate-"
        },
        {
          "Vehicle-\nMTS-": "m is\nset\nto 4, corresponding to the four\ntasks: driver emo-",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "space model, capturing forward temporal dependencies.\nIn"
        },
        {
          "Vehicle-\nMTS-": "tion recognition, driver behavior\nrecognition,\ntraffic context",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "the global\nlayer,\nis\nreversed during\nthe spatial order of Ff"
        },
        {
          "Vehicle-\nMTS-": "recognition, and vehicle behavior\nrecognition.",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "the backward scanning to capture temporal interdependencies"
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "from both directions. Then,\nit\nis\nlinearly transformed with"
        },
        {
          "Vehicle-\nMTS-": "B. Mamba-based Multi-view Temporal-spatial Feature Ex-",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "B ¬∑ C‚ä§. During training,\nthe shared state parameters B and"
        },
        {
          "Vehicle-\nMTS-": "traction Subnetwork (MTS-Mamba)",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": ""
        },
        {
          "Vehicle-\nMTS-": "",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "C are updated via backpropagation to retain bidirectional"
        },
        {
          "Vehicle-\nMTS-": "Multi-view sequential\nimages from both vehicle interiors",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "temporal\ninformation.\nThe\nSSM calculates\nthe\ntemporal"
        },
        {
          "Vehicle-\nMTS-": "and\nexteriors\nexhibit\nstrong\ntemporal-spatial\ncorrelations.",
          "For\ninstance,\nsequential vehicle-interior\nimages\neffectively": "feature weight Wssm as:"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "Scanning\nAvgPool"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "ùêÖùüë\nAvgPool"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "Global Layer"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "Multi-View"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "Sequential Images"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "Fig. 3.\nThe structure diagram of MTS-Mamba, which includes the forward and backward scanning mechanisms, as well as the global-local spatial feature"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "extraction strategy."
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "features H1, H2 ‚àà RC√óH√óW extracted by MTS-Mamba"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "‚àÇL"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "with\nthe\nfrom the\n3D CNN module\nalong\nfeatures H3\n,\nB(s+1) = B(s) ‚àí Œ∑ ¬∑"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "Ô£±Ô£¥Ô£≤ Ô£¥Ô£≥\n‚àÇB(s)"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "the\nchannel\ndimension\nto\nobtain\nthe\ninitial\nfused\nfeature.\n(2)"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "‚àÇL"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "This\nfused\nfeature\nundergoes\nthree\nseparate\nconvolution\nC(s+1) = C(s) ‚àí Œ∑ ¬∑\n,"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "‚àÇC(s)"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "operations\nto produce query (Q), key (K), and value (V),"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "respectively. The computation process is as follows:\n(3)\nWssm = œÉ (cid:0)A ¬∑ dstate + B ¬∑ C‚ä§ ¬∑ ddim + D(cid:1) ,"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "where B(s+1) denotes the updated shared weight parameter,"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "(5)\nQ, K, V = Wq,k,v\n(cid:0)Concat(H1, H2, H3)(cid:1),"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "Œ∑\nis\nthe\nlearning rate, œÉ(¬∑)\nis\nthe\nsigmoid function, A ‚àà"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "RC√ón\nThe\nattention matrix\nis\ncomputed\nby\nperforming\na\ndot"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "is\nthe\nstate\ntransition matrix where n indicates\nthe"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "product between Q and K,\nand multiplying the\nresulting\nstate dimension, dstate and ddim are the state unit vector and"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "attention matrix by V to obtain the weighted attention scores,\nchannel unit vector, and D ‚àà RC is the bias vector."
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "which\nare\nreshaped\nback\nto RC√óH√óW to\nyield\nthe\ntask-\nAfter\ncapturing\nthe\nbidirectional\ntemporal\nfeatures,\nthe"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "shared features.\nlocal\nlayer uses 3√ó3 average pooling and linear projection"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "To adapt\nto each task‚Äôs specific focus, we design a multi-\nto extract\nlocal\nspatial\nthe global\nlayer\nfeatures Fl, while"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "gating mechanism. The\ntask-shared features\nare\ninput\ninto\nuses adaptive average pooling and linear projection to capture"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "four\ntask-specific\ngating\nunits,\none\nfor\neach\ntask, which\nglobal spatial features Fg. This captures detailed features and"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "perform convolution, BatchNorm, and Sigmoid operations to\nglobal context\nfrom spatial\ninformation at different\nscales,"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "calculate attention weights for\nthe multimodal\nproviding more comprehensive and rich spatial\nfeature rep-\nfeatures H1,"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "resentations. We then merge the multi-scale spatial\nfeatures\nH2, and H3. A weighted sum of these features is calculated"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "to produce the task-specific feature Fr, where r ‚àà {1, 2, 3, 4}\nFl and Fg and multiply them by the temporal feature weight"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "is\nrepresents each task. The task-specific feature Fr\ninformation Wssm, combining temporal dynamics with spatial"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "structure. A residual\nconnection\nis\nthen\nused\nto\ncombine"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "the merged\nfeatures with\nthe\noriginal\ninput\nfeatures Ff ,"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "(cid:19)\n(cid:18)"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "Q ¬∑ K‚ä§"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "3(cid:88) i\nproducing the final output\nfeature Fo:\n‚àö\n) ¬∑ V(cid:1)(cid:1)\n,\nBN(cid:0)Conv2D(cid:0)softmax(\nFr =\nHi‚äôœÉi\nr"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "d"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "=1"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "(6)\n(4)\nFo = Ff + Œ≥ ¬∑ LN(cid:0)Wssm ‚äô (Fl + Fg)(cid:1),"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "where œÉi\nis\nthe\ni-th\ngating Sigmoid\nfunction\nfor\ntask\nr,\nr"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "where Œ≥ is\nthe scaling factor, LN denotes\nlinear\nlayer, and"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "and BN denotes\nthe\nbatch\nnormalization\noperation. This"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "‚äô denotes element-wise multiplication."
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "design optimizes\nthe\nfeature\nfusion process\nfor\neach task"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "In summary, MTS-Mamba‚Äôs unique dual-path temporal-"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "by dynamically adjusting the importance of each modality‚Äôs"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "spatial\nfeature extraction structure effectively captures both"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "features, allowing the model\nto extract\ntask-specific features"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "multi-scale spatial features and bidirectional temporal depen-"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "while retaining shared characteristics. By alleviating negative"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "dencies. By introducing forward-backward scanning mech-"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "transfer\nacross\ntasks, MGMI\nenhances\nthe performance of"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "anisms, MTS-Mamba achieves a balance between accuracy"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "each task within the multi-task learning framework. Once"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "in temporal-spatial\nfeature modeling and computational ef-"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "multimodal\nfeatures are fused for each task,\nthe multi-task"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "ficiency,\nsignificantly\nenhancing\nreal-time\nperformance\nin"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "recognition layer applies independent pooling for each task‚Äôs"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "ADAS applications."
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "and inputs\nthem into their\nrespective classifiers,\nfeature Fr"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "for each task.\nproducing the prediction ÀÜyr"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "C. Multi-task Gated Multimodal\nIntegrator (MGMI)"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "We propose\nthe Multi-task Gated Multimodal\nIntegrator"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "IV. EXPERIMENTS"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "(MGMI), which introduces task-specific gating mechanisms"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "A. Dataset"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "to dynamically adjust\nthe fusion weights of different modal-"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "ity features, enabling task-driven feature selection and alle-\nThe AIDE dataset\nis an open-source collection designed"
        },
        {
          "Conv 2D\nConv 1D\nGELU\nLinear": "viating task conflicts. Specifically, we first concatenate the\nto\nadvance ADAS\nresearch,\nconsisting\nof\n2,898\nsamples"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "BatchNorm\nConv 3*1"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "MatMul"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "ùíóùíÇùíçùíñùíÜ\nSigmoid"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "Input"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "Conv 3*1\nOutput"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "(a) Self-Attention\n(b) Gate i"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "Fig. 4.\nStructure of the multi-task learning-based gated multimodal feature"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "integrator\n(MGMI)."
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "of\ntime-series multimodal\ndata,\nincluding multi-view im-"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "ages and driver\njoint positions. The multi-view images are"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "captured from four perspectives: front-view,\nleft-view, right-"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "view, and inside-view. Each sample is annotated with labels"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "for\nfour\ntasks: driver emotion recognition (DER), driver be-"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "havior\nrecognition (DBR),\ntraffic context\nrecognition (TCR),"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "and vehicle behavior\nrecognition (VBR). The dataset\nis split"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "into training,\ntesting, and validation sets with proportions of"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "65%, 15%, and 20%,\nrespectively,\nto ensure robust evalua-"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "tion."
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "B. Data Preprocessing"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "The preprocessing pipeline follows three sequential steps:"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "extraction of driver\nfacial and body images, data synchro-"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "nization, and data augmentation. First, driver facial and body"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "regions\nare\ncropped using bounding box coordinates\nfrom"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "inside-view images. The multimodal data (images and joint"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "positions) is synchronized at 16 frames per second, ensuring"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "temporal alignment of features across modalities. Each input"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "sequence\nis\nformed\nfrom 16\nconsecutive\nframes\nto\ncap-"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "ture temporal dependencies. To further augment\nthe dataset,"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "multi-view images are subjected to random horizontal and"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "vertical flips\n(50% probability),\nenhancing the diversity of"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "the training set and improving model\nrobustness."
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "C. Evaluation Metrics"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "To evaluate the model performance, we use the following"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "frames\nkey metrics: accuracy (Œ±acc), mean accuracy (Œ≤macc),"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "per second (FPS), and total parameter count (Params). Œ≤macc"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "represents the average accuracy across multiple tasks, provid-"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "ing an overall performance measure for multi-task learning."
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": "It\nis computed as:"
        },
        {
          "MatMul\nSoftMax\nùíåùíÜùíö": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "All\nexperiments were\nconducted\non\nan NVIDIA L40S"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "GPU with 48GB VRAM. Training was performed for 125"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "epochs, with\nstabilization\nof\nthe\noutcomes\nguiding\nearly"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "stopping. The\ninitial\nlearning\nrate was\nset\nto\n0.001\nand"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "dynamically adjusted: reduced to 0.0005 between epochs 25"
        },
        {
          "D. Experimental Details": "and 50, and further decreased to 0.00005 after epoch 50. The"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "batch size was 24, with Stochastic Gradient Descent\n(SGD)"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "used as\nthe optimization algorithm,\nfeaturing a momentum"
        },
        {
          "D. Experimental Details": "of 0.9 and weight decay of 0.0001 to ensure\nconvergence"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "while mitigating overfitting."
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "E. Comparison with SOTA Models"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "We\ncompared\nour model with\nstate-of-the-art methods,"
        },
        {
          "D. Experimental Details": "following the experimental setup from [51], which involves"
        },
        {
          "D. Experimental Details": "independent\nfeature\nextraction\nfrom multimodal\ndata\nand"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "multi-view images. Our model\nconsistently\noutperformed"
        },
        {
          "D. Experimental Details": "these baseline models across all evaluation metrics, including"
        },
        {
          "D. Experimental Details": "and\nparameter\ncount. With\ntask-specific Œ±acc, Œ≤macc, FPS,"
        },
        {
          "D. Experimental Details": "fewer\nthan 6M parameters, our model\nby\nimproved Œ≤macc"
        },
        {
          "D. Experimental Details": "3.48%-9.32% and\nachieved\nan\ninference\nspeed\nof\n142.32"
        },
        {
          "D. Experimental Details": "FPS,\nsignificantly surpassing the real-time requirements of"
        },
        {
          "D. Experimental Details": "ADAS systems."
        },
        {
          "D. Experimental Details": "These\nperformance\nimprovements\nare\nattributed\nto\nthe"
        },
        {
          "D. Experimental Details": "model‚Äôs design optimizations. Specifically,\nthe joint\nfeature"
        },
        {
          "D. Experimental Details": "extraction\nacross\nsimilar modalities\nreduces model\ncom-"
        },
        {
          "D. Experimental Details": "plexity\nand\nenhances\ninter-modal\ninteractions, while\nthe"
        },
        {
          "D. Experimental Details": "adaptive weighting strategy addresses\ntask-specific\nfeature"
        },
        {
          "D. Experimental Details": "importance, mitigating negative transfer and improving scal-"
        },
        {
          "D. Experimental Details": "ability. These innovations enable our model\nto achieve su-"
        },
        {
          "D. Experimental Details": "perior\nresults\nin terms of both performance and efficiency,"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "demonstrating its practical applicability for ADAS."
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "F\n. Ablation Experiment"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "We conducted a series of ablation experiments to evaluate"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "the individual contributions of the MTS-Mamba and MGMI"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "modules, as well as their key components, and to investigate"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "the interactive effects between tasks and multimodal data."
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "1) Ablation experiments on MTS-Mamba and MGMI:"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "We designed three experimental configurations to assess the"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "contributions of the MTS-Mamba and MGMI modules. In the"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "first group, we replaced MTS-Mamba with a simple VGG16"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "network and MGMI with basic\nconcatenation fusion. The"
        },
        {
          "D. Experimental Details": "results, shown in Table II, demonstrate significant\nimprove-"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "ments\nby 5.2%-12.04% when both MTS-Mamba\nin Œ≤macc"
        },
        {
          "D. Experimental Details": "and MGMI were used. Specifically,\nreplacing VGG16 with"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "MTS-Mamba improved Œ≤macc by 7.66% and increased FPS"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "by 52.89, while reducing parameters by over four times. Sim-"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "ilarly,\nreplacing concatenation fusion with MGMI\nresulted"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "in a 5.2% increase in Œ≤macc with only an additional 0.26M"
        },
        {
          "D. Experimental Details": "parameters. These findings highlight the effectiveness of both"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "MTS-Mamba and MGMI\nin efficiently extracting temporal-"
        },
        {
          "D. Experimental Details": "spatial\nfeatures\nand mitigating task conflicts\nin multi-task"
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "learning (MTL)."
        },
        {
          "D. Experimental Details": ""
        },
        {
          "D. Experimental Details": "We\nalso\nevaluated\nthe\ncontribution\nof\nthe\nforward-"
        },
        {
          "D. Experimental Details": "backward\ntemporal\nscanning mechanisms\nand\nthe\nglobal-"
        },
        {
          "D. Experimental Details": "local\nspatial\nfeature\nextraction module\nin MTS-Mamba."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SECOND-BEST RESULTS ARE": "",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "MILLIONS, TE REPRESENTS TEMPORAL EMBEDDING, TRANSE REPRESENTS TRANSFORMER ENCODER [44].",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": ""
        },
        {
          "SECOND-BEST RESULTS ARE": "",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "Backbone",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "Œ±acc(%) ‚Üë"
        },
        {
          "SECOND-BEST RESULTS ARE": "",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": ""
        },
        {
          "SECOND-BEST RESULTS ARE": "",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": ""
        },
        {
          "SECOND-BEST RESULTS ARE": "Multi-view Scene Images",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "Driver\nImages",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "DBR"
        },
        {
          "SECOND-BEST RESULTS ARE": "VGG16 [41]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "VGG16 [41]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "64.57"
        },
        {
          "SECOND-BEST RESULTS ARE": "Res18 [21]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "Res18 [21]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "64.33"
        },
        {
          "SECOND-BEST RESULTS ARE": "",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": ""
        },
        {
          "SECOND-BEST RESULTS ARE": "CMT [18]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "CMT [18]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "68.75"
        },
        {
          "SECOND-BEST RESULTS ARE": "GLMDriveNet\n[28]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "GLMDriveNet\n[28]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "66.57"
        },
        {
          "SECOND-BEST RESULTS ARE": "PP-Res18+TransE [44]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "Res18/34 [21]+TransE [44]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "67.32"
        },
        {
          "SECOND-BEST RESULTS ARE": "Res34 [21]+TransE [44]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "Res18/34 [21]+TransE [44]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "67.08"
        },
        {
          "SECOND-BEST RESULTS ARE": "",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": ""
        },
        {
          "SECOND-BEST RESULTS ARE": "Res50 [21]+TransE [44]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "Res34/50 [21]+TransE [44]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "65.65"
        },
        {
          "SECOND-BEST RESULTS ARE": "",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": ""
        },
        {
          "SECOND-BEST RESULTS ARE": "VGG16 [41]+TransE [44]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "VGG13/16 [41]+TransE [44]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "67.15"
        },
        {
          "SECOND-BEST RESULTS ARE": "VGG19 [41]+TransE [44]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "VGG16/19 [41]+TransE [44]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "65.48"
        },
        {
          "SECOND-BEST RESULTS ARE": "3D-Res34 [20]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "3D-Res34 [20]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "63.05"
        },
        {
          "SECOND-BEST RESULTS ARE": "MobileNet-V1-3D [22]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "MobileNet-V1-3D [22]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "64.20"
        },
        {
          "SECOND-BEST RESULTS ARE": "MobileNet-V2-3D [39]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "MobileNet-V2-3D [39]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "61.74"
        },
        {
          "SECOND-BEST RESULTS ARE": "ShuffleNet-V1-3D [53]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "ShuffleNet-V1-3D [53]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "68.97"
        },
        {
          "SECOND-BEST RESULTS ARE": "ShuffleNet-V2-3D [35]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "ShuffleNet-V2-3D [35]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "64.04"
        },
        {
          "SECOND-BEST RESULTS ARE": "",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": ""
        },
        {
          "SECOND-BEST RESULTS ARE": "C3D [43]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "C3D [43]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "63.95"
        },
        {
          "SECOND-BEST RESULTS ARE": "I3D [5]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "I3D [5]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "66.17"
        },
        {
          "SECOND-BEST RESULTS ARE": "SlowFast\n[11]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "SlowFast\n[11]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "61.58"
        },
        {
          "SECOND-BEST RESULTS ARE": "TimeSFormer\n[2]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "TimeSFormer\n[2]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "65.18"
        },
        {
          "SECOND-BEST RESULTS ARE": "Video Swin Transformer\n[32]",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "Video Swin Transformer\n[32]",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "65.63"
        },
        {
          "SECOND-BEST RESULTS ARE": "MTS-Mamba",
          ", AND OUR METHOD IS HIGHLIGHTED WITH": "MTS-Mamba",
          ". P(M) REPRESENTS THE NUMBER OF PARAMETERS IN": "69.31"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "INDICATES THAT THE CORRESPONDING COMPONENT IS USED, WHILE",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "TEMPORAL SCANNING MECHANISMS AND THE GLOBAL-LOCAL SPATIAL"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "‚ÄùW/O‚Äù DENOTES THAT THE COMPONENT IS NOT USED.",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "FEATURE EXTRACTION MODULE IN MTS-MAMBA."
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "Global-\nDual-path\nŒ±acc(%) ‚Üë\nŒ≤macc"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "FPS ‚Üë\nP(M) ‚Üì"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "MTS-\nŒ±acc(%) ‚Üë\nŒ≤macc",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "(%) ‚Üë\nLocal\nScanning"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "MGMI\nFPS ‚Üë\nP(M) ‚Üì",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "DER\nDBR\nTCR\nVBR"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "(%) ‚Üë\nMamba\nDER\nDBR\nTCR\nVBR",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": ""
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "5.99\nw/\nw/o\n73.15\n66.85\n91.21\n80.22\n77.86\n146.59"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "w/o\nw/o\n67.38\n58.75\n83.06\n69.38\n69.64\n101.83\n27.68",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "148.84\n5.99\nw/o\nw/\n72.78\n67.30\n92.11\n80.65\n78.21"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "158.84\n5.73\nw/\nw/o\n72.13\n64.57\n92.05\n77.16\n76.48",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": ""
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "75.00\n69.31\n96.29\n86.11\n81.68\n5.99\nw/\nw/\n142.32"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "w/o\nw/\n70.54\n62.31\n90.37\n73.84\n74.02\n89.43\n28.12",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": ""
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "75.00\n69.31\n96.29\n86.11\n81.68\nw/\nw/\n142.32\n5.99",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": ""
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "TABLE IV"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "ABLATION EXPERIMENT RESULTS OF THE SELF-ATTENTION AND"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "MULTI-GATING MECHANISMS IN MGMI."
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "When\neither\nof\nthese\ncomponents was\nremoved,\nŒ≤macc",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": ""
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "Self-\nMulti-gating\nŒ±acc(%) ‚Üë\nŒ≤macc"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "dropped by 3.47%-3.82%, as shown in Table III. This decline",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": ""
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "(%) ‚Üë\nAttention\nMechanism\nDER\nDBR\nTCR\nVBR"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "underscores\nthe\nimportance of\nthe\nsynergy between these",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": ""
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "w/o\nw/o\n72.13\n64.57\n92.05\n77.16\n76.48"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "components\nin extracting rich spatiotemporal\nfeatures\nand",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "w/\nw/o\n74.60\n65.84\n93.29\n84.75\n79.62"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "synchronizing bidirectional\ntemporal\ninformation, providing",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "w/o\nw/\n73.53\n64.92\n91.99\n82.39\n78.21"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "robust\nrepresentations\nfor multi-task recognition. Addition-",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": "75.00\n69.31\n96.29\n86.11\n81.68\nw/\nw/"
        },
        {
          "ABLATION EXPERIMENT RESULTS OF MTS-MAMBA AND MGMI. ‚ÄùW/‚Äù": "ally, despite these performance gains,\nthe model maintained",
          "ABLATION EXPERIMENT RESULTS OF THE FORWARD-BACKWARD": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tion (TCR and VRB). We performed two sets of experiments": "(Table V): we retained only the driver state recognition tasks",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "(DER and DBR), excluding the traffic environment tasks (TCR",
          "robust algorithms in this field.": "REFERENCES"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "and VBR). The results\nfor DER and\nshowed a drop in Œ±acc",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "[1]\nSimegnew Yihunie Alaba, Ali C Gurbuz, and John E Ball. Emerging"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "DBR by 1.86%-2.13%.\nIn the second set, we retained only",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "trends\nin autonomous vehicle perception: Multimodal\nfusion for 3d"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "the\ntraffic\nenvironment\nrecognition\ntasks\n(TCR and VBR),",
          "robust algorithms in this field.": "object detection. World Electric Vehicle Journal, 15(1):20, 2024."
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "[2] Gedas Bertasius, Heng Wang, and Lorenzo Torresani.\nIs\nspace-time"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "excluding the driver state classification tasks (DER and DBR),",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "International\nattention\nall\nyou\nneed\nfor\nvideo\nunderstanding?\nIn"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "which resulted in a more\ndrop of 4.82%-\nsignificant Œ±acc",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "Conference on Machine Learnin (ICML), page 4, 2021."
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "5.86%. These results demonstrate that\nthe tasks within the",
          "robust algorithms in this field.": "[3] Han Bi, Ge Yu, Yu He, Wenzhuo Liu, and Zijie Zheng. Vm-bhinet:"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "MTL framework benefit\nfrom significant\nsynergies.\nJointly",
          "robust algorithms in this field.": "Vision mamba bimanual hand interaction network for 3d interacting"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "hand mesh recovery from a single rgb image. Proceedings of the ACM"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "learning both driver state and traffic environment recognition",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "on Computer Graphics and Interactive Techniques, 8(1):1‚Äì16, 2025."
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "tasks enhances the model‚Äôs overall accuracy and generaliza-",
          "robust algorithms in this field.": "[4] Kaidi Cao,\nJiaxuan You,\nand Jure Leskovec.\nRelational multi-task"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "tion capability.",
          "robust algorithms in this field.": "arXiv preprint\nlearning: Modeling relations between data and tasks."
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "arXiv:2303.07666, 2023."
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "4) Ablation experiments on multimodal data: We validate",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "[5]\nJoao Carreira and Andrew Zisserman. Quo vadis, action recognition?"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "the independent contributions of each modality through ab-",
          "robust algorithms in this field.": "a new model and the kinetics dataset. In proceedings of the IEEE/CVF"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "Conference\non Computer Vision\nand Pattern Recognition\n(CVPR),"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "lation experiments by categorizing the modalities into three",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "pages 6299‚Äì6308, 2017."
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "groups: vehicle-exterior\nimages (front-view,\nleft-view,\nright-",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "[6]\nSihan Chen, Lianqing Zheng, Libo Huang, Jie Bai, Xichan Zhu, and"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "view),\nvehicle-interior\nimages\n(inside-view,\ndriver‚Äôs\nfacial",
          "robust algorithms in this field.": "Zhixiong Ma. Umt-net: A uniform multi-task network with adaptive"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "task weighting.\nIEEE Transactions on Intelligent Vehicles, 9(1):2304‚Äì"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "and body images), and joint data (posture and gesture). We",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "2317, 2023."
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "trained the model with each data group separately, and the",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "[7]\nTianlong Chen, Xuxi Chen, Xianzhi Du, Abdullah Rashwan, Fan"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "results are shown in Table VI.",
          "robust algorithms in this field.": "Yang, Huizhong Chen, Zhangyang Wang,\nand Yeqing Li.\nAdamv-"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "moe: Adaptive multi-task vision mixture-of-experts.\nIn Proceedings"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "The results demonstrate that models trained with a single",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "of\nthe IEEE/CVF International Conference on Computer Vision, pages"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "modality perform worse\nthan the multimodal model, with",
          "robust algorithms in this field.": "17346‚Äì17357, 2023."
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "[8] Wonhyeok Choi, Mingyu Shin, Hyukzae Lee, Jaehoon Cho, Jaehyeon"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "a\ndrop\nof\n5.82% to\nconfirms\nthe\n14.18% in Œ≤macc. This",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "Park, and Sunghoon Im. Multi-task learning for real-time autonomous"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "crucial\nrole\nof multimodal\ndata\nin ADAS-related\ntasks.",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "driving leveraging task-adaptive\nattention generator.\nIn 2024 IEEE"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "Further\nanalysis\nreveals\nthat\ndifferent modalities\nbenefit",
          "robust algorithms in this field.": "International Conference on Robotics and Automation (ICRA), pages"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "different\ntasks:\nvehicle-exterior\nimages\nimprove\naccuracy",
          "robust algorithms in this field.": "14732‚Äì14739.\nIEEE, 2024."
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "[9]\nSauhaarda Chowdhuri, Tushar Pankaj,\nand Karl Zipser.\nMultinet:"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "for\nTCR and VBR, while vehicle-interior\nimages\nand joint",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "2019\nMulti-modal multi-task\nlearning\nfor\nautonomous\ndriving.\nIn"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "DER\nDBR\ndata\nenhance\nthe\naccuracy\nof\nand\ntasks. This",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "IEEE Winter Conference on Applications of Computer Vision (WACV),"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "variation arises because\neach modality expresses different",
          "robust algorithms in this field.": "pages 1496‚Äì1504.\nIEEE, 2019."
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "[10]\nJialei Cui,\nJianwei Du, Wenzhuo Liu, and Zhouhui Lian.\nTextnerf:"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "information‚Äîvehicle-exterior images reflect road conditions,",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "A novel scene-text\nimage synthesis method based on neural\nradiance"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "while\nvehicle-interior\nimages\nand\njoint\ndata\ncapture\nthe",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "the\nIEEE/CVF Conference on Computer\nfields.\nIn Proceedings of"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "driver‚Äôs behavior and facial expressions. These findings vali-",
          "robust algorithms in this field.": "Vision and Pattern Recognition, pages 22272‚Äì22281, 2024."
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "[11] Christoph Feichtenhofer, Haoqi Fan,\nJitendra Malik,\nand Kaiming"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "date the necessity of our MGMI design, which adaptively ad-",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "of\nHe.\nSlowfast\nnetworks\nfor\nvideo\nrecognition.\nIn Proceedings"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "justs the weights of different modalities to alleviate negative",
          "robust algorithms in this field.": ""
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "",
          "robust algorithms in this field.": "the IEEE/CVF International Conference on Computer Vision (ICCV),"
        },
        {
          "tion (TCR and VRB). We performed two sets of experiments": "transfer and improve task performance in MTL scenarios.",
          "robust algorithms in this field.": "pages 6202‚Äì6211, 2019."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "ABLATION EXPERIMENTAL RESULTS FOR DRIVER STATE RECOGNITION"
        },
        {
          "TABLE V": "TASKS (I.E., DER, DBR) AND TRAFFIC ENVIRONMENT RECOGNITION"
        },
        {
          "TABLE V": "TASKS (I.E., TCR, VBR)."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "Task"
        },
        {
          "TABLE V": "Œ±acc(%) ‚Üë"
        },
        {
          "TABLE V": "DER\nDBR\nTCR\nVBR\nDriver States\nTraffic Environment"
        },
        {
          "TABLE V": "w/\nw/o\n73.14\n67.18\n-\n-"
        },
        {
          "TABLE V": "w/o\nw/\n-\n-\n91.47\n80.25"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "75.00\n69.31\n96.29\n86.11\nw/\nw/"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "TABLE VI"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "RESULTS OF THE ABLATION EXPERIMENTS ON MULTIMODAL DATA."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "Vehicle-exterior\nVehicle-interior\nŒ±acc(%) ‚Üë\nŒ≤macc"
        },
        {
          "TABLE V": "Joints"
        },
        {
          "TABLE V": "(%) ‚Üë\nImages\nImages\nDER\nDBR\nTCR\nVBR"
        },
        {
          "TABLE V": "(cid:34)\n69.78\n60.27\n92.71\n80.68\n75.86"
        },
        {
          "TABLE V": "(cid:34)"
        },
        {
          "TABLE V": "71.23\n61.49\n84.86\n72.03\n72.40"
        },
        {
          "TABLE V": "(cid:34)\n70.39\n65.53\n73.33\n60.74\n67.50"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "(cid:34)\n(cid:34)\n(cid:34)\n75.00\n69.31\n96.29\n86.11\n81.68"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "tion (TCR and VRB). We performed two sets of experiments"
        },
        {
          "TABLE V": "(Table V): we retained only the driver state recognition tasks"
        },
        {
          "TABLE V": "(DER and DBR), excluding the traffic environment tasks (TCR"
        },
        {
          "TABLE V": "and VBR). The results\nfor DER and\nshowed a drop in Œ±acc"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "DBR by 1.86%-2.13%.\nIn the second set, we retained only"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "the\ntraffic\nenvironment\nrecognition\ntasks\n(TCR and VBR),"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "excluding the driver state classification tasks (DER and DBR),"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "which resulted in a more\ndrop of 4.82%-\nsignificant Œ±acc"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "5.86%. These results demonstrate that\nthe tasks within the"
        },
        {
          "TABLE V": "MTL framework benefit\nfrom significant\nsynergies.\nJointly"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "learning both driver state and traffic environment recognition"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "tasks enhances the model‚Äôs overall accuracy and generaliza-"
        },
        {
          "TABLE V": "tion capability."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "4) Ablation experiments on multimodal data: We validate"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "the independent contributions of each modality through ab-"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "lation experiments by categorizing the modalities into three"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "groups: vehicle-exterior\nimages (front-view,\nleft-view,\nright-"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "view),\nvehicle-interior\nimages\n(inside-view,\ndriver‚Äôs\nfacial"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "and body images), and joint data (posture and gesture). We"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "trained the model with each data group separately, and the"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "results are shown in Table VI."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "The results demonstrate that models trained with a single"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "modality perform worse\nthan the multimodal model, with"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "a\ndrop\nof\n5.82% to\nconfirms\nthe\n14.18% in Œ≤macc. This"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "crucial\nrole\nof multimodal\ndata\nin ADAS-related\ntasks."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "Further\nanalysis\nreveals\nthat\ndifferent modalities\nbenefit"
        },
        {
          "TABLE V": "different\ntasks:\nvehicle-exterior\nimages\nimprove\naccuracy"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "for\nTCR and VBR, while vehicle-interior\nimages\nand joint"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "DER\nDBR\ndata\nenhance\nthe\naccuracy\nof\nand\ntasks. This"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "variation arises because\neach modality expresses different"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "information‚Äîvehicle-exterior images reflect road conditions,"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "while\nvehicle-interior\nimages\nand\njoint\ndata\ncapture\nthe"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "driver‚Äôs behavior and facial expressions. These findings vali-"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "date the necessity of our MGMI design, which adaptively ad-"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "justs the weights of different modalities to alleviate negative"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "transfer and improve task performance in MTL scenarios."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "segmentation method based on boundary fracture correction for\nfroth",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Zhu, Pengfei Li, Zilong Chen, Huiming Yang, Zhiwei Li, Lening"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "scale measurement. Applied Intelligence, pages 1‚Äì22, 2024.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Wang, et al. Mmtl-uniad: A unified framework for multimodal and"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[13] Min Gao, Jian-Yu Li, Chun-Hua Chen, Yun Li, Jun Zhang, and Zhi-",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "multi-task learning in assistive driving perception.\nIn Proceedings"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Hui Zhan. Enhanced multi-task learning and knowledge graph-based",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "of\nthe Computer Vision and Pattern Recognition Conference, pages"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "IEEE Transactions on Knowledge and Data\nrecommender\nsystem.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "6864‚Äì6874, 2025."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Engineering, 35(10):10281‚Äì10294, 2023.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[32]\nZe Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin,"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[14] Yan Gong, Jianli Lu, Wenzhuo Liu, Zhiwei Li, Xinmin Jiang, Xin Gao,",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "and Xingang Wu.\nSifdrivenet: Speed and image\nfusion for driving",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "conference on computer vision and pattern recognition, pages 3202‚Äì"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "IEEE Transactions on Computational\nbehavior classification network.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "3211, 2022."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Social Systems, 2023.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[33]\nZhe Liu, Tengteng Huang, Bingling Li, Xiwu Chen, Xi Wang, and"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[15] Yan Gong, Jianli Lu, Jiayi Wu, and Wenzhuo Liu. Multi-modal fusion",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Xiang Bai. Epnet++: Cascade bi-directional fusion for multi-modal 3d"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "arXiv preprint\ntechnology based on vehicle information: A survey.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "IEEE transactions on pattern analysis and machine\nobject detection."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "arXiv:2211.06080, 2022.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "intelligence, 45(7):8324‚Äì8341, 2022."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[16] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[34]\nJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "selective state spaces. arXiv preprint arXiv:2312.00752, 2023.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Chi. Modeling task relationships\nin multi-task learning with multi-"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[17] Chenghao Guo, Haizhuang Liu,\nJiansheng Chen,\nand Huimin Ma.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "the 24th ACM SIGKDD\ngate mixture-of-experts.\nIn Proceedings of"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Temporal\ninformation fusion network for driving behavior prediction.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "international conference on knowledge discovery & data mining, pages"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "IEEE Transactions on Intelligent Transportation Systems, 24(9):9415‚Äì",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "1930‚Äì1939, 2018."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "9424, 2023.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[35] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.\nShuf-"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[18]\nJianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "flenet v2: Practical guidelines for efficient cnn architecture design.\nIn"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Wang,\nand Chang Xu.\nCmt: Convolutional\nneural\nnetworks meet",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "European Conference on Computer Vision (ECCV), pages 116‚Äì131,"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "of\nthe\nIEEE/CVF Conference\nvision\ntransformers.\nIn Proceedings",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "2018."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "on Computer Vision and Pattern Recognition (CVPR), pages 12175‚Äì",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[36] Manuel Martin, Alina Roitberg, Monica Haurilet, Matthias Horne,"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "12185, June 2022.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Simon Rei√ü, Michael Voit,\nand Rainer Stiefelhagen.\nDrive&act: A"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[19] Meilan Hao, Zhongkang Zhang, Lei Li, Kejian Dong, Long Cheng,",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "multi-modal\ndataset\nfor fine-grained\ndriver\nbehavior\nrecognition\nin"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Prayag Tiwari,\nand Xin Ning.\nCoarse\nto fine-based\nimage‚Äìpoint",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "the IEEE/CVF International\nautonomous vehicles.\nIn Proceedings of"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "cloud fusion network for 3d object detection.\nInformation Fusion,",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Conference on Computer Vision, pages 2801‚Äì2810, 2019."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "112:102551, 2024.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[37]\nLuntian Mou, Yiyuan Zhao, Chao Zhou, Bahareh Nakisa, Moham-"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[20] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotem-",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "mad Naim Rastgoo, Lei Ma, Tiejun Huang, Baocai Yin, Ramesh"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "poral\n3d\ncnns\nretrace\nthe\nhistory\nof\n2d\ncnns\nand\nimagenet?\nIn",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Jain,\nand Wen Gao.\nDriver\nemotion\nrecognition with\na\nhybrid"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Proceedings of\nthe\nIEEE/CVF Conference on Computer Vision and",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "IEEE Transactions\non\nattentional multimodal\nfusion\nframework."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Pattern Recognition (CVPR), pages 6546‚Äì6555, 2018.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Affective Computing, 14(4):2970‚Äì2981, 2023."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren,\nand Jian Sun.\nDeep",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[38] Yeqiang Qian, John M Dolan, and Ming Yang. Dlt-net: Joint detection"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Proceedings\nof\nthe\nresidual\nlearning\nfor\nimage\nrecognition.\nIn",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "IEEE Transactions on\nof drivable areas,\nlane lines, and traffic objects."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Intelligent Transportation Systems, 21(11):4670‚Äì4679, 2019."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "(CVPR), pages 770‚Äì778, 2016.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[39] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[22] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko,",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "and Liang-Chieh Chen. Mobilenetv2:\nInverted residuals\nand linear"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "bottlenecks. In Proceedings of the IEEE/CVF Conference on Computer"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Mobilenets: Efficient convolutional neural networks for mobile vision",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Vision and Pattern Recognition (CVPR), pages 4510‚Äì4520, 2018."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "applications. arXiv preprint arXiv:1704.04861, 2017.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[40] Xiaoqiang Shi, Zhenyu Yin, Guangjie Han, Wenzhuo Liu, Li Qin,"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[23] Yi Huang, Wenzhuo Liu, Yaoyu Li, Lei Yang, Hanqi Jiang, Zhiwei Li,",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Yuanguo Bi, and Shurui Li. Bssnet: A real-time semantic segmentation"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "and Jun Li. Mfe-ssnet: Multi-modal\nfusion-based end-to-end steering",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "network for road scenes inspired from autoencoder. IEEE Transactions"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "angle and vehicle speed prediction network. Automotive Innovation,",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "on Circuits and Systems for Video Technology, 2023."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "pages 1‚Äì14, 2024.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[41] Karen\nSimonyan\nand Andrew Zisserman.\nVery\ndeep\nconvolu-"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[24] Keishi\nIshihara, Anssi Kanervisto,\nJun Miura, and Ville Hautamaki.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "arXiv\npreprint\ntional\nnetworks\nfor\nlarge-scale\nimage\nrecognition."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Multi-task learning with attention for end-to-end autonomous driving.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "arXiv:1409.1556, 2014."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "the IEEE/CVF conference on computer vision and\nIn Proceedings of",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[42] Qifan Tan, Xuqi Yang, Cheng Qiu, Wenzhuo Liu, Yize Li, Zhengxia"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "pattern recognition, pages 2902‚Äì2911, 2021.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Zou, and Jing Huang. Graph-based target association for multi-drone"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[25] Wei-Hong Li\nand Hakan Bilen.\nKnowledge distillation\nfor multi-",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "collaborative perception under imperfect detection conditions. Drones,"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "task learning.\nIn Computer Vision‚ÄìECCV 2020 Workshops: Glasgow,",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "9(4):300, 2025."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "UK, August 23‚Äì28, 2020, Proceedings, Part VI 16, pages 163‚Äì176.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[43] Du Tran, Lubomir Bourdev, Rob\nFergus, Lorenzo Torresani,\nand"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Springer, 2020.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Manohar Paluri.\nLearning\nspatiotemporal\nfeatures with\n3d\nconvo-"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[26]\nZhiwei Li, Tingzhen Zhang, Meihua Zhou, Dandan Tang, Pengwei",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "of\nthe\nIEEE/CVF International\nlutional\nnetworks.\nIn Proceedings"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Zhang, Wenzhuo Liu, Qiaoning Yang, Tianyu Shen, Kunfeng Wang,",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Conference on Computer Vision (ICCV), pages 4489‚Äì4497, 2015."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "and Huaping Liu. Mipd: A multi-sensory interactive perception dataset",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "for\nembodied intelligent driving.\narXiv preprint arXiv:2411.05881,",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "2024.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Information Processing Systems\nis all you need. Advances in Neural"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[27]\nShengchao Liu, Yingyu Liang,\nand Anthony Gitter.\nLoss-balanced",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "(NIPS), 30, 2017."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "task weighting\nto\nreduce\nnegative\ntransfer\nin multi-task\nlearning.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[45] Xiaofeng Wang, Zheng Zhu, Wenbo Xu, Yunpeng Zhang, Yi Wei,"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "of\nthe AAAI\nconference\non\nartificial\nIn Proceedings\nintelligence,",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Xu Chi, Yun Ye, Dalong Du, Jiwen Lu, and Xingang Wang. Openoc-"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "volume 33, pages 9977‚Äì9978, 2019.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "cupancy: A large scale benchmark for surrounding semantic occupancy"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[28] Wenzhuo Liu, Yan Gong, Guoying Zhang, Jianli Lu, Yunlai Zhou, and",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "perception.\nIn Proceedings of the IEEE/CVF International Conference"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Junbin Liao.\nGlmdrivenet: Global‚Äìlocal multimodal\nfusion driving",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "on Computer Vision, pages 17850‚Äì17859, 2023."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "behavior classification network. Engineering Applications of Artificial",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[46] Xiaoyu Wang, Kangyao Huang, Xinyu Zhang, Honglin Sun, Wenzhuo"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Intelligence, 129:107575, 2024.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Liu, Huaping Liu,\nJun Li, and Pingping Lu.\nPath planning for air-"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[29] Wenzhuo Liu, Jianli Lu, Junbin Liao, Yicheng Qiao, Guoying Zhang,",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "ground robot considering modal switching point optimization.\nIn 2023"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Jiayin Zhu, Bozhang Xu, and Zhiwei Li.\nFmdnet: Feature-attention-",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "International Conference\non Unmanned Aircraft\nSystems\n(ICUAS),"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "embedding-based\nmultimodal-fusion\ndriving-behavior-classification",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "pages 87‚Äì94.\nIEEE, 2023."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "network.\nIEEE Transactions on Computational Social Systems, 2024.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[47] Dong Wu, Man-Wen Liao, Wei-Tian Zhang, Xing-Gang Wang, Xiang"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "[30] Wenzhuo Liu, Yicheng Qiao, Zhiwei Li, Wenshuo Wang, Wei Zhang,",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "Bai, Wen-Qing Cheng,\nand Wen-Yu Liu.\nYolop: You\nonly\nlook"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Jiayin Zhu, Yanhuan Jiang, Li Wang, Hong Wang, Huaping Liu, et al.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "once for panoptic driving perception. Machine Intelligence Research,"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Umd-net: A unified multi-task\nassistive\ndriving\nnetwork\nbased\non",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "19(6):550‚Äì562, 2022."
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "IEEE Transactions on Intelligent Transportation\nmultimodal\nfusion.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "[48] Yang Xing, Chen Lv, Dongpu Cao, and Efstathios Velenis. Multi-scale"
        },
        {
          "[12] Yongqi Gan, Wenzhuo Liu,\nJianwang Gan, and Guoying Zhang. A": "Systems, 2025.",
          "[31] Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo,\nJiayin": "driver behavior modeling based on deep spatial-temporal\nrepresenta-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "technologies, 130:103288, 2021."
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "[49]\nShaoqing Xu, Dingfu Zhou,\nJin Fang,\nJunbo Yin, Zhou Bin,\nand"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "Liangjun Zhang.\nFusionpainting: Multimodal\nfusion with adaptive"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "Intel-\nattention for 3d object detection.\nIn 2021 IEEE International"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "ligent Transportation Systems Conference (ITSC), pages 3047‚Äì3054."
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "IEEE, 2021."
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "[50] Chenxiao Yang,\nJunwei Pan, Xiaofeng Gao, Tingyu Jiang, Dapeng"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "Liu, and Guihai Chen. Cross-task knowledge distillation in multi-task"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "the AAAI conference on artificial\nrecommendation.\nIn Proceedings of"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "intelligence, volume 36, pages 4318‚Äì4326, 2022."
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "[51] Dingkang Yang, Shuai Huang, Zhi Xu, Zhenpeng Li, Shunli Wang,"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "Mingcheng Li, Yuzheng Wang, Yang Liu, Kun Yang, Zhaoyu Chen,"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "et al.\nAide: A vision-driven multi-view, multi-modal, multi-tasking"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "Proceedings\nof\nthe\ndataset\nfor\nassistive\ndriving\nperception.\nIn"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "IEEE/CVF\nInternational\nConference\non\nComputer\nVision,\npages"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "20459‚Äì20470, 2023."
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "[52]\nJiao Zhan, Yarong Luo, Chi Guo, Yejun Wu, Jiawei Meng, and Jingnan"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "Liu.\nYolopx: Anchor-free multi-task learning network for panoptic"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "driving perception. Pattern Recognition, 148:110152, 2024."
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "[53] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin,\nand Jian Sun.\nShuf-"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "flenet: An extremely efficient convolutional neural network for mobile"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "the IEEE/CVF Conference on Computer\ndevices.\nIn Proceedings of"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "Vision and Pattern Recognition (CVPR), pages 6848‚Äì6856, 2018."
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "[54] Xinyu Zhang, Yan Gong, Jianli Lu, Zhiwei Li, Shixiang Li, Shu Wang,"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "Wenzhuo Liu, Li Wang, and Jun Li. Oblique convolution: A novel"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "IEEE Transactions on\nconvolution idea for\nredefining lane detection."
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "Intelligent Vehicles, 2023."
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "[55] Dong Zhou, Hongyi Liu, Huimin Ma, Xiang Wang, Xiaoqin Zhang,"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "and Yuhan Dong. Driving behavior prediction considering cognitive"
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "IEEE Transactions on Intelligent Trans-\nprior\nand driving context."
        },
        {
          "tion for intelligent vehicles. Transportation research part C: emerging": "portation Systems, 22(5):2669‚Äì2678, 2020."
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emerging trends in autonomous vehicle perception: Multimodal fusion for 3d object detection",
      "authors": [
        "Ali Simegnew Yihunie Alaba",
        "John Gurbuz",
        "Ball"
      ],
      "year": "2024",
      "venue": "World Electric Vehicle Journal"
    },
    {
      "citation_id": "2",
      "title": "Is space-time attention all you need for video understanding",
      "authors": [
        "Gedas Bertasius",
        "Heng Wang",
        "Lorenzo Torresani"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learnin (ICML)"
    },
    {
      "citation_id": "3",
      "title": "Vm-bhinet: Vision mamba bimanual hand interaction network for 3d interacting hand mesh recovery from a single rgb image",
      "authors": [
        "Han Bi",
        "Ge Yu",
        "Yu He",
        "Wenzhuo Liu",
        "Zijie Zheng"
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on Computer Graphics and Interactive Techniques"
    },
    {
      "citation_id": "4",
      "title": "Relational multi-task learning",
      "authors": [
        "Kaidi Cao",
        "Jiaxuan You",
        "Jure Leskovec"
      ],
      "year": "2023",
      "venue": "Modeling relations between data and tasks",
      "arxiv": "arXiv:2303.07666"
    },
    {
      "citation_id": "5",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "Joao Carreira",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "6",
      "title": "Umt-net: A uniform multi-task network with adaptive task weighting",
      "authors": [
        "Sihan Chen",
        "Lianqing Zheng",
        "Libo Huang",
        "Jie Bai",
        "Xichan Zhu",
        "Zhixiong Ma"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "7",
      "title": "Adamvmoe: Adaptive multi-task vision mixture-of-experts",
      "authors": [
        "Tianlong Chen",
        "Xuxi Chen",
        "Xianzhi Du",
        "Abdullah Rashwan",
        "Fan Yang",
        "Huizhong Chen",
        "Zhangyang Wang",
        "Yeqing Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "8",
      "title": "Multi-task learning for real-time autonomous driving leveraging task-adaptive attention generator",
      "authors": [
        "Wonhyeok Choi",
        "Mingyu Shin",
        "Hyukzae Lee",
        "Jaehoon Cho",
        "Jaehyeon Park",
        "Sunghoon Im"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "9",
      "title": "Multinet: Multi-modal multi-task learning for autonomous driving",
      "authors": [
        "Sauhaarda Chowdhuri",
        "Tushar Pankaj",
        "Karl Zipser"
      ],
      "year": "2019",
      "venue": "2019 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "10",
      "title": "Textnerf: A novel scene-text image synthesis method based on neural radiance fields",
      "authors": [
        "Jialei Cui",
        "Jianwei Du",
        "Wenzhuo Liu",
        "Zhouhui Lian"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Slowfast networks for video recognition",
      "authors": [
        "Christoph Feichtenhofer",
        "Haoqi Fan",
        "Jitendra Malik",
        "Kaiming He"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "12",
      "title": "A segmentation method based on boundary fracture correction for froth scale measurement",
      "authors": [
        "Yongqi Gan",
        "Wenzhuo Liu",
        "Jianwang Gan",
        "Guoying Zhang"
      ],
      "year": "2024",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Enhanced multi-task learning and knowledge graph-based recommender system",
      "authors": [
        "Min Gao",
        "Jian-Yu Li",
        "Chun-Hua Chen",
        "Yun Li",
        "Jun Zhang",
        "Zhi-Hui Zhan"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "14",
      "title": "Sifdrivenet: Speed and image fusion for driving behavior classification network",
      "authors": [
        "Yan Gong",
        "Jianli Lu",
        "Wenzhuo Liu",
        "Zhiwei Li",
        "Xinmin Jiang",
        "Xin Gao",
        "Xingang Wu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "15",
      "title": "Multi-modal fusion technology based on vehicle information: A survey",
      "authors": [
        "Yan Gong",
        "Jianli Lu",
        "Jiayi Wu",
        "Wenzhuo Liu"
      ],
      "year": "2022",
      "venue": "Multi-modal fusion technology based on vehicle information: A survey",
      "arxiv": "arXiv:2211.06080"
    },
    {
      "citation_id": "16",
      "title": "Linear-time sequence modeling with selective state spaces",
      "authors": [
        "Albert Gu",
        "Tri Dao",
        "Mamba"
      ],
      "year": "2023",
      "venue": "Linear-time sequence modeling with selective state spaces",
      "arxiv": "arXiv:2312.00752"
    },
    {
      "citation_id": "17",
      "title": "Temporal information fusion network for driving behavior prediction",
      "authors": [
        "Chenghao Guo",
        "Haizhuang Liu",
        "Jiansheng Chen",
        "Huimin Ma"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "18",
      "title": "Cmt: Convolutional neural networks meet vision transformers",
      "authors": [
        "Jianyuan Guo",
        "Kai Han",
        "Han Wu",
        "Yehui Tang",
        "Xinghao Chen",
        "Yunhe Wang",
        "Chang Xu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "19",
      "title": "Coarse to fine-based image-point cloud fusion network for 3d object detection",
      "authors": [
        "Meilan Hao",
        "Zhongkang Zhang",
        "Lei Li",
        "Kejian Dong",
        "Long Cheng",
        "Prayag Tiwari",
        "Xin Ning"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "20",
      "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?",
      "authors": [
        "Kensho Hara",
        "Hirokatsu Kataoka",
        "Yutaka Satoh"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "21",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "22",
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "Menglong Andrew G Howard",
        "Bo Zhu",
        "Dmitry Chen",
        "Weijun Kalenichenko",
        "Tobias Wang",
        "Marco Weyand",
        "Hartwig Andreetto",
        "Adam"
      ],
      "year": "2017",
      "venue": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "arxiv": "arXiv:1704.04861"
    },
    {
      "citation_id": "23",
      "title": "Mfe-ssnet: Multi-modal fusion-based end-to-end steering angle and vehicle speed prediction network",
      "authors": [
        "Yi Huang",
        "Wenzhuo Liu",
        "Yaoyu Li",
        "Lei Yang",
        "Hanqi Jiang",
        "Zhiwei Li",
        "Jun Li"
      ],
      "year": "2024",
      "venue": "Automotive Innovation"
    },
    {
      "citation_id": "24",
      "title": "Multi-task learning with attention for end-to-end autonomous driving",
      "authors": [
        "Keishi Ishihara",
        "Anssi Kanervisto",
        "Jun Miura",
        "Ville Hautamaki"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "25",
      "title": "Knowledge distillation for multitask learning",
      "authors": [
        "Wei-Hong Li",
        "Hakan Bilen"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020 Workshops: Glasgow, UK"
    },
    {
      "citation_id": "26",
      "title": "A multi-sensory interactive perception dataset for embodied intelligent driving",
      "authors": [
        "Zhiwei Li",
        "Tingzhen Zhang",
        "Meihua Zhou",
        "Dandan Tang",
        "Pengwei Zhang",
        "Wenzhuo Liu",
        "Qiaoning Yang",
        "Tianyu Shen",
        "Kunfeng Wang",
        "Huaping Liu",
        "Mipd"
      ],
      "year": "2024",
      "venue": "A multi-sensory interactive perception dataset for embodied intelligent driving",
      "arxiv": "arXiv:2411.05881"
    },
    {
      "citation_id": "27",
      "title": "Loss-balanced task weighting to reduce negative transfer in multi-task learning",
      "authors": [
        "Shengchao Liu",
        "Yingyu Liang",
        "Anthony Gitter"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "28",
      "title": "Glmdrivenet: Global-local multimodal fusion driving behavior classification network",
      "authors": [
        "Wenzhuo Liu",
        "Yan Gong",
        "Guoying Zhang",
        "Jianli Lu",
        "Yunlai Zhou",
        "Junbin Liao"
      ],
      "year": "2024",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "29",
      "title": "Fmdnet: Feature-attentionembedding-based multimodal-fusion driving-behavior-classification network",
      "authors": [
        "Wenzhuo Liu",
        "Jianli Lu",
        "Junbin Liao",
        "Yicheng Qiao",
        "Guoying Zhang",
        "Jiayin Zhu",
        "Bozhang Xu",
        "Zhiwei Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "30",
      "title": "Umd-net: A unified multi-task assistive driving network based on multimodal fusion",
      "authors": [
        "Wenzhuo Liu",
        "Yicheng Qiao",
        "Zhiwei Li",
        "Wenshuo Wang",
        "Wei Zhang",
        "Jiayin Zhu",
        "Yanhuan Jiang",
        "Li Wang",
        "Hong Wang",
        "Huaping Liu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "31",
      "title": "Mmtl-uniad: A unified framework for multimodal and multi-task learning in assistive driving perception",
      "authors": [
        "Wenzhuo Liu",
        "Wenshuo Wang",
        "Yicheng Qiao",
        "Qiannan Guo",
        "Jiayin Zhu",
        "Pengfei Li",
        "Zilong Chen",
        "Huiming Yang",
        "Zhiwei Li",
        "Lening Wang"
      ],
      "year": "2025",
      "venue": "Proceedings of the Computer Vision and Pattern Recognition Conference"
    },
    {
      "citation_id": "32",
      "title": "Video swin transformer",
      "authors": [
        "Ze Liu",
        "Jia Ning",
        "Yue Cao",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "Han Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "33",
      "title": "Epnet++: Cascade bi-directional fusion for multi-modal 3d object detection",
      "authors": [
        "Zhe Liu",
        "Tengteng Huang",
        "Bingling Li",
        "Xiwu Chen",
        "Xi Wang",
        "Xiang Bai"
      ],
      "year": "2022",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "34",
      "title": "Modeling task relationships in multi-task learning with multigate mixture-of-experts",
      "authors": [
        "Jiaqi Ma",
        "Zhe Zhao",
        "Xinyang Yi",
        "Jilin Chen",
        "Lichan Hong",
        "Ed Chi"
      ],
      "year": "2018",
      "venue": "Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "35",
      "title": "Shufflenet v2: Practical guidelines for efficient cnn architecture design",
      "authors": [
        "Ningning Ma",
        "Xiangyu Zhang",
        "Hai-Tao Zheng",
        "Jian Sun"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "36",
      "title": "Drive&act: A multi-modal dataset for fine-grained driver behavior recognition in autonomous vehicles",
      "authors": [
        "Manuel Martin",
        "Alina Roitberg",
        "Monica Haurilet",
        "Matthias Horne",
        "Simon Rei√ü",
        "Michael Voit",
        "Rainer Stiefelhagen"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "37",
      "title": "Driver emotion recognition with a hybrid attentional multimodal fusion framework",
      "authors": [
        "Luntian Mou",
        "Yiyuan Zhao",
        "Chao Zhou",
        "Bahareh Nakisa ; Rastgoo",
        "Lei Ma",
        "Tiejun Huang",
        "Baocai Yin",
        "Ramesh Jain",
        "Wen Gao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Dlt-net: Joint detection of drivable areas, lane lines, and traffic objects",
      "authors": [
        "Yeqiang Qian",
        "John Dolan",
        "Ming Yang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "39",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "Mark Sandler",
        "Andrew Howard",
        "Menglong Zhu",
        "Andrey Zhmoginov",
        "Liang-Chieh Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "40",
      "title": "Bssnet: A real-time semantic segmentation network for road scenes inspired from autoencoder",
      "authors": [
        "Xiaoqiang Shi",
        "Zhenyu Yin",
        "Guangjie Han",
        "Wenzhuo Liu",
        "Li Qin",
        "Yuanguo Bi",
        "Shurui Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "41",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "42",
      "title": "Graph-based target association for multi-drone collaborative perception under imperfect detection conditions",
      "authors": [
        "Qifan Tan",
        "Xuqi Yang",
        "Cheng Qiu",
        "Wenzhuo Liu",
        "Yize Li",
        "Zhengxia Zou",
        "Jing Huang"
      ],
      "year": "2025",
      "venue": "Drones"
    },
    {
      "citation_id": "43",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "Du Tran",
        "Lubomir Bourdev",
        "Rob Fergus",
        "Lorenzo Torresani",
        "Manohar Paluri"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "44",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "≈Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "45",
      "title": "Openoccupancy: A large scale benchmark for surrounding semantic occupancy perception",
      "authors": [
        "Xiaofeng Wang",
        "Zheng Zhu",
        "Wenbo Xu",
        "Yunpeng Zhang",
        "Yi Wei",
        "Xu Chi",
        "Yun Ye",
        "Dalong Du",
        "Jiwen Lu",
        "Xingang Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "46",
      "title": "Path planning for airground robot considering modal switching point optimization",
      "authors": [
        "Xiaoyu Wang",
        "Kangyao Huang",
        "Xinyu Zhang",
        "Honglin Sun",
        "Wenzhuo Liu",
        "Huaping Liu",
        "Jun Li",
        "Pingping Lu"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Unmanned Aircraft Systems (ICUAS)"
    },
    {
      "citation_id": "47",
      "title": "Yolop: You only look once for panoptic driving perception",
      "authors": [
        "Dong Wu",
        "Man-Wen Liao",
        "Wei-Tian Zhang",
        "Xing-Gang Wang",
        "Xiang Bai",
        "Wen-Qing Cheng",
        "Wen-Yu Liu"
      ],
      "year": "2022",
      "venue": "Machine Intelligence Research"
    },
    {
      "citation_id": "48",
      "title": "Multi-scale driver behavior modeling based on deep spatial-temporal representa-tion for intelligent vehicles",
      "authors": [
        "Yang Xing",
        "Chen Lv",
        "Dongpu Cao",
        "Efstathios Velenis"
      ],
      "year": "2021",
      "venue": "Multi-scale driver behavior modeling based on deep spatial-temporal representa-tion for intelligent vehicles"
    },
    {
      "citation_id": "49",
      "title": "Fusionpainting: Multimodal fusion with adaptive attention for 3d object detection",
      "authors": [
        "Shaoqing Xu",
        "Dingfu Zhou",
        "Jin Fang",
        "Junbo Yin",
        "Zhou Bin",
        "Liangjun Zhang"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Intelligent Transportation Systems Conference (ITSC)"
    },
    {
      "citation_id": "50",
      "title": "Cross-task knowledge distillation in multi-task recommendation",
      "authors": [
        "Chenxiao Yang",
        "Junwei Pan",
        "Xiaofeng Gao",
        "Tingyu Jiang",
        "Dapeng Liu",
        "Guihai Chen"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "51",
      "title": "Aide: A vision-driven multi-view, multi-modal, multi-tasking dataset for assistive driving perception",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Zhi Xu",
        "Zhenpeng Li",
        "Shunli Wang",
        "Mingcheng Li",
        "Yuzheng Wang",
        "Yang Liu",
        "Kun Yang",
        "Zhaoyu Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "52",
      "title": "Yolopx: Anchor-free multi-task learning network for panoptic driving perception",
      "authors": [
        "Jiao Zhan",
        "Yarong Luo",
        "Chi Guo",
        "Yejun Wu",
        "Jiawei Meng",
        "Jingnan Liu"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "53",
      "title": "Shufflenet: An extremely efficient convolutional neural network for mobile devices",
      "authors": [
        "Xiangyu Zhang",
        "Xinyu Zhou",
        "Mengxiao Lin",
        "Jian Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "54",
      "title": "Oblique convolution: A novel convolution idea for redefining lane detection",
      "authors": [
        "Xinyu Zhang",
        "Yan Gong",
        "Jianli Lu",
        "Zhiwei Li",
        "Shixiang Li",
        "Shu Wang",
        "Wenzhuo Liu",
        "Li Wang",
        "Jun Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "55",
      "title": "Driving behavior prediction considering cognitive prior and driving context",
      "authors": [
        "Dong Zhou",
        "Hongyi Liu",
        "Huimin Ma",
        "Xiang Wang",
        "Xiaoqin Zhang",
        "Yuhan Dong"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    }
  ]
}