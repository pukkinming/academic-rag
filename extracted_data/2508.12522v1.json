{
  "paper_id": "2508.12522v1",
  "title": "Musaco: Multimodal Subject-Specific Selection And Adaptation For Expression Recognition With Co-Training",
  "published": "2025-08-17T23:08:21Z",
  "authors": [
    "Muhammad Osama Zeeshan",
    "Natacha Gillet",
    "Alessandro Lameiras Koerich",
    "Marco Pedersoli",
    "Francois Bremond",
    "Eric Granger"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Personalized expression recognition (ER) involves adapting a machine learning model to subject-specific data for improved recognition of expressions with considerable interpersonal variability. Subject-specific ER can benefit significantly from multi-source domain adaptation (MSDA) methods -where each domain corresponds to a specific subject -to improve model accuracy and robustness. Despite promising results, state-of-the-art MSDA approaches often overlook multimodal information or blend sources into a single domain, limiting subject diversity and failing to explicitly capture unique subject-specific characteristics. To address these limitations, we introduce MuSACo, a multimodal subject-specific selection and adaptation method for ER based on co-training. It leverages complementary information across multiple modalities and multiple source domains for subject-specific adaptation. This makes MuSACo particularly relevant for affective computing applications in digital health, such as patient-specific assessment for stress or pain, where subject-level nuances are crucial. MuSACo selects source subjects relevant to the target and generates pseudo-labels using the dominant modality for class-aware learning, in conjunction with a class-agnostic loss to learn from less confident target samples. Finally, source features from each modality are aligned, while only confident target features are combined. Our experimental results 1 on challenging multimodal ER datasets -BioVid and StressID -show that MuSACo can outperform UDA (blending) and state-of-the-art MSDA methods.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Expression recognition (ER) has achieved significant success in computer vision due to its relevance in numer- 1 Our code is provided in suppl. materials and will be made public. ous real-world applications, such as pain estimation, stress monitoring, and affective computing  [1, 2, 18, 32, 37] . These models can achieve a high level of performance under supervised settings or when provided with limited labeled data  [25, 27] . However, their effectiveness mainly depends on the availability of annotated datasets, which are difficult to obtain, and often fail to generalize for subtle expressions and across diverse individuals due to variations in cultural, environmental, and individual expressiveness  [13] . Unsupervised domain adaptation (UDA)  [10, 20, 43]  has emerged as a promising direction to address this challenge by adapting a model from a labeled source domain to an unlabeled target domain. In the context of ER, this facilitates personalization, where a model trained on labeled expression data from multiple source subjects is adapted to a new, unlabeled target subject. While existing UDA approaches  [11, 18, 49]  adapt to the unlabeled target dataset treated as a domain by either using single or blended labeled source subjects, recent advances in multi-source domain adaptation (MSDA)  [45, 46]  leverages information from multiple source subjects (domains). This encourages diversity in the source model for adaptation to an unlabeled target subject (domain). This method facilitates more precise and targeted adaptation strategies for developing a personalized ER model (see Fig.  1(a) ).\n\nDespite these advances, the majority of subject-specific ER approaches are predominantly constrained to a single modality  [11, 46] . However, human expressions and finegrained affective cues are often difficult to capture reliably from a single modality alone  [19, 47] . To address this limitation, we focus on a multimodal setting that leverages the complementary strengths of multimodal data. Multimodal ER (MER) has been widely studied in the literature  [1, 2, 26, 34] , typically outperforming unimodal systems by leveraging complementary information from multiple modalities. MER approaches integrate diversity from various modalities, such as visual, textual, audio, or physiolog- ical signals  [7]  to enhance model performance. In subjectbased ER, most MER methods focus on fully or weakly labeled data  [26, 34] , hindering the ability of the model to have a better adaptation to unlabeled data. Nevertheless, there are some studies  [15, 39]  that introduced multiple modalities, specifically for EEG. Eye movement signals with a single source or combine multiple sources into a single domain (see Fig.  1(b) ). Their reliance on a single source often fails to provide diversity in source domains. Furthermore, blending multiple sources into a single domain  [16]  cannot explicitly capture the unique subject-specific characteristics of each source subject, which is crucial for subjectspecific target adaptation.\n\nIn this paper, we propose MuSACo, a novel approach for Multimodal Subject-Specific Selection and Adaptation for ER using Co-Training in an MSDA setting (Fig.  1(c )). Our method exploits the complementary information from multiple modalities while selecting the most relevant source subjects for the target. It takes advantage of the unique characteristics of each subject for better performance on the target subject (domain 2 ). There are two key challenges when considering multiple modalities for subject-specific MSDA:\n\n(1) effectively leveraging multiple source subject information within multimodal systems, and (2) achieving crossdomain and cross-modal alignment.\n\nTo address the first challenge, we propose the selection of source subjects most relevant to the target subject. While prior work  [45]  has shown that selecting source subjects based on their relevance to the target improves adaptation, it relies solely on a single modality to guide the source selection process. In this paper, we propose a co-training-based strategy for selecting source subjects that leverage multiple modalities, as different modalities complement each other by providing unique characteristics from various perspec- 2 The terms domain and subject are used interchangeably. tives. Thus, selecting sources using co-training benefits the adaptation process. Furthermore, to improve generalization across individuals, we encourage the model to focus on expression-specific features by disentangling identityrelated information during source training.\n\nFor the second challenge, we first focus on cross-domain alignment. State-of-the-art MSDA methods  [17, 30]  highlight the effectiveness of class-aware alignment in bridging dissimilar distributions. This typically involves generating pseudo-labels (PLs) for the target subject. In our proposed method, we extend this by leveraging co-training for the generation of target PLs. Specifically, PLs for unlabeled target data are generated by selecting predictions from the dominant modality (e.g., visual or physiological), chosen by their probability score and threshold. This ensures diversity in modality-specific feature representations and captures complementary aspects of the data. These PLs then guide class-aware alignment between source and target subjects, minimizing distribution mismatch. Although PLs allow improving the model performance, they heavily depend on the choice of a threshold to filter out unreliable samples. In particular, high threshold values ensure the sample quality. While setting a high threshold value often results in discarding potentially useful but less confident samples. To mitigate this, we introduce a class-agnostic loss that aligns non-confident target samples with the source. This encourages the unreliable samples to contribute to the learning process to improve the robustness of the model. For modality alignment, including a fusion module  [1, 26]  has proven to enhance MER performance. In MuSACo, a fusion module is added to concatenate features from different modalities for each selected source subject. For the target, only reliable samples (selected via co-training) are combined.\n\nThe contributions of this paper are summarized as follows. (1) MuSACo: a novel multimodal MSDA method for accurate subject specific adaptation. It selects relevant source subjects from the target using co-training, leveraging complementary information across modalities. (2) An effective alignment process that relies on co-training to generate confident target PLs for class-aware loss, combined with class-agnostic loss to handle non-confident target samples. (3) Extensive experiments performed on challenging MER datasets (BioVid and StressID) shows the efficacy of MuSACo. It can outperform the UDA (blending) and stateof-the-art MSDA methods on this multimodal data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work (A) Multi-Source Domain Adaptation:",
      "text": "MSDA techniques for image classification can be classified into discrepancy-based methods  [17, 23]  that reduce the domain shift by estimating the discrepancy between source and target domains.  [17] , generate the target pseudo-labels by creating clusters for each class; the initialization will depend on the source domains. Contrastive learning  [17, 30]  methods focus on minimizing the disparity within the same class while maximizing with the opposite classes. Adversarial learning  [22, 48] , and self-supervised  [6, 36]  approaches, which train different classifiers for each source while generating pseudo-labels from the classifier agreement to adapt to the target domain. Recently, Zeeshan et al.  [45]  proposed a subject-based MSDA method for recognition expression that considers subjects as domains to tackle many sources (up to 77) for the adaptation process. Even though there are advancements in MSDA, they all depend on unimodal (visual) modality, which leads us to introduce multimodality in the MSDA setting for subject-based ER. (b) Multimodal Expression Recognition: Multimodal ER is widely explored in the literature. Praveen et al.  [26]  proposed a cross-attention method that fused multiple modalities for expression recognition. Aslam et al.  [1]  presented an LUPI paradigm that learns from multiple modalities but uses only a single modality at the time of inference. Similarly, in  [2] , the author presented an optimal transport knowledge distilling method from different modalities. Multimodal methods also receive attention in domain adaptation for EEG and eye movement signals. Wang et al.  [39]  presented a variational autoencoder (VAE) multimodal method leveraging cycle consistency and adversarial learning losses to reduce the shift between domains and overcome the modality separation problem. Likewise, in  [15] , they adapt a model by reducing the discrepancy (CORAL loss  [33] ) between source and target distributions while adding an intra-inter domain loss for modality alignment for a cross-subject expression recognition using multimodal EEG signals. This paper exploits multiple modalities (visual and physiological) in MSDA for adapting to a new, unlabeled target subject. (c) Disentanglement in Expression Recognition: Dis-entanglement methods have been used in ER to suppress identity or pose information that interferes with ER. Jiang and Deng  [14]  proposed a GAN-based model with separate encoders for expression, identity, and pose, followed by a decoder and discriminator to guide expression reconstruction. Xie et al.  [42]  introduced TDGAN, which fuses identity from one image and expression from another. While effective for visual data, these GAN-based approaches rely on image reconstruction, making them unsuitable for nonvisual modalities like physiological signals, where such reconstruction is not meaningful. Pichler et al.  [24]  proposed a KNIFE method that disentangled the information by estimating and minimizing the entropy between two estimators. Motivated by this strategy, our MuSACo adopts a similar formulation to decouple identity-and expression-specific representations.\n\n(d) Co-training for Domain Adaptation: Co-training is a well-established semi-supervised and unsupervised learning approach  [3]  that is used in many applications, such as semantic segmentation  [9, 41] , task decomposition  [44] , or in image classification  [5] . It typically trains the model with modalities to produce more reliable pseudo-labels (PLs), while domain adaptation usually trains the model with a single modality that incorporates multiple classifiers to account for different views. Chen et al.  [5]  propose a method that introduces two different classifiers trained with different features controlled via a weighting mechanism while generating target pseudo-labels from the most confident classifier. MuSACo introduces a co-training method for multimodal data for the selection of source subjects. In addition, we select target confident PLs based on multiple classifiers trained on different modalities.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Musaco Approach",
      "text": "MuSACo seeks to leverage complementary information from multiple modalities within MSDA using co-training to improve target personalization. This is accomplished through a two-step training process. Firstly, the nondiscriminative identity information is disentangled from the source subjects while training task-specific backbones using source supervision. This ensures that only discriminative features are retained for recognizing the expression. Second, a co-training strategy is employed to select the most relevant source subjects for the target. For each sourcetarget pair, similarity scores are computed from each modality. Sources that produce high similarity scores in either modality are selected, and a threshold (τ ss ) is applied to filter out less relevant sources. Exploiting the trained backbones, we generate target PLs through co-training. The modality with the highest confidence in its prediction is chosen, and the samples are considered reliable if their confidence exceeds a threshold (τ pl ). For domain alignment, class-aware alignment is performed on the target samples selected using τ pl and class-agnostic alignment using nonconfident samples. Finally, for modality alignment, features are concatenated from different modalities for every selected source and target subject, which are used to train the fusion module. MuSACo is illustrated in Fig.  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Architecture",
      "text": "A multimodal multi-source expression classification task is considered, where a model is trained using several labeled source subjects S m = {S m1 , . . . , S ma , . . . , S m D } and a single unlabeled target subject T m , where m is the number of modalities and a = {1, . . . , D} is the number of source subjects. In MuSACo, without loss of generality, we consider two modalities: visual (v) and physiological (p), where m ∈ {v, p}. Each source S ma contains N s labeled samples {(x s mj , y s mj , ýs mj )|1 < j < N s }, where y s mj represents expression labels and ýs mj represents identity labels. We also define expression classes as c = {1, . . . , C} The unlabeled target consists of N t samples {x t mj |1 < j < N t }. Backbones are defined as B = {B 1 , . . . , B M }, where each B belongs to its respective modality. Accordingly, we consider two backbones that correspond to B v (visual) and B p (physiological) modalities. It takes input from x s m and x t m and return embeddings of source h s m = h s v , h s p and target h t m = h t v , h t p . Finally, the fusion module F is a two-layer perceptron that takes input extracted from S m and T m to perform feature fusion for the alignment of modalities.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training Backbones With Disentanglement",
      "text": "Two backbones are trained for each modality -visual B v and physiological B p -using multiple labeled source subjects S m . To learn expression-discriminative representations, each modality is supervised using a cross-entropy loss:\n\nwhere h i mj is the embedding from modality m, y i mj is the ground-truth expression label, and f m is the modalityspecific expression classifier. The loss L s m is computed independently for each modality to ensure effective learning of modality-specific features. Disentangled Identity-Information. Our approach is inspired by KNIFE  [24] , a differentiable entropy estimator, to disentangle identity information from expression-relevant features in each modality. We estimate the marginal entropy H(h s m ) and the conditional entropy H(h s mj | Ý s mj ). For each sample in the source subjects, the identity labels ýs mj are converted into one-hot representations Ý s mj ∈ R k and define the disentanglement loss as:\n\nThe classification loss L s m and disentanglement loss L d m are jointly optimized to preserve task-relevant features while removing identity information 3  .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Domain Adaptation Via Co-Training",
      "text": "(a) Selection of Source Subjects. Selecting the most relevant source subjects from S m is a critical step in MuSACo method, where the model is trained on diverse sources to ensure robustness. However, when adapting to a target subject T m captured in a specific environment and cultural context (e.g., an adult Caucasian male), the source subjects must align with these characteristics to ensure effective adaptation. To address this, we leverage our multimodal system to define selection criteria based on co-training, which ensures that the chosen sources are highly relevant to the target subject. Given two modalities visual and physiological, estimate the cosine similarity between S and T using embeddings h s m and h t m ,\n\nwhere ||.|| is the euclidean norm calculated for h s m and h t m . Q m (S, T) is estimated for each source and target pairs as,\n\nHere, P m represents a dictionary list containing all pairwise distances computed using cosine similarity for each modality. Due to the distinct architectures of the modalities, such as a 2D CNN for the visual modality and a 1D CNN for the physiological modality, we normalize P m to the range [0, 1] to ensure comparability across modalities. We define P v and P p as the similarity scores for the visual and physiological modalities, respectively. These scores are merged into a combined set P = (P v P p ), from which we select source subjects whose distances exceed a predefined threshold τ ss .\n\nThe selection process is formalized as:\n\nThe selected source subjects for each modality form a subset S m , where S m ⊆ S m . This selection process ensures that only the most relevant source subjects are used for the target adaptation.\n\n(b) Generation of Target Pseudo-Labels (PLs). In MSDA, reliable PLs determine the performance of a model on an unseen target subject  [45] . MuSACo benefits from the multimodal by selecting confident target PLs using cotraining. Given two backbones, B v and B p We introduce samples x t m from T m to produce softmax probabilities with respect to each modality, p v = σ(h s v ) and p p = σ(h s p ), taking the maximum probability p = max(p v , p p ) and applying a predetermined threshold τ pl to select the confident samples, defined as:\n\nwhere T l m represents the set of confident target samples for m modalities, we also store the confident classes C =\n\nTo differentiate between confident and non-confident samples, we create T u m , defined as:\n\nwhere T u m , defined the non-confident target samples, for the estimation of class-agnostic alignment (Section 3.3). (c) Class-Aware Domain Alignment. We exploit class information to perform intra-class and inter-class alignment across domains. For intra-class alignment, samples from the same class are encouraged to reside in the same distribution space. Conversely, for inter-class alignment, samples from different classes are pushed further apart in the feature space. Class-aware alignment is computed using maximum mean discrepancy (MMD)  [31] , a statistical measure for estimating the disparity between two distributions. To sample out examples from the same class, we perform class-aware sampling on source and target subjects. The classes that belong to T l m were denoted as C. We sample out examples from every subject belonging to multiple labeled sources. In each mini-batch, examples from every source and target are extracted for the same class. The intra-class discrepancy is defined as:\n\nwhere, h c ma extracts features from each source subject, h c m are the features extracted from the target subject, and C are the confident expression classes. Inter-class is defined as:\n\nwhere MMD is computed from the opposite classes ć for every source with the target. Alignment loss is estimated for every modality (visual and physio), ensuring that source and target align in their respective distribution space.\n\n(d) Class-Agnostic Domain Alignment.\n\nMost MSDA methods  [17, 30]  employ class-aware alignment to bridge dissimilar distributions, often relying on PLs for the target subject. However, these pseudo-labels are prone to noise, and while denoising techniques have been proposed  [6, 28, 30, 43] , they typically use pre-determined thresholds to retain only the most confident pseudo-labels (CPL). This reliance on thresholds can limit performance, especially in ER, where subtle expressions may lead to discarding useful but less confident samples. To address this, a domainagnostic loss is introduced that aligns non-confident samples to ensure that all target samples contribute to the adaptation process. This approach maximizes the utilization of available data for improving the target adaptation. The domain agnostic loss is defined as:\n\nwhere h s ma is the feature vector extracted from S m subjects that contain samples not belong to C. The total alignment loss for both modalities is calculated as:\n\nwhere L agn and L aw are class-agnostic and class-aware losses respectively. For each modality, the intra-class and class-agnostic discrepancy is minimized, while the interclass discrepancy is maximized. (e) Modality Alignment. In multimodal settings, aligning modalities by adding a fusion network has proven to be very successful in enhancing model performance  [1, 26] . In MuSACo method, to align the modalities, a fusion module F is introduced that projects the combined modality embeddings into a shared feature space. To train F for multiple source subjects, we use the embeddings h s v and h s p to perform feature concatenation h s = h s v ⊕ h s p , and pass through F to calculate a cross-entropy loss:\n\nwhere D is the source subject that contains the N s number of samples. Modality alignment is estimated for each S and T subject individually. We only calculated for the confident samples for the target T l m , the embeddings are extracted h t v and h t p , and applied concatenation function h t = h t v ⊕ h t p , then F is applied to calculate the cross-entropy loss:\n\nwhere N t l is the total number of reliable samples, and y j is the CPL. The final objective is to jointly optimize all four losses L = L s + γL t unsup + αL agn + βL aw , where γ, α, and β hyper-parameters weight the contribution of each loss.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Protocol",
      "text": "Datasets. All the methods were evaluated on two challenging multimodal ER datasets for subject-based adaptation. Biovid Heat Pain (PartA)  [38]  dataset consists of 87 subjects with 5 classes, corresponding to BL (no pain) and four pain intensity levels from PA-1 to PA-4. MuSACo follows the same protocol as  [45]  to consider 77 subjects as sources and 10 subjects as targets. StressID  [4]  dataset comprised 65 subjects taken in a lab-controlled environment. Due to the unavailability of certain modalities in 11 of the participants, our method considers 54 individuals, which includes both modalities (visual and physiological), from which 44 source and 10 target subjects are selected. More details are provided in the supplementary material. In both datasets, visual and physiological (EDA) modalities are considered in all of our experiments. Implementation Detail. In addition to the visual modality, the physiological modality, specifically electrodermal activity (EDA), is incorporated for both the BioVid and StressID datasets. For visual, ResNet18  [12]  is used, following the same protocol as  [45] . The images are resized to 100×100 resolution. For the physiological modality, an LSTM-based 1D-CNN network is included, consisting of two convolutional layers, one LSTM layer, and one fully connected layer. In addition, the expression head is constructed using 2-MLP layers. A batch size of 32 for the target subject, and the model is trained for 20 epochs. All the target subjects are split into train/val/test sets, and all the results are reported using a target test set. To select the confident target samples, τ pl is set to 0.95. For selection of source subjects, τ ss is set to 0.55 (Sec. 4.4-b). Baseline Methods. The reported results correspond to the top-1% multimodal target accuracy across all target subjects. The evaluation begins with the Lower Bound, where a model is trained solely on labeled source subjects (77 for BioVid, 44 for StressID) and directly evaluated on the target test set without any target supervision. Results are reported for visual-only, physio-only, and fusion (vision + physio) modalities. Next, the MM-UDA (blending) setting blends all the source subjects into a single domain, then adapts to the unlabeled target subject using various UDA techniques, including DANN  [8] , CDAN  [21] , MMD  [31] , and MuSACo (UDA), which only use co-training to generate target pseudo-labels. Furthermore, MuSACo (MSDA) is compared against MM-MSDA approaches, including CAN  [17] , Sub-based MSDA, and CMSDA  [30] . In all methods, a 1D-CNN-LSTM network is added for the physiological modality to ensure a fair comparison with our proposed approach. Finally, results for an Upper Bound setting are reported, where models are fine-tuned using labeled multimodal data from the target subject.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Comparison With State-Of-The-Art Methods",
      "text": "Tab. 1 shows results on the BioVid dataset. All domain adaptation approaches outperform the lower-bound baselines (visual and both), confirming the effectiveness of multimodal domain adaptation in the context of pain estimation. Notably, the Physio-only baseline outperforms many standard UDA and MSDA methods, achieving an average accuracy of 34.5%. This reflects findings in prior work  [2, 37, 40] , where physiological signals are shown to be strong indicators of pain intensity, especially in subtle expression settings where visual cues are often weak or ambiguous. In contrast, the Visual-only baseline performs significantly worse (28.8%), and the naïve fusion of both modalities (without adaptation) does not exceed the performance of using physiological data alone. Among all methods, our approach achieves the highest overall performance. MuSACo (UDA) outperforms both the lowerbound and UDA baselines and is competitive with MSDA methods. Further, MuSACo (MSDA) achieves even better results, with a 9.3% gain over physio-only and 7.5% over MuSACo (UDA). It also outperforms MSDA baselines, with an average gain of 9.2% over CAN  [17]  and Sub-based top-k  [45] , and 7% over CMSDA  [30] . This shows our method effectively exploits modality complementarity, adapting to subject-specific signals in subtle pain recognition. Tab. 2 presents the performance on the StressID dataset. As expected, training without adaptation (lower bound) yields significantly lower accuracy across most subjects. Notably, fusing both modalities in the lower bound produces more stable performance compared to using either modality alone. This is due to the complementary nature of the modalities; when the visual modality underperforms, the physiological input often compensates, result-ing in more balanced performance across subjects. Among existing methods, CAN achieves the highest performance on Sub-2, 8, whereas Sub-based top-k gains the performance on Sub-3 that is comparable with our method. MuSACo (UDA) improved the performance on 3 out of 10 subjects while, on average, attaining a similar performance as MSDA methods. MuSACo (MSDA) outperformed every other method on all target subjects, with an overall gain of 15.7% from lower-bound (both), 7.4% from MMD, 6.7% from MuSACo (UDA) and CAN, and 6.6% from Subbased MSDA.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Visualizations",
      "text": "In Fig.  3  (left), t-SNE  [35]  shows how different methods separate class embeddings. The source-only model yields noisy, overlapping features. Subject-based MSDA  [45]  improves clustering but struggles with overlaps (e.g., PA3 in PA4). In contrast, MuSACo better clusters same-class samples and separates opposing ones. Fig.  3  (right) shows the closest and furthest source subjects to a target (Sub-10, BioVid), based on similarity scores from visual and physi-",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Studies",
      "text": "(a) Impact of difference loss components. To evaluate the contribution of each loss component, an incremental ablation study is conducted by gradually introducing loss terms.\n\nAs shown in Tab. 3, adding the unsupervised target loss L t significantly improves accuracy over the source-only baseline. Further gains are achieved by incorporating the classagnostic loss L agn and the class-aware loss L aw , both of which contribute to improving the adaptation process in the absence of target labels. The combination of all loss terms achieves the highest accuracy of 40.7%.\n\n(b) Impact of selection of source subjects threshold. As shown in Fig.  4 , setting τ ss = 0 disables the source selection mechanism, including all source subjects (77 for Biovid), achieving an average accuracy of 37.6%. As τ ss increases, fewer but more relevant source subjects are selected based on similarity scores, improving performance. The best accuracy of 39.8% is achieved at τ ss = 0.55, where only 21 source subjects are selected on average. This demonstrates the effectiveness of co-training in identifying complementary and informative sources for improved target adaptation. A breakdown of subject-wise selection of each target subject are provided in the suppl. material.\n\nTable  3 . Impact of individual loss components.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Accuracy (%)",
      "text": "Source Selection Threshold (τ ss )\n\nSelected Subjects  Figure 4 . Source subject threshold (τss) selection based on the accuracy. The red circle highlights the selected τss = 0.55.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "Domain adaptation methods typically focus on individual modalities, or in the case of multimodal MSDA, they aggregate data from all source subjects, losing critical subjectspecific cues necessary for effective adaptation to unlatarget data. In this paper, MuSACo, a multimodal subject-specific MSDA is introduced that effectively leverages subject-level information for a personalized adaptation. In particular, MuSACo selects the most similar source subjects based on multimodal similarity and applies cotraining to generate pseudo-labels for the target. Classaware and class-agnostic losses are combined to leverage both confident and uncertain target samples effectively. Final alignment is achieved via feature concatenation across modalities. MuSACo is validated on challenging MER datasets and shows consistent gains over baselines, UDA (blending), and MSDA methods. MuSACo supports personalized modeling by adapting to each target subject through relevant sources, making it well-suited for health-related applications, where subject-specific cues are crucial. While the current method requires training a separate model per target subject, future work will explore lightweight or continual adaptation approaches to reduce retraining overhead and enable faster subject-specific adaptation.\n\n6. Supplementary 6.1. Algorithm (a) Selection of Subject-Specific Sources. Algorithm 1 shows the subject-specific selection of source subjects with co-training. Given source subjects S and specific target subject T, extracted features using visual B v and physiological B p encoders. Construct a similarity metric z v and z p by measuring CosineSimilarity between every source subject and target. To estimate the maximum score, we normalize the similarities, merge them, and sort them in descending order, followed by a threshold that selects the most relevant subjects from the target.\n\n# Compute similarities 10:\n\nz p i ← cos(X p s , X p t )\n\n12:\n\n# Append similarities 13:",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Hyperparameters",
      "text": "Training of backbones. momentum=0.9, weight decay=5e-4, stochastic gradient descent (SGD) optimizer  [29] , learning rate=1e-4 with lr scheduler (eta min=0.00002). Weighting parameters. In MuSACo, we give different weights to different loss functions. For the contribution of class-agnostic loss γ = 0.5, and for class-aware loss α = 0.1. Disentanglement. Knife is very sensitive to hyperparameters; we have explored several parameters to make it work with the Perform co-training on T to generate pseudo-labels (after every n epochs)\n\n3:\n\nfor iteration do 4:\n\n# Class-aware alignment 5:\n\nClass-aware domain sampling of S and T",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "6:",
      "text": "Estimate intra-class discrepancy using (Eq: 8)\n\n7:\n\nEstimate inter-class discrepancy using (Eq: 9)\n\n8:\n\nCompute class-aware loss L aw (Eq: 12)\n\n9:\n\n# Domain-agnostic alignment Estimate domain agnostic using (Eq 10)\n\n12:\n\nCompute domain-agnostic loss L agn (Eq: 11)\n\n13:\n\n# Modality alignment Compute modality align loss L s using (Eq: 13)\n\n17:\n\nCompute modality align L t unsup using (Eq: 14)\n\n18:\n\nend for 19: end for expression recognition task. The most critical parameters that is selected for our experiments are: zd dim=1024, zc dim=77, hidden state=512, layers=3, nb mixture=10, with learning-rate=0.01.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Detail On Training The Source Backbones",
      "text": "Training of the source backbones (visual and physiological) involves disentangling the identity-related features from the expression task. Our method is inspired by KNIFE  [24] , a fully differentiable entropy estimator, which we adapted for disentanglement in a multi-modal framework. The KNIFE estimator optimizes the backbones (visual and physiological) by decoupling non-task-related information through individual modality-specific gradient-based optimization. We leverage the KNIFE estimator to minimize the mutual information between modalities in eliminating identity features and enhancing the disentanglement of task-relevant features across distributions. We first estimate the marginal entropy of embedding h s m as,\n\nwhere E is the expectation over the distribution h s m and p(.) is the probability density. To estimate the conditional entropy between the features and identities, For each sample in the source subjects, we convert the identity labels ýs mj into one-hot representations Ý s mj ∈ R k . Then, conditional entropy is calculated as,\n\nwhere p(., .) is the conditional probability density between features h s mj and prediction Ý s mj . The total learning loss is calculated as:\n\nBy minimizing L d m , the model is encouraged to decouple the information associated with ýs m from the feature embeddings h s m . Furthermore, we introduce a fixed Identityhead I m head composed of two fully connected layers. It takes embeddings h s m without gradient backpropagation. I m works as a regularizer in conjunction with disentanglement loss to suppress non-discriminative identity-related information during target adaptation. The disentanglement loss decouples identity-related information, while the I m helps to constrain redundant (non-task-specific) features, ensuring the model focuses on learning relevant expressionrelated representations.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Ablation",
      "text": "(a) Impact of Weighting Hyperparameters. We performed a sequential weight sensitivity analysis for the key loss terms, L t unsup , L aw , and L agn shown in Tab. 5). First, we varied the weight of the target PL loss while keeping the others fixed at 1. Using the best result, we tuned the classaware alignment loss, followed by the class-agnostic loss, each time fixing the previously selected best weights. This approach highlights the individual contribution of each loss to overall performance. quantity. To mitigate the issue of excluding useful but uncertain samples when using a high threshold, we introduce a domain-agnostic loss that ensures the model will also learn from the non-confident samples. Note that target groundtruth labels were used for this analysis to evaluate the effectiveness of pseudo-labels.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Weight",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Musaco",
      "text": "MuSACo (inspired by the layered harmony of Musaca) integrates two synergistic modules for subject-specific adaptation: a co-training-based source subject selection module that identifies the most relevant sources using complementary cues from multiple modalities, and an adaptation module that aligns source and target domains using class-aware and class-agnostic losses. Like Musaca's layered composition-where distinct elements work in harmony-MuSACo leverages modality-specific strengths to guide pseudo-label generation and fuse information effectively. This enables robust, personalized adaptation for each target subject while maintaining consistency across modalities.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison of MuSACo against unimodal MSDA and multimodal UDA (blending) ER methods for subject-based adaptation.",
      "page": 2
    },
    {
      "caption": "Figure 1: (b)). Their reliance on a single source",
      "page": 2
    },
    {
      "caption": "Figure 2: An overview of MuSACo shown in the particular case of M = 2 modalities (with visual + physiological). First, the similarity",
      "page": 4
    },
    {
      "caption": "Figure 3: (left), t-SNE [35] shows how different methods",
      "page": 7
    },
    {
      "caption": "Figure 3: (right) shows",
      "page": 7
    },
    {
      "caption": "Figure 3: Left: t-SNE visualizations. The source-only produced indistinguishable feature class clusters. Sub-based MSDA [45] reduces",
      "page": 8
    },
    {
      "caption": "Figure 4: , setting τss = 0 disables the source se-",
      "page": 8
    },
    {
      "caption": "Figure 4: Source subject threshold (τss) selection based on the",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Lower Bound": "MM-UDA\n(Blending)",
          "Visual-only\nPhysio-only\nFusion": "DANN [8]\nCDAN [21]\nMMD [31]\nMuSACo\n(UDA)",
          "37.1\n27.2\n26.3\n29.6\n28.4\n28.6\n35.6\n22.1\n33.6\n19.7\n28.8\n41.3\n33.2\n40.3\n34.2\n33.4\n31.2\n37.3\n33.9\n35.6\n25.2\n34.5\n39.1\n35.2\n28.6\n25.6\n39.2\n36.6\n44.5\n27.8\n28.8\n20.1\n32.5": "39.4\n28.4\n31.5\n33.4\n37.6\n31.5\n34.2\n26.6\n34.3\n27.4\n32.4\n32.6\n24.1\n29.2\n32.1\n29.6\n26.8\n28.4\n23.6\n23.4\n22.6\n27.2\n34.1\n26.3\n31.5\n36.7\n34.2\n33.5\n43.4\n34.1\n28.6\n31.2\n33.3\n38.2\n43.5\n29.9\n37.6\n40.0\n22.3\n38.6\n40.0\n47.6\n25.8\n36.3"
        },
        {
          "Lower Bound": "MM-MSDA",
          "Visual-only\nPhysio-only\nFusion": "CAN [17]\nSub-basedtop-k [45]\nCMSDA [30]\nMuSACo\n(MSDA)",
          "37.1\n27.2\n26.3\n29.6\n28.4\n28.6\n35.6\n22.1\n33.6\n19.7\n28.8\n41.3\n33.2\n40.3\n34.2\n33.4\n31.2\n37.3\n33.9\n35.6\n25.2\n34.5\n39.1\n35.2\n28.6\n25.6\n39.2\n36.6\n44.5\n27.8\n28.8\n20.1\n32.5": "43.2\n32.2\n27.2\n33.6\n33.4\n32.5\n36.6\n32.4\n41.5\n33.8\n34.6\n43.5\n42.2\n28.5\n22.5\n29.1\n32.5\n40.6\n41.2\n38.0\n28.4\n34.6\n39.4\n39.4\n29.4\n36.6\n39.2\n39.1\n38.4\n35.7\n45.2\n25.6\n36.8\n49.2\n43.3\n47.3\n38.2\n40.1\n44.3\n46.4\n45.2\n50.0\n34.3\n43.8"
        },
        {
          "Lower Bound": "Upper Bound",
          "Visual-only\nPhysio-only\nFusion": "Fine-tuning",
          "37.1\n27.2\n26.3\n29.6\n28.4\n28.6\n35.6\n22.1\n33.6\n19.7\n28.8\n41.3\n33.2\n40.3\n34.2\n33.4\n31.2\n37.3\n33.9\n35.6\n25.2\n34.5\n39.1\n35.2\n28.6\n25.6\n39.2\n36.6\n44.5\n27.8\n28.8\n20.1\n32.5": "83.6\n78.5\n70.0\n71.6\n71.5\n64.5\n76.8\n76.9\n78.5\n64.8\n73.6"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Lower Bound": "MM-UDA\n(Blending)",
          "Visual-only\nPhysio-only\nFusion": "DANN [8]\nCDAN [21]\nMMD [31]\nMuSACo\n(UDA)",
          "71.1\n23.2\n51.4\n63.3\n55.6\n63.2\n72.7\n48.7\n60.0\n59.6\n56.8\n57.7\n81.1\n60.0\n28.2\n60.0\n69.7\n84.2\n51.1\n34.6\n29.7\n55.6\n68.3\n30.4\n54.2\n57.6\n65.3\n67.8\n79.7\n48.6\n61.2\n59.4\n59.2": "65.3\n65.3\n73.3\n66.4\n67.3\n72.3\n73.2\n55.3\n70.4\n57.2\n66.6\n58.1\n68.1\n72.6\n59.9\n67.9\n70.7\n76.4\n59.5\n60.3\n49.4\n64.2\n66.3\n52.3\n71.6\n74.5\n71.9\n74.5\n81.2\n57.2\n68.4\n57.2\n67.5\n71.2\n73.5\n60.0\n53.4\n69.7\n70.0\n68.7\n82.4\n72.7\n61.2\n68.2"
        },
        {
          "Lower Bound": "MM-MSDA",
          "Visual-only\nPhysio-only\nFusion": "CAN [17]\nSub-basedtop-k [45]\nMuSACo\n(MSDA)",
          "71.1\n23.2\n51.4\n63.3\n55.6\n63.2\n72.7\n48.7\n60.0\n59.6\n56.8\n57.7\n81.1\n60.0\n28.2\n60.0\n69.7\n84.2\n51.1\n34.6\n29.7\n55.6\n68.3\n30.4\n54.2\n57.6\n65.3\n67.8\n79.7\n48.6\n61.2\n59.4\n59.2": "81.2\n60.0\n69.3\n67.2\n59.3\n62.3\n74.2\n83.3\n66.8\n59.3\n68.2\n73.6\n70.8\n74.9\n70.6\n59.8\n77.9\n71.8\n49.9\n63.9\n69.8\n68.3\n71.2\n81.2\n73.6\n74.6\n73.5\n78.5\n86.3\n60.0\n79.2\n71.5\n74.9"
        },
        {
          "Lower Bound": "Upper Bound",
          "Visual-only\nPhysio-only\nFusion": "Fine-tuning",
          "71.1\n23.2\n51.4\n63.3\n55.6\n63.2\n72.7\n48.7\n60.0\n59.6\n56.8\n57.7\n81.1\n60.0\n28.2\n60.0\n69.7\n84.2\n51.1\n34.6\n29.7\n55.6\n68.3\n30.4\n54.2\n57.6\n65.3\n67.8\n79.7\n48.6\n61.2\n59.4\n59.2": "83.4\n84.5\n98.2\n98.1\n89.7\n87.6\n90.0\n89.6\n89.4\n95.7\n90.6"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Privileged knowledge distillation for dimensional emotion recognition in the wild",
      "authors": [
        "Muhammad Haseeb",
        "Muhammad Osama Zeeshan",
        "Marco Pedersoli",
        "Alessandro Koerich",
        "Simon Bacon",
        "Eric Granger"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "2",
      "title": "Distilling privileged multimodal information for expression recognition using optimal transport",
      "authors": [
        "Muhammad Haseeb",
        "Muhammad Osama Zeeshan",
        "Soufiane Belharbi",
        "Marco Pedersoli",
        "Alessandro Koerich",
        "Simon Bacon",
        "Eric Granger"
      ],
      "year": "2006",
      "venue": "18th International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "3",
      "title": "Combining labeled and unlabeled data with co-training",
      "authors": [
        "Avrim Blum",
        "Tom Mitchell"
      ],
      "year": "1998",
      "venue": "Proceedings of the eleventh annual conference on Computational learning theory"
    },
    {
      "citation_id": "4",
      "title": "Stressid: a multimodal dataset for stress identification",
      "authors": [
        "Hava Chaptoukaev",
        "Valeriya Strizhkova",
        "Michele Panariello",
        "Bianca Dalpaos",
        "Aglind Reka",
        "Valeria Manera",
        "Susanne Thümmler",
        "Esma Ismailova",
        "Massimiliano Todisco",
        "Maria Zuluaga"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "Advances in neural information processing systems",
      "authors": [
        "Minmin Chen",
        "Kilian Weinberger",
        "John Blitzer"
      ],
      "year": "2011",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "Robust target training for multi-source domain adaptation",
      "authors": [
        "Zhongying Deng",
        "Da Li",
        "Yi-Zhe Song",
        "Tao Xiang"
      ],
      "year": "2022",
      "venue": "Robust target training for multi-source domain adaptation"
    },
    {
      "citation_id": "7",
      "title": "Human emotion recognition: Review of sensors and methods",
      "authors": [
        "Andrius Dzedzickis",
        "Artūras Kaklauskas",
        "Vytautas Bucinskas"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "8",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Yaroslav Ganin",
        "Evgeniya Ustinova",
        "Hana Ajakan",
        "Pascal Germain",
        "Hugo Larochelle",
        "Mario Franc ¸ois Laviolette",
        "Victor March",
        "Lempitsky"
      ],
      "year": "2016",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "9",
      "title": "Co-training for unsupervised domain adaptation of semantic segmentation models",
      "authors": [
        "Gabriel Jose L Gómez",
        "Antonio Villalonga",
        "López"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "10",
      "title": "A kernel two-sample test",
      "authors": [
        "Arthur Gretton",
        "Karsten Borgwardt",
        "J Malte",
        "Bernhard Rasch",
        "Alexander Schölkopf",
        "Smola"
      ],
      "year": "2012",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "11",
      "title": "Personalized broad learning system for facial expression",
      "authors": [
        "Jing Han",
        "Lun Xie",
        "Jing Liu",
        "Xue Li"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "12",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "13",
      "title": "Cultural confusions show that facial expressions are not universal",
      "authors": [
        "Rachael Jack",
        "Caroline Blais",
        "Christoph Scheepers",
        "Philippe Schyns",
        "Roberto Caldara"
      ],
      "year": "2009",
      "venue": "Current biology"
    },
    {
      "citation_id": "14",
      "title": "Disentangling identity and pose for facial expression recognition",
      "authors": [
        "Jing Jiang",
        "Weihong Deng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Cfda-csf: A multi-modal domain adaptation method for cross-subject emotion recognition",
      "authors": [
        "Magdiel Jiménez",
        "Gibran Fuentes-Pineda"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Mmda: A multimodal and multisource domain adaptation method for cross-subject emotion recognition from eeg and eye movement signals",
      "authors": [
        "Magdiel Jiménez-Guarneros",
        "Gibran Fuentes-Pineda",
        "Jonas Grande-Barreto"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "17",
      "title": "Contrastive adaptation network for single-and multi-source domain adaptation. IEEE transactions on pattern analysis and machine intelligence",
      "authors": [
        "Guoliang Kang",
        "Lu Jiang",
        "Yunchao Wei",
        "Yi Yang",
        "Alexander Hauptmann"
      ],
      "year": "2020",
      "venue": "Contrastive adaptation network for single-and multi-source domain adaptation. IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "18",
      "title": "Deep emotion transfer network for cross-database facial expression recognition",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2018",
      "venue": "2018 24th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "19",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "Wei Liu",
        "Jie-Lin Qiu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "20",
      "title": "Unsupervised domain adaptation with residual transfer networks",
      "authors": [
        "Mingsheng Long",
        "Han Zhu",
        "Jianmin Wang",
        "Michael Jordan"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Conditional adversarial domain adaptation",
      "authors": [
        "Mingsheng Long",
        "Zhangjie Cao",
        "Jianmin Wang",
        "Michael Jordan"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "22",
      "title": "Stem: An approach to multi-source domain adaptation with guarantees",
      "authors": [
        "Van-Anh",
        "Tuan Nguyen",
        "Trung Nguyen",
        "Quan Le",
        "Dinh Hung Tran",
        "Phung"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Moment matching for multi-source domain adaptation",
      "authors": [
        "Xingchao Peng",
        "Qinxun Bai",
        "Xide Xia",
        "Zijun Huang",
        "Kate Saenko",
        "Bo Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "24",
      "title": "A differential entropy estimator for training neural networks",
      "authors": [
        "Georg Pichler",
        "Pierre Jean",
        "A Colombo",
        "Malik Boudiaf",
        "Günther Koliander",
        "Pablo Piantanida"
      ],
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "25",
      "title": "Deep weakly supervised domain adaptation for pain localization in videos",
      "authors": [
        "Eric Gnana Praveen",
        "Patrick Granger",
        "Cardinal"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "26",
      "title": "A joint cross-attention model for audio-visual fusion in dimensional emotion recognition",
      "authors": [
        "Wheidima Gnana Praveen",
        "Nasib Carneiro De Melo",
        "Haseeb Ullah",
        "Osama Aslam",
        "Théo Zeeshan",
        "Marco Denorme",
        "Alessandro Pedersoli",
        "Simon Koerich",
        "Patrick Bacon",
        "Cardinal"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "27",
      "title": "Deep domain adaptation with ordinal regression for pain assessment using weakly-labeled videos",
      "authors": [
        "Eric Gnana Praveen Rajasekhar",
        "Patrick Granger",
        "Cardinal"
      ],
      "year": "2021",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "28",
      "title": "Multi-source unsupervised domain adaptation via pseudo target domain",
      "authors": [
        "Yong-Hui Chuan-Xian Ren",
        "Xi-Wen Liu",
        "Ke-Kun Zhang",
        "Huang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "29",
      "title": "An overview of gradient descent optimization algorithms",
      "authors": [
        "Sebastian Ruder"
      ],
      "year": "2016",
      "venue": "An overview of gradient descent optimization algorithms",
      "arxiv": "arXiv:1609.04747"
    },
    {
      "citation_id": "30",
      "title": "Multi-source domain adaptation via supervised contrastive learning and confident consistency regularization",
      "authors": [
        "Marin Scalbert",
        "Maria Vakalopoulou",
        "Florent Couzinié-Devy"
      ],
      "year": "2021",
      "venue": "Multi-source domain adaptation via supervised contrastive learning and confident consistency regularization",
      "arxiv": "arXiv:2106.16093"
    },
    {
      "citation_id": "31",
      "title": "Equivalence of distance-based and rkhs-based statistics in hypothesis testing. The annals of statistics",
      "authors": [
        "Dino Sejdinovic",
        "Bharath Sriperumbudur",
        "Arthur Gretton",
        "Kenji Fukumizu"
      ],
      "year": "2013",
      "venue": "Equivalence of distance-based and rkhs-based statistics in hypothesis testing. The annals of statistics"
    },
    {
      "citation_id": "32",
      "title": "Disentangled source-free personalization for facial expression recognition with neutral target data",
      "authors": [
        "Masoumeh Sharafi",
        "Emma Ollivier",
        "Muhammad Osama Zeeshan",
        "Soufiane Belharbi",
        "Marco Pedersoli",
        "Alessandro Koerich",
        "Simon Bacon"
      ],
      "year": "2025",
      "venue": "Disentangled source-free personalization for facial expression recognition with neutral target data",
      "arxiv": "arXiv:2503.20771"
    },
    {
      "citation_id": "33",
      "title": "Deep coral: Correlation alignment for deep domain adaptation",
      "authors": [
        "Baochen Sun",
        "Kate Saenko"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "34",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of selected topics in signal processing"
    },
    {
      "citation_id": "35",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "36",
      "title": "Your classifier can secretly suffice multi-source domain adaptation",
      "authors": [
        "Naveen Venkat",
        "Jogendra Nath Kundu",
        "Durgesh Singh",
        "Ambareesh Revanur"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "37",
      "title": "Joint multimodal transformer for emotion recognition in the wild",
      "authors": [
        "Paul Waligora",
        "Muhammad Haseeb Aslam",
        "Muhammad Osama Zeeshan",
        "Soufiane Belharbi",
        "Alessandro Koerich",
        "Marco Pedersoli",
        "Simon Bacon",
        "Eric Granger"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "The biovid heat pain database data for the advancement and systematic validation of an automated pain recognition system",
      "authors": [
        "Steffen Walter",
        "Sascha Gruss",
        "Hagen Ehleiter",
        "Junwen Tan",
        "Harald Traue",
        "Philipp Werner",
        "Ayoub Al-Hamadi",
        "Stephen Crawcour",
        "Gustavo Moreira Da Adriano O Andrade",
        "Silva"
      ],
      "year": "2013",
      "venue": "2013 IEEE international conference on cybernetics (CYBCO)"
    },
    {
      "citation_id": "39",
      "title": "Multi-modal domain adaptation variational autoencoder for eeg-based emotion recognition",
      "authors": [
        "Yixin Wang",
        "Shuang Qiu",
        "Dan Li",
        "Changde Du",
        "Bao-Liang Lu",
        "Huiguang He"
      ],
      "year": "2022",
      "venue": "IEEE/CAA Journal of Automatica Sinica"
    },
    {
      "citation_id": "40",
      "title": "Analysis of facial expressiveness during experimentally induced heat pain",
      "authors": [
        "Philipp Werner",
        "Ayoub Al-Hamadi",
        "Steffen Walter"
      ],
      "year": "2017",
      "venue": "2017 Seventh international conference on affective computing and intelligent interaction workshops and demos (ACIIW)"
    },
    {
      "citation_id": "41",
      "title": "Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation",
      "authors": [
        "Yingda Xia",
        "Dong Yang",
        "Zhiding Yu",
        "Fengze Liu",
        "Jinzheng Cai",
        "Lequan Yu",
        "Zhuotun Zhu",
        "Daguang Xu",
        "Alan Yuille",
        "Holger Roth"
      ],
      "year": "2020",
      "venue": "Medical image analysis"
    },
    {
      "citation_id": "42",
      "title": "Facial expression recognition with two-branch disentangled generative adversarial network",
      "authors": [
        "Siyue Xie",
        "Haifeng Hu",
        "Yizhen Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "43",
      "title": "Deep cocktail network: Multi-source unsupervised domain adaptation with category shift",
      "authors": [
        "Ruijia Xu",
        "Ziliang Chen",
        "Wangmeng Zuo",
        "Junjie Yan",
        "Liang Lin"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "44",
      "title": "Deep co-training with task decomposition for semisupervised domain adaptation",
      "authors": [
        "Luyu Yang",
        "Yan Wang",
        "Mingfei Gao",
        "Abhinav Shrivastava",
        "Q Kilian",
        "Wei-Lun Weinberger",
        "Ser-Nam Chao",
        "Lim"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "45",
      "title": "Subject-based domain adaptation for facial expression recognition",
      "authors": [
        "Muhammad Osama Zeeshan",
        "Muhammad Haseeb Aslam",
        "Soufiane Belharbi",
        "Alessandro Koerich",
        "Marco Pedersoli",
        "Simon Bacon",
        "Eric Granger"
      ],
      "year": "2024",
      "venue": "2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "46",
      "title": "Progressive multisource domain adaptation for personalized facial expression recognition",
      "authors": [
        "Muhammad Osama Zeeshan",
        "Marco Pedersoli"
      ],
      "year": "2025",
      "venue": "Progressive multisource domain adaptation for personalized facial expression recognition",
      "arxiv": "arXiv:2504.04252"
    },
    {
      "citation_id": "47",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "Jianhua Zhang",
        "Zhong Yin",
        "Peng Chen",
        "Stefano Nichele"
      ],
      "year": "2020",
      "venue": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review"
    },
    {
      "citation_id": "48",
      "title": "Madan: multi-source adversarial domain aggregation network for domain adaptation",
      "authors": [
        "Sicheng Zhao",
        "Bo Li",
        "Pengfei Xu",
        "Xiangyu Yue",
        "Guiguang Ding",
        "Kurt Keutzer"
      ],
      "year": "2021",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "49",
      "title": "Discriminative feature adaptation for cross-domain facial expression recognition",
      "authors": [
        "Ronghang Zhu",
        "Gaoli Sang",
        "Qijun Zhao"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Biometrics (ICB)"
    }
  ]
}