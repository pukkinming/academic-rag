{
  "paper_id": "2509.16193v1",
  "title": "Are Multimodal Foundation Models All That Is Needed For Emofake Detection?",
  "published": "2025-09-19T17:55:20Z",
  "authors": [
    "Mohd Mujtaba Akhtar",
    "Girish",
    "Orchid Chetia Phukan",
    "Swarup Ranjan Behera",
    "Pailla Balakrishna Reddy",
    "Ananda Chandra Nayak",
    "Sanjib Kumar Nayak",
    "Arun Balaji Buduru"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this work, we investigate multimodal foundation models (MFMs) for EmoFake detection (EFD) and hypothesize that they will outperform audio foundation models (AFMs). MFMs due to their cross-modal pre-training, learns emotional patterns from multiple modalities, while AFMs rely only on audio. As such, MFMs can better recognize unnatural emotional shifts and inconsistencies in manipulated audio, making them more effective at distinguishing real from fake emotional expressions. To validate our hypothesis, we conduct a comprehensive comparative analysis of state-of-the-art (SOTA) MFMs (e.g. LanguageBind) alongside AFMs (e.g. WavLM). Our experiments confirm that MFMs surpass AFMs for EFD. Beyond individual foundation models (FMs) performance, we explore FMs fusion, motivated by findings in related research areas such synthetic speech detection and speech emotion recognition. To this end, we propose SCAR, a novel framework for effective fusion. SCAR introduces a nested cross-attention mechanism, where representations from FMs interact at two stages sequentially to refine information exchange. Additionally, a self-attention refinement module further enhances feature representations by reinforcing important cross-FM cues while suppressing noise. Through SCAR with synergistic fusion of MFMs, we achieve SOTA performance, surpassing both standalone FMs and conventional fusion approaches and previous works on EFD.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "In recent years, significant research has focused on advancing audio deepfake detection techniques to address various categories of deepfakes, including synthetic speech, scene-fake, and singfake. Synthetic speech deepfakes leverage advance text-to-speech (TTS) and voice conversion (VC) models to generate highly realistic voices, often indistinguishable from genuine recordings  [1] . Scene-fake manipulations involve altering background acoustic environments to misrepresent situational context  [2] , while singfake techniques modify vocal attributes to synthesize high-fidelity singing voices  [3] . These deepfakes pose serious risks, such as identity fraud and misinformation through synthetic speech, manipulation of forensic evidence via scene-fake techniques, and copyright infringement or reputational harm from sing-fake synthesis. Such threats undermine trust in audio authenticity, highlighting the need for robust detection measures. As such researchers have proposed various techniques for audio deepfake detection  [4] ,  [5] ,  [6] . However, with the wider and easier availability of foundation models (FMs) in recent times, audio deepfake detection research * Contributed equally as first authors.\n\nhas seen sufficient advancement  [7] ,  [8] ,  [9] . These FMs are pre-trained on diverse large-scale audio data and provide uplift in performance as well as prevention of training models from scratch and this trend of leveraging FMs is consistent across synthetic speech detection  [10] , scenefake detection  [11] , and singfake detection  [12] . Despite these significant advancements, an emerging vulnerability remains largely unaddressed i.e. emotionally manipulated speech deepfakes (EmoFake). Fig.  1 : Demonstration of EmoFake: Speaker A's happy speech is manipulated to synthesize a sad emotional tone in the audio while maintaining the same spoken content ('I'm fine, thanks!'). EmoFake (Figure  1 ) alters a speaker's emotional attributes while preserving linguistic content and speaker identity. Unlike conventional synthetic speech, it subtly modifies pitch, tone, and intensity to create realistic emotional manipulations. This emerging threat poses serious risks in security, forensics, and misinformation, enabling malicious actors to exploit emotional deception for fraud and manipulation. As such Zhao et al.  [13]  proposed the first initial dataset for EmoFake Detection (EFD) and presented initial baselines on it. However, they haven't explored FMs for EFD which have shown its efficacy in detecting various types of audio deepfakes. In this study, we focus on EFD and explore FMs for the first time for EFD, to the best of our knowledge. We hypothesize that multimodal FMs (MFMs) such as LanguageBind (LB), ImageBind (IB) will outperform audio FMs (AFMs) (WavLM, wav2vec2, Unispeech-SAT, etc.) for EFD. Unlike AFMs, which rely solely on audio, MFMs due to their cross-modal pre-training, learns emotional patterns across multiple modalities and this strength enables MFMs to more effectively detect unnatural emotional shifts and inconsistencies in manipulated audio, making them superior at distinguishing genuine from fake emotional expressions. To test our hypothesis, we conduct a comprehensive comparative analysis of state-of-the-art (SOTA) MFMs and AFMs. Our experiments confirm that MFMs consistently outperform AFMs arXiv:2509.16193v1 [eess.AS] 19 Sep 2025 in EFD. Further, inspired from research in synthetic speech detection  [14] , speech emotion recognition  [15]  where fusion of FMs have shown improved performance. We also explore this direction and we are the primary study to explore fusion of FMs for EFD. To this end, we introduce SCAR (NeSted Cross-Attention NetwoRk), a novel framework for effective fusion of FMs. SCAR employs a novel nested cross-attention mechanism that enables multi-stage interaction between FMs representational space, refining information exchange. Additionally, a selfattention refinement module enhances feature representations by amplifying critical cross-FMs cues while suppressing noise. By leveraging SCAR with the fusion of MFMs, we achieve topmost performance surpassing both standalone FMs, baseline fusion techniques, and thus achieving SOTA in comparison to previous SOTA work for EFD. Our study offers the following key contributions:\n\n• We present the first comprehensive comparative exploration into various SOTA MFMs and AFMs to investigate the efficacy of MFMs for EFD. We show that MFMs are the best for EFD attributing to their multimodal pretraining. • We propose, SCAR for fusion of FMs. With SCAR through the fusion of MFMs, we report the topmost performance against individual FMs, baseline fusion techniques and setting SOTA in comparison to previous works. The resources i.e., models and code are accessible at 1  for future research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Foundation Models",
      "text": "In this section, we discuss the SOTA AFMs and MFMs used in our study. Audio Foundation Models: Unispeech-SAT 2    [16] , Wav2vec2 3    [17] , WavLM 4    [18] , and HuBERT 5    [19]  were pre-trained in self-supervised manner. We use the base versions of Unispeech-SAT, Wav2vec2, WavLM, HuBERT with 94.68M, 95.04M, 94.70M, 94.68M parameters respectively and pre-trained on librispeech 960 hours of english data. Unispeech-SAT and WavLM reports the SOTA performance across different tasks in SUPERB. Unispeech-SAT was pre-trained in multi-task speaker-aware format and WavLM was pre-trained for solving masked speech modeling and speech denoising simultaneously. Wav2vec2 follows contrastive objective during its pre-training while HuBERT solves BERT-like mask prediction objective. We also consider Whisper 6    [20]  that has shown its effectiveness for related synthetic speech deepfake detection  [21]  and singfake detection  [22] . Whisper was pre-trained on 96 languages and follows vanilla transformer encoder-decoder architecture. It was pre-trained in a multi-task learning manner and we use its base version with 74M parameters. We resample the audio samples to 16KHz before feeding to the AFMs. We extract representations from the last hidden layer of the frozen AFMs by application of pooling-average. The extracted representations are of dimension: 768 for Unispeech-SAT, Wav2vec2, WavLM, HuBERT; 512 for Whisper. For Whisper, we use only its encoder for extracting representations by discarding the decoder. Multimodal Foundation Models: We use IB 7    [23]  and LB 8    [24]  for MFMs. IB is a multimodal learning framework that aligns modalities-images, text, audio, depth, thermal, and IMU-into a unified embedding space with image modality as the binding modality. It empolys a VIT enocder with contrastive learning objective. Without requiring direct supervision for all modality pairs, IB enables zero-shot recognition and retrieval for multimodal tasks. LB aligns multiple modalities such as video, audio, depth, and infrared-directly to the language space, eliminating reliance on image-based alignment. It employs a contrastive learning approach with a frozen 12-layer Transformer-based language encoder. It was pre-trained on VIDAL-10M, a dataset comprising 10 million video-text, infrared-text, depth-text, and audio-text pairs. We resample the audios to 16KHz before passing to the MFMs and extract representations from the last hidden state of the audio encoders of the MFMs by using average pooling. We extract representations of 768, 1024 size for LB and IB respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Modeling",
      "text": "In this section, we give detailed explanation of the downstream modeling involved with individual FMs followed by the proposed framework, SCAR for fusion of FMs. We use fully connected network (FCN) and CNN as downstreams as used by previous research in related areas such as SER  [25]  and shout intensity prediction  [26] . The CNN (Figure  2 (b) ) consists of a convolutional block with 1D CNN layer of 32 kernels with size 3 followed by maxpooling. The features are flattened and passed to a FCN block with two dense layers of 512 and 128 neurons followed by the output layer for binary classification. We use sigmoid as the activation function in the output layer. For FCN, we keep the same architectural details as the FCN block in CNN model. The CNN models trainable parameters ranges from 0.5 to 0.7M and for FCN, it ranges from 0.45 to 0.6M depending on the input representation dimension size.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Scar",
      "text": "The architecture of the proposed framework is illustrated in Figure  2 . SCAR introduces a novel nested cross-attention mechanism that facilitates multi-stage interactions within the representational space of FMs, refining information exchange. Furthermore, a self-attention refinement module strengthens feature representations by emphasizing key cross-FMs cues while minimizing noise. The workflow of SCAR is as follows: First, representations are extracted from FMs and passed through a convolutional block with same modeling paradigm as done with individual FMs above. Following this, the flattened\n\nUsing these, the cross-attention outputs for the first crossattention step are computed as:\n\nA second cross-attention step refines these representations:\n\nThe cross-attention outputs for the second crossattention step are computed as follows:\n\nFinally, a self-attention refinement step is applied to each modality separately: b denote the outputs at different stages of attention processing. This hierarchical cross-attention attention structure enables structured and progressive alignment of representations, enhancing the interaction between FMs while refining feature representations. The final fused representation is obtained by concatenating the refined outputs:\n\nb ). The concatenated features are then passed to a FCN block with two dense layers of 512 and 128 neurons respectively followed by the final output layer for binary classification. We keep the number of attention heads in nested cross attention for both the crossattention stages as 2. For self-attention refinement stage, we keep the number of heads also as 2. SCAR consists of 1.8 to 2.3M trainable parameters depending on the dimension size of the FMs representations.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iv. Result And Analysis",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Dataset",
      "text": "We utilize the sole open-source dataset for EFD, introduced by Zhao et al.  [13] . This dataset includes recordings in both English and Chinese, containing samples from multiple speakers across five primary emotions: Neutral, Happy, Angry, Sad, and",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Experimental Results",
      "text": "We use Equal Error Rate (EER) as the evaluation metric following previous research on EFD  [13] . Table I presents the evaluation scores of downstream models trained on representations from various FMs. We observe that MFMs consistently achieve lower EER values across both language subsets and downstream network architectures in comparison to unimodal AFMs. This validates our hypothesis that MFMs outperform AFMs in EFD as their cross-modal pretraining allows them to learn emotional patterns from multiple modalities. This enhances their ability to detect unnatural emotional shifts and inconsistencies in manipulated audio. Among the AFMs, Whisper achieved the highest performance, which can be attributed to its multilingual pretraining. This aligns with previous research on synthetic speech detection  [14] , which found that multilingual AFMs better capture pitch, tone, and intensity variations, enhancing their ability to detect synthetic or manipulated speech. Overall, CNN-based models generally yield lower EER scores compared to FCNs with all the FMs. We also plot the t-SNE plots of raw representations from the FMs in Figure  3 . We can observe clear clusters across the classes for MFMs and thus providing support to our results.\n\nTable  II  presents the evaluation scores for different combinations of FMs. We use concatenation as baseline fusion technique. For modeling concatentation-based fusion, we follow similar architecture as SCAR and discard nested cross-attention block and the self-attention refinement. We follow the same training specifications as used for SCAR. We can observe clear dominance of SCAR based fusions of FMs compared to concatenation-based technique and this shows the strength of SCAR for effective fusion. The topmost performance is achieved by fusion of MFMs i.e. LB and IB through SCAR and this shows the emergence of strong complementary behavior among them. Comparison to SOTA: We compare our best model SCAR with LB and IB with previous SOTA work  [13] . They reported the EER values as 3.65% and 8.34% for English and Chinese subsets respectively. However, SCAR with LB and IB reported the best EER of 1.15% and 1.02% for English and Chinese respectively and thus setting new SOTA for EFD.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusion",
      "text": "Our study shows that MFMs outperform AFMs for EFD due to their cross-modal pretraining, which enables better recognition of unnatural emotional shifts in manipulated audio. Further, we also introduce, SCAR, a novel framework for fusion of FMs that leverages nested cross-attention and self-attention refinement. SCAR with the fusion of MFMs achieves achieves SOTA results, surpassing standalone FMs and baseline fusion method, establishing a new benchmark for EFD. Also, our study serves as a guide for future research exploring FMs for EFD.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Demonstration of EmoFake: Speaker A’s happy speech",
      "page": 1
    },
    {
      "caption": "Figure 1: ) alters a speaker’s emotional attributes",
      "page": 1
    },
    {
      "caption": "Figure 2: (b)) consists of",
      "page": 2
    },
    {
      "caption": "Figure 2: SCAR introduces a novel nested cross-attention",
      "page": 2
    },
    {
      "caption": "Figure 2: Modeling Architectures: Subfigure (a) FCN and CNN; Subfigure (b) shows the proposed framework, SCAR; Subfigure (c)",
      "page": 3
    },
    {
      "caption": "Figure 3: The subfigures (a) LB (b) IB (c) Wav2vec2 and (d)",
      "page": 4
    },
    {
      "caption": "Figure 3: We can observe clear clusters across the",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.43\n2.51\n5.67\n3.62": "",
          "2.21\n2.24\n5.25\n2.93\n2.04\n1.32\n1.36": "",
          "7.68": "4.36\n8.78\n10.94",
          "7.35": "3.29\n4.91"
        },
        {
          "2.43\n2.51\n5.67\n3.62": "",
          "2.21\n2.24\n5.25\n2.93\n2.04\n1.32\n1.36": "",
          "7.68": "",
          "7.35": "10.23"
        },
        {
          "2.43\n2.51\n5.67\n3.62": "2.31",
          "2.21\n2.24\n5.25\n2.93\n2.04\n1.32\n1.36": "",
          "7.68": "3.25\n2.19\n2.10",
          "7.35": "3.11\n1.89\n1.81"
        },
        {
          "2.43\n2.51\n5.67\n3.62": "1.64\n1.87",
          "2.21\n2.24\n5.25\n2.93\n2.04\n1.32\n1.36": "",
          "7.68": "",
          "7.35": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Asvspoof 2019: A large-scale public database of synthesized, converted and replayed speech",
      "authors": [
        "X Wang"
      ],
      "year": "2020",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "2",
      "title": "Scenefake: An initial dataset and benchmarks for scene fake audio detection",
      "authors": [
        "J Yi"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "Singfake: Singing voice deepfake detection",
      "authors": [
        "Y Zang",
        "Y Zhang",
        "M Heydari",
        "Z Duan"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Spoofing speech detection using modified relative phase information",
      "authors": [
        "L Wang",
        "S Nakagawa",
        "Z Zhang",
        "Y Yoshida",
        "Y Kawakami"
      ],
      "year": "2017",
      "venue": "IEEE Journal of selected topics in signal processing"
    },
    {
      "citation_id": "5",
      "title": "Spoofed speech detection with a focus on speaker embedding",
      "authors": [
        "H Tran"
      ],
      "venue": "Spoofed speech detection with a focus on speaker embedding",
      "doi": "10.21437/Interspeech.2024-481"
    },
    {
      "citation_id": "6",
      "title": "Speech foundation model ensembles for the controlled singing voice deepfake detection (ctrsvdd) challenge 2024",
      "authors": [
        "A Guragain",
        "T Liu",
        "Z Pan",
        "H Sailor",
        "Q Wang"
      ],
      "year": "2024",
      "venue": "2024 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "7",
      "title": "Audio deepfake detection with self-supervised wavlm and multi-fusion attentive classifier",
      "authors": [
        "Y Guo",
        "H Huang",
        "X Chen",
        "H Zhao",
        "Y Wang"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Towards generalisable and calibrated audio deepfake detection with self-supervised representations",
      "authors": [
        "O Pascu",
        "A Stan",
        "D Oneata",
        "E Oneata",
        "H Cucu"
      ],
      "venue": "Towards generalisable and calibrated audio deepfake detection with self-supervised representations",
      "doi": "10.21437/Interspeech.2024-1302"
    },
    {
      "citation_id": "9",
      "title": "A robust audio deepfake detection system via multi-view feature",
      "authors": [
        "Y Yang"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Improved deepfake detection using whisper features",
      "authors": [
        "P Kawa",
        "M Plata",
        "M Czuba",
        "P Szymański",
        "P Syga"
      ],
      "venue": "Improved deepfake detection using whisper features",
      "doi": "10.21437/Interspeech.2023-1537"
    },
    {
      "citation_id": "11",
      "title": "Comprehensive layer-wise analysis of ssl models for audio deepfake detection",
      "authors": [
        "Y Kheir",
        "Y Samih",
        "S Maharjan",
        "T Polzehl",
        "S Möller"
      ],
      "year": "2025",
      "venue": "Comprehensive layer-wise analysis of ssl models for audio deepfake detection",
      "arxiv": "arXiv:2502.03559"
    },
    {
      "citation_id": "12",
      "title": "Singing voice graph modeling for singfake detection",
      "authors": [
        "X Chen",
        "H Wu",
        "R Jang",
        "H.-Y Lee"
      ],
      "venue": "Singing voice graph modeling for singfake detection",
      "doi": "10.21437/Interspeech.2024-1185"
    },
    {
      "citation_id": "13",
      "title": "Emofake: An initial dataset for emotion fake audio detection",
      "authors": [
        "Y Zhao",
        "J Yi",
        "J Tao",
        "C Wang",
        "Y Dong"
      ],
      "year": "2024",
      "venue": "China National Conference on Chinese Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "Heterogeneity over homogeneity: Investigating multilingual speech pre-trained models for detecting audio deepfake",
      "authors": [
        "O Chetia Phukan",
        "G Kashyap",
        "A Buduru",
        "R Sharma"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: NAACL 2024",
      "doi": "10.18653/v1/2024.findings-naacl.160"
    },
    {
      "citation_id": "15",
      "title": "Investigation of ensemble of self-supervised models for speech emotion recognition",
      "authors": [
        "Y Wu",
        "P Yue",
        "C Cheng",
        "T Li"
      ],
      "year": "2023",
      "venue": "2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "16",
      "title": "Unispeech-sat: Universal speech representation learning with speaker aware pre-training",
      "authors": [
        "S Chen"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "18",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Hubert: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "20",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "21",
      "title": "Improved deepfake detection using whisper features",
      "authors": [
        "P Kawa",
        "M Plata",
        "M Czuba",
        "P Szymański",
        "P Syga"
      ],
      "year": "2023",
      "venue": "Improved deepfake detection using whisper features",
      "arxiv": "arXiv:2306.01428"
    },
    {
      "citation_id": "22",
      "title": "Deepfake detection of singing voices with whisper encodings",
      "authors": [
        "F Sharma",
        "P Gupta"
      ],
      "year": "2025",
      "venue": "ICASSP 2025 -2025 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP49660.2025.10887871"
    },
    {
      "citation_id": "23",
      "title": "Imagebind: One embedding space to bind them all",
      "authors": [
        "R Girdhar"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Languagebind: Extending video-language pretraining to n-modality by semantic alignment",
      "authors": [
        "B Zhu"
      ],
      "year": "2023",
      "venue": "Languagebind: Extending video-language pretraining to n-modality by semantic alignment",
      "arxiv": "arXiv:2310.01852"
    },
    {
      "citation_id": "25",
      "title": "Transforming the embeddings: A lightweight technique for speech emotion recognition tasks",
      "authors": [
        "O Chetia Phukan",
        "A Balaji",
        "R Buduru",
        "Sharma"
      ],
      "venue": "Transforming the embeddings: A lightweight technique for speech emotion recognition tasks",
      "doi": "10.21437/Interspeech.2023-2561"
    },
    {
      "citation_id": "26",
      "title": "Investigating the effectiveness of speaker embeddings for shout intensity prediction",
      "authors": [
        "T Fukumori",
        "T Ishida",
        "Y Yamashita"
      ],
      "year": "2023",
      "venue": "2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference"
    }
  ]
}