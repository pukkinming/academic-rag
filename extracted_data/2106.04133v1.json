{
  "paper_id": "2106.04133v1",
  "title": "Efficient Speech Emotion Recognition Using Multi-Scale Cnn And Attention",
  "published": "2021-06-08T06:45:42Z",
  "authors": [
    "Zixuan Peng",
    "Yu Lu",
    "Shengfeng Pan",
    "Yunfeng Liu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition from speech is a challenging task. Recent advances in deep learning have led bi-directional recurrent neural network (Bi-RNN) and attention mechanism as a standard method for speech emotion recognition, extracting and attending multi-modal features -audio and text, and then fusing them for downstream emotion classification tasks. In this paper, we propose a simple yet efficient neural network architecture to exploit both acoustic and lexical information from speech. The proposed framework using multi-scale convolutional layers (MSCNN) to obtain both audio and text hidden representations. Then, a statistical pooling unit (SPU) is used to further extract the features in each modality. Besides, an attention module can be built on top of the MSCNN-SPU (audio) and MSCNN (text) to further improve the performance. Extensive experiments show that the proposed model outperforms previous state-of-the-art methods on IEMOCAP dataset with four emotion categories (i.e., angry, happy, sad and neutral) in both weighted accuracy (WA) and unweighted accuracy (UA), with an improvement of 5.0% and 5.2% respectively under the ASR setting.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech-based emotion recognition has raised a lot of attention in both speech and natural language processing in recent years. Emotion recognition -the task of automatically recognizing the human emotional states (i.e. happy, sad, anger, neutral) expressed in natural speech. It has been an important sub-task in building an intelligent system in many fields, such as customer support call review and analysis, mental health surveillance, human-machine interaction, etc. One important challenge in speech emotion recognition is that, very often, the interaction between audio and language can change the expressed emotional states. For example, the utterance 'Yes, I did quite a lot' can be ambiguous without knowing prosody information. In contrast, 'You know what, I'm sick and tired of listening to you' can be considered neutral if the voice is flat and no lexical content is provided. Thus, it is expected to consider both lexical and acoustic information in emotion recognition from speech. Recently, deep learning based approaches has shown great performance in emotion recognition  [1, 2, 3] . Recurrent neural networks (RNN) and attention mechanism have demonstrated impressive results in this task. In  [2] , an attention network is used to learn the alignment between speech and text, together with Bi-LSTM network to model the sequence in emotion recognition. In addition,  [3]  proposed a multi-hop attention to select relevant parts of the textual data and then attend to the audio feature for later classification purpose. However, these proposed methods are typically computationally expensive and complex in network structure. In this paper, we first propose a simple convolutional neural network (CNN) and pooling -based model termed as multiscale CNN with statistical pooling units (MSCNN-SPU), which learns both speech and text modalities in tandem effectively for emotion recognition. Additionally, with an attention module built on top of the MSCNN-SPU, resulting in MSCNN-SPU-ATT, the overall performance can be further improved. In our extensive experiments on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset  [4] , we show that a) Our MSCNN-SPU model outperforms previous state-of-the-art (SOTA) approaches for bi-modal speech emotion recognition by 4.4% and 4.3% relative improvement in terms of WA and UA; b) Attention module (MSCNN-SPU-ATT) can further improve the overall performance by 0.6% and 0.9% compare to the MSCNN-SPU. The rest of the work is structured as follows, Section 2 compares our work with prior studies in speech emotion recognition. We then present our proposed model in detail in Section 3. We show the extensive experimental results in Section 4 to compare with previous works, and we conclude the paper in Section 5.\n\nReproducibility. All our code will be available in opensource on Github 1 .\n\n1 https://github.com/julianyulu/icassp2021-mscnn-spu arXiv:2106.04133v1 [cs.SD] 8 Jun 2021",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Various approaches to address speech emotion recognition tasks have been investigated using classical machine learning algorithm. For example, previous works studied to model handcrafted temporal features from raw signal using Hidden Markov Models (HMMs)  [5] , or rely on high-level statistical features using Gaussian Mixture Models (GMMs)  [6] .\n\nBenefited from the development of deep learning, many approaches based on deep neural networks (DNNs) have emerged recently. Researchers have demonstrated the effectiveness of CNNs in emotion classification with audio features  [7, 8]  and text information  [9] . Additionally, RNN based models are also investigated to tackle the problem through sequence modeling  [10] . However, either audio or text is used in these methods; while human emotional state is usually expressed through an interaction between speech and text.\n\nMulti-modal approaches make use of both text and audio features. In  [11] , a hybrid approach using WordNet and part-of-speech tagging are combined with standard audio features, then classified by a Support Vector Machine. Using DNNs,  [12]  extracted text features from multi-resolution CNNs and audio information from BiLSTM, and optimized the task using a weighted sum of classification loss and verification loss. In  [2] , an LSTM network is used to model the sequence for both audio and text. Then, a soft alignment between audio and text is performed using attention.  [1]  fused learned features by introducing attention mechanism between text and audio. Specifically, they proposed a so-called multi-hop-attention mechanism to improve the performance and achieved competitive results on the IEMOCAP dataset, which further exploiting the increasingly complicated modelling scheme such as residual learning, attention, etc.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model",
      "text": "In this section, we will discuss the architecture of our model. We begin with the multi-scale convolutional neural network in use. Next, the grouped parallel pooling layers -named as statistical pooling unit (SPU), and the attention layer are investigated accordingly.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Scale Cnn",
      "text": "Motivated by the Text-CNN architecture used in  [9] , We adopt and build multiple CNN layers using a group of filters with different kernel sizes for the two separated path, text and audio. We name it as Multi-scale CNN (MSCNN). As shown in Figure  1 , various single layer two-dimensional convolutions with ReLU activation  [13]  are applied in parallel to the input features for text and audio. We employ [a 1 , ..., a N ] to represent the sequence of acoustic feature vectors (i.e. Mel-Frequencycepstral Coefficients, or MFCC) in an utterance , where N is the number of frames; [t 1 , ..., t M ] represents word embedding vectors, with M indicates the number of tokens in a sentence. Let Ω A,T = {(s, d A,T ), s ∈ S A,T } be sets of kernels for audio modality (superscript 'A') and text modality (superscript 'T'), where s is the kernel size along the dimension of input sequence, and d A and d T are the dimensions of MFCC and word embedding vectors respectively.\n\nBy applying the MSCNN, a set of feature maps are obtained:\n\nwhere k is the kernel function, and x stands for the input feature. Each of the output feature map y A,T (s,d)∈Ω is a 2D-matrix which keeps the sequential context while having a series of hidden representations corresponding to the various CNN filters.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Statistical Pooling Unit (Spu)",
      "text": "Previous work with CNNs for emotion recognition task frequently use a single layer of global max-pooling or global average-pooling, and has been proven efficient in  [7, 8] . It is intuitive that the abstraction provided by different pooling techniques can help in modeling different emotions. Therefore, we propose a statistic pooling unit (SPU, denoted as G SPU γ∈{max,avg,std} ) which consists of three parallel one-dimensional poolings along the sequence modelling direction: a) global max pooling; b) global average pooling; c) global standard deviation pooling, as shown in equation 3. The SPU operation is applied to the output of MSCNN, as shown in the following equation:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Attention",
      "text": "Inspired by the concept of attention mechanism in  [1, 3] , we propose a bi-modal attention layer build on top of the audio-MSCNN-SPU and the text-MSCNN. Different from previous work, We consider the outputs from the former as context vectors e γ∈{max,avg,std} (i.e. the max-pooling, avg-pooling, stdpooling feature vectors from the audio branch). The weighting coefficient s γ k is computed as a product between the context vectors e γ and the k th output feature map from text-MSCNN h k in terms of the outputs from max-pooling, avgpooling and std-pooling, respectively, as shown in Figure  1 . The resulting attention vector S is obtained by weighting h k with s γ k , as indicated in the following equations:\n\n, where γ ∈ {max, avg, std}\n\nS = concat(S max , S avg , S std ) (6)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Classification Layer",
      "text": "For speech emotion classification, audio and text feature vectors from SPU as well as the attended vector are concatenated, combining with SWEM vector which is the concatenation of results from various poolings over the learned word embeddings directly  [14] . The resulting vector is passed through a fully-connected layer for dimensionality reduction. Finally, a softmax layer is used to classify the input example into one of the m-class emotions, with categorical cross entropy as the loss function:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "We discuss the dataset, feature extraction, implementation details and evaluation results in this section.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data",
      "text": "We use the IEMOCAP dataset  [4] , which is a widely used benchmark in emotion recognition research. It contains approximately 12 hours of audiovisual data from 10 experi-enced actors (5 males and 5 females) in both improvised and scripted English conversations. For each dialogue, the emotional information is provided in the mode of audio, transcriptions, video, and motion capture recordings. We use audio and transcriptions only in this research. To be comparable with previous researches [1, 2, 3], 4 categories of emotions are used: angry (1103 utterances), sad (1084 utterances), neutral (1708 utterances) and happy (1636 utterances, merged with excited), resulting in a total of 5531 utterances. Following previous work, we perform a 10-fold cross-validation with 8, 1, 1 in train, dev, test set respectively. Every experiment is run for 3 times to avoid randomness, and the averaged result is used as the final performance score.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Extraction And Implementation Details",
      "text": "For the audio feature, we use 32-dimensional MFCC feature (frame size 25 ms, hop length 10 ms with Hamming window) combined with its first-and second-order frame-to-frame difference, making it a feature with dimension of 96 in total. The MFCC features are extracted using the librosa  [15]  package. Besides, the X-vector embeddings  [16]  are used as a complementary audio feature, which is extracted from a pre-trained TDNN model on the VoxCeleb dataset  [17]  in the speaker identification task, using the Kaldi speech recognition toolkit  [18] . For the text feature, we use 300-dimensional GloVe  [19]  embedding as the pretrained word embedding for the tokenized transcripts. In addition to the ground-truth text provided by the IEMOCAP database, audio-based ASR transcripts are obtained through the Speech-to-Text API from Google 2  . The performance of the Google Speech-to-Text API is evaluated in terms of the word error rate (WER), which yields 5.80%.\n\nTo implement our model, the filter number is set to 128 for every CNN layer. In text encoder, SWEM-max and SWEMavg features  [14]  are obtained from the word embeddings and then appended to the output of text-SPU. On the other hand, X-vector embeddings are appended to the output of audio-SPU. We minimize the cross-entropy loss using Adam optimizer with a learning rate of 0.0005. Gradient clipping is employed with a norm 1. The dropout method is applied with a dropout rate of 0.3 for the purpose of regularization. The batch size is set to 64. Besides, The evaluation metrics used are weighted accuracy (WA) and unweighted accuracy (UA)  3  .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Performance Evaluation",
      "text": "The experimental results are presented in Table  1 . First, we train models with single modality (utterance or ground-truth transcripts only). For speech modality, we use MSCNN+SPU as proposed in Section 3. Besides, we also report the experimental results using Audio-BRE (LSTM) in  [3] , CNN+LSTM in  [20]  and TDNN+LSTM in  [21]  for comparison. For text modality, we employ MSCNN+SPU to compare with the Text-BRE  [3] . Second, we compare our proposed approach with other multimodal approaches. One straightforward way is to train one LSTM network for each modality, then concatenating the last hidden state from each, as depicted in MDRE  [1] . Learning alignment  [2]  employs an LSTM network to model the sequence for both audio and text. Then, a soft alignment between audio and text is performed using attention in the model. In MHA  [3] , a so-called multi-hop attention is proposed, using hidden representation of one modality as a context vector and apply attention method to the other modality, then repeating such scheme several times. As shown in Table  1 , Our proposed approach achieves the best results on both WA (80.3%) and UA (81.4%) comparing to the other approaches reported in their original papers. In practical scenario, the ground-truth transcript may not be available. Therefore, we also performed experiments using ASR-processed transcript shown in Table  1 . The ASRprocessed transcript degrades the performance (roughly 2%) comparing to ground-truth transcripts. However, the performance of our model is still competitive, specifically, it outperforms the previous SOTA by 5.0% and 5.2% in WA and UA respectively. Furthermore, we conducted an ablation study to analyze the influence of each component in our model, as illustrated in Table  2 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we proposed a simple yet effective CNN and attention based neural network to solve the emotion recog-nition task from speech. The proposed model combines audio content and text information, forming a multimodal approach for effective emotion recognition. Extensive experiments show that the proposed MSCNN-SPU-ATT architecture outperforms previous SOTA in 4-class emotion classification by 5.0% and 5.2% in terms of WA and UA respectively in IEMOCAP dataset. The model is further tested on ASRprocessed transcripts and achieved competitive results which shows its robustness in real world scenario when ground-truth transcripts are not available.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , various single layer two-dimensional convolu-",
      "page": 2
    },
    {
      "caption": "Figure 1: The resulting attention vector S is obtained by weighting hk",
      "page": 2
    },
    {
      "caption": "Figure 1: Architecture of the MSCNN-SPU-ATT model. From bottom to top: a) Input layer: prepare MFCC features from the",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "ABSTRACT"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "Emotion recognition from speech is a challenging task. Re-"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "cent advances in deep learning have led bi-directional recur-"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "rent neural network (Bi-RNN) and attention mechanism as a"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "standard method for speech emotion recognition, extracting"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "and attending multi-modal features - audio and text, and then"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "fusing them for downstream emotion classiﬁcation tasks.\nIn"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "this paper, we propose a simple yet efﬁcient neural network"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "architecture to exploit both acoustic and lexical\ninformation"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "from speech. The proposed framework using multi-scale con-"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "volutional layers (MSCNN) to obtain both audio and text hid-"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "den representations.\nThen, a statistical pooling unit\n(SPU)"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "is used to further extract\nthe features in each modality. Be-"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "sides, an attention module can be built on top of the MSCNN-"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "SPU (audio) and MSCNN (text) to further improve the perfor-"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "mance. Extensive experiments show that the proposed model"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "outperforms previous state-of-the-art methods on IEMOCAP"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "dataset with four emotion categories (i.e., angry, happy, sad"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "and neutral) in both weighted accuracy (WA) and unweighted"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "accuracy (UA), with an improvement of 5.0% and 5.2% re-"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "spectively under the ASR setting."
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "Index\nTerms— Speech\nEmotion Recognition,\nDeep"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "Learning and Natural Language Processing"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "1.\nINTRODUCTION"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "Speech-based emotion recognition has raised a lot of atten-"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "tion in both speech and natural language processing in recent"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "years.\nEmotion recognition -\nthe task of automatically rec-"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "ognizing the human emotional states (i.e. happy, sad, anger,"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "neutral) expressed in natural speech. It has been an important"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "sub-task in building an intelligent system in many ﬁelds, such"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "as customer support call\nreview and analysis, mental health"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "surveillance, human-machine interaction, etc."
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "One important challenge in speech emotion recognition is"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "that, very often,\nthe interaction between audio and language"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "can change the expressed emotional states. For example,\nthe"
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": ""
        },
        {
          "panacea raynor@hotmail.com, {julianlu, nickpan, glenliu}@wezhuiyi.com": "utterance ‘Yes,\nI did quite a lot’ can be ambiguous without"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "embedding vectors, with M indicates the number of\ntokens"
        },
        {
          "2. RELATED WORK": "Various approaches\nto address\nspeech emotion recognition",
          "where N is the number of frames; [t1, ..., tM ] represents word": "in a sentence. Let ΩA,T = {(s, dA,T ), s ∈ SA,T } be sets of"
        },
        {
          "2. RELATED WORK": "tasks have been investigated using classical machine learn-",
          "where N is the number of frames; [t1, ..., tM ] represents word": "kernels for audio modality (superscript ‘A’) and text modality"
        },
        {
          "2. RELATED WORK": "ing algorithm. For example, previous works studied to model",
          "where N is the number of frames; [t1, ..., tM ] represents word": "(superscript ‘T’), where s is the kernel size along the dimen-"
        },
        {
          "2. RELATED WORK": "handcrafted temporal features from raw signal using Hidden",
          "where N is the number of frames; [t1, ..., tM ] represents word": "sion of input sequence, and dA and dT are the dimensions of"
        },
        {
          "2. RELATED WORK": "Markov Models (HMMs) [5], or rely on high-level statistical",
          "where N is the number of frames; [t1, ..., tM ] represents word": "MFCC and word embedding vectors respectively."
        },
        {
          "2. RELATED WORK": "features using Gaussian Mixture Models (GMMs) [6].",
          "where N is the number of frames; [t1, ..., tM ] represents word": "By applying the MSCNN, a set of\nfeature maps are ob-"
        },
        {
          "2. RELATED WORK": "Beneﬁted from the development of deep learning, many",
          "where N is the number of frames; [t1, ..., tM ] represents word": "tained:"
        },
        {
          "2. RELATED WORK": "approaches\nbased\non\ndeep\nneural\nnetworks\n(DNNs)\nhave",
          "where N is the number of frames; [t1, ..., tM ] represents word": "(x, θ) = (cid:8)yA,T\n|α ∈ Ω(cid:9)\nGMSCNN\n(1)"
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "α\nα"
        },
        {
          "2. RELATED WORK": "emerged recently.\nResearchers have demonstrated the\nef-",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "(cid:100) d\n(cid:100) s"
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "2 (cid:101)\n2 (cid:101)"
        },
        {
          "2. RELATED WORK": "fectiveness of CNNs\nin emotion classiﬁcation with audio",
          "where N is the number of frames; [t1, ..., tM ] represents word": "(cid:88)\n(cid:88)"
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "k[m, n] · x[i − m, j − n]\nyA,T"
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "α=(s,d)∈Ω[i, j] ="
        },
        {
          "2. RELATED WORK": "features [7, 8] and text\ninformation [9]. Additionally, RNN",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "m=(cid:98)− s\nn=(cid:98)− d"
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "2 (cid:99)\n2 (cid:99)"
        },
        {
          "2. RELATED WORK": "based models\nare\nalso investigated to tackle\nthe problem",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "(2)"
        },
        {
          "2. RELATED WORK": "through sequence modeling [10]. However, either audio or",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "where k is the kernel function, and x stands for the input fea-"
        },
        {
          "2. RELATED WORK": "text is used in these methods; while human emotional state is",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "ture. Each of the output feature map yA,T"
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "(s,d)∈Ω is a 2D-matrix"
        },
        {
          "2. RELATED WORK": "usually expressed through an interaction between speech and",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "which keeps the sequential context while having a series of"
        },
        {
          "2. RELATED WORK": "text.",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "hidden representations corresponding to the various CNN ﬁl-"
        },
        {
          "2. RELATED WORK": "Multi-modal approaches make use of both text and au-",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "ters."
        },
        {
          "2. RELATED WORK": "dio features.\nIn [11], a hybrid approach using WordNet and",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "part-of-speech\ntagging\nare\ncombined with\nstandard\naudio",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "3.2.\nStatistical Pooling Unit (SPU)"
        },
        {
          "2. RELATED WORK": "features,\nthen classiﬁed by a Support Vector Machine. Us-",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "ing DNNs, [12] extracted text features from multi-resolution",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "Previous work with CNNs for emotion recognition task fre-"
        },
        {
          "2. RELATED WORK": "CNNs and audio information from BiLSTM, and optimized",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "quently use a single layer of global max-pooling or global"
        },
        {
          "2. RELATED WORK": "the task using a weighted sum of classiﬁcation loss and ver-",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "average-pooling, and has been proven efﬁcient\nin [7, 8].\nIt"
        },
        {
          "2. RELATED WORK": "iﬁcation loss.\nIn [2],\nan LSTM network is used to model",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "is\nintuitive that\nthe abstraction provided by different pool-"
        },
        {
          "2. RELATED WORK": "the sequence for both audio and text.\nThen,\na soft align-",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "ing\ntechniques\ncan\nhelp\nin modeling\ndifferent\nemotions."
        },
        {
          "2. RELATED WORK": "ment between audio and text\nis performed using attention.",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "Therefore, we\npropose\na\nstatistic\npooling\nunit\n(SPU,\nde-"
        },
        {
          "2. RELATED WORK": "[1]\nfused learned features by introducing attention mecha-",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "noted\nas GSPU\nconsists\nof\nthree\nparallel\nγ∈{max,avg,std}) which"
        },
        {
          "2. RELATED WORK": "nism between text and audio.\nSpeciﬁcally,\nthey proposed a",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "one-dimensional poolings along the sequence modelling di-"
        },
        {
          "2. RELATED WORK": "so-called multi-hop-attention mechanism to improve the per-",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "rection:\na) global max pooling; b) global average pooling;"
        },
        {
          "2. RELATED WORK": "formance and achieved competitive results on the IEMOCAP",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "c) global standard deviation pooling, as shown in equation 3."
        },
        {
          "2. RELATED WORK": "dataset, which further exploiting the increasingly complicated",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "The SPU operation is applied to the output of MSCNN, as"
        },
        {
          "2. RELATED WORK": "modelling scheme such as residual learning, attention, etc.",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "shown in the following equation:"
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "(cid:111)\n(cid:110)"
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "(x)(cid:1)|α ∈ Ω, γ ∈ {max, avg, std}\n(3)\n(cid:0)GMSCNN\nE =\nGSPU"
        },
        {
          "2. RELATED WORK": "3. MODEL",
          "where N is the number of frames; [t1, ..., tM ] represents word": "α\nγ"
        },
        {
          "2. RELATED WORK": "In this section, we will discuss the architecture of our model.",
          "where N is the number of frames; [t1, ..., tM ] represents word": "3.3. Attention"
        },
        {
          "2. RELATED WORK": "We begin with the multi-scale convolutional neural network",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "Inspired by the concept of attention mechanism in [1, 3], we"
        },
        {
          "2. RELATED WORK": "in use.\nNext,\nthe grouped parallel pooling layers\n- named",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "propose a bi-modal attention layer build on top of the audio-"
        },
        {
          "2. RELATED WORK": "as statistical pooling unit\n(SPU), and the attention layer are",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "MSCNN-SPU and the text-MSCNN. Different from previous"
        },
        {
          "2. RELATED WORK": "investigated accordingly.",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "work, We consider the outputs from the former as context vec-"
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "the max-pooling, avg-pooling, std-\ntors eγ∈{max,avg,std} (i.e."
        },
        {
          "2. RELATED WORK": "3.1. Multi-scale CNN",
          "where N is the number of frames; [t1, ..., tM ] represents word": "pooling feature vectors from the audio branch). The weight-"
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "ing coefﬁcient sγ"
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "k is computed as a product between the con-"
        },
        {
          "2. RELATED WORK": "Motivated by the Text-CNN architecture used in [9], We",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "feature map from text-\ntext vectors eγ\nand the kth output"
        },
        {
          "2. RELATED WORK": "adopt and build multiple CNN layers using a group of ﬁlters",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "MSCNN hk in terms of the outputs from max-pooling, avg-"
        },
        {
          "2. RELATED WORK": "with different kernel sizes for the two separated path, text and",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "pooling and std-pooling,\nrespectively, as shown in Figure 1."
        },
        {
          "2. RELATED WORK": "audio. We name it as Multi-scale CNN (MSCNN). As shown",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "The resulting attention vector S is obtained by weighting hk"
        },
        {
          "2. RELATED WORK": "in Figure 1, various\nsingle layer\ntwo-dimensional convolu-",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "with sγ"
        },
        {
          "2. RELATED WORK": "",
          "where N is the number of frames; [t1, ..., tM ] represents word": "k, as indicated in the following equations:"
        },
        {
          "2. RELATED WORK": "tions with ReLU activation [13] are applied in parallel\nto the",
          "where N is the number of frames; [t1, ..., tM ] represents word": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": "multiple scales followed by statistic pooling units and attention; e) Output layer: emotion classiﬁcation with softmax layer after"
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": "(5)"
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": "(6)"
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": "(7)"
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": ""
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": "[4], which is a widely used"
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": "It contains ap-"
        },
        {
          "raw audio and word embedding vectors from the text; b) c) d) MSCNN + SPU + Attention: concatenate features extracted at": "from 10 experi-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Comparison results on the IEMOCAP dataset us- Table 2. Ablation study on proposed model. The gain for",
      "data": [
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "Methods\nModality"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "Speech-only"
        },
        {
          "font indicates best performance.": "Audio-BRE [3]\nA"
        },
        {
          "font indicates best performance.": "A\nCNN+LSTM [20]"
        },
        {
          "font indicates best performance.": "TDNN+LSTM [21]\nA"
        },
        {
          "font indicates best performance.": "Audio-CNN (ours)\nA"
        },
        {
          "font indicates best performance.": "A\nAudio-CNN-xvector (ours)"
        },
        {
          "font indicates best performance.": "Ground-truth transcript"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "Text-BRE [3]\nT"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "Text-CNN (ours)\nT"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "A+T\nMDRE [1]"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "Learning alignment [2]\nA+T"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "MHA [3]\nA+T"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "MSCNN-SPU (ours)\nA+T"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "MSCNN-SPU-ATT (ours)\nA+T"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "ASR transcript"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "Text-CNN (ours)\nT"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "MDRE [1]\nA+T"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "Learning alignment [2]\nA+T"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "MHA [3]\nA+T"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "MSCNN-SPU (ours)\nA+T"
        },
        {
          "font indicates best performance.": ""
        },
        {
          "font indicates best performance.": "A+T\nMSCNN-SPU-ATT (ours)"
        },
        {
          "font indicates best performance.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Comparison results on the IEMOCAP dataset us- Table 2. Ablation study on proposed model. The gain for",
      "data": [
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "ing speech-only, ground-truth transcript, and ASR processed",
          "Table 2. Ablation study on proposed model.": "each component is shown.",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "transcript\nfrom Google Cloud Speech API.\n‘A’ and ‘T’\nrep-",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "Methods",
          "The gain for": "WA\nUA"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "resents audio modality and text modality respectively. Bold",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "font indicates best performance.",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "MSCNN-SPU-ATT",
          "The gain for": "80.3%\n81.4%"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "MSCNN-SPU",
          "The gain for": "79.5%\n80.4%"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "Methods\nModality\nWA\nUA",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "w/o X-vectors",
          "The gain for": "78.5%\n79.3%"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "Speech-only",
          "Table 2. Ablation study on proposed model.": "w/o Text-SPU (with max-pooling only)",
          "The gain for": "77.7%\n78.6%"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "Audio-BRE [3]\nA\n64.6%\n65.2%",
          "Table 2. Ablation study on proposed model.": "w/o Text-SWEM",
          "The gain for": "77.2%\n78.3%"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "A\n68.8%\n59.4%\nCNN+LSTM [20]",
          "Table 2. Ablation study on proposed model.": "w/o Audio-SPU (with max-pooling only)",
          "The gain for": "73.5%\n74.0%"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "TDNN+LSTM [21]\nA\n70.1%\n60.7%",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "Audio-CNN (ours)\nA\n65.4%\n66.7%",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "A\n66.6%\n68.4%\nAudio-CNN-xvector (ours)",
          "Table 2. Ablation study on proposed model.": "4.3. Performance Evaluation",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "Ground-truth transcript",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": "The experimental results are presented in Table 1. First, we"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "Text-BRE [3]\nT\n69.8%\n70.3%",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": "train models with single modality (utterance or ground-truth"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "Text-CNN (ours)\nT\n67.8%\n67.7%",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": "transcripts only). For speech modality, we use MSCNN+SPU"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "A+T\n71.8%\n-\nMDRE [1]",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": "as proposed in Section 3. Besides, we also report the experi-"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "Learning alignment [2]\nA+T\n72.5%\n70.9%",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": "mental results using Audio-BRE (LSTM) in [3], CNN+LSTM"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "MHA [3]\nA+T\n76.5%\n77.6%",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": "in [20] and TDNN+LSTM in [21] for comparison. For text"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "MSCNN-SPU (ours)\nA+T\n79.5%\n80.4%",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "modality, we",
          "The gain for": "employ MSCNN+SPU to compare with the"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "MSCNN-SPU-ATT (ours)\nA+T\n80.3%\n81.4%",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": "Text-BRE [3]. Second, we compare our proposed approach"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "ASR transcript",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": "with other multimodal approaches. One straightforward way"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "is to train one LSTM network for each modality,",
          "The gain for": "then con-"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "Text-CNN (ours)\nT\n62.4%\n61.5%",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "catenating the\nlast hidden state\nfrom each,",
          "The gain for": "as depicted in"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "MDRE [1]\nA+T\n69.1%\n-",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": "MDRE [1]. Learning alignment [2] employs an LSTM net-"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "Learning alignment [2]\nA+T\n70.4%\n69.5%",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "work to model",
          "The gain for": "the sequence for both audio and text. Then,"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "MHA [3]\nA+T\n73.0%\n73.9%",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "a soft alignment between audio and text",
          "The gain for": "is performed using"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "MSCNN-SPU (ours)\nA+T\n77.4%\n78.2%",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "attention in the model.",
          "The gain for": "In MHA [3], a so-called multi-hop"
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "A+T\n78.0%\n79.1%\nMSCNN-SPU-ATT (ours)",
          "Table 2. Ablation study on proposed model.": "",
          "The gain for": ""
        },
        {
          "Table 1. Comparison results on the IEMOCAP dataset us-": "",
          "Table 2. Ablation study on proposed model.": "attention is proposed,",
          "The gain for": "using hidden representation of one"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "“Deep neural networks for emotion recognition combin-"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "ing audio and transcripts,” in INTERSPEECH, 2018."
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "[13] Vinod Nair and Geoffrey E. Hinton,\n“Rectiﬁed linear"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "units improve restricted boltzmann machines,” in ICML,"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "2010."
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "[14] Dinghan\nShen, Guoyin Wang, Wenlin Wang, Mar-"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "tin Renqiang Min, Qinliang Su, Yizhe Zhang, Chun-"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "yuan Li, Ricardo Henao, and Lawrence Carin, “Baseline"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "needs more\nlove:\nOn simple word-embedding-based"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "models and associated pooling mechanisms,”\nin Pro-"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "ceedings of the 56th Annual Meeting of the Association"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "for Computational Linguistics (Volume 1: Long Papers),"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "Melbourne, Australia, July 2018, pp. 440–450, Associ-"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "ation for Computational Linguistics."
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "[15] Brian McFee, Colin Raffel, Dawen Liang, Daniel Ellis,"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "Matt Mcvicar, Eric Battenberg, and Oriol Nieto,\n“li-"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "brosa: Audio and music signal analysis in python,”\n01"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "2015, pp. 18–24."
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "[16] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "S. Khudanpur,\n“X-vectors:\nRobust dnn embeddings"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "for speaker\nrecognition,”\nin 2018 IEEE International"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "(ICASSP), 2018, pp. 5329–5333."
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "[17] Arsha Nagrani,\nJoon Son Chung, Weidi Xie, and An-"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "drew Zisserman, “Voxceleb: Large-scale speaker veriﬁ-"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "cation in the wild,” Computer Speech & Language, vol."
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "60, pp. 101027, 10 2019."
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "[18] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luk´aˇs"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "Burget, Ondrej Glembek, Nagendra Goel, Mirko Han-"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "nemann, Petr Motl´ıˇcek, Yanmin Qian, Petr Schwarz, Jan"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "Silovsk´y, Georg Stemmer, and Karel Vesel,\n“The kaldi"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "IEEE 2011 Workshop on\nspeech recognition toolkit,”"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "Automatic Speech Recognition and Understanding, 01"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "2011."
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": ""
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "[19]\nJeffrey Pennington, Richard Socher, and Christopher D."
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "Manning,\n“Glove: Global vectors for word representa-"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "tion,”\nin Empirical Methods in Natural Language Pro-"
        },
        {
          "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,": "cessing (EMNLP), 2014, pp. 1532–1543."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "nition task from speech. The proposed model combines au-": "dio content and text\ninformation,\nforming a multimodal ap-",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "“Lstm recurrent neural networks for short\ntext and sen-"
        },
        {
          "nition task from speech. The proposed model combines au-": "proach for effective emotion recognition. Extensive experi-",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "timent classiﬁcation,” 05 2017, pp. 553–562."
        },
        {
          "nition task from speech. The proposed model combines au-": "ments show that\nthe proposed MSCNN-SPU-ATT architec-",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "[11]\nJasmine Bhaskar, K. Sruthi,\nand Prema Nedungadi,"
        },
        {
          "nition task from speech. The proposed model combines au-": "ture outperforms previous SOTA in 4-class emotion classiﬁ-",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "“Hybrid approach for emotion classiﬁcation of audio"
        },
        {
          "nition task from speech. The proposed model combines au-": "cation by 5.0% and 5.2% in terms of WA and UA respectively",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "Pro-\nconversation based on text and speech mining,”"
        },
        {
          "nition task from speech. The proposed model combines au-": "in IEMOCAP dataset. The model\nis further tested on ASR-",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "cedia Computer Science, vol. 46, pp. 635 – 643, 2015."
        },
        {
          "nition task from speech. The proposed model combines au-": "processed transcripts and achieved competitive results which",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "shows its robustness in real world scenario when ground-truth",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "[12]\nJaejin Cho, Raghavendra Pappagari, Purva Kulkarni,"
        },
        {
          "nition task from speech. The proposed model combines au-": "transcripts are not available.",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "Jes´us Villalba, Yishay Carmiel,\nand Najim Dehak,"
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "“Deep neural networks for emotion recognition combin-"
        },
        {
          "nition task from speech. The proposed model combines au-": "6. REFERENCES",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "ing audio and transcripts,” in INTERSPEECH, 2018."
        },
        {
          "nition task from speech. The proposed model combines au-": "[1] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "[13] Vinod Nair and Geoffrey E. Hinton,\n“Rectiﬁed linear"
        },
        {
          "nition task from speech. The proposed model combines au-": "“Multimodal\nspeech emotion recognition using audio",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "units improve restricted boltzmann machines,” in ICML,"
        },
        {
          "nition task from speech. The proposed model combines au-": "and text,” 12 2018.",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "2010."
        },
        {
          "nition task from speech. The proposed model combines au-": "[2] Haiyang Xu, Hui Zhang, Kun Han, Yun Wang, Yiping",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "[14] Dinghan\nShen, Guoyin Wang, Wenlin Wang, Mar-"
        },
        {
          "nition task from speech. The proposed model combines au-": "Peng, and Xiangang Li, “Learning Alignment for Mul-",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "tin Renqiang Min, Qinliang Su, Yizhe Zhang, Chun-"
        },
        {
          "nition task from speech. The proposed model combines au-": "timodal Emotion Recognition from Speech,”\nin Proc.",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "yuan Li, Ricardo Henao, and Lawrence Carin, “Baseline"
        },
        {
          "nition task from speech. The proposed model combines au-": "Interspeech 2019, 2019, pp. 3569–3573.",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "needs more\nlove:\nOn simple word-embedding-based"
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "models and associated pooling mechanisms,”\nin Pro-"
        },
        {
          "nition task from speech. The proposed model combines au-": "[3] Seunghyun Yoon,\nSeokhyun Byun,\nSubhadeep Dey,",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "ceedings of the 56th Annual Meeting of the Association"
        },
        {
          "nition task from speech. The proposed model combines au-": "and Kyomin Jung,\n“Speech emotion recognition using",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "ICASSP 2019 - 2019",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "for Computational Linguistics (Volume 1: Long Papers),"
        },
        {
          "nition task from speech. The proposed model combines au-": "multi-hop attention mechanism,”",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "IEEE International Conference on Acoustics,\nSpeech",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "Melbourne, Australia, July 2018, pp. 440–450, Associ-"
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "ation for Computational Linguistics."
        },
        {
          "nition task from speech. The proposed model combines au-": "and Signal Processing (ICASSP), pp. 2822–2826, 2019.",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "[4] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "[15] Brian McFee, Colin Raffel, Dawen Liang, Daniel Ellis,"
        },
        {
          "nition task from speech. The proposed model combines au-": "Kazemzadeh,\nEmily Mower\nProvost,\nSamuel Kim,",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "Matt Mcvicar, Eric Battenberg, and Oriol Nieto,\n“li-"
        },
        {
          "nition task from speech. The proposed model combines au-": "Jeannette\nChang,\nSungbok\nLee,\nand\nShrikanth",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "brosa: Audio and music signal analysis in python,”\n01"
        },
        {
          "nition task from speech. The proposed model combines au-": "Narayanan,\n“Iemocap:\nInteractive\nemotional dyadic",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "2015, pp. 18–24."
        },
        {
          "nition task from speech. The proposed model combines au-": "Language Resources and\nmotion capture database,”",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "[16] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and"
        },
        {
          "nition task from speech. The proposed model combines au-": "Evaluation, vol. 42, pp. 335–359, 12 2008.",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "S. Khudanpur,\n“X-vectors:\nRobust dnn embeddings"
        },
        {
          "nition task from speech. The proposed model combines au-": "[5] Tin Nwe, S.W. Foo, and Liyanage De Silva,\n“Speech",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "for speaker\nrecognition,”\nin 2018 IEEE International"
        },
        {
          "nition task from speech. The proposed model combines au-": "emotion\nrecognition\nusing\nhidden markov models,”",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "nition task from speech. The proposed model combines au-": "Speech Communication, vol. 41, pp. 603–623, 11 2003.",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "(ICASSP), 2018, pp. 5329–5333."
        },
        {
          "nition task from speech. The proposed model combines au-": "[6] Daniel Neiberg, K. Ejenius, and K. Laskowski,\n“Emo-",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "[17] Arsha Nagrani,\nJoon Son Chung, Weidi Xie, and An-"
        },
        {
          "nition task from speech. The proposed model combines au-": "tion recognition in spontaneous\nspeech using gmms,”",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "drew Zisserman, “Voxceleb: Large-scale speaker veriﬁ-"
        },
        {
          "nition task from speech. The proposed model combines au-": "INTER SPEECH, pp. 809–812, 01 2007.",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "cation in the wild,” Computer Speech & Language, vol."
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "60, pp. 101027, 10 2019."
        },
        {
          "nition task from speech. The proposed model combines au-": "[7] B. Zhang, C. Quan, and F. Ren,\n“Study on cnn in the",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "recognition of emotion in audio and images,”\nin 2016",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "[18] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luk´aˇs"
        },
        {
          "nition task from speech. The proposed model combines au-": "IEEE/ACIS 15th International Conference on Computer",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "Burget, Ondrej Glembek, Nagendra Goel, Mirko Han-"
        },
        {
          "nition task from speech. The proposed model combines au-": "and Information Science (ICIS), 2016, pp. 1–5.",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "nemann, Petr Motl´ıˇcek, Yanmin Qian, Petr Schwarz, Jan"
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "Silovsk´y, Georg Stemmer, and Karel Vesel,\n“The kaldi"
        },
        {
          "nition task from speech. The proposed model combines au-": "[8] Dias Issa, M. Fatih Demirci, and Adnan Yazici, “Speech",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "IEEE 2011 Workshop on\nspeech recognition toolkit,”"
        },
        {
          "nition task from speech. The proposed model combines au-": "emotion recognition with deep convolutional neural net-",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "Automatic Speech Recognition and Understanding, 01"
        },
        {
          "nition task from speech. The proposed model combines au-": "works,” Biomedical Signal Processing and Control, vol.",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "2011."
        },
        {
          "nition task from speech. The proposed model combines au-": "59, pp. 101894, 2020.",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": ""
        },
        {
          "nition task from speech. The proposed model combines au-": "[9] Yoon Kim, “Convolutional neural networks for sentence",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "[19]\nJeffrey Pennington, Richard Socher, and Christopher D."
        },
        {
          "nition task from speech. The proposed model combines au-": "the 2014 Conference on\nclassiﬁcation,” Proceedings of",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "Manning,\n“Glove: Global vectors for word representa-"
        },
        {
          "nition task from speech. The proposed model combines au-": "Empirical Methods in Natural Language Processing, 08",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "tion,”\nin Empirical Methods in Natural Language Pro-"
        },
        {
          "nition task from speech. The proposed model combines au-": "2014.",
          "[10]\nJakub Nowak, Ahmet Taspinar,\nand Rafal\nScherer,": "cessing (EMNLP), 2014, pp. 1532–1543."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[20] Aharon Satt, Shai Rozenberg, and Ron Hoory, “Efﬁcient": ""
        },
        {
          "[20] Aharon Satt, Shai Rozenberg, and Ron Hoory, “Efﬁcient": "spectrograms,” in INTERSPEECH, 2017."
        },
        {
          "[20] Aharon Satt, Shai Rozenberg, and Ron Hoory, “Efﬁcient": "[21] Mousmita Sarma, Pegah Ghahremani, Daniel Povey,"
        },
        {
          "[20] Aharon Satt, Shai Rozenberg, and Ron Hoory, “Efﬁcient": "Nagendra Kumar Goel, Kandarpa Kumar Sarma,"
        },
        {
          "[20] Aharon Satt, Shai Rozenberg, and Ron Hoory, “Efﬁcient": ""
        },
        {
          "[20] Aharon Satt, Shai Rozenberg, and Ron Hoory, “Efﬁcient": "signals using dnns,”\nin Proc."
        },
        {
          "[20] Aharon Satt, Shai Rozenberg, and Ron Hoory, “Efﬁcient": "pp. 3097–3101."
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "venue": "Multimodal speech emotion recognition using audio and text"
    },
    {
      "citation_id": "3",
      "title": "Learning Alignment for Multimodal Emotion Recognition from Speech",
      "authors": [
        "Haiyang Xu",
        "Hui Zhang",
        "Kun Han",
        "Yun Wang",
        "Yiping Peng",
        "Xiangang Li"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Subhadeep Dey",
        "Kyomin Jung"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Provost",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources Evaluation"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "S Tin Nwe",
        "Liyanage Foo",
        "Silva De"
      ],
      "year": "2003",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition in spontaneous speech using gmms",
      "authors": [
        "K Daniel Neiberg",
        "K Ejenius",
        "Laskowski"
      ],
      "venue": "Emotion recognition in spontaneous speech using gmms"
    },
    {
      "citation_id": "8",
      "title": "Study on cnn in the recognition of emotion in audio and images",
      "authors": [
        "B Zhang",
        "C Quan",
        "F Ren"
      ],
      "year": "2016",
      "venue": "2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS)"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "M Dias Issa",
        "Adnan Demirci",
        "Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "10",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Yoon Kim"
      ],
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Lstm recurrent neural networks for short text and sentiment classification",
      "authors": [
        "Jakub Nowak",
        "Ahmet Taspinar",
        "Rafal Scherer"
      ],
      "year": "2017",
      "venue": "Lstm recurrent neural networks for short text and sentiment classification"
    },
    {
      "citation_id": "12",
      "title": "Hybrid approach for emotion classification of audio conversation based on text and speech mining",
      "authors": [
        "Jasmine Bhaskar",
        "K Sruthi",
        "Prema Nedungadi"
      ],
      "year": "2015",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "13",
      "title": "Deep neural networks for emotion recognition combining audio and transcripts",
      "authors": [
        "Jaejin Cho",
        "Raghavendra Pappagari",
        "Purva Kulkarni",
        "Jesús Villalba",
        "Yishay Carmiel",
        "Najim Dehak"
      ],
      "year": "2018",
      "venue": "Deep neural networks for emotion recognition combining audio and transcripts"
    },
    {
      "citation_id": "14",
      "title": "Rectified linear units improve restricted boltzmann machines",
      "authors": [
        "Vinod Nair",
        "Geoffrey Hinton"
      ],
      "year": "2010",
      "venue": "ICML"
    },
    {
      "citation_id": "15",
      "title": "Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms",
      "authors": [
        "Dinghan Shen",
        "Guoyin Wang",
        "Wenlin Wang",
        "Martin Renqiang Min",
        "Qinliang Su",
        "Yizhe Zhang",
        "Chunyuan Li",
        "Ricardo Henao",
        "Lawrence Carin"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "16",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "Daniel Ellis",
        "Matt Mcvicar",
        "Eric Battenberg",
        "Oriol Nieto"
      ],
      "year": "2015",
      "venue": "librosa: Audio and music signal analysis in python"
    },
    {
      "citation_id": "17",
      "title": "X-vectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Voxceleb: Large-scale speaker verification in the wild",
      "authors": [
        "Arsha Nagrani",
        "Son Joon",
        "Weidi Chung",
        "Andrew Xie",
        "Zisserman"
      ],
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "19",
      "title": "The kaldi speech recognition toolkit",
      "authors": [
        "Daniel Povey",
        "Arnab Ghoshal",
        "Gilles Boulianne",
        "Lukáš Burget",
        "Ondrej Glembek",
        "Nagendra Goel",
        "Mirko Hannemann",
        "Petr Motlíček",
        "Yanmin Qian",
        "Petr Schwarz",
        "Jan Silovský",
        "Georg Stemmer",
        "Karel Vesel"
      ],
      "year": "2011",
      "venue": "IEEE"
    },
    {
      "citation_id": "20",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "21",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "Aharon Satt",
        "Shai Rozenberg",
        "Ron Hoory"
      ],
      "year": "2017",
      "venue": "Efficient emotion recognition from speech using deep learning on spectrograms"
    },
    {
      "citation_id": "22",
      "title": "Emotion identification from raw speech signals using dnns",
      "authors": [
        "Mousmita Sarma",
        "Pegah Ghahremani",
        "Daniel Povey",
        "Kumar Nagendra",
        "Kandarpa Goel",
        "Najim Sarma",
        "Dehak"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    }
  ]
}