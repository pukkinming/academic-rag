{
  "paper_id": "2410.11376v1",
  "title": "Physioformer: Integrating Multimodal Physiological Signals And Symbolic Regression For Explainable Affective State Prediction â‹†",
  "published": "2024-10-15T08:16:33Z",
  "authors": [
    "Zhifeng Wang",
    "Wanxuan Wu",
    "Chunyan Zeng"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "As affective computing becomes increasingly crucial in health monitoring and psychological intervention, accurately identifying affective states is a key challenge. While traditional machine learning models have achieved some success in affective computation, their ability to handle complex, multimodal physiological signals is limited. Most affective computing tasks still rely heavily on traditional methods, with few deep learning models applied, particularly in multimodal signal processing. Given the importance of stress monitoring for mental health, developing a highly reliable and accurate affective computing model is essential. In this context, we propose a novel model-PhysioFormer, for affective state prediction using physiological signals. PhysioFormer model integrates individual attributes and multimodal physiological data to address inter-individual variability, enhancing its reliability and generalization across different individuals. By incorporating feature embedding and affective representation modules, PhysioFormer model captures dynamic changes in time-series data and multimodal signal features, significantly improving accuracy. The model also includes an explainability model that uses symbolic regression to extract laws linking physiological signals to affective states, increasing transparency and explainability. Experiments conducted on the Wrist and Chest subsets of the WESAD dataset confirmed the model's superior performance, achieving over 99% accuracy, outperforming existing SOTA models. Sensitivity and ablation experiments further demonstrated PhysioFormer's reliability, validating the contribution of its individual components. The integration of symbolic regression not only enhanced model explainability but also highlighted the complex relationships between physiological signals and affective states. Future work will focus on optimizing the model for larger datasets and real-time applications, particularly in more complex environments. Additionally, further exploration of physiological signals and environmental factors will help build a more comprehensive affective computing system, advancing its use in health monitoring and psychological intervention.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction 1.Background",
      "text": "With the rapid development of society and the increasing pace of life, the importance of emotions for individual physical and mental health has become increasingly evident  [1] . Emotions are spontaneously generated from daily experiences, stimulating the body to produce hormones and affecting various aspects, including bodily movements, facial expressions, and physiological characteristics  [2] . The accumulation of negative emotions can lead to depression. It is estimated that about one in five men and one in three women worldwide will experience major depression during their lifetime. Although other mental illnesses, such as schizophrenia and bipolar disorder, are less common, they still have a significant impact on individuals' lives  [3] . The contemporary focus on mental and physical stress is growing, as the effects of stress on the human body are both widespread and profound, particularly in its impact on the brain, cardiovascular system, immune system, and metabolism. Chronic stress not only leads to mental health issues but may also alter gene expression through epigenetic mechanisms, further affecting physiological functions  [4] .\n\nIn existing research, physiological signals have been applied across various fields. First, in the domain of healthcare, monitoring physiological signals enables early warnings of chronic diseases and health management  [5] . Second, in affective computation and mental health, analyzing signals such as electrodermal activity and electrocardiograms can identify individuals' affective states, providing effective tools for emotion management. Additionally, in intelligent human-computer interaction, the application of physiological signals can optimize user experience by allowing devices to respond according to users' physiological and affective states  [6] . Given the significant impact of negative emotions on daily life, monitoring affective states through physiological signals is particularly important. Moreover, the current global healthcare trend is shifting toward preventing chronic diseases and reducing treatment costs, with wearable devices playing a crucial role in this development  [7] . These devices can monitor users' physiological signals in real time, providing personalized health recommendations and helping individuals detect potential health issues early, thus promoting a shift in healthcare systems from reactive treatment to proactive prevention.\n\nTraditional affective computation has largely relied on questionnaires  [8] , which are often highly subjective. The reliability of the results is closely tied to the respondent's attentiveness and the seriousness with which they approach the questionnaire. Moreover, individuals are often unable to accurately perceive their own affective states. For instance, people who are under chronic stress may not be fully aware of the extent of the stress they are experiencing. Therefore, using questionnaires to identify affective states is undoubtedly time-consuming and inefficient. As a result, subsequent research has shifted towards utilizing machine learning and deep learning methods to predict affective states through physiological signals.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Gap",
      "text": "Numerous studies have demonstrated that physiological signals such as electrocardiograms (ECG), electrodermal activity (EDA), and electroencephalograms (EEG) play a crucial role in affective computation. For example, Han et al.  [9]  proposed a method that combines graph signal processing (GSP) with convolutional neural networks (CNNs), utilizing GSP to process EEG data in order to better capture the spatial features within EEG signals, for EEG-based affective classification. Song et al.  [10]  introduced a dynamic graph convolutional neural network (DGCNN) for EEG affective recognition. The core of this approach involves constructing a graph structure based on EEG signals and using graph convolutional networks (GCNs) to capture the spatiotemporal dependencies between different brain regions, thereby improving the accuracy of affective computation. Ashwin et al.  [11]  assessed individual stress states by monitoring multiple physiological indicators such as heart rate (HR), EDA, and body temperature, and applying machine learning algorithms (e.g., k-nearest neighbors (KNN), support vector machine (SVM)) to classify the extracted features.\n\nDespite significant progress in the field of affective recognition, several key challenges and research gaps remain. First, in terms of reliability, while the complexity and diversity of physiological signals are widely acknowledged, many studies still inadequately collect and utilize multimodal physiological data. This shortfall not only limits the model's generalization capability under multimodal data conditions but also affects its stability in cross-individual affective recognition tasks. Due to the high variability of individual physiological signals, the generalizability and reliability of existing models in practical applications are weakened, and the lack of data support further restricts their applicability in diverse scenarios. Second, although models exhibit high accuracy in certain tasks, affective states are inherently dynamic processes. Existing methods significantly lack the ability to capture the continuity and temporal features of emotional changes, especially when dealing with complex physiological signals. This limitation reduces the model's ability to accurately recognize dynamic affective states. Finally, the issue of model explainability remains prominent. Most deep learning models still function as \"black boxes,\" making it difficult to clearly explain their decision-making processes and feature selection  [12] . This lack of transparency not only reduces users' trust in model predictions but also limits the model's explainability and transparency in real-world applications. Therefore, developing an affective recognition model that emphasizes the integration and utilization of multimodal physiological data, enhances the ability to capture dynamic processes, and improves decision-making transparency could not only advance the accuracy and practicality of affective recognition but also drive progress in stress monitoring and mental health interventions. machine learning models and existing deep learning models tend to exhibit instability. Therefore, in designing the PhysioFormer model, we incorporated three submodules: ContribNet, AffectNet, and AffectAnalyser, which not only process physiological signal data in parallel but also effectively capture the temporal dynamics and complexity of physiological signals. This enhances the model's ability to detect the timeliness and continuity of emotional changes. Additionally, we introduced an upper triangular matrix encoding in the ContribNet module, allowing the model to flexibly focus on important information at different time points while also sensitizing it to the temporal dynamics of physiological signals. This addresses the limitations of traditional machine learning models and existing deep learning networks in effectively capturing the intrinsic relationships within temporally continuous data. Furthermore, to account for individual baseline physiological state differences, we incorporated features describing individual attributes into the dataset, such as age, gender, height, weight, smoking habits, and whether the individual exercised today. These individual attributes features provide rich contextual information, helping the model generalize better across different individuals and improving the accuracy and reliability of affective state prediction. The introduction of individual attributes features allows the model to better understand and explain physiological signals, thereby enhancing its adaptability. 2. Improving reliability in cross-subject affective computation. To enhance the reliability of cross-subject affective computation, we employed a cross-validation training approach. In each epoch, the model sequentially uses data from each individual for training, explicitly learning and adapting to the physiological signal characteristics of different individuals. This training method effectively reduces the risk of overfitting to specific individuals, thereby improving the model's generalization ability and making it more robust in cross-subject affective computation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Development Of A High-Precision Affective Computation Model. The Physioformer Model Has Demonstrated",
      "text": "exceptional ability in capturing the intrinsic relationships within physiological signal data, significantly improving the accuracy of predicting individuals' affective states. Through rigorous experiments and in-depth analysis on the WEASD dataset, we validated the effectiveness of the PhysioFormer model. On both the Wrist and Chest sub-datasets, the PhysioFormer model achieved over 99% accuracy, far surpassing the performance of current state-of-the-art (SOTA) models. These results not only highlight the potential of PhysioFormer in the field of affective computation but also demonstrate its reliability and reliability in handling complex physiological signals, providing strong support for precise prediction of individual affective states.  4 . Enhancing model explainability. To improve the model's explainability, we integrated an Explanation Model using symbolic regression to generate mathematical formulas that describe the relationship between input variables and outputs  [13] . In our research, this model analyzed the influence of each physiological indicator on affective states, generating formulas that reveal the impact of signals like heart rate variability (HRV), EDA, and ECG on affective prediction. These formulas not only enhance transparency but also help users better understand the model's decision-making process, increasing both explainability and trust in the model's predictions.\n\nThe structure of this paper is arranged as follows. Section 2 provides a detailed overview of the related work in the field. Section 3 defines the mathematical notation and model architecture used in this study. In section 4, we propose and describe the PhysioFormer model and the symbolic regression task in detail. Section 5 covers the dataset, baseline models, experimental setup, and evaluation metrics. Finally, section 6 summarizes the study and discusses future research directions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Affective Computation",
      "text": "Affective computing aims to analyze physiological signals, facial expressions, vocal patterns, and other behavioral data using sensors, algorithms, and machine learning techniques to predict or identify an individual's affective state  [14] . In recent years, the research focus of affective computing has shifted from single-mode affective classification to the fusion of multimodal data. By integrating multiple sources of signals-such as physiological signals (e.g., heart rate, electrodermal activity), facial expressions, vocal features, and environmental factors-researchers can more accurately capture and recognize an individual's affective state. This multimodal approach not only improves the accuracy of affective recognition but also enhances the timeliness and continuity of tracking affective state changes.\n\nBernhard et al.  [15]  proposed a deep learning-based method for text-based affective recognition, with key innovations including bidirectional processing using a Bidirectional Long Short-Term Memory (BiLSTM) network, combined with Dropout regularization and a weighted loss function to address small datasets and class imbalance. Additionally, the study introduced a transfer learning method called Sent2Affect, where the network is pre-trained on affective analysis tasks and then fine-tuned for affective recognition by adjusting the output layer. This approach improved the model's performance on small-scale datasets. Experiments conducted on six benchmark datasets demonstrated that this method significantly outperformed traditional machine learning approaches, achieving a 23.2% increase in F1 scores for classification tasks and an 11.6% reduction in mean squared error for regression tasks.\n\nAshwin et al.  [11]  induced stress states in participants through acute stress manipulation tasks (such as the Maastricht Acute Stress Task  [16]  and the Montreal Imaging Stress Test  [17] ) and recorded physiological signals, including HR, HRV, EDA, and respiratory rate, using wearable sensors. The resulting stress detection models achieved accuracy rates of 97% in controlled environments and 93% in everyday settings. This study confirmed the effectiveness of multimodal physiological signal fusion in stress detection and demonstrated the feasibility of wearable devices for stress monitoring across different environments. Similarly, Sarkar et al.  [18]  proposed an affective recognition method based on ECG signals using a self-supervised deep multi-task learning framework. By pre-training the network on signal transformation tasks and then transferring it to affective classification, the model showed significant performance improvements on the AMIGOS, DREAMER, WESAD, and SWELL datasets, outperforming traditional fully supervised methods. This validates the efficacy of multi-task learning in ECG-based affective recognition. Akre et al.  [19]  introduced a depression symptom detection framework using data collected from iPhones and Apple Watches. A gradient boosting classifier processed health data, including vital signs, activity levels, and sleep patterns. The model exhibited moderate predictive accuracy, with ROC AUC values ranging from 0.63 to 0.72, demonstrating the potential of personalized sensor data for depression detection. These studies collectively highlight the significant potential of multimodal physiological signals and personalized health data in the detection and recognition of stress, emotions, and depression symptoms.\n\nKoldijk et al.  [20]  collected a multimodal dataset specifically designed for research on stress and user modeling, incorporating physiological signals (such as ECG, EMG, EEG), facial expressions, and behavioral data from mouse and keyboard usage. In a controlled experimental environment, participants performed various tasks (e.g., writing, reading, and meetings) to simulate real-world work scenarios. Stress levels were validated using a combination of questionnaires, interviews, and objective measurements. The experimental results indicated a significant correlation between induced stress conditions and both physiological and behavioral data, demonstrating the value of the SWELL dataset for developing robust stress detection models.\n\nSiddharth et al.  [21]  explored affective computing using multimodal data, leveraging biosignals (EEG, ECG, GSR, HRV) and visual data (facial video) in experiments conducted on four publicly available multimodal affective datasets: DEAP, AMIGOS, MAHNOB-HCI, and DREAMER. These datasets include a range of physiological signals and visual information, with participants self-reporting affective states after watching videos. The results showed that combining biosignals with visual data significantly improved affective classification performance, with the model outperforming previous research particularly on the DEAP and MAHNOB-HCI datasets. Additionally, the study addressed issues such as discrepancies in sampling rates, sensor positions, and the number of signal channels through dataset fusion and transfer learning. Both studies emphasize the potential of multimodal data in stress and affective computing, offering valuable resources for the development of personalized stress management and affective recognition systems.\n\nInspired by the success of deep learning in processing multimodal information  [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44] , this study focuses exclusively on using physiological signals for affective computing, aiming to accurately identify participants' affective states through these signal features. In contrast to multimodal data fusion approaches, this research emphasizes optimizing the processing and classification of single physiological signals, exploring their potential in affective recognition. This approach offers the possibility of simplifying affective computing systems while reducing the complexity of data collection and processing.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Explainability Method",
      "text": "Symbolic regression is defined as the process of discovering symbolic expressions that fit data for an unknown function. Although this problem is theoretically considered NP-hard, in practice, many functions exhibit simplifying properties such as symmetry, separability, and compositionality, making the task feasible  [13] . Symbolic regression can be applied across various fields, including economics, psychology, and biomedical engineering. It aids researchers in uncovering hidden mathematical models from experimental data, thus revealing the underlying dynamics of complex systems.\n\nThe SINDy (Sparse Identification of Nonlinear Dynamics) method proposed by Brunton et al.  [45]  combines symbolic regression with sparse regression to overcome the limitations of traditional symbolic regression in handling complex nonlinear equations, such as high computational cost and overfitting. SINDy reduces the candidate function space and uses convex optimization to generate concise and explainable equations, ensuring both efficiency and explainability in large-scale systems. This method has been successfully applied to complex systems such as fluid vortices and chaotic Lorenz systems, demonstrating broad applicability. Rogers et al.  [46]  further expanded the application of symbolic regression by integrating it with model-based design of experiments (MBDoE). Symbolic regression generates explainable expressions, while MBDoE optimizes experimental conditions, allowing for rapid differentiation between model candidates and significantly improving process optimization efficiency. This approach has performed exceptionally well in industrial processes like multiphase product synthesis. By incorporating physical knowledge constraints, the complexity of symbolic regression is controlled, enhancing both the explainability and practical value of the models. This combined method shows significant potential for applications in digital manufacturing, process engineering, and new product development, making it suitable for both laboratory research and industrial production optimization.\n\nThe expanded application of symbolic regression in psychology and other fields demonstrates its significant interdisciplinary potential. Masato et al.  [47]  were the first to apply the symbolic regression tool AI-Feynman to intertemporal choice experiments in psychology, successfully generating seven candidate discount function models, some of which outperformed existing hyperbolic discount models. This study shows that symbolic regression can not only automatically uncover hidden patterns in psychology but also significantly improve the automation and precision of research, shifting away from traditional approaches reliant on human intuition and experience. The introduction of symbolic regression offers psychologists a novel data analysis method, enhancing the accuracy of theoretical modeling and the reproducibility of studies, thereby showcasing the unique value of applying AI technology to the social sciences. Liu et al.  [48]  further expanded the application of symbolic regression by integrating it with deep learning in the field of knowledge tracing. Their method automatically extracts algebraic expressions of learners' cognitive states, revealing the underlying patterns of skill acquisition. In experiments on the large-scale Lumosity training dataset, symbolic regression not only improved the model's fit but also discovered entirely new patterns of skill acquisition, verifying some existing theoretical findings. This approach provides theoretical support for analyzing large-scale behavioral data, particularly excelling in dynamic learning processes and naturally generated data, addressing challenges like the unobservability of cognitive states and the expanding search space of symbolic models. The application of symbolic regression in psychology, education, and related fields opens new avenues for research on skill acquisition, cognitive diagnostics, and personalized learning path modeling, demonstrating its broad potential in automated pattern discovery and model building.\n\nInspired by the high performance of deep learning modeling  [49, 50, 51, 52, 53, 41, 54, 55, 56, 57, 58, 59, 60, 61] , this paper introduces symbolic regression into affective computing and combines it with deep learning models, offering a new approach for automated affective state inference. Traditional affective computing relies on black-box models, which, while accurate, lack explainability. Symbolic regression addresses this by extracting explainable algebraic expressions from neural networks, enabling the analysis of complex relationships between physiological signals and behavioral features. This improves model transparency and provides a theoretical basis for the quantitative analysis of affective states.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Sequential Model",
      "text": "Sequential models play a crucial role in affective computing, particularly when handling time-series data, where they have demonstrated exceptional performance. Traditional sequence models such as Hidden Markov Models (HMM)  [62]  and Conditional Random Fields (CRF)  [63]  have been widely applied in fields like natural language processing. However, with the rise of deep learning, more advanced models like Recurrent Neural Networks (RNN)  [64]  and Long Short-Term Memory Networks (LSTM)  [65]  have been introduced into affective computing to better capture temporal dependencies.\n\nRNN are among the earliest deep learning models used for processing sequential data. They capture contextual information in time series through recurrently connected neurons and are suitable for various sequence tasks, such as speech recognition and machine translation. However, RNNs face challenges when dealing with long-term dependencies due to the vanishing and exploding gradient problems, making it difficult to retain distant dependencies  [64] . To address these limitations, LSTM networks were developed. LSTMs use unique memory cells and gating mechanisms to effectively overcome the information loss problem in traditional RNNs when handling long sequences.\n\nTheir memory cells retain important information over time, controlling what to keep and what to forget  [66] . LSTM networks excel at tasks involving long-term dependencies, particularly in affective computing, such as speech affective recognition  [67]  and affective state prediction based on physiological signals  [68] . The memory cells in LSTM allow the model to capture and retain affective state information over extended periods, improving the accuracy of affective recognition. The combination of LSTM with CNNs further enhances the performance of multimodal affective recognition. CNNs are adept at processing spatial features, such as facial expressions in images or other static physiological signals, while LSTMs handle temporal features, such as dynamic changes in speech and heart rate. This combination allows multimodal affective computing to process both complex time-series data and integrate information from different modalities, leading to more accurate affective state predictions.\n\nRecent studies have further advanced the application of Transformer-based sequence models in affective computing. Unlike LSTM, Transformer models process sequential data in parallel through the self-attention mechanism, overcoming the efficiency bottlenecks that RNNs and LSTMs face when handling long sequences. This makes Transformers particularly suitable for processing longer time-series data  [69] . The self-attention mechanism flexibly captures global contextual information within a sequence without relying on sequential order, making Transformer models more effective in feature extraction and temporal modeling. Mittal et al.  [70]  proposed an M3ER model based on multimodal data, integrating facial expressions, text, and speech modalities. The model incorporates multiplicative fusion techniques to weigh the reliability of each modality, allowing it to automatically emphasize more reliable modalities while suppressing those with higher noise levels. Additionally, M3ER applies canonical correlation analysis (CCA) to filter out irrelevant signals from modalities and generate proxy features, enhancing the model's robustness against noise and missing data. Experimental results on the IEMOCAP and CMU-MOSEI benchmark datasets show that this method improved accuracy in affective computation tasks by approximately 5% compared to previous models.\n\nRecent researchers have applied the aforementioned approaches to affective computation tasks. Kumar et al.  [71]  proposed two deep learning-based methods for speech affective recognition: CNN-LSTM and Vision Transformer (ViT). The study experimentally compared the performance of these two models in handling affective recognition tasks, focusing on the advantages of CNN-LSTM in audio feature extraction and affective classification, as well as the potential of ViT for processing speech signals through Mel-spectrograms. The CNN-LSTM model achieved an accuracy of 88.50% on the EMO-DB dataset, while the Vision Transformer model reached 85.36%. This work highlights the potential of deep learning technologies, particularly attention mechanisms and image processing techniques, in speech affective recognition, demonstrating the effectiveness of combining CNN and LSTM to extract affective features from speech signals.\n\nInspired by the powerful information extraction capabilities of deep learning  [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88] , this paper focuses on physiological signal sequence models, aiming to explore how to better utilize physiological signals for affective state prediction and enhance the accuracy and generalization of affective computing through multimodal approaches.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Preliminary",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Notations And Definitions",
      "text": "In this section, we formally define the computational modules involved in the affective computation tasks of this study and provide a set of mathematical notations used throughout the paper, as presented in the Table  1 .\n\nDefinition 1 (ContribNet): ContribNet is a neural network model used to compute the contribution level of physiological indicator ğ‘ ğ‘— , consisting of a batch normalization layer and two linear transformation layers. Specifically, the input data is first processed through batch normalization, followed by a linear transformation through the first weight matrix î‰ƒ\n\n))\n\nWhere:\n\nâ€¢ î‰ƒ 1 âˆˆ â„ ğ»Ã—(ğ‘˜+ğ‘š) and î‰ƒ 2 âˆˆ â„ 1Ã—ğ» represent the weight matrices for each layer; â€¢ ğ’· 1 âˆˆ â„ ğ» and ğ’· 2 âˆˆ â„ represent the biases;\n\nâ€¢ ğ» denotes the hidden layer dimension;\n\nâ€¢ ğœ denotes the activation function ReLU;\n\nâ€¢ ğµğ‘ represents the batch normalization operation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Definition 2 (Affectnet):",
      "text": "AffectNet is used to compute the affective state level reflected by physiological indicator ğ‘ ğ‘— . It extracts and transforms input features progressively through a combination of linear transformations and nonlinear activation functions across multiple hidden layers, ultimately generating the final affective state prediction. Specifically, after the input features are processed through batch normalization, they first undergo a linear transformation by the first layer weights ğ”š ğ‘ ğ‘— 1 , with an added bias ğ”Ÿ ğ‘ ğ‘— 1 , followed by nonlinear activation through the activation function ğœ. This process continues in subsequent layers until the final layer ğ‘™, where the weights ğ”š ğ‘™ produce the output. This output represents the affective state level reflected by physiological indicator ğ‘ ğ‘– for participant ğ‘ ğ‘– . The deep structure of the network allows it to capture complex feature interactions through layer-by-layer learning, enabling the estimation of the participant's affective state. This model can be formally expressed as:\n\n))\n\nWhere:\n\nğ‘™ represent the weight matrices for each layer;\n\nâ€¢ ğ”Ÿ\n\nğ‘™ represent the bias vectors for each layer; â€¢ ğœ denotes the activation function ReLU;\n\nâ€¢ ğµğ‘ represents the batch normalization operation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Definition 3 (Affectanalyser):",
      "text": "AffectAnalyser is a network model used to calculate an individual's affective state at the current moment. Each layer performs a linear transformation on the input features using weight matrices and bias vectors, progressively deriving the individual's affective prediction. Specifically, the input features undergo a linear transformation in the first layer, where a linear operation is performed using the weight matrix â„Š 1 and a bias ğ‘ 1 is added to obtain an intermediate result. This is then followed by a second linear transformation, processed by the weight matrix â„Š 2 and bias vector ğ‘ 2 , ultimately generating the affective state estimate for individual ğ‘ ğ‘– . This model can be formally represented as:\n\nWhere:\n\nâ€¢ â„Š 1 and â„Š 2 represent the weight matrices of the first and second layers, respectively;\n\nâ€¢ ğ‘ 1 and ğ‘ 2 represent the bias vectors of the first and second layers, respectively.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Problem Formulation",
      "text": "The objective of this study is to identify an individual's affective state using data obtained from human monitoring devices. The task is defined as a set îˆ¼ = {ğ‘ 1 , ğ‘",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Research Method",
      "text": "The research task of this study can be divided into two parts. First, a PhysioFormer model is constructed to predict an individual's affective state at a given moment. Through the collaborative functioning of three submodules: feature embedding, affective representation, and affective state prediction. The model effectively transforms physiological data into predicted affective states. Second, symbolic regression is applied to fit various monitoring indicators, enabling a more precise capture and understanding of the relationships between these indicators and affective states. This approach not only enhances the accuracy of affective state predictions but also provides a deeper analysis and explanations of the physiological indicators.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "The Physioformer Model",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Model Overview",
      "text": "In this section, our task is to predict an individual's affective state using physiological signal data. The structure of the PhysioFormer model proposed in this paper is shown in the Figure  1 , and it can be divided into three submodules: (1) Feature Embedding Module, which is responsible for encoding the input features and generating feature representations containing physiological data; (2) Affective Representation Module, which models and represents the individual's affective state based on the feature representations output by the Feature Embedding Module, capturing and describing the user's affective state through neural networks; (3) Prediction Module, which uses the previously obtained features to predict the individual's affective state at the current moment.\n\nBased on the aforementioned model architecture, we define the input and output of the entire model as follows:\n\nâ€¢ Input: The input data consists of various physiological signals, including but not limited to HR, HRV, and EDA. These physiological signals are captured in real time through wearable devices or other physiological monitoring tools and are provided to the model in the form of time series. Each input feature vector not only contains these physiological signals but also includes individual attributes (such as age, gender, etc.), enabling the model to better capture inter-individual differences.\n\nâ€¢ Output: The output of the model is a prediction of the individual's current affective state, providing a classification of the affective state as one of three categories: tense, calm, or excited.\n\nAfter data preprocessing, the raw physiological signal data is first processed by merging individual attributes features with physiological features to form a comprehensive feature vector. The ContribNet is then constructed for each physiological indicator to calculate its contribution level to affective prediction, and an attention mechanism is introduced to re-encode the physiological indicators. The re-encoded physiological indicators are then concatenated with individual attributes again to form a new comprehensive feature vector. For each physiological indicator, we construct an AffectNet to calculate the affective state level. Finally, the affective states of all physiological indicators are fed into AffectAnalyser, which predicts the individual's affective state at the current moment. The detailed algorithm is shown in Algorithm 1.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Data Preprocessing",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Device",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Feature Embedding Module",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Data Preprocessing",
      "text": "The raw dataset containing physiological signals typically consists of sampling times and their corresponding measurements. Before using this data as input for the neural network, proper preprocessing is required. The following section will describe in detail the data preprocessing methods employed in this study to ensure data quality.\n\nA. Denoising. During the process of data monitoring, various types of noise are often present, such as power line interference, motion artifacts, and other environmental noise  [89] . To address this, we employed a Butterworth lowpass filter to remove high-frequency noise. This is a smoothing filter with maximally flat response characteristics  [90] . of the monitoring data is ğ¿ seconds, and the length of each window is set to ğ‘‡ seconds. Therefore, the monitoring data of participant ğ‘ ğ‘– in the dataset can be divided into ğœ‰ ğ‘ ğ‘– = âŒŠ ğ¿ ğ‘‡ âŒ‹ non-overlapping time windows. The set of all windows can be represented as:\n\nThe data contained in the ğ‘›-th window can be represented as:\n\nHere, ğ‘¥ ğ‘ ğ‘– (ğ‘¡) represents the monitoring data at time ğ‘¡. Each window ğ‘‹ ğ‘ ğ‘ ğ‘– is independent, meaning there is no overlap between windows, which helps to avoid data overlap issues during feature extraction and subsequent analysis. C. Feature Extraction. Physiological signal data from the human body is complex and diverse, making it difficult to directly extract feature information. Therefore, after segmenting the dataset according to window size, feature extraction is necessary to transform the dataset into a format that can reveal affective states and be more easily processed by neural network models. To better understand and handle this data, we analyzed each type of physiological indicator and described the feature extraction methods for each indicator within each window after the fixed-size segmentation.\n\nFor each indicator, we calculated its mean, standard deviation, maximum, and minimum values within each window, collectively referred to as the basic statistical features of the signal.\n\nâ€¢ ACC: The ACC data contains three dimensions (x, y, z), and the basic statistical features were calculated for each dimension. Additionally, the sum of the acceleration across the three dimensions was computed, followed by calculating the corresponding basic statistical features.\n\nâ€¢ EDA: A low-pass filter was first applied to the raw data, and the cvxEDA  [91]  algorithm was used to compute the relevant statistical features. From these, three features that reflect both short-term affective responses and long-term affective states, as well as patterns of autonomic nervous system activity, were selected, and their corresponding basic statistical features were calculated.\n\nâ€¢ EMG: A low-pass filter was first applied to the raw data, and then the basic statistical features were calculated within each window.\n\nâ€¢ BVP: For this indicator, the peak frequency within each window was computed. As this indicator reflects heart activity, HRV was also calculated within the window using the NeuroKit2 tool  [92] .\n\nâ€¢ ECG: The NeuroKit2 tool was used to compute HRV within the window, and HRV features such as standard deviation (SDNN) and root mean square of successive differences (RMSSD) were extracted.\n\nâ€¢ TEMP: TEMP is a numerical signal, and its basic statistical features were calculated within each window. Additionally, the slope of TEMP within the window was computed to reveal temperature trends over a specific time period.\n\nâ€¢ RESP: RESP is a numerical signal, and the basic statistical features of the respiratory rate within the window were calculated.\n\nFor detailed information on the specific features, please refer to Appendix A, which provides the variable definitions and descriptions for each feature.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Feature Embedding Module",
      "text": "The core task of feature embedding module is to extract features from the data obtained through sliding windows. This step not only effectively captures the temporal characteristics of physiological signals but also enhances the model's adaptability and reliability to individual differences by incorporating individual attributes. By merging individual attributes with physiological features to form a comprehensive feature vector, the model can gain a more holistic understanding of each individual's unique physiological responses in different affective states. The contribution level of physiological indicators is calculated using ContribNet, and an attention mechanism is introduced to re-encode the physiological indicators. This allows the model to dynamically adjust its focus on various physiological signals, ensuring that the most informative features are fully utilized in affective prediction. This refined feature processing method not only improves the accuracy of affective computation but also enhances the model's ability to capture complex affective states, laying a solid foundation for subsequent affective representation and affective computation.\n\nSpecifically, the dataset is divided into multiple fixed-length windows, each containing a processed segment of continuous time-series data, which represents the overall features of each physiological indicator. This section uses two types of data to represent feature embedding. One part is the individual attributes features, containing basic information about the individual, such as age, gender, and height. Here, the individual attributes features of participant ğ‘ ğ‘– are denoted as ğ´ ğ‘ ğ‘– . The other part is the windowed monitoring data, consisting of the feature values calculated from the physiological indicator data within a given window. The monitoring data for participant ğ‘ ğ‘– is denoted as ğµ ğ‘ ğ‘– . These feature data include both discrete and continuous data. For discrete data, one-hot encoding is used to represent categorical information in a format suitable for model processing. Continuous data are directly represented by their raw values to preserve their true quantitative information. By integrating individual attributes features ğ´ ğ‘ ğ‘– with monitoring data ğµ ğ‘ ğ‘– , the overall features of the participant across all windows can be represented as:\n\nHere, âŠ• denotes the feature concatenation operation, and ğ‘ƒ ğ¹ ğ‘ ğ‘– âˆˆ â„ ğ‘˜+ğ‘š , where ğ‘š represents the dimension of the features calculated from the physiological indicator data.\n\nGiven that the overall static features ğ‘ƒ ğ¹ ğ‘ ğ‘– consist of multiple dimensions and exhibit complex linear relationships, this study constructs a ContribNet network to process these high-dimensional features. The network possesses nonlinear modeling capabilities and can effectively capture and represent the complex relationships between input features through a multi-layer structure. To accommodate the characteristics and requirements of different indicators, we built an independent ContribNet for each type of physiological indicator. In summary, the contribution level of physiological indicator ğ‘ ğ‘— can be represented as:\n\nIn the feature encoding process, special attention must be paid to the temporal variation of physiological indicators. One of the key characteristics of time series data is its sequential nature, meaning that the physiological state at a given moment is often influenced by prior moments, with this influence being progressive. Therefore, accurately capturing these dynamic temporal changes is critical during feature extraction and encoding. In the feature encoding process, two types of data are used: individual attributes features and physiological indicators. Individual attributes features can be considered static features that remain constant over the entire time period, such as age and gender, and thus do not require temporal feature fusion. However, physiological indicators vary over time, and it is essential to capture their dynamic characteristics to more accurately reflect the changes in an individual's physiological state over time. To model temporal dependencies, we constructed an upper triangular matrix. This matrix ensures that the features at each time point depend only on the current and prior moments, without relying on future moments. This approach ensures that the model is sensitive to the temporal dynamics of physiological signals, allowing it to capture more subtle emotional changes. Based on the process described above, the encoded physiological indicator ğ‘ ğ‘— for participant ğ‘ ğ‘– can be represented as:\n\n) ) ğ‘‡  (8)  Here, ğ‘‡ ğ‘Ÿğ‘–ğ‘¢ denotes the upper triangular matrix operation, and ğœ‰ represents the number of windows.\n\nBased on the aforementioned encoding process, the overall features of physiological indicator ğ‘ ğ‘— for individual ğ‘ ğ‘– across all windows can be represented as:",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Affective Representation Module",
      "text": "The core task of affective representation module is to map the overall features of an individual across all time windows to affective state levels, which is crucial for accurately predicting affective states. By aggregating features from the time series, the affective representation module can identify and model the temporal patterns of affective states. The AffectNet network is used here to map high-dimensional features into a lower-dimensional affective state space, simplifying data complexity while retaining key affective information and handling subtle affective changes. By establishing contextual relationships within the time series, the affective representation module captures the continuity and evolution of affective states, thus more accurately reflecting an individual's actual affective experience, laying the foundation for final affective classification and state prediction.\n\nTo achieve this task, we constructed an AffectNet network, which has strong feature integration capabilities and can effectively fuse and process multiple types of feature information. This method allows for more precise capture and reflection of the dynamic changes in an individual's affective state, enhancing the accuracy and reliability of affective state predictions. When building the model, an individual AffectNet network was constructed for each type of physiological indicator, allowing it to process its corresponding physiological data independently and dynamically adjust its parameters in response to variations in different physiological indicators. This enables more precise feature extraction and affective state prediction. In summary, the affective state level reflected by physiological indicator ğ‘ ğ‘— can be represented as:\n\nHere, Î˜ ğ‘ ğ‘– = {ğœƒ ğ‘ ğ‘— ğ‘ ğ‘– |ğ¢ âˆˆ ğ, ğ£ âˆˆ ğŒ} is used to represent the set of affective state levels mapped from all physiological indicators.\n\nAdditionally, to improve overall prediction accuracy and model reliability while reducing error accumulation, we introduced the initial affective state as a baseline, denoted as ğ‘¢ ğ‘ ğ‘– , representing the initial affective state level before training. Thus, Î˜ ğ‘ ğ‘– is adjusted to:\n\nThus, the model is not only able to capture affective state changes caused by variations in physiological indicators but also takes into account the individual's initial state, providing a more comprehensive affective state assessment.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Prediction Module And Model Training",
      "text": "The core task of the Prediction module is to use the individual's affective state levels to predict their current affective state. This module analyzes and processes the affective state levels output by the affective representation module and applies the AffectAnalyser network to map the affective state levels to specific affective categories.\n\nWe constructed an AffectAnalyser network to accomplish this task. By mapping the input feature vector to a high-dimensional feature space, it captures the complex relationships between input features, enabling effective classification. Based on the adjusted affective state levels Î¦ ğ‘ ğ‘– obtained earlier, the final affective prediction is represented as:\n\nDuring the training process, it is essential to account for the complexity and diversity of physiological data. Therefore, we chose to use the cross-entropy loss function. The cross-entropy loss function is well-suited for classification problems, as it measures the difference between the predicted probability distribution and the true label distribution, providing a natural probabilistic explanations and exhibiting favorable gradient properties. Additionally, we introduced a regularization term into the loss function, which helps constrain the size of the model parameters, smoothens them, prevents overfitting, and improves the model's generalization ability.\n\nThe complete loss function for the model is as follows:\n\nThe above function consists of two parts: the first part is the Cross Entropy loss function, where ğ‘’ ğ‘ ğ‘– represents the predicted affective state of the ğ‘–-th individual, and Ãªğ‘ ğ‘– represents the true affective state of the ğ‘–-th individual. This function is used to calculate the difference between the predicted and actual values in the classification task. The second part is the regularization term, where ğœ† is the regularization coefficient that controls the strength of the regularization term.\n\nBased on the aforementioned loss function, we selected the RMSprop optimizer to optimize the model. RMSprop dynamically adjusts the learning rate for each parameter by taking the moving average of the squared gradients, thus providing an adaptive learning rate for each parameter. Given that the loss function includes both the crossentropy loss and the regularization term, this may result in a non-stationary objective function, meaning that statistical data may exhibit time-varying and time-dependent properties during different periods  [93] . The RMSprop optimizer calculates the moving average of the squared gradients using exponential decay, effectively handling this non-stationary objective function and significantly improving the stability of the optimization process. Additionally, since RMSprop automatically adjusts the learning rate, it typically converges faster compared to fixed learning rate optimizers such as Stochastic Gradient Descent (SGD).\n\nIn this study, the model's training mode adopts a cross-validation strategy. Specifically, in each epoch, the model sequentially uses data from each individual for training, with the goal of explicitly learning and adapting to the physiological signal characteristics of different individuals. This method allows the model to progressively capture the unique physiological features of each individual, enhancing its ability to generalize across individuals. Specifically, in each epoch, the model is trained through the following process.\n\nFor the ğ‘–-th participant ğ‘ ğ‘– , the prediction of their affective state can be expressed as:\n\nHere, ğ´ ğ‘ ğ‘– represents the individual attributes features, and ğ‘‘ ğ‘ ğ‘– ,ğ‘ ğ‘— represents the physiological data features.\n\nFor each participant ğ‘ ğ‘– , the loss function îˆ¸ is used to measure the difference between the predicted value ğ‘’ ğ‘ ğ‘– and the true value â„ğ‘ğ‘¡ğ‘’ ğ‘ ğ‘– :\n\nIn each epoch, the model sequentially uses data from each participant for training and updates the model parameters using the gradient descent method. For the ğ‘–-th participant in the ğ‘¡-th epoch, the parameter update formula is:\n\nHere, ğœ‘ (ğ‘¡) represents the model parameters at the end of the current epoch, and ğœ‘ (ğ‘¡-1) represents the model parameters at the end of the previous epoch.\n\nIn each epoch, the model is trained on data from all ğ‘ participants. The entire training process can be represented as:\n\nHere, îˆ¸(ğ‘’ ğ‘ ğ‘– , Ãªğ‘ ğ‘– ) represents the loss function for each participant, and ğœ‚ represents the learning rate.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Symbolic Laws Extraction",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Extraction Process Overview",
      "text": "The physiological signal data involved in affective computing tasks are complex and multidimensional, often containing highly nonlinear relationships. Although the PhysioFormer model can capture these intricate dynamics, its \"black box\" nature tends to limit explainability, making it difficult to clearly understand the relationship between inputs and outputs. In this task, we aim to uncover the specific roles of various physiological indicators in affective computation, as well as the complex interactions between physiological signals. This will not only help us better understand how multiple physiological indicators jointly influence affective states, but also enhance the trust of users and researchers in the model's predictions.\n\nTo achieve this goal, we designed an Explanation Model that combines symbolic distillation with deep learning to improve both the explainability and performance of the model. The detailed process structure is shown in the Figure  3 . First, the input data is processed through the Feature Embedding and Affective Representation modules, generating features and state information. The trained PhysioFormer model generates feature importance scores, based on which key features are selected. Using symbolic regression, predicted values áº½ are generated and compared to the model's predicted values ğ‘’. This process extracts symbolic laws for each physiological indicator, making the internal mechanisms of the model more transparent and providing explanations and quantifications of the influence of different physiological indicators on affective states.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Symbolic Distillation For Physiological Indicators",
      "text": "To enhance the explainability of the internal mechanisms of the PhysioFormer model, we introduced a Explanation Model, transforming the influence of various physiological indicators on affective state prediction into symbolic expressions. In this study, we employed symbolic regression techniques with the aim of finding the optimal mathematical formula that fits the given data, thereby revealing the model's intrinsic decision logic and the interactions between physiological indicators. Let ğ‘ƒ represent the network model to be analyzed, ğ‘† represent the symbolic regression model, and ğ‘‹ âˆˆ â„ ğœ‰Ã— (ğ‘˜+ğ‘š) represent the input data to the model. According to the objective of symbolic regression, we can define the optimization goal of symbolic regression as:\n\nHere, ğœ€ represents the mathematical formula calculated by symbolic regression, ğ‘‹ denotes the range of input data, ğœ‰ represents the number of windows into which the individual's physiological data is segmented, ğ‘˜ denotes the feature dimension of the individual attributes features, and ğ‘š represents the dimension of the features computed from the physiological data. Since symbolic regression is a typical combinatorial optimization problem, the solution space grows exponentially with the number of variables. Using all the features for computation would incur enormous computational costs and reduce the model's efficiency and stability. To address this issue, we applied a gradient-based feature importance estimation method to select a subset of features, which were then used for the symbolic regression task. Specifically, this method calculates the gradient of the intermediate outputs of the model with respect to the input features. These gradients reflect how small changes in the input features influence the output  [94] . The resulting gradients may be either positive or negative, but we are concerned with the magnitude of change rather than the direction. Therefore, the absolute value of the gradients is taken to represent feature importance, and these absolute values are normalized to obtain relative importance scores. This process can be expressed by the following formula:\n\nHere, ğ‘¦ ğ‘– represents the intermediate output of the model. The gradient of the output ğ‘† with respect to the input features ğ‘‹ is calculated and then the absolute value is taken:\n\nThe importance score for each feature is calculated:\n\nHere, ğ¼ ğ‘— represents the importance score of the ğ‘—-th feature.\n\nBased on the calculated feature importance scores, a subset of features is selected for symbolic regression, denoted as ğœŒ(ğ‘‹). Based on the aforementioned process, the optimization objective of symbolic regression can be defined as:\n\nDuring the model optimization process, the absolute error loss function was used to optimize the model. Since each indicator's ContribNet and AffectNet operate independently, these components can perform symbolic regression in parallel. However, considering the coupling relationships and sequential dependencies between the components, the analysis process must be carried out step-by-step according to a predefined order to ensure overall model consistency and optimal performance.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Experiments",
      "text": "This section outlines the experimental design for comparing the performance of the PhysioFormer model with nine existing affective computation networks on two sub-datasets, Wrist and Chest, from the publicly available WESAD dataset. The goal is to evaluate the effectiveness of the proposed framework in affective computation tasks. Through this experiment, we aim to address five core research questions (RQ), providing empirical evidence to thoroughly understand the advantages and contributions of PhysioFormer and further validate its potential application in the field of affective computation.\n\nâ€¢ RQ1: How does the performance of the PhysioFormer model compare to existing SOTA models in affective computation tasks? This research question seeks to evaluate the advantages of the PhysioFormer model in processing multimodal physiological signals.\n\nâ€¢ RQ2: Does the choice of different window sizes during data preprocessing significantly affect the model's prediction performance? This question explores how the window size, a critical preprocessing parameter, impacts the model's effectiveness and accuracy.\n\nâ€¢ RQ3: How does the number of hidden layer neurons at different scales affect the performance of the Physio-Former model during training? This research question aims to assess the balance between the number of neurons in the model's structure, model complexity, and performance.\n\nâ€¢ RQ4: Does the feature embedding module have a significant impact on the model's prediction performance? This question seeks to uncover the role of feature embedding in extracting useful information and enhancing model performance, providing insights into its importance.\n\nâ€¢ RQ5: Does the inclusion of individual attributes features (e.g., age, gender) significantly improve the model's prediction performance? This question investigates the role of individual characteristics in affective computation tasks and further explains their impact on the model's generalization ability.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Dataset",
      "text": "The dataset used in this study is the publicly available WESAD dataset, designed for affective computation. It contains physiological and motion data recorded by two devices: one from a chest-worn device (RespiBAN), which includes six monitoring indicators collected at a frequency of 700Hz; and another from a wrist-worn device (Empatica E4), which includes four monitoring indicators with different sampling frequencies  [95] . The physiological indicators included in this dataset are introduced below:\n\nâ€¢ Accelerometer, ACC: ACC data records changes in acceleration in three-dimensional space (x, y, z axes), reflecting the intensity, direction, and frequency of body movement.\n\nâ€¢ Electrodermal Activity, EDA: EDA data records changes in skin conductance, which are often closely related to psychological states.\n\nâ€¢ Electromyography, EMG: EMG data captures the electrical activity of muscle fibers during contraction and relaxation, providing information on muscle function and nervous system control.\n\nâ€¢ Blood Volume Pulse, BVP: BVP data records changes in blood volume through peripheral vessels with each heartbeat, which can be used to monitor cardiovascular health and analyze heart rate variability (HRV).\n\nâ€¢ Electrocardiogram, ECG: ECG data records the electrical activity generated by the heart during each heartbeat, providing insights into heart health.\n\nâ€¢ Temperature, TEMP: TEMP data records measurements of the body's core temperature, reflecting the individual's temperature status at specific time points.\n\nâ€¢ Respiration, RESP: RESP data captures the respiratory cycle and airflow changes during breathing, reflecting an individual's respiratory rate and depth.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Compared Method",
      "text": "In this subsection, we provide an overview of the nine models used in the comparison experiments, which include both traditional machine learning methods and deep learning approaches. Specifically, the machine learning methods include Random Forest, SVM, KNN, AdaBoost, Decision Tree, and Linear Discriminant Analysis (LDA); the deep learning methods include Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and Long Short-Term Memory (LSTM). These models have been widely applied in previous affective computation studies and have achieved significant results. To ensure fairness and reliability in the comparison experiments, we reconstructed these models in this study and conducted experiments on the two sub-datasets (Wrist and Chest) of the WESAD dataset. Through this process, we are able to evaluate the performance differences between the PhysioFormer model and existing models under the same data conditions, thereby validating the effectiveness of the proposed approach and its advantages in affective computation tasks.\n\nâ€¢ Bobade et al.  [96]  proposed a stress detection system based on multimodal physiological data, utilizing machine learning to identify individuals' stress states. The models used for classification tasks include KNN, LDA, Random Forest, Decision Tree, AdaBoost, and SVM, along with a simple feedforward neural network (ANN). The classification tasks were divided into two categories: a three-class task (pleasant, neutral, and stress) and a binary task (stress vs. non-stress). The experimental results showed that in the three-class task, the best accuracy achieved by machine learning methods was 81.65%, while in the binary task, the accuracy reached  93",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Evaluation Metrics And Basic Parameterization",
      "text": "In this experiment, we used three metrics to evaluate the model's performance: accuracy (ACC), F1-Score, and Mean Squared Error (MSE). To calculate ACC and F1-Score, we first need to compute the confusion matrix. The formula is as follows:",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "ğ‘‡ ğ‘ƒ ğ´ ğ¹ ğ‘ƒ ğµ ğ¹ ğ‘ƒ ğ¶ ğ¹ ğ‘ ğ´ ğ‘‡ ğ‘ƒ ğµ ğ¹ ğ‘ƒ ğ¶ ğ¹ ğ‘ ğ´ ğ¹ ğ‘ ğµ ğ‘‡ ğ‘ƒ",
      "text": "Here, TP represents the number of samples correctly predicted as the correct class, FP represents the number of samples incorrectly predicted as the correct class, and FN represents the number of samples incorrectly predicted as the wrong class.\n\nACC calculates the proportion of correctly predicted samples among all test samples. Based on the confusion matrix, the formula for calculating ACC is as follows:\n\nF1-Score is the harmonic mean of Precision and Recall, and it can effectively handle situations with imbalanced data distributions. For each class ğ‘– âˆˆ {ğ´, ğµ, ğ¶}, the formula for calculating F1-Score is as follows:\n\nğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› ğ‘– = ğ‘‡ ğ‘ƒ ğ‘– ğ‘‡ ğ‘ƒ ğ‘– + ğ¹ ğ‘ƒ ğ‘– (26) MSE calculates the average of the squared differences between the predicted values and the actual values, providing a measure of the degree of difference between the model's predictions and the true values. The formula for MSE is:\n\nHere, ğ‘¦ ğ‘– and Å·ğ‘– represent the predicted value and the true value, respectively. The code framework for the experiments in this paper was implemented using Python 3.10 and PyTorch 2.2.2. The experiments were conducted on a Mac with an Apple M1 Pro CPU (8 cores) with 16GB of memory and a Windows machine equipped with a GeForce GTX 1650 GPU with 4GB of memory.\n\nThe basic configuration parameters are as follows:\n\nâ€¢ The maximum number of epochs was set to 150, with an early stopping mechanism to prevent overfitting.\n\nâ€¢ The initial learning rate was set to 1e-4, and a StepLR scheduler was used to dynamically adjust the learning rate.\n\nâ€¢ The batch size was adjusted depending on the dataset and window size.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Model Experiment Result And Analysis",
      "text": "",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Performance Evaluation Results (Rq1)",
      "text": "In this experiment, we used a 30-second window to segment the dataset and conducted comparative experiments between the PhysioFormer model and various traditional machine learning and deep learning models. The traditional machine learning models include Random Forest, SVM, KNN, AdaBoost, Decision Tree, and LDA, while the deep learning models include CNN, RNN, and LSTM. The specific experimental results are shown in the Table  2 :\n\nThe experimental results show that the PhysioFormer model outperformed all other models across all metrics. Its ACC on the Wrist and Chest datasets reached 99.54% and 99.21%, respectively, with F1-scores of 99.54 and 99.17, and the lowest MSE of 0.025 and 0.057, respectively. This demonstrates that PhysioFormer exhibits exceptionally high predictive accuracy and stability when processing these datasets.\n\nIn contrast, SVM performed exceptionally well among traditional machine learning algorithms, particularly on the Chest dataset, where both ACC and F1-score exceeded 98%, and MSE remained low at 0.134 and 0.128, respectively. This indicates that SVM handles high-dimensional data very effectively. The Random Forest model also performed well on both the Wrist and Chest datasets, with ACC and F1-scores approaching 97%, and MSEs of 0.151 and 0.122, respectively, demonstrating its high accuracy and low error when processing physiological data. KNN's performance on the Wrist dataset was close to that of Random Forest, but it underperformed slightly on the Chest dataset, particularly in terms of F1-score and MSE. This could be related to KNN's sensitivity to data distribution. AdaBoost and Decision Tree models performed slightly worse than the previously mentioned models, particularly in terms of ACC and F1-score. For example, AdaBoost achieved ACC values of 90.50% and 91.74% on the Wrist and Chest datasets, respectively, while Decision Tree performed noticeably better on the Chest dataset (95.41%) than on the Wrist dataset (88.27%). LDA performed the worst, especially on the Chest dataset, where ACC and F1-score were 72.93% and 68.69%, respectively, and the error was high, indicating that LDA struggles with this type of complex data.\n\nAmong deep learning models, CNN performed relatively well, particularly on the Wrist dataset, where ACC reached 98.33%, F1-score was 97.95%, and MSE was low at 0.065, showing CNN's strength in handling physiological data with spatial features. However, CNN's performance significantly dropped on the Chest dataset, with ACC at only 81.46%, F1-score falling to 75.07%, and MSE increasing to 0.185. This may suggest that CNN has limitations in adapting to different data channels. LSTM showed stability in handling time-series data, achieving 96.87% ACC, 97.02% F1-score, and 0.184 MSE on the Wrist dataset. Although its performance declined on the Chest dataset (ACC: 87.00%, F1-score: 87.39%), it was still more stable than RNN. RNN showed relatively poor performance on both datasets, especially on the Chest dataset, where ACC and F1-score were 79.26% and 80.03%, respectively, with a high MSE of 0.437, indicating that RNN may have limitations in capturing long-term dependencies and handling more complex physiological signals.\n\nBased on the experimental results, it is evident that the PhysioFormer model addresses many of the shortcomings present in traditional machine learning and deep learning models, demonstrating its unique advantages and outstanding performance. Firstly, PhysioFormer model, through its advanced architecture design, effectively overcomes the limitations of traditional machine learning models in handling multimodal physiological signals. By integrating multilevel feature embedding and affective representation modules, it extracts more representative and distinctive features from multimodal signals, excelling across diverse data types. Secondly, PhysioFormer model excels in addressing the stability issues found in deep learning models. Its adaptive feature processing mechanism not only captures dynamic changes in time-series data but also effectively fuses information across different modalities, ensuring consistent performance across various datasets. This stability gives PhysioFormer model a significant advantage in handling complex real-world scenarios. Finally, PhysioFormer model's low MSE indicates minimal prediction error, which is attributed to its innovative design in multimodal fusion and time-series modeling. By introducing a feature embedding module, PhysioFormer model is better equipped to handle long-term dependencies and sequential information, making it far superior to other models in affective computation tasks.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Convergence Rate",
      "text": "For each dataset, we trained the PhysioFormer model and recorded its loss values during the training process. The variation in the loss values over time during training is shown in the Figure  4 .\n\nThe results show that all models exhibit a rapid decline in loss values during the initial stages of training, indicating that the models quickly learn effective features, significantly reducing prediction error. More importantly, all models demonstrate good convergence in the later stages of training, with the final loss values stabilizing, suggesting that the PhysioFormer model is capable of achieving convergence when processing both the Wrist and Chest datasets with different time windows.\n\nNotably, the datasets with different time windows show some variation in convergence speed and final loss values. Although models with all time windows eventually converge, the dataset with a 30-second window exhibits faster convergence and lower final loss values. This indicates that the model can reach an optimal state more quickly with shorter time windows, while also reducing the risk of overfitting during training. In contrast, as the time window length increases, the model's convergence slows down slightly, and the final loss value is slightly higher. This may be attributed to the longer time windows introducing more temporal dependencies; while such information helps the model capture long-term trends, it also increases optimization difficulty, requiring more time for the model to converge.\n\nOverall, the experimental results indicate that the PhysioFormer model consistently converges across datasets with different time windows, and it shows faster convergence and lower final loss values with shorter time windows. This further validates the efficiency and reliability of the PhysioFormer model in processing multimodal physiological signals.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "Sensitivity Tests",
      "text": "When designing and optimizing complex models, sensitivity tests is a critical step. Through sensitivity experiments, we can systematically evaluate the model's response to different parameters, revealing how these parameters impact the model's performance. This not only helps to understand the internal mechanisms of the model but also provides a scientific basis for optimizing model parameters. Specifically, in this experiment, we explored the effects of adjusting  the window size and the number of hidden layer neurons in the PhysioFormer model on its performance across different datasets. This allowed us to identify the optimal model configuration.\n\nThe effect of window size. (RQ2)This experiment aimed to evaluate the impact of different window sizes on the model's performance. We selected four different window lengths: 30 seconds, 60 seconds, 90 seconds, and 120 seconds, and tested them on the sensor data from both the Wrist and Chest positions. For each window size, we calculated the model's ACC, F1-Score, and MSE to comprehensively assess its performance. The experimental results are shown in the Table  3 .\n\nIn this experiment, we evaluated the impact of different window sizes (30 seconds, 60 seconds, 90 seconds, and 120 seconds) on the performance of the PhysioFormer model. The results show that window size significantly affects the model's predictive performance. With a 30-second window, the model performed best, achieving ACC of 99.54% and 99.21%, and F1-Score of 99.54 and 99.17 on the Wrist and Chest datasets, respectively, with the lowest MSE of 0.025 and 0.057. This suggests that shorter time windows can more effectively capture subtle variations in physiological signals, resulting in higher predictive accuracy and stability.\n\nHowever, as the window size increased, the model's performance declined. With a 60-second window, although the model maintained relatively high accuracy and F1-Score, MSE increased, indicating that handling longer time sequences introduced more temporal dependencies, adding complexity and increasing prediction error. For the 90second and 120-second windows, particularly for the 90-second Chest dataset, accuracy and F1-Score dropped significantly, and MSE increased further, suggesting that excessively long windows may make it difficult for the model to handle complex temporal dependencies, affecting prediction performance. In summary, the 30-second window provides the optimal balance, capturing sufficient information while avoiding redundancy and complexity, ensuring higher predictive accuracy and lower error.\n\nThe effect of the number of neurons in the hidden layer. (RQ3)This experiment aimed to evaluate the impact of the number of hidden layer neurons in the ContribNet and AffectNet models within the PhysioFormer framework on overall model performance. To achieve this, we selected four different scales of neuron counts  (50, 100, 200, 500)  and conducted experiments on both the Wrist and Chest datasets to assess the performance of the ContribNet and AffectNet models within PhysioFormer. The experimental results are shown in the Table  4  and Table 5 .\n\nOn the Wrist dataset, the performance of the model fluctuated with an increase in the number of neurons in the ContribNet and AffectNet models. For instance, when the number of neurons in ContribNet was fixed at 50, the combination with 500 neurons in AffectNet performed best, achieving an accuracy of 99.03%. However, when the number of neurons in ContribNet was increased to 100, the model reached its peak performance, with an accuracy of 99.54% when AffectNet also had 100 neurons. Overall, the results from the Wrist dataset indicate that increasing the number of neurons beyond a certain point did not continue to improve the model's accuracy. In some cases, performance even declined, which could be related to overfitting due to the increased complexity of the model.\n\nOn the Chest dataset, the results similarly showed the impact of neuron count on model performance. When the number of neurons in both ContribNet and AffectNet was set to 50, the model performed relatively well, achieving an accuracy of 99.08%. The best performance, with an accuracy of 99.21%, was observed when both AffectNet and ContribNet had 100 neurons. As the neuron count increased, particularly with configurations of 500 neurons, the model's performance stabilized but did not significantly surpass the performance with fewer neurons.\n\nOverall, the results suggest that while increasing the number of hidden layer neurons can improve the model's performance, this improvement is limited. Too many neurons can lead to increased model complexity, which raises the risk of overfitting, especially with relatively limited data. The best model performance often occurred with moderate neuron configurations, such as 100 or 200 neurons. In conclusion, setting the number of neurons in both ContribNet and AffectNet to 100 offers optimal performance for the PhysioFormer model.",
      "page_start": 20,
      "page_end": 22
    },
    {
      "section_name": "Ablation Studies",
      "text": "Ablation studies are used to assess the contribution of various components or features within a model. They help evaluate the actual impact of each model component. In this experiment, we gradually removed specific modules from the model to evaluate their influence on overall performance, thereby validating their effectiveness. Specifically, we designed two ablation experiments: one to verify the importance of the Feature Embedding module and another to assess the role of individual attributes in the model. The results of these experiments will provide scientific evidence for model optimization and improvement.\n\nThe Validity of Feature Embedding. (RQ4)In this experiment, we evaluated the performance differences between the model with the feature embedding module and the model without it on the Wrist and Chest datasets. To assess the effectiveness of the feature embedding module, we conducted two sets of comparison experiments: one using the complete model with the feature embedding module, and the other by removing it. ACC was used as the primary evaluation metric. The experimental results are shown in the Figure  5 . According to the experimental results, the model with the feature embedding module achieved an accuracy of 99.54% on the Wrist dataset, compared to 97.65% for the model without it, showing an improvement of approximately 1.89%. On the Chest dataset, the model with the feature embedding module achieved an accuracy of 99.21%, while the model without the feature embedding module had an accuracy of 89.87%, reflecting an improvement of about 9.34%.\n\nThese results demonstrate that the model with the physiological data feature embedding module outperforms the one without it on both the Wrist and Chest datasets, with a particularly notable improvement on the Chest dataset. This indicates that the feature embedding module effectively quantifies and highlights features that significantly contribute to physiological signal prediction, better capturing key characteristics in the data and enhancing the model's predictive capability.\n\nThe Validity of Individual Attributes Features. (RQ5)In this experiment, we evaluated the performance differences between models that include individual attributes features (such as age, gender, etc.) alongside physiological monitoring data and models that only use physiological monitoring data on the Wrist and Chest datasets. The experiment was designed in two parts: one model used the complete input data, including individual attributes features and physiological monitoring data, while the other model excluded the individual attributes features and used only the physiological monitoring data for training and testing. This comparative experiment allowed us to assess the impact of individual attributes features on the model's performance. ACC was used as the primary evaluation metric. The experimental results are shown in the Figure  6 .\n\nThe model including individual attributes features data achieved an accuracy of 99.54% on the Wrist dataset, compared to 97.61% for the model without individual attributes features, reflecting an improvement of approximately 1.93%. On the Chest dataset, the model with individual attributes features reached an accuracy of 99.21%, while the model without these features achieved 98.85%, showing an improvement of about 0.36%. The results indicate that the model incorporating individual attributes features data outperforms the one without it on both the Wrist and Chest datasets. Although the improvement on the Chest dataset is relatively modest, it still demonstrates the positive impact of individual attributes features data on model performance. This suggests that individual attributes features data contributes to enhancing the model's accuracy by providing additional context, helping the model capture and predict physiological signal changes more precisely. By combining individual attributes features with physiological monitoring data, the model gains a more comprehensive understanding of physiological states, resulting in improved overall prediction performance.",
      "page_start": 22,
      "page_end": 24
    },
    {
      "section_name": "Discovered Laws And Analysis",
      "text": "",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Features Distribution And Importance",
      "text": "The Wrist dataset contains four types of physiological indicators, while the Chest dataset contains six types of physiological indicators. The distribution of the number of features computed from the physiological indicators in both datasets is shown in the Figure  7 . From the distribution chart, it is clear that ACC and EDA are the primary features in the Wrist dataset, each accounting for nearly one-third of the total features. Individual attributes also make up a significant portion, while BVP and TEMP have smaller proportions. In the Chest dataset, ACC and EDA similarly occupy major positions, each representing more than a quarter of the total features. Individual attributes also hold a large share, while ECG and TEMP occupy a notable portion, with EMG and RESP having smaller proportions. In summary, ACC and EDA are the dominant features in both datasets, but the Chest dataset includes a greater variety of physiological indicators.\n\nIn the PhysioFormer model, the importance scores of feature data for the four physiological indicators in the Wrist dataset (ACC, EDA, BVP, and TEMP) are shown in the Figure  8 . By analyzing the figure, it is evident that ACC-related features significantly influence all physiological indicators. Specifically, features like ğ´ğ¶ğ¶_ğ‘¦_ğ‘šğ‘’ğ‘ğ‘› and ğ‘ğ‘’ğ‘¡_ğ´ğ¶ğ¶_ğ‘šğ‘’ğ‘ğ‘› not only have high importance for ACC itself but also show strong correlations with EDA, BVP, and TEMP. This suggests that physical activity intensity plays a key role in monitoring and predicting physiological states, likely due to the effects of movement on cardiovascular activity, electrodermal activity, and body temperature, making it important across multiple physiological indicators.\n\nAdditionally, individual attributes features (such as smoking status and weight) play a crucial role in TEMP and EDA. Smoking status shows high importance for TEMP, indicating a significant impact of smoking on temperature regulation. Similarly, weight has a notable influence on EDA, possibly due to its effect on skin conductance. Furthermore, features like temperature change rate and standard deviation are highly important for the TEMP indicator, reflecting the dynamic changes involved in the process of temperature regulation.  The visualization of the importance scores for all features in the Wrist dataset, with darker colors indicating higher importance scores. In the subsequent symbolic distillation task, the top ten features with the highest importance scores will be selected for further analysis and modeling.\n\nThe importance scores of feature data for the six physiological indicators in the Chest dataset (ACC, EDA, ECG, TEMP, EMG, and RESP) are shown in the Figure  9 . In the subsequent symbolic regression task, the top 10 features with the highest importance scores were extracted for further analysis, as detailed in Appendix B. ACC-related features still show a dominant influence across all six physiological indicators, further underscoring the critical role of physical activity in the monitoring and prediction of various physiological states.\n\nAdditionally, in the Chest dataset, the impact of individual attributes on physiological features becomes more pronounced. Specifically, weight significantly affects ECG and RESP indicators, while height has a notable impact on TEMP and ECG. Weight may influence the function of the cardiovascular and respiratory systems; changes in weight can lead to variations in blood pressure and heart rate, which are reflected in ECG features. Furthermore, weight gain can lead to breathing difficulties, which can be captured by the RESP indicator. Height may affect the ratio of body surface area to volume, influencing heat dissipation and thermoregulation mechanisms, which can be observed in TEMP features.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Figure 9:",
      "text": "The visualization of the importance scores for all features in the Chest dataset, with darker colors indicating higher importance scores. In the subsequent symbolic distillation task, the top ten features with the highest importance scores will be selected for further analysis and modeling.",
      "page_start": 25,
      "page_end": 26
    },
    {
      "section_name": "Discovered Laws Expression",
      "text": "In this section, symbolic regression was used to model each physiological indicator in both the Wrist and Chest datasets. The symbolic regression algorithm employed was implemented using the open-source symbolic regression library PySR, with the predefined operators including \"+\", \"-\", \"Ã—\", \"ğ‘ ğ‘–ğ‘›\", \"ğ‘ğ‘œğ‘ \", \"ğ‘™ğ‘œğ‘”\", \"ğ‘’ğ‘¥ğ‘\", and \"ğ‘ğ‘œğ‘¤\".\n\nThrough symbolic regression, we obtained several formulas (with complexity less than 15). The next step is to select an optimal formula to explain the physiological indicator. In the selection process, we first focus on formulas that have converged, as these indicate that the model reached a stable state during training, with the loss value no longer significantly decreasing, suggesting that the model has well-fitted the data and found a relatively optimal parameter combination.\n\nAmong the converged formulas, we selected the one with the fewest variables. This is because formulas with fewer variables simplify the structure to some extent, improving the explainability of the model. Although these formulas may still be complex, they are able to capture subtle patterns and complex relationships in the data, which is critical for accurate modeling and prediction. Additionally, choosing formulas with fewer variables helps reduce the risk of overfitting, enhancing the model's generalization ability, and making it more stable and reliable when applied to new data.\n\nBased on the training process and the formula selection method described above, the Complexity-Loss curve and the selected optimal formula are shown in the Figure  10  and Figure  11 , with the red dot indicating the chosen formula. The specific formulas generated through symbolic regression can be found in Appendix C.\n\nFrom the experimental results, these curves exhibit a typical pattern: as the complexity of the formulas increases, the loss value rapidly decreases, but after reaching a certain level of complexity, the downward trend slows, showing convergence or slight fluctuations. Specifically, across all physiological indicator curves, as the formula complexity  increases from low to moderate (usually in the complexity range of 2 to 6), the loss value drops rapidly. This indicates that increasing complexity in the initial stages significantly improves the model's ability to fit the data, allowing it to better capture the relationship between physiological signals and affective states. When the formula complexity reaches a moderate level (typically between 8 and 10), the loss value continues to decrease, but the rate of decline slows. For example, in the ACC and EDA curves, the loss value stabilizes as complexity reaches 8 to 10. This phase suggests that increasing formula complexity has diminishing returns in reducing the loss, and the model begins to converge. As the complexity increases further (usually beyond 12), the curve flattens, with minimal changes in the loss value, and in some cases, slight fluctuations are observed. For instance, in the BVP and TEMP curves, the loss value shows little to no significant reduction beyond complexity 12, indicating that the model's complexity has reached a sufficient level. Further increases in complexity may introduce the risk of overfitting, without significant improvement in model performance.\n\nAdditionally, the complexity-loss curves reveal that formulas for certain physiological indicators converge more slowly as complexity increases. This phenomenon may be attributed to several factors: First, some physiological indicator data may contain higher levels of noise and variability, making it more challenging for the model to accurately capture patterns in the data. Second, interactions between different physiological indicators may involve multiple physiological mechanisms, requiring more complex mathematical expressions to explain these relationships. Finally, limitations in the search space and computational resources may also affect the ability to efficiently find optimal formulas.\n\nOverall, these curves illustrate the typical trade-off between model complexity and performance. Increasing complexity initially leads to significant improvements in model accuracy, but after reaching a certain level, the gains",
      "page_start": 26,
      "page_end": 27
    },
    {
      "section_name": "Table 6",
      "text": "The table presents the evaluation results of the selected formulas using the ğ‘… 2 metric, which measures the goodness of fit between the predicted values and the actual values. The ğ‘… 2 values for all the selected formulas are listed, providing a clear view of the fitting performance of each formula. To evaluate the fit of each formula, we used the ğ‘… 2 metric, which measures the goodness of fit between the predicted values and the actual values. The closer the ğ‘… 2 value is to 1, the stronger the model's ability to explain the data. We calculated the ğ‘… 2 values for all selected formulas, and the results are shown in the Table  6 .\n\nAccording to the results shown in the table, the high-fitting formulas include ACC and BVP from the Wrist dataset, as well as ACC, TEMP, and ECG from the Chest dataset, with ğ‘… 2 values close to or reaching 0.98 to 0.99. This indicates that these formulas predict the corresponding data very well, and the model is able to explain the variability in the data accurately. These physiological indicators are well predicted and explained under the current model and formula combinations. Moderate-fitting formulas include TEMP from the Wrist dataset and EDA and RESP from the Chest dataset, with ğ‘… 2 values ranging from 0.39 to 0.76. These formulas perform reasonably well but still have room for improvement. While they capture some important patterns in the data, they do not fully explain the data's variability.\n\nLow-fitting formulas include EDA from the Wrist dataset and EMG from the Chest dataset, both with low ğ‘… 2 values of 0.15, indicating poor predictive performance for these formulas. The model struggles to capture the patterns in the data for these indicators, which may be due to the model's insufficient complexity or interference from noise and outliers in the data.\n\nThe results suggest that the model performs satisfactorily for most physiological indicators, but its predictive ability needs improvement for certain indicators. Future work can focus on the following areas: first, introducing more relevant features to bring additional information and enhance the model's predictive ability; second, conducting data cleaning and preprocessing for indicators with low ğ‘… 2 values to remove noise and outliers, thereby improving data quality.",
      "page_start": 29,
      "page_end": 31
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this study, we proposed and implemented the PhysioFormer model for affective computation based on multimodal physiological signals. PhysioFormer model effectively addresses the variability in physiological signals across individuals by integrating individual attributes and multimodal signals. This integration allows the model to demonstrate high reliability and generalization in cross-individual affective computation tasks, ensuring its stability across different affective computation tasks, making it highly applicable and reliable in real-world scenarios. By incorporating feature embedding and affective representation modules, PhysioFormer is able to capture the temporal dependencies and multimodal features of physiological signals, significantly enhancing its accuracy. Extensive experiments on the Wrist and Chest subsets of the WESAD dataset demonstrated PhysioFormer's superior performance in affective computation tasks. The experimental results showed that PhysioFormer achieved over 99% accuracy on both subsets, outperforming the current SOTA models. Additionally, we introduced an Explanation model to the PhysioFormer model, utilizing symbolic regression techniques to extract symbolic laws that reveal the relationships between physiological signals and affective states. This enhancement improves the model's explainability, offering new insights into the intrinsic connections between physiological signals and affective states.\n\nHowever, despite the promising results, this study reveals several key areas for future research. First, the scalability of PhysioFormer to larger datasets and real-world applications remains to be validated, particularly how to balance the complexity and computational efficiency in real-time affective recognition systems. Second, addressing crossindividual variability, future work should further optimize the feature embedding module to improve the model's generalization across different individuals, ensuring reliability in broader populations. Additionally, integrating more physiological signals (e.g., blood pressure, EEG) and environmental factors (e.g., noise, light intensity) could help build a more comprehensive affective recognition system, further enhancing the model's performance and adaptability.\n\nFuture research can also expand and refine the symbolic approach to extract more explainable mathematical formulas, thereby improving model explainability and transparency. This is particularly important for applying the PhysioFormer model in emotion-driven real-world applications such as health monitoring and psychological interventions. Moreover, further exploration of how symbolic regression can be used to explain the relationship between multimodal physiological signals and complex affective states will provide new directions and inspiration for future research in affective computation.",
      "page_start": 31,
      "page_end": 31
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , and it can be divided into three",
      "page": 8
    },
    {
      "caption": "Figure 1: PhysioFormer model architecture consists of three submodules: the Feature Embedding Module, Affective",
      "page": 9
    },
    {
      "caption": "Figure 2: illustrates the process of data segmentation",
      "page": 10
    },
    {
      "caption": "Figure 2: Example of time window segmentation in the dataset. This figure illustrates the time window segmentation",
      "page": 10
    },
    {
      "caption": "Figure 3: First, the input data is processed through the Feature Embedding and Affective Representation modules,",
      "page": 14
    },
    {
      "caption": "Figure 3: The figure shows the process of Explanation model. First, the input data undergoes feature extraction and affective",
      "page": 15
    },
    {
      "caption": "Figure 4: The results show that all models exhibit a rapid decline in loss values during the initial stages of training, indicating",
      "page": 20
    },
    {
      "caption": "Figure 4: Convergence trends of the PhysioFormer model across datasets by splitting the WESAD dataset through windows",
      "page": 21
    },
    {
      "caption": "Figure 5: The role of feature embedding in affective computation tasks on both datasets, with the results presented in",
      "page": 23
    },
    {
      "caption": "Figure 6: The model including individual attributes features data achieved an accuracy of 99.54% on the Wrist dataset,",
      "page": 23
    },
    {
      "caption": "Figure 6: The role of individual attributes features in affective computation tasks on both datasets, with the results",
      "page": 24
    },
    {
      "caption": "Figure 7: Figure 7: Distribution map of the number of features. The figure shows the distribution of the number of features calculated",
      "page": 24
    },
    {
      "caption": "Figure 8: By analyzing the figure, it is evident that",
      "page": 25
    },
    {
      "caption": "Figure 8: The visualization of the importance scores for all features in the Wrist dataset, with darker colors indicating",
      "page": 25
    },
    {
      "caption": "Figure 9: In the subsequent symbolic regression task, the top 10 features",
      "page": 25
    },
    {
      "caption": "Figure 9: The visualization of the importance scores for all features in the Chest dataset, with darker colors indicating",
      "page": 26
    },
    {
      "caption": "Figure 10: and Figure 11, with the red dot indicating the chosen formula.",
      "page": 26
    },
    {
      "caption": "Figure 10: Complexity-Loss curve in Wrist dataset. The red dot indicate the selected formula, and the corresponding",
      "page": 27
    },
    {
      "caption": "Figure 11: Complexity-Loss curve in Chest dataset. The red dot indicate the selected formula, and the corresponding",
      "page": 28
    },
    {
      "caption": "Figure 12: and Figure 13.",
      "page": 28
    },
    {
      "caption": "Figure 12: The comparison of the fitted curves for affective indicators based on the selected formulas for four randomly",
      "page": 29
    },
    {
      "caption": "Figure 13: The comparison of the fitted curves for affective indicators based on the selected formulas for four randomly",
      "page": 30
    },
    {
      "caption": "Figure 14: Individual features table, which describes all individual attributes features, their types, and the meanings they",
      "page": 36
    },
    {
      "caption": "Figure 15: Features table for Wrist dataset, which describes all features and the meanings they represent.",
      "page": 37
    },
    {
      "caption": "Figure 16: Features table for Chest dataset, which describes all features and the meanings they represent.",
      "page": 38
    },
    {
      "caption": "Figure 19: Indicators law table of ACC in Wrist",
      "page": 41
    },
    {
      "caption": "Figure 20: Indicators law table of EDA in Wrist",
      "page": 41
    },
    {
      "caption": "Figure 21: Indicators law table of BVP in Wrist",
      "page": 42
    },
    {
      "caption": "Figure 22: Indicators law table of TEMP in Wrist",
      "page": 42
    },
    {
      "caption": "Figure 23: Indicators law table of ACC in Chest",
      "page": 43
    },
    {
      "caption": "Figure 24: Indicators law table of EDA in Chest",
      "page": 43
    },
    {
      "caption": "Figure 25: Indicators law table of ECG in Chest",
      "page": 44
    },
    {
      "caption": "Figure 26: Indicators law table of TEMP in Chest",
      "page": 44
    },
    {
      "caption": "Figure 27: Indicators law table of EMG in Chest",
      "page": 45
    },
    {
      "caption": "Figure 28: Indicators law table of RESP in Chest",
      "page": 45
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Definitions of mathematical notation used in this paper.",
      "data": [
        {
          "Notations": "îˆ¼",
          "Description": "A set containing ğ‘ participants is denoted as îˆ¼ = ğ‘1, ğ‘2, â€¦ , ğ‘ğ‘"
        },
        {
          "Notations": "îˆ­",
          "Description": "A set of ğ‘˜ individual attributes is denoted as îˆ­ = {ğ‘1, ğ‘2, â€¦ , ğ‘ğ‘˜}, each element represents a feature."
        },
        {
          "Notations": "îˆ®",
          "Description": "A set of ğ‘€ physiological\nindicators obtained from a monitoring device is denoted as îˆ® = {ğ‘1, ğ‘2, â€¦ , ğ‘ğ‘€ }."
        },
        {
          "Notations": "ğ·",
          "Description": "Physiological monitoring data of ğ‘ participants with ğ‘€ indicators."
        },
        {
          "Notations": "ğ‘’",
          "Description": "The affective state of the participants at the current moment."
        },
        {
          "Notations": "î‰„, ğ‘‹",
          "Description": "The set of all windows after window segmentation, and the data contained within a specific window."
        },
        {
          "Notations": "ğœ‰",
          "Description": "The number of window segments."
        },
        {
          "Notations": "ğ‘ƒ ğ¹",
          "Description": "Integrated features of ğ´ and ğµ, denoted as ğ‘ƒ ğ¹ = ğ´ âŠ• ğµ."
        },
        {
          "Notations": "ğ›¼ğ‘ğ‘—",
          "Description": "The contribution level of physiological\nindicator ğ‘ğ‘—"
        },
        {
          "Notations": "ğ›½ğ‘ğ‘—",
          "Description": "The encoded physiological\nindicator ğ‘ğ‘—."
        },
        {
          "Notations": "ğ›¾ ğ‘ğ‘—",
          "Description": "Integrated features of ğ´ and ğ›½ğ‘ğ‘— , denoted as ğ›¾ ğ‘ğ‘— = ğ´ âŠ• ğ›½ğ‘ğ‘— ."
        },
        {
          "Notations": "ğœƒğ‘ğ‘—",
          "Description": "The affective state level reflected by the physiological\nindicator ğ‘ğ‘—."
        },
        {
          "Notations": "ğ‘¢",
          "Description": "Initial affective state level."
        },
        {
          "Notations": "Î˜",
          "Description": "The affective state level reflected by all physiological\nindicators."
        },
        {
          "Notations": "Î¦",
          "Description": "Affective state level\nfrom all\nindicators, adjusted by initial state."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Upper Triangular Matrix": "Affective Representation Module\nPrediction Module\nAffectNet\nAffectAnalyser\nâ€¦"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 2: The performance comparation beween the proposed PhysioFormer Model and the state-of-the-art (SOTA) methods in",
      "data": [
        {
          "ACC": "Wrist",
          "F1-Score": "Wrist",
          "MSE": "Wrist"
        },
        {
          "ACC": "97.46",
          "F1-Score": "97.40",
          "MSE": "0.151"
        },
        {
          "ACC": "97.88",
          "F1-Score": "97.88",
          "MSE": "0.134"
        },
        {
          "ACC": "97.45",
          "F1-Score": "97.62",
          "MSE": "0.138"
        },
        {
          "ACC": "90.50",
          "F1-Score": "90.53",
          "MSE": "0.175"
        },
        {
          "ACC": "88.27",
          "F1-Score": "88.29",
          "MSE": "0.205"
        },
        {
          "ACC": "85.20",
          "F1-Score": "84.51",
          "MSE": "0.303"
        },
        {
          "ACC": "98.33",
          "F1-Score": "97.95",
          "MSE": "0.065"
        },
        {
          "ACC": "84.93",
          "F1-Score": "84.98",
          "MSE": "0.391"
        },
        {
          "ACC": "96.87",
          "F1-Score": "97.02",
          "MSE": "0.184"
        },
        {
          "ACC": "99.54",
          "F1-Score": "99.54",
          "MSE": "0.025"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table 3: TheperformancecomparationoftheproposedPhysioFormermodelondifferentdatasetsdividedbydifferentwindowsizes",
      "data": [
        {
          "ACC": "Wrist",
          "F1-Score": "Wrist",
          "MSE": "Wrist"
        },
        {
          "ACC": "99.54",
          "F1-Score": "99.54",
          "MSE": "0.025"
        },
        {
          "ACC": "97.61",
          "F1-Score": "97.59",
          "MSE": "0.108"
        },
        {
          "ACC": "97.87",
          "F1-Score": "97.86",
          "MSE": "0.081"
        },
        {
          "ACC": "97.70",
          "F1-Score": "97.68",
          "MSE": "0.089"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table 4: TheperformancecomparisonoftheproposedPhysioFormermodelontheWristdatasetwithdifferentnumbersofhidden",
      "data": [
        {
          "ContribNet": ""
        },
        {
          "ContribNet": "50"
        },
        {
          "ContribNet": "100"
        },
        {
          "ContribNet": "200"
        },
        {
          "ContribNet": "500"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table 4: TheperformancecomparisonoftheproposedPhysioFormermodelontheWristdatasetwithdifferentnumbersofhidden",
      "data": [
        {
          "ContribNet": ""
        },
        {
          "ContribNet": "50"
        },
        {
          "ContribNet": "100"
        },
        {
          "ContribNet": "200"
        },
        {
          "ContribNet": "500"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Individual Features": "Variable \nName"
        },
        {
          "Individual Features": "Age"
        },
        {
          "Individual Features": "Height"
        },
        {
          "Individual Features": "Weight"
        },
        {
          "Individual Features": "Gender"
        }
      ],
      "page": 36
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Wrist Dataset Features": "Variable \nName"
        },
        {
          "Wrist Dataset Features": "\"\n!\"\"!"
        },
        {
          "Wrist Dataset Features": "\"\n!\"\"'"
        },
        {
          "Wrist Dataset Features": "\"\n!\"\"%"
        },
        {
          "Wrist Dataset Features": "\"\n!\"\"#"
        },
        {
          "Wrist Dataset Features": "(\n!\"\"!"
        },
        {
          "Wrist Dataset Features": "(\n!\"\"'"
        },
        {
          "Wrist Dataset Features": "(\n!\"\"%"
        },
        {
          "Wrist Dataset Features": "(\n!\"\"#"
        },
        {
          "Wrist Dataset Features": ",\n!\"\"!"
        },
        {
          "Wrist Dataset Features": ",\n!\"\"'"
        },
        {
          "Wrist Dataset Features": ",\n!\"\"%"
        },
        {
          "Wrist Dataset Features": ",\n!\"\"#"
        },
        {
          "Wrist Dataset Features": "$\n!\"\"!"
        },
        {
          "Wrist Dataset Features": "$\n!\"\"'"
        },
        {
          "Wrist Dataset Features": "$\n!\"\"%"
        },
        {
          "Wrist Dataset Features": "In the column \"Variable Name\", subscripts 1, 2, 3, and 4 denote the mean, standard deviation(std), maximum(max), and \nminimum(min) values of the feature within the current window, respectively."
        }
      ],
      "page": 37
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chest Dataset Features": "Variable \nName"
        },
        {
          "Chest Dataset Features": "\"\n!\"\"!"
        },
        {
          "Chest Dataset Features": "\"\n!\"\"#"
        },
        {
          "Chest Dataset Features": "\"\n!\"\"$"
        },
        {
          "Chest Dataset Features": "\"\n!\"\"%"
        },
        {
          "Chest Dataset Features": ")\n!\"\"!"
        },
        {
          "Chest Dataset Features": ")\n!\"\"#"
        },
        {
          "Chest Dataset Features": ")\n!\"\"$"
        },
        {
          "Chest Dataset Features": ")\n!\"\"%"
        },
        {
          "Chest Dataset Features": "*\n!\"\"!"
        },
        {
          "Chest Dataset Features": "*\n!\"\"#"
        },
        {
          "Chest Dataset Features": "*\n!\"\"$"
        },
        {
          "Chest Dataset Features": "*\n!\"\"%"
        },
        {
          "Chest Dataset Features": ",\n!\"\"!"
        },
        {
          "Chest Dataset Features": ",\n!\"\"#"
        },
        {
          "Chest Dataset Features": ",\n!\"\"$"
        },
        {
          "Chest Dataset Features": ",\n!\"\"%"
        },
        {
          "Chest Dataset Features": "#$!!"
        },
        {
          "Chest Dataset Features": "In the column \"Variable Name\", subscripts 1, 2, 3, and 4 denote the mean, standard deviation(std), maximum(max), and \nminimum(min) values of the feature within the current window, respectively."
        }
      ],
      "page": 38
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Wrist_EDA\nWrist_EDA\nWrist_EDA": "Feature Name\nFeature Name"
        },
        {
          "Wrist_EDA\nWrist_EDA\nWrist_EDA": "EDA_tonic_mean\nEDA_tonic_mean"
        },
        {
          "Wrist_EDA\nWrist_EDA\nWrist_EDA": "ACC_y_mean\nACC_y_mean"
        },
        {
          "Wrist_EDA\nWrist_EDA\nWrist_EDA": "BVP_mean\nBVP_mean"
        },
        {
          "Wrist_EDA\nWrist_EDA\nWrist_EDA": "EDA_phasic_mean\nEDA_phasic_mean"
        },
        {
          "Wrist_EDA\nWrist_EDA\nWrist_EDA": "ACC_z_mean\nACC_z_mean"
        },
        {
          "Wrist_EDA\nWrist_EDA\nWrist_EDA": "TEMP_mean\nTEMP_mean\n(#)'#\nTEMP_mean\nTEMP_mean\n(#)'#"
        },
        {
          "Wrist_EDA\nWrist_EDA\nWrist_EDA": "EDA_mean\nEDA_mean\n#$!#\n7\nEDA_mean\nEDA_mean\n#$!#"
        },
        {
          "Wrist_EDA\nWrist_EDA\nWrist_EDA": "$\n$\nEDA_smna_mean\nEDA_smna_mean\n#$!#\n#$!#\n8\nEDA_smna_mean\n#$!#\nEDA_smna_mean\n#$!#"
        },
        {
          "Wrist_EDA\nWrist_EDA\nWrist_EDA": "%\n%\n%\nNet_ACC_std\nNet_ACC_std\n!\"\"!\n%\n!\"\"!\n9\nNet_ACC_std\n!\"\"!\nNet_ACC_std\n!\"\"!"
        },
        {
          "Wrist_EDA\nWrist_EDA\nWrist_EDA": "\"\n\"\n\"\nACC_y_std\n\"\nACC_y_std\n!\"\"!\n!\"\"!\n10\nACC_y_std\n!\"\"!\nACC_y_std\n!\"\"!"
        }
      ],
      "page": 39
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Wrist_ACC\nWrist_ACC\nWrist_ACC": "No.\nNo."
        },
        {
          "Wrist_ACC\nWrist_ACC\nWrist_ACC": "1\n1"
        },
        {
          "Wrist_ACC\nWrist_ACC\nWrist_ACC": "2\n2"
        },
        {
          "Wrist_ACC\nWrist_ACC\nWrist_ACC": "3\n3"
        },
        {
          "Wrist_ACC\nWrist_ACC\nWrist_ACC": "BVP_max\n4\nBVP_max\n%&'&\nBVP_max\n4\nBVP_max\n%&'&"
        },
        {
          "Wrist_ACC\nWrist_ACC\nWrist_ACC": "'\nEDA_phasic_mean\n5\nEDA_phasic_mean\nEDA_phasic_mean\n#$!#\n5\nEDA_phasic_mean\n#$!#"
        },
        {
          "Wrist_ACC\nWrist_ACC\nWrist_ACC": "6\n6"
        },
        {
          "Wrist_ACC\nWrist_ACC\nWrist_ACC": "7\n7"
        },
        {
          "Wrist_ACC\nWrist_ACC\nWrist_ACC": "8\n8"
        },
        {
          "Wrist_ACC\nWrist_ACC\nWrist_ACC": "9\n9"
        },
        {
          "Wrist_ACC\nWrist_ACC\nWrist_ACC": "10\n10"
        }
      ],
      "page": 39
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Wrist_TEMP\nWrist_TEMP\nWrist_TEMP\nWrist_TEMP": "Feature Name\nVariable Name\nFeature Name\nVariable Name\nNo.\nFeature Name\nVariable Name\nFeature Name\nVariable Name"
        },
        {
          "Wrist_TEMP\nWrist_TEMP\nWrist_TEMP\nWrist_TEMP": "TEMP_slope\n(#)'$\nTEMP_slope\n(#)'$\n1\nTEMP_slope\n(#)'$\nTEMP_slope\n(#)'$"
        },
        {
          "Wrist_TEMP\nWrist_TEMP\nWrist_TEMP\nWrist_TEMP": "*\n*\nACC_z_mean\n*\n!\"\"#\nACC_z_mean\n*\n!\"\"#\n2\nACC_z_mean\n!\"\"#\nACC_z_mean\n!\"\"#"
        },
        {
          "Wrist_TEMP\nWrist_TEMP\nWrist_TEMP\nWrist_TEMP": "TEMP_std\n(#)'!\nTEMP_std\n(#)'!\n3\nTEMP_std\n(#)'!\nTEMP_std\n(#)'!"
        },
        {
          "Wrist_TEMP\nWrist_TEMP\nWrist_TEMP\nWrist_TEMP": "'\n'\nEDA_phasic_mean\n#$!#\nEDA_phasic_mean\n#$!#\n4\nEDA_phasic_mean\n#$!#\nEDA_phasic_mean\n#$!#"
        },
        {
          "Wrist_TEMP\nWrist_TEMP\nWrist_TEMP\nWrist_TEMP": "5\nSmoker_NO\n*+,-./,\nSmoker_NO\n*+,-./,\nSmoker_NO\n*+,-./,\nSmoker_NO\n*+,-./,"
        },
        {
          "Wrist_TEMP\nWrist_TEMP\nWrist_TEMP\nWrist_TEMP": "6\nBVP_peak_freq\n%&'-.\nBVP_peak_freq\n%&'-.\nBVP_peak_freq\n%&'-.\nBVP_peak_freq\n%&'-."
        },
        {
          "Wrist_TEMP\nWrist_TEMP\nWrist_TEMP\nWrist_TEMP": "\"\n\"\n\"\n7\nACC_y_mean\n\"\n!\"\"#\nACC_y_mean\n!\"\"#\nACC_y_mean\nACC_y_mean\n!\"\"#\n!\"\"#"
        },
        {
          "Wrist_TEMP\nWrist_TEMP\nWrist_TEMP\nWrist_TEMP": "+\n+\n8\nACC_x_mean\n!\"\"#\n+\nACC_x_mean\n+\n!\"\"#\nACC_x_mean\nACC_x_mean\n!\"\"#\n!\"\"#"
        },
        {
          "Wrist_TEMP\nWrist_TEMP\nWrist_TEMP\nWrist_TEMP": "%\n%\n9\nNet_ACC_mean\n!\"\"#\n%\nNet_ACC_mean\n%\n!\"\"#\nNet_ACC_mean\nNet_ACC_mean\n!\"\"#\n!\"\"#"
        },
        {
          "Wrist_TEMP\nWrist_TEMP\nWrist_TEMP\nWrist_TEMP": "10\nTEMP_mean\n(#)'#\nTEMP_mean\n(#)'#\nTEMP_mean\nTEMP_mean\n(#)'#\n(#)'#"
        }
      ],
      "page": 39
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Wrist_BVP\nWrist_BVP\nWrist_BVP\nWrist_BVP": "Feature Name\nNo.\nFeature Name\nVariable Name\nNo.\nFeature Name\nNo.\nFeature Name\nVariable Name"
        },
        {
          "Wrist_BVP\nWrist_BVP\nWrist_BVP\nWrist_BVP": "$\nEDA_smna_mean\n1\nEDA_smna_mean\n#$!#\n1\nEDA_smna_mean\n1\nEDA_smna_mean\n#$!#"
        },
        {
          "Wrist_BVP\nWrist_BVP\nWrist_BVP\nWrist_BVP": "TEMP_mean\n2\nTEMP_mean\n(#)'#\n2\nTEMP_mean\n2\nTEMP_mean\n(#)'#"
        },
        {
          "Wrist_BVP\nWrist_BVP\nWrist_BVP\nWrist_BVP": "*\nACC_z_mean\n3\nACC_z_mean\n*\n!\"\"#\n3\nACC_z_mean\n3\nACC_z_mean\n!\"\"#"
        },
        {
          "Wrist_BVP\nWrist_BVP\nWrist_BVP\nWrist_BVP": ")\nEDA_tonic_mean\n4\nEDA_tonic_mean\n#$!#\n4\nEDA_tonic_mean\n4\nEDA_tonic_mean\n#$!#"
        },
        {
          "Wrist_BVP\nWrist_BVP\nWrist_BVP\nWrist_BVP": "'\n5\nEDA_phasic_mean\n5\nEDA_phasic_mean\n#$!#\nEDA_phasic_mean\n5\nEDA_phasic_mean\n#$!#"
        },
        {
          "Wrist_BVP\nWrist_BVP\nWrist_BVP\nWrist_BVP": "\"\n6\nACC_y_mean\n\"\n6\nACC_y_mean\n!\"\"#\nACC_y_mean\n6\nACC_y_mean\n!\"\"#"
        },
        {
          "Wrist_BVP\nWrist_BVP\nWrist_BVP\nWrist_BVP": "7\nEDA_mean\n7\nEDA_mean\n#$!#\nEDA_mean\n7\nEDA_mean\n#$!#"
        },
        {
          "Wrist_BVP\nWrist_BVP\nWrist_BVP\nWrist_BVP": "%\n8\nNet_ACC_mean\n8\nNet_ACC_mean\n%\n!\"\"#\nNet_ACC_mean\n8\nNet_ACC_mean\n!\"\"#"
        },
        {
          "Wrist_BVP\nWrist_BVP\nWrist_BVP\nWrist_BVP": "9\nBVP_mean\n9\nBVP_mean\n%&'#\nBVP_mean\n9\nBVP_mean\n%&'#"
        },
        {
          "Wrist_BVP\nWrist_BVP\nWrist_BVP\nWrist_BVP": "+\n10\nACC_x_mean\n10\nACC_x_mean\n+\n!\"\"#\nACC_x_mean\n10\nACC_x_mean\n!\"\"#"
        }
      ],
      "page": 39
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chest_EDA\nChest_EDA\nChest_EDA\nChest_EDA": "No.\nFeature Name\nVariable Name\nFeature Name\nVariable Name\nFeature Name\nVariable Name\nNo.\nFeature Name\nVariable Name"
        },
        {
          "Chest_EDA\nChest_EDA\nChest_EDA\nChest_EDA": "1\n1\nRESP_std\nRESP_std\n)\"*$!\n)\"*$!\nRESP_std\n1\nRESP_std\n)\"*$!\n)\"*$!"
        },
        {
          "Chest_EDA\nChest_EDA\nChest_EDA\nChest_EDA": "&\n&\n&\n2\n&\n2\nACC_y_max\nACC_y_max\n&''(\n&''(\nACC_y_max\n2\nACC_y_max\n&''(\n&''("
        },
        {
          "Chest_EDA\nChest_EDA\nChest_EDA\nChest_EDA": "3\n3\nEDA_mean\nEDA_mean\n\"%&#\n\"%&#\nEDA_mean\n3\nEDA_mean\n\"%&#\n\"%&#"
        },
        {
          "Chest_EDA\nChest_EDA\nChest_EDA\nChest_EDA": "4\n4\nECG_std\nECG_std\n\"'(!\n\"'(!\nECG_std\n4\nECG_std\n\"'(!\n\"'(!"
        },
        {
          "Chest_EDA\nChest_EDA\nChest_EDA\nChest_EDA": "$\n$\n$\n5\n5\nEDA_smna_std\nEDA_smna_std\n\"%&!\n\"%&!\nEDA_smna_std\nEDA_smna_std\n\"%&!\n\"%&!"
        },
        {
          "Chest_EDA\nChest_EDA\nChest_EDA\nChest_EDA": "\"\n\"\n\"\n6\nEDA_phasic_mean\nEDA_phasic_mean\n\"%&#\n\"%&#\nEDA_phasic_mean\nEDA_phasic_mean\n\"%&#\n\"%&#"
        },
        {
          "Chest_EDA\nChest_EDA\nChest_EDA\nChest_EDA": "7\n7\nNet_ACC_mean\nNet_ACC_mean"
        },
        {
          "Chest_EDA\nChest_EDA\nChest_EDA\nChest_EDA": "8\n8\nACC_x_mean\nACC_x_mean\n8"
        },
        {
          "Chest_EDA\nChest_EDA\nChest_EDA\nChest_EDA": "9\n9\nEDA_phasic_std\n9"
        },
        {
          "Chest_EDA\nChest_EDA\nChest_EDA\nChest_EDA": "10\n10\nEDA_phasic_min\n10"
        }
      ],
      "page": 40
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chest_ACC\nChest_ACC\nChest_ACC\nChest_ACC": "No.\nNo."
        },
        {
          "Chest_ACC\nChest_ACC\nChest_ACC\nChest_ACC": "1\n1"
        },
        {
          "Chest_ACC\nChest_ACC\nChest_ACC\nChest_ACC": "2\n2"
        },
        {
          "Chest_ACC\nChest_ACC\nChest_ACC\nChest_ACC": "3\n3"
        },
        {
          "Chest_ACC\nChest_ACC\nChest_ACC\nChest_ACC": "4\n4"
        },
        {
          "Chest_ACC\nChest_ACC\nChest_ACC\nChest_ACC": "5\n5"
        },
        {
          "Chest_ACC\nChest_ACC\nChest_ACC\nChest_ACC": "6\n6"
        },
        {
          "Chest_ACC\nChest_ACC\nChest_ACC\nChest_ACC": "7\n7"
        },
        {
          "Chest_ACC\nChest_ACC\nChest_ACC\nChest_ACC": "8\nECG_max\n\"'((\n8\nECG_max\n8\nECG_max\n\"'((\n\"'((\n8\nECG_max\n\"'(("
        },
        {
          "Chest_ACC\nChest_ACC\nChest_ACC\nChest_ACC": "'\n'\n9\nEDA_tonic_mean\n\"%&#\nEDA_tonic_mean\n9\nEDA_tonic_mean\n\"%&#\n\"%&#\n9\nEDA_tonic_mean\n\"%&#"
        },
        {
          "Chest_ACC\nChest_ACC\nChest_ACC\nChest_ACC": ")\n)\n10\nACC_x_min\n&''%\n)\n10\nACC_x_min\n10\nACC_x_min\n&''%\n&''%\n10\nACC_x_min\n&''%"
        }
      ],
      "page": 40
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chest_TEMP\nChest_TEMP\nChest_TEMP\nChest_TEMP": "No.\nNo.\nFeature Name\nNo."
        },
        {
          "Chest_TEMP\nChest_TEMP\nChest_TEMP\nChest_TEMP": "1\n1\nEDA_phasic_std\n1"
        },
        {
          "Chest_TEMP\nChest_TEMP\nChest_TEMP\nChest_TEMP": "2\n2\n2"
        },
        {
          "Chest_TEMP\nChest_TEMP\nChest_TEMP\nChest_TEMP": "3\n3\n3"
        },
        {
          "Chest_TEMP\nChest_TEMP\nChest_TEMP\nChest_TEMP": "4\n4\n4"
        },
        {
          "Chest_TEMP\nChest_TEMP\nChest_TEMP\nChest_TEMP": "5\n5\n5"
        },
        {
          "Chest_TEMP\nChest_TEMP\nChest_TEMP\nChest_TEMP": "6\n6\n6"
        },
        {
          "Chest_TEMP\nChest_TEMP\nChest_TEMP\nChest_TEMP": "7\nEDA_phasic_mean"
        },
        {
          "Chest_TEMP\nChest_TEMP\nChest_TEMP\nChest_TEMP": "8\n8\n8"
        },
        {
          "Chest_TEMP\nChest_TEMP\nChest_TEMP\nChest_TEMP": "9\n9\n9"
        },
        {
          "Chest_TEMP\nChest_TEMP\nChest_TEMP\nChest_TEMP": "10\n10\n10"
        }
      ],
      "page": 40
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chest_ECG\nChest_ECG\nChest_ECG\nChest_ECG": "No."
        },
        {
          "Chest_ECG\nChest_ECG\nChest_ECG\nChest_ECG": "1"
        },
        {
          "Chest_ECG\nChest_ECG\nChest_ECG\nChest_ECG": "2"
        },
        {
          "Chest_ECG\nChest_ECG\nChest_ECG\nChest_ECG": "3"
        },
        {
          "Chest_ECG\nChest_ECG\nChest_ECG\nChest_ECG": "4"
        },
        {
          "Chest_ECG\nChest_ECG\nChest_ECG\nChest_ECG": "5"
        },
        {
          "Chest_ECG\nChest_ECG\nChest_ECG\nChest_ECG": "6"
        },
        {
          "Chest_ECG\nChest_ECG\nChest_ECG\nChest_ECG": "7"
        },
        {
          "Chest_ECG\nChest_ECG\nChest_ECG\nChest_ECG": "8"
        },
        {
          "Chest_ECG\nChest_ECG\nChest_ECG\nChest_ECG": "9"
        },
        {
          "Chest_ECG\nChest_ECG\nChest_ECG\nChest_ECG": ")\n10\nACC_x_std\n)\n)\n&''!\n)\n10\nACC_x_std\n10\nACC_x_std\n&''!\n10\nACC_x_std\n&''!\n&''!"
        }
      ],
      "page": 40
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chest_EMG\nChest_EMG\nChest_EMG\nChest_EMG": "No."
        },
        {
          "Chest_EMG\nChest_EMG\nChest_EMG\nChest_EMG": "1"
        },
        {
          "Chest_EMG\nChest_EMG\nChest_EMG\nChest_EMG": "2"
        },
        {
          "Chest_EMG\nChest_EMG\nChest_EMG\nChest_EMG": "3"
        },
        {
          "Chest_EMG\nChest_EMG\nChest_EMG\nChest_EMG": "4"
        },
        {
          "Chest_EMG\nChest_EMG\nChest_EMG\nChest_EMG": "5"
        },
        {
          "Chest_EMG\nChest_EMG\nChest_EMG\nChest_EMG": "6"
        },
        {
          "Chest_EMG\nChest_EMG\nChest_EMG\nChest_EMG": "7"
        },
        {
          "Chest_EMG\nChest_EMG\nChest_EMG\nChest_EMG": "8"
        },
        {
          "Chest_EMG\nChest_EMG\nChest_EMG\nChest_EMG": "9"
        },
        {
          "Chest_EMG\nChest_EMG\nChest_EMG\nChest_EMG": "10"
        }
      ],
      "page": 40
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chest_RESP\nChest_RESP\nChest_RESP\nChest_RESP": "No.\nNo.\nFeature Name\nNo."
        },
        {
          "Chest_RESP\nChest_RESP\nChest_RESP\nChest_RESP": "1\nEDA_phasic_mean"
        },
        {
          "Chest_RESP\nChest_RESP\nChest_RESP\nChest_RESP": "2\n2\n2"
        },
        {
          "Chest_RESP\nChest_RESP\nChest_RESP\nChest_RESP": "3\nECG_HRV_RMSSD"
        },
        {
          "Chest_RESP\nChest_RESP\nChest_RESP\nChest_RESP": "4\nECG_HRV_SDNN"
        },
        {
          "Chest_RESP\nChest_RESP\nChest_RESP\nChest_RESP": "'\n5\nEDA_tonic_mean\n'\n\"%&#\n'\nEDA_tonic_mean\nEDA_tonic_mean\n\"%&#\n\"%&#"
        },
        {
          "Chest_RESP\nChest_RESP\nChest_RESP\nChest_RESP": "6\n6\nEDA_smna_std\n6"
        },
        {
          "Chest_RESP\nChest_RESP\nChest_RESP\nChest_RESP": "7\n7\nACC_x_mean\n7"
        },
        {
          "Chest_RESP\nChest_RESP\nChest_RESP\nChest_RESP": "8\n8\nTEMP_min\n8"
        },
        {
          "Chest_RESP\nChest_RESP\nChest_RESP\nChest_RESP": "9\n9\n9"
        },
        {
          "Chest_RESP\nChest_RESP\nChest_RESP\nChest_RESP": "10\n10\n10"
        }
      ],
      "page": 40
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Wrist_ACC": "CX"
        },
        {
          "Wrist_ACC": "1"
        },
        {
          "Wrist_ACC": "3"
        },
        {
          "Wrist_ACC": "5"
        },
        {
          "Wrist_ACC": "CX\n6"
        },
        {
          "Wrist_ACC": "1\n7"
        },
        {
          "Wrist_ACC": "3\n8"
        },
        {
          "Wrist_ACC": "5\n9"
        },
        {
          "Wrist_ACC": "6\n10"
        },
        {
          "Wrist_ACC": "7\n11"
        },
        {
          "Wrist_ACC": "8\n12"
        },
        {
          "Wrist_ACC": "13"
        },
        {
          "Wrist_ACC": "14\n10"
        },
        {
          "Wrist_ACC": "15\n11"
        }
      ],
      "page": 41
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Wrist_EDA\n5\n(\")%!* -0.04) â€“ (-2890.27)": "CX\n6"
        },
        {
          "Wrist_EDA\n5\n(\")%!* -0.04) â€“ (-2890.27)": "1\n7"
        },
        {
          "Wrist_EDA\n5\n(\")%!* -0.04) â€“ (-2890.27)": "3\n9"
        },
        {
          "Wrist_EDA\n5\n(\")%!* -0.04) â€“ (-2890.27)": "5\n11"
        },
        {
          "Wrist_EDA\n5\n(\")%!* -0.04) â€“ (-2890.27)": "6\n13"
        },
        {
          "Wrist_EDA\n5\n(\")%!* -0.04) â€“ (-2890.27)": "7\n14"
        },
        {
          "Wrist_EDA\n5\n(\")%!* -0.04) â€“ (-2890.27)": "9\n15"
        },
        {
          "Wrist_EDA\n5\n(\")%!* -0.04) â€“ (-2890.27)": "11"
        },
        {
          "Wrist_EDA\n5\n(\")%!* -0.04) â€“ (-2890.27)": "13"
        },
        {
          "Wrist_EDA\n5\n(\")%!* -0.04) â€“ (-2890.27)": "14"
        },
        {
          "Wrist_EDA\n5\n(\")%!* -0.04) â€“ (-2890.27)": "15"
        }
      ],
      "page": 41
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Wrist_BVP": "CX"
        },
        {
          "Wrist_BVP": "1"
        },
        {
          "Wrist_BVP": "3"
        },
        {
          "Wrist_BVP": "5"
        },
        {
          "Wrist_BVP": "6"
        },
        {
          "Wrist_BVP": "7\nCX"
        },
        {
          "Wrist_BVP": "9\n1"
        },
        {
          "Wrist_BVP": "10\n3"
        },
        {
          "Wrist_BVP": "11\n5"
        },
        {
          "Wrist_BVP": "13\n6"
        },
        {
          "Wrist_BVP": "15\n7"
        }
      ],
      "page": 42
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7\n-438.12 * ((1.14^ ($)#\") + (-0.8))\nWrist_TEMP": "8\nCX"
        },
        {
          "7\n-438.12 * ((1.14^ ($)#\") + (-0.8))\nWrist_TEMP": "9\n1"
        },
        {
          "7\n-438.12 * ((1.14^ ($)#\") + (-0.8))\nWrist_TEMP": "11\n3"
        },
        {
          "7\n-438.12 * ((1.14^ ($)#\") + (-0.8))\nWrist_TEMP": "13\n5"
        },
        {
          "7\n-438.12 * ((1.14^ ($)#\") + (-0.8))\nWrist_TEMP": "15\n7"
        },
        {
          "7\n-438.12 * ((1.14^ ($)#\") + (-0.8))\nWrist_TEMP": "8"
        },
        {
          "7\n-438.12 * ((1.14^ ($)#\") + (-0.8))\nWrist_TEMP": "9"
        },
        {
          "7\n-438.12 * ((1.14^ ($)#\") + (-0.8))\nWrist_TEMP": "11"
        },
        {
          "7\n-438.12 * ((1.14^ ($)#\") + (-0.8))\nWrist_TEMP": "13"
        },
        {
          "7\n-438.12 * ((1.14^ ($)#\") + (-0.8))\nWrist_TEMP": "15"
        }
      ],
      "page": 42
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chest_ACC": "CX"
        },
        {
          "Chest_ACC": "1\nCX"
        },
        {
          "Chest_ACC": "3\n1"
        },
        {
          "Chest_ACC": "4\n3"
        },
        {
          "Chest_ACC": "5\n4"
        },
        {
          "Chest_ACC": "6\n5"
        },
        {
          "Chest_ACC": "7\n6"
        },
        {
          "Chest_ACC": "8\n7"
        },
        {
          "Chest_ACC": "9\n8"
        },
        {
          "Chest_ACC": "10\n9"
        },
        {
          "Chest_ACC": "11\n10"
        },
        {
          "Chest_ACC": "12\n11"
        },
        {
          "Chest_ACC": "13\n12"
        },
        {
          "Chest_ACC": "14\n13"
        },
        {
          "Chest_ACC": "15\n14"
        }
      ],
      "page": 43
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CX\nEquation\nChest_EDA": "1\nCX"
        },
        {
          "CX\nEquation\nChest_EDA": "3\n1"
        },
        {
          "CX\nEquation\nChest_EDA": "5\n3"
        },
        {
          "CX\nEquation\nChest_EDA": "6\n5"
        },
        {
          "CX\nEquation\nChest_EDA": "7\n6"
        },
        {
          "CX\nEquation\nChest_EDA": "8\n7"
        },
        {
          "CX\nEquation\nChest_EDA": "9\n8"
        },
        {
          "CX\nEquation\nChest_EDA": "10\n9"
        },
        {
          "CX\nEquation\nChest_EDA": "11\n10"
        },
        {
          "CX\nEquation\nChest_EDA": "12\n11"
        },
        {
          "CX\nEquation\nChest_EDA": "13\n12"
        },
        {
          "CX\nEquation\nChest_EDA": "14\n13"
        },
        {
          "CX\nEquation\nChest_EDA": "15\n14"
        },
        {
          "CX\nEquation\nChest_EDA": "15"
        }
      ],
      "page": 43
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chest_ECG": "CX"
        },
        {
          "Chest_ECG": "1"
        },
        {
          "Chest_ECG": "3"
        },
        {
          "Chest_ECG": "5"
        },
        {
          "Chest_ECG": "6\nCX"
        },
        {
          "Chest_ECG": "7\n1"
        },
        {
          "Chest_ECG": "9\n3"
        },
        {
          "Chest_ECG": "10\n5"
        },
        {
          "Chest_ECG": "11\n6"
        },
        {
          "Chest_ECG": "13\n7"
        },
        {
          "Chest_ECG": "14\n9"
        }
      ],
      "page": 44
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "&)\n#''%\n' * (0.0004 - #''!\n5\nChest_TEMP": "CX"
        },
        {
          "&)\n#''%\n' * (0.0004 - #''!\n5\nChest_TEMP": "7\n1"
        },
        {
          "&)\n#''%\n' * (0.0004 - #''!\n5\nChest_TEMP": "8\n3"
        },
        {
          "&)\n#''%\n' * (0.0004 - #''!\n5\nChest_TEMP": "9\n5"
        },
        {
          "&)\n#''%\n' * (0.0004 - #''!\n5\nChest_TEMP": "10"
        },
        {
          "&)\n#''%\n' * (0.0004 - #''!\n5\nChest_TEMP": "12"
        },
        {
          "&)\n#''%\n' * (0.0004 - #''!\n5\nChest_TEMP": "14"
        },
        {
          "&)\n#''%\n' * (0.0004 - #''!\n5\nChest_TEMP": "15"
        },
        {
          "&)\n#''%\n' * (0.0004 - #''!\n5\nChest_TEMP": "10"
        },
        {
          "&)\n#''%\n' * (0.0004 - #''!\n5\nChest_TEMP": "12"
        },
        {
          "&)\n#''%\n' * (0.0004 - #''!\n5\nChest_TEMP": "14"
        },
        {
          "&)\n#''%\n' * (0.0004 - #''!\n5\nChest_TEMP": "15"
        }
      ],
      "page": 44
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chest_EMG": "CX"
        },
        {
          "Chest_EMG": "1"
        },
        {
          "Chest_EMG": "3"
        },
        {
          "Chest_EMG": "5"
        },
        {
          "Chest_EMG": "7\nCX"
        },
        {
          "Chest_EMG": "9\n1"
        },
        {
          "Chest_EMG": "11\n3"
        },
        {
          "Chest_EMG": "12\n5"
        },
        {
          "Chest_EMG": "13\n7"
        },
        {
          "Chest_EMG": "14\n9"
        },
        {
          "Chest_EMG": "15\n11"
        }
      ],
      "page": 45
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5\n1080.99 - (1.11 ^ Weight)\nChest_RESP": "7\nCX"
        },
        {
          "5\n1080.99 - (1.11 ^ Weight)\nChest_RESP": "8\n1"
        },
        {
          "5\n1080.99 - (1.11 ^ Weight)\nChest_RESP": "9\n3"
        },
        {
          "5\n1080.99 - (1.11 ^ Weight)\nChest_RESP": "10\n5"
        },
        {
          "5\n1080.99 - (1.11 ^ Weight)\nChest_RESP": "11\n7"
        },
        {
          "5\n1080.99 - (1.11 ^ Weight)\nChest_RESP": "12\n8"
        },
        {
          "5\n1080.99 - (1.11 ^ Weight)\nChest_RESP": "13\n9"
        },
        {
          "5\n1080.99 - (1.11 ^ Weight)\nChest_RESP": "15\n10"
        },
        {
          "5\n1080.99 - (1.11 ^ Weight)\nChest_RESP": "11"
        },
        {
          "5\n1080.99 - (1.11 ^ Weight)\nChest_RESP": "12"
        },
        {
          "5\n1080.99 - (1.11 ^ Weight)\nChest_RESP": "13"
        },
        {
          "5\n1080.99 - (1.11 ^ Weight)\nChest_RESP": "15"
        }
      ],
      "page": 45
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Spatio-temporal representation learning enhanced speech emotion recognition with multi-head attention mechanisms",
      "authors": [
        "Z Chen",
        "M Lin",
        "Z Wang",
        "Q Zheng",
        "C Liu"
      ],
      "year": "2023",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2023.111077"
    },
    {
      "citation_id": "2",
      "title": "Psychological factors enhanced heterogeneous learning interactive graph knowledge tracing for understanding the learning process",
      "authors": [
        "Z Wang",
        "W Wu",
        "C Zeng",
        "H Luo",
        "J Sun"
      ],
      "year": "2024",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2024.1359199"
    },
    {
      "citation_id": "3",
      "title": "Mental health",
      "authors": [
        "S Dattani",
        "L RodÃ©s-Guirao",
        "H Ritchie",
        "M Roser"
      ],
      "year": "2023",
      "venue": "Mental health"
    },
    {
      "citation_id": "4",
      "title": "Brain on stress: How the social environment gets under the skin",
      "authors": [
        "B Mcewen"
      ],
      "year": "2012",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": "10.1073/pnas.1121254109"
    },
    {
      "citation_id": "5",
      "title": "Autosense: unobtrusively wearable sensor suite for inferring the onset, causality, and consequences of stress in the field",
      "authors": [
        "E Ertin",
        "N Stohs",
        "S Kumar",
        "A Raij",
        "M Absi",
        "S Shah"
      ],
      "year": "2011",
      "venue": "Proceedings of the 9th ACM Conference on Embedded Networked Sensor Systems, SenSys '11",
      "doi": "10.1145/2070942.2070970"
    },
    {
      "citation_id": "6",
      "title": "Enhancing safety of transport by road by on-line monitoring of driver emotions",
      "authors": [
        "S Nadai",
        "M D'incÃ ",
        "F Parodi",
        "M Benza",
        "A Trotta",
        "E Zero",
        "L Zero",
        "R Sacile"
      ],
      "year": "2016",
      "venue": "2016 11th System of Systems Engineering Conference (SoSE)",
      "doi": "10.1109/SYSOSE.2016.7542941"
    },
    {
      "citation_id": "7",
      "title": "Emotion and Stress Recognition Utilizing Galvanic Skin Response and Wearable Technology: A Real-time Approach for Mental Health Care",
      "authors": [
        "E Hosseini",
        "R Fang",
        "R Zhang",
        "S Rafatirad",
        "H Homayoun"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)",
      "doi": "10.1109/BIBM58861.2023.10386049"
    },
    {
      "citation_id": "8",
      "title": "Perceived stress scale: Reliability and validity study in greece",
      "authors": [
        "E Andreou",
        "E Alexopoulos",
        "C Lionis",
        "L Varvogli",
        "C Gnardellis",
        "G Chrousos",
        "C Darviri"
      ],
      "year": "2011",
      "venue": "International Journal of Environmental Research and Public Health",
      "doi": "10.3390/ijerph8083287"
    },
    {
      "citation_id": "9",
      "title": "Eeg-based emotion classification using convolutional neural network",
      "authors": [
        "H Mei",
        "X Xu"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)",
      "doi": "10.1109/SPAC.2017.8304263"
    },
    {
      "citation_id": "10",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2018.2817622"
    },
    {
      "citation_id": "11",
      "title": "Stress detection using wearable physiological sensors and machine learning algorithm",
      "authors": [
        "V Ashwin",
        "R Jegan",
        "P Rajalakshmy"
      ],
      "year": "2022",
      "venue": "2022 6th International Conference on Electronics, Communication and Aerospace Technology",
      "doi": "10.1109/ICECA55336.2022.10009326"
    },
    {
      "citation_id": "12",
      "title": "Abs-cam: A gradient optimization interpretable approach for explanation of convolutional neural networks, Signal",
      "authors": [
        "C Zeng",
        "K Yan",
        "Z Wang",
        "Y Yu",
        "S Xia",
        "N Zhao"
      ],
      "year": "2023",
      "venue": "Image and Video Processing",
      "doi": "10.1007/s11760-022-02313-0"
    },
    {
      "citation_id": "13",
      "title": "A physics-inspired method for symbolic regression",
      "authors": [
        "S.-M Udrescu",
        "M Tegmark",
        "A Feynman"
      ],
      "year": "2020",
      "venue": "Science Advances",
      "doi": "10.1126/sciadv.aay2631"
    },
    {
      "citation_id": "14",
      "title": "A systematic literature review of modalities, trends, and limitations in emotion recognition, affective computing, and sentiment analysis",
      "authors": [
        "R GarcÃ­a-HernÃ¡ndez",
        "H Luna-GarcÃ­a",
        "J Celaya-Padilla",
        "A GarcÃ­a-HernÃ¡ndez",
        "L Reveles-GÃ³mez",
        "L Flores-Chaires",
        "J Delgado-Contreras",
        "D Rondon",
        "K Villalba-Condori"
      ],
      "year": "2024",
      "venue": "Applied Sciences",
      "doi": "10.3390/app14167165"
    },
    {
      "citation_id": "15",
      "title": "Decision support with text-based emotion recognition: Deep learning for affective computing",
      "authors": [
        "B Kratzwald",
        "S Ilic",
        "M Kraus",
        "S Feuerriegel",
        "H Prendinger"
      ],
      "year": "2018",
      "venue": "Decision support with text-based emotion recognition: Deep learning for affective computing",
      "arxiv": "arXiv:1803.06397"
    },
    {
      "citation_id": "16",
      "title": "The imaging maastricht acute stress test (imast): a neuroimaging compatible psychophysiological stressor",
      "authors": [
        "C Quaedflieg",
        "T Meyer",
        "T Smeets"
      ],
      "year": "2013",
      "venue": "Psychophysiology",
      "doi": "10.1111/psyp.12058"
    },
    {
      "citation_id": "17",
      "title": "The montreal imaging stress task: using functional imaging to investigate the effects of perceiving and processing psychosocial stress in the human brain",
      "authors": [
        "K Dedovic",
        "R Renwick",
        "N Mahani",
        "V Engert",
        "S Lupien",
        "J Pruessner"
      ],
      "year": "2005",
      "venue": "Journal of Psychiatry and Neuroscience",
      "doi": "10.1016/S0895-4356(01)00408-1"
    },
    {
      "citation_id": "18",
      "title": "Self-supervised ECG Representation Learning for Emotion Recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.3014842",
      "arxiv": "arXiv:2002.03898"
    },
    {
      "citation_id": "19",
      "title": "Detection of Symptoms of Depression Using Data From the iPhone and Apple Watch",
      "authors": [
        "S Akre",
        "Z Cohen",
        "A Welborn",
        "T Zbozinek",
        "B Balliu",
        "J Flint",
        "A Bui",
        "M Craske"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)",
      "doi": "10.1109/BIBM58861.2023.10385797"
    },
    {
      "citation_id": "20",
      "title": "The swell knowledge work dataset for stress and user modeling research",
      "authors": [
        "S Koldijk",
        "M Sappelli",
        "S Verberne",
        "M Neerincx",
        "W Kraaij"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction, ICMI '14",
      "doi": "10.1145/2663204.2663257"
    },
    {
      "citation_id": "21",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "T Siddharth",
        "T Jung",
        "Sejnowski"
      ],
      "year": "2019",
      "venue": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "arxiv": "arXiv:1905.07039"
    },
    {
      "citation_id": "22",
      "title": "Gsista-net: Generalized structure ista networks for image compressed sensing based on optimized unrolling algorithm",
      "authors": [
        "C Zeng",
        "Y Yu",
        "Z Wang",
        "S Xia",
        "H Cui",
        "X Wan"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-024-18724-9"
    },
    {
      "citation_id": "23",
      "title": "High-quality image compressed sensing and reconstruction with multi-scale dilated convolutional neural network",
      "authors": [
        "Z Wang",
        "Z Wang",
        "C Zeng",
        "Y Yu",
        "X Wan"
      ],
      "year": "2023",
      "venue": "Circuits, Systems, and Signal Processing",
      "doi": "10.1007/s00034-022-02181-6"
    },
    {
      "citation_id": "24",
      "title": "Multi-channel representation learning enhanced unfolding multi-scale compressed sensing network for high quality image reconstruction",
      "authors": [
        "C Zeng",
        "S Xia",
        "Z Wang",
        "X Wan"
      ],
      "year": "2023",
      "venue": "Entropy",
      "doi": "10.3390/e25121579"
    },
    {
      "citation_id": "25",
      "title": "Gbh-yolov5: Ghost convolution with bottleneckcsp and tiny target prediction head incorporating yolov5 for pv panel defect detection",
      "authors": [
        "L Li",
        "Z Wang",
        "T Zhang"
      ],
      "year": "2023",
      "venue": "Electronics",
      "doi": "10.3390/electronics12030561"
    },
    {
      "citation_id": "26",
      "title": "Cascade neural network-based joint sampling and reconstruction for image compressed sensing, Signal",
      "authors": [
        "C Zeng",
        "J Ye",
        "Z Wang",
        "N Zhao",
        "M Wu"
      ],
      "year": "2022",
      "venue": "Image and Video Processing",
      "doi": "10.1007/s11760-021-01955-w"
    },
    {
      "citation_id": "27",
      "title": "Meconformer: Highly representative embedding extractor for speaker verification via incorporating selective convolution into deep speaker encoder",
      "authors": [
        "Q Zheng",
        "Z Chen",
        "Z Wang",
        "H Liu",
        "M Lin"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2023.123004"
    },
    {
      "citation_id": "28",
      "title": "Squeeze-and-excitation self-attention mechanism enhanced digital audio source recognition based on transfer learning, Circuits, Systems, and Signal Processing",
      "authors": [
        "C Zeng",
        "Y Zhao",
        "Z Wang",
        "K Li",
        "X Wan",
        "M Liu"
      ],
      "year": "2024",
      "venue": "Squeeze-and-excitation self-attention mechanism enhanced digital audio source recognition based on transfer learning, Circuits, Systems, and Signal Processing",
      "doi": "10.1007/s00034-024-02850-8"
    },
    {
      "citation_id": "29",
      "title": "An end-to-end transfer learning framework of source recording device identification for audio sustainable security",
      "authors": [
        "Z Wang",
        "J Zhan",
        "G Zhang",
        "D Ouyang",
        "H Guo"
      ],
      "year": "2023",
      "venue": "Sustainability",
      "doi": "10.3390/su151411272"
    },
    {
      "citation_id": "30",
      "title": "Enfformer: Long-short term representation of electric network frequency for digital audio tampering detection",
      "authors": [
        "C Zeng",
        "K Li",
        "Z Wang"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2024.111938"
    },
    {
      "citation_id": "31",
      "title": "Playback attack detection based on channel pattern noise",
      "authors": [
        "Z.-F Wang",
        "Q.-H He",
        "X.-Y Zhang",
        "H.-Y Luo",
        "Z.-S Su"
      ],
      "year": "2011",
      "venue": "Journal of South China University of Technology"
    },
    {
      "citation_id": "32",
      "title": "Discriminative component analysis enhanced feature fusion of electrical network frequency for digital audio tampering detection",
      "authors": [
        "C Zeng",
        "S Kong",
        "Z Wang",
        "K Li",
        "Y Zhao",
        "X Wan",
        "Y Chen"
      ],
      "year": "2024",
      "venue": "Circuits, Systems, and Signal Processing",
      "doi": "10.1007/s00034-024-02787-y"
    },
    {
      "citation_id": "33",
      "title": "Channel pattern noise based playback attack detection algorithm for speaker recognition",
      "authors": [
        "Z.-F Wang",
        "G Wei",
        "Q.-H He"
      ],
      "year": "2011",
      "venue": "2011 International Conference on Machine Learning and Cybernetics",
      "doi": "10.1109/ICMLC.2011.6016982"
    },
    {
      "citation_id": "34",
      "title": "Digital audio tampering detection based on spatio-temporal representation learning of electrical network frequency",
      "authors": [
        "C Zeng",
        "S Kong",
        "Z Wang",
        "K Li",
        "Y Zhao",
        "X Wan",
        "Y Chen"
      ],
      "year": "2024",
      "venue": "Digital audio tampering detection based on spatio-temporal representation learning of electrical network frequency",
      "doi": "10.1007/s11042-024-18887-5"
    },
    {
      "citation_id": "35",
      "title": "Liveness detection using time drift between lip movement and voice",
      "authors": [
        "Z.-Y Zhu",
        "Q.-H He",
        "X.-H Feng",
        "Y.-X Li",
        "Z.-F Wang"
      ],
      "year": "2013",
      "venue": "Liveness detection using time drift between lip movement and voice",
      "doi": "10.1109/ICMLC.2013.6890423"
    },
    {
      "citation_id": "36",
      "title": "Deletion and insertion tampering detection for speech authentication based on fluctuating super vector of electrical network frequency",
      "authors": [
        "C Zeng",
        "S Kong",
        "Z Wang",
        "S Feng",
        "N Zhao",
        "J Wang"
      ],
      "year": "2024",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2024.103046"
    },
    {
      "citation_id": "37",
      "title": "Recording source identification using device universal background model",
      "authors": [
        "Z Wang",
        "Q Liu",
        "J Chen",
        "H Yao"
      ],
      "year": "2015",
      "venue": "International Conference of Educational Innovation through Technology (EITT)",
      "doi": "10.1109/EITT.2015.11"
    },
    {
      "citation_id": "38",
      "title": "Spatio-temporal representation learning enhanced source cell-phone recognition from speech recordings",
      "authors": [
        "C Zeng",
        "S Feng",
        "Z Wang",
        "X Wan",
        "Y Chen",
        "N Zhao"
      ],
      "year": "2024",
      "venue": "Journal of Information Security and Applications",
      "doi": "10.1016/j.jisa.2023.103672"
    },
    {
      "citation_id": "40",
      "title": "Digital audio tampering detection based on enf consistency",
      "authors": [
        "J Wang",
        "C.-Y Wang",
        "Q.-S Zeng",
        "Y Min",
        "M.-Z Tian",
        "Zuo"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Wavelet Analysis and Pattern Recognition (ICWAPR)",
      "doi": "10.1109/ICWAPR.2018.8521378"
    },
    {
      "citation_id": "41",
      "title": "Audio source recording device recognition based on representation learning of sequential gaussian mean matrix",
      "authors": [
        "C Zeng",
        "S Feng",
        "Z Wang",
        "Y Zhao",
        "K Li",
        "X Wan"
      ],
      "year": "2024",
      "venue": "Forensic Science International: Digital Investigation",
      "doi": "10.1016/j.fsidi.2023.301676"
    },
    {
      "citation_id": "42",
      "title": "Robust speaker identification of iot based on stacked sparse denoising autoencoders",
      "authors": [
        "Z Wang",
        "S Duan",
        "C Zeng",
        "X Yu",
        "Y Yang",
        "H Wu"
      ],
      "year": "2020",
      "venue": "2020 International Conferences on Internet of Things (iThings)",
      "doi": "10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics50389.2020.00056"
    },
    {
      "citation_id": "43",
      "title": "Digital audio tampering detection based on deep temporal-spatial features of electrical network frequency",
      "authors": [
        "C Zeng",
        "S Kong",
        "Z Wang",
        "K Li",
        "Y Zhao"
      ],
      "year": "2023",
      "venue": "Information",
      "doi": "10.3390/info14050253"
    },
    {
      "citation_id": "44",
      "title": "Source acquisition device identification from recorded audio based on spatiotemporal representation learning with multi-attention mechanisms",
      "authors": [
        "C Zeng",
        "S Feng",
        "D Zhu",
        "Z Wang"
      ],
      "year": "2023",
      "venue": "Entropy",
      "doi": "10.3390/e25040626"
    },
    {
      "citation_id": "45",
      "title": "Stacked autoencoder networks based speaker recognition",
      "authors": [
        "C.-Y Zeng",
        "C.-F Ma",
        "Z.-F Wang",
        "J.-X Ye"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Machine Learning and Cybernetics (ICMLC)",
      "doi": "10.1109/ICMLC.2018.8526953"
    },
    {
      "citation_id": "46",
      "title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
      "authors": [
        "S Brunton",
        "J Proctor",
        "J Kutz"
      ],
      "year": "2016",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": "10.1073/pnas.1517384113"
    },
    {
      "citation_id": "47",
      "title": "Integrating knowledge-guided symbolic regression and model-based design of experiments to automate process flow diagram development",
      "authors": [
        "A Rogers",
        "A Lane",
        "C Mendoza",
        "S Watson",
        "A Kowalski",
        "P Martin",
        "D Zhang"
      ],
      "year": "2024",
      "venue": "Chemical Engineering Science",
      "doi": "10.1016/j.ces.2024.120580"
    },
    {
      "citation_id": "48",
      "title": "Application of the symbolic regression program aifeynman to psychology",
      "authors": [
        "M Miyazaki",
        "K.-I Ishikawa",
        "K Nakashima",
        "H Shimizu",
        "T Takahashi",
        "N Takahashi"
      ],
      "year": "2023",
      "venue": "Frontiers in Artificial Intelligence",
      "doi": "10.3389/frai.2023.1039438"
    },
    {
      "citation_id": "49",
      "title": "Automated discovery of symbolic laws governing skill acquisition from naturally occurring data",
      "authors": [
        "S Liu",
        "Q Li",
        "X Shen",
        "J Sun",
        "Z Yang"
      ],
      "year": "2024",
      "venue": "Nature Computational Science",
      "doi": "10.1038/s43588-024-00629-0"
    },
    {
      "citation_id": "50",
      "title": "Audio tampering forensics based on representation learning of enf phase sequence",
      "authors": [
        "C Zeng",
        "Y Yang",
        "Z Wang",
        "S Kong",
        "S Feng"
      ],
      "year": "2022",
      "venue": "International Journal of Digital Crime and Forensics",
      "doi": "10.4018/IJDCF.302894"
    },
    {
      "citation_id": "51",
      "title": "Shallow and deep feature fusion for digital audio tampering detection",
      "authors": [
        "Z Wang",
        "Y Yang",
        "C Zeng",
        "S Kong",
        "S Feng",
        "N Zhao"
      ],
      "year": "2022",
      "venue": "EURASIP Journal on Advances in Signal Processing",
      "doi": "10.1186/s13634-022-00900-4"
    },
    {
      "citation_id": "52",
      "title": "Spatial and temporal learning representation for end-to-end recording device identification",
      "authors": [
        "C Zeng",
        "D Zhu",
        "Z Wang",
        "M Wu",
        "W Xiong",
        "N Zhao"
      ],
      "year": "2021",
      "venue": "EURASIP Journal on Advances in Signal Processing",
      "doi": "10.1186/s13634-021-00763-1"
    },
    {
      "citation_id": "53",
      "title": "Robust speaker recognition based on stacked auto-encoders",
      "authors": [
        "Z Wang",
        "C Zeng",
        "S Duan",
        "H Ouyang",
        "H Xu"
      ],
      "year": "2021",
      "venue": "Advances in Networked-Based Information Systems"
    },
    {
      "citation_id": "54",
      "title": "Deep and shallow feature fusion and recognition of recording devices based on attention mechanism",
      "authors": [
        "C Zeng",
        "D Zhu",
        "Z Wang",
        "Y Yang"
      ],
      "year": "2021",
      "venue": "Advances in Intelligent Networking and Collaborative Systems"
    },
    {
      "citation_id": "55",
      "title": "An end-to-end deep source recording device identification system for web media forensics",
      "authors": [
        "C Zeng",
        "D Zhu",
        "Z Wang",
        "Z Wang",
        "N Zhao",
        "L He"
      ],
      "year": "2020",
      "venue": "International Journal of Web Information Systems",
      "doi": "10.1108/IJWIS-06-2020-0038"
    },
    {
      "citation_id": "56",
      "title": "Sae based unified double jpeg compression detection system for web image forensics",
      "authors": [
        "Z Wang",
        "C Zuo",
        "C Zeng"
      ],
      "year": "2021",
      "venue": "International Journal of Web Information Systems",
      "doi": "10.1108/IJWIS-11-2020-0073"
    },
    {
      "citation_id": "57",
      "title": "Image compressed sensing and reconstruction of multi-scale residual network combined with channel attention mechanism",
      "authors": [
        "C Zeng",
        "Z Wang",
        "Z Wang",
        "K Yan",
        "Y Yu"
      ],
      "year": "2021",
      "venue": "Journal of Physics: Conference Series",
      "doi": "10.1088/1742-6596/2010/1/012134"
    },
    {
      "citation_id": "58",
      "title": "Occlusion handling using moving volume and ray casting techniques for augmented reality systems",
      "authors": [
        "Y Tian",
        "X Wang",
        "H Yao",
        "J Chen",
        "Z Wang",
        "L Yi"
      ],
      "year": "2018",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-017-5228-2"
    },
    {
      "citation_id": "59",
      "title": "Virtual chime-bells experimental system based on multi-modal fusion",
      "authors": [
        "Z Wang",
        "Q Liu",
        "H Yao",
        "J Chen"
      ],
      "year": "2015",
      "venue": "International Conference of Educational Innovation through Technology (EITT)",
      "doi": "10.1109/EITT.2015.20"
    },
    {
      "citation_id": "60",
      "title": "Image reconstruction of iot based on parallel cnn",
      "authors": [
        "C Zeng",
        "Z Wang",
        "Z Wang"
      ],
      "year": "2020",
      "venue": "2020 International Conferences on Internet of Things (iThings)",
      "doi": "10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics50389.2020.00057"
    },
    {
      "citation_id": "61",
      "title": "An evaluation of html5 and webgl for medical imaging applications",
      "authors": [
        "Q Min",
        "Z Wang",
        "N Liu"
      ],
      "year": "2018",
      "venue": "Journal of Healthcare Engineering",
      "doi": "10.1155/2018/1592821"
    },
    {
      "citation_id": "62",
      "title": "Double compression detection based on feature fusion",
      "authors": [
        "Z.-F Wang",
        "L Zhu",
        "Q.-S Min",
        "C.-Y Zeng"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Machine Learning and Cybernetics (ICMLC)",
      "doi": "10.1109/ICMLC.2017.8108951"
    },
    {
      "citation_id": "63",
      "title": "A tutorial on hidden markov models and selected applications in speech recognition",
      "authors": [
        "L Rabiner"
      ],
      "year": "1989",
      "venue": "Proceedings of the IEEE",
      "doi": "10.1109/5.18626"
    },
    {
      "citation_id": "64",
      "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "authors": [
        "J Lafferty",
        "A Mccallum",
        "F Pereira"
      ],
      "year": "2001",
      "venue": "Proceedings of the Eighteenth International Conference on Machine Learning"
    },
    {
      "citation_id": "65",
      "title": "Recurrent neural network based language model",
      "authors": [
        "T Mikolov",
        "M KarafiÃ¡t",
        "L Burget",
        "J ÄŒernockÃ½",
        "S Khudanpur"
      ],
      "year": "2010",
      "venue": "Interspeech 2010, ISCA",
      "doi": "10.21437/Interspeech.2010-343"
    },
    {
      "citation_id": "66",
      "title": "Long-short term memory neural networks language modeling for handwriting recognition",
      "authors": [
        "V Frinken",
        "F Zamora-MartÃ­nez",
        "S EspaÃ±a-Boquera",
        "M Castro-Bleda",
        "A Fischer",
        "H Bunke"
      ],
      "year": "2012",
      "venue": "Proceedings of the 21st International Conference on Pattern Recognition"
    },
    {
      "citation_id": "67",
      "title": "Learning to forget: continual prediction with lstm",
      "authors": [
        "F Gers",
        "J Schmidhuber",
        "F Cummins"
      ],
      "year": "1999",
      "venue": "Ninth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470)",
      "doi": "10.1049/cp:19991218"
    },
    {
      "citation_id": "68",
      "title": "Speech emotion recognition using cnn and lstm",
      "authors": [
        "M Dhavale",
        "P Bhandari"
      ],
      "year": "2022",
      "venue": "2022 6th International Conference On Computing, Communication, Control And Automation (ICCUBEA)",
      "doi": "10.1109/ICCUBEA54992.2022.10010751"
    },
    {
      "citation_id": "69",
      "title": "Decoding human emotions: Analyzing multi-channel eeg data using lstm networks",
      "authors": [
        "S Sateesh",
        "S Bk"
      ],
      "year": "2024",
      "venue": "Decoding human emotions: Analyzing multi-channel eeg data using lstm networks",
      "arxiv": "arXiv:2408.10328"
    },
    {
      "citation_id": "70",
      "title": "Using transformers for multimodal emotion recognition: Taxonomies and state of the art review",
      "authors": [
        "S Hazmoune",
        "F Bougamouza"
      ],
      "year": "2024",
      "venue": "Engineering Applications of Artificial Intelligence",
      "doi": "10.1016/j.engappai.2024.108339"
    },
    {
      "citation_id": "71",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v34i02.5492"
    },
    {
      "citation_id": "72",
      "title": "Speech emotion recognition using cnn-lstm and vision transformer",
      "authors": [
        "C Kumar",
        "A Maharana",
        "S Krishnan",
        "S Hanuma",
        "G Lal",
        "V Ravi"
      ],
      "year": "2023",
      "venue": "Innovations in Bio-Inspired Computing and Applications"
    },
    {
      "citation_id": "73",
      "title": "Slbdetection-net: Towards closed-set and open-set student learning behavior detection in smart classroom of k-12 education",
      "authors": [
        "Z Wang",
        "L Li",
        "C Zeng",
        "S Dong",
        "J Sun"
      ],
      "year": "2025",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2024.125392"
    },
    {
      "citation_id": "74",
      "title": "Design and implementation of an ai-enabled visual report tool as formative assessment to promote learning achievement and self-regulated learning: An experimental study",
      "authors": [
        "X Liao",
        "X Zhang",
        "Z Wang",
        "H Luo"
      ],
      "year": "2024",
      "venue": "British Journal of Educational Technology",
      "doi": "10.1111/bjet.13424"
    },
    {
      "citation_id": "75",
      "title": "Sbd-net: Incorporating multi-level features for an efficient detection network of student behavior in smart classrooms",
      "authors": [
        "Z Wang",
        "M Wang",
        "C Zeng",
        "L Li"
      ],
      "year": "2024",
      "venue": "Applied Sciences",
      "doi": "10.3390/app14188357"
    },
    {
      "citation_id": "76",
      "title": "Advanced mathematics exercise recommendation based on automatic knowledge extraction and multilayer knowledge graph",
      "authors": [
        "S Dong",
        "X Tao",
        "R Zhong",
        "Z Wang",
        "M Zuo",
        "J Sun"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Learning Technologies",
      "doi": "10.1109/TLT.2023.3333669"
    },
    {
      "citation_id": "77",
      "title": "Students' classroom behavior detection system incorporating deformable detr with swin transformer and light-weight feature pyramid network",
      "authors": [
        "Z Wang",
        "J Yao",
        "C Zeng",
        "L Li",
        "C Tan"
      ],
      "year": "2023",
      "venue": "Systems",
      "doi": "10.3390/systems11070372"
    },
    {
      "citation_id": "78",
      "title": "A unified interpretable intelligent learning diagnosis framework for learning performance prediction in intelligent tutoring systems",
      "authors": [
        "Z Wang",
        "W Yan",
        "C Zeng",
        "Y Tian",
        "S Dong"
      ],
      "year": "2023",
      "venue": "International Journal of Intelligent Systems",
      "doi": "10.1155/2023/4468025"
    },
    {
      "citation_id": "79",
      "title": "Enhanced convolutional neural networks based learner authentication for personalized e-learning system",
      "authors": [
        "Z Wang",
        "M Wang",
        "C Zeng",
        "J Yao",
        "Y Yang",
        "H Xu"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Intelligent Education and Intelligent Research (IEIR)",
      "doi": "10.1109/IEIR59294.2023.10391216"
    },
    {
      "citation_id": "80",
      "title": "Student learning behavior recognition incorporating data augmentation with learning feature representation in smart classrooms",
      "authors": [
        "Z Wang",
        "L Li",
        "C Zeng",
        "J Yao"
      ],
      "year": "2023",
      "venue": "Sensors",
      "doi": "10.3390/s23198190"
    },
    {
      "citation_id": "81",
      "title": "Multiple learning features-enhanced knowledge tracing based on learner-resource response channels",
      "authors": [
        "Z Wang",
        "Y Hou",
        "C Zeng",
        "S Zhang",
        "R Ye"
      ],
      "year": "2023",
      "venue": "Sustainability",
      "doi": "10.3390/su15129427"
    },
    {
      "citation_id": "82",
      "title": "Designing effective instructional feedback using a diagnostic and visualization system: Evidence from a high school biology class",
      "authors": [
        "L Ma",
        "X Zhang",
        "Z Wang",
        "H Luo"
      ],
      "year": "2023",
      "venue": "Systems",
      "doi": "10.3390/systems11070364"
    },
    {
      "citation_id": "83",
      "title": "Knowledge relation rank enhanced heterogeneous learning interaction modeling for neural graph forgetting knowledge tracing",
      "authors": [
        "L Li",
        "Z Wang"
      ],
      "year": "2023",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0295808"
    },
    {
      "citation_id": "84",
      "title": "Knowledge graph-enhanced intelligent tutoring system based on exercise representativeness and informativeness",
      "authors": [
        "L Li",
        "Z Wang"
      ],
      "year": "2023",
      "venue": "International Journal of Intelligent Systems",
      "doi": "10.1155/2023/2578286"
    },
    {
      "citation_id": "85",
      "title": "Calibrated q-matrix-enhanced deep knowledge tracing with relational attention mechanism",
      "authors": [
        "L Li",
        "Z Wang"
      ],
      "year": "2023",
      "venue": "Applied Sciences",
      "doi": "10.3390/app13042541"
    },
    {
      "citation_id": "86",
      "title": "Yolov5 enhanced learning behavior recognition and analysis in smart classroom with multiple students",
      "authors": [
        "Z Wang",
        "J Yao",
        "C Zeng",
        "W Wu",
        "H Xu",
        "Y Yang"
      ],
      "year": "2022",
      "venue": "2022 International Conference on Intelligent Education and Intelligent Research (IEIR)",
      "doi": "10.1109/IEIR56323.2022.10050042"
    },
    {
      "citation_id": "87",
      "title": "Smart contract vulnerability detection for educational blockchain based on graph neural networks",
      "authors": [
        "Z Wang",
        "W Wu",
        "C Zeng",
        "J Yao",
        "Y Yang",
        "H Xu"
      ],
      "year": "2022",
      "venue": "2022 International Conference on Intelligent Education and Intelligent Research (IEIR)",
      "doi": "10.1109/IEIR56323.2022.10050059"
    },
    {
      "citation_id": "88",
      "title": "Deep knowledge tracing based on spatial and temporal representation learning for learning performance prediction",
      "authors": [
        "L Lyu",
        "Z Wang",
        "H Yun",
        "Z Yang",
        "Y Li"
      ],
      "year": "2022",
      "venue": "Applied Sciences",
      "doi": "10.3390/app12147188"
    },
    {
      "citation_id": "89",
      "title": "Integrating a cloud learning environment into english-medium instruction to enhance non-native english-speaking students' learning",
      "authors": [
        "Q Min",
        "Z Wang",
        "N Liu"
      ],
      "year": "2019",
      "venue": "Innovations in Education and Teaching International",
      "doi": "10.1080/14703297.2018.1483838"
    },
    {
      "citation_id": "90",
      "title": "Discriminating stress from cognitive load using a wearable eda device",
      "authors": [
        "C Setz",
        "B Arnrich",
        "J Schumm",
        "R La Marca",
        "G TrÃ¶ster",
        "U Ehlert"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Information Technology in Biomedicine",
      "doi": "10.1109/TITB.2009.2036164"
    },
    {
      "citation_id": "91",
      "title": "Electronic Filter Simulation & Design",
      "authors": [
        "G Bianchi",
        "R Sorrentino"
      ],
      "year": "2007",
      "venue": "Electronic Filter Simulation & Design"
    },
    {
      "citation_id": "92",
      "title": "cvxEDA: A Convex Optimization Approach to Electrodermal Activity Processing",
      "authors": [
        "A Greco",
        "G Valenza",
        "A Lanata",
        "E Scilingo",
        "L Citi"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Biomedical Engineering",
      "doi": "10.1109/TBME.2015.2474131"
    },
    {
      "citation_id": "93",
      "title": "Neurokit2: A python toolbox for neurophysiological signal processing",
      "authors": [
        "D Makowski",
        "T Pham",
        "Z Lau",
        "J Brammer",
        "F Lespinasse",
        "H Pham",
        "C SchÃ¶lzel",
        "S Chen"
      ],
      "year": "2021",
      "venue": "Behavior Research Methods",
      "doi": "10.3758/s13428-020-01516-y"
    },
    {
      "citation_id": "94",
      "title": "Koopa: learning non-stationary time series dynamics with koopman predictors, NIPS '23",
      "authors": [
        "Y Liu",
        "C Li",
        "J Wang",
        "M Long"
      ],
      "year": "2024",
      "venue": "Koopa: learning non-stationary time series dynamics with koopman predictors, NIPS '23"
    },
    {
      "citation_id": "95",
      "title": "Axiomatic attribution for deep networks",
      "authors": [
        "M Sundararajan",
        "A Taly",
        "Q Yan"
      ],
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine Learning"
    },
    {
      "citation_id": "96",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection, ICMI '18",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Duerichen",
        "C Marberger",
        "K Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Introducing wesad, a multimodal dataset for wearable stress and affect detection, ICMI '18",
      "doi": "10.1145/3242969.3242985"
    },
    {
      "citation_id": "97",
      "title": "Stress Detection with Machine Learning and Deep Learning using Multimodal Physiological Data",
      "authors": [
        "P Bobade",
        "M Vani"
      ],
      "year": "2020",
      "venue": "2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA)",
      "doi": "10.1109/ICIRCA48905.2020.9183244"
    },
    {
      "citation_id": "98",
      "title": "Continuous stress detection using the sensors of commercial smartwatch",
      "authors": [
        "P Siirtola"
      ],
      "year": "2019",
      "venue": "Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers",
      "doi": "10.1145/3341162.3344831"
    },
    {
      "citation_id": "99",
      "title": "Emotion recognition using cvxeda-based features",
      "authors": [
        "H Ferdinando",
        "E Alasaarela"
      ],
      "year": "2018",
      "venue": "Journal of Telecommunication, Electronic and Computer Engineering (JTEC)"
    },
    {
      "citation_id": "100",
      "title": "A systematic exploration of deep neural networks for eda-based emotion recognition",
      "authors": [
        "D Yu",
        "S Sun"
      ],
      "year": "2020",
      "venue": "Information",
      "doi": "10.3390/info11040212"
    }
  ]
}