{
  "paper_id": "2211.03371v1",
  "title": "Hi,Kia: A Speech Emotion Recognition Dataset For Wake-Up Words",
  "published": "2022-11-07T08:57:16Z",
  "authors": [
    "Taesu Kim",
    "SeungHeon Doh",
    "Gyunpyo Lee",
    "Hyungseok Jeon",
    "Juhan Nam",
    "Hyeon-Jeong Suk"
  ],
  "keywords": [
    "such as Ok Google or Hey Siri and",
    "as a result",
    "the length is very short. RAVDESS and TESS have the lexically-matched characteristics. Especially",
    "TESS has the shortest average duration utterance about 2.06 seconds as shown in Table  1 . However",
    "WUW utterances are generally even shorter",
    "less or equal to a second. OK Aura",
    "a recently released WUW dataset",
    "contains 1247 utterances from 80 speakers with rich metadata annotations such as gender",
    "room size",
    "accent",
    "and emotions  [3] . The dataset d"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Wake-up words (WUW) is a short sentence used to activate a speech recognition system to receive the user's speech input. WUW utterances include not only the lexical information for waking up the system but also non-lexical information such as speaker identity or emotion. In particular, recognizing the user's emotional state may elaborate the voice communication. However, there is few dataset where the emotional state of the WUW utterances is labeled. In this paper, we introduce Hi, KIA, a new WUW dataset which consists of 488 Korean accent emotional utterances collected from four male and four female speakers and each of utterances is labeled with four emotional states including anger, happy, sad, or neutral. We present the step-bystep procedure to build the dataset, covering scenario selection, post-processing, and human validation for label agreement. Also, we provide two classification models for WUW speech emotion recognition using the dataset. One is based on traditional handcraft features and the other is a transfer-learning approach using a pre-trained neural network. These classification models could be used as benchmarks in further research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Voice recognition technology has rapidly advanced, deploying voice user interfaces (VUIs) in a wide range of applications. Studies revealed that the VUIs appeal the users because they are invisible  [4]  and the emotional communication is more anticipated than the conventional interface types  [5] . In particular, the in-vehicle VUIs have received great interests as they ensure safer driving  [6] ,  [7]  and enrich the driver's emotional experience  [8] . The most frequent use of VUIs is to speak short wake-up words (WUW) to activate the interface. Previous studies have focused on detecting WUW utterances accurately and quickly  [9] ,  [10] . In addition, some of them recognized the speaker's identity to reject imposters from WUW utterances  [11] ,  [12] . Activating VUIs using WUW is similar to the genuine way of speaking when the user issues a command  [13] . In many cases, users will speak as if they were talking with another person when giving commands to use VUIs. This naturally conveys the emotion while the user speaks. Therefore, understanding the emotions of WUW can provide another dimension to enhance human-machine interaction.\n\nSpeech emotion recognition (SER) requires a dataset that includes rich emotional utterances and labels. There are a handful of SER datasets, for example, IEMOCAP  [14] , EmoDB  [15] , RAVDESS  [1] , and TESS  [2] . The SER datasets were designed for text-independent emotion recognition. In other words, the system based on the datasets should recognize the speaker's emotion regardless of the lexical information. On the other hand, the SER dataset for WUW is confined to the signature keyword such as Ok Google or Hey Siri and, as a result, the length is very short. RAVDESS and TESS have the lexically-matched characteristics. Especially, TESS has the shortest average duration utterance about 2.06 seconds as shown in Table  1 . However, WUW utterances are generally even shorter, less or equal to a second. OK Aura, a recently released WUW dataset, contains 1247 utterances from 80 speakers with rich metadata annotations such as gender, room size, accent, and emotions  [3] . The dataset distinguishes the utterances with three emotions such as annoyed, friendly, or neutral; however, only 218 out of 1247 are labeled.\n\nWe constructed a new emotion-labeled WUW dataset, Hi, KIA, to address the current limitations. It is composed of 488 recordings from eight Korean voice actors and actresses that correspond to four emotional states including angry, happy, sad and neutral. Compared to the existing datasets, Hi, KIA has the shortest average utterance length (0.64 sec) and the emotion of all WUW samples are manually labeled. We present the step-by-step procedure to build the dataset, covering scenario selection, recording/post-processing, and human validation for label agreement. In addition, we provide two classification models for WUW emotion recognition using the dataset. One is based on a traditional approach using hand-craft audio features and the other is a deep-learning approach using a pretrained neural network in a transfer-learning setting. We release the Hi, KIA dataset 1  and the source code of the classification models  2  . We expect that they can be used for VUI-based applications in the future.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Dataset",
      "text": "Hi, KIA is a speech emotion recognition dataset for WUW. It is labeled with four emotional states including anger, happy, sad, or neutral. While this dataset was originally designed for in-vehicle VUIs, it can be also used for general-purpose text-dependent speech emotion recognition. The entire dataset development was conducted via online platforms due to the COVID-19 pandemic situation. This section describes the stepby-step procedure.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Scenario Selection",
      "text": "The first step is selecting scenarios where the speaker utters WUW in different emotional states. While it is possible to ask actors and actress to imagine an emotional state and speak the short words without a context, it can limit variations of nuances within the same emotional state. To maximize the diversity and naturalness of emotional speech rendering by the voice actors and actresses, we prepared a set of user scenarios. The scenario was provided as a text script which starts with Hi, KIA (WUW) and ends with a contextual sentence that evokes an emotional state.\n\nFor scenario selection, we worked with eight graduate students who have more than three years of experience in the area of affective computing. They were requested to come up with various situations where they use the VUI in a certain emotion. We first brought up five driver's emotions (anger, stress, happiness, fear, and sadness) based on a study by Zepf et al.  [16]  and asked them to propose at least two scenarios for each emotion. As a result, we collected 53 scenarios after merging duplicated ones. We mapped them to the valence-arousal coordination as shown in Figure  1(a) . Subsequently, we excluded the emotion category of the 4th quadrant in the emotion circumplex as there are few scenarios, and clustered the entire scenarios into angry (low valence-high arousal), happy (high valence-high arousal), sad (low valencelow arousal), and neutral (mid valence-mid arousal). Then, we selected three representative scenarios for each group, which are colored in red, orange, blue, and black in Figure  1(a) .\n\nWe prepared cards for the selected scenarios to facilitate the recording process. In each card, a sentence is presented as a recording guide on top of an illustrated car interior. Additionally, a reference image was provided as the background to describe the situation of the corresponding sentence as shown in Figure  1(b) . Then, we could augment the scenario by providing an image that visualizes the situation. The visual cue is a simple yet effective method to motivate the participants Fig.  1:  The participants watched with one of the visualized scenarios and pretended to be in the given situation and the emotional circumstance. When ready, they clicked the record button and spoke out the dialogues that began with Hi, KIA. (voice actors and actresses in our case) to immerse themselves into the given scenario easily  [17] ,  [18] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Recording And Post-Processing",
      "text": "We recruited four voice actors and four voice actresses online who have similar recording conditions. Their average age was 31.38 years with a standard deviation of 3.90 years. The recording process was as follows: 1) We provided a recording tutorial online. In the tutorial, we provided the voice actors and actresses with an instruction to express intended emotions to be elicited. In addition, we requested them to place their mouse position 30 cm away from the microphone during the recording. After the first recording session, we provided feedback on the quality of the recorded audio to ensure that their emotions were captured correctly. Finally, the voice actors and actresses were asked to record five more utterances. As a result, we collected 576 audio files for the 12 scenarios. After we collected the voice recordings of complete sentences, we cropped out the WUW segment at the beginning of the recorded audio.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Human Validation",
      "text": "We conducted human validation to remove improper data from the collected recordings. The eight graduate students who participated in scenario selection conducted the evaluation. Given all 576 recordings presented in a random order, they classified each recording into 'angry', 'happy', 'sad', and 'neutral'. If the recording was difficult to recognize, it was classified as 'unknown'. From the validation result, we removed 88 recordings that all human evaluators predicted differently from the true label. This resulted in 488 recordings as the final dataset as shown in Table  I .\n\nWe combined the human evaluators' responses and calculated the confusion matrix as shown in Figure  2 . We found that the evaluators were relatively good at classifying 'sad' emotion. On the contrary, they had difficulty identifying 'angry' and 'neutral' voices; they instead evaluated 'angry' as 'neutral' and 'neutral' as 'sad'. We also noticed that they felt a higharousal voice as emotional ground states: participants observed 'angry' and 'happy' as 'neutral' emotions. It indicated that people have difficulty recognizing high-arousal impressions from WUW.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Wake-Up Word Emotion Recognition",
      "text": "We define WUW emotion recognition as a task that predicts the speaker's emotional state from WUW utterances. We conducted the emotion recognition task using Hi, KIA. Considering the small size of the dataset, we explored two training strategies. One is using hand-craft audio features based on domain knowledge. The other is fine-tuning a pretrained neural network model with the small dataset by leveraging the generalization capability of the model trained with a largescale dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Hand-Craft Features",
      "text": "Speech emotion recognition is related to various acoustic properties of speech, including pitch, loudness and timbre. Traditional approaches used low-level descriptors (LLDs) or high-level statistical functions from the speech signals as input features for the classification  [19] ,  [20] ,  [21] . we used the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS)  [22] , which include frequency, energy, and spectral domain features. The 88-dimensional eGeMAPS feature was z-score standardized using fixed mean and standard deviation values. Figure  3  shows two violin plots of energy and pitch distributions from the entire dataset. The general trend shows that the high-arousal group ('angry', 'happy') is distinguished well from the low arousal group ('sad', 'neutral'). For comparison with deep neural network based features, utterance-level eGeMAPS features were used as input of a logistic regression classifier.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Fine-Tuning With Pretrained Wav2Vec 2.0",
      "text": "More recent approaches use deep learning for speech emotion recognition  [23] . However, the lack of annotated data has limited the approaches. To address the issue, transfer learning using a large pre-trained neural network, such as Wav2vec  [24] , improved the emotion recognition accuracy  [25] ,  [26] ,  [27] ,  [28] . With Hi, KIA, we also conducted a similar transfer learning using Wav2vec2.0. Pretrained Wav2vec2.0 Wav2vec2.0  [29]    Fine-tuning methods We extracted features using Wav2vec2.0 and obtained utterance-level features through average pooling. Following the previous work  [28] , we explored different finetuning strategies for the modules in Wav2vec2.0. The first is measuring the emotion recognition performance in a vanila Wav2vec2.0 trained with a long-length libri-speech corpus without emotional supervision (no fine-tuning). The second is fine-tuning either the encoder network or the context network, which is responsible for a low-level or high-level representation, respectively. The last is fine-tuning the entire networks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Experiment Setup",
      "text": "Data split and Metrics The Hi, KIA dataset contains 488 clips of speech from four male and four female speakers. For speaker independence, we performed 8-fold cross-validation, where utterances from seven speakers are used for train, validation set, and utterances from the remaining one speaker are used for test set in each fold. As a result, we report both weighted accuracy (WA, the overall accuracy across all classes) and unweighted accuracy (UA, the average of the accuracy for each of the classes).\n\nHyper-parameters We conducted experiments using the pretrained model: wav2vec2.0-base. The wav2vec-2.0-base was composed of 12 transformer blocks and 7 convolutional blocks (each has 512 channels). Our implementation was based on the Huggingface transformers repository  [30] . We optimized the  model using AdamW  [31]  with parameters β 1 =0.9, β 2 =0.999. The learning rate used for training was 5e 5 , the epoch was 200. We used full audio with 16,000 Hz sampling rate, and 1 batch size.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Results",
      "text": "Table  III  shows classification performance of classification models and human validation. In the case of human validation, the final score was calculated as the average performance of 8 human evaluators. Wav2vec2.0 feature without fine-tuning does not perform better than hand-craft features. This indicates that it is difficult to extract high-level emotion features only with self-supervised learning. Fine-tuning Wav2vec2.0 significantly improves the classification accuracy. Among the three setups, fine-tuning the contextual network works best, achieving 68.64% in WA and 68.51% in UA. This indicates that, for a small dataset, it is more efficient to update parameters related to high-level representations rather than update all parameters. Another interesting result is that fine-tuning the contextual network outperforms human validation. This is presumably due to the subjective nature of emotion recognition.\n\nFigure  4  shows WA of 8 folds with 4 males and 4 females. What is noteworthy here is that the the wav2vec2.0 feature outperforms human validation performance in most female folds. Human validation performance is relatively stable in both male and female folds. Hand-craft features and 'Wav2vec2.0 FT' shows a performance gap between male and female fold, especially lower performance than human validation in fold M1, M2, M4 and F5.\n\nFigure  5  shows the confusion matrix by hand-craft feature and Wav2vec2.0 contextual network fine-tuning. Both models are good at discriminating arousal and valence differences ('happy', 'sad') but hand-craft features are weak in understand- ing valence differences within high arousal ('angry', 'happy') and sad-neutral label pairs. This problem is alleviated in Wav2vec2.0 contextual network fine-tuning. Compared with Figure  2  and Figure  5 , Wav2vec2.0 contextual network finetuning outperforms human validation in discriminating high arousal emotion and neural.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusions",
      "text": "This paper proposes a new public dataset, Hi, KIA, an emotion-labeled WUW dataset. We described a carefullydesigned procedure to collect short emotional utterances. After conducting human validation, we finalize the dataset composed of 488 recordings. It is a shot-length speech dataset that contains the Korean accent and utterance-level emotion annotations with four emotion classes. We have also presented baseline results for short utterance-level speech emotion recognition on this dataset, using hand-crafted features and transfer learning to overcome the limitation of a small dataset. The results show that we can achieve high accuracy in the fourway emotion recognition. As future work, we will add data on more type of emotions, such as calm or relax emotions. Then, we will conduct in-depth analysis of essential audio features to better understand WUW emotion classification. In addition, we plan to develop a speech emotion recognition model robust to speaker and gender.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (b). Then, we could augment the scenario by",
      "page": 2
    },
    {
      "caption": "Figure 1: The participants watched with one of the visualized scenarios and pretended to be in the given situation and the emotional",
      "page": 2
    },
    {
      "caption": "Figure 2: We found that",
      "page": 3
    },
    {
      "caption": "Figure 2: Confusion matrix of human validation after removing",
      "page": 3
    },
    {
      "caption": "Figure 3: shows two violin plots of energy and pitch",
      "page": 3
    },
    {
      "caption": "Figure 3: Utterance-level energy and pitch distribution of WUW",
      "page": 4
    },
    {
      "caption": "Figure 4: Fold-wise WA performance. WAV2VEC2 FT means the",
      "page": 4
    },
    {
      "caption": "Figure 4: shows WA of 8 folds with 4 males and 4 females.",
      "page": 4
    },
    {
      "caption": "Figure 5: shows the confusion matrix by hand-craft feature",
      "page": 4
    },
    {
      "caption": "Figure 5: Confusion Matrix of BASELINE and WAV2VEC2.FT.",
      "page": 5
    },
    {
      "caption": "Figure 2: and Figure 5, Wav2vec2.0 contextual network ﬁne-",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 1: However, WUW utterances are generally",
      "page": 1
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "2",
      "title": "Toronto emotional speech set (tess)",
      "authors": [
        "M Pichora-Fuller",
        "K Dupuis"
      ],
      "year": "2020",
      "venue": "Scholars Portal Dataverse"
    },
    {
      "citation_id": "3",
      "title": "Tase: Task-aware speech enhancement for wake-up word detection in voice assistants",
      "authors": [
        "G Cámbara",
        "F López",
        "D Bonet",
        "P Gómez",
        "C Segura",
        "M Farrús",
        "J Luque"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "4",
      "title": "Development and comparison of customized voice-assistant systems for independent living older adults",
      "authors": [
        "S Shalini",
        "T Levins",
        "E Robinson",
        "K Lane",
        "G Park",
        "M Skubic"
      ],
      "year": "2019",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "5",
      "title": "Improving automotive safety by pairing driver emotion and car voice emotion",
      "authors": [
        "C Nass",
        "I.-M Jonsson",
        "H Harris",
        "B Reaves",
        "J Endo",
        "S Brave",
        "L Takayama"
      ],
      "year": "2005",
      "venue": "Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "6",
      "title": "The smartphone and the driver's cognitive workload: A comparison of apple, google, and microsoft's intelligent personal assistants",
      "authors": [
        "D Strayer",
        "J Cooper",
        "J Turrill",
        "J Coleman",
        "R Hopman"
      ],
      "year": "2017",
      "venue": "Canadian Journal of Experimental Psychology"
    },
    {
      "citation_id": "7",
      "title": "Effects of user experience on user resistance to change to the voice user interface of an in-vehicle infotainment system: Implications for platform and standards competition",
      "authors": [
        "D.-H Kim",
        "H Lee"
      ],
      "year": "2016",
      "venue": "International Journal of Information Management"
    },
    {
      "citation_id": "8",
      "title": "How users react to proactive voice assistant behavior while driving",
      "authors": [
        "M Schmidt",
        "W Minker",
        "S Werner"
      ],
      "year": "2020",
      "venue": "Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "9",
      "title": "Adversarial music: Real world audio adversary against wake-word detection system",
      "authors": [
        "J Li",
        "S Qu",
        "X Li",
        "J Szurley",
        "J Kolter",
        "F Metze"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "10",
      "title": "On front-end gain invariant modeling for wake word spotting",
      "authors": [
        "Y Gao",
        "N Stein",
        "C.-C Kao",
        "Y Cai",
        "M Sun",
        "T Zhang",
        "S Vitaladevuni"
      ],
      "year": "2020",
      "venue": "On front-end gain invariant modeling for wake word spotting",
      "arxiv": "arXiv:2010.06676"
    },
    {
      "citation_id": "11",
      "title": "Self-defined text-dependent wake-up-words speaker recognition system",
      "authors": [
        "T.-H Tsai",
        "P.-C Hao",
        "C.-L Wang"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "12",
      "title": "Hi-mia: A far-field text-dependent speaker verification database and the baselines",
      "authors": [
        "X Qin",
        "H Bu",
        "M Li"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Developing voice-based branding: insights from the mercedes case",
      "authors": [
        "M Vernuccio",
        "M Patrizi",
        "A Pastore"
      ],
      "year": "2020",
      "venue": "Journal of Product & Brand Management"
    },
    {
      "citation_id": "14",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "15",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "16",
      "title": "Driver emotion recognition for intelligent vehicles: A survey",
      "authors": [
        "S Zepf",
        "J Hernandez",
        "A Schmitt",
        "W Minker",
        "R Picard"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "17",
      "title": "The sas approach: combining qualitative and quantitative knowledge in environmental scenarios",
      "authors": [
        "J Alcamo"
      ],
      "year": "2008",
      "venue": "Developments in integrated environmental assessment"
    },
    {
      "citation_id": "18",
      "title": "Learning from the future: Competitive foresight scenarios",
      "authors": [
        "L Fahey",
        "R Randall"
      ],
      "year": "1997",
      "venue": "Learning from the future: Competitive foresight scenarios"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition by speech signals",
      "authors": [
        "O.-W Kwon",
        "K Chan",
        "J Hao",
        "T.-W Lee"
      ],
      "year": "2003",
      "venue": "European conference on speech communication and technology"
    },
    {
      "citation_id": "20",
      "title": "Automatic speech emotion recognition using support vector machine",
      "authors": [
        "P Shen",
        "Z Changjun",
        "X Chen"
      ],
      "year": "2011",
      "venue": "International Conference on Electronic & Mechanical Engineering and Information Technology"
    },
    {
      "citation_id": "21",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "22",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "23",
      "title": "Emotion identification from raw speech signals using dnns",
      "authors": [
        "M Sarma",
        "P Ghahremani",
        "D Povey",
        "N Goel",
        "K Sarma",
        "N Dehak"
      ],
      "year": "2018",
      "venue": "Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "24",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "25",
      "title": "Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2021",
      "venue": "Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition",
      "arxiv": "arXiv:2110.06309"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition with multi-task learning",
      "authors": [
        "X Cai",
        "J Yuan",
        "R Zheng",
        "L Huang",
        "K Church"
      ],
      "year": "2021",
      "venue": "Proceedings of Interspeech"
    },
    {
      "citation_id": "27",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "28",
      "title": "Temporal context in speech emotion recognition",
      "authors": [
        "Y Xia",
        "L.-W Chen",
        "A Rudnicky",
        "R Stern"
      ],
      "year": "2021",
      "venue": "Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "29",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "30",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz"
      ],
      "year": "2020",
      "venue": "Conference on empirical methods in natural language processing: system demonstrations"
    },
    {
      "citation_id": "31",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    }
  ]
}