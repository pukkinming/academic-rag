{
  "paper_id": "2506.12325v1",
  "title": "Gsdnet: Revisiting Incomplete Multimodal-Diffusion From Graph Spectrum Perspective For Conversation Emotion Recognition",
  "published": "2025-06-14T03:24:19Z",
  "authors": [
    "Yuntao Shou",
    "Jun Yao",
    "Tao Meng",
    "Wei Ai",
    "Cen Chen",
    "Keqin Li"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition in conversations (MERC) aims to infer the speaker's emotional state by analyzing utterance information from multiple sources (i.e., video, audio, and text). Compared with unimodality, a more robust utterance representation can be obtained by fusing complementary semantic information from different modalities. However, the modality missing problem severely limits the performance of MERC in practical scenarios. Recent work has achieved impressive performance on modality completion using graph neural networks and diffusion models, respectively. This inspires us to combine these two dimensions through the graph diffusion model to obtain more powerful modal recovery capabilities. Unfortunately, existing graph diffusion models may destroy the connectivity and local structure of the graph by directly adding Gaussian noise to the adjacency matrix, resulting in the generated graph data being unable to retain the semantic and topological information of the original graph. To this end, we propose a novel Graph Spectral Diffusion Network (GSDNet), which maps Gaussian noise to the graph spectral space of missing modalities and recovers the missing data according to its original distribution. Compared with previous graph diffusion methods, GSDNet only affects the eigenvalues of the adjacency matrix instead of destroying the adjacency matrix directly, which can maintain the global topological information and important spectral features during the diffusion process. Extensive experiments have demonstrated that GSDNet achieves state-of-the-art emotion recognition performance in various modality loss scenarios.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition in conversations (MERC) aims to build an emotion recognition model with cross- * Corresponding author domain understanding, reasoning, and learning capabilities by integrating video, audio, and text data  [Li et al., 2023] ,  [Tsai et al., 2019] . MERC research mainly focuses on how to effectively encode discriminative representations from different modalities and achieve more accurate information fusion and analysis  [Ramesh et al., 2021] ,  [Ding et al., 2023] ,  [Shou et al., 2022] ,  [Shou et al., 2023] ,  [Meng et al., 2024b] .\n\nHowever, the modality missing problem is unavoidable in the real world, and it may severely degrade the performance of multimodal understanding models  [Wang et al., 2023a] ,  [Meng et al., 2024a] ,  [Shou et al., 2024d] ,  [Ai et al., 2024] . For example, the text modality may fail due to environmental noise interference, resulting in the inability to obtain valid text information. The acoustic modality may lose part of the sound information due to sensor failure  [Meng et al., 2024c] ,  [Shou et al., 2025a] . The visual modality may be affected by factors such as poor lighting conditions, object occlusion, or privacy protection requirements, resulting in the inability to obtain image or video data  [Shou et al., 2025b] ,  [Ai et al., 2025] . Therefore, how to design and optimize a multimodal emotion recognition model for conversation that can cope with modality loss has become an important direction of current research  [Wang et al., 2024] ,  [Zhang et al., 2024] .\n\nRecently, graph neural networks (GCNs)  [Ai et al., 2023] ,  [Shou et al., 2024c] ,  [Shou et al., 2024e]  and diffusion generative models (DGMs) have shown outstanding performance in multiple tasks such as vision, and language  [Hu et al., 2021] ,  [Jo et al., 2022] . The core of GCNs is message passing based on graph structure to establish complex dependencies  [Shou et al., 2024a] ,  [Shou et al., 2024b] . The core of DGMs is forward denoising and backward denoising to learn the original distribution of data. Considering the potential of GCNs and DGMs, some researchers have tried to apply GCNs and SGMs to modality completion. They alleviate the impact of modality loss on multimodal emotion recognition performance from different perspectives. For GCNs,  [Lian et al., 2023]  applied graph completion networks to model semantic dependencies between different modal data to achieve modal recovery. For DGMs,  [Wang et al., 2024]  proposed modal diffusion to learn the original distribution of data to achieve modal recovery. All of the above methods have shown significant results because they have strong modeling capabilities in their respective dimensions. Specifically, GCNs and DGMs respectively model the dependence and distribution between multi-modal data to achieve modal recovery. Generally, fusing complex semantic information between multi-modal features and capturing the original distribution of multi-modal data are crucial for emotion recognition performance in missing modalities. This inspires us to combine these two dimensions to obtain more powerful modal recovery capabilities.\n\nHowever, unlike visual data, which has dense structural information, the structure of graph data is generally sparse, causing the data generated by the graph diffusion model to be unable to retain the topological information of the original graph. As shown in Fig.  1 , the image perturbed by Gaussian noise still retains recognizable numerical patterns and local structural information in the early and middle stages of forward diffusion. For example, even if the details of the image are gradually covered by noise, its global contour and edge information can still be captured by the model to a certain extent. This enables the model to effectively restore the content of the image using this residual information during the reverse diffusion process. However, during the forward diffusion of graph data, the topological structure of the graph adjacency matrix is rapidly lost and a dense noise matrix is formed. Intuitively, the diffusion method of inserting Gaussian noise into the graph adjacency matrix seriously undermines the ability to learn the graph topology and feature representation. From a theoretical perspective, running diffusion over the entire space of the adjacency matrix will cause the signal-to-noise ratio (SNR) to drop rapidly and approach zero. Since the SNR is basically zero, the scoring network will not be able to effectively capture the gradient information of the original distribution during training.\n\nTo overcome these problems, we propose a novel Graph Spectral Diffusion Network (GSDNet) to strictly restrict the diffusion of Gaussian noise to the spectral space of the adjacency matrix. Specifically, we perform eigendecomposition on the adjacency matrix, decomposing it into eigenvalue matrix and eigenvector matrix and adding Gaussian noise to the eigenvalues without interfering with the eigenvectors. On the one hand, by operating the eigenvalues in the spectral space, the direct destruction of the local structure of the adjacency matrix can be effectively avoided, ensuring the generated graph still conforms to the global semantics. On the other hand, this constrained diffusion process can more naturally capture and preserve the topological information of the graph, ensuring that the generated graph has consistent spectral features. Overall, our contributions are as:\n\n• We design a novel modality completion model, the Graph Spectral Diffusion Network (GSDNet), which can simultaneously model the dependencies between multimodal features and the distribution of original data to obtain powerful modality recovery capabilities.\n\n• We strictly limit the diffusion of Gaussian noise in the spectral space of the adjacency matrix to avoid the destruction of the graph structure and ensure that the graph data generated by GSDNet still conforms to the global semantics.\n\n• We conduct extensive experiments on multiple realworld datasets to demonstrate that our GSDNet outperforms state-of-the-art methods for conversational emotion recognition in incomplete multimodal scenarios.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Incomplete Multimodal Learning",
      "text": "In practical applications, missing modalities are an inevitable problem  [Fu et al., 2025] . To address this challenge, an effective approach is to find a low-dimensional subspace that can be shared by all modalities, in which the correlation between different modalities is maximized. However, strategies based on shared low-dimensional subspaces often ignore the complementarity between heterogeneous modalities. To overcome this shortcoming, another more effective approach is to restore the missing modality through the existing modality. This process not only requires inferring the content of the missing modality based on the features of the known modality but also ensures that the restored modality can work together with other modalities.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Score-Based Generative Models",
      "text": "Score-based generative models (SGMs) estimate the probability distribution of data by parameterizing the score function  [Song and Ermon, 2019] ,  [Song and Ermon, 2020]  3 Preliminary Information",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Score-Based Generative Models",
      "text": "SGMs are efficient generative models that can generate highquality data and model complex data distribution. A typical SGMs consists of a forward noising and a backward denoising, where the forward noising gradually adds noise to the real data to transform the data from the real distribution to the noise distribution, and the backward denoising starts from the noise sample and gradually removes the noise using the score function to restore the real data sample. Given an input X ∈ R d and a complicated data distribution D, a forward noising process can be obtained through a stochastic differential equation (SDE) as follows:\n\nwhere σ t the diffusion coefficient, B represents the Brownian motion.\n\nAssuming that p t is a probability density function, the reverse denoising process can be established through the reversed time SDE as follows:\n\nwhere d t = -dt is the negative infinitesimal time step, B is the reversed time Brownian motion.\n\nIn the reverse denoising process, the scoring network s(X(t), t; θ) provides gradient information for the current noise sample X t , indicating how to adjust the value of the sample so as to gradually restore the original data distribution. During the training process, the scoring network is to minimize the gap between the model estimated score function and the true score function through the score matching loss function as follows:\n\nwhere U(0, T ) is a uniform distribution over [0, T ]. Given a well-trained scoring network s θ * , we can generate realistic data by solving the learned reverse-time SDE as follows:\n\nwhere π is the prior information of the data.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Score-Based Graph Generative Models",
      "text": "In\n\nwhere σ X,t , and σ A,t the diffusion coefficient, B A t , and B X t represent the Brownian motion.\n\nAssuming that p t is a probability density function, the reverse denoising process can be established through the reversed time SDE as follows:\n\nwhere BA t , and BX t represent the reversed time Brownian motion.\n\nGiven G 0 , the joint probability distribution of X t and A t can be simplified to the product of two simpler distributions  [Jo et al., 2022] , so that the objective function of denoising score matching can be simplified in form as follows:\n\n4 Problem Definition\n\nAssume that the set {X 1 , X 2 , . . . , X M } represents the M modalities, where X k represents the input of the k-th modality. We introduce a binary indicator α ∈ {0, 1} to identify the availability status of each modality. If the k-th modality is missing, let α k = 0; conversely, if the k-th modality is available, let α k = 1. We can define a set of missing modalities I m = {k|α k = 0}. In this incomplete modality scenario, the goal is to recover these unobserved modalities to make up for the missing information. The process of modal recovery usually needs to rely on the existing observed modal information I o = {k|α k = 1}, and complete it by modeling the correlation between modalities.\n\nOur main idea is to recover the missing emotion modality I m from its latent distribution space conditioned on the observed modality I o . We use the observed modality I o as a semantic condition to guide the generation of the missing modality, ensuring that the recovered modality data is consistent and relevant to the real data. Formally, we denote the data distribution of the missing modality as p(X m ) and the data distribution of the available modality as p(X Io ). Our ultimate goal is to sample the missing modality data from the conditional distribution p(X m |X Io ). Inspired by graph completion networks  [Lian et al., 2023]  and diffusion modality generation  [Wang et al., 2024] , we combine the advantages of GNNs and diffusion models to simultaneously model the complementary semantic information between modalities and reconstruct high-quality missing modality features.\n\nHowever, directly adding Gaussian noise to the adjacency matrix may seriously destroy the local structure of the graph, resulting in an unreasonable generated adjacency matrix. To overcome these problems, we propose to strictly restrict the diffusion of Gaussian noise to the spectral space of the adjacency matrix. On the one hand, by operating the eigenvalues in the spectral space, the direct destruction of the local structure of the adjacency matrix can be effectively avoided, ensuring that the generated graph still conforms to the global semantics. On the other hand, this constrained diffusion process can more naturally capture and preserve the topological information of the graph, ensuring that the generated graph has good connectivity and consistent spectral features.\n\nSpecifically, we consider a multi-step diffusion model to gradually construct the conditional distribution by perturbing X m and Λ m , where A m = U m Λ m U m , U are the eigenvectors and Λ is the diagonal eigenvalues. In the t-th step, the conditional transfer distribution of the modal features and the adjacency matrix can be expressed as p t (X m (t)|X Io (0)) and p t (Λ m (t)|Λ Io (0)) and can be approximated as follows:\n\nAccording to the score-based diffusion model  [Jo et al., 2022] , we calculated the conditional transition probability score p t (X m (t)|X Io (0)) and p t (Λ m (t)|Λ Io (0)) as follows:\n\nwhere X Io (t) and Λ Io (t) is a random sample from p t (X Io (t))|X Io (0)) and p t (Λ Io (t))|Λ Io (0)), respectively. Eq. 9 is held because:\n\nFinally, we derive the score-matching objective as follows:\n\n5 THE PROPOSED METHOD",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Modality Encoder",
      "text": "Since the original features of text, audio, and video modalities usually have significant dimensional differences, directly using the original features of the modalities to recover missing modalities may lead to difficulties in semantic alignment or even introduce noise.To ensure that the unimodal sequence representations of the three modalities can be mapped to in the same feature space, we input these modalities into a onedimensional convolutional layer to achieve feature alignment:\n\nwhere l m represents the size of the one-dimensional convolution kernel corresponding to the m-th modality. N represents the number of utterances in the conversation. d represents the dimension of the common feature space.\n\nTo make full use of the position and order information in the sequence, we introduced position embedding when processing the sequence after convolution:\n\nwhere pos represents the index of the sequence, and dimension i represents the index of the feature dimension.\n\nOverall, we feed position embeddings into a convolutional sequence as follows:\n\nThe framework of GSDNet. Given incomplete input data, GSDNet encodes shallow features through 1D-Conv and combines position embedding information. In the missing modal graph diffusion network, we sample from the prior noise distribution and add it to the node features and diagonal eigenvalues, and then solve the inverse time diffusion through the score model to denoise the features to generate new samples. Finally, the reconstructed features are used as complete data to predict the emotion label.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Missing Modality Graph Spectral Diffusion Network",
      "text": "We train two scoring networks s θ and s ϕ to model the distribution of missing modalities m ∈ I m , respectively. Similar to score-based diffusion  [Jo et al., 2022] , the corresponding inverse-time SDE can be derived as follows:\n\nThe core of Eq. 15 is to guide the inverse diffusion process through the trained score network, gradually transforming random noise into missing modal data consistent with the true distribution. We assume that the language modality X l , the visual modality X v , and the corresponding diagonal eigenvalues Λ l and Λ v are observed, while the acoustic modality X a and the corresponding diagonal eigenvalues Λ a are missing. Our goal is to model the missing acoustic modality X a and Λ a , and recover their data through the score network s θ and s ϕ conditioned on X l , X v , Λ l and Λ v as follows:\n\nwhere ∆t is a discrete time step size and ϵ t ∼ N (0, I). After enough iterations, we can gradually guide the noise data to approach the target distribution and finally obtain the restored acoustic modal data X ′ a and the corresponding diagonal eigenvalues Λ ′ a . To generate more refined acoustic modalities, we input the restored acoustic data xa into a specially designed acoustic modal reconstruction module D X and diagonal eigenvalues reconstruction module D Λ to obtain the final reconstructed acoustic modal Xa = D X (X ′ a ) and diagonal eigenvalues Λa = D Λ (Λ ′ a ). We define a reconstruction loss function L rec to measure the difference between the reconstructed data and the original target modal data under any missing patterns as follows:\n\nTherefore, the loss of the missing modality graph diffusion network is as follows:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Multimodal Fusion And Prediction",
      "text": "The recovered data and the observed available data are combined to obtain the complete multimodal data H and the adjacency matrix A. To achieve the fusion of multimodal data, we use GCN to capture the complementary semantic information between the modalities as follows:\n\nwhere W (l) is the learnable weight matrix of the l-th layer.\n\nTo train the entire model, we combine the losses of the above reconstruction and prediction tasks into a joint optimization objective function as follows:\n\nwhere β is a hyperparameter.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Databases And Setup 6.1 Datasets",
      "text": "We conduct extensive experiments on two MERC datasets to conduct experiments, including CMU-MOSI  [Zadeh et al., 2016] , and CMU-MOSEI  [Zadeh et al., 2018] . On the two datasets, we extract the lexical modality features via pretrained RoBERTa-Large model  [Liu et al., 2019]  and obtain a 1024-dimensional word embedding. For visual modality, each video frame was encoded via DenseNet model  [Huang et al., 2017]  and obtain a 1024-dimensional visual feature.\n\nThe acoustic modality was processed by wav2vec  [Schneider et al., 2019]  to obtain the 512-dimensional acoustic features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Discussion",
      "text": "7.1 Comparison with the state-of-the-arts\n\nTables 1 and 2 lists the quantitative results of the different missing modalities and the random missing ratio on CMU-MOSI and CMU-CMSEI datasets, showing the performance of different methods under the missing modal. Specifically, GSDNet achieved the best results on the two datasets, verifying its superiority in dealing with modal missing. The performance improvement of GSDNet may be attributed to its ability to explicitly restore the missing modality, which not only helps to restore the lost information but also provides additional complementary information for MERC. In addition, GSDNet has a significant advantage maintaining consistency between the restored modality and the original modality. This distribution consistency ensures that the information fusion between different modalities is smoother and more accurate, further improving the overall performance of the model. Compared with other MERC methods, the performance degradation of GSDNet decreases as the modal missing rate increases. In practical applications, when the modal missing rate is high, most recovery-based models will experience significant performance degradation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct ablation experiments on the CMU-MOSI and CMU-MOSEI datasets. The results in Table  3  show that GS-DNet consistently outperforms all variants. Removing the frequency diffusion degrades the performance, which high- lights the role of frequency diffusion in capturing the distribution of multimodal data.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Visualization Of Embedding Space",
      "text": "Fig.  3  shows the distribution of the restored data and the original data in the feature space obtained by different restoration methods under the condition of fixed missing modalities. In order to compare these distributions more intuitively, we use t-SNE dimensionality reduction technology on the CMU-MOSEI dataset to project the high-dimensional features into two-dimensional space for visualization. As can be seen from Fig.  3 , the modal data restored by GSDNet is closest to the distribution of the original data, which shows that GSDNet can better maintain the original feature distribution of the data when restoring the missing modalities. In contrast, there is a clear difference between the distribution of the restored data of other methods and the original data, especially in some local areas, the degree of overlap of the distribution is low. baseline models under different missing rates, our proposed GSDNet always outperforms other baseline methods in the CMU-MOSI and CMU-MOSEI datasets and all missing rate conditions. Specifically, GSDNet not only shows strong interpolation performance under low missing rates but also has more outstanding performance advantages under high missing rates. The experimental results show that speaker dependency and data distribution consistency play a vital role in data interpolation tasks. Most baseline methods often ignore the synergy of these dependencies, which limits their interpolation performance when dealing with missing data. In contrast, GSDNet can use the speakers relationship to perform more accurate interpolation while maintaining data distribution consistency through the graph diffusion model, so that GSDNet can always maintain relatively good performance under various missing rates.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Imputation Performance",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we introduce a novel Graph Spectral Diffusion Network (GSDNet), which maps Gaussian noise to the graph spectral space of missing modalities and recover the missing data according to original distribution. Compared with previous graph diffusion methods, GSDNet only affects the eigenvalues of the adjacency matrix instead of destroying the adjacency matrix directly, which can maintain the global topological information and important spectral features during the diffusion process. Extensive experiments have demonstrated that GSDNet achieves state-of-the-art emotion recognition performance in various modality loss scenarios.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the difference between images and graphs",
      "page": 2
    },
    {
      "caption": "Figure 1: , the image perturbed by Gaussian",
      "page": 2
    },
    {
      "caption": "Figure 2: The framework of GSDNet. Given incomplete input data, GSDNet encodes shallow features through 1D-Conv and combines",
      "page": 5
    },
    {
      "caption": "Figure 3: Visualization of restored modalities. Avail. indicates available.",
      "page": 7
    },
    {
      "caption": "Figure 3: shows the distribution of the restored data and the orig-",
      "page": 7
    },
    {
      "caption": "Figure 3: , the modal data restored by GSDNet is closest to the",
      "page": 7
    },
    {
      "caption": "Figure 4: shows the interpolation results of different methods un-",
      "page": 7
    },
    {
      "caption": "Figure 4: The comparison of interpolation performance under differ-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "shouyuntao@stu.xjtu.edu.cn, yj@ahnu.edu.cn, mengtao@hnu.edu.cn, aiwei@hnu.edu.cn,"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "chencen@scut.edu.cn, lik@newpaltz.edu"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "domain understanding,\nreasoning, and learning capabilities"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "by integrating video, audio, and text data [Li et al., 2023],"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "[Tsai et al., 2019]. MERC research mainly focuses on how to"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "effectively encode discriminative representations from differ-"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "ent modalities and achieve more accurate information fusion"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "and analysis [Ramesh et al., 2021], [Ding et al., 2023], [Shou"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "et al., 2022], [Shou et al., 2023], [Meng et al., 2024b]."
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "However,\nthe modality missing problem is unavoidable in"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "the real world, and it may severely degrade the performance"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "of multimodal understanding models [Wang et al., 2023a],"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "[Meng et al., 2024a], [Shou et al., 2024d], [Ai et al., 2024]."
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "For example,\nthe text modality may fail due to environmen-"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "tal noise interference, resulting in the inability to obtain valid"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "text information. The acoustic modality may lose part of the"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "sound information due to sensor failure [Meng et al., 2024c],"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "[Shou et al., 2025a].\nThe visual modality may be affected"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "by factors such as poor lighting conditions, object occlusion,"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "or privacy protection requirements,\nresulting in the inability"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "to obtain image or video data [Shou et al., 2025b],\n[Ai et"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "al., 2025]. Therefore, how to design and optimize a multi-"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "modal emotion recognition model\nfor conversation that can"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "cope with modality loss has become an important direction"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "of current research [Wang et al., 2024], [Zhang et al., 2024]."
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "Recently, graph neural networks (GCNs) [Ai et al., 2023],"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "[Shou et al., 2024c], [Shou et al., 2024e] and diffusion gener-"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "ative models (DGMs) have shown outstanding performance"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "in multiple tasks\nsuch as vision,\nand language [Hu et al.,"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "2021], [Jo et al., 2022]. The core of GCNs is message pass-"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "ing based on graph structure to establish complex dependen-"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "cies [Shou et al., 2024a],\n[Shou et al., 2024b]. The core of"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "DGMs is forward denoising and backward denoising to learn"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "the original distribution of data.\nConsidering the potential"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "of GCNs and DGMs,\nsome researchers have tried to apply"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "GCNs and SGMs\nto modality completion.\nThey alleviate"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "the impact of modality loss on multimodal emotion recog-"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "nition performance from different perspectives.\nFor GCNs,"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "[Lian et al.,\n2023]\napplied graph completion networks\nto"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "model semantic dependencies between different modal data"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "to achieve modal recovery. For DGMs,\n[Wang et al., 2024]"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": ""
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "proposed modal diffusion to learn the original distribution of"
        },
        {
          "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA": "data to achieve modal\nrecovery. All of\nthe above methods"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Forward:": ""
        },
        {
          "Forward:": ""
        },
        {
          "Forward:": ""
        },
        {
          "Forward:": ""
        },
        {
          "Forward:": ""
        },
        {
          "Forward:": ""
        },
        {
          "Forward:": "Reversed:"
        },
        {
          "Forward:": "(a) Diffusion process on the image"
        },
        {
          "Forward:": "Forward:"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reversed:": "(a) Diffusion process on the image",
          "rally capture and preserve the topological\ninformation of the": "graph, ensuring that the generated graph has consistent spec-"
        },
        {
          "Reversed:": "Forward:",
          "rally capture and preserve the topological\ninformation of the": "tral features. Overall, our contributions are as:"
        },
        {
          "Reversed:": "",
          "rally capture and preserve the topological\ninformation of the": "• We\ndesign\na\nnovel modality\ncompletion model,\nthe"
        },
        {
          "Reversed:": "",
          "rally capture and preserve the topological\ninformation of the": "Graph Spectral Diffusion Network (GSDNet), which"
        },
        {
          "Reversed:": "",
          "rally capture and preserve the topological\ninformation of the": "can\nsimultaneously model\nthe\ndependencies\nbetween"
        },
        {
          "Reversed:": "",
          "rally capture and preserve the topological\ninformation of the": "multimodal features and the distribution of original data"
        },
        {
          "Reversed:": "",
          "rally capture and preserve the topological\ninformation of the": "to obtain powerful modality recovery capabilities."
        },
        {
          "Reversed:": "Reversed:",
          "rally capture and preserve the topological\ninformation of the": ""
        },
        {
          "Reversed:": "",
          "rally capture and preserve the topological\ninformation of the": "• We strictly limit\nthe diffusion of Gaussian noise in the"
        },
        {
          "Reversed:": "(b) Diffusion process on the graph",
          "rally capture and preserve the topological\ninformation of the": ""
        },
        {
          "Reversed:": "",
          "rally capture and preserve the topological\ninformation of the": "spectral space of the adjacency matrix to avoid the de-"
        },
        {
          "Reversed:": "Figure 1:\nIllustration of the difference between images and graphs",
          "rally capture and preserve the topological\ninformation of the": ""
        },
        {
          "Reversed:": "",
          "rally capture and preserve the topological\ninformation of the": "struction of the graph structure and ensure that the graph"
        },
        {
          "Reversed:": "in the diffusion processes.",
          "rally capture and preserve the topological\ninformation of the": ""
        },
        {
          "Reversed:": "",
          "rally capture and preserve the topological\ninformation of the": "data generated by GSDNet still conforms to the global"
        },
        {
          "Reversed:": "have shown significant results because they have strong mod-",
          "rally capture and preserve the topological\ninformation of the": "semantics."
        },
        {
          "Reversed:": "eling capabilities in their respective dimensions. Specifically,",
          "rally capture and preserve the topological\ninformation of the": ""
        },
        {
          "Reversed:": "",
          "rally capture and preserve the topological\ninformation of the": "• We\nconduct\nextensive\nexperiments\non multiple\nreal-"
        },
        {
          "Reversed:": "GCNs and DGMs respectively model the dependence and dis-",
          "rally capture and preserve the topological\ninformation of the": ""
        },
        {
          "Reversed:": "",
          "rally capture and preserve the topological\ninformation of the": "world datasets to demonstrate that our GSDNet outper-"
        },
        {
          "Reversed:": "tribution between multi-modal data to achieve modal recov-",
          "rally capture and preserve the topological\ninformation of the": ""
        },
        {
          "Reversed:": "",
          "rally capture and preserve the topological\ninformation of the": "forms state-of-the-art methods for conversational emo-"
        },
        {
          "Reversed:": "ery. Generally, fusing complex semantic information between",
          "rally capture and preserve the topological\ninformation of the": ""
        },
        {
          "Reversed:": "",
          "rally capture and preserve the topological\ninformation of the": "tion recognition in incomplete multimodal scenarios."
        },
        {
          "Reversed:": "multi-modal\nfeatures and capturing the original distribution",
          "rally capture and preserve the topological\ninformation of the": ""
        },
        {
          "Reversed:": "of multi-modal data are crucial for emotion recognition per-",
          "rally capture and preserve the topological\ninformation of the": ""
        },
        {
          "Reversed:": "formance in missing modalities. This inspires us to combine",
          "rally capture and preserve the topological\ninformation of the": "2\nRelated Work"
        },
        {
          "Reversed:": "these two dimensions to obtain more powerful modal recov-",
          "rally capture and preserve the topological\ninformation of the": ""
        },
        {
          "Reversed:": "",
          "rally capture and preserve the topological\ninformation of the": "2.1\nIncomplete Multimodal Learning"
        },
        {
          "Reversed:": "ery capabilities.",
          "rally capture and preserve the topological\ninformation of the": ""
        },
        {
          "Reversed:": "However, unlike visual data, which has dense structural",
          "rally capture and preserve the topological\ninformation of the": "In practical applications, missing modalities are an inevitable"
        },
        {
          "Reversed:": "information,\nthe structure of graph data is generally sparse,",
          "rally capture and preserve the topological\ninformation of the": "problem [Fu et al., 2025]. To address this challenge, an ef-"
        },
        {
          "Reversed:": "causing the data generated by the graph diffusion model\nto",
          "rally capture and preserve the topological\ninformation of the": "fective approach is to find a low-dimensional subspace that"
        },
        {
          "Reversed:": "be unable to retain the topological information of the original",
          "rally capture and preserve the topological\ninformation of the": "can be shared by all modalities,\nin which the correlation be-"
        },
        {
          "Reversed:": "graph. As shown in Fig. 1, the image perturbed by Gaussian",
          "rally capture and preserve the topological\ninformation of the": "tween different modalities\nis maximized.\nHowever,\nstrate-"
        },
        {
          "Reversed:": "noise still\nretains recognizable numerical patterns and local",
          "rally capture and preserve the topological\ninformation of the": "gies based on shared low-dimensional subspaces often ignore"
        },
        {
          "Reversed:": "structural\ninformation in the early and middle stages of for-",
          "rally capture and preserve the topological\ninformation of the": "the complementarity between heterogeneous modalities. To"
        },
        {
          "Reversed:": "ward diffusion. For example, even if the details of the image",
          "rally capture and preserve the topological\ninformation of the": "overcome this shortcoming, another more effective approach"
        },
        {
          "Reversed:": "are gradually covered by noise,\nits global contour and edge",
          "rally capture and preserve the topological\ninformation of the": "is to restore the missing modality through the existing modal-"
        },
        {
          "Reversed:": "information can still be captured by the model\nto a certain",
          "rally capture and preserve the topological\ninformation of the": "ity. This process not only requires inferring the content of the"
        },
        {
          "Reversed:": "extent. This enables the model to effectively restore the con-",
          "rally capture and preserve the topological\ninformation of the": "missing modality based on the features of the known modality"
        },
        {
          "Reversed:": "tent of\nthe image using this residual\ninformation during the",
          "rally capture and preserve the topological\ninformation of the": "but also ensures that the restored modality can work together"
        },
        {
          "Reversed:": "reverse diffusion process. However, during the forward dif-",
          "rally capture and preserve the topological\ninformation of the": "with other modalities.\nExisting modality restoration meth-"
        },
        {
          "Reversed:": "fusion of graph data,\nthe topological structure of\nthe graph",
          "rally capture and preserve the topological\ninformation of the": "ods can be divided into several\ntypes,\nincluding zero-based"
        },
        {
          "Reversed:": "adjacency matrix is rapidly lost and a dense noise matrix is",
          "rally capture and preserve the topological\ninformation of the": "restoration [Parthasarathy and Sundaram,\n2020],\naverage-"
        },
        {
          "Reversed:": "formed.\nIntuitively,\nthe diffusion method of inserting Gaus-",
          "rally capture and preserve the topological\ninformation of the": "based restoration [Zhang et al., 2020],\nand deep learning-"
        },
        {
          "Reversed:": "sian noise into the graph adjacency matrix seriously under-",
          "rally capture and preserve the topological\ninformation of the": "based restoration [Pham et al., 2019]. Since zero-filling and"
        },
        {
          "Reversed:": "mines the ability to learn the graph topology and feature rep-",
          "rally capture and preserve the topological\ninformation of the": "average-based restoration methods do not use any supervised"
        },
        {
          "Reversed:": "resentation.\nFrom a theoretical perspective,\nrunning diffu-",
          "rally capture and preserve the topological\ninformation of the": "information, the data they restore often have a significant gap"
        },
        {
          "Reversed:": "sion over the entire space of the adjacency matrix will cause",
          "rally capture and preserve the topological\ninformation of the": "with the original data. In contrast, deep learning-based meth-"
        },
        {
          "Reversed:": "the signal-to-noise ratio (SNR) to drop rapidly and approach",
          "rally capture and preserve the topological\ninformation of the": "ods, with their powerful\nfeature\nlearning capabilities,\ncan"
        },
        {
          "Reversed:": "zero.\nSince the SNR is basically zero,\nthe scoring network",
          "rally capture and preserve the topological\ninformation of the": "more accurately estimate the missing modality. For example,"
        },
        {
          "Reversed:": "will not be able to effectively capture the gradient\ninforma-",
          "rally capture and preserve the topological\ninformation of the": "Tran et al.\n[Tran et al., 2017] used a cascaded residual au-"
        },
        {
          "Reversed:": "tion of the original distribution during training.",
          "rally capture and preserve the topological\ninformation of the": "toencoder to restore the missing modality, and the network’s"
        },
        {
          "Reversed:": "To overcome these problems, we propose a novel Graph",
          "rally capture and preserve the topological\ninformation of the": "residual learning mechanism made the restoration effect more"
        },
        {
          "Reversed:": "Spectral Diffusion Network (GSDNet) to strictly restrict\nthe",
          "rally capture and preserve the topological\ninformation of the": "accurate.\nIn addition, some researchers have proposed deep"
        },
        {
          "Reversed:": "diffusion of Gaussian noise to the spectral space of\nthe ad-",
          "rally capture and preserve the topological\ninformation of the": "learning methods based on cross-modal\nrestoration strate-"
        },
        {
          "Reversed:": "jacency matrix.\nSpecifically, we perform eigendecomposi-",
          "rally capture and preserve the topological\ninformation of the": "gies, using cycle consistency loss to ensure the matching de-"
        },
        {
          "Reversed:": "tion on the adjacency matrix, decomposing it into eigenvalue",
          "rally capture and preserve the topological\ninformation of the": "gree between the restored modality and the original modality"
        },
        {
          "Reversed:": "matrix and eigenvector matrix and adding Gaussian noise to",
          "rally capture and preserve the topological\ninformation of the": "[Zhao et al., 2021]. Other studies use graph neural networks"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "ple, Lian et al.\n[Lian et al., 2023] introduced a graph neural",
          "minimize the gap between the model estimated score func-": "tion and the true score function through the score matching"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "network framework and combined the relationship between",
          "minimize the gap between the model estimated score func-": "loss function as follows:"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "nodes and edges to enhance the correlation between modali-",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "2]\nLs = Et∼U (0,T )[EXt|X0 [∥s(X(t), t; θ) − ∇Xt log pt(Xt|X0)∥2"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "ties.",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "(3)"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "where U(0, T ) is a uniform distribution over\n[0, T ]. Given"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "2.2\nScore-based Generative Models",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "a well-trained scoring network sθ∗ , we can generate realistic"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "Score-based generative models (SGMs) estimate the proba-",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "data by solving the learned reverse-time SDE as follows:"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "bility distribution of data by parameterizing the score function",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "[Song and Ermon, 2019], [Song and Ermon, 2020], [Song et",
          "minimize the gap between the model estimated score func-": "d ˆXt = (f ( ˆXt, t) − σ2\nt sθ∗ ( ˆXt))d¯t + σtd ¯Bt"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "(4)"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "al., ]. Specifically, SGMs model\nthe scoring network s(x; θ)",
          "minimize the gap between the model estimated score func-": "ˆ"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "t ∈ [0, 1]\nX1 ∼ π,"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "through learnable parameters θ,\nthereby training the model",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "Unlike\nlikelihood-based genera-\nto estimate ∇x log p(x).",
          "minimize the gap between the model estimated score func-": "where π is the prior information of the data."
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "tive models (e.g.,\nregularized flows [Kingma and Dhariwal,",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "3.2\nScore-based Graph Generative Models"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "2018]), score-based generative models do not require regular-",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "ization of the generation process. Specifically,\nin likelihood-",
          "minimize the gap between the model estimated score func-": "In the graph generation model, given a graph G(X, A) ∈"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "based methods, model\ntraining usually relies on maximiz-",
          "minimize the gap between the model estimated score func-": "Rn×d × Rn×n with n nodes, where X ∈ Rn×d is the feature"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "ing the likelihood function, which means that regularization",
          "minimize the gap between the model estimated score func-": "vectors of each node with dimension d , and A ∈ Rn×n is"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "terms need to be introduced to prevent overfitting and ensure",
          "minimize the gap between the model estimated score func-": "the connection relationship between nodes. The goal of the"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "model stability.\nIn contrast, SGMs estimate the gradient of",
          "minimize the gap between the model estimated score func-": "graph generation model is to learn the underlying data distri-"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "the data distribution by optimizing the score function. This",
          "minimize the gap between the model estimated score func-": "bution of the graph, A standard Graph Diffusion Score-based"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "approach usually does not require explicit regularization and",
          "minimize the gap between the model estimated score func-": "Model\n[Jo et al., 2022],\n[Luo et al., 2023] gradually gen-"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "reduces the complexity of model training. In addition, SGMs",
          "minimize the gap between the model estimated score func-": "erates a perturbation graph through a diffusion process and"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "only need to focus on estimating the gradient of the data dis-",
          "minimize the gap between the model estimated score func-": "learns the generation process of the graph through a scoring"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "tribution by learning the score function, avoiding the com-",
          "minimize the gap between the model estimated score func-": "network. Specifically, given each graph sample (X, A), a for-"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "plexity of accurately modeling the entire distribution process.",
          "minimize the gap between the model estimated score func-": "ward noising process is obtained through an SDE as follows:"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "(cid:40)"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "dXt = f X (Xt, t)dt + σX,tdBX"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "3\nPreliminary Information",
          "minimize the gap between the model estimated score func-": "(5)"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "dAt = f A(At, t)dt + σA,tdBA"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "3.1\nScore-based Generative Models",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": ", and BX\nthe diffusion coefficient, BA\nwhere σX,t, and σA,t"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "SGMs are efficient generative models that can generate high-",
          "minimize the gap between the model estimated score func-": "t\nt"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "represent the Brownian motion."
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "quality data and model complex data distribution. A typical",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "is a probability density function,\nthe re-\nAssuming that pt"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "SGMs consists of a forward noising and a backward denois-",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "verse denoising process can be established through the re-"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "ing, where the forward noising gradually adds noise to the",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "versed time SDE as follows:"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "real data to transform the data from the real distribution to",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "(cid:40)"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "the noise distribution, and the backward denoising starts from",
          "minimize the gap between the model estimated score func-": "d ¯Xt = (cid:0)f X ( ¯Xt, t) − σ2\nX,t∇X log pt( ¯Xt, ¯At)(cid:1) d¯t + σX,td ¯BX"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "the noise sample and gradually removes the noise using the",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "d ¯At = (cid:0)f A( ¯At, t) − σ2\nA,t∇A log pt( ¯Xt, ¯At)(cid:1) d¯t + σA,td ¯BA"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "score function to restore the real data sample. Given an in-",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "(6)"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "put X ∈ Rd and a complicated data distribution D, a forward",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "BX\nBA\nrepresent\nthe reversed time Brownian\n, and\nwhere\nt\nt"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "noising process can be obtained through a stochastic differ-",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "motion."
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "ential equation (SDE) as follows:",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "Given G0,\nthe joint probability distribution of Xt and At"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "t ∈ [0, 1]\n(1)\nX0 ∼ D, dXt = f (Xt, t)dt + σtdBt,",
          "minimize the gap between the model estimated score func-": "can be simplified to the product of two simpler distributions"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "[Jo et al., 2022], so that\nthe objective function of denoising"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "where σt the diffusion coefficient, B represents the Brownian",
          "minimize the gap between the model estimated score func-": "score matching can be simplified in form as follows:"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "motion.",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "Lθ\ns = Et∼U (0,T )EGt|G∥sθ − ∇ log pt|0(Xt|X0)∥2"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "is a probability density function,\nthe re-\nAssuming that pt",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "(7)"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "verse denoising process can be established through the re-",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "Lϕ\ns = Et∼U (0,T )EGt|G∥sϕ − ∇ log pt|0(At|A0)∥2"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "versed time SDE as follows:",
          "minimize the gap between the model estimated score func-": ""
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "",
          "minimize the gap between the model estimated score func-": "4\nProblem Definition"
        },
        {
          "(GNNs) to solve the modality restoration problem. For exam-": "d ¯Xt = (f ( ¯Xt, t) − σ2\nt ∇ log pt( ¯Xt))d¯t + σtd ¯Bt",
          "minimize the gap between the model estimated score func-": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "usually needs to rely on the existing observed modal\ninfor-": "mation Io = {k|αk = 1}, and complete it by modeling the",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "∇Xm log pt(Xm(t)|XIo (0))"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "correlation between modalities.",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "≈ ∇Xm log Ept(XIo (t)|XIo (0))[pt(Xm(t)|XIo (t))]"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "Our main idea is to recover\nthe missing emotion modal-",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "≈ ∇Xm log pt(Xm(t)|XIo (t))"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "ity Im from its latent distribution space conditioned on the",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "= ∇Xm log pt([Xm(t); XIo (t)])"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "(9)"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "observed modality Io. We use the observed modality Io as",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "∇Λm log pt(Λm(t)|ΛIo (0))"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "a semantic condition to guide the generation of\nthe missing",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "≈ ∇Λm log Ept(ΛIo (t)|ΛIo (0))[pt(Λm(t)|ΛIo (t))]"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "modality, ensuring that\nthe recovered modality data is con-",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "sistent and relevant\nto the real data. Formally, we denote the",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "≈ ∇Λm log pt(Λm(t)|ΛIo (t))"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "data distribution of\nthe missing modality as p(Xm) and the",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "= ∇Λm log pt([Am(t); ΛIo (t)])"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "data distribution of the available modality as p(XIo ). Our ul-",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "is\na\nrandom sample\nfrom\nwhere XIo(t)\nand ΛIo (t)"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "timate goal\nis to sample the missing modality data from the",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "respectively.\npt(XIo (t))|XIo (0)) and pt(ΛIo(t))|ΛIo(0)),"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "conditional distribution p(Xm|XIo ). Inspired by graph com-",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "Eq. 9 is held because:"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "pletion networks [Lian et al., 2023] and diffusion modality",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "generation [Wang et al., 2024], we combine the advantages",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "∇Xm log pt([Xm(t); XIo (t)])"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "of GNNs and diffusion models to simultaneously model\nthe",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "= ∇Xm log pt(Xm(t)|XIo (t)) + ∇Xm log pt(XIo (t))"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "complementary semantic information between modalities and",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "= ∇Xm log pt(Xm(t)|XIo (t))"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "reconstruct high-quality missing modality features.",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "(10)"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "∇Λm log pt([Λm(t); ΛIo (t)])"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "However, directly adding Gaussian noise to the adjacency",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "= ∇Λm log pt(Λm(t)|ΛIo (t)) + ∇Am log pt(ΛIo (t))"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "matrix may seriously destroy the local structure of the graph,",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "= ∇Λm log pt(Λm(t)|ΛIo (t))"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "resulting in an unreasonable generated adjacency matrix. To",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "Finally, we derive the score-matching objective as follows:"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "overcome these problems, we propose to strictly restrict\nthe",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "diffusion of Gaussian noise to the spectral space of\nthe ad-",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "Lθ\nEGt|G∥sθ − ∇ log pt|0(Xm(t)|XIo (0))∥2"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "s = EXIo,Xm ,t∼U (0,T )"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "jacency matrix. On the one hand, by operating the eigenval-",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "Lϕ\nEGt|G∥sϕ − ∇ log pt|0(Λm(t)|ΛIo (0)∥2"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "ues in the spectral space,\nthe direct destruction of\nthe local",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "s = EΛIo ,Λm,t∼U (0,T )"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "(11)"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "structure of the adjacency matrix can be effectively avoided,",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "ensuring that the generated graph still conforms to the global",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "semantics. On the other hand, this constrained diffusion pro-",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "5\nTHE PROPOSED METHOD"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "cess can more naturally capture and preserve the topological",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "5.1\nModality Encoder"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "information of\nthe graph, ensuring that\nthe generated graph",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "Since the original features of text, audio, and video modali-"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "has good connectivity and consistent spectral features.",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "ties usually have significant dimensional differences, directly"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "using the original features of the modalities to recover miss-"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "Specifically, we consider a multi-step diffusion model\nto",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "ing modalities may lead to difficulties in semantic alignment"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "gradually construct the conditional distribution by perturbing",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "or even introduce noise.To ensure that the unimodal sequence"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "Xm and Λm, where Am = UmΛmUm, U are the eigenvec-",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "representations of\nthe three modalities can be mapped to in"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "tors and Λ is the diagonal eigenvalues.\nIn the t-th step,\nthe",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "the same feature space, we input these modalities into a one-"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "conditional transfer distribution of the modal features and the",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "dimensional convolutional layer to achieve feature alignment:"
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "adjacency matrix can be expressed as pt(Xm(t)|XIo (0)) and",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": ""
        },
        {
          "usually needs to rely on the existing observed modal\ninfor-": "pt(Λm(t)|ΛIo (0)) and can be approximated as follows:",
          "score pt(Xm(t)|XIo (0)) and pt(Λm(t)|ΛIo(0)) as follows:": "(12)\nX′\nm = Conv1D (Xm, lm) ∈ RN ×d, m ∈ {t, a, v}"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Λϕ,t": "∇Λtlogpt(Xt,Λt)"
        },
        {
          "Λϕ,t": ""
        },
        {
          "Λϕ,t": "Λ0"
        },
        {
          "Λϕ,t": "Gaussian noise\nPositional Embedding"
        },
        {
          "Λϕ,t": "Figure 2: The framework of GSDNet. Given incomplete input data, GSDNet encodes shallow features through 1D-Conv and combines"
        },
        {
          "Λϕ,t": "position embedding information. In the missing modal graph diffusion network, we sample from the prior noise distribution and add it to the"
        },
        {
          "Λϕ,t": "node features and diagonal eigenvalues, and then solve the inverse time diffusion through the score model to denoise the features to generate"
        },
        {
          "Λϕ,t": "new samples. Finally, the reconstructed features are used as complete data to predict the emotion label."
        },
        {
          "Λϕ,t": ""
        },
        {
          "Λϕ,t": ""
        },
        {
          "Λϕ,t": ""
        },
        {
          "Λϕ,t": "(cid:88)"
        },
        {
          "Λϕ,t": ""
        },
        {
          "Λϕ,t": "Lrec ="
        },
        {
          "Λϕ,t": ""
        },
        {
          "Λϕ,t": "i∈Im"
        },
        {
          "Λϕ,t": ""
        },
        {
          "Λϕ,t": ""
        },
        {
          "Λϕ,t": "network is as follows:"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: The performance of different methods is shown at different missing ratios on the CMU-MOSI and CMU-MOSEI datasets. The",
      "data": [
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": "The values reported in each cell represent the ACC2/F1/ACC7. Bold indicates the best performance."
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": "Datasets"
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": "CMU-MOSI"
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": "CMU-MOSEI"
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        },
        {
          "Table 1: The performance of different methods is shown under different missing modalities on the CMU-MOSI and CMU-MOSEI datasets.": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: The performance of different methods is shown at different missing ratios on the CMU-MOSI and CMU-MOSEI datasets. The",
      "data": [
        {
          "{l, v, a}": "Average",
          "84.2/84.2/51.2": "74.6/72.5/46.8",
          "84.3/84.2/52.4": "73.7/73.5/47.1",
          "85.2/85.1/51.5": "74.7/73.7/47.1",
          "85.1/85.1/53.4": "76.0/75.3/48.5",
          "87.3/87.2/54.9": "77.7/77.6/50.7"
        },
        {
          "{l, v, a}": "",
          "84.2/84.2/51.2": "",
          "84.3/84.2/52.4": "",
          "85.2/85.1/51.5": "",
          "85.1/85.1/53.4": "",
          "87.3/87.2/54.9": ""
        },
        {
          "{l, v, a}": "",
          "84.2/84.2/51.2": "",
          "84.3/84.2/52.4": "",
          "85.2/85.1/51.5": "",
          "85.1/85.1/53.4": "",
          "87.3/87.2/54.9": ""
        },
        {
          "{l, v, a}": "Missing Rate",
          "84.2/84.2/51.2": "MCTN",
          "84.3/84.2/52.4": "MMIN",
          "85.2/85.1/51.5": "GCNet",
          "85.1/85.1/53.4": "IMDer",
          "87.3/87.2/54.9": "GSDNet (Ours)"
        },
        {
          "{l, v, a}": "0.0",
          "84.2/84.2/51.2": "81.4/81.5/43.4",
          "84.3/84.2/52.4": "84.6/84.4/44.8",
          "85.2/85.1/51.5": "85.2/85.1/44.9",
          "85.1/85.1/53.4": "85.7/85.6/45.3",
          "87.3/87.2/54.9": "87.7/87.3/46.8"
        },
        {
          "{l, v, a}": "0.1",
          "84.2/84.2/51.2": "78.4/78.5/39.8",
          "84.3/84.2/52.4": "81.8/81.8/41.2",
          "85.2/85.1/51.5": "82.3/82.3/42.1",
          "85.1/85.1/53.4": "84.9/84.8/44.8",
          "87.3/87.2/54.9": "87.1/86.5/46.2"
        },
        {
          "{l, v, a}": "0.2",
          "84.2/84.2/51.2": "75.6/75.7/38.5",
          "84.3/84.2/52.4": "79.0/79.1/38.9",
          "85.2/85.1/51.5": "79.4/79.5/40.0",
          "85.1/85.1/53.4": "83.5/83.4/44.3",
          "87.3/87.2/54.9": "86.4/86.1/45.2"
        },
        {
          "{l, v, a}": "0.3",
          "84.2/84.2/51.2": "71.3/71.2/35.5",
          "84.3/84.2/52.4": "76.1/76.2/36.9",
          "85.2/85.1/51.5": "77.2/77.2/38.2",
          "85.1/85.1/53.4": "81.2/81.0/42.5",
          "87.3/87.2/54.9": "85.2/85.0/44.3"
        },
        {
          "{l, v, a}": "0.4",
          "84.2/84.2/51.2": "68.0/67.6/32.9",
          "84.3/84.2/52.4": "71.7/71.6/34.9",
          "85.2/85.1/51.5": "74.3/74.4/36.6",
          "85.1/85.1/53.4": "78.6/78.5/39.7",
          "87.3/87.2/54.9": "83.3/82.9/42.1"
        },
        {
          "{l, v, a}": "0.5",
          "84.2/84.2/51.2": "65.4/64.8/31.2",
          "84.3/84.2/52.4": "67.2/66.5/32.2",
          "85.2/85.1/51.5": "70.0/69.8/33.9",
          "85.1/85.1/53.4": "76.2/75.9/37.9",
          "87.3/87.2/54.9": "81.2/81.1/40.6"
        },
        {
          "{l, v, a}": "0.6",
          "84.2/84.2/51.2": "63.8/62.5/29.7",
          "84.3/84.2/52.4": "64.9/64.0/29.1",
          "85.2/85.1/51.5": "67.7/66.7/29.8",
          "85.1/85.1/53.4": "74.7/74.0/35.8",
          "87.3/87.2/54.9": "80.1/79.7/38.7"
        },
        {
          "{l, v, a}": "0.7",
          "84.2/84.2/51.2": "61.2/59.0/27.5",
          "84.3/84.2/52.4": "62.8/61.0/28.4",
          "85.2/85.1/51.5": "65.7/65.4/28.1",
          "85.1/85.1/53.4": "71.9/71.2/33.4",
          "87.3/87.2/54.9": "77.6/77.3/35.6"
        },
        {
          "{l, v, a}": "Average",
          "84.2/84.2/51.2": "70.6/70.1/34.8",
          "84.3/84.2/52.4": "73.5/73.1/35.8",
          "85.2/85.1/51.5": "75.2/75.1/36.7",
          "85.1/85.1/53.4": "79.6/79.3/40.5",
          "87.3/87.2/54.9": "83.6/83.2/42.3"
        },
        {
          "{l, v, a}": "0.0",
          "84.2/84.2/51.2": "84.2/84.2/51.2",
          "84.3/84.2/52.4": "84.3/84.2/52.4",
          "85.2/85.1/51.5": "85.2/85.1/51.5",
          "85.1/85.1/53.4": "85.1/85.1/53.4",
          "87.3/87.2/54.9": "87.3/87.2/54.9"
        },
        {
          "{l, v, a}": "0.1",
          "84.2/84.2/51.2": "81.8/81.6/49.8",
          "84.3/84.2/52.4": "81.9/81.3/50.6",
          "85.2/85.1/51.5": "82.3/82.1/51.2",
          "85.1/85.1/53.4": "84.8/84.6/53.1",
          "87.3/87.2/54.9": "86.7/86.5/54.2"
        },
        {
          "{l, v, a}": "0.2",
          "84.2/84.2/51.2": "79.0/78.7/48.6",
          "84.3/84.2/52.4": "79.8/78.8/49.6",
          "85.2/85.1/51.5": "80.3/79.9/50.2",
          "85.1/85.1/53.4": "82.7/82.4/52.0",
          "87.3/87.2/54.9": "85.3/85.1/53.5"
        },
        {
          "{l, v, a}": "0.3",
          "84.2/84.2/51.2": "76.9/76.2/47.4",
          "84.3/84.2/52.4": "77.2/75.5/48.1",
          "85.2/85.1/51.5": "77.5/76.8/49.2",
          "85.1/85.1/53.4": "81.3/80.7/51.3",
          "87.3/87.2/54.9": "83.3/83.0/52.2"
        },
        {
          "{l, v, a}": "0.4",
          "84.2/84.2/51.2": "74.3/74.1/45.6",
          "84.3/84.2/52.4": "75.2/72.6/47.5",
          "85.2/85.1/51.5": "76.0/74.9/48.0",
          "85.1/85.1/53.4": "79.3/78.1/50.0",
          "87.3/87.2/54.9": "81.4/81.2/51.4"
        },
        {
          "{l, v, a}": "0.5",
          "84.2/84.2/51.2": "73.6/72.6/45.1",
          "84.3/84.2/52.4": "73.9/70.7/46.7",
          "85.2/85.1/51.5": "74.9/73.2/46.7",
          "85.1/85.1/53.4": "79.0/77.4/49.2",
          "87.3/87.2/54.9": "80.5/80.1/50.7"
        },
        {
          "{l, v, a}": "0.6",
          "84.2/84.2/51.2": "73.2/71.1/43.8",
          "84.3/84.2/52.4": "73.2/70.3/45.6",
          "85.2/85.1/51.5": "74.1/72.1/45.1",
          "85.1/85.1/53.4": "78.0/75.5/48.5",
          "87.3/87.2/54.9": "79.4/79.1/49.4"
        },
        {
          "{l, v, a}": "0.7",
          "84.2/84.2/51.2": "72.7/70.5/43.6",
          "84.3/84.2/52.4": "73.1/69.5/44.8",
          "85.2/85.1/51.5": "73.2/70.4/44.5",
          "85.1/85.1/53.4": "77.3/74.6/47.6",
          "87.3/87.2/54.9": "78.2/78.1/48.6"
        },
        {
          "{l, v, a}": "Average",
          "84.2/84.2/51.2": "77.0/76.1/46.9",
          "84.3/84.2/52.4": "77.3/75.4/48.2",
          "85.2/85.1/51.5": "77.9/76.8/48.3",
          "85.1/85.1/53.4": "80.9/79.8/50.6",
          "87.3/87.2/54.9": "82.8/82.5/51.9"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: The performance of different methods is shown at different missing ratios on the CMU-MOSI and CMU-MOSEI datasets. The",
      "data": [
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "6.2\nBaselines",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "not only helps to restore the lost\ninformation but also pro-"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "vides additional complementary information for MERC.\nIn"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "We\ncompare\nour\nproposed method GSDNet\nto\nthe\nstate-",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": ""
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "addition, GSDNet has a significant advantage in maintaining"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "of-the-art\nincomplete\nlearning methods,\nincluding MCTN,",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": ""
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "consistency between the restored modality and the original"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "MMIN [Zhao et al., 2021], GCNet [Lian et al., 2023], DiC-",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": ""
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "modality. This distribution consistency ensures that the infor-"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "MoR [Wang et al., 2023a], IMDer [Wang et al., 2023b].",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": ""
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "mation fusion between different modalities is smoother and"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "more accurate, further improving the overall performance of"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "7\nRESULTS AND DISCUSSION",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "the model. Compared with other MERC methods, the perfor-"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "mance degradation of GSDNet decreases as the modal miss-"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "7.1\nComparison with the state-of-the-arts",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": ""
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "ing rate increases.\nIn practical applications, when the modal"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "missing rate is high, most recovery-based models will expe-"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "Tables 1 and 2 lists the quantitative results of\nthe different",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": ""
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "rience significant performance degradation."
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "missing modalities and the random missing ratio on CMU-",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": ""
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "MOSI and CMU-CMSEI datasets, showing the performance",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": ""
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "7.2\nAblation study"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "of different methods under the missing modal. Specifically,",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": ""
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "GSDNet achieved the best\nresults on the two datasets, ver-",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "We conduct ablation experiments on the CMU-MOSI and"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "ifying its\nsuperiority in dealing with modal missing.\nThe",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "CMU-MOSEI datasets. The results in Table 3 show that GS-"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "performance improvement of GSDNet may be attributed to",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "DNet consistently outperforms all variants.\nRemoving the"
        },
        {
          "Average\n77.0/76.1/46.9\n77.3/75.4/48.2": "its ability to explicitly restore the missing modality, which",
          "82.8/82.5/51.9\n77.9/76.8/48.3\n79.9/78.6/49.6\n80.9/79.8/50.6": "frequency diffusion degrades the performance, which high-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3: Visualization of restored modalities. Avail.": "lights the role of frequency diffusion in capturing the distri-",
          "indicates available.": ""
        },
        {
          "Figure 3: Visualization of restored modalities. Avail.": "",
          "indicates available.": ""
        },
        {
          "Figure 3: Visualization of restored modalities. Avail.": "\u0000*\u00006\u0000'\u00001\u0000H\u0000W\u0000\u0003\u0000\u000b\u00002\u0000X\u0000U\u0000V\u0000\f\n\u0000\u0013\u0000\u0011\u0000\u001a\nbution of multimodal data.",
          "indicates available.": ""
        },
        {
          "Figure 3: Visualization of restored modalities. Avail.": "\u0000,\u00000\u0000'\u0000H\u0000U",
          "indicates available.": ""
        },
        {
          "Figure 3: Visualization of restored modalities. Avail.": "\u0000\u0013\u0000\u0011\u0000\u0019\n\u0000'\u0000L\u0000&\u00000\u0000R\u00005",
          "indicates available.": ""
        },
        {
          "Figure 3: Visualization of restored modalities. Avail.": "Table 3: Ablation study of graph spectral diffusion on GSDNet un-\n\u0000\u0013\u0000\u0011\u0000\u0018",
          "indicates available.": ""
        },
        {
          "Figure 3: Visualization of restored modalities. Avail.": "",
          "indicates available.": ""
        },
        {
          "Figure 3: Visualization of restored modalities. Avail.": "\u00000\u00006\u0000(\nder average random missing ratios.\n\u0000\u0013\u0000\u0011\u0000\u0017",
          "indicates available.": ""
        },
        {
          "Figure 3: Visualization of restored modalities. Avail.": "",
          "indicates available.": ""
        },
        {
          "Figure 3: Visualization of restored modalities. Avail.": "\u0000\u0013\u0000\u0011\u0000\u0016\nCMU-MOSI\nCMU-MOSEI",
          "indicates available.": ""
        },
        {
          "Figure 3: Visualization of restored modalities. Avail.": "\u0000\u0013\u0000\u0011\u0000\u0015\nMethods",
          "indicates available.": ""
        },
        {
          "Figure 3: Visualization of restored modalities. Avail.": "F1\nF1\nACC2\nACC7 ACC2\nACC7\n\u0000\u0013\u0000\u0011\u0000\u0014",
          "indicates available.": ""
        },
        {
          "Figure 3: Visualization of restored modalities. Avail.": "\u0000\u0013\u0000\u0011\u0000\u0013\nGSDNet\n75.7\n70.6\n35.3\n78.1\n77.4\n47.4\n\u0000\u0013\u0000\u0011\u0000\u0014\n\u0000\u0013\u0000\u0011\u0000\u0015\n\u0000\u0013\u0000\u0011\u0000\u0016",
          "indicates available.": "\u0000\u0013\u0000\u0011\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u0018\n\u0000\u0013\u0000\u0011\u0000\u0019\n\u0000\u0013\u0000\u0011\u0000\u001a"
        },
        {
          "Figure 3: Visualization of restored modalities. Avail.": "83.6\n83.2\n42.3\n82.8\n82.5\n51.9\nGSDNet w/spectral",
          "indicates available.": "\u00000\u0000L\u0000V\u0000V\u0000L\u0000Q\u0000J\u0000\u0003\u00005\u0000D\u0000W\u0000H"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0018\n\u0000\u0013\u0000\u0011\u0000\u0014"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "\u0000\u0013\u0000\u0011\u0000\u0013\n\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\n\u0000\u0013\u0000\u0011\u0000\u0014\n\u0000\u0013\u0000\u0011\u0000\u0015\n\u0000\u0013\u0000\u0011\u0000\u0016\n\u0000\u0013\u0000\u0011\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u0018\n\u0000\u0013\u0000\u0011\u0000\u0019\n\u0000\u0013\u0000\u0011\u0000\u001a\n\u0000\u0013\u0000\u0011\u0000\u0014\n\u0000\u0013\u0000\u0011\u0000\u0015\n\u0000\u0013\u0000\u0011\u0000\u0016\n\u0000\u0013\u0000\u0011\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u0018\n\u0000\u0013\u0000\u0011\u0000\u0019\n\u0000\u0013\u0000\u0011\u0000\u001a"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "\u00000\u0000L\u0000V\u0000V\u0000L\u0000Q\u0000J\u0000\u0003\u00005\u0000D\u0000W\u0000H\n\u00000\u0000L\u0000V\u0000V\u0000L\u0000Q\u0000J\u0000\u0003\u00005\u0000D\u0000W\u0000H"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "(a) CMU-MOSI\n(b) CMU-MOSEI"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "Figure 4: The comparison of interpolation performance under differ-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "ent missing rates shows the interpolation effects of various methods"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "when dealing with different missing rates."
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "baseline models under different missing rates, our proposed"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "GSDNet always outperforms other baseline methods in the"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "CMU-MOSI and CMU-MOSEI datasets and all missing rate"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "conditions. Specifically, GSDNet not only shows strong in-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "terpolation performance under low missing rates but also has"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "more outstanding performance advantages under high miss-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "ing rates. The experimental results show that speaker depen-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "dency and data distribution consistency play a vital\nrole in"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "data interpolation tasks. Most baseline methods often ignore"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "the synergy of these dependencies, which limits their interpo-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "lation performance when dealing with missing data.\nIn con-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "trast, GSDNet can use the speakers relationship to perform"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "more accurate interpolation while maintaining data distribu-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "tion consistency through the graph diffusion model, so that"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u0015": "GSDNet can always maintain relatively good performance"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "References"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "[Ai et al., 2023] Wei Ai, FuChen Zhang, Tao Meng, YunTao"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "Shou, HongEn Shao,\nand Keqin Li.\nA two-stage mul-"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "timodal emotion recognition model based on graph con-"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "trastive learning. In 2023 IEEE 29th International Confer-"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "ence on Parallel and Distributed Systems (ICPADS), pages"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "397–404. IEEE, 2023."
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "[Ai et al., 2024] Wei Ai, Yuntao Shou, Tao Meng, and Keqin"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "Li. Der-gcn: Dialog and event relation-aware graph con-"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "volutional neural network for multimodal dialog emotion"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "IEEE Transactions on Neural Networks and\nrecognition."
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "Learning Systems, 2024."
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "[Ai et al., 2025] Wei Ai, Fuchen Zhang, Yuntao Shou, Tao"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "Meng, Haowen Chen,\nand Keqin Li.\nRevisiting multi-"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "modal emotion recognition in conversation from the per-"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "the AAAI\nspective of graph spectrum.\nIn Proceedings of"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "Conference on Artificial\nIntelligence, volume 39, pages"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "11418–11426, 2025."
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "[Ding et al., 2023] Yi Ding, Neethu Robinson, Chengxuan"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "Tong, Qiuhao Zeng, and Cuntai Guan.\nLggnet: Learn-"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "ing\nfrom local-global-graph\nrepresentations\nfor\nbrain–"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "IEEE Transactions on Neural Net-\ncomputer\ninterface."
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "works and Learning Systems, 2023."
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "[Fu et al., 2025] Fangze Fu, Wei Ai, Fan Yang, Yuntao Shou,"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "Tao Meng, and Keqin Li.\nSdr-gnn: Spectral domain re-"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "construction graph neural network for\nincomplete mul-"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "timodal\nlearning in conversational\nemotion recognition."
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "Knowledge-Based Systems, 309:112825, 2025."
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "[Hu et al., 2021]\nJingwen Hu, Yuchen Liu,\nJinming Zhao,"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "and Qin Jin. Mmgcn: Multimodal fusion via deep graph"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": ""
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "convolution network for emotion recognition in conversa-"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "the 59th Annual Meeting of\nthe\ntion.\nIn Proceedings of"
        },
        {
          "Foundation Youth Project (Grant No. 2025JJ60420).": "Association for Computational Linguistics and the 11th"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "cessing (Volume 1: Long Papers), pages 5666–5675, 2021."
        },
        {
          "under various missing rates.": "8\nConclusions",
          "International Joint Conference on Natural Language Pro-": "[Huang et al., 2017] Gao Huang, Zhuang Liu, Laurens Van"
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "Der Maaten,\nand Kilian Q Weinberger.\nDensely con-"
        },
        {
          "under various missing rates.": "In this paper, we introduce a novel Graph Spectral Diffusion",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "the\nnected convolutional networks.\nIn Proceedings of"
        },
        {
          "under various missing rates.": "Network (GSDNet), which maps Gaussian noise to the graph",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "IEEE Conference on Computer Vision and Pattern Recog-"
        },
        {
          "under various missing rates.": "spectral space of missing modalities and recover the missing",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "nition, pages 4700–4708, 2017."
        },
        {
          "under various missing rates.": "data according to original distribution. Compared with previ-",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "ous graph diffusion methods, GSDNet only affects the eigen-",
          "International Joint Conference on Natural Language Pro-": "[Jo et al., 2022]\nJaehyeong\nJo,\nSeul\nLee,\nand\nSung\nJu"
        },
        {
          "under various missing rates.": "values of the adjacency matrix instead of destroying the ad-",
          "International Joint Conference on Natural Language Pro-": "Hwang.\nScore-based generative modeling of graphs via"
        },
        {
          "under various missing rates.": "jacency matrix directly, which can maintain the global\ntopo-",
          "International Joint Conference on Natural Language Pro-": "the system of stochastic differential equations.\nIn Inter-"
        },
        {
          "under various missing rates.": "logical information and important spectral features during the",
          "International Joint Conference on Natural Language Pro-": "national conference on machine learning, pages 10362–"
        },
        {
          "under various missing rates.": "diffusion process. Extensive experiments have demonstrated",
          "International Joint Conference on Natural Language Pro-": "10383. PMLR, 2022."
        },
        {
          "under various missing rates.": "that GSDNet\nachieves\nstate-of-the-art\nemotion recognition",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "[Kingma and Dhariwal, 2018] Durk P Kingma and Prafulla"
        },
        {
          "under various missing rates.": "performance in various modality loss scenarios.",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "Dhariwal. Glow: Generative flow with invertible 1x1 con-"
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "Advances\nin Neural\nInformation Processing\nvolutions."
        },
        {
          "under various missing rates.": "Acknowledgments",
          "International Joint Conference on Natural Language Pro-": "Systems, 31, 2018."
        },
        {
          "under various missing rates.": "This work is supported by the National Natural Science Foun-",
          "International Joint Conference on Natural Language Pro-": "[Li et al., 2023] Yong Li, Yuanzhi Wang, and Zhen Cui. De-"
        },
        {
          "under various missing rates.": "dation of China (Grant No. 62372478), the Research Founda-",
          "International Joint Conference on Natural Language Pro-": "coupled multimodal distilling for emotion recognition.\nIn"
        },
        {
          "under various missing rates.": "tion of Education Bureau of Hunan Province of China (Grant",
          "International Joint Conference on Natural Language Pro-": "Proceedings of\nthe IEEE/CVF Conference on Computer"
        },
        {
          "under various missing rates.": "No.\n22B0275), and the Hunan Provincial Natural Science",
          "International Joint Conference on Natural Language Pro-": "Vision and Pattern Recognition, pages 6631–6640, 2023."
        },
        {
          "under various missing rates.": "Foundation Youth Project (Grant No. 2025JJ60420).",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "[Lian et al., 2023] Zheng Lian, Lan Chen, Licai Sun, Bin"
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "Liu,\nand Jianhua Tao.\nGcnet:\nGraph completion net-"
        },
        {
          "under various missing rates.": "References",
          "International Joint Conference on Natural Language Pro-": "work for incomplete multimodal learning in conversation."
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "IEEE Transactions on pattern analysis and machine intel-"
        },
        {
          "under various missing rates.": "[Ai et al., 2023] Wei Ai, FuChen Zhang, Tao Meng, YunTao",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "ligence, 45(7):8419–8432, 2023."
        },
        {
          "under various missing rates.": "Shou, HongEn Shao,\nand Keqin Li.\nA two-stage mul-",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "timodal emotion recognition model based on graph con-",
          "International Joint Conference on Natural Language Pro-": "[Liu et al., 2019] Yinhan\nLiu, Myle Ott,\nNaman Goyal,"
        },
        {
          "under various missing rates.": "trastive learning. In 2023 IEEE 29th International Confer-",
          "International Joint Conference on Natural Language Pro-": "Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike"
        },
        {
          "under various missing rates.": "ence on Parallel and Distributed Systems (ICPADS), pages",
          "International Joint Conference on Natural Language Pro-": "Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:"
        },
        {
          "under various missing rates.": "397–404. IEEE, 2023.",
          "International Joint Conference on Natural Language Pro-": "arXiv\nA robustly optimized bert pretraining approach."
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "preprint arXiv:1907.11692, 2019."
        },
        {
          "under various missing rates.": "[Ai et al., 2024] Wei Ai, Yuntao Shou, Tao Meng, and Keqin",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "Li. Der-gcn: Dialog and event relation-aware graph con-",
          "International Joint Conference on Natural Language Pro-": "[Luo et al., 2023] Tianze\nLuo,\nZhanfeng\nMo,\nand"
        },
        {
          "under various missing rates.": "volutional neural network for multimodal dialog emotion",
          "International Joint Conference on Natural Language Pro-": "Sinno\nJialin Pan.\nFast\ngraph\ngeneration\nvia\nspectral"
        },
        {
          "under various missing rates.": "IEEE Transactions on Neural Networks and\nrecognition.",
          "International Joint Conference on Natural Language Pro-": "IEEE Transactions on Pattern Analysis and\ndiffusion."
        },
        {
          "under various missing rates.": "Learning Systems, 2024.",
          "International Joint Conference on Natural Language Pro-": "Machine Intelligence, 2023."
        },
        {
          "under various missing rates.": "[Ai et al., 2025] Wei Ai, Fuchen Zhang, Yuntao Shou, Tao",
          "International Joint Conference on Natural Language Pro-": "[Meng et al., 2024a] Tao Meng, Yuntao Shou, Wei Ai, Jiayi"
        },
        {
          "under various missing rates.": "Meng, Haowen Chen,\nand Keqin Li.\nRevisiting multi-",
          "International Joint Conference on Natural Language Pro-": "Du, Haiyan Liu, and Keqin Li. A multi-message passing"
        },
        {
          "under various missing rates.": "modal emotion recognition in conversation from the per-",
          "International Joint Conference on Natural Language Pro-": "framework based on heterogeneous graphs\nin conversa-"
        },
        {
          "under various missing rates.": "the AAAI\nspective of graph spectrum.\nIn Proceedings of",
          "International Joint Conference on Natural Language Pro-": "tional emotion recognition. Neurocomputing, 569:127109,"
        },
        {
          "under various missing rates.": "Conference on Artificial\nIntelligence, volume 39, pages",
          "International Joint Conference on Natural Language Pro-": "2024."
        },
        {
          "under various missing rates.": "11418–11426, 2025.",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "[Meng et al., 2024b] Tao Meng, Yuntao Shou, Wei Ai, Nan"
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "Yin, and Keqin Li. Deep imbalanced learning for multi-"
        },
        {
          "under various missing rates.": "[Ding et al., 2023] Yi Ding, Neethu Robinson, Chengxuan",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "IEEE Trans-\nmodal emotion recognition in conversations."
        },
        {
          "under various missing rates.": "Tong, Qiuhao Zeng, and Cuntai Guan.\nLggnet: Learn-",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "actions on Artificial Intelligence, 2024."
        },
        {
          "under various missing rates.": "ing\nfrom local-global-graph\nrepresentations\nfor\nbrain–",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "IEEE Transactions on Neural Net-\ncomputer\ninterface.",
          "International Joint Conference on Natural Language Pro-": "[Meng et al., 2024c] Tao Meng,\nFuchen\nZhang,\nYuntao"
        },
        {
          "under various missing rates.": "works and Learning Systems, 2023.",
          "International Joint Conference on Natural Language Pro-": "Shou, Hongen Shao, Wei Ai, and Keqin Li. Masked graph"
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "learning with recurrent alignment for multimodal emotion"
        },
        {
          "under various missing rates.": "[Fu et al., 2025] Fangze Fu, Wei Ai, Fan Yang, Yuntao Shou,",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "IEEE/ACM Transactions on\nrecognition in conversation."
        },
        {
          "under various missing rates.": "Tao Meng, and Keqin Li.\nSdr-gnn: Spectral domain re-",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "Audio, Speech, and Language Processing, 2024."
        },
        {
          "under various missing rates.": "construction graph neural network for\nincomplete mul-",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "timodal\nlearning in conversational\nemotion recognition.",
          "International Joint Conference on Natural Language Pro-": "[Parthasarathy and Sundaram, 2020] Srinivas\nParthasarathy"
        },
        {
          "under various missing rates.": "Knowledge-Based Systems, 309:112825, 2025.",
          "International Joint Conference on Natural Language Pro-": "and Shiva Sundaram. Training strategies to handle miss-"
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "ing modalities for audio-visual expression recognition.\nIn"
        },
        {
          "under various missing rates.": "[Hu et al., 2021]\nJingwen Hu, Yuchen Liu,\nJinming Zhao,",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "Companion Publication of the 2020 International Confer-"
        },
        {
          "under various missing rates.": "and Qin Jin. Mmgcn: Multimodal fusion via deep graph",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "",
          "International Joint Conference on Natural Language Pro-": "ence on Multimodal Interaction, pages 400–404, 2020."
        },
        {
          "under various missing rates.": "convolution network for emotion recognition in conversa-",
          "International Joint Conference on Natural Language Pro-": ""
        },
        {
          "under various missing rates.": "the 59th Annual Meeting of\nthe\ntion.\nIn Proceedings of",
          "International Joint Conference on Natural Language Pro-": "[Pham et al., 2019] Hai\nPham,\nPaul\nPu\nLiang,\nThomas"
        },
        {
          "under various missing rates.": "Association for Computational Linguistics and the 11th",
          "International Joint Conference on Natural Language Pro-": "Manzini, Louis-Philippe Morency, and Barnab´as P´oczos."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Found in translation: Learning robust joint representations": "by cyclic translations between modalities.\nIn Proceed-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Generative modeling by estimating gradients of\nthe data"
        },
        {
          "Found in translation: Learning robust joint representations": "ings of the AAAI conference on artificial intelligence, vol-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "distribution. Advances in Neural Information Processing"
        },
        {
          "Found in translation: Learning robust joint representations": "ume 33, pages 6892–6899, 2019.",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Systems, 32, 2019."
        },
        {
          "Found in translation: Learning robust joint representations": "[Ramesh et al., 2021] Aditya\nRamesh,\nMikhail\nPavlov,",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "[Song and Ermon, 2020] Yang\nSong\nand\nStefano Ermon."
        },
        {
          "Found in translation: Learning robust joint representations": "Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford,",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Improved techniques\nfor\ntraining score-based generative"
        },
        {
          "Found in translation: Learning robust joint representations": "Mark Chen, and Ilya Sutskever.\nZero-shot\ntext-to-image",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "models. Advances in Neural Information Processing Sys-"
        },
        {
          "Found in translation: Learning robust joint representations": "International Conference\non Machine\ngeneration.\nIn",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "tems, 33:12438–12448, 2020."
        },
        {
          "Found in translation: Learning robust joint representations": "Learning, pages 8821–8831. Pmlr, 2021.",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "[Song et al., ] Yang Song, Jascha Sohl-Dickstein, Diederik P"
        },
        {
          "Found in translation: Learning robust joint representations": "[Schneider et al., 2019] Steffen Schneider, Alexei Baevski,",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Kingma, Abhishek Kumar,\nStefano Ermon,\nand Ben"
        },
        {
          "Found in translation: Learning robust joint representations": "Ronan Collobert, and Michael Auli. wav2vec: Unsuper-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Poole. Score-based generative modeling through stochas-"
        },
        {
          "Found in translation: Learning robust joint representations": "vised pre-training for speech recognition.\nIn Proceedings",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "tic differential equations.\nIn International Conference on"
        },
        {
          "Found in translation: Learning robust joint representations": "of the Interspeech, pages 3465–3469, 2019.",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Learning Representations."
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "[Tran et al., 2017] Luan Tran, Xiaoming Liu,\nJiayu Zhou,"
        },
        {
          "Found in translation: Learning robust joint representations": "[Shou et al., 2022] Yuntao Shou, Tao Meng, Wei Ai,\nSi-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "and Rong Jin. Missing modalities imputation via cascaded"
        },
        {
          "Found in translation: Learning robust joint representations": "han Yang, and Keqin Li. Conversational emotion recog-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "the IEEE confer-\nresidual autoencoder.\nIn Proceedings of"
        },
        {
          "Found in translation: Learning robust joint representations": "nition studies based on graph convolutional neural net-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "ence on computer vision and pattern recognition, pages"
        },
        {
          "Found in translation: Learning robust joint representations": "works and a dependent syntactic analysis. Neurocomput-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "1405–1414, 2017."
        },
        {
          "Found in translation: Learning robust joint representations": "ing, 501:629–639, 2022.",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "[Tsai et al., 2019] Yao-Hung\nHubert\nTsai,\nShaojie\nBai,"
        },
        {
          "Found in translation: Learning robust joint representations": "[Shou et al., 2023] Yuntao Shou, Tao Meng, Wei Ai, Nan",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and"
        },
        {
          "Found in translation: Learning robust joint representations": "Yin, and Keqin Li.\nA comprehensive survey on multi-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Ruslan Salakhutdinov.\nMultimodal\ntransformer\nfor un-"
        },
        {
          "Found in translation: Learning robust joint representations": "modal\nconversational\nemotion\nrecognition with\ndeep",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "aligned multimodal\nlanguage sequences.\nIn Proceedings"
        },
        {
          "Found in translation: Learning robust joint representations": "learning. arXiv preprint arXiv:2312.05735, 2023.",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "of\nthe 57th Conference of\nthe Association for Computa-"
        },
        {
          "Found in translation: Learning robust joint representations": "[Shou et al., 2024a] Yuntao Shou, Wei Ai,\nJiayi Du, Tao",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "tional Linguistics, pages 6558–6569, 2019."
        },
        {
          "Found in translation: Learning robust joint representations": "Meng,\nHaiyan\nLiu,\nand Nan Yin.\nEfficient\nlong-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "[Wang et al., 2023a] Yuanzhi Wang, Zhen Cui, and Yong Li."
        },
        {
          "Found in translation: Learning robust joint representations": "distance\nlatent\nrelation-aware graph neural network for",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Distribution-consistent modal\nrecovering for\nincomplete"
        },
        {
          "Found in translation: Learning robust joint representations": "arXiv\nmulti-modal emotion recognition in conversations.",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "multimodal learning.\nIn Proceedings of the IEEE/CVF In-"
        },
        {
          "Found in translation: Learning robust joint representations": "preprint arXiv:2407.00119, 2024.",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "ternational Conference on Computer Vision, pages 22025–"
        },
        {
          "Found in translation: Learning robust joint representations": "[Shou et al., 2024b] Yuntao\nShou,\nXiangyong\nCao,\nand",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "22034, 2023."
        },
        {
          "Found in translation: Learning robust joint representations": "Deyu Meng. Spegcl: Self-supervised graph spectrum con-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "[Wang et al., 2023b] Yuanzhi Wang, Yong Li,\nand Zhen"
        },
        {
          "Found in translation: Learning robust joint representations": "trastive learning without positive samples. arXiv preprint",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Cui.\nIncomplete multimodality-diffused emotion recogni-"
        },
        {
          "Found in translation: Learning robust joint representations": "arXiv:2410.10365, 2024.",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "tion. Advances in Neural Information Processing Systems,"
        },
        {
          "Found in translation: Learning robust joint representations": "[Shou et al., 2024c] Yuntao\nShou, Huan Liu, Xiangyong",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "36:17117–17128, 2023."
        },
        {
          "Found in translation: Learning robust joint representations": "Cao, Deyu Meng, and Bo Dong. A low-rank matching at-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "[Wang et al., 2024] Yuanzhi Wang, Yong Li, and Zhen Cui."
        },
        {
          "Found in translation: Learning robust joint representations": "tention based cross-modal feature fusion method for con-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Incomplete multimodality-diffused emotion recognition."
        },
        {
          "Found in translation: Learning robust joint representations": "versational emotion recognition. IEEE Transactions on Af-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Advances in Neural Information Processing Systems, 36,"
        },
        {
          "Found in translation: Learning robust joint representations": "fective Computing, 2024.",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "2024."
        },
        {
          "Found in translation: Learning robust joint representations": "[Shou et al., 2024d] Yuntao\nShou,\nTao Meng, Wei\nAi,",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "[Zadeh et al., 2016] Amir Zadeh, Rowan Zellers, Eli Pincus,"
        },
        {
          "Found in translation: Learning robust joint representations": "Fuchen Zhang, Nan Yin, and Keqin Li. Adversarial align-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "and Louis-Philippe Morency. Multimodal sentiment\nin-"
        },
        {
          "Found in translation: Learning robust joint representations": "ment and graph fusion via information bottleneck for mul-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "tensity analysis in videos: Facial gestures and verbal mes-"
        },
        {
          "Found in translation: Learning robust joint representations": "timodal emotion recognition in conversations. Information",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "sages.\nIEEE Intelligent Systems, 31(6):82–88, 2016."
        },
        {
          "Found in translation: Learning robust joint representations": "Fusion, 112:102590, 2024.",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "[Zadeh et al., 2018] AmirAli Bagher Zadeh, Paul Pu Liang,"
        },
        {
          "Found in translation: Learning robust joint representations": "[Shou et al., 2024e] Yuntao Shou, Tao Meng, Fuchen Zhang,",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Soujanya\nPoria,\nErik\nCambria,\nand\nLouis-Philippe"
        },
        {
          "Found in translation: Learning robust joint representations": "Nan Yin, and Keqin Li. Revisiting multi-modal emotion",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Morency. Multimodal language analysis in the wild: Cmu-"
        },
        {
          "Found in translation: Learning robust joint representations": "learning with broad state space models and probability-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "mosei dataset and interpretable dynamic fusion graph.\nIn"
        },
        {
          "Found in translation: Learning robust joint representations": "guidance fusion. arXiv preprint arXiv:2404.17858, 2024.",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Proceedings of the 56th Annual Meeting of the Association"
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "for Computational Linguistics (Volume 1: Long Papers),"
        },
        {
          "Found in translation: Learning robust joint representations": "[Shou et al., 2025a] Yuntao Shou, Haozhi Lan, and Xiangy-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "pages 2236–2246, 2018."
        },
        {
          "Found in translation: Learning robust joint representations": "ong Cao. Contrastive graph representation learning with",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "adversarial cross-view reconstruction and information bot-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "[Zhang et al., 2020] Changqing Zhang, Yajie Cui, Zongbo"
        },
        {
          "Found in translation: Learning robust joint representations": "tleneck. Neural Networks, 184:107094, 2025.",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Han,\nJoey Tianyi Zhou, Huazhu Fu,\nand Qinghua Hu."
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "IEEE transactions on\nDeep partial multi-view learning."
        },
        {
          "Found in translation: Learning robust joint representations": "[Shou et al., 2025b] Yuntao Shou, Tao Meng, Wei Ai, and",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "pattern analysis and machine\nintelligence,\n44(5):2402–"
        },
        {
          "Found in translation: Learning robust joint representations": "Keqin Li. Dynamic graph neural ode network for multi-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "2415, 2020."
        },
        {
          "Found in translation: Learning robust joint representations": "modal emotion recognition in conversation.\nIn Proceed-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": ""
        },
        {
          "Found in translation: Learning robust joint representations": "ings of\nthe 31st\nInternational Conference on Computa-",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "[Zhang et al., 2024] Yunhua Zhang, Hazel Doughty,\nand"
        },
        {
          "Found in translation: Learning robust joint representations": "tional Linguistics, pages 256–268, 2025.",
          "[Song and Ermon, 2019] Yang\nSong\nand\nStefano Ermon.": "Ad-\nCees Snoek.\nLearning unseen modality interaction."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "vances\nin Neural\nInformation Processing\nSystems,\n36,": "2024."
        },
        {
          "vances\nin Neural\nInformation Processing\nSystems,\n36,": "[Zhao et al., 2021]\nJinming Zhao, Ruichen Li, and Qin Jin."
        },
        {
          "vances\nin Neural\nInformation Processing\nSystems,\n36,": "Missing modality imagination network for emotion recog-"
        },
        {
          "vances\nin Neural\nInformation Processing\nSystems,\n36,": "nition with uncertain missing modalities.\nIn Proceedings"
        },
        {
          "vances\nin Neural\nInformation Processing\nSystems,\n36,": "of the 59th Annual Meeting of the Association for Compu-"
        },
        {
          "vances\nin Neural\nInformation Processing\nSystems,\n36,": "tational Linguistics and the 11th International Joint Con-"
        },
        {
          "vances\nin Neural\nInformation Processing\nSystems,\n36,": "ference on Natural Language Processing (Volume 1: Long"
        },
        {
          "vances\nin Neural\nInformation Processing\nSystems,\n36,": "Papers), pages 2608–2618, 2021."
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A two-stage multimodal emotion recognition model based on graph contrastive learning",
      "year": "2023",
      "venue": "2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS)"
    },
    {
      "citation_id": "2",
      "title": "Der-gcn: Dialog and event relation-aware graph convolutional neural network for multimodal dialog emotion recognition",
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "3",
      "title": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Lggnet: Learning from local-global-graph representations for braincomputer interface",
      "authors": [
        "Ding"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "5",
      "title": "Sdr-gnn: Spectral domain reconstruction graph neural network for incomplete multimodal learning in conversational emotion recognition. Knowledge-Based Systems",
      "authors": [
        "Fu"
      ],
      "year": "2017",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Kingma and Dhariwal, 2018] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions",
      "year": "2018",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "7",
      "title": "Gcnet: Graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Tianze Luo, Zhanfeng Mo, and Sinno Jialin Pan. Fast graph generation via spectral diffusion",
      "authors": [
        "Liu"
      ],
      "year": "2019",
      "venue": "A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "9",
      "title": "Parthasarathy and Sundaram, 2020] Srinivas Parthasarathy and Shiva Sundaram. Training strategies to handle missing modalities for audio-visual expression recognition",
      "authors": [
        "Meng"
      ],
      "year": "2019",
      "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "10",
      "title": "Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis",
      "authors": [
        "Schneider"
      ],
      "year": "2019",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Efficient longdistance latent relation-aware graph neural network for multi-modal emotion recognition in conversations",
      "authors": [
        "Shou"
      ],
      "year": "2023",
      "venue": "Xiangyong Cao, and Deyu Meng",
      "arxiv": "arXiv:2312.05735"
    },
    {
      "citation_id": "12",
      "title": "A low-rank matching attention based cross-modal feature fusion method for conversational emotion recognition",
      "authors": [
        "Shou"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations",
      "authors": [
        "Shou"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "14",
      "title": "Contrastive graph representation learning with adversarial cross-view reconstruction and information bottleneck",
      "authors": [
        "Shou"
      ],
      "year": "2024",
      "venue": "Neural Networks",
      "arxiv": "arXiv:2404.17858"
    },
    {
      "citation_id": "15",
      "title": "Song and Ermon, 2019] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution",
      "authors": [
        "Shou"
      ],
      "year": "2019",
      "venue": "Proceedings of the 31st International Conference on Computational Linguistics"
    },
    {
      "citation_id": "16",
      "title": "Score-based generative modeling through stochastic differential equations",
      "authors": [
        "Song"
      ],
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "17",
      "title": "Missing modalities imputation via cascaded residual autoencoder",
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "18",
      "title": "Distribution-consistent modal recovering for incomplete multimodal learning",
      "authors": [
        "Tsai"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics"
    },
    {
      "citation_id": "19",
      "title": "Incomplete multimodality-diffused emotion recognition",
      "authors": [
        "Wang"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "Multimodal language analysis in the wild: Cmumosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Wang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Deep partial multi-view learning",
      "authors": [
        "Zhang"
      ],
      "year": "2020",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "22",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    }
  ]
}