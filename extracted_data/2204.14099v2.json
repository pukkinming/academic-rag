{
  "paper_id": "2204.14099v2",
  "title": "Climate And Weather: Inspecting Depression Detection Via Emotion Recognition",
  "published": "2022-04-29T13:44:22Z",
  "authors": [
    "Wen Wu",
    "Mengyue Wu",
    "Kai Yu"
  ],
  "keywords": [
    "depression detection",
    "emotion",
    "transfer learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic depression detection has attracted increasing amount of attention but remains a challenging task. Psychological research suggests that depressive mood is closely related with emotion expression and perception, which motivates the investigation of whether knowledge of emotion recognition can be transferred for depression detection. This paper uses pretrained features extracted from the emotion recognition model for depression detection, further fuses emotion modality with audio and text to form multimodal depression detection. The proposed emotion transfer improves depression detection performance on DAIC-WOZ as well as increases the training stability. The analysis of how the emotion expressed by depressed individuals is further perceived provides clues for further understanding of the relationship between depression and emotion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Major Depressive Disorder (MDD) is a disease of great concern that affects more than 264 million people worldwide  [1] . Psychology studies suggest that depressed mood can directly influence individual's emotion expression and perception  [2] . Cognitive biases and deficits caused by MDD affect emotion regulation ability such as habitually increasing the use of emotion regulation strategies that serve to down-regulate positive emotion and reducing the use of strategies that serve to up-regulate positive emotion  [3] . Such a relation can be analogized as climate and weather: depression is a long-term mood that affects transient emotional expressions.\n\nInspired by the relation between emotion and depression, two related machine learning tasks can be transferred -knowledge from emotion recognition may be lent for depression detection. Depression detection aims at predicting one's mental state (depressed or healthy) from audio or text while emotion recognition outputs discrete emotional classes (e.g., happy, angry, frustrated) or continuous attribute scores (e.g., valence, activation, dominance). Similarities lie between these two tasks such as the audio and text are the main modalities used to predict one's emotion category or depressive state.\n\nThough datasets on emotion and depression are generally small in size due to difficulties associated with data acquisition and costs of annotations, data sparsity is more servere for depression dataset since clinical interview is a sparse scenario. Compared with emotions, short-termed and transitional, depression is a more longitudinal mental state. Emotions are usually annotated at segment-level † Mengyue Wu and Kai Yu are the corresponding authors.\n\n(i.e., one label per sentence) while the depression diagnosis decisions are usually made at session-level (i.e., one label per interview). Given the same size of data, the number of effective samples for depression detection task is smaller.\n\nPrior work  [4]  investigated emotional speech in depression detection but relied on a rather small dataset with continuous affect ratings and depression scores. This paper uses larger emotion datasets for better generalization capability and presents robust depression detection results by using emotion features extracted from a wellpretrained emotion model. Furthermore, this paper aims at investigating the interaction between emotion and depression, looking at how transient emotion expression (weather) reflect longtidutinal depression (climate) and how longtidutinal depression (climate) can influence transient emotion expression (weather). By analyzing the emotional content of the depression data, this paper provides clues for understanding the relationship between depression and emotion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Transfer Emotion To Depression",
      "text": "The overall process of transferring knowledge from emotion recognition to depression detection is illustrated in Fig.  1 . We first introduce the features used in the experiments, followed by our pretrained emotion model and the knowledge transfer approach in detail.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Representation",
      "text": "Spectral based features have consistently been observed to change with a speaker's mental state  [5] . Log Mel filterbank features (FBKs) have been commonly used as audio representation for speech-based emotion recognition  [6] [7] [8]  and depression detection  [9, 10] . In this paper, 40-dimensional (-d) FBKs with a 10 ms frame shift and 25 ms frame length along with first derivative coefficients are used as audio features.\n\nThe use of large unsupervised pretrained language models have achieved excellent performance on a variety of language tasks. Pretrained sentence-level embeddings derived from a large pretrained language model, such as BERT  [11] , have drawn much attention. In this paper, we use the pretrained BERT-base model to encode the transcription of each single utterance into a 768-d vector.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pretraining: Emotion Recognition",
      "text": "The emotion model proposed in  [8]  is adapted for emotion recognition pretraining, which consists of an audio branch and a text branch.  The outputs of its hidden layer are extracted as emotion features and used as the input to a depression detection system. Emotion modality is then fused with audio and text modalities to form multimodal depression detection.\n\nIn the audio branch, a time delay neural network with residual connections (Res-TDNN)  [12]  is used as the encoder to derive the frame-level vectors, which consists of four Res-TDNN layers with layer width 512 and context width [2,+2], {-1,+2},{-3,+3},{-7,+2}. We then utilize a five-head self-attentive layer  [13]  to pool the framelevel vectors across time in the input window. A modified penalty term is used for the five-head self-attentive layer, with three heads set to obtain more \"spiky\" attention weight distributions and the other two set to obtain \"smoother\" distributions  [14] .\n\nThe text branch focuses on capturing meaning from the speech transcriptions. The embeddings for consecutive utterances of context [-3,+3] (three in the past, current, and three in the future) are used as the input since the emotion of each utterance is often strongly related to its context in a spoken dialogue  [15] . In the text branch, a shared fully-connected (FC) layer is used to reduce the dimension of each input sentence embedding, and the resulting vectors are then integrated by another five-head self-attentive layer, whose attentionweight distribution reflects the extent to which sentences in the context affect the current emotion.\n\nDuring pretraining, the output of the audio branch (128-d) and the text branch (320-d) are then concatenated and fed into an FC layer for fusion, followed by the classification layer.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Depression Detection With Emotion Transfer",
      "text": "The parameters of the pretrained emotion model are then fixed and the pretrained model is treated as an emotion feature extractor. The emotion features are obtained at the input to the fusing layer as shown in Fig.  1 . As mentioned in Section 1, emotion recognition is usually conducted at segment-level and the emotion features are thus extracted at segment-level. An additional bi-directional Long Short-Term Memory (bi-LSTM) model is then trained to pool the segment-level features and produce session-level decision of depres-sion diagnosis.\n\nThen, emotion is seen as a separate modality besides audio and text. The system above serves as a unimodal system for emotion modality.\n\nAudio and text modality are trained using the same features as in pretraining the emotion model (FBKs and BERTs), with audio trained through a bi-directional Gate Recurrent Unit (bi-GRU) and text through a bi-LSTM model. As shown in Fig.  1 , three modalities are then fused for multimodal depression detection.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments And Results",
      "text": "We use separate datasets for experiments on emotion recognition and depression detection, described in Section 3.1. Section 3.2 demonstrates the pretrained emotion recognition models with different configurations. Emotion features extracted from different pretrained models are then used for depression detection, results compared in Section 3.3. The best performing emotion features then represent the emotion modality to fuse with audio and text modality in Section 3.4.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "The emotion datsets involved are IEMOCAP  [16]  and CMU-MOSEI  [17] . We experiment on these two datasets for a comprehensive interaction with depression, as they represent different emotion aspects, i.e. categorical label and sentiment analysis.\n\nEmotion features extracted from the emotion recognition model pretrained on these two datasets are compared and the best performing emotion features are selected as the emotion modality for fusion on depression detection.\n\nIEMOCAP: The IEMOCAP corpus is a multimodal dyadic conversational dataset. It contains a total of 5 sessions and 10 different speakers, with a session being a conversation of two exclusive speakers. IEMOCAP provides both discrete categorical based annotations, and continuous attribute based annotations. To be consistent with previous studies, only utterances with ground truth labels belonging to \"angry\", \"happy\", \"excited\", \"sad\", and \"neutral\" were used. The \"excited\" class was merged with \"happy\" to better balance the size of each emotion class.\n\nCMU-MOSEI: CMU-MOSEI is a multimodal sentiment and emotion analysis dataset made up of 23,454 movie review video clips taken from YouTube. Each sample is labeled by human annotators with a sentiment score from -3 (strongly negative) to 3 (strongly positive).\n\nThe downstream depression detection experiment is conducted on a benchmark dataset DAIC-WOZ  [18] .\n\nDAIC-WOZ: DAIC-WOZ dataset is a commonly used dataset within depression detection which encompasses 50 hours of data collected from 189 clinical interviews from a total of 142 patients. This corpus is created from semi-structured clinical interviews. Thirty speakers within the training (28 %) and 12 within the development (34 %) set are classified to have depression. Following prior works, results in our paper are reported on the development subset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pretrained Emotion Recognition",
      "text": "IEMOCAP-based model was trained using multitask learning. A combination of cross entropy and Huber loss was used for classification of categorical labels and regression of continuous labels respectively. Both the weighted accuracy (WA) and unweighted accuracy (UA) are reported for classification. Mean absolute error (MAE) is reported for regression. MOSEI-based model was trained by binary sentiment classification. The model performance was evaluated using binary accuracy (Acc2) and F1 score.\n\nThe use of audio features, text features and their combination were investigated for both datasets and the results are shown in Table 1. The systems were well-trained compared with the SOTA results 1",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Depression Detection Using Extracted Emotion Features",
      "text": "Emotion features were respectively extracted from the pretrained models in Table  1  and fed as the input for depression detection on DAIC-WOZ, corresponding results compared in Table  2 . To reduce the impact of initialization on sparse datasets, experiments were run for 20 different seeds and both the average (AVG) and the best (MAX) are reported along with the standard deviation (STD) across seeds. Overall, features extracted from MOSEI-based models produce better results than those from IEMOCAP-based models. We speculate this relates with the fact that CMU-MOSEI is a larger dataset than IEMOCAP (66 hours vs 12 hours) and contains much more speakers (1000 vs 10), resulting in stronger generalization potential. Further, IEMOCAP is an acted dataset extravagantly conveying emotions while CMU-MOSEI includes excerpts with emotions naturally expressed, exhibiting a similar format to DAIC-WOZ.\n\nMerging audio and text modalities improves the emotion classification results by ∼ 4% for IEMOCAP-based model (see Table 1(a)), but leads to a decrease in depression detection performance. The discrepancies between Table  1  and Table  2  indicates that although emotion information is useful in depression detection, higher emotion classification results do not necessarily correspond to better depression detection performance. Besides, for both IEMO-CAP and MOSEI, features extracted from emotion model pretrained using only audio information (abbr. audio-based emotion features) yields poor performance, which will be further discussed in Section 4.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Fusion",
      "text": "We then treat emotion as an individual modality. The best performing emotion features were selected as the input to the emotion modality, which is the last row in Table  2  extracted from MOSEIbased model. Emotion modality is then fused with audio and text modalities, shown in Table  3 . Again, the experiments were run for 1 SOTA classification accuracy on IEMOCAP is around 77% for fused system  [19] [20] [21] . The results cannot be compared directly due to inconsistent selection of test set. SOTA results on MOSEI vary from 80 to 84%  [22, 23] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mod. F1(Max) F1(Avg) F1(Std)",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion Analysis Of Depression Data",
      "text": "This section analyzes the emotional content of depression data through different modalities. DAIC training set was fed into the trained emotion model in Section 3.2. Outputs were averaged across all input samples. Results predicted by IEMOCAP-and MOSEIbased models are respectively shown in Fig.  2  and Fig.  3 . Fig.  2 (a) illustrates the average classification probability score over four discrete emotion categories, where the use of different modalities yields inconsistent emotional distributions. The dominant emotion is angry for the audio-alone system and sad for the text-alone system. Happy has the second highest score in both models and dominates the fused model. For all modalities, depressed samples have higher score in angry and sad while healthy samples have higher score in neutral and happy.\n\nFig.  2 (b) represents the average attribute emotional assessment of valence (1-negative, 5-positive), activation (1-calm, 5-excited), and dominance (1-weak, 5-strong). Audio-alone system has the lowest valence score and highest dominance score among three models, which indicates that audio information is more prone to negative and aggressive, which aligns with the dominant categorical prediction of angry. Text-alone system has the lowest activation and dominance score, indicating that text information is more prone to calm and weak. The fused model is relatively positive and excited, in line with the categorical prediction of happy. The overall observation against three systems is accordant: depressed samples have higher dominance and activation while lower valence. A possible explanation is that depressed subjects might not reveal their emotions in a direct way -they might disguise and pretend to be fine. \"Smiling Depression\" has recently become a rising topic. Besides, in the interview situation, depressed people could be more nervous and their emotion would be amplified while healthy people tend to be more relaxed and calm. This finding is interesting and may be helpful for future psychology study as well.\n\nAs shown in Fig.  3 , the sentiment of audio-alone system is slightly biased towards positive while that of text-alone system is biased toward negative. Negative sentiment is discovered after fusing two modalities. In general, depressed samples have higher negative score than healthy samples. As mentioned in Section 3.2, audio-based emotion features yield worse performance than textbased ones. Combined with the above observation, it is possible that negative emotion is more effective in detecting depression. This also echoes the psychological finding that depression causes increase in habitual use of emotion regulation strategies that serve to down-regulate positive emotion.\n\nTo conclude, it is affirmative that depressed mood affects how an individual perceives and expresses emotion. However, according to our results, depression is not always perceived as sad and negative. Such findings on one hand emphasizes the complexity in depression behavioural signals, but on the other hand lead to a conclusion that emotion can be quite helpful in predicting one's mental state. As a matter of fact, not all depressed patients outwardly express emotional symptoms such as sadness or hopelessness  [24, 25] . We will elaborate individuals' emotion expression analysis in a case-study format.\n\nCase Study Contradicted emotions from audio and text are sometimes detected from depressed subjects. Two typical depressed samples from DAIC training set are analyzed. Participant 350 was predicted as happy by the audio-alone system and sad by the text-alone system. From the perspective of speech, the participant has a relaxed tone, an uplifted intonation, and laughter from time to time. These traits are strongly correlated with a happy voice and difficult to discriminate from healthy subjects. By contrast, when observing the text modality, the overall sentiment is negative filled with destructive words such as \"introvert\", \"calm down\", \"mistreatment\", \"abuse\", \"annoyed\". Another sample from participant 426 was predicted as angry by the audio-alone system and sad by the text-alone system. This participant's speech most qualified as angry: he expressed less and spoke in a downward tone with short finishing.\n\nAs reported, humans can convey inconsistent emotional messages through different modalities  [26] . We conjecture that emotion expressions carry ample personalized identities, subsequently causing difficulties in clustering into a unified depressive representation. Emotions are transient and polytropic while depressive mood is more long-lasting and stabilized. Whilst most people experience depressive symptoms in their life, it is considered an illness, according to the Diagnostic and Statistical Manual of Mental Disorders (DSM) definition  [27] , when an individual has either a depressed mood or markedly diminished interest or pleasure for longer than a two-week period. Climate can be inferred from accumulated weather observations however one particular single daily temperature does not necessarily align with the overall profile. This echoes our current finding: emotions are helpful in asserting one's mental state nevertheless the relations are not univocal.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces a novel knowledge transfer approach from emotion recognition to depression detection. A robust depression detection method is derived from utilizing emotion features extracted from the pretrained emotion model. In addition, emotion can be seen as an individual modality. We present results fusing emotion with audio and text, elevated at an F1 of 0.87.\n\nDiversed emotional content of depression data indicates that emotion expressed through audio and text are sometimes inconsistent and negative emotion might be more effective in detecting depression. The analysis provides clues for understanding the relationship between emotion and depression, in particular how healthy/depressed subjects express their emotions. The relationship between emotion and depressed mood is, to some extent, similar to the relationship between weather and climate. Emotion expressions are helpful for detecting depression however the relations are complex: depression influences emotions but not simply negative as we expected. Depressed patients can exhibit happy emotions just like there are still sunny days in the rainy season.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: We ﬁrst intro-",
      "page": 1
    },
    {
      "caption": "Figure 1: Process of transferring knowledge from emotion recogni-",
      "page": 2
    },
    {
      "caption": "Figure 1: As mentioned in Section 1, emotion recognition",
      "page": 2
    },
    {
      "caption": "Figure 1: , three modalities",
      "page": 2
    },
    {
      "caption": "Figure 2: and Fig. 3.",
      "page": 3
    },
    {
      "caption": "Figure 2: (a) illustrates the average classiﬁcation probability score",
      "page": 3
    },
    {
      "caption": "Figure 2: (b) represents the average attribute emotional assessment",
      "page": 3
    },
    {
      "caption": "Figure 2: Emotion distribution of depressed data. Model trained on IEMOCAP.",
      "page": 4
    },
    {
      "caption": "Figure 3: , the sentiment of audio-alone system is",
      "page": 4
    },
    {
      "caption": "Figure 3: Binary sentiment prediction of depressed data. Model trained",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "(i.e., one label per sentence) while the depression diagnosis deci-"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "sions are usually made at session-level (i.e., one label per interview)."
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "Given the same size of data, the number of effective samples for de-"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "pression detection task is smaller."
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "Prior work [4] investigated emotional speech in depression de-"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "tection but relied on a rather small dataset with continuous affect rat-"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "ings and depression scores. This paper uses larger emotion datasets"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "for better generalization capability and presents robust depression"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "detection results by using emotion features extracted from a well-"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "pretrained emotion model. Furthermore,\nthis paper aims at\ninvesti-"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "gating the interaction between emotion and depression,\nlooking at"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "how transient emotion expression (weather) reﬂect longtidutinal de-"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "pression (climate) and how longtidutinal depression (climate) can"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "inﬂuence transient emotion expression (weather). By analyzing the"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "emotional content of the depression data,\nthis paper provides clues"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "for understanding the relationship between depression and emotion."
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "2. TRANSFER EMOTION TO DEPRESSION"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "The overall process of transferring knowledge from emotion recog-"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "nition to depression detection is illustrated in Fig. 1. We ﬁrst\nintro-"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "duce the features used in the experiments, followed by our pretrained"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "emotion model and the knowledge transfer approach in detail."
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "2.1. Feature representation"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "Spectral based features have consistently been observed to change"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "with a speaker’s mental state [5]. Log Mel ﬁlterbank features (FBKs)"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "have been commonly used as audio representation for speech-based"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "emotion recognition [6–8] and depression detection [9, 10].\nIn this"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "paper, 40-dimensional (-d) FBKs with a 10 ms frame shift and 25 ms"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "frame length along with ﬁrst derivative coefﬁcients are used as audio"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "features."
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "The use of large unsupervised pretrained language models have"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "achieved excellent performance on a variety of language tasks. Pre-"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "trained sentence-level embeddings derived from a large pretrained"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "language model, such as BERT [11], have drawn much attention. In"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "this paper, we use the pretrained BERT-base model\nto encode the"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "transcription of each single utterance into a 768-d vector."
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "2.2. Pretraining: emotion recognition"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": ""
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "The emotion model proposed in [8] is adapted for emotion recogni-"
        },
        {
          "AI Institute, Shanghai Jiao Tong University, Shanghai, China": "tion pretraining, which consists of an audio branch and a text branch."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio branch\nPretrained": "FBK",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "Emotion Model",
          "sion diagnosis.": "Then, emotion is seen as a separate modality besides audio and"
        },
        {
          "Audio branch\nPretrained": "a(1)",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "text.\nThe system above serves as a unimodal system for emotion"
        },
        {
          "Audio branch\nPretrained": "a(2)\nSelf-\nRes-",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "modality."
        },
        {
          "Audio branch\nPretrained": "attentive\nTDNN",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "…",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "a(T)",
          "sion diagnosis.": "Audio and text modality are trained using the same features as"
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "in pretraining the emotion model\n(FBKs and BERTs), with audio"
        },
        {
          "Audio branch\nPretrained": "Fusing",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "Text branch",
          "sion diagnosis.": "trained through a bi-directional Gate Recurrent Unit (bi-GRU) and"
        },
        {
          "Audio branch\nPretrained": "layer",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "BERT",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "text through a bi-LSTM model. As shown in Fig. 1, three modalities"
        },
        {
          "Audio branch\nPretrained": "b(u-k)\nFC",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "…",
          "sion diagnosis.": "are then fused for multimodal depression detection."
        },
        {
          "Audio branch\nPretrained": "Classification \nSelf-",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "b(u)\nFC",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "layer\nattentive",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "…",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "b(u+k)\nFC",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "3. EXPERIMENTS AND RESULTS"
        },
        {
          "Audio branch\nPretrained": "Prediction",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "Text features\nEmotion features\nAudio features",
          "sion diagnosis.": "We use separate datasets for experiments on emotion recognition and"
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "depression detection, described in Section 3.1. Section 3.2 demon-"
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "strates the pretrained emotion recognition models with different con-"
        },
        {
          "Audio branch\nPretrained": "Bi-LSTM\nBi-LSTM\nBi-GRU",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "128*3\n128*3\n64*2",
          "sion diagnosis.": "ﬁgurations.\nEmotion features extracted from different pretrained"
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "models are then used for depression detection, results compared in"
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "Section 3.3. The best performing emotion features then represent the"
        },
        {
          "Audio branch\nPretrained": "Depression",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "Detection",
          "sion diagnosis.": "emotion modality to fuse with audio and text modality in Section 3.4."
        },
        {
          "Audio branch\nPretrained": "Depressed / Healthy",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "3.1. Datasets"
        },
        {
          "Audio branch\nPretrained": "Fig. 1.\nProcess of\ntransferring knowledge from emotion recogni-",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "tion to depression detection. An emotion recognition model\nis ﬁrst",
          "sion diagnosis.": "The\nemotion\ndatsets\ninvolved\nare\nIEMOCAP\n[16]\nand CMU-"
        },
        {
          "Audio branch\nPretrained": "trained. The outputs of its hidden layer are extracted as emotion fea-",
          "sion diagnosis.": "MOSEI\n[17]. We\nexperiment on these\ntwo datasets\nfor\na\ncom-"
        },
        {
          "Audio branch\nPretrained": "tures and used as the input\nto a depression detection system. Emo-",
          "sion diagnosis.": "prehensive interaction with depression, as they represent different"
        },
        {
          "Audio branch\nPretrained": "tion modality is then fused with audio and text modalities to form",
          "sion diagnosis.": "emotion aspects, i.e. categorical label and sentiment analysis."
        },
        {
          "Audio branch\nPretrained": "multimodal depression detection.",
          "sion diagnosis.": "Emotion features extracted from the emotion recognition model"
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "pretrained on these two datasets are compared and the best perform-"
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "ing emotion features are selected as the emotion modality for fusion"
        },
        {
          "Audio branch\nPretrained": "In the audio branch, a time delay neural network with residual",
          "sion diagnosis.": "on depression detection."
        },
        {
          "Audio branch\nPretrained": "connections (Res-TDNN) [12] is used as the encoder to derive the",
          "sion diagnosis.": "IEMOCAP: The IEMOCAP corpus is a multimodal dyadic con-"
        },
        {
          "Audio branch\nPretrained": "frame-level vectors, which consists of four Res-TDNN layers with",
          "sion diagnosis.": "versational dataset.\nIt contains a total of 5 sessions and 10 different"
        },
        {
          "Audio branch\nPretrained": "layer width 512 and context width [2,+2], {-1,+2},{-3,+3},{-7,+2}.",
          "sion diagnosis.": "speakers, with a session being a conversation of two exclusive speak-"
        },
        {
          "Audio branch\nPretrained": "We then utilize a ﬁve-head self-attentive layer [13] to pool the frame-",
          "sion diagnosis.": "ers. IEMOCAP provides both discrete categorical based annotations,"
        },
        {
          "Audio branch\nPretrained": "level vectors across time in the input window. A modiﬁed penalty",
          "sion diagnosis.": "and continuous attribute based annotations.\nTo be consistent with"
        },
        {
          "Audio branch\nPretrained": "term is used for the ﬁve-head self-attentive layer, with three heads set",
          "sion diagnosis.": "previous studies, only utterances with ground truth labels belonging"
        },
        {
          "Audio branch\nPretrained": "to obtain more “spiky” attention weight distributions and the other",
          "sion diagnosis.": "to “angry”, “happy”, “excited”, “sad”, and “neutral” were used. The"
        },
        {
          "Audio branch\nPretrained": "two set to obtain “smoother” distributions [14].",
          "sion diagnosis.": "“excited” class was merged with “happy” to better balance the size"
        },
        {
          "Audio branch\nPretrained": "The text branch focuses on capturing meaning from the speech",
          "sion diagnosis.": "of each emotion class."
        },
        {
          "Audio branch\nPretrained": "transcriptions. The embeddings for consecutive utterances of context",
          "sion diagnosis.": "CMU-MOSEI: CMU-MOSEI\nis a multimodal\nsentiment and"
        },
        {
          "Audio branch\nPretrained": "[-3,+3] (three in the past, current, and three in the future) are used",
          "sion diagnosis.": "emotion analysis dataset made up of 23,454 movie review video"
        },
        {
          "Audio branch\nPretrained": "as the input since the emotion of each utterance is often strongly re-",
          "sion diagnosis.": "clips taken from YouTube. Each sample is labeled by human annota-"
        },
        {
          "Audio branch\nPretrained": "lated to its context\nin a spoken dialogue [15].\nIn the text branch, a",
          "sion diagnosis.": "tors with a sentiment score from -3 (strongly negative) to 3 (strongly"
        },
        {
          "Audio branch\nPretrained": "shared fully-connected (FC)\nlayer\nis used to reduce the dimension",
          "sion diagnosis.": "positive)."
        },
        {
          "Audio branch\nPretrained": "of each input sentence embedding, and the resulting vectors are then",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "integrated by another ﬁve-head self-attentive layer, whose attention-",
          "sion diagnosis.": "The downstream depression detection experiment is conducted on a"
        },
        {
          "Audio branch\nPretrained": "weight distribution reﬂects the extent to which sentences in the con-",
          "sion diagnosis.": "benchmark dataset DAIC-WOZ [18]."
        },
        {
          "Audio branch\nPretrained": "text affect the current emotion.",
          "sion diagnosis.": "DAIC-WOZ: DAIC-WOZ dataset\nis a commonly used dataset"
        },
        {
          "Audio branch\nPretrained": "During pretraining,\nthe output of the audio branch (128-d) and",
          "sion diagnosis.": "within depression detection which encompasses 50 hours of data col-"
        },
        {
          "Audio branch\nPretrained": "the text branch (320-d) are then concatenated and fed into an FC",
          "sion diagnosis.": "lected from 189 clinical interviews from a total of 142 patients. This"
        },
        {
          "Audio branch\nPretrained": "layer for fusion, followed by the classiﬁcation layer.",
          "sion diagnosis.": "corpus is created from semi-structured clinical\ninterviews.\nThirty"
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "speakers within the training (28 %) and 12 within the development"
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "(34 %) set are classiﬁed to have depression. Following prior works,"
        },
        {
          "Audio branch\nPretrained": "2.3. Depression detection with emotion transfer",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "results in our paper are reported on the development subset."
        },
        {
          "Audio branch\nPretrained": "The parameters of the pretrained emotion model are then ﬁxed and",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "the pretrained model\nis treated as an emotion feature extractor. The",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "",
          "sion diagnosis.": "3.2. Pretrained emotion recognition"
        },
        {
          "Audio branch\nPretrained": "emotion features are obtained at\nthe input\nto the fusing layer as",
          "sion diagnosis.": ""
        },
        {
          "Audio branch\nPretrained": "shown in Fig. 1. As mentioned in Section 1, emotion recognition",
          "sion diagnosis.": "IEMOCAP-based model was\ntrained using multitask learning.\nA"
        },
        {
          "Audio branch\nPretrained": "is usually conducted at segment-level and the emotion features are",
          "sion diagnosis.": "combination of cross entropy and Huber loss was used for classiﬁca-"
        },
        {
          "Audio branch\nPretrained": "thus extracted at segment-level. An additional bi-directional Long",
          "sion diagnosis.": "tion of categorical labels and regression of continuous labels respec-"
        },
        {
          "Audio branch\nPretrained": "Short-Term Memory (bi-LSTM) model\nis then trained to pool\nthe",
          "sion diagnosis.": "tively. Both the weighted accuracy (WA) and unweighted accuracy"
        },
        {
          "Audio branch\nPretrained": "segment-level features and produce session-level decision of depres-",
          "sion diagnosis.": "(UA) are reported for classiﬁcation. Mean absolute error (MAE) is"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: Depression detection results using emotion features ex-",
      "data": [
        {
          "reported for regression. MOSEI-based model was trained by binary": "sentiment classiﬁcation. The model performance was evaluated us-",
          "Mod.": "",
          "F1(MAX)": "",
          "F1(AVG)": "",
          "F1(STD)": ""
        },
        {
          "reported for regression. MOSEI-based model was trained by binary": "",
          "Mod.": "A",
          "F1(MAX)": "0.558",
          "F1(AVG)": "0.529",
          "F1(STD)": "0.018"
        },
        {
          "reported for regression. MOSEI-based model was trained by binary": "ing binary accuracy (Acc2) and F1 score.",
          "Mod.": "T",
          "F1(MAX)": "0.621",
          "F1(AVG)": "0.562",
          "F1(STD)": "0.022"
        },
        {
          "reported for regression. MOSEI-based model was trained by binary": "The use of audio features,\ntext\nfeatures and their combination",
          "Mod.": "A+T",
          "F1(MAX)": "0.600",
          "F1(AVG)": "0.554",
          "F1(STD)": "0.025"
        },
        {
          "reported for regression. MOSEI-based model was trained by binary": "were investigated for both datasets and the results are shown in Ta-",
          "Mod.": "",
          "F1(MAX)": "",
          "F1(AVG)": "",
          "F1(STD)": ""
        },
        {
          "reported for regression. MOSEI-based model was trained by binary": "",
          "Mod.": "A",
          "F1(MAX)": "0.558",
          "F1(AVG)": "0.534",
          "F1(STD)": "0.014"
        },
        {
          "reported for regression. MOSEI-based model was trained by binary": "ble 1. The systems were well-trained compared with the SOTA re-",
          "Mod.": "",
          "F1(MAX)": "",
          "F1(AVG)": "",
          "F1(STD)": ""
        },
        {
          "reported for regression. MOSEI-based model was trained by binary": "",
          "Mod.": "T",
          "F1(MAX)": "0.750",
          "F1(AVG)": "0.718",
          "F1(STD)": "0.018"
        },
        {
          "reported for regression. MOSEI-based model was trained by binary": "sults1.",
          "Mod.": "A+T",
          "F1(MAX)": "0.828",
          "F1(AVG)": "0.771",
          "F1(STD)": "0.027"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Depression detection results using emotion features ex-",
      "data": [
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": "sults1."
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": "IEMOCAP"
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": "Table 1. Emotion recognition results of pretrained models. Mod."
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": "denotes modality present from {(A)udio, (T)ext}."
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        },
        {
          "ble 1. The systems were well-trained compared with the SOTA re-": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Depression detection results using emotion features ex-",
      "data": [
        {
          "tection results on DAIC-WOZ. A pattern can be observed that when": "facing conﬂicted votes, accuracy is guaranteed to follow the emotion"
        },
        {
          "tection results on DAIC-WOZ. A pattern can be observed that when": ""
        },
        {
          "tection results on DAIC-WOZ. A pattern can be observed that when": ""
        },
        {
          "tection results on DAIC-WOZ. A pattern can be observed that when": ""
        },
        {
          "tection results on DAIC-WOZ. A pattern can be observed that when": "Modality"
        },
        {
          "tection results on DAIC-WOZ. A pattern can be observed that when": "Direct Audio"
        },
        {
          "tection results on DAIC-WOZ. A pattern can be observed that when": "Direct Text"
        },
        {
          "tection results on DAIC-WOZ. A pattern can be observed that when": "Emotion"
        },
        {
          "tection results on DAIC-WOZ. A pattern can be observed that when": ""
        },
        {
          "tection results on DAIC-WOZ. A pattern can be observed that when": "Fused"
        },
        {
          "tection results on DAIC-WOZ. A pattern can be observed that when": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "“Multimodal language analysis in the wild: Cmu-mosei dataset"
        },
        {
          "References": "[1]\nSpencer L James et al,\n“A systematic analysis for the global",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "and interpretable dynamic fusion graph,”\nin Proc. ACL, Mel-"
        },
        {
          "References": "burden of disease\nstudy 2017,”\nThe Lancet, vol. 392, no.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "bourne, 2018."
        },
        {
          "References": "10159, pp. 1789–1858, 2018.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "[18] D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast, A. Gainer,"
        },
        {
          "References": "[2] M. Quinn J. Joormann, “Cognitive processes and emotion reg-",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "K. Georgila, J. Gratch, A. Hartholt, M. Lhommet, G. Lucas,"
        },
        {
          "References": "ulation in depression,” Depression and anxiety, vol. 31, 2014.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "S. Marsella, F. Morbini, A. Nazarian, S. Scherer, G. Stra-"
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "tou, A. Suri, D. Traum, R. Wood, Y. Xu, A. Rizzo, and L. P."
        },
        {
          "References": "[3] W. M. Vanderlind, A. R. Baskin-Sommers Y. Millgram, M. S.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "Morency,\n“Simsensei kiosk: A virtual human interviewer for"
        },
        {
          "References": "Clark,\nand J.\nJoormann,\n“Understanding positive\nemotion",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "healthcare decision support,” in Proc. AAMAS, Paris, 2014."
        },
        {
          "References": "deﬁcits in depression: From emotion preferences to emotion",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "regulation,” Clinical Psychology Review, vol. 76, pp. 101826,",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "[19]\nS. Poria, N. Majumder, D. Hazarika, E. Cambria, A. Gelbukh,"
        },
        {
          "References": "2020.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "and A. Hussain,\n“Multimodal sentiment analysis: Addressing"
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "key issues and setting up the baselines,” IEEE Intelligent Sys-"
        },
        {
          "References": "[4] B. Stasak, J. Epps, N. Cummins, and R. Goecke,\n“An inves-",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "tems, vol. 33, no. 6, pp. 17–25, 2018."
        },
        {
          "References": "tigation of emotional speech in depression classiﬁcation,”\nin",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "[20]\nS. Yoon, S. Byun, S. Dey, and K. Jung, “Speech emotion recog-"
        },
        {
          "References": "Proc. Interspeech, San Francisco, 2016.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "nition using multi-hop attention mechanism,” in Proc. ICASSP,"
        },
        {
          "References": "[5] N. Cummins, S. Scherer, J. Krajewski, S. Schnieder, J. Epps,",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "Brighton, 2019."
        },
        {
          "References": "and T. F. Quatieri,\n“A review of depression and suicide risk",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "[21] R. Li, Z. Wu,\nJ.\nJia, Y. Bu, S. Zhao,\nand H. Meng,\n“To-"
        },
        {
          "References": "assessment using speech analysis,” Speech Commun., vol. 71,",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "wards discriminative representation learning for speech emo-"
        },
        {
          "References": "no. C, pp. 10–49, 2015.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "tion recognition,” in Proc. IJCAI, Macao, China, 2019."
        },
        {
          "References": "[6] A. Metallinou, S. Lee, and S. Narayanan, “Decision level com-",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "[22] Y. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L. P. Morency, and"
        },
        {
          "References": "bination of multiple modalities for recognition and analysis of",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "R. Salakhutdinov, “Multimodal transformer for unaligned mul-"
        },
        {
          "References": "emotional expression,” in Proc. ICASSP, Dallas, 2010.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "timodal language sequences,” in Proc. ACL, Florence, 2019."
        },
        {
          "References": "[7] Y. Kim, H. Lee, and E. M. Provost,\n“Deep learning for\nro-",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "[23] Z. Sun, P. K. Sarma, W. A. Sethares, and Y. Liang, “Learning"
        },
        {
          "References": "bust feature generation in audiovisual emotion recognition,” in",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "relationships between text, audio, and video via deep canonical"
        },
        {
          "References": "Proc. ICASSP, Vancouver, 2013.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "correlation for multimodal language analysis,” in Proc. AAAI,"
        },
        {
          "References": "[8] W. Wu, C. Zhang, and P. C. Woodland,\n“Emotion recognition",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "New York, 2020."
        },
        {
          "References": "by fusing time synchronous and time asynchronous represen-",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "[24] A. J. Mitchell, A. Vaze, and S. Rao,\n“Clinical diagnosis of"
        },
        {
          "References": "tations,” in Proc. ICASSP, Toronto, 2021.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "depression in primary care: a meta-analysis,” The Lancet, vol."
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "374, no. 9690, pp. 609–619, 2009."
        },
        {
          "References": "[9] X. Ma, H. Yang, Q. Chen, D. Huang, and Y. Wang,\n“Depau-",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "dionet: An efﬁcient deep model\nfor audio based depression",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "[25]\nI. Schumann, A. Schneider, C. Kantert, B. L¨owe, and K. Linde,"
        },
        {
          "References": "classiﬁcation,” in Proc. AVEC, New York, 2016.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "“Physicians’ attitudes, diagnostic process and barriers regard-"
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "ing depression diagnosis in primary care: a systematic review"
        },
        {
          "References": "[10] E. Rejaibi, D. Kadoch, K. Bentounes, R. Alfred, M. Daoudi,",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "of qualitative studies,” Family Practice, vol. 29, no. 3, pp. 255–"
        },
        {
          "References": "A. Hadid, and A. Othmani,\n“Clinical depression and affect",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "263, 2011."
        },
        {
          "References": "recognition with emoaudionet,”\nArXiv, vol. abs/1911.00310,",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "2019.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "[26] A. Metallinou, S. Lee, and S. Narayanan, “Audio-visual emo-"
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "tion recognition using gaussian mixture models for\nface and"
        },
        {
          "References": "[11]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,\n“BERT:",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "voice,” in Proc. ISM, Berkeley, 2008."
        },
        {
          "References": "Pre-training of deep bidirectional Transformers for\nlanguage",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "understanding,” in Proc. NAACL-HLT, Minneapolis, 2019.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "[27] American Psychiatric Association, Diagnostic and statistical"
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "manual of mental disorders: DSM-5 (5th ed.), American Psy-"
        },
        {
          "References": "[12]\nF. Kreyssig, C. Zhang, and P.C. Woodland, “Improved TDNNs",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": "chiatric Association, Washington, DC, 2013."
        },
        {
          "References": "using deep kernels and frequency dependent Grid-RNNs,”\nin",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "Proc. ICASSP, Calgary, 2018.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "[13] Z. Lin, M. Feng, C.N. dos Santos, Y. Mo, X. Bing, B. Zhou,",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "and Y. Bengio,\n“A structured self-attentive sentence embed-",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "ding,” in Proc. ICLR, Toulon, 2017.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "[14] G. Sun, C. Zhang, and P.C. Woodland,\n“Speaker diarisation",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "using 2D self-attentive combination of embeddings,”\nin Proc.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "ICASSP, Brighton, 2019.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "[15]\nF. Eyben, S. Buchholz, N. Braunschweiler, J. Latorre, V. Wan,",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "M. Gales, and K. Knill,\n“Unsupervised clustering of emotion",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "and voice styles for expressive TTS,” in Proc. ICASSP, Kyoto,",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "2012.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "[16] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "S. Kim, J.N. Chang, S. Lee, and S.S. Narayanan, “IEMOCAP:",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "Lan-\nInteractive emotional dyadic motion capture database,”",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        },
        {
          "References": "guage Resources and Evaluation, vol. 42, pp. 335–359, 2008.",
          "[17] A. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L. P. Morency,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A systematic analysis for the global burden of disease study 2017",
      "authors": [
        "James Spencer"
      ],
      "year": "2018",
      "venue": "The Lancet"
    },
    {
      "citation_id": "2",
      "title": "Cognitive processes and emotion regulation in depression",
      "authors": [
        "Quinn Joormann"
      ],
      "year": "2014",
      "venue": "Depression and anxiety"
    },
    {
      "citation_id": "3",
      "title": "Understanding positive emotion deficits in depression: From emotion preferences to emotion regulation",
      "authors": [
        "W Vanderlind",
        "A Baskin-Sommers",
        "Y Millgram",
        "M Clark",
        "J Joormann"
      ],
      "year": "2020",
      "venue": "Clinical Psychology Review"
    },
    {
      "citation_id": "4",
      "title": "An investigation of emotional speech in depression classification",
      "authors": [
        "B Stasak",
        "J Epps",
        "N Cummins",
        "R Goecke"
      ],
      "year": "2016",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "5",
      "title": "A review of depression and suicide risk assessment using speech analysis",
      "authors": [
        "N Cummins",
        "S Scherer",
        "J Krajewski",
        "S Schnieder",
        "J Epps",
        "T Quatieri"
      ],
      "year": "2015",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "6",
      "title": "Decision level combination of multiple modalities for recognition and analysis of emotional expression",
      "authors": [
        "A Metallinou",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2010",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "7",
      "title": "Deep learning for robust feature generation in audiovisual emotion recognition",
      "authors": [
        "Y Kim",
        "H Lee",
        "E Provost"
      ],
      "year": "2013",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2021",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "9",
      "title": "Depaudionet: An efficient deep model for audio based depression classification",
      "authors": [
        "X Ma",
        "H Yang",
        "Q Chen",
        "D Huang",
        "Y Wang"
      ],
      "year": "2016",
      "venue": "Proc. AVEC"
    },
    {
      "citation_id": "10",
      "title": "Clinical depression and affect recognition with emoaudionet",
      "authors": [
        "E Rejaibi",
        "D Kadoch",
        "K Bentounes",
        "R Alfred",
        "M Daoudi",
        "A Hadid",
        "A Othmani"
      ],
      "year": "2019",
      "venue": "ArXiv"
    },
    {
      "citation_id": "11",
      "title": "BERT: Pre-training of deep bidirectional Transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. NAACL-HLT"
    },
    {
      "citation_id": "12",
      "title": "Improved TDNNs using deep kernels and frequency dependent Grid-RNNs",
      "authors": [
        "F Kreyssig",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "13",
      "title": "A structured self-attentive sentence embedding",
      "authors": [
        "Z Lin",
        "M Feng",
        "C Santos",
        "Y Mo",
        "X Bing",
        "B Zhou",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "14",
      "title": "Speaker diarisation using 2D self-attentive combination of embeddings",
      "authors": [
        "G Sun",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2019",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Unsupervised clustering of emotion and voice styles for expressive TTS",
      "authors": [
        "F Eyben",
        "S Buchholz",
        "N Braunschweiler",
        "J Latorre",
        "V Wan",
        "M Gales",
        "K Knill"
      ],
      "year": "2012",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "16",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "17",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L Morency"
      ],
      "year": "2018",
      "venue": "Proc. ACL, Melbourne"
    },
    {
      "citation_id": "18",
      "title": "Simsensei kiosk: A virtual human interviewer for healthcare decision support",
      "authors": [
        "D Devault",
        "R Artstein",
        "G Benn",
        "T Dey",
        "E Fast",
        "A Gainer",
        "K Georgila",
        "J Gratch",
        "A Hartholt",
        "M Lhommet",
        "G Lucas",
        "S Marsella",
        "F Morbini",
        "A Nazarian",
        "S Scherer",
        "G Stratou",
        "A Suri",
        "D Traum",
        "R Wood",
        "Y Xu",
        "A Rizzo",
        "L Morency"
      ],
      "year": "2014",
      "venue": "Proc. AAMAS"
    },
    {
      "citation_id": "19",
      "title": "Multimodal sentiment analysis: Addressing key issues and setting up the baselines",
      "authors": [
        "S Poria",
        "N Majumder",
        "D Hazarika",
        "E Cambria",
        "A Gelbukh",
        "A Hussain"
      ],
      "year": "2018",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "S Yoon",
        "S Byun",
        "S Dey",
        "K Jung"
      ],
      "year": "2019",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "21",
      "title": "Towards discriminative representation learning for speech emotion recognition",
      "authors": [
        "R Li",
        "Z Wu",
        "J Jia",
        "Y Bu",
        "S Zhao",
        "H Meng"
      ],
      "year": "2019",
      "venue": "Proc. IJCAI"
    },
    {
      "citation_id": "22",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "23",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Z Sun",
        "P Sarma",
        "W Sethares",
        "Y Liang"
      ],
      "year": "2020",
      "venue": "Proc. AAAI"
    },
    {
      "citation_id": "24",
      "title": "Clinical diagnosis of depression in primary care: a meta-analysis",
      "authors": [
        "A Mitchell",
        "A Vaze",
        "S Rao"
      ],
      "year": "2009",
      "venue": "The Lancet"
    },
    {
      "citation_id": "25",
      "title": "Physicians' attitudes, diagnostic process and barriers regarding depression diagnosis in primary care: a systematic review of qualitative studies",
      "authors": [
        "I Schumann",
        "A Schneider",
        "C Kantert",
        "B Löwe",
        "K Linde"
      ],
      "year": "2011",
      "venue": "Family Practice"
    },
    {
      "citation_id": "26",
      "title": "Audio-visual emotion recognition using gaussian mixture models for face and voice",
      "authors": [
        "A Metallinou",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Proc. ISM"
    },
    {
      "citation_id": "27",
      "title": "Diagnostic and statistical manual of mental disorders",
      "year": "2013",
      "venue": "DSM-5"
    }
  ]
}