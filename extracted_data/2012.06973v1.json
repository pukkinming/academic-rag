{
  "paper_id": "2012.06973v1",
  "title": "Deep Learning And Cuda-Computing Method For Facial Landmarks And Emotion Labels Prediction On Thermal Images",
  "published": "2020-12-13T05:55:19Z",
  "authors": [
    "Chirag Kyal"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Introduction:",
      "text": "One of the key research areas in computer vision addressed by a vast number of publications is the processing and understanding of images containing human faces. The most often addressed tasks include face detection, facial landmark localization, face recognition and facial expression analysis. Other, more specialized tasks such as affective computing, the extraction of vital signs from videos or analysis of social interaction usually require one or several of the aforementioned tasks that have to be performed. In our work, we analyze that a large number of tasks for facial image processing in thermal infrared images that are currently solved using specialized rule-based methods or not solved at all can be addressed with modern learning-based approaches. We have used USTC-NVIE database for training of a number of machine learning algorithms for facial landmark localization.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology:",
      "text": "All selected frames of USTC-NVIE database were manually annotated with the 68-point landmark set. This extensive set of annotations using a widely established scheme allows using the database for a substantial number of algorithms, allowing assessment of their performance on thermal infrared data. Figure  1  shows examples of annotated frames while Figure  2  shows the exact localization of the 68 landmark positions in the face. Both the landmarks as well as the connectivity information are stored, allowing selection of landmarks of specific facial areas such as eyes or mouth separately. After landmarking all images, the dataset has been checked for annotation consistency, ensuring that landmark positions correspond to the same facial features in all database images.\n\nDeep learning -the use of multilayer neural networks for machine learning tasks -is the currently dominating research area in image processing. The most commonly used networks in computer vision are various types of convolutional neural networks, a sub-type of neural networks that uses locally connected convolutional layers in addition to (or as replacement for) classical fully connected layers. and as output it yields a transformed version of the image warped into a normalized canonical form together with a first estimate of the facial landmarks as a heatmap and 256 features from the last, fully connected layer of the first stage. After the image has been transformed by the first stage, all subsequent stages have an identical layout. Consequently, their input and output are a canonical image, a landmark heatmap and the feature map. We implemented only one single second stage after the initial stage as further stages drastically increase the required training time while are reported to yield no improvement in detection precision. The network returns the landmark positions and not a full-face model. However, since the goal of the algorithm is the landmark detection and improving fitting accuracy.\n\nCUDA-is a parallel computing (Fig:  4 ) platform and application programming interface (API) model created by Nvidia. It allows software developers and software engineers to use a CUDA-enabled graphics processing unit (GPU) for general purpose processingan approach termed GPGPU (General-Purpose computing on Graphics Processing Units). The CUDA platform is a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements, for the execution of compute kernels. The above Deep Alignment Network is accelerated by using CUDA Computing over PyTorch so that the training, validation and testing part can be completed in less time. In GPU-accelerated applications, the sequential part of the workload runs on the CPUwhich is optimized for single-threaded performancewhile the compute intensive portion of the application runs on thousands of GPU cores in parallel.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experimental Result:",
      "text": "Here, we describe how the algorithms for each task were trained and evaluated. Results of the facial expression analysis evaluation was obtained using leave one-subject-out-cross validation. In this validation type, we removed all images of a given subject from the database, trained the algorithms on all remaining subjects and tested their performance on the subject previously removed from the database. While requiring a large number of evaluation runs, this method was chosen as it gives the best impression of the overall algorithm and database performance due to the maximal possible overlap of training data with the full database while still allowing evaluation on unseen subjects. We did not perform the same type of crossvalidation for the facial landmark detection as the DAN algorithm requires substantially longer for training its neural network than the other used algorithms; training a DAN instance requires about 72 hours on a GeForce 980Ti GPU. Therefore, we randomly picked 270 images from 8 subjects from the database as test set and trained the landmark detection algorithms on the remaining images to perform evaluation within reasonable time. ) has given the least good result as compared to other models with an accuracy of less than 40%, whereas VGG16 and ResNet have given better result with an accuracy of about 60% and 65% respectively. However, the prediction accuracy of the Face landmark annotation in our proposed model (DAN) is above 80% for all the tested images. Therefore, in our experiment DAN has given better result for this problem.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conclusion:",
      "text": "In our work, we have manually annotated the high-resolution thermal face image database for different computer vision tasks and evaluated how different algorithms perform on commonly appearing problems when trained using the database. We have thoroughly described the database's image acquisition procedure and its contents. Afterwards, we can evaluate its suitability for facial landmark detection and facial expression recognition. We were able to show that both tasks can be solved robustly by using learning-based approaches that are trained using the database. Evaluation has shown that the learning-based approaches, several of which have not been used for these tasks in the thermal infrared domain before, clearly outperform previously presented methods. In conclusion we were able to show that using a sufficiently large and well annotated database can be used to train different learning-based algorithms which should be preferred over algorithm-based approaches due to their increased performance.\n\nReference: -M.  Feature tracking is one of fundamental steps in many computer vision algorithms and the KLT (Kanade-LucasTomasi) method has been successfully used for optical flow estimation. Feature tracking is the foundation of several high-level computer vision tasks such as motion estimation, structure from motion, and image registration. Since the early works of Lucas and Kanade and Shi and Tomasi, the Kanade-Lucas-Tomasi (KLT) feature tracker has been used as a de facto standard in handling point features in a sequence of images. From the original formulation a wide variety of extensions has been proposed for better performance. Baker and Matthews summarized and analyzed the KLT variants in a unifying framework. KLT makes use of spatial intensity information to direct the search for the position that yields the best match. It is faster than traditional techniques for examining far fewer potential matches between the images.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Methodology:",
      "text": "The KLT is a local minimizer of the error function e between a template T and a new image I at frame t + 1 given the spatial window W and the parameter p at frame t. (5)\n\nwith the parameter update rule w(x; pt+1) = w(x; pt) • w(x; δp) -1 (6) This is called inverse compositional image alignment and it takes advantage of a single Hessian computation only when a feature is registered.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Objective:",
      "text": "The main objective to use KLT Tracker algorithm is to make all the frontal faces of USTC-NVIE database in a same alignment (Fig:  1 ) so that the DAN (previous assignment) algorithm can be applied on all the spontaneous faces and face landmarks can be detected automatically. Figure  3  deploys the principle of frame by frame feature points tracking.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion:",
      "text": "We have implemented a feature tracking and optical flow detecting algorithm -KLT Tracker. This tracker is used to make all the frames of the USTC-NVIE database in a same alignment so that we can easily detect the feature points on it using DAN algorithm. Figure  6  represents the Flow Chart of the KLT Tracker algorithm.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Thermal Features Extraction From Roi Patches",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Introduction:",
      "text": "There are numerous works have been done in Facial Emotion Recognition (FER) in visible domain. However, visible imagery is incapable of discerning the true human emotions. In other words, people can conceal their true emotions due to circumstantial reasons. Hence the other possible means is to study the thermal signature of the facial regions as a spontaneous feature to detect true emotion. Unlike visible imagery, the pixel intensity of thermal images corresponds to surface temperature and emissivity of the object. The intensity value is directly proportional to the surface temperature of the object (in our case the object is face). The change in the surface temperature of the skin is mainly caused due to blood flow through superficial blood vessels. Limited attempts have been done in the area of Thermal Imagery based Emotion Recognition (TIER).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Motivation:",
      "text": "We obtained the sensitive portions of the face which undergoes prominent changes (increase or decrease of thermal intensity) during the emotional elicitation using the ratio-based method. The chosen regions are forehead, nose, maxillary, left cheek and right cheek. For the sake of normalization, we have resized the cropped images into a consistent size. The choice of size for the respective regions are mentioned below. The BSIF filters are trained accordingly with the cropped images for feature extraction purpose. We have used DAN algorithm (discussed in Assignment 1) to auto detect 68 landmark points over the face. After detecting those feature points, we can extract different ROI regions by using simple distance techniques.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Objective:",
      "text": "The main objective of this assignment is to extract different region of interests such as forehead, nose, left cheek, right cheek, nose and maxillary. These regions will act as features for emotion detection because the temperature if these regions changes with the temperature change in the face. The temperature of human face is directly impacted by our mental expression and emotions. Human may hide their emotions externally but they can't hide their inner facial emotions. Visual Imagery fails to solve this trap, where the Thermal Imaging comes into picture. Different facial regions change their temperature according to the emotion of the human, different emotions have different temperature variation on the face. Those changes in temperature can be easily detected by using a thermal IR camera. So those ROI need to be extracted properly so that different machine learning techniques can be applied on it to classify those emotions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Different Roi Regions:",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion:",
      "text": "Visible imagery is incapable of detecting the true human emotions. People can conceal their true emotions due to circumstantial reasons. Hence the other possible means is to study the thermal signature of the facial regions as a spontaneous feature to detect true emotion. Unlike visible imagery, the pixel intensity of thermal images corresponds to surface temperature and emissivity of the object. The intensity value is directly proportional to the surface temperature of the object (in our case the object is face). The change in the surface temperature of the skin is mainly caused due to blood flow through superficial blood vessels. Hence, by using different deep learning models we can classify these regions to detect the change in temperature as well as change in emotion of human beings. Rich spectral information of thermal images provides a non-invasive way to characterize the skin tissues and thereby improves thermal face recognition accuracy. However, the increased computational complexity is reduced by efficient feature selection method. We amalgamate pixel selection with spectral discrimination. The pixel selection process choses the informative pixels, which improves the computational performance, whereas, covariance similarity encompasses the complete spectral information. We compare the covariance matrices formed from the selected pixels obtained by DAN algorithm (discussed in Assignment 1). A detailed study of the covariance similarity measures has been conducted. The rich spectral information provided by thermal data enhances the discrimination ability, which in turn decreases the inter-personal similarities. We have used covariance measure-based facial emotion recognition strategy, where we compare the utility of common covariance measures. we identify only the important facial points and carry out face emotion recognition task on the reflectance patterns obtained from these points. We exploit the ability of spectroscopy to characterize skin components. Since, the covariance matrix of raw or windowed fiducial point spectra leads to a compact representation of the entire face, the pixel selection process achieves satisfactory recognition result albeit at reduced computational cost.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Methodology:",
      "text": "We present a novel covariance similarity approach of thermal facial emotion recognition and include a complete study of the prevalent covariance similarity measures. We identify the 68 fiducial or landmark points by DAN algorithm and calculate the covariance matrix from the raw fiducial point spectra (raw FPS) (Fig.  1 ). The emotion matching procedure involves calculation of similarity between covariance matrix of the probe and the gallery. A schematic of the workflow is shown in Fig.  2 . ➢ Covariance Similarity Measure Covariance matrix represents the data in a cohesive and compact structure. Covariance matrix is a specific symmetric positive definite matrix, which arises at several application domains such as-human identification, image segmentation, multi-camera tracking etc. We represent each thermal face cube of a specific subject by a covariance matrix, which encodes the spatial as well as spectral details of the data. However, choosing proper measures to compare the similarity between two covariance matrices is a non-trivial task because the covariance spaces form a Riemannian manifold rather than traditional Euclidean space. The similarity measure adopted should ideally consider the manifold geometry into consideration. Assume that we need to compute the similarity between covariance matrices X and Y. The following covariance similarity measures can be employed-",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "• Cholesky Distance (Chol)",
      "text": "This measure is based on Cholesky distance which assumes that every PSD matrix can be written in terms of inner product of lower triangular matrices, such that-X = L1L T 1 and Y = L2L T 2. The Cholesky distance between covariance matrices X and Y can be written as-DCHOL (X, Y) = || L1 -L2 ||F\n\n• Affine Invariant Riemannian Metric (AIRM) Affine Invariant Riemannian Metric (AIRM) between two SPD matrices X and Y are calculated by-",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Introduction:",
      "text": "Manifold learning plays an important role in many applications such as pattern representation and classification. Several nonlinear techniques have been introduced to learn the intrinsic manifold embedded in the ambient space of high dimensionality. However, these nonlinear techniques yield maps that are defined only on the training data points and it remains unclear how to evaluate the maps on novel testing points. To address this problem, locality preserving projection (LPP) was proposed. Since LPP does not make use of class label information, it cannot perform well in classification. Local discriminant embedding (LDE) incorporates the class information into the construction of embedding and derives the embedding for nearest-neighbor classification in a low-dimensional space. Furthermore, both LPP and LDE suffer from singularity of matrix when the number of training samples is much smaller than the dimensionality of each sample. To address this problem, a novel method called discriminant neighborhood embedding (DNE) is proposed. DNE is inspired by an intuition of dynamics theory. Multi-class data points in high-dimensional space are supposed to be pulled or pushed by discriminant neighbors to form an optimum embedding of low dimensionality for classification.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Discriminant Neighborhood Embedding:",
      "text": "Suppose N multi-class data {x1, x2, . . . , xN} are sampled from underlying manifold M embedded in high-dimensional ambient space Rn and any subset of data points in the same class is assumed to lie in a submanifold of M. We seek an embedding characterized by intra-class compactness and inter-class separability. The larger the distance between two points is, the weaker the interaction becomes. Mutual force can be distinguished as intra-class attraction or inter-class repulsion between the pair of data points from the same or different class, respectively. Furthermore, one neighbor for a point is referred to as intra-class neighbor if they belong to the same class; and inter-class neighbor otherwise.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows examples of annotated frames while Figure 2",
      "page": 2
    },
    {
      "caption": "Figure 3: deploys the principle of frame by frame feature points tracking.",
      "page": 5
    },
    {
      "caption": "Figure 6: represents the Flow Chart of the",
      "page": 7
    },
    {
      "caption": "Figure 2: Fig:1 Raw Fiducial Point Spectra (raw FPS)",
      "page": 10
    },
    {
      "caption": "Figure 3: represents an example of",
      "page": 12
    },
    {
      "caption": "Figure 1: Points are pulled and pushed by their neighbors due to the local",
      "page": 13
    },
    {
      "caption": "Figure 1: An inner point is enveloped by the points of the same class and just acted on by local force of intra-class attraction. One",
      "page": 13
    },
    {
      "caption": "Figure 1: , whose k nearest-neighbors include intra and inter-class",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Deep Learning \nTime  Required \nfor \nAccuracy \nModels \ntraining and Testing \n%": "DAN \n72 hrs \n> 80%"
        },
        {
          "Deep Learning \nTime  Required \nfor \nAccuracy \nModels \ntraining and Testing \n%": "VGG16 \n78 hrs \n~ 60%"
        },
        {
          "Deep Learning \nTime  Required \nfor \nAccuracy \nModels \ntraining and Testing \n%": "ResNet \n76 hrs \n~ 65%"
        },
        {
          "Deep Learning \nTime  Required \nfor \nAccuracy \nModels \ntraining and Testing \n%": "Inception Net \n65 hrs \n< 40%"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A global geometric framework for nonlinear dimensionality reduction",
      "authors": [
        "J Tenenbaum",
        "V Silva",
        "J Langford"
      ],
      "year": "2000",
      "venue": "Science"
    },
    {
      "citation_id": "2",
      "title": "Nonlinear dimensionality reduction by locally linear embedding",
      "authors": [
        "S Roweis",
        "L Saul"
      ],
      "year": "2000",
      "venue": "Science"
    },
    {
      "citation_id": "3",
      "title": "Laplacian eigenmaps and spectral techniques for embedding and clustering",
      "authors": [
        "M Belkin",
        "P Niyogi"
      ],
      "year": "2001",
      "venue": "Proceedings of Conference on Advances in Neural Information Processing System"
    },
    {
      "citation_id": "4",
      "title": "Face recognition using Laplacian faces",
      "authors": [
        "X He",
        "S Yan",
        "Y Hu",
        "P Niyogi",
        "H.-J Zhang"
      ],
      "year": "2005",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "5",
      "title": "Local discriminant embedding and its variants",
      "authors": [
        "H.-T Chen",
        "H.-W Chang",
        "T.-L Liu"
      ],
      "year": "2005",
      "venue": "Proceedings of International Conference on Computer Vision and Pattern Recognition"
    }
  ]
}