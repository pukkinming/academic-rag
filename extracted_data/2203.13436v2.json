{
  "paper_id": "2203.13436v2",
  "title": "Frame-Level Prediction Of Facial Expressions, Valence, Arousal And Action Units For Mobile Devices",
  "published": "2022-03-25T03:53:27Z",
  "authors": [
    "Andrey V. Savchenko"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we consider the problem of real-time videobased facial emotion analytics, namely, facial expression recognition, prediction of valence and arousal and detection of action unit points. We propose the novel frame-level emotion recognition algorithm by extracting facial features with the single EfficientNet model pre-trained on Affect-Net. As a result, our approach may be implemented even for video analytics on mobile devices. Experimental results for the large scale Aff-Wild2 database from the third Affective Behavior Analysis in-the-wild (ABAW) Competition demonstrate that our simple model is significantly better when compared to the VggFace baseline. In particular, our method is characterized by 0.15-0.2 higher performance measures for validation sets in uni-task Expression Classification, Valence-Arousal Estimation and Expression Classification. Due to simplicity, our approach may be considered as a new baseline for all four sub-challenges.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective computing, emotional intelligence  [20]  and, in particular, analysis of humans' emotional states based on facial videos, is an essential task for many systems with manmachine interaction  [4] , education, health  [28]  and mobile services  [2, 7, 25] . Many facial analysis tasks, such as face recognition, age and gender prediction, have reached high accuracy appropriate for many practical applications  [1, 23] . However, but the ability to understand human emotions is still far from maturity  [8] . The personal bias and backgrounds increase the uncertainty of emotion perception and contextual information  [4] . As a result, the datasets used to train FER (facial expression recognition) models are not very large and contain a lot of noise and inconsistencies in emotional labels of photos  [19] . The video-based FER is even more complex task, because human emotions may change rapidly, and many frames do not contain enough information to reliably predict facial (macro) expression. Hence, the authors of the datasets are required to provide the labeling at frame level  [22] . Thus, the number of video datasets for in-the-wild affective computing has been very limited.\n\nThe situation has changed with an appearance of the Af-fWild dataset  [13, 30] . It has been recently extended in the AffWild2 database  [14]  with more videos and annotations for the following tasks: (1) frame-level FER; (2) detection of action units (AU), i.e., specific movements of facial muscles from Facial Action Coding System (FACS)  [5] ; and (3) prediction of valence and arousal, i.e., how active or passive, positive or negative is the human behavior.\n\nThough FER has been a topic of major research  [15] , many models learn too many features specific for a concrete dataset, which is not practical for in-the-wild settings  [8] . The development of in-the-wild affect prediction engines has been accelerated by a couple of ABAW (Affective Behavior Analysis in-the-wild) competitions  [10, 16] . The third place in the first and second tasks was achieved by the authors of the paper  [28]  who proposed the multitask learning (MTL) technique for the incomplete labels of these correlated tasks. The multi-modal audiovisual ensemble model  [8]  took the second place, while the winner of these two sub-challenges was a multi-task streaming network  [32] . The latter captures identity-invariant emotional features using an advanced facial embedding. The valencearousal challenge was won by deep ensembles with iterative distillation and pseudo-labeling  [4] .\n\nAs one can notice, most successful previous solutions use MTL  [12, 32]  to boost their performance. As a result, the authors of the third ABAW contest  [9]  decided to inspire researchers studying not only MTL, but also the uni-task models. The baseline uses the deep convolutional neural network (CNN), namely, VGG16, pre-trained on the VG-GFACE dataset to make a decision in all tasks independently  [9] .\n\nIn this paper, we discuss our solution for all four tasks from the ABAW3 challenge. Most participants of such con-tests are mainly focused on improvement of accuracy metrics, so that they usually develop complex ensemble models  [4, 32] . Hence, they cannot be implemented in real-time analysis of affective behavior in mobile or embedded systems. Hence, the main motivation of this paper is to develop a single  [11]  and lightweight model  [24]  that not only achieve high accuracy but may be used in mobile applications  [7] . As a result, we contributed the novel model based on the EfficientNet architecture  [27]  that is much better than the baseline in terms of both size and performance. The weights of our CNN are tuned on external AffectNet dataset  [19] , so the facial embeddings extracted by this neural network do not learn any features that are specific to the Aff-Wild2 dataset. Thus, our method may become a new baseline for future challenges with the ABAW challenges.\n\nThis paper is structured as follows. Section 2 introduces our efficient model and the training procedure. Experimental results for all tasks of ABAW challenge are presented in Section 3. Finally, concluding comments and future work are discussed in Section 4.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Approach",
      "text": "Three frame-level affective behavior analysis tasks  [9]  are considered for an input video {X(t)}, t = 1, 2, ..., T with T frames, namely: It is assumed that the facial regions are preliminarily extracted, so that X(t) contain cropped faces. The supervised learning scenario is considered, where a training set of N reference facial images {X n }, n ∈ {1, ..., N } are given, and the facial expression e n ∈ {1, ..., C EXP R }, C AUdimensional binary vector AU n of action units, valence V n , arousal A n of the n-th reference image are known, though it is possible that some labels are unavailable.\n\nIn this paper, we will use conventional approach based on pre-training of deep CNN using large FER dataset. As we pay special attention to offline recognition on mobile devices  [26] , it is reasonable to use such architectures as MobileNets or EfficientNets  [27] . The presented approach consists of the following steps:\n\n1. Pre-training of a lightweight model on face identification task using very large facial dataset of celebrities  [1] .\n\n2. Fine-tuning the model from item (1) on static photos from external dataset to obtain an emotional CNN  [19] .\n\n3. The outputs of the emotional CNN (embeddings and expression scores) from item (2) are used to extract facial features of each video frame from the AffWild2 dataset [14].\n\n4. These embeddinsg and scores are used to train simple frame-level MLP-based classification/regression models given the training set of each challange.\n\n5. Optional post-processing of frame-level outputs on models from item (4) computed for validation and test sets to make the predictions more smooth.\n\nLet us consider the details of our approach. At first, a large external VGGFace2 facial dataset  [1]  with 9131 subjects is used to pre-train a CNN on face recognition task. The faces cropped by MTCNN (multi-task cascaded neural network)  [31]  detector without any margins were utilized for training, so that most parts of the background, hairs, etc. is not presented. As a result, the learned facial features are more suitable for emotional analysis. We trained the model totally of 8 epochs by the Adam optimizer and SAM (Sharpness-Aware Minimization)  [6] . The models with the highest accuracy on validation set, namely, 92.1%, 94.19% and 95.49% for MobileNet-v1, EfficientNet-B0 and EfficientNet-B2, respectively, were used.\n\nSecond, the resulted CNN is fine-tuned on the training set of 287,651 photos from the AffectNet dataset  [19]  annotated with C = 8 basic expressions (Anger, Contempt, Disgust, Fear, Happiness, Neutral, Sadness and Surprise). It is necessary to emphasize that the annotations of valence and arousal from the AffectNet dataset were not used in the pre-training. The last layer of the network pre-trained on VGGFace2 is replaced by the new head (fully-connected layer with C outputs and softmax activation), so that the penultimate layer with D neurons can be considered as an extractor of facial features. The weighted categorical crossentropy (softmax) loss was optimized  [19] . The new head was trained during 3 epochs with learning rate 0.001. Finally, the whole network is fine-tuned with a learning rate of 0.0001 at the last 5 epochs. The details of this training procedure are available in  [24] . As a result, we fine-tuned three models, namely, MobileNet-v1, EfficientNet-B0 and EfficientNet-B2, that reached accuracy 60.71%, 61.32% and 63.03%, on the validation part of the AffectNet.\n\nThird, such an emotional CNN was used as a feature extractor for frames X(t) and reference images X n . Though the cropped facial images provided by the organizers of the challenge have different (typically, low) resolution, they were resized to 224x224 pixels for the first two models, while the latter CNN requires input images with resolution 300x300. We examine two types of features: (1) facial image embeddings (output of penultimate layer)  [24, 29] ; and (2) scores (predictions of emotional class probabilities at the output of last softmax layer). As a result, D-dimensional embeddings x(t) and x n and C-dimensional scores s(t) and s n are obtained. Three kinds of features have been examined, namely: (1) embeddings only; (2) scores only; and (3) concatenation of embeddings and scores  [21] . According to the rules of the uni-task challenges, the pre-trained model can be pre-trained on any task (e.g., VA estimation, Expression Classification, AU detection, Face Recognition), so that the expression scores returned by our model trained on the AffectNet can be used as facial features to predict Valence/Arousal and AUs. When we refined the model given the ABAW3 dataset, only the annotations available for a concrete challenge have been used to train a classification and regression models.\n\nFourth, we trained a shallow feed-forward neural network, such as multi-class logistic regression or MLP (multilayered perceptron) with one hidden layer for each of three tasks as follows:\n\n1. The output layer for expression recognition task contains C EXP R neurons with softmax activation. The weighted categorical cross-entropy was minimized for the first task. The final solution is taken in favor of facial expression with the maximal predicted probability.\n\n2. Two neurons with tanh activations are used at the last layer to predict valence and arousal. The loss function is computed as 1 -0.5(CCC V + CCC A )  [15] , where CCC V and CCC A are estimates of the Concordance Correlation Coefficient (CCC) for valence and arousal, respectively.\n\n3. Action unit detector contains C AU output units with sigmoid activation. The weighted binary cross-entropy loss is minimized. To predict the final binary vector, the outputs of this model are matched with a fixed threshold. We examine two possibilities, namely, one threshold (0.5) for each action unit or individual threshold for each action unit. In the latter case, the best threshold is chosen from the list of 10 values {0.1, 0.2, ..., 0.9} by maximizing the class-level F1 score for the validation set.\n\nThe model for each task is trained on 20 epochs with early stopping and Adam optimizer (learning rate 0.001). Fig.  1  contains the most general case of the proposed model with three outputs is trained for the multi-task learning challenge. If the uni-task challenge is considered, only one output layer is used. Here, the facial regions are detected in each frame using MTCNN. The emotional features are extracted using our EfficientNet model. These features are fed into MLP to solve one of the tasks or all tasks together in the multi-task learning scenario. If the facial region is not detected in a couple of frames, we perform the bilinear interpolation of the outputs of the model for two frames with detected faces. If face detection fails for several first or last frames of the video, we will simply use predictions for the closest frame with detected face.\n\nFifth, it is possible to smooth the predictions for k ≥ 1 The training script for the presented approach is made publicly available 1 . The CNNs used for feature extraction, namely, MobileNet v1 (TensorFlow's mobilenet 7.h5) and EfficientNets (PyTorch's enet b0 8 best vgaf and enet b2 8), are also available in this repository 2 . Finally, the possibility to use our model for mobile devices is demonstrated. The sample output of the demo Android application is presented in Fig.  2 . It is possible to recognize facial expressions of all subjects in either any photo from the gallery or the video captured from the frontal camera.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Results",
      "text": "In this section, four tasks from the third ABAW challenge are considered. We used the cropped images officially provided by the organizers of this challenge.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Uni-Task Expression Recognition",
      "text": "The first experiment is devoted to the uni-task FER with C EXP R = 8 classes (anger, disgust, fear, happiness, sadness, surprise, neutral and other). The frame files missed in the cropped directory were ignored. As a result, the training and validation sets contains 585,317 and 280,532 files, respectively. Two performance metrics were computed, namely, (1) macro-averaged F1 score P EXP R  [9] ; and (2) top-1 unbalanced accuracy. The ablation study for several feature extractors and classifiers is presented in Table  1 . Here, the absence of prefix \"1 hidden layer\" stands for the neural network without hidden layers. First, embeddings are classified more accurately when compared to emotional scores. Second, though EfficientNet-B2 has 2% greater accuracy than EfficientNet-B0 on the validation part of AffectNet  [24] , the latter model provides much better performance in this challenge. As a result, our best mean F1-score is 0.16 higher than P EXP R of the baseline VG-GFACE provided by organizers of this challenge. However, even we used the weighted cross-entropy as a loss function, the imbalance of the dataset still influences the overall quality. Table  2  demonstrates the F1 scores of our best model for each class. As one can notice, the quality for anger and, especially, fear emotions is very low.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Uni-Task Action Unit Detection",
      "text": "In the second experiment, we examine the uni-task Action Unit Detection problem. The training set contains 1,356,861 images and C AU = 12 action units (AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23, AU24, AU25 and AU26), while 445836 facial frames are included into the validation set. The unweighted average F1-score P AU of our models (Table  3 ) is again up to 0.14 points greater than the baseline performance. This sults when the threshold for each AU is equal to 0.5, and for different thresholds tuned for each AU separately. In the latter case, the following 12 thresholds were automically found using the validation set: 0.8, 0.8, 0.7, 0.5, 0.5, 0.5, 0.6, 0.8, 0.8, 0.8, 0.3, and 0.7. The results are very similar to the first experiment: embeddings are classified more accurately, and EfficientNet-B0 is the best model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Uni-Task Valence-Arousal Prediction",
      "text": "In the third experiment, the uni-task Valence-Arousal Estimation is analyzed. The number of labeled images here is much higher, so that 1,555,919 and 338,755 frames were put into the training and validation sets. The estimates of CCC for valence and arousal together with their mean P V A are shown in Table  4 . As one can notice, EfficientNet-B0 is still the best model, which has twice-higher P V A when compared to the baseline  [9] . In contrast to the previous experiment, the greatest CCC is achieved by very fast regression models for C = 8 emotional scores of the AffectNet dataset. Conventional usage of embeddings leads to 0.02-0.09 lower CCCs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Aggregation Of Frame-Level Predictions",
      "text": "Next, we studied the impact of smoothing on the performance measures for EfficientNet-B0 and the best classifiers from the previous experiment. Five submissions have been prepared for AU, EXPR and VA challenges by using the frame-level predictions without smoothing and box (mean) and median filters with kernel sizes k = 5 and k = 15. The performance measures on the validation and test sets for each challenge are shown in Table  5 . The best results are obtained by using the large slicing window (k = 15 frames). The mean filter is in most cases better than the median filter except the AU challenge. The proposed approach is much more accurate than the baseline on both validation and test sets. For example, our model has 10% greater F1-scores for the test set from the Expression and Action Unit Challenges. The most impressive is the increase of the CCC metric in the Valence-Arousal Challenge where we improved the baseline by more than 20%.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multi-Task Learning",
      "text": "In the last experiment, we trained a complete multi-task model (Fig.  1 ) on the set of 142,225 images and computed the metric P M T L = P EXP R + P V A + P AU using 26,876 validation frames. The results are reported in Table  6 . It is important to notice two differences with previous experiments: (1) the best model here is EfficientNet-B2, and (2) MLP with 1 hidden layer is worse than a very simple logistic regression. In fact, the latter does not use knowledge about multiple tasks and makes predictions for each task independently. During the challenge, we made four submissions by using two CNNs (EfficientNet-B0 and B2) and training the feed-forward neural network classifier on the training set and concatenation of the training and validation sets. Despite the superiority of EfficientNet-B2 on the validation set (Table  6 ), the best performance metric P M T L = 0.809 is obtained for EfficientNet-B0, though the difference with EfficientNet-B2 (P M T L = 0.8083) is not significant. Anyway, our simple model is much more accurate when compared to the baseline. Indeed, its metric P M T L is equal to 0.3 and 0.28 for validation and test set, respectively. Thus, we improved the performance by more than 50% and took the third place in MTL sub-challenge.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have presented the frame-level facial emotion analysis model (Fig.  1 ) based on concatenation of embeddings and scores at the output of EfficientNet. The latter CNN was carefully pre-trained on the VGGFace2  [1]  and AffectNet  [24]  datasets. Its experimental study for the tasks of the third ABAW challenge  [9]  have demonstrated that our technique is much more accurate than the baseline VGGFACE. Moreover, we have implemented an Android mobile application (Fig.  2 ) with publicly available source code to demonstrate the real-time efficiency of our approach and motivate practitioners to implement the facial emotion analytic engines on-device. Our approach is based on a single lightweight neural network, so that it may be not as accurate as ensembles of many CNNs  [4] .\n\nOur best EfficientNet-B0 model is characterized by F1score 0.38 for expression recognition, mean CCC 0.46 for valence and arousal prediction, and F1-score 0.54 for action unit detection. Our approach took the third place in the MTL challenge, fourth places in the Expression and Valence-Arousal Challenges and fifth place in the Action Unit Challenges. In average, there is only one team who took slightly lower average place in all four tasks  [33] . Their transformer-based multimodal solution is the winner in the uni-task Expression Classification and Action Unit Detection, but our method is better in other two challenges. As the proposed model has not been fine-tuned on the Af-fWild2 dataset, we can claim that the facial features extracted by our networks lead to the most robust decisions. Due to simplicity, our approach may be considered as a new baseline for all four sub-challenges.\n\nIn the future, it is necessary to integrate our approach into more complex pipelines. For example, we process frames independently, so that any sequential and attention models can benefit from the usage of our facial emotional features  [3, 17, 18] . Moreover, it is worth studying the combination of our models into a large ensemble with different representations of input videos. Finally, as our current best model for MTL solves each task independently, it is important to properly use the correlation between different tasks in the multi-task learning scenario  [32] .",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed model for the multi-task learning.",
      "page": 2
    },
    {
      "caption": "Figure 2: Sample screen of Android demo application.",
      "page": 3
    },
    {
      "caption": "Figure 1: contains the most general case of the proposed model",
      "page": 3
    },
    {
      "caption": "Figure 2: It is possible to recognize facial ex-",
      "page": 4
    },
    {
      "caption": "Figure 1: ) on the set of 142,225 images and computed",
      "page": 5
    },
    {
      "caption": "Figure 1: ) based on concatenation of",
      "page": 6
    },
    {
      "caption": "Figure 2: ) with publicly available source",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": ""
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "information to reliably predict\nfacial\n(macro) expression."
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "Hence,\nthe authors of\nthe datasets are required to provide"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "the labeling at frame level [22]. Thus, the number of video"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "datasets for in-the-wild affective computing has been very"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "limited."
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "The situation has changed with an appearance of the Af-"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "fWild dataset [13, 30].\nIt has been recently extended in the"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "AffWild2 database [14] with more videos and annotations"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "for the following tasks:\n(1) frame-level FER; (2) detection"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "of action units (AU), i.e., speciﬁc movements of facial mus-"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "cles from Facial Action Coding System (FACS) [5]; and (3)"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "prediction of valence and arousal,\ni.e., how active or pas-"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "sive, positive or negative is the human behavior."
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "Though FER has been a topic of major\nresearch [15],"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "many models\nlearn too many features\nspeciﬁc for a con-"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "crete dataset, which is not practical\nfor\nin-the-wild set-"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "tings [8]. The development of in-the-wild affect prediction"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "engines has been accelerated by a couple of ABAW (Affec-"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "tive Behavior Analysis in-the-wild) competitions [10, 16]."
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "The third place in the ﬁrst and second tasks was achieved"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "by the authors of\nthe paper\n[28] who proposed the multi-"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "task learning (MTL) technique for the incomplete labels of"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": ""
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "these correlated tasks. The multi-modal audiovisual ensem-"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "ble model\n[8]\ntook the second place, while the winner of"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "these two sub-challenges was a multi-task streaming net-"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "work [32]. The latter captures identity-invariant emotional"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "features using an advanced facial embedding. The valence-"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "arousal challenge was won by deep ensembles with iterative"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "distillation and pseudo-labeling [4]."
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "As one can notice, most\nsuccessful previous\nsolutions"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "use MTL [12, 32] to boost\ntheir performance. As a result,"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "the authors of the third ABAW contest [9] decided to inspire"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "researchers studying not only MTL, but also the uni-task"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "models.\nThe baseline uses the deep convolutional neural"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "network (CNN), namely, VGG16, pre-trained on the VG-"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "GFACE dataset\nto make a decision in all\ntasks\nindepen-"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "dently [9]."
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "In this paper, we discuss our solution for all\nfour\ntasks"
        },
        {
          "Laboratory of Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia": "from the ABAW3 challenge. Most participants of such con-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": "Figure 1. Proposed model for the multi-task learning."
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": "it"
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": "MobileNets or EfﬁcientNets [27]. The presented approach"
        },
        {
          "4’56%!7&/,$!": ""
        },
        {
          "4’56%!7&/,$!": "consists of the following steps:"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "devices [26],\nit\nis reasonable to use such architectures as": ""
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "MobileNets or EfﬁcientNets [27]. The presented approach"
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": ""
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "consists of the following steps:"
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "1. Pre-training of a lightweight model on face identiﬁ-"
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "cation task using very large facial dataset of celebri-"
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "ties [1]."
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": ""
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "2. Fine-tuning the model from item (1) on static photos"
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "from external dataset to obtain an emotional CNN [19]."
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": ""
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "3. The outputs of\nthe emotional CNN (embeddings and"
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": ""
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "expression scores)\nfrom item (2) are used to extract"
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": ""
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "facial features of each video frame from the AffWild2"
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": ""
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "dataset [14]."
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": ""
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": ""
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "4. These embeddinsg and scores are used to train simple"
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "frame-level MLP-based classiﬁcation/regression mod-"
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": ""
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "els given the training set of each challange."
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": ""
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "5. Optional\npost-processing\nof\nframe-level\noutputs\non"
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "models from item (4) computed for validation and test"
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": ""
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": "sets to make the predictions more smooth."
        },
        {
          "devices [26],\nit\nis reasonable to use such architectures as": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "with the highest accuracy on validation set, namely, 92.1%,"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "94.19% and 95.49% for MobileNet-v1, EfﬁcientNet-B0 and"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "EfﬁcientNet-B2, respectively, were used."
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "Second,\nthe resulted CNN is ﬁne-tuned on the training"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "set of 287,651 photos from the AffectNet dataset\n[19] an-"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "notated with C = 8 basic expressions (Anger, Contempt,"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "Disgust, Fear, Happiness, Neutral, Sadness and Surprise)."
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "It is necessary to emphasize that the annotations of valence"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "and arousal from the AffectNet dataset were not used in the"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "pre-training.\nThe last\nlayer of\nthe network pre-trained on"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "VGGFace2 is\nreplaced by the new head (fully-connected"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "layer with C outputs and softmax activation),\nso that\nthe"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "penultimate layer with D neurons can be considered as an"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "extractor of facial features. The weighted categorical cross-"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "entropy (softmax) loss was optimized [19]. The new head"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "was trained during 3 epochs with learning rate 0.001.\nFi-"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "nally,\nthe whole network is ﬁne-tuned with a learning rate"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "of 0.0001 at\nthe last 5 epochs. The details of this training"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "procedure are available in [24]. As a result, we ﬁne-tuned"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "three models, namely, MobileNet-v1, EfﬁcientNet-B0 and"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "EfﬁcientNet-B2,\nthat\nreached accuracy 60.71%,\n61.32%"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "and 63.03%, on the validation part of the AffectNet."
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "Third, such an emotional CNN was used as a feature ex-"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "tractor for frames X(t) and reference images Xn. Though"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "the cropped facial\nimages provided by the organizers of"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "the challenge have different (typically, low) resolution, they"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "were resized to 224x224 pixels\nfor\nthe ﬁrst\ntwo models,"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "while the latter CNN requires input images with resolution"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "300x300. We examine two types of features:\n(1) facial im-"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "age embeddings (output of penultimate layer) [24, 29]; and"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "(2) scores (predictions of emotional class probabilities at the"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "output of\nlast softmax layer). As a result, D-dimensional"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "embeddings x(t) and xn and C-dimensional scores s(t) and"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "sn are obtained. Three kinds of features have been exam-"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "ined, namely: (1) embeddings only; (2) scores only; and (3)"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "concatenation of embeddings and scores [21]. According to"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "the rules of\nthe uni-task challenges,\nthe pre-trained model"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "can be pre-trained on any task (e.g., VA estimation, Ex-"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "pression Classiﬁcation, AU detection, Face Recognition),"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "so that the expression scores returned by our model trained"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "on the AffectNet can be used as facial features to predict Va-"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "lence/Arousal and AUs. When we reﬁned the model given"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "the ABAW3 dataset, only the annotations available for a"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "concrete challenge have been used to train a classiﬁcation"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "and regression models."
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "Fourth, we trained a shallow feed-forward neural net-"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "work, such as multi-class logistic regression or MLP (multi-"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "layered perceptron) with one hidden layer for each of three"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "tasks as follows:"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": ""
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "1. The output\nlayer for expression recognition task con-"
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "The\ntains CEXP R neurons with softmax activation."
        },
        {
          "SAM (Sharpness-Aware Minimization)\n[6].\nThe models": "weighted categorical cross-entropy was minimized for"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: demonstrates the F1 scores of our best model",
      "data": [
        {
          "Model": "VGGFACE",
          "Method": "Baseline [9]",
          "F1-score PEXP R": "0.23",
          "Accuracy": "-"
        },
        {
          "Model": "",
          "Method": "embeddings",
          "F1-score PEXP R": "0.285",
          "Accuracy": "0.398"
        },
        {
          "Model": "Our",
          "Method": "embeddings, 1 hidden layer",
          "F1-score PEXP R": "0.338",
          "Accuracy": "0.460"
        },
        {
          "Model": "MobileNet [2]",
          "Method": "scores",
          "F1-score PEXP R": "0.236",
          "Accuracy": "0.435"
        },
        {
          "Model": "",
          "Method": "scores, 1 hidden layer",
          "F1-score PEXP R": "0.286",
          "Accuracy": "0.433"
        },
        {
          "Model": "Our EfﬁcientNet-",
          "Method": "embeddings",
          "F1-score PEXP R": "0.307",
          "Accuracy": "0.428"
        },
        {
          "Model": "B0 [24]",
          "Method": "embeddings, 1 hidden layer",
          "F1-score PEXP R": "0.381",
          "Accuracy": "0.500"
        },
        {
          "Model": "Our EfﬁcientNet-",
          "Method": "embeddings",
          "F1-score PEXP R": "0.305",
          "Accuracy": "0.412"
        },
        {
          "Model": "B2",
          "Method": "embeddings, 1 hidden layer",
          "F1-score PEXP R": "0.317",
          "Accuracy": "0.435"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: demonstrates the F1 scores of our best model",
      "data": [
        {
          "B2\nembeddings, 1 hidden layer": "Table 1. Expression Challenge Results on the Aff-Wild2’s validation set.",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "consecutive frames by using point-wise mean (box) or me-",
          "0.317\n0.435": "Expression\nF1-score"
        },
        {
          "B2\nembeddings, 1 hidden layer": "dian ﬁlter with kernel size k [21].\nIf k is equal\nto 1,\nthe",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "Neutral\n0.609"
        },
        {
          "B2\nembeddings, 1 hidden layer": "frame-level predictions will be used. Otherwise, the slicing",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "Anger\n0.151"
        },
        {
          "B2\nembeddings, 1 hidden layer": "window with size k is processed for every t-th frame,\ni.e.,",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "Disgust\n0.516"
        },
        {
          "B2\nembeddings, 1 hidden layer": "we took the predictions at the output of our MLP classiﬁers",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "Fear\n0.016"
        },
        {
          "B2\nembeddings, 1 hidden layer": "2 , t− k\n2 +1, ..., t−1, t+1, ..., t+ k\n2 −1, t+ k\n2 .",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "Happiness\n0.477"
        },
        {
          "B2\nembeddings, 1 hidden layer": "The ﬁnal decision function for the frame t is computed as a",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "Sadness\n0.461"
        },
        {
          "B2\nembeddings, 1 hidden layer": "point-wise mean or median of these k predictions.",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "Surprise\n0.303"
        },
        {
          "B2\nembeddings, 1 hidden layer": "The training script\nfor\nthe presented approach is made",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "Other\n0.512"
        },
        {
          "B2\nembeddings, 1 hidden layer": "publicly available1.\nThe CNNs used for\nfeature\nextrac-",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "tion, namely, MobileNet v1 (TensorFlow’s mobilenet 7.h5)",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "Table 2. F1 scores for FER with the best EfﬁcientNet-B0 model."
        },
        {
          "B2\nembeddings, 1 hidden layer": "and\nEfﬁcientNets\n(PyTorch’s\nenet b0 8 best vgaf\nand",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "enet b2 8), are also available in this repository2. Finally, the",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "possibility to use our model for mobile devices is demon-",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "several feature extractors and classiﬁers is presented in Ta-"
        },
        {
          "B2\nembeddings, 1 hidden layer": "strated. The sample output of the demo Android application",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "ble 1. Here,\nthe absence of preﬁx “1 hidden layer” stands"
        },
        {
          "B2\nembeddings, 1 hidden layer": "is presented in Fig. 2.\nIt\nis possible to recognize facial ex-",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "for\nthe neural network without hidden layers.\nFirst, em-"
        },
        {
          "B2\nembeddings, 1 hidden layer": "pressions of all subjects in either any photo from the gallery",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "beddings are classiﬁed more accurately when compared to"
        },
        {
          "B2\nembeddings, 1 hidden layer": "or the video captured from the frontal camera.",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "emotional scores. Second,\nthough EfﬁcientNet-B2 has 2%"
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "greater accuracy than EfﬁcientNet-B0 on the validation part"
        },
        {
          "B2\nembeddings, 1 hidden layer": "3. Experimental results",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "of AffectNet\n[24],\nthe latter model provides much better"
        },
        {
          "B2\nembeddings, 1 hidden layer": "In this section,\nfour\ntasks from the third ABAW chal-",
          "0.317\n0.435": "performance in this challenge. As a result, our best mean"
        },
        {
          "B2\nembeddings, 1 hidden layer": "lenge are considered. We used the cropped images ofﬁcially",
          "0.317\n0.435": "F1-score is 0.16 higher\nthe baseline VG-\nthan PEXP R of"
        },
        {
          "B2\nembeddings, 1 hidden layer": "provided by the organizers of this challenge.",
          "0.317\n0.435": "GFACE provided by organizers of this challenge. However,"
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "even we used the weighted cross-entropy as a loss function,"
        },
        {
          "B2\nembeddings, 1 hidden layer": "3.1. Uni-task Expression Recognition",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "the imbalance of the dataset still inﬂuences the overall qual-"
        },
        {
          "B2\nembeddings, 1 hidden layer": "The ﬁrst experiment is devoted to the uni-task FER with",
          "0.317\n0.435": "ity. Table 2 demonstrates the F1 scores of our best model"
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "for each class. As one can notice, the quality for anger and,"
        },
        {
          "B2\nembeddings, 1 hidden layer": "fear, happiness, sad-\nCEXP R = 8 classes (anger, disgust,",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "ness, surprise, neutral and other).\nThe frame ﬁles missed",
          "0.317\n0.435": "especially, fear emotions is very low."
        },
        {
          "B2\nembeddings, 1 hidden layer": "in the cropped directory were ignored.\nAs a result,\nthe",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "training and validation sets contains 585,317 and 280,532",
          "0.317\n0.435": "3.2. Uni-task Action Unit Detection"
        },
        {
          "B2\nembeddings, 1 hidden layer": "ﬁles,\nrespectively.\nTwo performance metrics were com-",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "In the second experiment, we examine the uni-task Ac-"
        },
        {
          "B2\nembeddings, 1 hidden layer": "puted, namely,\n(1) macro-averaged F1 score PEXP R [9];",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "tion Unit Detection problem.\nThe\ntraining set\ncontains"
        },
        {
          "B2\nembeddings, 1 hidden layer": "and (2) top-1 unbalanced accuracy. The ablation study for",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "1,356,861 images and CAU = 12 action units (AU1, AU2,"
        },
        {
          "B2\nembeddings, 1 hidden layer": "1https : / / github . com / HSE - asavchenko / face -",
          "0.317\n0.435": "AU4, AU6, AU7, AU10, AU12, AU15, AU23, AU24, AU25"
        },
        {
          "B2\nembeddings, 1 hidden layer": "emotion - recognition / blob / main / src / abaw _ train .",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "and AU26), while 445836 facial\nframes are included into"
        },
        {
          "B2\nembeddings, 1 hidden layer": "ipynb",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "",
          "0.317\n0.435": "the validation set. The unweighted average F1-score PAU"
        },
        {
          "B2\nembeddings, 1 hidden layer": "2https : / / github . com / HSE - asavchenko / face -",
          "0.317\n0.435": ""
        },
        {
          "B2\nembeddings, 1 hidden layer": "emotion - recognition / tree / main / models / affectnet _",
          "0.317\n0.435": "of our models (Table 3)\nis again up to 0.14 points greater"
        },
        {
          "B2\nembeddings, 1 hidden layer": "emotions",
          "0.317\n0.435": "than the baseline performance. This table contains the re-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 5: The best results",
      "data": [
        {
          "Model": "VGGFACE",
          "Method": "Baseline [9]",
          "F1-score, threshold 0.5": "-",
          "F1-score PAU , different thresholds": "0.39"
        },
        {
          "Model": "",
          "Method": "embeddings",
          "F1-score, threshold 0.5": "0.473",
          "F1-score PAU , different thresholds": "0.524"
        },
        {
          "Model": "Our",
          "Method": "embeddings, 1 hidden layer",
          "F1-score, threshold 0.5": "0.477",
          "F1-score PAU , different thresholds": "0.529"
        },
        {
          "Model": "MobileNet [2]",
          "Method": "scores",
          "F1-score, threshold 0.5": "0.432",
          "F1-score PAU , different thresholds": "0.442"
        },
        {
          "Model": "",
          "Method": "scores, 1 hidden layer",
          "F1-score, threshold 0.5": "0.452",
          "F1-score PAU , different thresholds": "0.487"
        },
        {
          "Model": "Our EfﬁcientNet-",
          "Method": "embeddings",
          "F1-score, threshold 0.5": "0.491",
          "F1-score PAU , different thresholds": "0.518"
        },
        {
          "Model": "B0 [24]",
          "Method": "embeddings, 1 hidden layer",
          "F1-score, threshold 0.5": "0.508",
          "F1-score PAU , different thresholds": "0.537"
        },
        {
          "Model": "Our EfﬁcientNet-",
          "Method": "embeddings",
          "F1-score, threshold 0.5": "0.468",
          "F1-score PAU , different thresholds": "0.503"
        },
        {
          "Model": "B2",
          "Method": "embeddings, 1 hidden layer",
          "F1-score, threshold 0.5": "0.482",
          "F1-score PAU , different thresholds": "0.512"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 5: The best results",
      "data": [
        {
          "embeddings, 1 hidden layer": "",
          "0.482": "",
          "0.512": ""
        },
        {
          "embeddings, 1 hidden layer": "Model",
          "0.482": "CCC V",
          "0.512": "Mean CCC PV A"
        },
        {
          "embeddings, 1 hidden layer": "ResNet-50",
          "0.482": "0.31",
          "0.512": "0.24"
        },
        {
          "embeddings, 1 hidden layer": "Our",
          "0.482": "0.303",
          "0.512": "0.376"
        },
        {
          "embeddings, 1 hidden layer": "MobileNet [2]",
          "0.482": "0.404",
          "0.512": "0.413"
        },
        {
          "embeddings, 1 hidden layer": "Our EfﬁcientNet-",
          "0.482": "0.309",
          "0.512": "0.372"
        },
        {
          "embeddings, 1 hidden layer": "B0 [24]",
          "0.482": "0.429",
          "0.512": "0.463"
        },
        {
          "embeddings, 1 hidden layer": "Our EfﬁcientNet-",
          "0.482": "0.377",
          "0.512": "0.426"
        },
        {
          "embeddings, 1 hidden layer": "B2",
          "0.482": "0.408",
          "0.512": "0.443"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 5: The best results",
      "data": [
        {
          "B2\nscores": "",
          "0.408\n0.477\n0.443": "Table 4. Valence-Arousal Challenge Results on the Aff-Wild2’s validation set."
        },
        {
          "B2\nscores": "sults when the threshold for each AU is equal\nto 0.5, and",
          "0.408\n0.477\n0.443": "frame-level predictions without smoothing and box (mean)"
        },
        {
          "B2\nscores": "for different thresholds tuned for each AU separately. In the",
          "0.408\n0.477\n0.443": "and median ﬁlters with kernel sizes k = 5 and k = 15."
        },
        {
          "B2\nscores": "latter case,\nthe following 12 thresholds were automically",
          "0.408\n0.477\n0.443": "The performance measures on the validation and test sets"
        },
        {
          "B2\nscores": "found using the validation set: 0.8, 0.8, 0.7, 0.5, 0.5, 0.5,",
          "0.408\n0.477\n0.443": "for each challenge are shown in Table 5. The best\nresults"
        },
        {
          "B2\nscores": "0.6, 0.8, 0.8, 0.8, 0.3, and 0.7. The results are very simi-",
          "0.408\n0.477\n0.443": "are obtained by using the large slicing window (k = 15"
        },
        {
          "B2\nscores": "lar to the ﬁrst experiment: embeddings are classiﬁed more",
          "0.408\n0.477\n0.443": "frames). The mean ﬁlter is in most cases better than the me-"
        },
        {
          "B2\nscores": "accurately, and EfﬁcientNet-B0 is the best model.",
          "0.408\n0.477\n0.443": "dian ﬁlter except the AU challenge. The proposed approach"
        },
        {
          "B2\nscores": "",
          "0.408\n0.477\n0.443": "is much more accurate than the baseline on both valida-"
        },
        {
          "B2\nscores": "3.3. Uni-task Valence-Arousal Prediction",
          "0.408\n0.477\n0.443": ""
        },
        {
          "B2\nscores": "",
          "0.408\n0.477\n0.443": "tion and test sets. For example, our model has 10% greater"
        },
        {
          "B2\nscores": "",
          "0.408\n0.477\n0.443": "F1-scores for\nthe test set\nfrom the Expression and Action"
        },
        {
          "B2\nscores": "In the third experiment, the uni-task Valence-Arousal Es-",
          "0.408\n0.477\n0.443": ""
        },
        {
          "B2\nscores": "",
          "0.408\n0.477\n0.443": "Unit Challenges.\nThe most\nimpressive is\nthe increase of"
        },
        {
          "B2\nscores": "timation is analyzed. The number of labeled images here is",
          "0.408\n0.477\n0.443": ""
        },
        {
          "B2\nscores": "",
          "0.408\n0.477\n0.443": "the CCC metric in the Valence-Arousal Challenge where"
        },
        {
          "B2\nscores": "much higher,\nso that 1,555,919 and 338,755 frames were",
          "0.408\n0.477\n0.443": ""
        },
        {
          "B2\nscores": "",
          "0.408\n0.477\n0.443": "we improved the baseline by more than 20%."
        },
        {
          "B2\nscores": "put\ninto the training and validation sets. The estimates of",
          "0.408\n0.477\n0.443": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 6: ), the best performance metric the MTL challenge, fourth places in the Expression and",
      "data": [
        {
          "Validation set": "PAU",
          "Test set": "CCC V"
        },
        {
          "Validation set": "0.39",
          "Test set": "0.18"
        },
        {
          "Validation set": "0.5367",
          "Test set": "0.4014"
        },
        {
          "Validation set": "0.5447",
          "Test set": "0.4083"
        },
        {
          "Validation set": "0.5430",
          "Test set": "0.4061"
        },
        {
          "Validation set": "0.5445",
          "Test set": "0.4174"
        },
        {
          "Validation set": "0.5478",
          "Test set": "0.4135"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: ), the best performance metric the MTL challenge, fourth places in the Expression and",
      "data": [
        {
          "Table 5. Results of smoothing techniques on the Aff-Wild2’s validation and test sets.": "Method"
        },
        {
          "Table 5. Results of smoothing techniques on the Aff-Wild2’s validation and test sets.": "Baseline [9]"
        },
        {
          "Table 5. Results of smoothing techniques on the Aff-Wild2’s validation and test sets.": "embeddings + scores"
        },
        {
          "Table 5. Results of smoothing techniques on the Aff-Wild2’s validation and test sets.": "embeddings + scores, 1 hidden layer"
        },
        {
          "Table 5. Results of smoothing techniques on the Aff-Wild2’s validation and test sets.": "embeddings + scores"
        },
        {
          "Table 5. Results of smoothing techniques on the Aff-Wild2’s validation and test sets.": "embeddings + scores, 1 hidden layer"
        },
        {
          "Table 5. Results of smoothing techniques on the Aff-Wild2’s validation and test sets.": "embeddings + scores"
        },
        {
          "Table 5. Results of smoothing techniques on the Aff-Wild2’s validation and test sets.": "embeddings + scores, 1 hidden layer"
        },
        {
          "Table 5. Results of smoothing techniques on the Aff-Wild2’s validation and test sets.": "embeddings + scores, different AU thresholds"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: ), the best performance metric the MTL challenge, fourth places in the Expression and",
      "data": [
        {
          "embeddings + scores, different AU thresholds": "",
          "1.150\n0.302\n0.490\n0.384": "Table 6. Multi-Task-Learning Challenge Results on the Aff-Wild2’s validation set."
        },
        {
          "embeddings + scores, different AU thresholds": "the training set and concatenation of the training and vali-",
          "1.150\n0.302\n0.490\n0.384": "valence and arousal prediction, and F1-score 0.54 for ac-"
        },
        {
          "embeddings + scores, different AU thresholds": "dation sets. Despite the superiority of EfﬁcientNet-B2 on",
          "1.150\n0.302\n0.490\n0.384": "tion unit detection. Our approach took the third place in"
        },
        {
          "embeddings + scores, different AU thresholds": "the validation set\n(Table 6),\nthe best performance metric",
          "1.150\n0.302\n0.490\n0.384": "the MTL challenge,\nfourth places\nin the Expression and"
        },
        {
          "embeddings + scores, different AU thresholds": "PM T L = 0.809 is obtained for EfﬁcientNet-B0, though the",
          "1.150\n0.302\n0.490\n0.384": "Valence-Arousal Challenges and ﬁfth place in the Action"
        },
        {
          "embeddings + scores, different AU thresholds": "is not\ndifference with EfﬁcientNet-B2 (PM T L = 0.8083)",
          "1.150\n0.302\n0.490\n0.384": "Unit Challenges.\nIn average,\nthere is only one team who"
        },
        {
          "embeddings + scores, different AU thresholds": "signiﬁcant. Anyway, our simple model\nis much more ac-",
          "1.150\n0.302\n0.490\n0.384": "took slightly lower\naverage place\nin all\nfour\ntasks\n[33]."
        },
        {
          "embeddings + scores, different AU thresholds": "curate when compared to the baseline.\nIndeed,\nits metric",
          "1.150\n0.302\n0.490\n0.384": "Their transformer-based multimodal solution is the winner"
        },
        {
          "embeddings + scores, different AU thresholds": "to 0.3 and 0.28 for validation and test set,\nPM T L is equal",
          "1.150\n0.302\n0.490\n0.384": "in the uni-task Expression Classiﬁcation and Action Unit"
        },
        {
          "embeddings + scores, different AU thresholds": "respectively. Thus, we improved the performance by more",
          "1.150\n0.302\n0.490\n0.384": "Detection, but our method is better in other two challenges."
        },
        {
          "embeddings + scores, different AU thresholds": "than 50% and took the third place in MTL sub-challenge.",
          "1.150\n0.302\n0.490\n0.384": "As the proposed model has not been ﬁne-tuned on the Af-"
        },
        {
          "embeddings + scores, different AU thresholds": "",
          "1.150\n0.302\n0.490\n0.384": "fWild2 dataset, we can claim that\nthe facial\nfeatures ex-"
        },
        {
          "embeddings + scores, different AU thresholds": "4. Conclusion",
          "1.150\n0.302\n0.490\n0.384": "tracted by our networks lead to the most robust decisions."
        },
        {
          "embeddings + scores, different AU thresholds": "",
          "1.150\n0.302\n0.490\n0.384": "Due to simplicity, our approach may be considered as a new"
        },
        {
          "embeddings + scores, different AU thresholds": "In this paper, we have presented the frame-level\nfacial",
          "1.150\n0.302\n0.490\n0.384": ""
        },
        {
          "embeddings + scores, different AU thresholds": "",
          "1.150\n0.302\n0.490\n0.384": "baseline for all four sub-challenges."
        },
        {
          "embeddings + scores, different AU thresholds": "emotion analysis model (Fig. 1) based on concatenation of",
          "1.150\n0.302\n0.490\n0.384": ""
        },
        {
          "embeddings + scores, different AU thresholds": "embeddings and scores at\nthe output of EfﬁcientNet. The",
          "1.150\n0.302\n0.490\n0.384": ""
        },
        {
          "embeddings + scores, different AU thresholds": "",
          "1.150\n0.302\n0.490\n0.384": "In the future,\nit\nis necessary to integrate our approach"
        },
        {
          "embeddings + scores, different AU thresholds": "latter CNN was carefully pre-trained on the VGGFace2 [1]",
          "1.150\n0.302\n0.490\n0.384": ""
        },
        {
          "embeddings + scores, different AU thresholds": "",
          "1.150\n0.302\n0.490\n0.384": "into more\ncomplex pipelines.\nFor\nexample, we process"
        },
        {
          "embeddings + scores, different AU thresholds": "and AffectNet [24] datasets.\nIts experimental study for the",
          "1.150\n0.302\n0.490\n0.384": ""
        },
        {
          "embeddings + scores, different AU thresholds": "",
          "1.150\n0.302\n0.490\n0.384": "frames independently, so that any sequential and attention"
        },
        {
          "embeddings + scores, different AU thresholds": "tasks of the third ABAW challenge [9] have demonstrated",
          "1.150\n0.302\n0.490\n0.384": ""
        },
        {
          "embeddings + scores, different AU thresholds": "",
          "1.150\n0.302\n0.490\n0.384": "models can beneﬁt from the usage of our facial emotional"
        },
        {
          "embeddings + scores, different AU thresholds": "that our technique is much more accurate than the baseline",
          "1.150\n0.302\n0.490\n0.384": ""
        },
        {
          "embeddings + scores, different AU thresholds": "",
          "1.150\n0.302\n0.490\n0.384": "features [3, 17, 18]. Moreover, it is worth studying the com-"
        },
        {
          "embeddings + scores, different AU thresholds": "VGGFACE. Moreover, we have implemented an Android",
          "1.150\n0.302\n0.490\n0.384": ""
        },
        {
          "embeddings + scores, different AU thresholds": "",
          "1.150\n0.302\n0.490\n0.384": "bination of our models into a large ensemble with different"
        },
        {
          "embeddings + scores, different AU thresholds": "mobile application (Fig. 2) with publicly available source",
          "1.150\n0.302\n0.490\n0.384": ""
        },
        {
          "embeddings + scores, different AU thresholds": "",
          "1.150\n0.302\n0.490\n0.384": "representations of input videos. Finally, as our current best"
        },
        {
          "embeddings + scores, different AU thresholds": "code to demonstrate the real-time efﬁciency of our approach",
          "1.150\n0.302\n0.490\n0.384": ""
        },
        {
          "embeddings + scores, different AU thresholds": "",
          "1.150\n0.302\n0.490\n0.384": "model for MTL solves each task independently, it is impor-"
        },
        {
          "embeddings + scores, different AU thresholds": "and motivate practitioners to implement\nthe facial emotion",
          "1.150\n0.302\n0.490\n0.384": ""
        },
        {
          "embeddings + scores, different AU thresholds": "",
          "1.150\n0.302\n0.490\n0.384": "tant\nto properly use the correlation between different\ntasks"
        },
        {
          "embeddings + scores, different AU thresholds": "analytic engines on-device. Our approach is based on a sin-",
          "1.150\n0.302\n0.490\n0.384": ""
        },
        {
          "embeddings + scores, different AU thresholds": "",
          "1.150\n0.302\n0.490\n0.384": "in the multi-task learning scenario [32]."
        },
        {
          "embeddings + scores, different AU thresholds": "gle lightweight neural network, so that it may be not as ac-",
          "1.150\n0.302\n0.490\n0.384": ""
        },
        {
          "embeddings + scores, different AU thresholds": "curate as ensembles of many CNNs [4].",
          "1.150\n0.302\n0.490\n0.384": ""
        },
        {
          "embeddings + scores, different AU thresholds": "Our best EfﬁcientNet-B0 model\nis characterized by F1-",
          "1.150\n0.302\n0.490\n0.384": "Acknowledgements.\nThe work is\nsupported by RSF"
        },
        {
          "embeddings + scores, different AU thresholds": "score 0.38 for expression recognition, mean CCC 0.46 for",
          "1.150\n0.302\n0.490\n0.384": "(Russian Science Foundation) grant 20-71-10010."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "action unit recognition: Aff-Wild2, multi-task learning and"
        },
        {
          "References": "[1] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and An-",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "arcface. arXiv preprint arXiv:1910.04855, 2019. 1, 2"
        },
        {
          "References": "drew Zisserman. Vggface2: A dataset for recognising faces",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[15] Dimitrios Kollias and Stefanos Zafeiriou.\nAffect analysis"
        },
        {
          "References": "across pose and age.\nIn 13th Int. Conf. on Automatic Face &",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "in-the-wild: Valence-arousal, expressions, action units and a"
        },
        {
          "References": "Gesture Recognition (FG), pages 67–74. IEEE, 2018. 1, 2, 6",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "uniﬁed framework. arXiv preprint arXiv:2103.15792, 2021."
        },
        {
          "References": "[2]\nPolina Demochkina and Andrey V Savchenko. MobileEmo-",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "1, 3"
        },
        {
          "References": "tiFace: Efﬁcient facial image representations in video-based",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[16] Dimitrios Kollias and Stefanos Zafeiriou. Analysing affec-"
        },
        {
          "References": "emotion recognition on mobile devices.\nIn Int. Conf. on Pat-",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "tive behavior\nin the second ABAW2 competition.\nIn Int."
        },
        {
          "References": "tern Recognition (ICPR) International Workshops and Chal-",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Conf. Comput. Vis. (ICCV), pages 3652–3660, 2021. 1"
        },
        {
          "References": "lenges, Part V, pages 266–274. Springer, 2021. 1, 4, 5, 6",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[17]\nIlya Makarov, Maria Bakhanova, Sergey Nikolenko,\nand"
        },
        {
          "References": "[3]\nPolina Demochkina and Andrey V Savchenko. Neural net-",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Olga Gerasimova.\nSelf-supervised recurrent depth estima-"
        },
        {
          "References": "work model for video-based facial expression recognition in-",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "tion with attention mechanisms.\nPeerJ Computer Science,"
        },
        {
          "References": "the-wild on mobile devices.\nIn Int. Conf. on Information",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "8:e865, 2022. 6"
        },
        {
          "References": "Technology and Nanotechnology (ITNT), pages 1–5.\nIEEE,",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[18] Debin Meng, Xiaojiang Peng, Kai Wang,\nand Yu Qiao."
        },
        {
          "References": "2021. 6",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Frame attention networks for\nfacial expression recognition"
        },
        {
          "References": "[4] Didan Deng, Liang Wu, and Bertram E Shi.\nIterative distil-",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "in videos.\nIn IEEE Int. Conf. Image Process., pages 3866–"
        },
        {
          "References": "lation for better uncertainty estimates in multitask emotion",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "3870. IEEE, 2019. 6"
        },
        {
          "References": "recognition.\nIn Int. Conf. Comput. Vis. (ICCV), pages 3557–",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[19] Ali Mollahosseini, Behzad Hasani, and Mohammad H Ma-"
        },
        {
          "References": "3566, 2021. 1, 2, 6",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "hoor. AffectNet: A database for facial expression, valence,"
        },
        {
          "References": "[5]\nPaul Ed Ekman and Erika L Rosenberg. What\nthe face re-",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "IEEE Trans. Affective\nand arousal computing in the wild."
        },
        {
          "References": "veals: Basic and applied studies of spontaneous expression",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Computing, 10(1):18–31, 2017. 1, 2, 3"
        },
        {
          "References": "using the facial action coding system (FACS). 2005. 1",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[20] Matti Pietik¨ainen and Olli Silven.\nChallenges of artiﬁcial"
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "intelligence–from machine learning and computer vision to"
        },
        {
          "References": "[6]\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "emotional\nintelligence.\narXiv preprint arXiv:2201.01466,"
        },
        {
          "References": "Neyshabur.\nSharpness-aware minimization for\nefﬁciently",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "2022. 1"
        },
        {
          "References": "improving generalization. arXiv preprint arXiv:2010.01412,",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[21] Alexandr\nRassadin,\nAlexey\nGruzdev,\nand\nAndrey\nV"
        },
        {
          "References": "2020. 3",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Savchenko. Group-level emotion recognition using transfer"
        },
        {
          "References": "[7]\nIvan Grechikhin and Andrey V Savchenko. User modeling",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "learning from face identiﬁcation.\nIn 19th Int. Conf.on Mul-"
        },
        {
          "References": "on mobile device based on facial clustering and object de-",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "timodal Interaction (ICMI), pages 544–548. ACM, 2017. 3,"
        },
        {
          "References": "tection in photos and videos.\nIn Iberian Conf. on Pattern",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "4"
        },
        {
          "References": "Recognition and Image Analysis (IbPRIA), pages 429–440.",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[22] Anwar Saeed, Ayoub Al-Hamadi, Robert Niese, and Mof-"
        },
        {
          "References": "Springer, 2019. 1, 2",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "tah Elzobi. Frame-based facial expression recognition using"
        },
        {
          "References": "[8] Yue\nJin, Tianqing Zheng, Chao Gao,\nand Guoqiang Xu.",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "geometrical features. Advances in Human-Computer Inter-"
        },
        {
          "References": "A multi-modal\nand multi-task\nlearning method\nfor\nac-",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "action, 2014, 2014. 1"
        },
        {
          "References": "arXiv\npreprint\ntion\nunit\nand\nexpression\nrecognition.",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[23] Andrey V Savchenko.\nEfﬁcient\nfacial\nrepresentations\nfor"
        },
        {
          "References": "arXiv:2107.04187, 2021. 1",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "age, gender and identity recognition in organizing photo al-"
        },
        {
          "References": "[9] Dimitrios Kollias. ABAW: Valence-arousal estimation, ex-",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "bums using multi-output convnet. PeerJ Computer Science,"
        },
        {
          "References": "pression\nrecognition,\naction\nunit\ndetection & multi-task",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "5:e197, 2019. 1"
        },
        {
          "References": "learning challenges. arXiv preprint arXiv:2202.10659, 2022.",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[24] Andrey V Savchenko. Facial expression and attributes recog-"
        },
        {
          "References": "1, 2, 4, 5, 6",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "nition based on multi-task learning of lightweight neural net-"
        },
        {
          "References": "[10] Dimitrios Kollias, Attila Schulc, Elnar Hajiyev, and Stefanos",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "works.\nIn 19th Int. Symposium on Intelligent Systems and"
        },
        {
          "References": "Zafeiriou. Analysing affective behavior\nin the ﬁrst ABAW",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Informatics (SISY), pages 119–124. IEEE, 2021. 2, 3, 4, 5, 6"
        },
        {
          "References": "2020 competition.\nIn 15th Int. Conf. on Automatic Face and",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[25] Andrey V Savchenko. User preference prediction in visual"
        },
        {
          "References": "Gesture Recognition (FG), pages 794–800. IEEE, 2020. 1",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "data on mobile devices.\nIn Int. Joint Conf. on Neural Net-"
        },
        {
          "References": "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "works (IJCNN), pages 1–7. IEEE, 2021. 1"
        },
        {
          "References": "Zafeiriou.\nFace\nbehavior\na\nla\ncarte:\nExpressions,\naf-",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[26] Andrey V Savchenko, Kirill V Demochkin,\nand\nIvan S"
        },
        {
          "References": "arXiv preprint\nfect and action units\nin a single network.",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Grechikhin. Preference prediction based on a photo gallery"
        },
        {
          "References": "arXiv:1910.11111, 2019. 2",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": ""
        },
        {
          "References": "",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "analysis with scene recognition and object detection. Pattern"
        },
        {
          "References": "[12] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Recognition, 121:108248, 2022. 2"
        },
        {
          "References": "Zafeiriou.\nDistribution matching for heterogeneous multi-",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[27] Mingxing Tan and Quoc Le. EfﬁcientNet: Rethinking model"
        },
        {
          "References": "arXiv\npreprint\ntask\nlearning:\na\nlarge-scale\nface\nstudy.",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "scaling for\nconvolutional neural networks.\nIn Int. Conf."
        },
        {
          "References": "arXiv:2105.03790, 2021. 1",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Mach. Learn., pages 6105–6114, 2019. 2"
        },
        {
          "References": "[13] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[28]\nPhan Tran Dac Thinh, Hoang Manh Hung, Hyung-Jeong"
        },
        {
          "References": "Athanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Yang,\nSoo-Hyung Kim,\nand Guee-Sang Lee.\nEmotion"
        },
        {
          "References": "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "recognition with incomplete labels using modiﬁed multi-task"
        },
        {
          "References": "in-the-wild: Aff-Wild database and challenge, deep architec-",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "learning technique. arXiv preprint arXiv:2107.04192, 2021."
        },
        {
          "References": "tures, and beyond.\nInt. J. Comput. Vis., pages 1–23, 2019. 1",
          "[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "1"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": "[30]"
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": "[31] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao."
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": "[32] Wei Zhang, Zunhu Guo, Keyu Chen, Lincheng Li, Zhimeng"
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": "[33] Wei Zhang,"
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        },
        {
          "[29] Boris Tseytlin and Ilya Makarov. Hotel recognition via latent": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Qiong Cao",
        "Li Shen",
        "Weidi Xie",
        "Omkar Parkhi",
        "Andrew Zisserman"
      ],
      "year": "2006",
      "venue": "13th Int. Conf. on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "2",
      "title": "MobileEmo-tiFace: Efficient facial image representations in video-based emotion recognition on mobile devices",
      "authors": [
        "Polina Demochkina",
        "Andrey Savchenko"
      ],
      "year": "2006",
      "venue": "Int. Conf. on Pattern Recognition (ICPR) International Workshops and Challenges, Part V"
    },
    {
      "citation_id": "3",
      "title": "Neural network model for video-based facial expression recognition inthe-wild on mobile devices",
      "authors": [
        "Polina Demochkina",
        "Andrey Savchenko"
      ],
      "year": "2021",
      "venue": "Int. Conf. on Information Technology and Nanotechnology (ITNT)"
    },
    {
      "citation_id": "4",
      "title": "Iterative distillation for better uncertainty estimates in multitask emotion recognition",
      "authors": [
        "Didan Deng",
        "Liang Wu",
        "Bertram Shi"
      ],
      "year": "2006",
      "venue": "Int. Conf. Comput. Vis. (ICCV)"
    },
    {
      "citation_id": "5",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the facial action coding system (FACS)",
      "authors": [
        "Ed Paul",
        "Erika Ekman",
        "Rosenberg"
      ],
      "year": "2005",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the facial action coding system (FACS)"
    },
    {
      "citation_id": "6",
      "title": "Sharpness-aware minimization for efficiently improving generalization",
      "authors": [
        "Pierre Foret",
        "Ariel Kleiner",
        "Hossein Mobahi",
        "Behnam Neyshabur"
      ],
      "year": "2020",
      "venue": "Sharpness-aware minimization for efficiently improving generalization",
      "arxiv": "arXiv:2010.01412"
    },
    {
      "citation_id": "7",
      "title": "User modeling on mobile device based on facial clustering and object detection in photos and videos",
      "authors": [
        "Ivan Grechikhin",
        "Andrey Savchenko"
      ],
      "year": "2019",
      "venue": "Iberian Conf. on Pattern Recognition and Image Analysis (IbPRIA)"
    },
    {
      "citation_id": "8",
      "title": "A multi-modal and multi-task learning method for action unit and expression recognition",
      "authors": [
        "Jin Yue",
        "Tianqing Zheng",
        "Chao Gao",
        "Guoqiang Xu"
      ],
      "venue": "A multi-modal and multi-task learning method for action unit and expression recognition",
      "arxiv": "arXiv:2107.04187,2021.1"
    },
    {
      "citation_id": "9",
      "title": "ABAW: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2006",
      "venue": "ABAW: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "arxiv": "arXiv:2202.10659"
    },
    {
      "citation_id": "10",
      "title": "Analysing affective behavior in the first ABAW 2020 competition",
      "authors": [
        "Dimitrios Kollias",
        "Attila Schulc",
        "Elnar Hajiyev",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "15th Int. Conf. on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "11",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "12",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "13",
      "title": "Deep affect prediction in-the-wild: Aff-Wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect, action unit recognition: Aff-Wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "14",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "15",
      "title": "Analysing affective behavior in the second ABAW2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Int. Conf. Comput. Vis. (ICCV)"
    },
    {
      "citation_id": "16",
      "title": "Self-supervised recurrent depth estimation with attention mechanisms",
      "authors": [
        "Ilya Makarov",
        "Maria Bakhanova",
        "Sergey Nikolenko",
        "Olga Gerasimova"
      ],
      "year": "2022",
      "venue": "PeerJ Computer Science"
    },
    {
      "citation_id": "17",
      "title": "Frame attention networks for facial expression recognition in videos",
      "authors": [
        "Debin Meng",
        "Xiaojiang Peng",
        "Kai Wang",
        "Yu Qiao"
      ],
      "year": "2019",
      "venue": "IEEE Int. Conf. Image Process"
    },
    {
      "citation_id": "18",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Challenges of artificial intelligence-from machine learning and computer vision to emotional intelligence",
      "authors": [
        "Matti Pietikäinen",
        "Olli Silven"
      ],
      "year": "2022",
      "venue": "Challenges of artificial intelligence-from machine learning and computer vision to emotional intelligence",
      "arxiv": "arXiv:2201.01466"
    },
    {
      "citation_id": "20",
      "title": "Group-level emotion recognition using transfer learning from face identification",
      "authors": [
        "Alexandr Rassadin",
        "Alexey Gruzdev",
        "Andrey Savchenko"
      ],
      "year": "2004",
      "venue": "19th Int. Conf.on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "21",
      "title": "Frame-based facial expression recognition using geometrical features",
      "authors": [
        "Anwar Saeed",
        "Ayoub Al-Hamadi",
        "Robert Niese",
        "Moftah Elzobi"
      ],
      "year": "2014",
      "venue": "Advances in Human-Computer Interaction"
    },
    {
      "citation_id": "22",
      "title": "Efficient facial representations for age, gender and identity recognition in organizing photo albums using multi-output convnet",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2019",
      "venue": "PeerJ Computer Science"
    },
    {
      "citation_id": "23",
      "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2006",
      "venue": "19th Int. Symposium on Intelligent Systems and Informatics (SISY)"
    },
    {
      "citation_id": "24",
      "title": "User preference prediction in visual data on mobile devices",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2021",
      "venue": "Int. Joint Conf. on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "25",
      "title": "Preference prediction based on a photo gallery analysis with scene recognition and object detection",
      "authors": [
        "Andrey Savchenko",
        "Ivan Kirill V Demochkin",
        "Grechikhin"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Rethinking model scaling for convolutional neural networks",
      "authors": [
        "Mingxing Tan",
        "Quoc Le",
        "Efficientnet"
      ],
      "year": "2019",
      "venue": "In Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "27",
      "title": "Emotion recognition with incomplete labels using modified multi-task learning technique",
      "authors": [
        "Phan Tran",
        "Dac Thinh",
        "Manh Hoang",
        "Hyung-Jeong Hung",
        "Soo-Hyung Yang",
        "Guee-Sang Kim",
        "Lee"
      ],
      "year": "2021",
      "venue": "Emotion recognition with incomplete labels using modified multi-task learning technique",
      "arxiv": "arXiv:2107.04192"
    },
    {
      "citation_id": "28",
      "title": "Hotel recognition via latent image embeddings",
      "authors": [
        "Boris Tseytlin",
        "Ilya Makarov"
      ],
      "year": "2021",
      "venue": "Int. Work-Conf. on Artificial Neural Networks (IWANN)"
    },
    {
      "citation_id": "29",
      "title": "Aff-Wild: Valence and arousal 'in-the-wild' challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog. Worksh. (CVPRW)"
    },
    {
      "citation_id": "30",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "31",
      "title": "Prior aided streaming network for multi-task affective recognition at the 2nd ABAW2 competition",
      "authors": [
        "Wei Zhang",
        "Zunhu Guo",
        "Keyu Chen",
        "Lincheng Li",
        "Zhimeng Zhang",
        "Yu Ding"
      ],
      "year": "2006",
      "venue": "Prior aided streaming network for multi-task affective recognition at the 2nd ABAW2 competition",
      "arxiv": "arXiv:2107.03708"
    },
    {
      "citation_id": "32",
      "title": "Transformer-based multimodal information fusion for facial expression analysis",
      "authors": [
        "Wei Zhang",
        "Zhimeng Zhang",
        "Feng Qiu",
        "Suzhen Wang",
        "Bowen Ma",
        "Hao Zeng",
        "Rudong An",
        "Yu Ding"
      ],
      "year": "2022",
      "venue": "Transformer-based multimodal information fusion for facial expression analysis",
      "arxiv": "arXiv:2203.12367"
    }
  ]
}