{
  "paper_id": "2110.03414v1",
  "title": "Serab: A Multi-Lingual Benchmark For Speech Emotion Recognition",
  "published": "2021-10-07T13:01:34Z",
  "authors": [
    "Neil Scheidwasser-Clow",
    "Mikolaj Kegler",
    "Pierre Beckmann",
    "Milos Cernak"
  ],
  "keywords": [
    "emotion recognition",
    "computational paralinguistics",
    "deep neural networks",
    "speech processing",
    "transfer learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent developments in speech emotion recognition (SER) often leverage deep neural networks (DNNs). Comparing and benchmarking different DNN models can often be tedious due to the use of different datasets and evaluation protocols. To facilitate the process, here, we present the Speech Emotion Recognition Adaptation Benchmark (SERAB), a framework for evaluating the performance and generalization capacity of different approaches for utterancelevel SER. The benchmark is composed of nine datasets for SER in six languages. Since the datasets have different sizes and numbers of emotional classes, the proposed setup is particularly suitable for estimating the generalization capacity of pre-trained DNN-based feature extractors. We used the proposed framework to evaluate a selection of standard hand-crafted feature sets and state-of-the-art DNN representations. The results highlight that using only a subset of the data included in SERAB can result in biased evaluation, while compliance with the proposed protocol can circumvent this issue.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is a cornerstone of computational paralinguistics, the analysis of non-verbal elements of speech  [1] . Although a challenging task, being able to automatically and accurately classify emotions from voice could support a wide range of applications, including human-computer interaction  [1] , healthcare  [2] , and public safety  [3] . At its inception, the development of hand-engineered features has proven effective in tackling various SER-related problems  [1] . Such features are traditionally based on acoustic  [4]  or linguistic descriptors  [5] . More recently, deep neural networks (DNNs) trained in self-supervised fashion were able to produce generalizable representations suitable for a range of audio and speech processing tasks  [6, 7] . A notable advantage of such data-driven approaches over fixed hand-engineered feature sets is the ability to transfer learned knowledge from a large, unlabeled dataset to downstream tasks with less task-specific data available.\n\nHowever, the estimated performance and generalization capacity of such self-supervised DNNs can strongly depend on the evaluation protocol. This makes open-source benchmarks, typically composed of fixed dataset(s) and evaluation pipelines, instrumental for informative, fair, and accessible comparisons of different methods. In visual object recognition, ImageNet  [8]  has established itself as the de facto image dataset and benchmark for deep learning models. In natural language processing (NLP), GLUE  [9]  is a widely used bench-*NSC (neil.scheidwasser-clow@epfl.ch) performed this work as an intern at Logitech. mark, with nine different tasks encompassing various characteristics of language understanding (e.g., sentiment analysis, paraphrase, and inference tasks). As one of the largest audio datasets available, Au-dioSet  [10]  is commonly used for self-supervised pre-training, as well as a benchmarking method for audio event classification  [6, 7, 11] . A recently proposed HEAR challenge  [12]  focuses on evaluating general-purpose audio representations and extends the concept underlying AudioSet by including additional tasks. In speech representation learning, NOSS  [6]  was recently proposed as a platform for evaluating speech-specific feature extractors. It includes diverse non-semantic speech processing problems, such as speaker and language identification, as well as two SER tasks (CREMA-D  [13]  and SAVEE  [14] ).\n\nIn contrast to general audio and non-semantic speech representation learning, a standard, readily available multi-task SER benchmark is yet to be introduced. While recently  [15]  proposed a SERspecific benchmarking framework, it has two considerable shortcomings. First, it only includes a single dataset, implying the lack of diversity in terms of task difficulty, amount of task-specific data, or data acquisition setup (e.g., recording equipment and conditions). This effectively limits the estimation of generalization capacity, and thus the real-life impact of different methods. Second, the dataset is monolingual, with all speech material in English. As a paralinguistic cue, robust embeddings for speech emotion recognition should perform well across different languages.\n\nTo that end, we introduce the Speech Emotion Recognition Adaptation Benchmark (SERAB), a collection of nine SER tasks spanning six languages, different dataset sizes and emotion categories. To streamline the comparison of different approaches, we set up a custom evaluation pipeline. We employed the framework to evaluate recent state-of-the-art pre-trained DNNs for speech/audio feature extraction  [6, 7, 11, 16] , as well as a classic set of handcrafted features commonly used in computational paralinguistics  [4] . Lastly, we also propose a novel Transformer-based model, which performs on par with state-of-the-art approaches. Results obtained for a range of baselines demonstrate apparent differences in performance achieved on single datasets, and illustrate the benefits of using the complete SERAB benchmarking framework. The benchmark was designed to balance dataset popularity, language diversity, and open access. In speech emotion recognition, EmoDB, IEMOCAP and RAVDESS are among the most widely used datasets  [15, 23, 24] . In the same vein as  [24] , a 4-class subset of IEMOCAP (IEM4) was used to mitigate the severe class imbalance in the original dataset. For the other tasks, all samples and classes from the original datasets were used (Table  1 ). As already present in NOSS  [6] , CREMA-D and SAVEE were included in SERAB. To complete the benchmark, CaFE (French) and EMOVO (Italian) were chosen as Italic-language datasets, whereas AESDD (Greek) and ShEMO (Persian) represented the Hellenic and Indo-Iranian branches of the Indo-European family  [25] . Overall, the benchmark mainly comprises scripted and acted speech, excepting IEM4  [17] , RAVDESS  [22]  and ShEMO  [23]  which also feature spontaneous utterances.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Emotion",
      "text": "Each dataset was split into training, validation, and testing sets to respectively train, optimize and evaluate task-specific speech emotion classifiers. Excepting CREMA-D, each dataset was split into 60% training, 20% validation, and 20% testing sets. For CREMA-D, we followed a 70/10/20% (training/validation/testing) split that was applied in NOSS  [6] . Each data partition was speaker-independent, i.e., the sets of speakers included in each part were mutually disjoint. Since SERAB datasets vary in size, the fixed data split allows assessing how different methods cope with various amounts of taskspecific data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Evaluation Pipeline",
      "text": "The SERAB evaluation pipeline is used to assess representations of speech-based emotion obtained using different feature extractors (Fig.  1 ). In particular, the workflow includes processing the input utterances through the pre-trained/non-trainable feature extractor and using the resulting embeddings together with a task-specific classifier to predict speech emotion. By using simple classifiers, the classification accuracy reflects the utility of the extracted features for the utterance-level SER tasks.\n\nImportantly, the utterances included in SERAB vary in duration. The decision about how to integrate information across the utter- ance is a crucial design choice that may impact feature extractor performance. Thus, we left the decision to the authors of the feature extractor. Most of the current approaches tend to extract information from fixed-length frames and average the outputs across frames. However, other approaches may utilize temporal dependencies in the input utterance. As a result, we require the method to return one set of features for each input utterance with varying duration.\n\nTo assess a method's performance, all input utterances are processed through the feature extractor to obtain their embeddings (X). The resulting embeddings are then used as features for basic machine learning classifiers (F ) trained to predict emotion labels (Y). To assure the thorough evaluation, we considered several different classifiers: logistic regression (LR), support vector machine (SVM), linear and quadratic discriminant analysis (LDA/QDA), and random forests (RF). Classifier hyperparameters were optimized through grid-search using the training and validation portions of the data. The best-performing classifier from the grid-search procedure was evaluated on the set-aside test set. All classifier optimization and evaluation procedures were implemented using scikit-learn  [26] .\n\nTest-set classification accuracy in each task was used as the performance metric. The resulting accuracies across the nine SERAB tasks were averaged to quantify the method's benchmark performance. In addition to the unweighted mean accuracy (UM) across the SERAB tasks, we also computed the weighted average derived from the test set size (WM), as well as the geometric mean (GM).  [4]  is a hand-engineered acoustic feature set based on functionals of low-level descriptor contours. Although not directly data-driven, openSMILE is capable of outperforming DNN-based feature extractors, e.g., in problems with little task-specific data  [27] . Here, the most recent implementation of openSMILE 1 was used to extract features from each utterance in the SERAB tasks. Subsequently, for each task, the speech emotion classifier was optimized using the training and validation portions of the data and evaluated using the set-aside test set (Section 2.2). VGGish -VGGish  [11]  is one of the first DNN-based feature extractors for audio, inspired by the VGG-16 convolutional DNN (CNN)  [28] . Pre-trained model weights 2 were learned through supervised classification of audio events from the Youtube-8M dataset  [29]  (≥ 350,000 hours of video, 3,000 classes). The model uses fixed-size input windows. To cope with variable-length audio clips, each input utterance was split into non-overlapping 960 ms-long frames. A log-mel magnitude spectrogram (N = 64 mel frequency bins) was computed from a short-term Fourier transform with 25-ms windows in steps of 10 ms for each frame. The resulting frames were then fed to the pre-trained model for feature extraction. After processing M frames, the obtained M embeddings were averaged to obtain one feature set per utterance. The remaining evaluation followed the protocol outlined in Section 2.2. YAMNet -YAMNet  [16]  is another commonly used DNN-based feature extractor  [6, 7] . This approach utilizes MobileNetv1  [30] , an efficient CNN architecture optimized for mobile devices. Here, we used the weights 3 of the model pre-trained through supervised classification of events from AudioSet  [10]  (≈ 5,800 hours of audio, 521 classes). Since the model operates using fixed-size windows, the input utterances were processed analogously to VGGish. TRILL -While VGGish and YAMNet were trained on diverse audio sources (speech, music, environmental sounds, etc.), TRILL  [6]  was specifically developed as a non-semantic speech feature extractor. The DNN model adopted the architecture of ResNetish  [11]  and was pre-trained in self-supervised fashion using speech samples from AudioSet, which constitutes approximately 50% of the entire dataset (≈ 2,800 hours of audio). The pre-trained model 4 used herein was obtained from triplet loss optimization, which aims at minimizing the embedding-space distance between an anchor and a positive sample (i.e., from the same clip) while maximizing the distance between the same anchor and a negative sample (i.e., from a different clip). In the context of audio, temporally neighboring audio segments will be closer in the representation space and vice versa. Once again, the model operates on fixed-size frames, so the input utterances were processed analogously to VGGish and YAM-Net. Following  [6] , we used the embedding from the first 512-depth convolution layer (layer 19) which performed best on NOSS. BYOL-A -As an alternative to contrastive learning setups such as TRILL, BYOL-A  [7]  proposes bootstrapping your own latent (BYOL) for audio representation learning, inspired by the success of BYOL  [31]  for self-supervised image classification. Pre-trained on the entire AudioSet, this approach achieved state-of-the-art results in various audio classification tasks, even outperforming TRILL  [6]  in speech processing problems. Instead of assessing the temporal proximity of two different audio segments, BYOL-A relies on comparing two augmented versions of a single sample. More specifically, each 1 https://audeering.github.io/opensmile-python/ 2 https://tfhub.dev/google/vggish/1 3 https://tfhub.dev/google/yamnet/1 4 https://tfhub.dev/google/nonsemantic-speech-benchmark/trill/3 version is respectively fed to an online network and a target network. While both are composed of an encoder and a projection block, the online network includes a prediction layer which aims at predicting the projected representation of the second augmented view. Thus, BYOL (and BYOL-A) learns a representation by negating the random data augmentations to capture the essential information about the input. Regarding BYOL-A, pre-trained weights for models of different sizes were released by the authors  5  and used in this work.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Baseline Approaches Opensmile -Opensmile",
      "text": "Since the model accepts inputs of variable length, it returns a single embedding per input utterance. The resulting embeddings are used to train and evaluate the SER classifiers (Section 2.2). BYOL-S -While BYOL-A can achieve state-of-the-art results on a range of audio classification tasks, its general audio representation might not be optimal for speech processing and especially paralinguistic problems. Thus, we re-trained BYOL-A using only speech samples of AudioSet, leading to the speech-specific BYOL-S (S denoting speech). The model architecture, pre-training routine, and usage remained the same as in the original version. BYOL-S/CvT -In this model, we propose an extension of BYOL-S with a Transformer representation. More specifically, we replaced the convolution blocks in BYOL-S with Convolutional Transformer 6  (CvT)  [32] . CvT notably extends self-attention with depthwise convolution to project the queries, keys, and value embeddings. Between the attention modules, traditional convolution layers are added to decompose the input as in most CNNs. Consequently, CvT combines the qualities of CNNs (e.g., translation invariance) and Transformers (e.g., capturing long-range dependencies and generalization abilities). Here, each CvT stage included only one self-attention layer to allow fair comparisons with BYOL-S, both in terms of model architecture and the number of parameters. We experimented with three different configurations of the model. To explore the impact of model size, the number of filters in CvT stages was manipulated to reduce the number of parameters (Table  2 ), analogously to BYOL-A  [7] . In addition, the model was tested with three different embedding dimensions: 256, 512 and 2048. The latter used mean + max temporal aggregation in the last layer instead of global average pooling, in the same vein as  [7] . Like BYOL-S, the pre-training and application to SERAB tasks was analogous to BYOL-A.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Evaluation Results & Discussion",
      "text": "Table  2  presents configurations of different baseline approaches outlined in Section 3. For BYOL-like models (BYOL-A, -S, -S/CvT), we explored different model sizes and sizes of the output embedding fed to the task-specific classifiers predicting emotion labels. Benchmark performance for all baseline methods is presented in Table  3 .\n\nThe large BYOL-S, with a 2048 embedding size, emerged as the best model across all considered performance metrics and yields the best individual accuracy on two out of the nine SERAB tasks. Importantly, model ranks were generally similar across all benchmarkwide metrics. Thus, we chose to sort model performance by UM in accordance with previous benchmarks for computer vision systems  [33] . Although reaching slightly lower scores across the entire benchmark, the largest BYOL-S/CvT remained competitive by providing the best results in four out of nine tasks. Moreover, the increase in the embedding size and the overall model size tend to consistently improve the proposed approaches' performance.\n\nMore generally, all BYOL-inspired models, even with small sizes, achieved significantly higher scores (up to a 5% absolute difference in UM) than TRILL, VGGish, YAMNet, and openSMILE. This considerable difference in performance most likely originates from the vastly different pre-training strategies. Another reason for the supremacy of BYOL-derived models might be the fact that they are designed to process variable-length inputs rather than fixedlength frames as openSMILE, VGGish, YAMNet, and TRILL do. This, in turn, suggests that aggregation of the temporal context could improve utterance-level SER performance.\n\nInterestingly, BYOL-S models performed consistently better than original BYOL-A approaches. This indicates that specializing the pre-training task by focusing only on speech excerpts resulted in more suitable embeddings for SER. In such a speech-specific pre-training, the model presumably developed better capacity for representing speech, including language-independent paralinguistic cues such as speech-based emotion.\n\nOn the other hand, enriching BYOL-S with self-attention mechanisms via CvT did not yield a notable performance increase we anticipated, but the model was lighter with 0.3M fewer parameters than BYOL-S. The slight difference in terms of performance might be due to the minimal inductive biases implied in Transformer-like models, in contrast to CNNs  [34] . While advantageous when training large models on large datasets  [35] , such biases become critical in smaller network and smaller dataset setups  [34] . Thus, increasing the pre-training dataset's size could help develop the generalization ability of Transformers and thus improve the overall performance of BYOL-S/CvT.\n\nWhile SERAB allows comparing different models across a diverse range of tasks, more importantly, it provides a streamlined benchmarking platform for the comparison of different approaches. In particular, some tasks exhibit significant variations between models (e.g., EMOVO, SAVEE, EmoDB) such that the overall poorerperforming approaches may appear better than they really are. Some of these differences might be dataset-specific, introducing even larger bias that is not trivial to overcome. The inclusion of multiple tasks across different languages provides robust performance estimates, as shown by our evaluation of the baseline approaches.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "We introduce SERAB, a multi-lingual benchmark for speech emotion recognition. With the rapid emergence of DNN-based representations of speech and speech-based emotion, the benchmark provides a universal platform for comparing different methods. Due to the inclusion of diverse tasks spanning across different languages, dataset sizes, and emotional categories, SERAB produces robust estimates of performance and generalization capacity. We used SERAB to evaluate a range of recent baselines. Among the tested frameworks, BYOL-based approaches yielded superior performance across all considered metrics. Interestingly, pre-training BYOL-A models on only speech samples of AudioSet (BYOL-S) led to an almost 3% accuracy improvement compared to the original method. Presented evaluation results can be used as baselines for developing novel approaches, such as CvT-based methods explored here. Future work should focus on incorporating more datasets in even more languages into SERAB, as well as extending the task range to include regression problems such as valence or arousal estimation. To facilitate the usage of SERAB, the framework, including setup instructions, evaluation pipelines & examples, is freely available online  7  .",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). In particular, the workﬂow includes processing the input ut-",
      "page": 2
    },
    {
      "caption": "Figure 1: SERAB evaluation pipeline. The (pre-trained/non-trainable)",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3Imperial College London, London, United Kingdom": "mark, with nine different tasks encompassing various characteristics"
        },
        {
          "3Imperial College London, London, United Kingdom": "of language understanding (e.g., sentiment analysis, paraphrase, and"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "inference tasks). As one of the largest audio datasets available, Au-"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "dioSet\n[10]\nis commonly used for\nself-supervised pre-training, as"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "well as a benchmarking method for audio event classiﬁcation [6, 7,"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "11]. A recently proposed HEAR challenge [12] focuses on evaluat-"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "ing general-purpose audio representations and extends the concept"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "underlying AudioSet by including additional\ntasks.\nIn speech rep-"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "resentation learning, NOSS [6] was recently proposed as a platform"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "for evaluating speech-speciﬁc feature extractors.\nIt includes diverse"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "non-semantic speech processing problems, such as speaker and lan-"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "guage identiﬁcation, as well as two SER tasks (CREMA-D [13] and"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "SAVEE [14])."
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "In contrast\nto general audio and non-semantic speech represen-"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "tation learning, a standard, readily available multi-task SER bench-"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "mark is yet\nto be introduced. While recently [15] proposed a SER-"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "speciﬁc benchmarking framework,\nit has\ntwo considerable short-"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "comings. First,\nit only includes a single dataset,\nimplying the lack"
        },
        {
          "3Imperial College London, London, United Kingdom": "of diversity in terms of task difﬁculty, amount of task-speciﬁc data,"
        },
        {
          "3Imperial College London, London, United Kingdom": "or data acquisition setup (e.g., recording equipment and conditions)."
        },
        {
          "3Imperial College London, London, United Kingdom": "This effectively limits the estimation of generalization capacity, and"
        },
        {
          "3Imperial College London, London, United Kingdom": "thus the real-life impact of different methods. Second, the dataset is"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "monolingual, with all speech material\nin English. As a paralinguis-"
        },
        {
          "3Imperial College London, London, United Kingdom": "tic cue,\nrobust embeddings for speech emotion recognition should"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "perform well across different languages."
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "To that\nend, we\nintroduce\nthe Speech Emotion Recognition"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "Adaptation Benchmark (SERAB), a collection of nine SER tasks"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "spanning six languages, different dataset\nsizes and emotion cate-"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "gories.\nTo streamline the comparison of different approaches, we"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "set up a custom evaluation pipeline. We employed the framework to"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "evaluate recent state-of-the-art pre-trained DNNs for speech/audio"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "feature extraction [6, 7, 11, 16], as well as a classic set of hand-"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "crafted features commonly used in computational paralinguistics [4]."
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "Lastly, we also propose a novel Transformer-based model, which"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "performs on par with state-of-the-art approaches. Results obtained"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "for a range of baselines demonstrate apparent differences\nin per-"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "formance achieved on single datasets, and illustrate the beneﬁts of"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "using the complete SERAB benchmarking framework."
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "2.\nSPEECH EMOTION RECOGNITION ADAPTATION"
        },
        {
          "3Imperial College London, London, United Kingdom": "BENCHMARK (SERAB)"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "2.1. Tasks & datasets"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "A summary of the tasks used in SERAB is presented in Table 1. The"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "benchmark comprises nine speech emotion classiﬁcation tasks\nin"
        },
        {
          "3Imperial College London, London, United Kingdom": ""
        },
        {
          "3Imperial College London, London, United Kingdom": "six languages:\nfour in English (CREMA-D, IEMOCAP, RAVDESS"
        },
        {
          "3Imperial College London, London, United Kingdom": "& SAVEE), and one in French (CaFE), German (EmoDB), Greek"
        },
        {
          "3Imperial College London, London, United Kingdom": "(AESDD), Italian (EMOVO), and Persian (ShEMO). In each dataset,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1. SERAB tasks and datasets.": "provider website. Open-access datasets can be downloaded without registration.",
          "IEM4: 4-class IEMOCAP [17]. Restricted-access datasets require additional registration on the data": ""
        },
        {
          "Table 1. SERAB tasks and datasets.": "Dataset",
          "IEM4: 4-class IEMOCAP [17]. Restricted-access datasets require additional registration on the data": "Language"
        },
        {
          "Table 1. SERAB tasks and datasets.": "",
          "IEM4: 4-class IEMOCAP [17]. Restricted-access datasets require additional registration on the data": ""
        },
        {
          "Table 1. SERAB tasks and datasets.": "AESDD [18]",
          "IEM4: 4-class IEMOCAP [17]. Restricted-access datasets require additional registration on the data": "Greek"
        },
        {
          "Table 1. SERAB tasks and datasets.": "CaFE [19]",
          "IEM4: 4-class IEMOCAP [17]. Restricted-access datasets require additional registration on the data": "French"
        },
        {
          "Table 1. SERAB tasks and datasets.": "CREMA-D [13]",
          "IEM4: 4-class IEMOCAP [17]. Restricted-access datasets require additional registration on the data": "English"
        },
        {
          "Table 1. SERAB tasks and datasets.": "EmoDB [20]",
          "IEM4: 4-class IEMOCAP [17]. Restricted-access datasets require additional registration on the data": "German"
        },
        {
          "Table 1. SERAB tasks and datasets.": "EMOVO [21]",
          "IEM4: 4-class IEMOCAP [17]. Restricted-access datasets require additional registration on the data": "Italian"
        },
        {
          "Table 1. SERAB tasks and datasets.": "IEM4 [17]",
          "IEM4: 4-class IEMOCAP [17]. Restricted-access datasets require additional registration on the data": "English"
        },
        {
          "Table 1. SERAB tasks and datasets.": "RAVDESS [22]",
          "IEM4: 4-class IEMOCAP [17]. Restricted-access datasets require additional registration on the data": "English"
        },
        {
          "Table 1. SERAB tasks and datasets.": "SAVEE [14]",
          "IEM4: 4-class IEMOCAP [17]. Restricted-access datasets require additional registration on the data": "English"
        },
        {
          "Table 1. SERAB tasks and datasets.": "ShEMO [23]",
          "IEM4: 4-class IEMOCAP [17]. Restricted-access datasets require additional registration on the data": "Persian"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "SAVEE [14]\nSAV\nRestricted\nEnglish",
          "8": "7",
          "1,440\n24": "480\n4",
          "3.7\n1.5": "3.8\n0.5"
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "ShEMO [23]\nSHE\nOpen\nPersian",
          "8": "6",
          "1,440\n24": "3,000\n87",
          "3.7\n1.5": "4.0\n3.3"
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "speech samples have three attributes: audio data (i.e., the raw wave-",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": "Task-speciﬁc training"
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "form,\nin mono), speaker\nidentiﬁer, and emotion label\n(e.g., angry,",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "",
          "8": "",
          "1,440\n24": "Input utterance",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "happy, sad). The datasets vary in size (i.e., number of utterances),",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "",
          "8": "",
          "1,440\n24": "( L )",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": "Emotion label"
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "number of speakers, class distribution, and number of classes. While",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": "( Y )"
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "anger, happiness, and sadness are found across all datasets, disgust,",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "fear, neutral emotion, surprise, calm, and boredom appear in at least",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "one dataset. On the other hand, all datasets have roughly the same",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "",
          "8": "",
          "1,440\n24": "(pre-trained)",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": "Task-speciﬁc"
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "average utterance duration (between 2.5 & 4.5 seconds).",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "",
          "8": "",
          "1,440\n24": "Feature extractor",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": "classiﬁer"
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "The benchmark was designed to balance dataset popularity, lan-",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "",
          "8": "",
          "1,440\n24": "(e.g., openSMILE, TRILL)",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": "( F )"
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "guage diversity, and open access.\nIn speech emotion recognition,",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "EmoDB,\nIEMOCAP and RAVDESS are among the most widely",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": "Optimise F :"
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "used datasets [15, 23, 24].\nIn the same vein as [24], a 4-class sub-",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": "Train. & Valid."
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "set of IEMOCAP (IEM4) was used to mitigate the severe class im-",
          "8": "",
          "1,440\n24": "Utterance embedding",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": "Evaluate F :"
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "balance in the original dataset.\nFor\nthe other\ntasks,\nall\nsamples",
          "8": "",
          "1,440\n24": "( X )",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": "Test"
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "and classes from the original datasets were used (Table 1). As al-",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": ""
        },
        {
          "RAVDESS [22]\nRAV\nOpen\nEnglish": "ready present in NOSS [6], CREMA-D and SAVEE were included in",
          "8": "",
          "1,440\n24": "",
          "3.7\n1.5": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "openSMILE - openSMILE [4]\nis a hand-engineered acoustic fea-"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "ture set based on functionals of\nlow-level descriptor contours. Al-"
        },
        {
          "3. BASELINE APPROACHES": "though not directly data-driven, openSMILE is capable of outper-"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "forming DNN-based feature extractors, e.g.,\nin problems with lit-"
        },
        {
          "3. BASELINE APPROACHES": "tle task-speciﬁc data [27]. Here,\nthe most recent\nimplementation of"
        },
        {
          "3. BASELINE APPROACHES": "openSMILE1 was used to extract features from each utterance in the"
        },
        {
          "3. BASELINE APPROACHES": "SERAB tasks. Subsequently, for each task, the speech emotion clas-"
        },
        {
          "3. BASELINE APPROACHES": "siﬁer was optimized using the training and validation portions of the"
        },
        {
          "3. BASELINE APPROACHES": "data and evaluated using the set-aside test set (Section 2.2)."
        },
        {
          "3. BASELINE APPROACHES": "VGGish - VGGish [11]\nis one of\nthe ﬁrst DNN-based feature ex-"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "tractors\nfor\naudio,\ninspired by the VGG-16 convolutional DNN"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "(CNN)\n[28].\nPre-trained model weights2 were\nlearned\nthrough"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "supervised\nclassiﬁcation\nof\naudio\nevents\nfrom the Youtube-8M"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "dataset [29] (≥ 350,000 hours of video, 3,000 classes). The model"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "uses ﬁxed-size input windows.\nTo cope with variable-length au-"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "dio clips, each input utterance was split\ninto non-overlapping 960"
        },
        {
          "3. BASELINE APPROACHES": "ms-long frames. A log-mel magnitude spectrogram (N = 64 mel"
        },
        {
          "3. BASELINE APPROACHES": "frequency bins) was computed from a short-term Fourier transform"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "with 25-ms windows in steps of 10 ms for each frame. The resulting"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "frames were then fed to the pre-trained model\nfor\nfeature extrac-"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "tion. After processing M frames, the obtained M embeddings were"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "averaged to obtain one feature set per utterance.\nThe remaining"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "evaluation followed the protocol outlined in Section 2.2."
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "YAMNet\n- YAMNet\n[16]\nis another commonly used DNN-based"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "feature extractor\n[6, 7].\nThis approach utilizes MobileNetv1 [30],"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "an efﬁcient CNN architecture optimized for mobile devices. Here,"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "we used the weights3 of\nthe model pre-trained through supervised"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "classiﬁcation of events from AudioSet [10] (≈ 5,800 hours of audio,"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "521 classes). Since the model operates using ﬁxed-size windows, the"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "input utterances were processed analogously to VGGish."
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "TRILL - While VGGish and YAMNet were trained on diverse au-"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "dio sources (speech, music, environmental sounds, etc.), TRILL [6]"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "was speciﬁcally developed as a non-semantic speech feature extrac-"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "tor.\nThe DNN model adopted the architecture of ResNetish [11]"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "and was pre-trained in self-supervised fashion using speech sam-"
        },
        {
          "3. BASELINE APPROACHES": ""
        },
        {
          "3. BASELINE APPROACHES": "ples from AudioSet, which constitutes approximately 50% of the en-"
        },
        {
          "3. BASELINE APPROACHES": "tire dataset (≈ 2,800 hours of audio). The pre-trained model4 used"
        },
        {
          "3. BASELINE APPROACHES": "herein was obtained from triplet\nloss optimization, which aims at"
        },
        {
          "3. BASELINE APPROACHES": "minimizing the embedding-space distance between an anchor and"
        },
        {
          "3. BASELINE APPROACHES": "a positive sample (i.e.,\nfrom the same clip) while maximizing the"
        },
        {
          "3. BASELINE APPROACHES": "distance between the same anchor and a negative sample (i.e., from"
        },
        {
          "3. BASELINE APPROACHES": "a different clip).\nIn the context of audio,\ntemporally neighboring"
        },
        {
          "3. BASELINE APPROACHES": "audio segments will be closer\nin the representation space and vice"
        },
        {
          "3. BASELINE APPROACHES": "versa. Once again,\nthe model operates on ﬁxed-size frames, so the"
        },
        {
          "3. BASELINE APPROACHES": "input utterances were processed analogously to VGGish and YAM-"
        },
        {
          "3. BASELINE APPROACHES": "Net. Following [6], we used the embedding from the ﬁrst 512-depth"
        },
        {
          "3. BASELINE APPROACHES": "convolution layer (layer 19) which performed best on NOSS."
        },
        {
          "3. BASELINE APPROACHES": "BYOL-A - As\nan alternative\nto contrastive\nlearning setups\nsuch"
        },
        {
          "3. BASELINE APPROACHES": "as TRILL, BYOL-A [7] proposes bootstrapping your own latent"
        },
        {
          "3. BASELINE APPROACHES": "(BYOL) for audio representation learning, inspired by the success of"
        },
        {
          "3. BASELINE APPROACHES": "BYOL [31] for self-supervised image classiﬁcation. Pre-trained on"
        },
        {
          "3. BASELINE APPROACHES": "the entire AudioSet, this approach achieved state-of-the-art results in"
        },
        {
          "3. BASELINE APPROACHES": "various audio classiﬁcation tasks, even outperforming TRILL [6] in"
        },
        {
          "3. BASELINE APPROACHES": "speech processing problems. Instead of assessing the temporal prox-"
        },
        {
          "3. BASELINE APPROACHES": "imity of two different audio segments, BYOL-A relies on comparing"
        },
        {
          "3. BASELINE APPROACHES": "two augmented versions of a single sample. More speciﬁcally, each"
        },
        {
          "3. BASELINE APPROACHES": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "WM: weighted mean (by the number of utterances in the test set), GM: geometric mean. Models are sorted by their UM across all tasks. The"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "best performing approaches for each task and metric are denoted in bold."
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "Model\nAES\nCAF\nCRE\nEMB\nEMV"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "YAMNet\n53.6\n48.1\n53.9\n60.7\n35.7"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "VGGish\n46.4\n50.0\n55.5\n73.8\n36.2"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "TRILL, layer 19\n66.7\n68.5\n73.3\n81.0\n36.7"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "90.5\nopenSMILE\n70.0\n70.4\n72.8\n37.2"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "BYOL-A, 512\n71.5\n73.1\n70.2\n84.5\n39.3"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "BYOL-A, 2048\n72.0\n75.5\n73.7\n88.1\n38.3"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "BYOL-A, 1024\n75.4\n74.1\n71.3\n88.1\n44.4"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "Proposed:"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "BYOL-S/CvT, 256\n72.9\n71.8\n72.9\n85.7\n47.4"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "76.4\nBYOL-S, 512\n74.9\n74.4\n86.9\n34.2"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "BYOL-S, 1024\n75.4\n72.7\n75.3\n84.5\n39.3"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "BYOL-S/CvT, 512\n71.0\n75.5\n74.0\n88.1\n46.9"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "76.9\n48.5\nBYOL-S/CvT, 2048\n75.8\n71.3\n84.5"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "77.3\n76.9\nBYOL-S, 2048\n74.5\n88.1\n44.4"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "4. EVALUATION RESULTS & DISCUSSION"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": ""
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "Table 2 presents conﬁgurations of different baseline approaches out-"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "lined in Section 3. For BYOL-like models (BYOL-A, -S, -S/CvT),"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "we explored different model sizes and sizes of the output embedding"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "fed to the task-speciﬁc classiﬁers predicting emotion labels. Bench-"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "mark performance for all baseline methods is presented in Table 3."
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": ""
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "The large BYOL-S, with a 2048 embedding size, emerged as the"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": ""
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "best model across all considered performance metrics and yields the"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": ""
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "best\nindividual accuracy on two out of the nine SERAB tasks.\nIm-"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": ""
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "portantly, model ranks were generally similar across all benchmark-"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": ""
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "wide metrics.\nThus, we chose to sort model performance by UM"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": ""
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "in accordance with previous benchmarks for computer vision sys-"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": ""
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "tems [33]. Although reaching slightly lower scores across the en-"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": ""
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "tire benchmark,\nthe largest BYOL-S/CvT remained competitive by"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": ""
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "providing the best\nresults in four out of nine tasks. Moreover,\nthe"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "increase in the embedding size and the overall model size tend to"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "consistently improve the proposed approaches’ performance."
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "More generally,\nall BYOL-inspired models,\neven with small"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "sizes, achieved signiﬁcantly higher scores (up to a 5% absolute dif-"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "ference in UM) than TRILL, VGGish, YAMNet, and openSMILE."
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "This considerable difference in performance most\nlikely originates"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "from the vastly different pre-training strategies.\nAnother\nreason"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "for the supremacy of BYOL-derived models might be the fact\nthat"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "they are designed to process variable-length inputs rather than ﬁxed-"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "length frames as openSMILE, VGGish, YAMNet, and TRILL do."
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "This, in turn, suggests that aggregation of the temporal context could"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "improve utterance-level SER performance."
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "Interestingly, BYOL-S models\nperformed\nconsistently\nbetter"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "than original BYOL-A approaches. This indicates that specializing"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "the pre-training task by focusing only on speech excerpts resulted"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "in more suitable embeddings\nfor SER.\nIn such a speech-speciﬁc"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "pre-training,\nthe model presumably developed better capacity for"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "representing speech,\nincluding language-independent paralinguistic"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "cues such as speech-based emotion."
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": ""
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "On the other hand, enriching BYOL-S with self-attention mech-"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": ""
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "anisms via CvT did not yield a notable performance increase we"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": ""
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "anticipated, but\nthe model was lighter with 0.3M fewer parameters"
        },
        {
          "Table 3. Test accuracy (%) on the different downstream tasks in SERAB, referred to by their code from Table 1. UM: unweighted mean,": "than BYOL-S. The slight difference in terms of performance might"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "2018."
        },
        {
          "6. REFERENCES": "Computational Paralinguistics:\n[1] B. Schuller and A. Batliner,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[19]\nP. Gournay, O. Lahaie, and R. Lefebvre,\n“A Canadian French"
        },
        {
          "6. REFERENCES": "Emotion, Affect and Personality in Speech and Language Pro-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "emotional speech dataset,” in Proc. ACM Multimedia Systems,"
        },
        {
          "6. REFERENCES": "cessing, Wiley, Chichester, UK, 2013.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "2018, pp. 399–402."
        },
        {
          "6. REFERENCES": "[2] M. Van Puyvelde, X. Neyt, F. McGlone, and N. Pattyn, “Voice",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[20]\nF. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, and"
        },
        {
          "6. REFERENCES": "stress analysis: A new framework for voice and effort in human",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "B. Weiss,\n“A database of German emotional speech,”\nin EU-"
        },
        {
          "6. REFERENCES": "performance,” Front. Psychol., vol. 9, pp. 1994, 2018.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "ROSPEECH, 2005."
        },
        {
          "6. REFERENCES": "[3]\nI. Lefter, L. J. Rothkrantz, D. A. Van Leeuwen, and P. Wiggers,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[21] G. Costantini,\nI.\nIaderola, A.\nPaoloni,\nand M.\nTodisco,"
        },
        {
          "6. REFERENCES": "“Automatic stress detection in emergency (telephone) calls,”",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "“EMOVO corpus:\nan Italian emotional speech database,”\nin"
        },
        {
          "6. REFERENCES": "Int. J. Intell. Defence Support Syst., vol. 4, no. 2, pp. 148–168,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "LREC. ELRA, 2014, pp. 3501–3504."
        },
        {
          "6. REFERENCES": "2011.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[22]\nS. R. Livingstone and F. A. Russo, “The Ryerson Audio-Visual"
        },
        {
          "6. REFERENCES": "[4]\nF. Eyben, M. W¨ollmer, and B. Schuller,\n“openSMILE:\nthe",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "Database of Emotional Speech and Song (RAVDESS): A dy-"
        },
        {
          "6. REFERENCES": "Munich versatile and fast open-source audio feature extractor,”",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "namic, multimodal set of facial and vocal expressions in North"
        },
        {
          "6. REFERENCES": "in Proc. ACM Multimedia, 2010, pp. 1459–1462.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "American English,” PLOS ONE, vol. 13, no. 5, 2018."
        },
        {
          "6. REFERENCES": "[5] M. Schmitt and B. Schuller,\n“openXBOW:\nIntroducing the",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[23] O. M. Nezami, P. J. Lou, and M. Karami,\n“ShEMO: a large-"
        },
        {
          "6. REFERENCES": "J.\nPassau\nopen-source\ncrossmodal\nbag-of-words\ntoolkit,”",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "scale validated database for persian speech emotion detection,”"
        },
        {
          "6. REFERENCES": "Mach. Learn. Res., vol. 18, no. 96, pp. 1–5, 2017.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "Lang. Resour. Eval., vol. 53, no. 1, pp. 1–16, 2019."
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[24] R. Xia and Y. Liu, “A multi-task learning framework for emo-"
        },
        {
          "6. REFERENCES": "[6]\nJ. Shor, A. Jansen, R. Maor, O. Lang, O. Tuval, F. C. Quitry,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "tion recognition using 2D continuous space,” IEEE Trans. Af-"
        },
        {
          "6. REFERENCES": "M. Tagliasacchi,\nI. Shavitt, D. Emanuel, and Y. Haviv,\n“To-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "fect. Comput., vol. 8, no. 1, pp. 3–14, 2015."
        },
        {
          "6. REFERENCES": "wards Learning a Universal Non-Semantic Representation of",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "Speech,” in Proc. Interspeech, 2020, pp. 140–144.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[25] L. Campbell, Historical Linguistics: An Introduction,\nEdin-"
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "burgh University Press, Edinburgh, UK, 2013."
        },
        {
          "6. REFERENCES": "[7] D.\nNiizumi,\nD.\nTakeuchi,\nY\n.\nOhishi,\nN.\nHarada,\nand",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "K. Kashino,\n“BYOL for Audio: Self-supervised learning for",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[26]\nF.\nPedregosa,\nG.\nVaroquaux,\nA.\nGramfort,\nV\n. Michel,"
        },
        {
          "6. REFERENCES": "general-purpose audio representation,” in IJCNN, 2021.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,"
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "V\n. Dubourg, et al., “Scikit-learn: Machine learning in Python,”"
        },
        {
          "6. REFERENCES": "[8]\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li,\nand L. Fei-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "J. Mach. Learn. Res., vol. 12, pp. 2825–2830, 2011."
        },
        {
          "6. REFERENCES": "Fei,\n“Imagenet: A large-scale hierarchical\nimage database,”",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[27] B. Schuller, A. Batliner, C. Bergler, C. Mascolo,\nJ. Han,"
        },
        {
          "6. REFERENCES": "in CVPR. IEEE, 2009, pp. 248–255.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "I. Lefter, H. Kaya, S. Amiriparian, A. Baird, L. Stappen,"
        },
        {
          "6. REFERENCES": "[9] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bow-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "et al., “The INTERSPEECH 2021 Computational Paralinguis-"
        },
        {
          "6. REFERENCES": "man,\n“GLUE: A multi-task benchmark and analysis platform",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "tics Challenge: COVID-19 cough, COVID-19 speech, escala-"
        },
        {
          "6. REFERENCES": "for natural language understanding,” in ICLR, 2018.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "tion & primates,” arXiv preprint arXiv:2102.13468, 2021."
        },
        {
          "6. REFERENCES": "[10]\nJ.\nF. Gemmeke,\nD.\nP.\nEllis,\nD.\nFreedman,\nA.\nJansen,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[28] K. Simonyan and A. Zisserman, “Very deep convolutional net-"
        },
        {
          "6. REFERENCES": "W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter,\n“Audio",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "works for large-scale image recognition,” in ICLR, 2015."
        },
        {
          "6. REFERENCES": "set: An ontology and human-labeled dataset for audio events,”",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[29]\nS. Abu-El-Haija, N. Kothari,\nJ. Lee, P. Natsev, G. Toderici,"
        },
        {
          "6. REFERENCES": "in ICASSP. IEEE, 2017, pp. 776–780.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "B. Varadarajan, and S. Vijayanarasimhan,\n“Youtube-8M: A"
        },
        {
          "6. REFERENCES": "[11]\nS. Hershey,\nS. Chaudhuri, D.\nP.\nEllis,\nJ.\nF. Gemmeke,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "arXiv preprint\nlarge-scale video classiﬁcation benchmark,”"
        },
        {
          "6. REFERENCES": "A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "arXiv:1609.08675, 2016."
        },
        {
          "6. REFERENCES": "B. Seybold, et al.,\n“CNN architectures for\nlarge-scale audio",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[30] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,"
        },
        {
          "6. REFERENCES": "classiﬁcation,” in ICASSP. IEEE, 2017, pp. 131–135.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "T. Weyand, M. Andreetto, and H. Adam,\n“MobileNets: Efﬁ-"
        },
        {
          "6. REFERENCES": "[12] Neural Audio AI,\n“HEAR 2021 NeurIPS Challenge Holistic",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "cient convolutional neural networks for mobile vision applica-"
        },
        {
          "6. REFERENCES": "Evaluation of Audio Representations,” https://neuralaudio.ai/h",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "tions,” arXiv preprint arXiv:1704.04861, 2017."
        },
        {
          "6. REFERENCES": "ear2021-holistic-evaluation-of-audio-representations.html.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[31]\nJ.-B. Grill,\nF. Strub,\nF. Altch´e, C. Tallec,\nP. Richemond,"
        },
        {
          "6. REFERENCES": "[13] H. Cao,\nD. G. Cooper, M. K. Keutmann,\nR. C. Gur,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "E. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G."
        },
        {
          "6. REFERENCES": "A. Nenkova, and R. Verma, “CREMA-D: Crowd-sourced emo-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "Azar, et al.,\n“Bootstrap your own latent - a new approach to"
        },
        {
          "6. REFERENCES": "IEEE Trans. Affect. Com-\ntional multimodal actors dataset,”",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "self-supervised learning,” in NeurIPS, 2020, pp. 21271–21284."
        },
        {
          "6. REFERENCES": "put., vol. 5, no. 4, pp. 377–390, 2014.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[32] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and"
        },
        {
          "6. REFERENCES": "[14]\nS. Haq, P. J. Jackson, and J. Edge, “Speaker-dependent audio-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "L. Zhang, “CvT: Introducing convolutions to vision transform-"
        },
        {
          "6. REFERENCES": "visual emotion recognition,” in AVSP, 2009, pp. 53–58.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "ers,” arXiv preprint arXiv:2103.15808, 2021."
        },
        {
          "6. REFERENCES": "[15] W. Fan, X. Xu, X. Xing, W. Chen, and D. Huang, “LSSED: a",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[33] X.\nZhai,\nJ.\nPuigcerver,\nA.\nKolesnikov,\nP.\nRuyssen,"
        },
        {
          "6. REFERENCES": "large-scale dataset and benchmark for speech emotion recog-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "C. Riquelme, M. Lucic, J. Djolonga, A. S. Pinto, M. Neumann,"
        },
        {
          "6. REFERENCES": "nition,” in ICASSP. IEEE, 2021, pp. 641–645.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "A. Dosovitskiy, et al.,\n“A large-scale study of representation"
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "arXiv\nlearning with the visual\ntask adaptation benchmark,”"
        },
        {
          "6. REFERENCES": "[16] M. Plakal and D. Ellis, “YAMNet,” https://github.com/tensorf",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "low/models/tree/master/research/audioset/yamnet, 2020.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "preprint arXiv:1910.04867, 2019."
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[34] G. Cazenavette and S. Lucey,\n“On the bias against\ninductive"
        },
        {
          "6. REFERENCES": "[17] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "biases,” arXiv preprint arXiv:2105.14077, 2021."
        },
        {
          "6. REFERENCES": "S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “IEMOCAP:",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "Interactive emotional dyadic motion capture database,” Lang.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[35] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,"
        },
        {
          "6. REFERENCES": "Resour. Eval., vol. 42, no. 4, pp. 335–359, 2008.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "X.\nZhai,\nT.\nUnterthiner,\nM.\nDehghani,\nM. Minderer,"
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "G. Heigold, S. Gelly, et al., “An image is worth 16x16 words:"
        },
        {
          "6. REFERENCES": "[18] N. Vryzas, R. Kotsakis, A. Liatsou, C. A. Dimoulas,\nand",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "Transformers for image recognition at scale,” in ICLR, 2021."
        },
        {
          "6. REFERENCES": "G. Kalliris,\n“Speech emotion recognition for performance in-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "2018."
        },
        {
          "6. REFERENCES": "Computational Paralinguistics:\n[1] B. Schuller and A. Batliner,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[19]\nP. Gournay, O. Lahaie, and R. Lefebvre,\n“A Canadian French"
        },
        {
          "6. REFERENCES": "Emotion, Affect and Personality in Speech and Language Pro-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "emotional speech dataset,” in Proc. ACM Multimedia Systems,"
        },
        {
          "6. REFERENCES": "cessing, Wiley, Chichester, UK, 2013.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "2018, pp. 399–402."
        },
        {
          "6. REFERENCES": "[2] M. Van Puyvelde, X. Neyt, F. McGlone, and N. Pattyn, “Voice",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[20]\nF. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, and"
        },
        {
          "6. REFERENCES": "stress analysis: A new framework for voice and effort in human",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "B. Weiss,\n“A database of German emotional speech,”\nin EU-"
        },
        {
          "6. REFERENCES": "performance,” Front. Psychol., vol. 9, pp. 1994, 2018.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "ROSPEECH, 2005."
        },
        {
          "6. REFERENCES": "[3]\nI. Lefter, L. J. Rothkrantz, D. A. Van Leeuwen, and P. Wiggers,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[21] G. Costantini,\nI.\nIaderola, A.\nPaoloni,\nand M.\nTodisco,"
        },
        {
          "6. REFERENCES": "“Automatic stress detection in emergency (telephone) calls,”",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "“EMOVO corpus:\nan Italian emotional speech database,”\nin"
        },
        {
          "6. REFERENCES": "Int. J. Intell. Defence Support Syst., vol. 4, no. 2, pp. 148–168,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "LREC. ELRA, 2014, pp. 3501–3504."
        },
        {
          "6. REFERENCES": "2011.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[22]\nS. R. Livingstone and F. A. Russo, “The Ryerson Audio-Visual"
        },
        {
          "6. REFERENCES": "[4]\nF. Eyben, M. W¨ollmer, and B. Schuller,\n“openSMILE:\nthe",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "Database of Emotional Speech and Song (RAVDESS): A dy-"
        },
        {
          "6. REFERENCES": "Munich versatile and fast open-source audio feature extractor,”",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "namic, multimodal set of facial and vocal expressions in North"
        },
        {
          "6. REFERENCES": "in Proc. ACM Multimedia, 2010, pp. 1459–1462.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "American English,” PLOS ONE, vol. 13, no. 5, 2018."
        },
        {
          "6. REFERENCES": "[5] M. Schmitt and B. Schuller,\n“openXBOW:\nIntroducing the",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[23] O. M. Nezami, P. J. Lou, and M. Karami,\n“ShEMO: a large-"
        },
        {
          "6. REFERENCES": "J.\nPassau\nopen-source\ncrossmodal\nbag-of-words\ntoolkit,”",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "scale validated database for persian speech emotion detection,”"
        },
        {
          "6. REFERENCES": "Mach. Learn. Res., vol. 18, no. 96, pp. 1–5, 2017.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "Lang. Resour. Eval., vol. 53, no. 1, pp. 1–16, 2019."
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[24] R. Xia and Y. Liu, “A multi-task learning framework for emo-"
        },
        {
          "6. REFERENCES": "[6]\nJ. Shor, A. Jansen, R. Maor, O. Lang, O. Tuval, F. C. Quitry,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "tion recognition using 2D continuous space,” IEEE Trans. Af-"
        },
        {
          "6. REFERENCES": "M. Tagliasacchi,\nI. Shavitt, D. Emanuel, and Y. Haviv,\n“To-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "fect. Comput., vol. 8, no. 1, pp. 3–14, 2015."
        },
        {
          "6. REFERENCES": "wards Learning a Universal Non-Semantic Representation of",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "Speech,” in Proc. Interspeech, 2020, pp. 140–144.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[25] L. Campbell, Historical Linguistics: An Introduction,\nEdin-"
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "burgh University Press, Edinburgh, UK, 2013."
        },
        {
          "6. REFERENCES": "[7] D.\nNiizumi,\nD.\nTakeuchi,\nY\n.\nOhishi,\nN.\nHarada,\nand",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "K. Kashino,\n“BYOL for Audio: Self-supervised learning for",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[26]\nF.\nPedregosa,\nG.\nVaroquaux,\nA.\nGramfort,\nV\n. Michel,"
        },
        {
          "6. REFERENCES": "general-purpose audio representation,” in IJCNN, 2021.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,"
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "V\n. Dubourg, et al., “Scikit-learn: Machine learning in Python,”"
        },
        {
          "6. REFERENCES": "[8]\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li,\nand L. Fei-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "J. Mach. Learn. Res., vol. 12, pp. 2825–2830, 2011."
        },
        {
          "6. REFERENCES": "Fei,\n“Imagenet: A large-scale hierarchical\nimage database,”",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[27] B. Schuller, A. Batliner, C. Bergler, C. Mascolo,\nJ. Han,"
        },
        {
          "6. REFERENCES": "in CVPR. IEEE, 2009, pp. 248–255.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "I. Lefter, H. Kaya, S. Amiriparian, A. Baird, L. Stappen,"
        },
        {
          "6. REFERENCES": "[9] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bow-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "et al., “The INTERSPEECH 2021 Computational Paralinguis-"
        },
        {
          "6. REFERENCES": "man,\n“GLUE: A multi-task benchmark and analysis platform",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "tics Challenge: COVID-19 cough, COVID-19 speech, escala-"
        },
        {
          "6. REFERENCES": "for natural language understanding,” in ICLR, 2018.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "tion & primates,” arXiv preprint arXiv:2102.13468, 2021."
        },
        {
          "6. REFERENCES": "[10]\nJ.\nF. Gemmeke,\nD.\nP.\nEllis,\nD.\nFreedman,\nA.\nJansen,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[28] K. Simonyan and A. Zisserman, “Very deep convolutional net-"
        },
        {
          "6. REFERENCES": "W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter,\n“Audio",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "works for large-scale image recognition,” in ICLR, 2015."
        },
        {
          "6. REFERENCES": "set: An ontology and human-labeled dataset for audio events,”",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[29]\nS. Abu-El-Haija, N. Kothari,\nJ. Lee, P. Natsev, G. Toderici,"
        },
        {
          "6. REFERENCES": "in ICASSP. IEEE, 2017, pp. 776–780.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "B. Varadarajan, and S. Vijayanarasimhan,\n“Youtube-8M: A"
        },
        {
          "6. REFERENCES": "[11]\nS. Hershey,\nS. Chaudhuri, D.\nP.\nEllis,\nJ.\nF. Gemmeke,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "arXiv preprint\nlarge-scale video classiﬁcation benchmark,”"
        },
        {
          "6. REFERENCES": "A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "arXiv:1609.08675, 2016."
        },
        {
          "6. REFERENCES": "B. Seybold, et al.,\n“CNN architectures for\nlarge-scale audio",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[30] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,"
        },
        {
          "6. REFERENCES": "classiﬁcation,” in ICASSP. IEEE, 2017, pp. 131–135.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "T. Weyand, M. Andreetto, and H. Adam,\n“MobileNets: Efﬁ-"
        },
        {
          "6. REFERENCES": "[12] Neural Audio AI,\n“HEAR 2021 NeurIPS Challenge Holistic",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "cient convolutional neural networks for mobile vision applica-"
        },
        {
          "6. REFERENCES": "Evaluation of Audio Representations,” https://neuralaudio.ai/h",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "tions,” arXiv preprint arXiv:1704.04861, 2017."
        },
        {
          "6. REFERENCES": "ear2021-holistic-evaluation-of-audio-representations.html.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[31]\nJ.-B. Grill,\nF. Strub,\nF. Altch´e, C. Tallec,\nP. Richemond,"
        },
        {
          "6. REFERENCES": "[13] H. Cao,\nD. G. Cooper, M. K. Keutmann,\nR. C. Gur,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "E. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G."
        },
        {
          "6. REFERENCES": "A. Nenkova, and R. Verma, “CREMA-D: Crowd-sourced emo-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "Azar, et al.,\n“Bootstrap your own latent - a new approach to"
        },
        {
          "6. REFERENCES": "IEEE Trans. Affect. Com-\ntional multimodal actors dataset,”",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "self-supervised learning,” in NeurIPS, 2020, pp. 21271–21284."
        },
        {
          "6. REFERENCES": "put., vol. 5, no. 4, pp. 377–390, 2014.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[32] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and"
        },
        {
          "6. REFERENCES": "[14]\nS. Haq, P. J. Jackson, and J. Edge, “Speaker-dependent audio-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "L. Zhang, “CvT: Introducing convolutions to vision transform-"
        },
        {
          "6. REFERENCES": "visual emotion recognition,” in AVSP, 2009, pp. 53–58.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "ers,” arXiv preprint arXiv:2103.15808, 2021."
        },
        {
          "6. REFERENCES": "[15] W. Fan, X. Xu, X. Xing, W. Chen, and D. Huang, “LSSED: a",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[33] X.\nZhai,\nJ.\nPuigcerver,\nA.\nKolesnikov,\nP.\nRuyssen,"
        },
        {
          "6. REFERENCES": "large-scale dataset and benchmark for speech emotion recog-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "C. Riquelme, M. Lucic, J. Djolonga, A. S. Pinto, M. Neumann,"
        },
        {
          "6. REFERENCES": "nition,” in ICASSP. IEEE, 2021, pp. 641–645.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "A. Dosovitskiy, et al.,\n“A large-scale study of representation"
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "arXiv\nlearning with the visual\ntask adaptation benchmark,”"
        },
        {
          "6. REFERENCES": "[16] M. Plakal and D. Ellis, “YAMNet,” https://github.com/tensorf",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "low/models/tree/master/research/audioset/yamnet, 2020.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "preprint arXiv:1910.04867, 2019."
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[34] G. Cazenavette and S. Lucey,\n“On the bias against\ninductive"
        },
        {
          "6. REFERENCES": "[17] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "biases,” arXiv preprint arXiv:2105.14077, 2021."
        },
        {
          "6. REFERENCES": "S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “IEMOCAP:",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "Interactive emotional dyadic motion capture database,” Lang.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "[35] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,"
        },
        {
          "6. REFERENCES": "Resour. Eval., vol. 42, no. 4, pp. 335–359, 2008.",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "X.\nZhai,\nT.\nUnterthiner,\nM.\nDehghani,\nM. Minderer,"
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "G. Heigold, S. Gelly, et al., “An image is worth 16x16 words:"
        },
        {
          "6. REFERENCES": "[18] N. Vryzas, R. Kotsakis, A. Liatsou, C. A. Dimoulas,\nand",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        },
        {
          "6. REFERENCES": "",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": "Transformers for image recognition at scale,” in ICLR, 2021."
        },
        {
          "6. REFERENCES": "G. Kalliris,\n“Speech emotion recognition for performance in-",
          "teraction,”\nJ. Audio Eng. Soc., vol. 66, no. 6, pp. 457–467,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing",
      "authors": [
        "B Schuller",
        "A Batliner"
      ],
      "year": "2013",
      "venue": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing"
    },
    {
      "citation_id": "3",
      "title": "Voice stress analysis: A new framework for voice and effort in human performance",
      "authors": [
        "M Van Puyvelde",
        "X Neyt",
        "F Mcglone",
        "N Pattyn"
      ],
      "year": "2018",
      "venue": "Front. Psychol"
    },
    {
      "citation_id": "4",
      "title": "Automatic stress detection in emergency (telephone) calls",
      "authors": [
        "I Lefter",
        "L Rothkrantz",
        "D Van Leeuwen",
        "P Wiggers"
      ],
      "year": "2011",
      "venue": "Int. J. Intell. Defence Support Syst"
    },
    {
      "citation_id": "5",
      "title": "openSMILE: the Munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "6",
      "title": "openXBOW: Introducing the Passau open-source crossmodal bag-of-words toolkit",
      "authors": [
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "7",
      "title": "Towards Learning a Universal Non-Semantic Representation of Speech",
      "authors": [
        "J Shor",
        "A Jansen",
        "R Maor",
        "O Lang",
        "O Tuval",
        "F Quitry",
        "M Tagliasacchi",
        "I Shavitt",
        "D Emanuel",
        "Y Haviv"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "8",
      "title": "BYOL for Audio: Self-supervised learning for general-purpose audio representation",
      "authors": [
        "D Niizumi",
        "D Takeuchi",
        "Y Ohishi",
        "N Harada",
        "K Kashino"
      ],
      "year": "2021",
      "venue": "BYOL for Audio: Self-supervised learning for general-purpose audio representation"
    },
    {
      "citation_id": "9",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "CVPR"
    },
    {
      "citation_id": "10",
      "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "authors": [
        "A Wang",
        "A Singh",
        "J Michael",
        "F Hill",
        "O Levy"
      ],
      "year": "2018",
      "venue": "GLUE: A multi-task benchmark and analysis platform for natural language understanding"
    },
    {
      "citation_id": "11",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "12",
      "title": "CNN architectures for large-scale audio classification",
      "authors": [
        "S Hershey",
        "S Chaudhuri",
        "D Ellis",
        "J Gemmeke",
        "A Jansen",
        "R Moore",
        "M Plakal",
        "D Platt",
        "R Saurous",
        "B Seybold"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "13",
      "title": "HEAR 2021 NeurIPS Challenge Holistic Evaluation of Audio Representations",
      "authors": [
        "Neural Audio"
      ],
      "venue": "HEAR 2021 NeurIPS Challenge Holistic Evaluation of Audio Representations"
    },
    {
      "citation_id": "14",
      "title": "CREMA-D: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "15",
      "title": "Speaker-dependent audiovisual emotion recognition",
      "authors": [
        "S Haq",
        "P Jackson",
        "J Edge"
      ],
      "year": "2009",
      "venue": "AVSP"
    },
    {
      "citation_id": "16",
      "title": "LSSED: a large-scale dataset and benchmark for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "X Xing",
        "W Chen",
        "D Huang"
      ],
      "year": "2021",
      "venue": "ICASSP"
    },
    {
      "citation_id": "17",
      "title": "YAMNet",
      "authors": [
        "M Plakal",
        "D Ellis"
      ],
      "year": "2020",
      "venue": "YAMNet"
    },
    {
      "citation_id": "18",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Eval"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition for performance in-teraction",
      "authors": [
        "N Vryzas",
        "R Kotsakis",
        "A Liatsou",
        "C Dimoulas",
        "G Kalliris"
      ],
      "year": "2018",
      "venue": "J. Audio Eng. Soc"
    },
    {
      "citation_id": "20",
      "title": "A Canadian French emotional speech dataset",
      "authors": [
        "P Gournay",
        "O Lahaie",
        "R Lefebvre"
      ],
      "year": "2018",
      "venue": "Proc. ACM Multimedia Systems"
    },
    {
      "citation_id": "21",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A database of German emotional speech"
    },
    {
      "citation_id": "22",
      "title": "EMOVO corpus: an Italian emotional speech database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "LREC"
    },
    {
      "citation_id": "23",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "24",
      "title": "ShEMO: a largescale validated database for persian speech emotion detection",
      "authors": [
        "O Nezami",
        "P Lou",
        "M Karami"
      ],
      "year": "2019",
      "venue": "Lang. Resour. Eval"
    },
    {
      "citation_id": "25",
      "title": "A multi-task learning framework for emotion recognition using 2D continuous space",
      "authors": [
        "R Xia",
        "Y Liu"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "26",
      "title": "Historical Linguistics: An Introduction",
      "authors": [
        "L Campbell"
      ],
      "year": "2013",
      "venue": "Historical Linguistics: An Introduction"
    },
    {
      "citation_id": "27",
      "title": "Scikit-learn: Machine learning in Python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg"
      ],
      "year": "2011",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "28",
      "title": "The INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19 cough, COVID-19 speech, escalation & primates",
      "authors": [
        "B Schuller",
        "A Batliner",
        "C Bergler",
        "C Mascolo",
        "J Han",
        "I Lefter",
        "H Kaya",
        "S Amiriparian",
        "A Baird",
        "L Stappen"
      ],
      "year": "2021",
      "venue": "The INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19 cough, COVID-19 speech, escalation & primates",
      "arxiv": "arXiv:2102.13468"
    },
    {
      "citation_id": "29",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "ICLR"
    },
    {
      "citation_id": "30",
      "title": "Youtube-8M: A large-scale video classification benchmark",
      "authors": [
        "S Abu-El-Haija",
        "N Kothari",
        "J Lee",
        "P Natsev",
        "G Toderici",
        "B Varadarajan",
        "S Vijayanarasimhan"
      ],
      "year": "2016",
      "venue": "Youtube-8M: A large-scale video classification benchmark",
      "arxiv": "arXiv:1609.08675"
    },
    {
      "citation_id": "31",
      "title": "MobileNets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "A Howard",
        "M Zhu",
        "B Chen",
        "D Kalenichenko",
        "W Wang",
        "T Weyand",
        "M Andreetto",
        "H Adam"
      ],
      "year": "2017",
      "venue": "MobileNets: Efficient convolutional neural networks for mobile vision applications",
      "arxiv": "arXiv:1704.04861"
    },
    {
      "citation_id": "32",
      "title": "Bootstrap your own latent -a new approach to self-supervised learning",
      "authors": [
        "J.-B Grill",
        "F Strub",
        "F Altché",
        "C Tallec",
        "P Richemond",
        "E Buchatskaya",
        "C Doersch",
        "B Pires",
        "Z Guo",
        "M Azar"
      ],
      "venue": "Bootstrap your own latent -a new approach to self-supervised learning"
    },
    {
      "citation_id": "33",
      "title": "CvT: Introducing convolutions to vision transformers",
      "authors": [
        "H Wu",
        "B Xiao",
        "N Codella",
        "M Liu",
        "X Dai",
        "L Yuan",
        "L Zhang"
      ],
      "year": "2021",
      "venue": "CvT: Introducing convolutions to vision transformers",
      "arxiv": "arXiv:2103.15808"
    },
    {
      "citation_id": "34",
      "title": "A large-scale study of representation learning with the visual task adaptation benchmark",
      "authors": [
        "X Zhai",
        "J Puigcerver",
        "A Kolesnikov",
        "P Ruyssen",
        "C Riquelme",
        "M Lucic",
        "J Djolonga",
        "A Pinto",
        "M Neumann",
        "A Dosovitskiy"
      ],
      "year": "2019",
      "venue": "A large-scale study of representation learning with the visual task adaptation benchmark",
      "arxiv": "arXiv:1910.04867"
    },
    {
      "citation_id": "35",
      "title": "On the bias against inductive biases",
      "authors": [
        "G Cazenavette",
        "S Lucey"
      ],
      "year": "2021",
      "venue": "On the bias against inductive biases",
      "arxiv": "arXiv:2105.14077"
    },
    {
      "citation_id": "36",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2021",
      "venue": "ICLR"
    }
  ]
}