{
  "paper_id": "2412.13577v1",
  "title": "Bridge Then Begin Anew: Generating Target-Relevant Intermediate Model For Source-Free Visual Emotion Adaptation",
  "published": "2024-12-18T07:51:35Z",
  "authors": [
    "Jiankun Zhu",
    "Sicheng Zhao",
    "Jing Jiang",
    "Wenbo Tang",
    "Zhaopan Xu",
    "Tingting Han",
    "Pengfei Xu",
    "Hongxun Yao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Visual emotion recognition (VER), which aims at understanding humans' emotional reactions toward different visual stimuli, has attracted increasing attention. Given the subjective and ambiguous characteristics of emotion, annotating a reliable large-scale dataset is hard. For reducing reliance on data labeling, domain adaptation offers an alternative solution by adapting models trained on labeled source data to unlabeled target data. Conventional domain adaptation methods require access to source data. However, due to privacy concerns, source emotional data may be inaccessible. To address this issue, we propose an unexplored task: source-free domain adaptation (SFDA) for VER, which does not have access to source data during the adaptation process. To achieve this, we propose a novel framework termed Bridge then Begin Anew (BBA), which consists of two steps: domain-bridged model generation (DMG) and target-related model adaptation (TMA). First, the DMG bridges cross-domain gaps by generating an intermediate model, avoiding direct alignment between two VER datasets with significant differences. Then, the TMA begins training the target model anew to fit the target structure, avoiding the influence of source-specific knowledge. Extensive experiments are conducted on six SFDA settings for VER. The results demonstrate the effectiveness of BBA, which achieves remarkable performance gains compared with state-of-the-art SFDA methods and outperforms representative unsupervised domain adaptation approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the rapid development of social networks, people have become used to posting images to express their feelings  (Zhao et al. 2017) . Visual emotion recognition (VER) aims to identify human emotions elicited by images  (Zhao et al. 2022b) , attracting increasing attention and playing an essential role in various applications, such as depression detection  (Bokolo and Liu 2023)  and opinion mining  (Razali et al. 2021) . Advanced technologies based on supervised deep neural networks have been proposed to improve the VER performance  (Deng et al. 2024; Cen et al. 2024) . Existing methods mainly follow a supervised pipeline requiring sufficient emotion annotations. However, labeling reliable large-scale datasets for VER tasks is challenging in practical applications because of the intrinsic properties of emotions, such as subjectivity, complexity, and ambiguity  (Zhao et al. 2023) . To reduce the annotation burden, much attention has been devoted to unsupervised domain adaptation (UDA) for VER, which enables the models trained on labeled source data to generalize well on the unlabeled target data  (Zhao et al. 2019b ). However, existing UDA methods require access to the source data during adaptation  (Li, Guo, and Ge 2023; Zhao et al. 2024; Jiang et al. 2024) . Regarding privacy and security concerns, source-free domain adaptation (SFDA) has attracted much interest in dealing with the situation where labeled source data is unavailable  (Li et al. 2024 ).\n\nOn the one hand, although prior SFDA methods are effective for standard classification tasks, they face specific challenges and performance decreases when directly applied to VER. This is because of the large affective gap  (Zhao et al. 2014)  between VER datasets, which arises from annotator variations and the scope of data collection. The distribution gap significantly affects the feature alignment, which in turn reduces the confidence of pseudo-labels. As shown in Figure  1  (a), we use SHOT  (Liang, Hu, and Feng 2020) , a typical SFDA method, to generate pseudo-labels for VER datasets EmoSet → FI and standard classification datasets Real World → Clipart, respectively. The results show that the classification scores for emotional pseudo-labels are relatively low, which increases the risk of misalignment between features and labels, and can lead to a confirmation bias due to the accumulation of errors  (Ding et al. 2023 ). To address this issue, bridging the inter-domain variation to generate reliable pseudo-labels is necessary.\n\nOn the other hand, traditional SFDA methods are finetuned on the source model. However, the unclear class features of the VER dataset make the source model lack robustness. For a better illustration, as shown in Figure  1  (b), we train two ResNet-101 models  (He et al. 2016)  with the same number of categories on EmoSet and Clipart from the Office-Home  (Venkateswara et al. 2017)   We summarize the challenges above: the large affective gap leads to incorrect pseudo-labels, and lack of clarity in the distribution of the VER dataset can lead to overfitting the source domain model. To address these two challenges, we propose a novel SFDA framework for VER, termed Bridge then Begin Anew (BBA). BBA contains two steps: Domain-bridged model generation (DMG) and targetrelated model adaptation (TMA). To improve the confidence of pseudo-labels, DMG generates a bridge model to align emotional data across various domains, thus avoiding the challenges associated with directly fine-tuning emotional data. Moreover, we introduce a clustering-based pseudolabel post-processing and masking strategy to constrain the pseudo-label distribution and explore richer semantic contexts. Then, to eliminate the effects of overfitting the source model, TMA starts training anew, which allows the target model to learn feature relations independently, thus extending the exploration of the target-specific features. Furthermore, in order to better learn about the emotional features, we introduce polarity constraints to enhance the target model's discriminative ability for emotion categories.\n\nThe main contributions of our work are outlined below:\n\n• We propose a new task, i.e., source-free domain adaptation for visual emotion recognition (SFDA-VER), which focuses on the cross-domain transfer of VER without accessing source data during adaptation. • We propose a two-step framework, comprising DMG and TMA, to improve the reliability of pseudo-labels and eliminate the effects of source model overfitting. • We conduct experiments on six SFDA-VER settings, and the results show that our approach outperforms existing state-of-the-art methods by an average of +3.03.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works Visual Emotion Recognition",
      "text": "Deep learning and CNNs have revolutionized VER  (Zhao et al. 2023) . The field evolved from global feature extraction  (You et al. 2015; Zhu et al. 2017; Yang et al. 2018a; Rao, Li, and Xu 2020)  to focusing on emotion-rich local regions and their relationships  (Yang et al. 2018b; Zhao et al. 2019a; Rao et al. 2019; Yao et al. 2019; Zhang et al. 2020) . Recently, CLIP-based approaches have emerged  (Deng et al. 2022 (Deng et al. , 2024;; Cen et al. 2024) . However, all these methods require large amounts of accurately labeled emotional data to train the network. Therefore, domain adaptation methods are needed to alleviate the requirement for a large number of annotations during the training process.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Unsupervised Domain Adaptation For Ver",
      "text": "Due to the significant domain bias in VER datasets,  Zhao et al. (2019b)  introduced CycleEmotionGAN, an unsupervised approach for cross-domain emotion adaptation using emotional consistency loss.  Zhao et al. (2022a)  enhanced this with multi-scale similarity and improved emotional consistency. Considering the privacy constraints that limit direct access to source data for emotion analysis, our study presents a new source-free domain adaptation for VER tasks. It is required to adapt the source emotion recognition model to the target domain without accessing source data. In conventional methods (a), the source domain model is directly fine-tuned to align the source and target domains. This direct adaptation approach can be problematic due to significant differences between the source and target domains, potentially leading to suboptimal performance. In contrast, our Bridge then Begin Anew (BBA) approach (b) introduces a bridge model to generate more reliable pseudo-labels and stimulates the exploration of target domain-specific knowledge.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Source Free Domain Adaptation",
      "text": "The SFDA setting represents a more complex but realistic UDA scenario where source data are unavailable  (Li et al. 2024) . The following are some standard methods: Pseudolabeling is to label the unlabeled target samples based on the predictions of the source model  (Liang et al. 2021; Huang et al. 2022; Xie et al. 2022; Wang et al. 2022; Ahmed, Morerio, and Murino 2022; Liang et al. 2022) . Entropy minimization is a constraint which can be inversely employed to guide the optimization of the model  (Liang, Hu, and Feng 2020; Jeon, Lee, and Kang 2021; Mao et al. 2024; Kothandaraman, Chandra, and Manocha 2021) . In virtual source methods  (Du et al. 2023; Hao, Guo, and Yang 2021) , the variants of Generative Adversarial Networks (GANs)  (Li et al. 2020; Kurmi, Subramanian, and Namboodiri 2021)  are common approaches to construct virtual source data. However, the above methods are insufficient to address the task of VER due to low domain relationships between emotional domains. Unlike existing work, we focus on mining the inherent structure of the target domain.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods Problem Definition",
      "text": "In this paper, we focus on Source-Free Domain Adaptation for Visual Emotion Recognition (SFDA-VER), adapting from a labeled source domain to an unlabeled target domain. Our source dataset contains N S labeled images\n\n, where\n\n, where x i t ∈ X t . The feature spaces X s and Y s differ across domains. For simplicity, the superscript i is henceforth omitted. In particular, during the adaptation process, the data from the source domain is inaccessible; only the source model can be used for adaptation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Overview",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Domain-Bridged Model Generation",
      "text": "It is still a challenge for SFDA to generate high-confident pseudo-labels, especially for tasks with large inter-domain variations like VER. In traditional methods, the source model is directly used to generate pseudo-labels on the target data. These approaches make the confidence of pseudolabels highly dependent on the generalization ability of the source model. Due to category noise in VER data, the source model lacks robustness, resulting in low-confidence pseudolabels. Hence, we propose a bridge model to minimize domain gaps and generate high-confidence pseudo-labels.\n\nIn order to improve interclass distinction and reduce intraclass variability in VER features, we consider mitigating the pseudo-label mismatch problem caused by domain differences. Inspired by k-means, we propose a fused distance clustering method as a label optimization strategy, which imposes additional constraints on the model outputs. Specifically, we apply weighted k-means clustering to compute the centroid of each class within the target domain:\n\nwhere σ k (a) = exp(a k ) i exp(ai) denotes the k-th element in the softmax output of a K-dimensional vector a, ϕ s = f s • g s indicates source model, g s is the source feature extractor.\n\n, σ is the softmax operation, and 1 is the indicator function. After calculating the centroid of each class, we assign each sample to the nearest class as follows:\n\nwhere D f (P, Q) is the distance measure used to calculate the distance between each sample and centroid. Considering different distance metrics focus on various parts, we adopt a fused distance metric, i.e., we calculate Euclidean (D eu ), Cosine (D cos ), and Manhattan (D man ) distances between two features:\n\nThen, we update the category centroids according to:\n\nTo reduce intraclass variability, we consider the difference between VER images and traditional classification images: Unlike standard classification tasks, where different blocks in an image correspond to different semantics, blocks in VER images are usually associated with a unified emotion label. As such, we propose a masking-enhanced framework to leverage emotional consistency across different contexts. As shown in Figure  3 , models are encouraged to focus on local features to more effectively extract emotionally consistent features for better classification. Our method uses the random mask technique. Specifically, given a 2D image I of size H ×W , and the number of blocks to be masked n, which is randomly selected from a given range or distribution, the mask M is defined as:\n\nwhere m ij ∈ {0, 1} is the mask value at position (i, j), and if (i, j) ∈ randomly selected blocks, m ij = 1. Each block has a size of b × b. The masked image x M t is obtained by the element-wise multiplication of M and x t , denoted as:   8 ) and distillation loss L kd in Eq. (  9 ), respectively. better generalize to the target domain, the feature extractor g b is updated while the classifier f b is fixed. Both g b and g s are updated at this step, g b is updated by backpropagation as described in Eq. (  10 ), and g s is updated by EMA:\n\nwhere t is the training step, α e is the smoothing factor, θ s t , θ b t are the parameters of g s , g b . Then, we train the masked image together with the original target domain image to participate in the computation of the self-labeling loss and the knowledge distillation loss:\n\nwhere ŷs t = D f (g s (x t ), c k ) denotes the soft pseudo output of the teacher feature extractor g s after clustering, and c k denotes the final centroid of each class. The total loss function for the DMG is expressed as:\n\nwhere λ is the weight for L sl , controlling the importance of L sl during DMG step.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Target-Related Model Adaptation",
      "text": "As stated earlier, noise in the source data that is unrelated to the discriminative features can cause the source model to overfit, potentially constraining its representation of target features. To overcome it, we argue that the target model should be given sufficient latitude to explore the target feature space from scratch, enabling it to identify novel features. Following recent advances in self-distillation (Kim\n\nwhere D(, ) is a function that measures the distance between two emotion samples. In this paper, D(, )represents the KL divergence. ϕ b updates pseudo-labels by mixing ϕ t outputs, where α t is a momentum hyper-parameter:\n\nIn addition, we note that emotions naturally contain polarized information; categories with matching emotion polarities (positive or negative) show closer associations. To explore global and hierarchical polarity features when learning the target structure, we combine emotion polarity with the loss of self-labeling (SL) and the loss of information maximization (IM). Specifically, the distribution of emotion polarity is detailed as follows:\n\nwhere C pos and C neg represents the positive and negative emotion categories. p pos and p neg symbolize the aggregate predicted likelihood across all positive and negative emotion categories for each sample, and they are collectively referred to p e . Then, we propose a polarized IM loss that incorporates the emotion polarity distribution:\n\n)\n\nwhere L e and L d denote the entropy and diversity loss respectively. p k represents the predicted probability of the k th class, represented by σ(ϕ t (x t )) k . Moreover, we also incorporate the emotion polarity distribution into the SL loss, resulting in the following polarized SL loss:  (17)  where ŷe = arg max p e denotes the polarity pseudo-label. To illustrate the necessity and effectiveness of the proposed emotion polarity loss, we cite a simple example in Figure  4 . We can see that the standard IM loss L im gives the same loss value for different outputs ϕ t (x A t ) and ϕ t (x B t ), which are both 0.93. This is because the standard IM loss only focuses on the entropy of the output vector and overlooks the category relations. However, the polarity loss, which has values of 0.34 and 1.01, imposes an extra constraint on the model output. Furthermore, for the standard SL loss L sl , it is unchanged between the outputs ϕ t (x A t ), ϕ b (x t ) and ϕ t (x B t ), ϕ b (x t ), remaining at 2.07. After including the polarity loss, which are 0.10 and 1.42, the total loss differs. This highlights how the polarity loss considers an additional correlation across categories.\n\nOverall, the total loss for the TMA phase can be expressed as:\n\nwhere L pol = γL sl + δL im , γ and δ are the weights for L sl and L im .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Bba Learning",
      "text": "For training, in the DMG step, we use clustered postprocessed pseudo-labels to guide the training of both the target and masked target data. This approach aims to improve interclass separability and intraclass compactness, thereby generating more reliable pseudo-labels. The loss function is L dmg in Eq. (  10 ). In the TMA step, to avoid the effects of source domain model overfitting, the target model is randomly initialized and guided by DMG-corrected pseudolabels to learn the target structure. The loss function is L tma in Eq. (  18 ). For inference, we only use the target model, and the model size is maintained the same as the source model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments Experimental Settings",
      "text": "Extensive experiments are conducted on four VER datasets: FI  (You et al. 2016) , EmoSet  (Yang et al. 2023) , and Art-Photo  (Machajdik and Hanbury 2010)  are categorized into Mikels' eight emotion categories  (Zhao et al. 2016 ); In contrast, Emotion6 is classified according to the Ekman model's six emotion categories  (Peng et al. 2015) . We employ six SFDA-VER settings for experiments: FI ↔ EmoSet, FI ↔ ArtPhoto, and FI ↔ Emotion6, with the latter considered as binary classification tasks due to their distinct categorical frameworks.\n\nWe compare BBA with the following baselines: 1) Source only, which refers to a basic approach where the model is trained on the source domain and directly applied to the target domain. 2) SFDA methods, which include SHOT  (Liang, Hu, and Feng 2020) , SHOT++  (Liang et al. 2021) , G-SFDA  (Yang et al. 2021) , AaD  (Yang et al. 2022) , DaC  (Zhang et al. 2022b) , DINE  (Liang et al. 2022) , and TPDS  (Tang et al. 2024) . 3) UDA methods, which can utilize source domain data during the adaptation process, including MCC  (Jin et al. 2020) , ELS  (Zhang et al. 2022a ), and MIC  (Hoyer et al. 2023 ). 4) Oracle, which is the ideal scenario where the model is trained and tested within the target domain, representing the upper bound of performance.\n\nIn BBA, the EMA parameter α e of the source network is set to 0.999. The loss weights λ, γ, and δ are set to 0.9, 1, and 0.3 for all six settings, respectively. More information about evaluation metrics and implementation details can be found in the supplementary material.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Comparisons With State-Of-The-Art Methods",
      "text": "The results on FI ↔ EmoSet and FI ↔ Emotion6 are comprehensively detailed in Table  1  and Table 2 , respectively. More results for the other two settings, consisting of the small ArtPhoto dataset, can be found in the supplementary material. Compared to the prior state-of-the-art SFDA methods, BBA achieves average improvements ranging from +2.26 to +7.17 on FI ↔ EmoSet and from +3.81 to",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "In our ablation study, shown in Table  3 , we analyze the effectiveness of each component. We reveal that all modules are effective, with clustering and masking strategies bridging the domain gaps, enhancing feature discrimination, and generating more reliable pseudo-labels.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces a new task termed source-free domain adaptation for visual emotion recognition (SFDA-VER). To address this task, we propose a novel method called BBA, which consists of two steps: domain-bridged model generation (DMG) and target-related model adaptation (TMA). First, the DMG step bridges the source and target domains. The clustering post-processing strategy enhances inter-class separability, while the masking strategy increases intra-class compactness. These improvements bolster the separability of categories and generate more reliable pseudo-labels. Subsequently, the TMA step focuses on the target structure to begin training anew. The emotion polarity loss enhances the model's capabilities in emotion recognition. In summary, BBA addresses the unique challenges of the SFDA task within the VER dataset.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a), we use SHOT (Liang, Hu, and Feng 2020),",
      "page": 1
    },
    {
      "caption": "Figure 1: Illustration of challenges for SFDA-VER tasks. (a) shows the pseudo-label confidence generated by SHOT on the",
      "page": 2
    },
    {
      "caption": "Figure 2: An overview comparison between conventional SFDA methods (a) and our method (b). In conventional methods",
      "page": 3
    },
    {
      "caption": "Figure 2: summarizes our framework, Bridge then Begin",
      "page": 3
    },
    {
      "caption": "Figure 3: , models are encour-",
      "page": 4
    },
    {
      "caption": "Figure 3: Illustration of our masking strategy for feature",
      "page": 4
    },
    {
      "caption": "Figure 4: Numerical examples to validate the effectiveness",
      "page": 5
    },
    {
      "caption": "Figure 4: We can see that the standard IM loss Lim gives the same loss",
      "page": 5
    },
    {
      "caption": "Figure 5: Parameter analysis on EmoSet →FI (best viewed in color). (a): Analysis on λ. (b): Analysis on γ. (c): Analysis on δ.",
      "page": 7
    },
    {
      "caption": "Figure 6: t-SNE visualizations on FI →EmoSet. Different colors represent different classes.",
      "page": 7
    },
    {
      "caption": "Figure 6: visually analyzes the results before and after adap-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 3: Accuracy of different variants of our BBA on six",
      "data": [
        {
          "Method": "",
          "SF": "",
          "EmoSet → FI": "Acc",
          "FI → EmoSet": "Acc"
        },
        {
          "Method": "",
          "SF": "",
          "EmoSet → FI": "",
          "FI → EmoSet": ""
        },
        {
          "Method": "Source only\nOracle",
          "SF": "-\n-",
          "EmoSet → FI": "51.57\n67.32",
          "FI → EmoSet": "50.87\n71.96"
        },
        {
          "Method": "SHOT\nSHOT++\nG-SFDA\nDaC\nAaD\nDINE\nTPDS\nBBA (ours)",
          "SF": "✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓",
          "EmoSet → FI": "50.38\n50.42\n50.47\n50.24\n49.90\n55.25\n51.45\n57.36",
          "FI → EmoSet": "53.31\n54.15\n54.40\n51.55\n52.68\n56.36\n55.88\n58.76"
        },
        {
          "Method": "MCC\nELS\nMIC",
          "SF": "×\n×\n×",
          "EmoSet → FI": "56.58\n56.42\n56.26",
          "FI → EmoSet": "56.26\n56.41\n54.31"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: Accuracy of different variants of our BBA on six",
      "data": [
        {
          "Method": "",
          "SF": "",
          "Emotion6 → FI": "Acc",
          "FI → Emotion6": "Acc"
        },
        {
          "Method": "",
          "SF": "",
          "Emotion6 → FI": "",
          "FI → Emotion6": ""
        },
        {
          "Method": "Source only\nOracle",
          "SF": "-\n-",
          "Emotion6 → FI": "68.18\n89.20",
          "FI → Emotion6": "70.66\n82.99"
        },
        {
          "Method": "SHOT\nSHOT++\nG-SFDA\nDaC\nAaD\nDINE\nTPDS\nBBA (ours)",
          "SF": "✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓",
          "Emotion6 → FI": "67.22\n68.71\n65.39\n64.43\n70.04\n71.71\n72.42\n78.12",
          "FI → Emotion6": "67.90\n67.25\n66.89\n65.93\n67.84\n71.32\n63.23\n73.23"
        },
        {
          "Method": "MCC\nELS\nMIC",
          "SF": "×\n×\n×",
          "Emotion6 → FI": "74.04\n73.12\n71.94",
          "FI → Emotion6": "68.68\n71.32\n65.21"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Cleaning noisy labels by negative ensemble learning for source-free unsupervised domain adaptation",
      "authors": [
        "W Ahmed",
        "P Morerio",
        "V Murino"
      ],
      "year": "2022",
      "venue": "WACV"
    },
    {
      "citation_id": "2",
      "title": "Deep learning-based depression detection from social media: Comparative evaluation of ml and transformer techniques",
      "authors": [
        "B Bokolo",
        "Q Liu"
      ],
      "year": "2023",
      "venue": "Electronics"
    },
    {
      "citation_id": "3",
      "title": "MASANet: Multi-Aspect Semantic Auxiliary Network for Visual Sentiment Analysis",
      "authors": [
        "J Cen",
        "C Qing",
        "H Ou",
        "X Xu",
        "J Tan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Simple but powerful, a language-supervised method for image emotion classification",
      "authors": [
        "S Deng",
        "L Wu",
        "G Shi",
        "L Xing",
        "W Hu",
        "H Zhang",
        "Y Xiang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Learning to compose diversified prompts for image emotion classification",
      "authors": [
        "S Deng",
        "L Wu",
        "G Shi",
        "L Xing",
        "M Jian",
        "Y Xiang",
        "R Dong"
      ],
      "year": "2024",
      "venue": "Computational Visual Media"
    },
    {
      "citation_id": "6",
      "title": "Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment",
      "authors": [
        "Q Ding",
        "J Yin",
        "D Zhang",
        "J Gao"
      ],
      "year": "2023",
      "venue": "Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment",
      "arxiv": "arXiv:2307.02075"
    },
    {
      "citation_id": "7",
      "title": "Generation, augmentation, and alignment: A pseudo-source domain based method for source-free domain adaptation",
      "authors": [
        "Y Du",
        "H Yang",
        "M Chen",
        "H Luo",
        "J Jiang",
        "Y Xin",
        "C Wang"
      ],
      "year": "2023",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "8",
      "title": "Source-free Unsupervised Domain Adaptation with Surrogate Data Generation",
      "authors": [
        "Y Hao",
        "Y Guo",
        "C Yang"
      ],
      "year": "0198",
      "venue": "BMVC"
    },
    {
      "citation_id": "9",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "10",
      "title": "MIC: Masked image consistency for context-enhanced domain adaptation",
      "authors": [
        "L Hoyer",
        "D Dai",
        "H Wang",
        "L Van Gool"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "11",
      "title": "Relative alignment network for source-free multimodal video domain adaptation",
      "authors": [
        "Y Huang",
        "X Yang",
        "J Zhang",
        "C Xu"
      ],
      "year": "2022",
      "venue": "ACM MM"
    },
    {
      "citation_id": "12",
      "title": "Unsupervised multisource domain adaptation with no observable source data",
      "authors": [
        "H Jeon",
        "S Lee",
        "U Kang"
      ],
      "year": "2021",
      "venue": "PloS one"
    },
    {
      "citation_id": "13",
      "title": "Multi-source Domain Adaptation for Panoramic Semantic Segmentation",
      "authors": [
        "J Jiang",
        "S Zhao",
        "J Zhu",
        "W Tang",
        "Z Xu",
        "J Yang",
        "P Xu",
        "H Yao"
      ],
      "year": "2024",
      "venue": "Multi-source Domain Adaptation for Panoramic Semantic Segmentation",
      "arxiv": "arXiv:2408.16469"
    },
    {
      "citation_id": "14",
      "title": "Minimum class confusion for versatile domain adaptation",
      "authors": [
        "Y Jin",
        "X Wang",
        "M Long",
        "J Wang"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "15",
      "title": "Selfknowledge distillation with progressive refinement of targets",
      "authors": [
        "K Kim",
        "B Ji",
        "D Yoon",
        "S Hwang"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "16",
      "title": "SS-SFDA: Self-supervised source-free domain adaptation for road segmentation in hazardous environments",
      "authors": [
        "D Kothandaraman",
        "R Chandra",
        "D Manocha"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "17",
      "title": "Domain impression: A source data free domain adaptation method",
      "authors": [
        "V Kurmi",
        "V Subramanian",
        "V Namboodiri"
      ],
      "year": "2021",
      "venue": "WACV"
    },
    {
      "citation_id": "18",
      "title": "Temporal Ensembling for Semi-Supervised Learning",
      "authors": [
        "S Laine",
        "T Aila"
      ],
      "year": "2016",
      "venue": "Temporal Ensembling for Semi-Supervised Learning"
    },
    {
      "citation_id": "19",
      "title": "Model adaptation: Unsupervised domain adaptation without source data",
      "authors": [
        "J Li",
        "Z Yu",
        "Z Du",
        "L Zhu",
        "H Shen",
        "R Li",
        "Q Jiao",
        "W Cao",
        "H.-S Wong",
        "S Wu"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "20",
      "title": "Pseudo labels for unsupervised domain adaptation: A review",
      "authors": [
        "Y Li",
        "L Guo",
        "Y Ge"
      ],
      "year": "2023",
      "venue": "Electronics"
    },
    {
      "citation_id": "21",
      "title": "Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation",
      "authors": [
        "J Liang",
        "D Hu",
        "J Feng"
      ],
      "year": "2020",
      "venue": "ICML"
    },
    {
      "citation_id": "22",
      "title": "Dine: Domain adaptation from single and multiple black-box predictors",
      "authors": [
        "J Liang",
        "D Hu",
        "J Feng",
        "R He"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "23",
      "title": "Source data-absent unsupervised domain adaptation through hypothesis transfer and labeling transfer",
      "authors": [
        "J Liang",
        "D Hu",
        "Y Wang",
        "R He",
        "J Feng"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "24",
      "title": "Affective image classification using features inspired by psychology and art theory",
      "authors": [
        "J Machajdik",
        "A Hanbury"
      ],
      "year": "2010",
      "venue": "ACM MM"
    },
    {
      "citation_id": "25",
      "title": "Source Free Graph Unsupervised Domain Adaptation",
      "authors": [
        "H Mao",
        "L Du",
        "Y Zheng",
        "Q Fu",
        "Z Li",
        "X Chen",
        "S Han",
        "D Zhang",
        "K.-C Peng",
        "T Chen",
        "A Sadovnik",
        "A Gallagher"
      ],
      "year": "2015",
      "venue": "Proceedings of the 17th ACM International Conference on Web Search and Data Mining"
    },
    {
      "citation_id": "26",
      "title": "Learning multi-level deep representations for image emotion classification",
      "authors": [
        "T Rao",
        "X Li",
        "M Xu"
      ],
      "year": "2020",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "27",
      "title": "Multi-level region-based convolutional neural network for image emotion classification",
      "authors": [
        "T Rao",
        "X Li",
        "H Zhang",
        "M Xu"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "28",
      "title": "Opinion mining for national security: techniques, domain applications, challenges and research opportunities",
      "authors": [
        "N Razali",
        "N Malizan",
        "N Hasbullah",
        "M Wook",
        "N Zainuddin",
        "K Ishak",
        "S Ramli",
        "S Sukardi"
      ],
      "year": "2021",
      "venue": "Journal of Big Data"
    },
    {
      "citation_id": "29",
      "title": "Source-free domain adaptation via target prediction distribution searching",
      "authors": [
        "S Tang",
        "A Chang",
        "F Zhang",
        "X Zhu",
        "M Ye",
        "C Zhang"
      ],
      "year": "2024",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "30",
      "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
      "authors": [
        "A Tarvainen",
        "H Valpola"
      ],
      "year": "2017",
      "venue": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results"
    },
    {
      "citation_id": "31",
      "title": "Deep hashing network for unsupervised domain adaptation",
      "authors": [
        "H Venkateswara",
        "J Eusebio",
        "S Chakraborty",
        "S Panchanathan"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "32",
      "title": "Cross-domain contrastive learning for unsupervised domain adaptation",
      "authors": [
        "R Wang",
        "Z Wu",
        "Z Weng",
        "J Chen",
        "G.-J Qi",
        "Y.-G Jiang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Active learning for domain adaptation: An energybased approach",
      "authors": [
        "B Xie",
        "L Yuan",
        "S Li",
        "C Liu",
        "X Cheng",
        "G Wang"
      ],
      "year": "2022",
      "venue": "AAAI"
    },
    {
      "citation_id": "34",
      "title": "EmoSet: A large-scale visual emotion dataset with rich attributes",
      "authors": [
        "J Yang",
        "Q Huang",
        "T Ding",
        "D Lischinski",
        "D Cohen-Or",
        "H Huang"
      ],
      "year": "2023",
      "venue": "ICCV"
    },
    {
      "citation_id": "35",
      "title": "Retrieving and classifying affective images via deep metric learning",
      "authors": [
        "J Yang",
        "D She",
        "Y.-K Lai",
        "M.-H Yang"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "36",
      "title": "Visual sentiment prediction based on automatic discovery of affective regions",
      "authors": [
        "J Yang",
        "D She",
        "M Sun",
        "M.-M Cheng",
        "P Rosin",
        "L Wang"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Attracting and dispersing: A simple approach for source-free domain adaptation",
      "authors": [
        "S Yang",
        "S Jui",
        "J Van De Weijer"
      ],
      "year": "2022",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "38",
      "title": "Exploiting the intrinsic neighborhood structure for sourcefree domain adaptation",
      "authors": [
        "S Yang",
        "J Van De Weijer",
        "L Herranz",
        "S Jui"
      ],
      "year": "2021",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "39",
      "title": "Attention-aware polarity sensitive embedding for affective image retrieval",
      "authors": [
        "X Yao",
        "D She",
        "S Zhao",
        "J Liang",
        "Y.-K Lai",
        "J Yang"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "40",
      "title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks",
      "authors": [
        "Q You",
        "J Luo",
        "H Jin",
        "J Yang"
      ],
      "year": "2015",
      "venue": "AAAI"
    },
    {
      "citation_id": "41",
      "title": "Building a large scale dataset for image emotion recognition: The fine print and the benchmark",
      "authors": [
        "Q You",
        "J Luo",
        "H Jin",
        "J Yang"
      ],
      "year": "2016",
      "venue": "AAAI"
    },
    {
      "citation_id": "42",
      "title": "Object semantics sentiment correlation analysis enhanced image sentiment classification",
      "authors": [
        "J Zhang",
        "M Chen",
        "H Sun",
        "D Li",
        "Z Wang"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "43",
      "title": "2022a. Free Lunch for Domain Adversarial Training: Environment Label Smoothing",
      "authors": [
        "Y Zhang",
        "J Liang",
        "Z Zhang",
        "L Wang",
        "R Jin",
        "T Tan"
      ],
      "venue": "ICLR"
    },
    {
      "citation_id": "44",
      "title": "2022b. Divide and contrast: Source-free domain adaptation via adaptive contrastive learning",
      "authors": [
        "Z Zhang",
        "W Chen",
        "H Cheng",
        "Z Li",
        "S Li",
        "L Lin",
        "G Li"
      ],
      "venue": "NeurIPS"
    },
    {
      "citation_id": "45",
      "title": "2022a. Emotional semantics-preserved and feature-aligned cyclegan for visual emotion adaptation",
      "authors": [
        "S Zhao",
        "X Chen",
        "X Yue",
        "C Lin",
        "P Xu",
        "R Krishna",
        "J Yang",
        "G Ding",
        "A Sangiovanni-Vincentelli",
        "K Keutzer"
      ],
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "46",
      "title": "Realtime multimedia social event detection in microblog",
      "authors": [
        "S Zhao",
        "Y Gao",
        "G Ding",
        "T.-S Chua"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "47",
      "title": "Exploring principles-of-art features for image emotion recognition",
      "authors": [
        "S Zhao",
        "Y Gao",
        "X Jiang",
        "H Yao",
        "T.-S Chua",
        "X Sun"
      ],
      "year": "2014",
      "venue": "ACM MM"
    },
    {
      "citation_id": "48",
      "title": "Toward Label-Efficient Emotion and Sentiment Analysis",
      "authors": [
        "S Zhao",
        "X Hong",
        "J Yang",
        "Y Zhao",
        "G Ding"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "49",
      "title": "PDANet: Polarity-consistent deep attention network for fine-grained visual emotion regression",
      "authors": [
        "S Zhao",
        "Z Jia",
        "H Chen",
        "L Li",
        "G Ding",
        "K Keutzer"
      ],
      "year": "2019",
      "venue": "ACM MM"
    },
    {
      "citation_id": "50",
      "title": "Multisource multi-modal domain adaptation",
      "authors": [
        "S Zhao",
        "J Jiang",
        "W Tang",
        "J Zhu",
        "H Chen",
        "P Xu",
        "B Schuller",
        "J Tao",
        "H Yao",
        "G Ding"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "51",
      "title": "Cycleemotiongan: Emotional semantic consistency preserved cyclegan for adapting image emotions",
      "authors": [
        "S Zhao",
        "C Lin",
        "P Xu",
        "S Zhao",
        "Y Guo",
        "R Krishna",
        "G Ding",
        "K Keutzer"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "52",
      "title": "Predicting personalized emotion perceptions of social images",
      "authors": [
        "S Zhao",
        "H Yao",
        "Y Gao",
        "R Ji",
        "W Xie",
        "X Jiang",
        "T.-S Chua"
      ],
      "year": "2016",
      "venue": "ACM MM"
    },
    {
      "citation_id": "53",
      "title": "Affective image content analysis: Two decades review and new perspectives",
      "authors": [
        "S Zhao",
        "X Yao",
        "J Yang",
        "G Jia",
        "G Ding",
        "T.-S Chua",
        "B Schuller",
        "K Keutzer"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "54",
      "title": "Dependency Exploitation: A Unified CNN-RNN Approach for Visual Emotion Recognition",
      "authors": [
        "X Zhu",
        "L Li",
        "W Zhang",
        "T Rao",
        "M Xu",
        "Q Huang",
        "D Xu"
      ],
      "year": "2017",
      "venue": "IJCAI"
    }
  ]
}