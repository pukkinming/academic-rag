{
  "paper_id": "2303.10335v2",
  "title": "Multimodal Continuous Emotion Recognition: A Technical Report For Abaw5",
  "published": "2023-03-18T04:50:07Z",
  "authors": [
    "Su Zhang",
    "Ziyuan Zhao",
    "Cuntai Guan"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We used two multimodal models for continuous valencearousal recognition using visual, audio, and linguistic information. The first model is the same as we used in ABAW2 and ABAW3, which employs the leader-follower attention. The second model has the same architecture for spatial and temporal encoding. As for the fusion block, it employs a compact and straightforward channel attention, borrowed from the End2You toolkit. Unlike our previous attempts that use Vggish feature directly as the audio feature, this time we feed the pre-trained VGG model using logmel-spectrogram and finetune it during the training. To make full use of the data and alleviate over-fitting, cross-validation is carried out. The code is available at https://github.com/sucv/ABAW3.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is the process of identifying human emotion. It plays a crucial role in behavioral modeling, human-computer interaction, and affective computing. By using the dimensional model  [18] , any emotional state can be taken as a point located in a continuous space, with the valence and arousal being the axes. Continuous emotion recognition seeks to map the N sequential data points into M sequential emotional state points, where M usually equals N . The term \"continuous\" is endowed with two distinctive features. Spatially, it seeks to represent the emotional state as a point with continuous numerical values in the multi-dimensional space of the dimensional theory, as opposed to assigning categorical labels. Temporally, it incessantly forecasts the emotional state for a predetermined duration, forming a continuous record of the subject's emotional trajectory over a specified period.\n\nThe task of Continuous Emotion Recognition (CER) is challenging, mainly due to the following reasons. Firstly, the complexity of emotions over an extended period can be seen as a composition of multiple facial muscular movements, which can be defined as one-actions  [5]  using the Facial Action Coding System (FACS)  [19] . However, despite its atomic nature, human emotions, irrespective of the modality, typically exhibit large variations in their intensity, order, and duration, and take longer to unfold. Consequently, there is a need for models that can learn longrange temporal dependencies to address this issue. Secondly, emotions are highly subjective and vary significantly depending on individual experiences, making their perception susceptible to personal bias. For instance, children who have been subjected to physical abuse can detect signs of anger much faster than their counterparts  [15] . Therefore, the data collected from subjects and the ground truth provided by annotators are prone to individual biases. One promising technique to mitigate this issue is multimodality, which involves the integration of visual, audio, and physiological data to develop reliable CER models.\n\nIn order to address the challenges posed by temporal dynamic learning and cross-subject generality, we adopt two strategies: large window resampling and multimodal fusion. Large window resampling involves selecting a window size that determines the amount of contextual information the model considers to make a prediction for each time step. On the other hand, multimodal fusion is a widely accepted technique that leverages the complementary nature of multiple data modalities to improve prediction accuracy. This is because multimodal data can disambiguate conflicting emotional cues, such as a crying face with joyful vocal expressions, or a neutral face with a harsh intonation. Additionally, multimodal data can enhance recognition robustness in real-world scenarios characterized by uncontrolled environments, subject diversity, various illumination conditions, spontaneous behaviors, background noise, and unclear speech. For instance, when an actor's face is not visible, their voice or body gesture can serve as an emotional cue to support ongoing learning. This report details our methodology for the valencearousal estimation challenge from the fifth affective behav-Figure  1 . The architecture of our proposed model. The model consists of four components, i.e., the visual, audio, linguistic, and coattention blocks. The visual block has a cascaded 2DCNN-TCN structure, and the audio and linguistic blocks each contain a TCN. The three branches yield three independent spatial-temporal feature vectors. They are then fed to the attentive fusion block. Three independent attention encoders are used. For the i-th branch, its encoder consists of three independent linear layers, they adjust the dimension of the feature vector producing a query Qi, a key Ki, and a value Vi. They are then regrouped and concatenated to form the cross-modal counterparts.\n\nior analysis in-the-wild (ABAW5) workshop  [6-9, 11-14, 25] . ABAW5 aims for the affective behavior analysis in-thewild. The valence-arousal estimation sub-challenge bases on the extended version of the Aff-Wild2 database, in which the valence and arousal are annotated continuously for each video frame. Our work is an extension of the last year's attempt  [26]  on ABAW5  [14] . The extension are twofold. First, a pre-trained audio VGGnet is employed to serve as the audio backbone for the vggish  [4]  extraction on-thego. Thus, the expressiveness of deep feature on visual and audio modalities can be further improved through finetuning. Second, a compact and straightforward fusion block is borrowed from the End2You toolkit, in which the channel attention is employed to fuse the multimodal information. The results of using leader-follower attention and channel attention are reported.\n\nThe remainder of the paper is arranged as follows. Section 2 details the model architecture including the visual, aural, linguistic, and attentive fusion blocks. Section 3 elaborates the implementation details including the data preprocessing, training settings, and post-processing. Section 4 provides the continuous emotion recognition results. Section 5 concludes the work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Architecture",
      "text": "There are two architectures for our model, the leaderfollower attention network (LFAN) and channel attention network (CAN). Both of them consists of independent branches to extract the spatiotemporal encoding for each modality, followed by the fusion block using leaderfollower attention and channel attention, respectively. For each branch, a CNN backbone is employed to extract the spatial deep feature on-the-go for video frames and logmelspectrogram, followed by a temporal convolutional network (TCN)  [1]  to further learn the spatiotemporal encoding. For the linguistic branch, the pre-extracted bert features  [17]  are taken as the input to feed the TCN. For branches fed by lowlevel features, no backbone is employed as well. The illustration of LFAN using video frames, logmel-spectrograms, and speech tokens are shown in Fig 1 . As for the details of the channel attention-based fusion block, please refer to the End2You paper  [22]  (code).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Implementation Details",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Database",
      "text": "The ABAW5 competition uses the Aff-Wild2 database. The corpora of the valence-arousal estimation subchallenge includes 564 trials. The database is split into the training, validation and test sets. The partitioning is done in a subject independent manner so that any subject's data are included in only one partition. The partitioning produces 341, 71, and 152 trials for the training, validation, and test sets. Four experts annotate the videos using the method proposed in  [2] . In addition to the annotations (for the training and validation sets only) and the raw videos, the bounding boxes and landmarks for each raw video are also available to the participants.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Preprocessing",
      "text": "The visual preprocessing is carried out as follows. The cropped-aligned image data provided by the organizer are used. All the images are resized to 48 × 48 × 3 as a compromise to limited computational power. Given a trial from the training or validation set, the length N is determined by the number of the rows of the annotation text file which does not include -5. For the test set, the length N is determined by the frame number of the raw video. A zero matrix B of size N × 48 × 48 × 3 is initialized and then iterated over the rows. For the i-th row of B, it is assigned as the i-th jpg image if it exists, otherwise doing nothing.\n\nThe audio preprocessing firstly converts all the videos to mono with a 16K sampling rate in wav format. The logmelspectrogram are then extracted using the preprocessing code from the Vggish repository  1  . The only change is that we specified the hop length to be 1/f ps of the raw video, in order to synchronize with other modalities and annotations.\n\nThe linguistic preprocessing is carried out as follows. The mono wav file obtained from the audio preprocessing is fed to a pretrained speech recognition model from the Vosk toolkit  2  , from which the recognized words and the wordlevel timestamp are obtained. The recognized words are then fed to a pretrained punctuation model from Hugging Face  3  . After which a pretrained BERT model from the Pytorch library is employed to extract the word-level linguistic features. The linguistic features are obtained by summing together the last four layers of the BERT model  [21] . To synchronize, the word-level linguistic features are populated according to the timestamp of each word and each frame. Specifically, a word usually has a larger time span than that for a frame. Therefore, for one word, its feature is repetitively assigned to the time steps of all the frames within the time span.\n\nFor the valence-arousal labels, all the rows containing -5 are excluded. To ensure that the features have the same length as the corresponding trial, the feature matrices are either repeatedly padded (using the last feature points) or trimmed (starting from the rear), depending on whether the feature length is shorter than the trial length or not, respectively.\n\nThe audio preprocessing firstly converts all the videos to mono with a 16K sampling rate in wav format. After which, the logmel-spectrogram is extracted following the Vggish preprocessing code  4  . Note that the hop length for these features are set to be 1/f ps of the raw video, in order to synchronize with other modalities and annotations.\n\nThe linguistic preprocessing is carried out as follows. The mono wav file obtained from the audio preprocessing is fed to a pretrained speech recognition model from the Vosk toolkit  5  , from which the recognized words and the word-level timestamp are obtained. The recognized words are then fed to a pretrained punctuation and capitalization model from the deepmultilingualpunctuation toolkit  6  . After which a pretrained BERT model from the Pytorch library is employed to extract the word-level linguistic features. The linguistic features are obtained by summing together the last four layers of the BERT model  [21] . To synchronize, the word-level linguistic features are populated according to the timestamp of each word and each frame. Specifically, a word usually has a larger time span than that for a frame. Therefore, for one word, its feature is repetitively assigned to the time steps of all the frames within the time span.\n\nFor the valence-arousal labels, all the rows containing -5 are excluded. To ensure that the features and annotations have the same length, the feature matrices are either repeatedly padded (using the last feature points) or trimmed, depending on whether the feature length is shorter and longer than the trial length, respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Expansion",
      "text": "The AffWild2 database employed by ABAW5 contains 360, 72, and 162 trials in the training, validation, and testing sets, respectively. To make full use of the available data and alleviate over-fitting, 6-fold cross-validation is employed. By evenly splitting the training set into 5 folds, we have 6 × 72 trials in total. Note that the 0-th fold is exactly the original data partitioning. And there is no subject overlap across different folds. Moreover, during training and validation, the resampling window has a 33% overlap, resulting in 33% more data.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training",
      "text": "The batch size is 12. For each batch, the resampling window length and hop length are 300 and 200, respectively. I.e., the dataloader loads consecutive 300 feature points to form a minibatch, with a stride of 200. For any trials having feature points smaller than the window length, zero padding is employed. For visual data, the random flip, random crop with a size of 40 are employed for training and only the center crop is employed for validation. The data are then normalized so that mean = std = 0.5. For bert features, they are normalized so that mean = 0 and std = 1.\n\nThe CCC loss is used as the loss function. The Adam optimizer with a weight decay of 0.001 is employed. The maximal epoch number and early stopping counter are set to 100 and 20, respectively. The learning rate (LR) and minimal learning rate (MLR) are set to 1e -5 and 1e -8, respectively. The warmup scheme with ReduceLROnPlateau scheduler with a patience of 5 and factor of 0.1 is employed based on the validation CCC. Three groups of layers for the visual Resnet50 backbone and audio VGG backbone are manually selected for further fine-tuning. When epoch= 0, the first group is unfrozen. The learning rate is then linearly warmed up to 1e -5 within an epoch. The repetitive warm-up is carried out until epoch= 5. After which the ReduceLROn-Plateau takes over to update the learning rate. It gradually drops the learning rate in a factor of 0.1 should no higher validation CCC appears for a consecutive 5 epochs. When the learning rate is already 1e -8 and no higher validation CCC appears for the latest 5 epochs, the second group is unfrozen, the learning rate is also reset to 1e -5 before undergoing the warm-up scheme and ReduceLROnPlateau again. The procedure is repeated until no groups are available to unfrozen. Note that at the end of each epoch, the current best model state dictionary (i.e., the one with the greatest validation CCC) is loaded. By doing so, when there is no improvement on validation CCC, the training CCC cannot keep increasing and ends up with over-fitting.\n\nNote that unlike our previous attempts on ABAW2 and ABAW3, no post-processing is employed. Given the results from 6 folds, we simply choose the ones with the highest validation CCC to submit. Also note that the results for any fold are selected from 15 instances using different seeds.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Result And Discussion",
      "text": "The validation results of our methods against the baseline are reported in Table  1 . Note that for one emotion type, the reported result for each fold is the best instance selected from 15 instances using different seeds. Finally, the best results of Fold 0 from CAN and LFAN, together with other 3 best folds (measured in the average CCC) from CAN, are submitted to ABAW5.\n\nIn this study, two multimodal models are compared, namely CAN and LFAN, with the former utilizing only video frames and logmel-spectrograms while the latter incorporates an additional Bert feature. Validation results indicate that the CAN model with two modalities outperforms the LFAN model with three modalities, achieving a higher CCC score in Fold 0. It is worth noting that attempts to in-   [23]  0.504 0.428 0.466 SCLAB CNU  [16]  0.458 0.470 0.464 USTC-AC  [24]  0.325 0.232 0.278 Baseline  [10]  0.211 0.191 0.201 clude the Bert feature as a third modality in the CAN model did not result in improvement. This suggests that the current linguistic information encoded by the pre-trained Bert model does not provide useful information to the model, or at least not in its current form. The issue lies in the fact that the length of tokens is not equivalent to the length of the sampling window, rendering fine-tuning impossible during training. Future research is needed to explore how linguistic information can be effectively incorporated into the fusion process. Furthermore, the study highlights the efficacy of fine-tuning. Both Resnet50 and VGG16 models, trained using large public datasets and fed by video frames and logmel-spectrograms, show improvements in performance when the last few layers are fine-tuned during training. This improvement can be as high as 10% or more, compared to models that use pre-extracted CNN and Vggish features, albeit at a cost of 300% -500% longer training time.\n\nThe best results from all the teams are reported in Table  2 .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In our study, we aimed to develop effective models for continuous valence-arousal recognition, utilizing multimodal information from visual, audio, and linguistic sources. To overcome challenges posed by temporal dynamic learning and cross-subject generality, we employed two strategies: large window resampling and multimodal fusion. Specifically, we designed the CAN and LFAN models, with a resampling window length and hop length of 300 and 200, respectively, and multimodal fusion block to effectively integrate information from different modalities. Moving forward, our research will explore the incorporation of transformer-based networks for improved temporal dynamics learning.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of our proposed model. The model consists of four components, i.e., the visual, audio, linguistic, and co-",
      "page": 2
    },
    {
      "caption": "Figure 1: As for the details of",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "SituTech",
          "Valence": "0.619",
          "Arousal": "0.663",
          "Mean": "0.641"
        },
        {
          "Method": "Netease Fuxi Virtual Human [27]",
          "Valence": "0.649",
          "Arousal": "0.626",
          "Mean": "0.637"
        },
        {
          "Method": "Ours",
          "Valence": "0.553",
          "Arousal": "0.630",
          "Mean": "0.591"
        },
        {
          "Method": "CtyunAI [29]",
          "Valence": "0.501",
          "Arousal": "0.633",
          "Mean": "0.567"
        },
        {
          "Method": "HFUT-MAC [28]",
          "Valence": "0.523",
          "Arousal": "0.545",
          "Mean": "0.534"
        },
        {
          "Method": "HSE-NN-SberAI [20]",
          "Valence": "0.482",
          "Arousal": "0.528",
          "Mean": "0.505"
        },
        {
          "Method": "ACCC",
          "Valence": "0.462",
          "Arousal": "0.506",
          "Mean": "0.482"
        },
        {
          "Method": "PRL [23]",
          "Valence": "0.504",
          "Arousal": "0.428",
          "Mean": "0.466"
        },
        {
          "Method": "SCLAB CNU [16]",
          "Valence": "0.458",
          "Arousal": "0.470",
          "Mean": "0.464"
        },
        {
          "Method": "USTC-AC [24]",
          "Valence": "0.325",
          "Arousal": "0.232",
          "Mean": "0.278"
        },
        {
          "Method": "Baseline [10]",
          "Valence": "0.211",
          "Arousal": "0.191",
          "Mean": "0.201"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "authors": [
        "Shaojie Bai",
        "Zico Kolter",
        "Vladlen Koltun"
      ],
      "year": "2018",
      "venue": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "arxiv": "arXiv:1803.01271"
    },
    {
      "citation_id": "2",
      "title": "feeltrace': An instrument for recording perceived emotion in real time",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie",
        "Susie Savvidou",
        "Edelle Mcmahon",
        "Martin Sawey",
        "Marc Schröder"
      ],
      "year": "2000",
      "venue": "ISCA tutorial and research workshop (ITRW) on speech and emotion"
    },
    {
      "citation_id": "3",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "4",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "Shawn Hershey",
        "Sourish Chaudhuri",
        "P Daniel",
        "Ellis",
        "Aren Jort F Gemmeke",
        "R Channing Jansen",
        "Manoj Moore",
        "Devin Plakal",
        "Rif Platt",
        "Bryan Saurous",
        "Seybold"
      ],
      "year": "2017",
      "venue": "2017 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "5",
      "title": "Timeception for complex action recognition",
      "authors": [
        "Noureldien Hussein",
        "Efstratios Gavves",
        "Arnold Smeulders"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "arxiv": "arXiv:2202.10659"
    },
    {
      "citation_id": "7",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "8",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "9",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "10",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges"
    },
    {
      "citation_id": "11",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "13",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "14",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "Psychology",
      "authors": [
        "G David",
        "Myers"
      ],
      "year": "2004",
      "venue": "Psychology"
    },
    {
      "citation_id": "16",
      "title": "A transformer-based approach to video frame-level prediction in affective behaviour analysis in-thewild",
      "authors": [
        "Dang-Khanh Nguyen",
        "Ngoc-Huynh Ho",
        "Sudarshan Pant",
        "Hyung-Jeong Yang"
      ],
      "year": "2023",
      "venue": "A transformer-based approach to video frame-level prediction in affective behaviour analysis in-thewild"
    },
    {
      "citation_id": "17",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "18",
      "title": "Static and dynamic 3d facial expression recognition: A comprehensive survey",
      "authors": [
        "Georgia Sandbach",
        "Stefanos Zafeiriou",
        "Maja Pantic",
        "Lijun Yin"
      ],
      "year": "2012",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "19",
      "title": "Domain adaptive representation learning for facial action unit recognition",
      "authors": [
        "Nishant Sankaran",
        "Deen Dayal Mohan",
        "N Nagashri",
        "Srirangaraj Lakshminarayana",
        "Venu Setlur",
        "Govindaraju"
      ],
      "year": "2020",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Emotieffnet facial features in uni-task emotion recognition in video at abaw-5 competition",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2023",
      "venue": "Emotieffnet facial features in uni-task emotion recognition in video at abaw-5 competition"
    },
    {
      "citation_id": "21",
      "title": "Multi-modal continuous dimensional emotion recognition using recurrent neural network and self-attention mechanism",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop"
    },
    {
      "citation_id": "22",
      "title": "End2you-the imperial toolkit for multimodal profiling by end-to-end learning",
      "authors": [
        "Panagiotis Tzirakis",
        "Stefanos Zafeiriou",
        "Bjorn Schuller"
      ],
      "year": "2018",
      "venue": "End2you-the imperial toolkit for multimodal profiling by end-to-end learning",
      "arxiv": "arXiv:1802.01115"
    },
    {
      "citation_id": "23",
      "title": "Vision transformer for action units detection",
      "authors": [
        "Tu Vu",
        "Van Huynh",
        "Soo Hyung"
      ],
      "year": "2023",
      "venue": "Vision transformer for action units detection"
    },
    {
      "citation_id": "24",
      "title": "Facial affective behavior analysis method for 5th abaw competition",
      "authors": [
        "Shangfei Wang",
        "Yanan Chang",
        "Yi Wu",
        "Xiangyu Miao",
        "Jiaqiang Wu",
        "Zhouan Zhu",
        "Jiahe Wang",
        "Yufei Xiao"
      ],
      "year": "2023",
      "venue": "Facial affective behavior analysis method for 5th abaw competition"
    },
    {
      "citation_id": "25",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "26",
      "title": "Continuous emotion recognition using visual-audio-linguistic information: A technical report for abaw3",
      "authors": [
        "Su Zhang",
        "Ruyi An",
        "Yi Ding",
        "Cuntai Guan"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "Multimodal facial affective analysis based on masked autoencoder",
      "authors": [
        "Wei Zhang",
        "Bowen Ma",
        "Feng Qiu",
        "Yu Ding"
      ],
      "year": "2023",
      "venue": "Multimodal facial affective analysis based on masked autoencoder"
    },
    {
      "citation_id": "28",
      "title": "Facial affect recognition based on transformer encoder and audiovisual fusion for the abaw5 challenge",
      "authors": [
        "Ziyang Zhang",
        "Liuwei An",
        "Zishun Cui",
        "Ao Xu",
        "Tengteng Dong",
        "Yueqi Jiang",
        "Jingyi Shi",
        "Xin Liu",
        "Xiao Sun",
        "Meng Wang"
      ],
      "year": "2023",
      "venue": "Facial affect recognition based on transformer encoder and audiovisual fusion for the abaw5 challenge"
    },
    {
      "citation_id": "29",
      "title": "Continuous emotion recognition based on tcn and transformer",
      "authors": [
        "Weiwei Zhou",
        "Jiada Lu",
        "Zhaolong Xiong",
        "Weifeng Wang"
      ],
      "year": "2023",
      "venue": "Continuous emotion recognition based on tcn and transformer"
    }
  ]
}