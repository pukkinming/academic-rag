{
  "paper_id": "2107.05989v1",
  "title": "Emotion Recognition For Healthcare Surveillance Systems Using Neural Networks: A Survey",
  "published": "2021-07-13T11:17:00Z",
  "authors": [
    "Marwan Dhuheir",
    "Abdullatif Albaseer",
    "Emna Baccour",
    "Aiman Erbad",
    "Mohamed Abdallah",
    "Mounir Hamdi"
  ],
  "keywords": [
    "Emotion Recognition",
    "Neural networks",
    "speech emotion recognition",
    "facial emotion recognition",
    "audio-visual emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recognizing the patient's emotions using deep learning techniques has attracted significant attention recently due to technological advancements. Automatically identifying the emotions can help build smart healthcare centers that can detect depression and stress among the patients in order to start the medication early. Using advanced technology to identify emotions is one of the most exciting topics as it defines the relationships between humans and machines. Machines learned how to predict emotions by adopting various methods. In this survey, we present recent research in the field of using neural networks to recognize emotions. We focus on studying emotions' recognition from speech, facial expressions, and audio-visual input and show the different techniques of deploying these algorithms in the real world. These three emotion recognition techniques can be used as a surveillance system in healthcare centers to monitor patients. We conclude the survey with a presentation of the challenges and the related future work to provide an insight into the applications of using emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The advancement of deep learning technologies brings more attention to deployment scenarios in smart health systems  [1] -  [3] . The health industry uses many approaches based on machine learning, such as remote disease diagnosis, surveillance system in healthcare and elderly care centers, etc., to recognize patients' emotions. These systems are used for early emotion recognition to introduce prompt interventions reducing symptoms of depression and stress. In this survey, we present the recent work of using three different techniques to recognize emotions which are speech, facial and audio-visual. We focus on using a deep neural networks to identify patients' emotions. These methods can be used as a surveillance system and capture images, videos, and speech using different tools such as cameras and microphones. The areas that use patient's emotion recognition are wide, and its applications include many necessary daily life uses such as in safe driving, monitoring mental health, social security, and so on. Many surveys cover this topic with deep details from different perspectives, such as in  [4] -  [10] . The authors studied emotion recognition by focusing on body gesture, speech expressions, and audio-visual expressions. The authors of these surveys focused on multi-modal approaches that study either face or speech with body gestures to enhance the emotion recognition. The survey studies in  [6]    [8]  focus on studying emotion recognition by using facial emotion, and they used a device called Microsoft HoloLens (MHL) to observe the emotions in Augmented Reality (AR). They used the device as a sensor in the experiment to recognize emotions. Then, they compared their method with a method that recognizes emotion by using a webcam. The experiment concluded that using MHL with AR gave better accuracy than using a webcam.\n\nStudying Patients Emotion recognition for better health system has become necessary in the last decade as it helps in many fields, and one of these fields is the medical sector. It helps doctors recognize the patients' psychological problems and, hence, start the medication early  [4] . Many hospitals worldwide have begun incorporating AI in medicating patients, and many researchers are focusing on studying neural networks to recognize the patient's emotions. This survey presents one common AI technique to recognize emotions by using three different modalities: speech, facial, and audiovisual methods. We present the common techniques used to recognize emotions to give the readers a general overview of using neural networks in the medical sector.\n\nOur contribution in this survey is to study patients emotion recognition techniques which is considered a key to enhancing patients healthcare. Although many techniques are used for emotion recognition, such as recognizing emotions by using Electroencephalography (EEG), Electrocardiography (ECG), respiration, gesture, etc., we focus on the three methods captured by surveillance systems using cameras and microphones. The study focuses on three essential stages to make the final recognition decision: pre-processing, feature selection and extraction, and classification. We highlight the recent techniques and scenarios used in each stage.\n\nThe paper is organized as follows: Section II presents the common datasets, and section III illustrates speech emotion recognition. Section IV shows facial emotion recognition. Section V presents the Audio-visual emotion recognition. Finally, section VI presents the conclusion and future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Databases And Test Preparation",
      "text": "This section presents the commonly used datasets in recognizing emotions. To effectively design an emotion recognition system, it is crucial to have training data that comprises many different populations and environments. These datasets are utilized for training the suggested methods and approaches; therefore, they should be chosen carefully to conduct suitable experiments. In Table  I , we summarize and describe these datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Speech Emotion Recognition",
      "text": "Speech is the communication medium between humans. Many researchers use machines to interact with humans and to extract their feelings. However, it requires an extensive effort to make this interaction natural between humans and machines. In this section, we present the recent work in using machines to predict emotions, focusing on the different methods using neural networks. Using neural networks gives an advantage in terms of the efficiency to extract emotions from speech due to the automatic feature selection, which is challenging in traditional speech emotion recognition (SER) techniques.\n\nRecently, speech emotion recognition is becoming an attractive approach. New techniques deal with the complexity in extracting emotions and are affected by different factors such as age, gender  [9] , and the difficulty to process large data sets. The study in  [11]  explored speech emotion recognition using recurrent neural networks (RNN) with a focus on local attention. They used DL to learn short-time frame-level acoustic features and a suitable aggregation of these features. They used local attention because it can focus on particular regions of the more emotionally salient speech signal. This method is more accurate than the traditional SVM-based speech emotion recognition (SER) that uses fixed designed features. Furthermore, the speech emotion recognition is studied in  [14]    [15]  [16]  [17]  in which the authors focused on proposing an SER algorithm based on concatenated convolutional neural network (CNN) and RNN without using any hand-crafted extracting method. Authors in  [18]  studied speech emotion recognition by extracting the statistical features over segments of speech. The segments are extracted according to the matching of a couple of words. They tested it on Interactive Emotional Motion Capture (IEMOCAP) database. Learning utterance-level representation for speech emotion recognition was covered in  [19] , and they focused on encoding each utterance into a vector by pooing the activation of the final hidden layer. They formulated an optimization problem to minimize the utterancelevel target. To differentiate between verbal and nonverbal speech sounds in real-life conversations, the study in  [13]  used Prosodic Phrase (PPh) auto-tagger to extract the verbal and nonverbal segments, and the result showed that the nonverbal speech intervals gave an excellent performance and the sound feature ability to identify emotion recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "A. Preprocessing",
      "text": "After collecting data to be classified, it goes to the preprocessing step to prepare and mitigate the effects of noise. The input data is corrupted by noise that needs to be removed; otherwise, feature selection and extraction will not be sufficient enough for the classification  [20] . Several preprocessing techniques are used for feature extraction. Some methods do feature normalization so speakers and recordings variations do not influence the process of recognizing emotions using speech  [21] .\n\nDepending on the type of input data, a suitable approach can be applied. For example, voice input data are preprocessed to extract the data segments using the vocal cords' quasi-periodic vibration. In contrast, unvoiced input data are preprocessed by using turbulent airflow  [22] . Other methods commonly used are framing, windowing, and normalization, in which the choice of choosing the suitable method depends on the type of the input voice data  [10] . Many standard preprocessing techniques are used as noise reduction of the input data, like minimum mean square error (MMSE) and log-spectral amplitude MMSE (logMMSE)  [16] ,  [17] ,  [19] . Other efficient methods use sampling and frame operations to obtain a set of labeled samples  [23] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Feature Selection And Extraction",
      "text": "The speech signal is continuous by nature and carries information, and it contains emotions. Hence, according to the followed feature approach, global or local features can be selected accordingly. Global features, known as long-term or supra-segmental features, express the gross statistics such as mean, minimum, maximum values, and standard deviation. On the other hand, local features, known as short-term or segmental features, express the temporal features, in which the main goal is to approximate the stationary state  [13] [14] . As emotional features are distributed in a non-uniformly manner over all speech signals, the stationary states become crucial to be adopted  [10] . Table  II  presents details about the common features used in SER.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Classifiers",
      "text": "The classification of SER systems depends on the utterance of emotions. The classifiers can be divided into two parts. One followed the traditional classifiers such as the Hidden Markov Model (HMM) and Artificial Neural Networks (ANN). The second one uses deep learning algorithms. However, nowadays, most of the classification processes are done using deep neural networks as they can do feature selection and extraction at the same time  [18] . Table  III  presents the common classifiers that are used in recognizing emotions by using speech.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Facial Emotion Recognition",
      "text": "The second emotion recognition is to use DNN for facial emotion recognition. It is helpful as it depends on the images captured by camera in the healthcare units and process them. Authors in  [6]    [12]  used batch normalization to improve both generalization and optimization. The experiment was conducted on the Extended Cohn-Kanada (CK+) and Japanese Female Facial expression (JAFFE) datasets. The result showed that the fully convolutional network (FCN) and residual block cloud improve the system efficiency.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Preprocessing",
      "text": "Facial expressions contain many irrelevant variations such as various background, image illumination, and body poses, therefore, preprocessing data input is important. DNNs are used to learn many features and propose preprocessing to",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Reference",
      "text": "Feature Description  [24]  Prosody Features that human perceives like intonation and rhythm. SER uses mostly frames duration, intensity, and contour of fundamental frequency F0 for prosody features. Its frame duration typically ranges between 30-100 ms.  [25]  Spectral It aims to obtain the energy content of the available frequency bands in the speech signals. Spectral features commonly used are formant, cepstral, MFCC, linear predictive cepstral coefficient (LPCC), and perceptual linear prediction (PLP).  [26]  Voice quality It is obtained by the physical characteristics of the vocal tracts. The variations of the speech signals like jitter, shimmer and harmonics are defined as the constructions of voice quality features. Its duration is less than 10 ms, hence it is called sub-segmental level features.  [27]  Non-linear It is produced when vocal cords exert non-linear pressures, hence it cannot be represented by using traditional features methods. Nonlinear dynamic (NLD) was introduced to represent the features  [28]  Deep-learningbased\n\nDeep learning algorithms can be used to learn both low-level and high-level features hierarchically. The low-level descriptors (LLD) algorithms can be applied directly to deep learning algorithms.  [29]  Non-linguistics vocalization It contains speech disfluencies like laughter, breathing, crying, and different breaks. These features are important for SER and can be recognized by using an automatic speech recognition engine. align and normalize the data captured from the faces. Table  IV  presents the common preprocessing techniques that are used in recognizing emotions by using images.\n\nAuthors in  [12]  [38] used the normalization method for preprocessing input data to specify the face in the picture and identify the points of interest before passing the data to feature extraction. Moreover, the authors in  [12]  further divided the image normalization into two subtasks, subtracted local contrast and divisive local contrast, and decreased the mismatch in the image content. In  [8] , the authors used the face alignment method to process the input image in which they used an affine transformation specified by the centers of the eyes and the center of the mouth. The output data are further preprocessed by the normalization method.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Feature Selection",
      "text": "Deep learning is one effective technique to capture the features from the images. It can capture high-level details through hierarchical structures of many non-linear transformations and representations. The stage of feature selection includes selecting the training set for making them ready for machine learning algorithms. It focuses on choosing a suitable prediction for the learning system. This step helps to enhance the rate of prediction and enhance efficiency. Several tools are",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Traditional Based Classifiers Reference",
      "text": "Classifier Description  [30]  Hidden Markov Model (HMM)\n\nIn this model, the current state depends on previous state. This model uses little data to recognize speech emotions from contextual information. Its strong side is in natural databases.  [24]  Linear Discriminant Analysis (LDA)\n\nTo classify the input speech data, it uses dimensional reduction of input data, hence decreases the computational load.  [31]  Singular Vector Machine (SVM)\n\nIt achieves better in case of small databases and high dimension features in which these two features are common in SER. It also cares about both testing and training data.  [32]  k-Nearest Neighbor (k-NN)\n\nIt deals more with nonlinear feature inputs to create the relations. The disadvantages of this classifier, distance and k calculations are important.  [33]  Ensemble Classifiers (EC)\n\nIt minimizes the variances and decreases the over-fitting which are crucial in SER. If the features are correlated, this method cannot work well.  [34]  Gaussian Mixture Model (GMM)\n\nIt can work well when it is combined with discriminate classifiers like SVM because it can generate and learn the hidden features of speech emotion. Deep learning based classifiers  [35]  Artificial Neural Networks (ANN) This classifier achieves good results for nonlinear emotional features. Its latency is very short to predict the features, hence it is effective for applications that are sensitive to time.  [36]  CNN It has ability to decrease the signal processing, automatic learning of discriminative and global emotional features.  [11]  LSTM It has the ability to process the long contextual information and the long variance input utterance features.  [37]  Auto-Encoder Neural Network (AEN)\n\nIt has the ability to work in mismatched environments, it can learn features in low dimensional spaces, and nonlinear features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Table Iii Different Classifiers In Ser.",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Reference",
      "text": "Preprocessing method Description  [39]  Normalization It uses median filters to reduce the illumination and variations of the input images and improve the image quality.  [40]  Localization This preprocessing method uses the algorithm of Viola-Jones to recognize the input image. To detect the faces' size and location, Adaboost learning algorithm and haar lhaar-likeres algorithms are used.  [8]  Face Alignment It is used to remove the background and the areas that do not contain the face. To do that, the Viola-Jones (V-J) face detector is used for face detection because it is robust.  [8]  Data augmentation There are two types of data-augmentation approaches: 1) on-the-fly data augmentation and 2) offline data augmentation.  [41]  Histogram Equalization Method\n\nIt is used to overcome the variations in the image illuminations. This method is used to improve the contrast of the images and improve the face images' lighting. useful such as Weka and sci-kit learn which contain inbuilt tools for effective automated selection of features.\n\nThe study in  [12]  used DNN to extract the feature from a set of images in which they used several datasets such as CK+, JAFFE, and Cohn-Kanade. The result shows a performance improvement. The authors in  [8]  used different CNN techniques to extract features such as C3D for spatiotemporal extraction.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Feature Classification",
      "text": "It is the last stage in FER's system, which gives the final decision of the detected emotion such as sadness, anger, disgusting, etc. The widely used classification methods are SVM, Nearest Neighbor (NN), Softmax, Deep Neural Forest (NFs). The input extracted data uses either a particular face action or a specific face emotion for classification; however, the latter is commonly used  [8] .\n\nAuthors in  [12]  used Softmax classifier to classify the incoming data into six demotions. They used a single DNN that contains both convolutional layers and residual blocks to achieve higher accuracy and train deeper layers. Their model showed better performance and better accuracy than the state-of-the-art methods. Other studies  [8]  [42] used the SVM classifier. They first applied SVM with radial basis function (RBF) and optimized the output data using the grid search optimization method. Table  V  gives the most common classifiers that are used in FER.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Audio-Visual Emotion Recognition",
      "text": "The third emotion recognition application is to recognize emotions from audio-visual input  [46] . The study used deep learning algorithms on big emotional data. The classification method used is support vector machine (SVM). After extraction and classification of data, the output was fed to an extreme learning machine (ELM) as a fusion stage to predict the input features' emotions. Another work in  [47]  studied audio-visual emotion recognition by using two distinct state of the art methods. These methods are deep transfer learning and multiple temporal models. They used different DCNN for feature extraction and classification, and their result showed competitive performance. Moreover, the study in  [48]  used DCNN by incorporating it with multimodal systems for recognizing emotions. They tested their method on the RML dataset, and the result showed an improvement in the accuracy.\n\nPeople interact with each other by using various types of expressions. Emotions are expressed clearly by using verbal and nonverbal communication, and therefore it is easier to understand each other. Expanding the research to include many expressions has the advantage of facilitating research to Reference Classifier Description  [8]  Softmax loss To minimizes the cross-entropy between the calculated class likelihood and the ground-truth distribution.  [43]  Deep Neural Forest (NFs)\n\nIt uses NFs instead of softmax loss, and they achieved a similar result for recognizing emotions from faces images.\n\n[44] Support Vector Machine (SVM)\n\nIt is a supervised machine learning technique that uses four types of kernels to improve the performance of classification. These four kernels are linear, polynomial, Radial Basis Function (RBF) and sigmoid, and they work together to improve the performance.  [40]  ID3 Decision Tree (DT) This classifier is a rule based which uses the decision tree to extract the rules. Least Boolean evaluation is used to execute the classification.  [45]  (MFFNN) This classifier utilizes three different layers which are input, hidden, and output layers. It uses the algorithm of back-propagation to classify the input data. recognize emotions. Furthermore, by mixing audio and visual expression, researchers can benefit from the big data that will be created because one of the limitations of using either speech or facial is the limited number of datasets.\n\nIn Audio-Visual Recognition (AVR), three steps are applied to both speech and facial, then fusion is applied to extract the final emotion expression. These three steps are preprocessing, feature extraction then finally classification.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "A. Preprocessing",
      "text": "Data selection in audio-visual emotion (AVM) recognition is usually taken from videos. The videos' content is first portioned into many frames where they are the main source of visual-based features. Videos have the advantage of controlled datasets. These datasets have a fixed setup in which it focuses on the face's area as it contains all the expressions that give the specific emotion  [47] . For audio signals, the signals are extracted and converted to 16k Hz sampled by using quantized mono signals. The audio features take advantage of an extended version of GeMAPS (eGeMAPS), and the features are normalized to be zero mean and unit variance to be ready to forward to DNN input  [49] .\n\nThe study in  [7]  uses preprocessing technique by applying the Multi-Task Cascaded Convolutional Network (MTCNN) to extract the face expression and alignment for video frames. It divides the task into small segments, and CNN trains all the sub-tasks to confidently detect the features and prepare them for classification. Authors in  [50]  extracted frames at the beginning and end of each video for preprocessing the input data. They followed this method to avoid the repetition of training the systems with the same facial expressions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Feature Extraction",
      "text": "Feature extraction is the first stage to recognize emotions; therefore, it affects the whole system's performance. Recently, DCNN has been used to extract features and prepare them for the next stage. Authors in  [48]  used different methods to extract the features from facial and audio contents. They used prosodic, Mel-frequency Cepstral Coefficient (MFCC) to extract the features from facial and Gabor wavelet from audio, then combine the two extract features to represent audio-visual features and pass them to the final stage, classification.\n\nAuthors in  [51]  used statistical parts of the audio signal's energy and pitch contours to extract audio features, and they used the faces motion features such as the movements of the head, eyebrows, eyes, and mouth to record the facial features. After that, they send each feature separately to the classification stage to classify them and identify the specific emotion. There are many feature extraction methods to improve the systems performance and in table VI, we summarized the common three used methods.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Classification And Fusion",
      "text": "The study in  [50]  used AlexNet to do the classification process of multimodal emotion recognition, and they compared it with the model of validation of human decision-makers. They tested it on different datasets such as RML, SAVEE, eNTER. The result of this system is competitive with other studies in the same field.\n\nAfter classifying both speech and visual, the fusion stage comes to accommodate both layers to deliver the final decision of emotion in multimodal systems. The main function is to make layers that come from the speech section and visual section at the same length. The authors in  [47]  used the brutal force technique to find the optimum weight of incoming layers. At this step, the system gives the final decision of incoming expression and recognizes the emotion.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vi. Discussion And Future Work",
      "text": "In this section, we present the challenges and possible future directions. We start by discussing the datasets that are available for applying the different processes. Then, we discuss the three mentioned techniques separately by mentioning their strength and weakness.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Datasets",
      "text": "Most of the available datasets are acted expressions, and they are produced at studios or labs. These studios contain high-quality recordings, and they are noise-free. One of the challenges includes choosing a suitable method to extract features, system robustness to tone changes, the talking style, the speaking rate, the cultures and environment of people, which affect the way of expressing emotions. Moreover, most of the acted expressions are created from the same person, making them not real. The effectiveness of these datasets to be applied in real life where there is noise, and people's natural expressions are different from the features imitated in the studios depend on the content of the datasets' content.\n\nVGG-LSTM In this feature extraction model, the authors used VGG-16 to receive layers and extract the features from them, and then they will be forwarded to LSTM layers to recognize the emotion.  [47]  ResNet-LSTM In this model, the features are extracted from different layers as sub-tasks and they will be passed to LSTM layers.  [47]  C3D Network In this model C3D network is the instead of using traditional 2-D kernels to improve the system performance. Therefore, these datasets' effectiveness is challenging, and the accuracy of the classified expressions is questionable. The suggested solution is to use real datasets based on real experiments to be sure of the different techniques.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Speech Based Er",
      "text": "The feature selection and extraction method are aimed to prepare the data from utterances and noises and pass them to the classifiers to classify the data. One challenge in SER is that collecting and annotating large utterances are difficult due to the hardness of processing large datasets, especially the speech signals that are continuous by nature. All the available feature selection methods are designed to process small datasets. Even though deep learning-based classifiers are used to classify the input data, it is still challenging to choose a suitable classifier compatible with the used method in feature selection and extraction. Hence selecting a classifier that can improve the system's performance and increase the classification accuracy is an open problem to be discussed in future works.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Facial Based Er",
      "text": "The majority of the existing methods that are used in FER are based on different training datasets and training the variations of the expressions from the image such as illuminations, head pose, the distance between the corners of eyes and head, etc. The image is divided into subframes and layers that are processed to select and extract emotions from these layers. Therefore, training deep layers and flexible filters are sufficient for the training process to extracting the image's features and expressions. Nevertheless, this method is sensitive to the trained dataset's size, which might cause degradation in the classification accuracy and the whole classification method's performance. Hence, recently CNN has been used in the classification stage, especially CNN can train deep layers effectively as they are a good solution for the head pose variations and calculating the distances of corner points in the face.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Audio-Visual Based Er",
      "text": "In audio-visual-based emotion recognition, most techniques preprocess, select and extract, and classify speech and facial features on their own and then combine them in a fusion stage. The accuracy of the whole system depends on the fusion stage, and the synchronization between features coming from speech and facial features is crucial. Many fusion scenarios have been introduced to enhance the systems' accuracy, such as SVM, PCA, SVM-PCA, etc. Therefore, studying the fusion method that achieves exemplary performance is an open problem and needs looking.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, we present a survey of using convolutional neural networks to recognize patients emotions. We started by showing the different databases used in this field, with brief information about their constructions to do emotion recognition. Three important applications of using CNNs in emotion recognition were studied, which are speech-based emotion recognition, facial-based emotion recognition, and audio-visual-based emotion recognition. We studied in detail each section and explained the different approaches they used. The study of each section focuses on pre-processing, feature selection and extraction, and the classification method of each stage to identify the final expressions.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reference": "[5]",
          "Dataset": "Extended\nCohn-\nKanade (CK+)",
          "Type": "laboratory-\ncontrolled",
          "calsses": "anger, disgust, fear, happiness, sad-\nness, and surprise",
          "Description": "consisting of 593 sequences\nfrom 123 subjects"
        },
        {
          "Reference": "[5]",
          "Dataset": "M&M\nInitiative\n(MMI)",
          "Type": "laboratory-\ncontrolled",
          "calsses": "anger, disgust, fear, happiness, sad-\nness, and surprise",
          "Description": "consisting of more\nthan 1500 samples of\nimage\nsequences\nand static images of\nfaces"
        },
        {
          "Reference": "[10]",
          "Dataset": "Oulu-CASIA",
          "Type": "laboratory-\ncontrolled",
          "calsses": "happiness, sadness, surprise, anger,\nfear, disgust",
          "Description": "containing 2880 image sequences that were collected from 80\nsubjects."
        },
        {
          "Reference": "[10]",
          "Dataset": "(JAFFE)",
          "Type": "Scientiﬁc\nresearch",
          "calsses": "(happiness, neutral, sadness, anger,\nsurprise, disgust,\nfear)",
          "Description": "comprising of 213 samples of posed expressions\nthat were\ntaken from Japanese females"
        },
        {
          "Reference": "[8]",
          "Dataset": "FER2013",
          "Type": "open-source\ndataset",
          "calsses": "happiness, neutral,\nsadness, anger,\nsurprise, disgust,\nfear",
          "Description": "comprising of 35,685 samples of 48x48 pixel grayscale im-\nages for\nfacial expression."
        },
        {
          "Reference": "[10]",
          "Dataset": "AFEW\nand\nSFEW",
          "Type": "open-source\ndataset",
          "calsses": "happiness, neutral,\nsadness, anger,\nsurprise, disgust,\nfear",
          "Description": "consists\nof\nvideo\nclips\nthat were\ngathered\nfrom various\nmovies with unconstrained expressions, different head poses,\nocclusions, and illumination"
        },
        {
          "Reference": "[10]",
          "Dataset": "Multi-PIE",
          "Type": "open-source\ndataset",
          "calsses": "happiness, sadness, anger, surprise,\ndisgust,\nfear",
          "Description": "These\nimages were\ncollected\nfrom 337\nsubjects\nunder\n15\nviewpoints, and the illumination was 19"
        },
        {
          "Reference": "[5]",
          "Dataset": "BU-3DFE and\nBU-4DFE",
          "Type": "Research-\noriented dataset",
          "calsses": "happiness, sadness, anger, surprise,\ndisgust,\nfear",
          "Description": "consisting of 606 facial expression sequences from 100 people\nand the around 60,600 frame models."
        },
        {
          "Reference": "[11]",
          "Dataset": "EmotioNet",
          "Type": "Public dataset",
          "calsses": "happiness, sadness, anger, surprise,\ndisgust,\nfear",
          "Description": "consisting of one million facial\nexpression images gathered\nfrom the Internet"
        },
        {
          "Reference": "[10]",
          "Dataset": "RAF-DB",
          "Type": "Public dataset",
          "calsses": "happiness, sadness, anger, surprise,\ndisgust,\nfear",
          "Description": "comprising of 29,672 diverse facial\nimages that were down-\nloaded from the Internet"
        },
        {
          "Reference": "[8]",
          "Dataset": "AffectNet",
          "Type": "Public dataset",
          "calsses": "happiness, sadness, anger, surprise,\ndisgust,\nfear",
          "Description": "consisting of more than one million images gathered from the\nInternet"
        },
        {
          "Reference": "[12]",
          "Dataset": "EMO-DB",
          "Type": "Research-\noriented dataset",
          "calsses": "anger,\njoy,\nsadness, neutral,\nbore-\ndom, disgust, and fear",
          "Description": "containing 535 emotional expressions"
        },
        {
          "Reference": "[7]",
          "Dataset": "RML",
          "Type": "Research-\noriented dataset",
          "calsses": "anger,\ndisgust,\nfear,\njoy,\nsadness,\nand surprise",
          "Description": "consisting of 720 utterance expressions with eight subjects"
        },
        {
          "Reference": "[13]",
          "Dataset": "eNTERFACE05",
          "Type": "audio-visual\nemotion dataset",
          "calsses": "anger,\ndisgust,\nfear,\njoy,\nsadness,\nand surprise",
          "Description": "consisting of 1290 utterances"
        },
        {
          "Reference": "[7]",
          "Dataset": "BAUM-1s",
          "Type": "audio-visual\nemotion dataset",
          "calsses": "joy,\nanger,\nsadness,\ndisgust,\nfear,\nsurprise,\nboredom,\ncontempt,\nun-\nsure,\nthinking,\nconcentrating,\nand\nbothered",
          "Description": "consisting of 1222 utterances gathered from 31 Turkish sub-\njects"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reference": "[24]",
          "Feature": "Prosody",
          "Description": "Features\nthat human perceives\nlike\nintonation and rhythm. SER uses mostly frames duration,\nintensity,\nand contour of\nfundamental\nfrequency F0 for prosody features.\nIts frame duration typically ranges between 30-100 ms."
        },
        {
          "Reference": "[25]",
          "Feature": "Spectral",
          "Description": "It aims to obtain the energy content of\nthe available frequency bands\nin the speech signals. Spectral\nfeatures commonly\nused are formant, cepstral, MFCC,\nlinear predictive cepstral coefﬁcient\n(LPCC), and perceptual\nlinear prediction (PLP)."
        },
        {
          "Reference": "[26]",
          "Feature": "Voice quality",
          "Description": "It\nis obtained by the physical characteristics of\nthe vocal\ntracts. The variations of\nthe speech signals\nlike jitter,\nshimmer\nand harmonics are deﬁned as the constructions of voice quality features.\nIts duration is less than 10 ms, hence it\nis called\nsub-segmental\nlevel\nfeatures."
        },
        {
          "Reference": "[27]",
          "Feature": "Non-linear",
          "Description": "It\nis produced when vocal cords exert non-linear pressures, hence it cannot be represented by using traditional\nfeatures\nmethods. Nonlinear dynamic (NLD) was introduced to represent\nthe features"
        },
        {
          "Reference": "[28]",
          "Feature": "Deep-learning-\nbased",
          "Description": "Deep learning algorithms can be used to learn both low-level and high-level features hierarchically. The low-level descriptors\n(LLD) algorithms can be applied directly to deep learning algorithms."
        },
        {
          "Reference": "[29]",
          "Feature": "Non- linguistics\nvocalization",
          "Description": "It contains speech disﬂuencies like laughter, breathing, crying, and different breaks. These features are important\nfor SER\nand can be recognized by using an automatic speech recognition engine."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Traditional based classiﬁers": "Reference"
        },
        {
          "Traditional based classiﬁers": "[30]"
        },
        {
          "Traditional based classiﬁers": "[24]"
        },
        {
          "Traditional based classiﬁers": "[31]"
        },
        {
          "Traditional based classiﬁers": "[32]"
        },
        {
          "Traditional based classiﬁers": "[33]"
        },
        {
          "Traditional based classiﬁers": "[34]"
        },
        {
          "Traditional based classiﬁers": "Deep learning based classiﬁers"
        },
        {
          "Traditional based classiﬁers": "[35]"
        },
        {
          "Traditional based classiﬁers": "[36]"
        },
        {
          "Traditional based classiﬁers": "[11]"
        },
        {
          "Traditional based classiﬁers": "[37]"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reference": "[39]",
          "Preprocessing method": "Normalization",
          "Description": "It uses median ﬁlters to reduce the illumination and variations of the input\nimages and improve the image quality."
        },
        {
          "Reference": "[40]",
          "Preprocessing method": "Localization",
          "Description": "This preprocessing method uses the algorithm of Viola-Jones\nto recognize the input\nimage. To detect\nthe faces’\nsize and location, Adaboost\nlearning algorithm and haar\nlhaar-likeres algorithms are used."
        },
        {
          "Reference": "[8]",
          "Preprocessing method": "Face Alignment",
          "Description": "It\nis used to remove the background and the areas that do not contain the face. To do that,\nthe Viola-Jones (V-J)\nface detector\nis used for\nface detection because it\nis robust."
        },
        {
          "Reference": "[8]",
          "Preprocessing method": "Data augmentation",
          "Description": "There\nare\ntwo\ntypes\nof\ndata-augmentation\napproaches:\n1)\non-the-ﬂy\ndata\naugmentation\nand\n2)\nofﬂine\ndata\naugmentation."
        },
        {
          "Reference": "[41]",
          "Preprocessing method": "Histogram\nEqualiza-\ntion Method",
          "Description": "It\nis used to overcome the variations\nin the image illuminations. This method is used to improve the contrast of\nthe images and improve the face images’\nlighting."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reference": "[8]",
          "Classiﬁer": "Softmax loss",
          "Description": "To minimizes the cross-entropy between the calculated class likelihood and the ground-truth distribution."
        },
        {
          "Reference": "[43]",
          "Classiﬁer": "Deep Neural For-\nest\n(NFs)",
          "Description": "It uses NFs instead of softmax loss, and they achieved a similar result for recognizing emotions from faces images."
        },
        {
          "Reference": "[44]",
          "Classiﬁer": "Support\nVector\nMachine (SVM)",
          "Description": "It\nis\na\nsupervised machine\nlearning technique\nthat uses\nfour\ntypes of kernels\nto improve\nthe performance of\nclassiﬁcation. These\nfour kernels\nare\nlinear,\npolynomial, Radial Basis Function (RBF)\nand sigmoid,\nand they\nwork together\nto improve the performance."
        },
        {
          "Reference": "[40]",
          "Classiﬁer": "ID3\nDecision\nTree (DT)",
          "Description": "This classiﬁer\nis a rule based which uses the decision tree to extract\nthe rules. Least Boolean evaluation is used\nto execute the classiﬁcation."
        },
        {
          "Reference": "[45]",
          "Classiﬁer": "(MFFNN)",
          "Description": "This classiﬁer utilizes\nthree different\nlayers which are input, hidden, and output\nlayers.\nIt uses the algorithm of\nback-propagation to classify the input data."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Edge computing for smart health: Context-aware approaches, opportunities, and challenges",
      "authors": [
        "A Abdellatif",
        "A Mohamed",
        "C Chiasserini",
        "M Tlili",
        "A Erbad"
      ],
      "year": "2019",
      "venue": "IEEE Network"
    },
    {
      "citation_id": "2",
      "title": "Convolutional autoencoder approach for eeg compression and reconstruction in m-health systems",
      "authors": [
        "A Al-Marridi",
        "A Mohamed",
        "A Erbad"
      ],
      "year": "2018",
      "venue": "2018 14th International Wireless Communications Mobile Computing Conference (IWCMC)"
    },
    {
      "citation_id": "3",
      "title": "sshealth: Toward secure, blockchain-enabled healthcare systems",
      "authors": [
        "A Abdellatif",
        "A Al-Marridi",
        "A Mohamed",
        "A Erbad",
        "C Chiasserini",
        "A Refaey"
      ],
      "year": "2020",
      "venue": "IEEE Network"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from physiological signal analysis: a review",
      "authors": [
        "M Egger",
        "M Ley",
        "S Hanke"
      ],
      "year": "2019",
      "venue": "Electronic Notes in Theoretical Computer Science"
    },
    {
      "citation_id": "5",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "D Kaminska",
        "C Corneanu",
        "T Sapinski",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "6",
      "title": "Facial emotion recognition: A survey and real-world user experiences in mixed reality",
      "authors": [
        "D Mehta",
        "M Siddiqui",
        "A Javaid"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "7",
      "title": "Audio-visual emotion recognition in video clips",
      "authors": [
        "F Noroozi",
        "M Marjanovic",
        "A Njegus",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Dager: Deep age, gender and emotion recognition using convolutional neural network",
      "authors": [
        "A Dehghan",
        "E Ortiz",
        "G Shu",
        "S Masood"
      ],
      "year": "2017",
      "venue": "Dager: Deep age, gender and emotion recognition using convolutional neural network",
      "arxiv": "arXiv:1702.04280"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "11",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Extended deep neural network for facial emotion recognition",
      "authors": [
        "D Jain",
        "P Shamsolmoali",
        "P Sehdev"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using deep neural network considering verbal and nonverbal speech sounds",
      "authors": [
        "K.-Y Huang",
        "C.-H Wu",
        "Q.-B Hong",
        "M.-H Su",
        "Y.-H Chen"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks,\" in 2016 Asia-Pacific signal and information processing association annual summit and conference (APSIPA)",
      "authors": [
        "W Lim",
        "D Jang",
        "T Lee"
      ],
      "year": "2016",
      "venue": "Speech emotion recognition using convolutional and recurrent neural networks,\" in 2016 Asia-Pacific signal and information processing association annual summit and conference (APSIPA)"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition from spectrograms with deep convolutional neural network",
      "authors": [
        "A Badshah",
        "J Ahmad",
        "N Rahim",
        "S Baik"
      ],
      "year": "2017",
      "venue": "2017 international conference on platform technology and service"
    },
    {
      "citation_id": "16",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition from speech using convolutional neural network with recurrent neural network architecture",
      "authors": [
        "S Basu",
        "J Chakraborty",
        "M Aftabuddin"
      ],
      "year": "2017",
      "venue": "2017 2nd International Conference on Communication and Electronics Systems (ICCES)"
    },
    {
      "citation_id": "18",
      "title": "Segment-based speech emotion recognition using recurrent neural networks",
      "authors": [
        "E Tzinis",
        "A Potamianos"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "19",
      "title": "Learning utterance-level representations for speech emotion and age/gender recognition using deep neural networks",
      "authors": [
        "Z.-Q Wang",
        "I Tashev"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "20",
      "title": "Estimation of mental health quality of life using visual information during interaction with a communication agent",
      "authors": [
        "S Nakagawa",
        "S Yonekura",
        "H Kanazawa",
        "S Nishikawa",
        "Y Kuniyoshi"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "21",
      "title": "Software architecture for smart emotion recognition and regulation of the ageing adult",
      "authors": [
        "J Castillo",
        "Álvaro Castro-González",
        "A Fernández-Caballero",
        "J Latorre",
        "J Pastor",
        "A Fernández-Sotos",
        "M Salichs"
      ],
      "year": "2016",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "22",
      "title": "Brain activity patterns of phonemic representations are atypical in beginning readers with family risk for dyslexia",
      "authors": [
        "M Vandermosten",
        "J Correia",
        "J Vanderauwera",
        "J Wouters",
        "P Ghesquière",
        "M Bonte"
      ],
      "year": "2020",
      "venue": "Developmental Science"
    },
    {
      "citation_id": "23",
      "title": "Deep learning based affective model for speech emotion recognition",
      "authors": [
        "X Zhou",
        "J Guo",
        "R Bie"
      ],
      "year": "2016",
      "venue": "2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress"
    },
    {
      "citation_id": "24",
      "title": "Analysis of emotionally salient aspects of fundamental frequency for emotion detection",
      "authors": [
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "Analysis of emotionally salient aspects of fundamental frequency for emotion detection"
    },
    {
      "citation_id": "25",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "T Falk",
        "W.-Y Chan"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "26",
      "title": "A kinematic study of critical and non-critical articulators in emotional speech production",
      "authors": [
        "J Kim",
        "A Toutios",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "27",
      "title": "Time-frequency feature and amsgmm mask for acoustic emotion classification",
      "authors": [
        "L Zao",
        "D Cavalcante",
        "R Coelho"
      ],
      "year": "2014",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "28",
      "title": "Deep features-based speech emotion recognition for smart affective services",
      "authors": [
        "A Badshah",
        "N Rahim",
        "N Ullah",
        "J Ahmad",
        "K Muhammad",
        "M Lee",
        "S Kwon",
        "S Baik"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition using spectrogram & phoneme embedding",
      "authors": [
        "P Yenigalla",
        "A Kumar",
        "S Tripathi",
        "C Singh",
        "S Kar",
        "J Vepa"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "30",
      "title": "Modeling the temporal evolution of acoustic parameters for speech emotion recognition",
      "authors": [
        "S Ntalampiras",
        "N Fakotakis"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "31",
      "title": "Emotion recognition using a hierarchical binary decision tree approach",
      "authors": [
        "C.-C Lee",
        "E Mower",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "32",
      "title": "The relevance of voice quality features in speaker independent emotion recognition",
      "authors": [
        "M Lugger",
        "B Yang"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07"
    },
    {
      "citation_id": "33",
      "title": "Feature selection for emotion recognition based on random forest",
      "authors": [
        "S Gharsalli",
        "B Emile",
        "H Laurent",
        "X Desquesnes"
      ],
      "year": "2016",
      "venue": "VISIGRAPP"
    },
    {
      "citation_id": "34",
      "title": "Emotional speech classification using gaussian mixture models and the sequential floating forward selection algorithm",
      "authors": [
        "D Ververidis",
        "C Kotropoulos"
      ],
      "year": "2005",
      "venue": "2005 IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "35",
      "title": "Ensemble feature selection for domain adaptation in speech emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "37",
      "title": "Universum autoencoder-based domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "X Xu",
        "Z Zhang",
        "S Frühholz",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "38",
      "title": "Hybrid deep neural networks for face emotion recognition",
      "authors": [
        "N Jain",
        "S Kumar",
        "A Kumar",
        "P Shamsolmoali",
        "M Zareapoor"
      ],
      "year": "2018",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "39",
      "title": "Automatic facial expression recognition based on spatiotemporal descriptors",
      "authors": [
        "Y Ji",
        "K Idrissi"
      ],
      "year": "2012",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "40",
      "title": "Feature-adaptive motion energy analysis for facial expression recognition",
      "authors": [
        "S Noh",
        "H Park",
        "Y Jin",
        "J.-I Park"
      ],
      "year": "2007",
      "venue": "International Symposium on Visual Computing"
    },
    {
      "citation_id": "41",
      "title": "A new facial expression recognition based on curvelet transform and online sequential extreme learning machine initialized with spherical clustering",
      "authors": [
        "A Uc ¸ar",
        "Y Demir",
        "C Güzelis"
      ],
      "year": "2016",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "42",
      "title": "Facial emotion recognition based on biorthogonal wavelet entropy, fuzzy support vector machine, and stratified cross validation",
      "authors": [
        "Y Zhang",
        "Z Yang",
        "H Lu",
        "X Zhou",
        "P Phillips",
        "Q Liu",
        "S Wang"
      ],
      "year": "2016",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "43",
      "title": "Deep neural decision forests",
      "authors": [
        "P Kontschieder",
        "M Fiterau",
        "A Criminisi",
        "S Bulo"
      ],
      "year": "2015",
      "venue": "Proceedings"
    },
    {
      "citation_id": "44",
      "title": "Random gabor based templates for facial expression recognition in images with facial occlusion",
      "authors": [
        "L Zhang",
        "D Tjondronegoro",
        "V Chandran"
      ],
      "year": "2014",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "45",
      "title": "Animal communication: when i'm calling you, will you answer too?",
      "authors": [
        "N Vickers"
      ],
      "year": "2017",
      "venue": "Current biology"
    },
    {
      "citation_id": "46",
      "title": "Emotion recognition using deep learning approach from audio-visual emotional big data",
      "authors": [
        "M Hossain",
        "G Muhammad"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "47",
      "title": "Audio-visual emotion recognition using deep transfer learning and multiple temporal models",
      "authors": [
        "X Ouyang",
        "S Kawaai",
        "E Goh",
        "S Shen",
        "W Ding",
        "H Ming",
        "D.-Y Huang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "48",
      "title": "Multimodal deep convolutional neural network for audio-visual emotion recognition",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2016",
      "venue": "ACM on International Conference on Multimedia Retrieval"
    },
    {
      "citation_id": "49",
      "title": "Mec 2017: Multimodal emotion recognition challenge",
      "authors": [
        "Y Li",
        "J Tao",
        "B Schuller",
        "S Shan",
        "D Jiang",
        "J Jia"
      ],
      "year": "2018",
      "venue": "2018 First Asian Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "50",
      "title": "Audiovisual emotion recognition in wild",
      "authors": [
        "E Avots",
        "T Sapiński",
        "M Bachmann",
        "D Kamińska"
      ],
      "year": "2019",
      "venue": "Machine Vision and Applications"
    },
    {
      "citation_id": "51",
      "title": "Multimodal information fusion based human movement recognition",
      "authors": [
        "Y Shu",
        "H Zhang"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    }
  ]
}