{
  "paper_id": "2501.08393v1",
  "title": "Empathetic Conversational Agents: Utilizing Neural And Physiological Signals For Enhanced Empathetic Interactions",
  "published": "2025-01-14T19:19:37Z",
  "authors": [
    "Nastaran Saffaryazdi",
    "Tamil Selvan Gunasekaran",
    "Kate Laveys",
    "Elizabeth Broadbent",
    "Mark Billinghurst"
  ],
  "keywords": [
    "CCS Concepts:",
    "Human-centered computing ‚Üí Empirical studies in HCI",
    "Laboratory experiments",
    "‚Ä¢ Computing methodologies ‚Üí Machine learning",
    "Conversational agent, emotion recognition, empathy, physiological signals, Electroencephalogram, Electrodermal activity, Photoplethysmogram"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Conversational agents (CAs) are revolutionizing human-computer interaction by evolving from text-based chatbots to empathetic digital humans (DHs) capable of rich emotional expressions. This paper explores the integration of neural and physiological signals into the perception module of CAs to enhance empathetic interactions. By leveraging these cues, the study aims to detect emotions in real-time and generate empathetic responses and expressions. We conducted a user study where participants engaged in conversations with a DH about emotional topics. The DH responded and displayed expressions by mirroring detected emotions in realtime using neural and physiological cues. The results indicate that participants experienced stronger emotions and greater engagement during interactions with the Empathetic DH, demonstrating the effectiveness of incorporating neural and physiological signals for real-time emotion recognition. However, several challenges were identified, including recognition accuracy, emotional transition speeds, individual personality effects, and limitations in voice tone modulation. Addressing these challenges is crucial for further refining Empathetic DHs and fostering meaningful connections between humans and artificial entities. Overall, this research advances human-agent interaction and highlights the potential of real-time neural and physiological emotion recognition in creating empathetic DHs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Advancements In Affective Computing And Emotion Recognition",
      "text": "Due to their accessibility and ease of use, current methods of emotion recognition predominantly employ behavioral signals such as facial expressions  [Li and Deng 2018; Samadiani et al. 2019; Y. Sun et al. 2020 ], text and audio analysis  [Sailunaz et al. 2018 ]. These modalities are commonly used in many human-computer interaction (HCI) applications  [Samadiani et al. 2019] . Some applications, like gaming  [AlZoubi et al. 2023] , self-driving cars  [Siam et al. 2023] , online embedded systems  [Jeong and Ko 2018] , and social robots and conversational agents  [Antony et al. 2020; Luo et al. 2022; Ruiz-Garcia et al. 2018] , use real-time facial expression recognition and speech analysis to understand the user's state. Although these approaches display potential in identifying basic emotions, they encounter obstacles such as cultural disparities, diversity in age and gender, and the susceptibility of behavioral modalities to manipulation by the user, thereby allowing for easy deception.  [Hossain and Gedeon 2019] .\n\nResearchers have increasingly incorporated physiological measures like Electroencephalography (EEG)  [Alarcao and Fonseca 2017] , Galvanic Skin Response (GSR), heart rate variability (HRV)  [Udoviƒçiƒá et al. 2017 ], blood volume pressure (BVP), body temperature (BT), respiration rate (RR), etc  [Chunawale, Bedekar, et al. 2020]  to address these limitations for emotion recognition. Many researchers have shown that although EEG and physiological signals are weak, they can be used to reliably recognize underlying emotions  [Blaiech et al. 2013; Shu et al. 2018] . Combining these various modalities can further improve the accuracy of emotion recognition  [Huang et al. 2019; Zhu et al. 2020] . Despite advancements in neural and physiological emotion recognition, these modalities have rarely been used in real-time emotion recognition. Their potential and limitations need to be explored more deeply for real-life scenarios such as interaction with conversational agents.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Empathetic Conversational Agents",
      "text": "The evolution of conversational agents towards achieving empathetic interactions has seen significant advancements in the past few years. Empathetic conversational agents or affective conversational agents are designed to understand and respond to users in a way that reflects an understanding of the user's emotions, providing a more human-like interaction experience  [Smith et al. 2011; Yang et al. 2019] .  Samrose et al. [Samrose et al. 2020]  explored the potential of empathetic conversational agents in mitigating boredom, highlighting the importance of emotional intelligence in conversational agents to adjust user mood and enhance task performance. Sakhrani et al.  [Sakhrani et al. 2021 ] have focused on creating empathetic agents for mental health support, emphasizing the need for agents that can conduct empathetic and free-flowing conversations to assist individuals who may be hesitant to seek help from human therapists.\n\nMost conversational agents rely only on text emotion recognition, which is a widely studied area  [Antony et al. 2020; Luo et al. 2022; Provoost et al. 2017] . For example, Zaranis et al.  [Zaranis et al. 2021]  developed EmpBot, an empathetic chatbot that utilizes sentiment understanding and empathy forcing to enhance the empathetic responses of conversational agents. Ayshabi and Idicula [Ayshabi and Idicula 2021] introduced a model that employs a multi-resolution mechanism with multiple decoders to capture the nuances of user emotions and generate empathetic responses. This approach aims to improve the perception and expression of emotional states in dialogue systems. However, facial expressions and speech tone have more information about the user's emotion  [Luo et al. 2022] . For example, Hussain et al.  [Hussain et al. 2012 ] presented a novel architecture for affective interaction with conversational agents. They fused audio, video, and text to detect affective states. Takana et al.  [Tanaka et al. 2017 ] introduced a social skill training agent for people with Autism that uses audio-visual features for detecting emotional states. Similarly, Aneja et al.  [Aneja et al. 2021 ] introduced a conversational agent that can recognize human behavior during open-ended conversations and automatically align its responses to the conversational and expressive style of the other party by using multimodal inputs, including facial expressions and speech. Tavabi et al.  [Tavabi 2019 ] developed a multimodal deep neural network that integrates emotional tone in language and facial expressions to recognize opportunities for empathetic responses in humanagent interactions. This study highlights the potential of incorporating visual and auditory cues to enhance the emotional intelligence of Empathetic CAs.\n\nDespite these advancements, a significant gap remains in integrating the neural and physiological data spectrum for emotion recognition. While multimodal approaches have begun to incorporate visual and auditory cues, the potential of neural and physiological signals (e.g., EEG, HRV, and EDA) in enhancing the empathetic capabilities of conversational agents remains largely untapped. These data sources could offer deeper insights into the user's emotional state and reveal genuine emotions reliably  [Hossain and Gedeon 2019; Shu et al. 2018 ]. This enhanced understanding would empower CAs to generate more nuanced and authentically empathetic responses. However, conversational agents rarely use neural and physiological modalities to recognize emotion in real-time. One exception was the work of  Val-Calvo et al. [Val-Calvo et al. 2020] , who used an affective robot for storytelling and created a realistic paradigm for evoking emotions. They recorded EEG, BVP, GSR, and video data and estimated emotional state in real-time subject-dependently. However, the robot did not use the recognition result to adapt its behavior. Similarly, Ghandeharioun et al.  [Ghandeharioun et al. 2019]  introduced EMMA, which is an emotion-aware chatbot that detects emotions in real-time based on the user's smartphone location and provides wellness suggestions for mental health according to detected emotions using location data. However, they did not consider any other source of data, especially physiological cues. Equipping conversational agents with physiological emotion recognition is crucial, especially in scenarios where recording audio and video is challenging due to environmental factors like noise, poor camera angles, or low light conditions. Moreover, audio and video alone often fail to fully capture emotional states. For example, when supporting older adults, individuals with autism, or those who face difficulties expressing emotions verbally or behaviorally, leveraging neural and physiological cues can provide a more accurate and effective understanding of their emotional needs.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conversational Agent Design",
      "text": "We used a digital human (DH) developed by Soul Machines Ltd 1  (Figure  1 ) to create an empathetic conversational agent. The DH was a young adult female. We disabled all of the Soul Machine features of speech and facial expression recognition so that we could implement our own. We created two different designs of DH in this research. One is a neutral DH with a simple perceptual module. It did not have an emotion recognition module and responds with a neutral pre-programmed answer and takes appropriate action based on the user's input and dialog flow. The other is an empathetic DH whose perceptual module is based on neural and physiological signals. It recognizes emotions in real-time and creates the appropriate expression by mirroring detected emotion. It chooses the appropriate response from a dataset of responses that reflect and appreciate the user's detected emotions. We used a female DH whose appearance and voice were identical in both DHs.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Acquiring Human Emotional Responses",
      "text": "In this study, we employed the Octopus-Sensing software suite  [Saffaryazdi, Gharibnavaz, et al. 2022]  to gather data from multiple sensors and perform real-time monitoring simultaneously across all of them. The data acquisition included facial video, audio, Electroencephalography (EEG), electrodermal activity (EDA), and photoplethysmography (PPG). The EEG was captured by the OpenBCI EEG cap 2  , while the EDA and PPG signals were captured using the Shimmer3 sensor 3  . Participants began by providing informed consent and receiving instructions. They were then seated in front of a monitor and laptop and fitted with an OpenBCI cap and Shimmer sensor in their non-dominant hand. Participants were alone during the data collection period in a quiet, controlled laboratory setting. The experimental setup and the devices used for data collection are shown in Figure  2 . The acquired EEG, PPG, and EDA signals were primarily used for real-time emotion recognition. Although audio and video were not used in this paper, they were collected for potential future research.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Architecture",
      "text": "We developed an orchestration server, which acts as the Soul Machine orchestration layer, to perform several functions within the empathetic DH. The server's main tasks included understanding the user's intent, selecting the most suitable pre-written response from a database of responses, provided in the supplementary materials, and generating appropriate facial expressions. It controls the dialog flow and creates responses based on the detected emotions through a real-time emotion recognition system. The orchestration server is also responsible for regulating the expressions of the digital human based on the user's real-time emotional state. Figure  3  shows the overall architecture of the empathetic DH that works based on physiological responses.\n\nWe customized the appearance of the DH and disabled all emotion recognition features available through the Soul Machines web browser. Instead, we employed our own multimodal emotion  recognition system and integrated it with the DH using an orchestration server. In the empathetic condition, the real-time emotion recognition system detected emotions every 5 seconds using a 20-second sliding window with overlap. The DH updated its facial expressions every 5 seconds based on these detections. Additionally, during interactions, the DH responded according to the most frequently detected emotion during the user's speech.\n\nIn this study, we used the dataset from prior research  [Saffaryazdi, Kirkcaldy, et al. 2024 ] to train emotion recognition models. This dataset includes EEG, PPG, and EDA data collected from 16 participants during dyadic conversations conducted in both remote and face-to-face settings, with each condition consisting of 8 trials. Due to technical issues, particularly with EEG signal recording in this dataset, some data was missing. Consequently, we had 240 trials to train the EDA and PPG models and 204 trials to train the EEG model. Since this datsets labels are based on dimensional arousal-valence model and earlier research explored that dimensional emotion models are more sufficient, reliable, and more accurate for self-assessments in explaining emotions [Eerola2011comparison;  Lichtenstein et al. 2008] , and dataset labels are based on arousal-valence model, we used a 2D-dimensional emotion model, and the results of predictions are based on arousal and valence. We mapped 5 levels of arousal and valence levels to low(-1) and high(1) values based on 1.\n\nWe followed the same methodology as described in  [Saffaryazdi, Kirkcaldy, et al. 2024 ] to preprocess data, extract features, and train emotion recognition models using each signal. This approach has proven effective in recognizing emotions during conversations. We extracted features listed in Table  4  and used the Random Forest Classifier (RFC), previously employed in emotion classification research  [Chaparro et al. 2018; Menezes et al. 2017] , to train the models for real-time emotion recognition.\n\nùëâ ùëéùëôùëíùëõùëêùëíùëÖùëéùë°ùëí = {1, 2, 3, 4, 5} Eevery 5 seconds, we obtained the last 20 seconds of data from each signal, preprocessed them, extracted features from them, and fed them into the trained models to recognize emotions based on arousal and valence levels. We considered 20 seconds because the heartpy library that we used for extracting features from the PPG signal needed at least 20 seconds of data to extract HRV features. We used the weighted decision fusion method shown in Equation 3 to combine the recognition results from neural and physiological modalities and detect the emotional state. Equal weights were considered for detecting valence levels for all modalities' predictions. However, we considered a higher weight for EDA and PPG, which was 2 compared to EEG was 1 for arousal detection because PPG and EDA signals have been shown to have greater reliability in detecting arousal levels  [KO≈ÅODZIEJ et al. 2019; Tarnowski et al. 2018; Udoviƒçiƒá et al. 2017 ]. The Equation  4 shows how we mapped the result of fusion to -1 and 1 values. The empathetic DH expressed emotions based on the result of the fusion method, primarily by mirroring the emotional state as listed in Table  1 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "ùë• ‚àà [ùê¥ùëüùëúùë¢ùë†ùëéùëô, ùëâ ùëéùëôùëíùëõùëêùëí]",
      "text": "To map the result of predictions to the avatar's expressions, we used Table  1 . There are not many differences between various positive expressions (with various intensity or arousal levels) in the avatar's expressions. So we considered all as happiness which means mirroring positive emotions with happiness expression. For expressions during answering, Soul machines provide happiness strong and slight, which exactly matches the arousal level. For negative emotions, including anger or fear, the soul machine expressions were quite similar, which made the mapping easier. To respond to negative emotions, the avatar used compassionate talking.",
      "page_start": 6,
      "page_end": 8
    },
    {
      "section_name": "User Study Design",
      "text": "We conducted a within-group user study to investigate the empathy between a human and a DH when the DH behaves emphatically compared to when the DH behaves in a neutral way. The study procedure was approved by and performed in accordance with the guidelines and regulations of the University of Auckland Human Participants Ethics Committee on March 2, 2020 (reference number: 023799).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Procedure",
      "text": "We recruited 23 participants aged between 21 and 44 years old (ùúá = 30, ùúé = 6). The participants were university students and staff, and they were offered a $40 voucher as an incentive for participating in the study.\n\nAll participants had a set of conversations with both DHs and talked about different emotional topics in a random order after providing written informed consent. In the neutral condition, the DH asked questions, responded neutrally, and showed no expression when the participant was talking. In the empathetic condition, the DH reflected the recognized emotion in its expressions and responses. The empathetic DH expressed its feelings about the image, asked questions, and responded in an empathetic way according to the participant's response.\n\nWe followed the approach described in  [Saffaryazdi, Kirkcaldy, et al. 2024 ] to generate emotional conversations but replaced the human interviewer with the DH. We selected eight images from the IAPS image set  [Lang and Bradley 2007]  as the topic of conversation, two from each of the following categories: High Arousal-High Valence (HAHV), High Arousal-Low Valence (HALV), Low Arousal-High Valence (LAHV), and Low Arousal-Low Valence (LALV). We sorted the dataset according to arousal and valence level, ignored images with high negative subjects like suicide or violence, and chose images with the highest and lowest arousal and valence levels with popular subjects to discuss. After displaying an image from the IAPS dataset, the DH started conversing with the participant about their feelings about the image and related experiences.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion And Empathy Self-Report Data",
      "text": "After each conversation topic, all participants were asked to complete a self-report questionnaire which designed according to the definition of emotion and empathy and previous studies [Koelstra   [Jauniaux et al. 2020] . This data was used to assess real-time emotion recognition and the level of empathy between the participants and the digital human agent. In addition, we used the self-report data to evaluate whether the interaction with the digital human could evoke emotions in the participants.\n\nThe questionnaire consisted of 8 questions, categorized into three parts. The first part (questions 1 and 2) aimed to evaluate the participant's emotional state based on the 2-dimensional emotion model. We compared their responses to these questions with the results of real-time emotion recognition for each trial to assess the quality of the emotion recognition.\n\nThe second part (questions 3 to 5) were created according to the definition of empathy and previous studies to evaluate empathy  [Witchel et al. 2016 ][Jauniaux et al. 2020] . They were used to evaluate the perceived level of empathy displayed by the digital human agent towards the participants, including cognitive and affective empathy.\n\nThe third part (questions 6 to 8) focused on assessing the appropriateness and timing of the digital human's emotions and expressions. These questions were designed to gauge how well the DH's emotions and expressions aligned with the participant's emotions. Table  2  provides an overview of the questionnaire and its related factors.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Human-Agent Rapport",
      "text": "To measure the participants' feelings about their interaction with the agent, we used the postinteraction questionnaire after each condition, adapted from the questionnaire used in earlier research  [Cerekovic et al. 2014; Heerink et al. 2010] . As shown in Table  3 , This questionnaire featured statements that could be responded to on a 5-point Likert scale. This questionnaire evaluates the quality of interaction (QoI), degree of rapport (DoR), degree of liking the agent (DoL), and degree of social presence (DSP). The first three factors was derived from the perception of interaction questionnaire developed by Crekovic et al.  [Cerekovic et al. 2014] . They have been inspired by the study  [Cuperman and Ickes 2009] , in which the authors examine how the Big Five personality traits are expressed in mixed-sex dyadic interactions between strangers. To measure perceived interaction, they developed a \"Perception of Interaction\" questionnaire containing items that evaluate various aspects of participants' interaction experiences. The last factor, DSP, has been validated by  Heerink et al. [Heerink et al. 2010 ] with Cronbach's alpha of 0.83, indicating good reliability. Table  3  shows the details of the human-agent rapport questionnaire.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Analysis",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Self-Report Data",
      "text": "Since all participants had conversations with neutral and empathetic DHs, we performed a withingroup statistical analysis to see the effect of DH type on humans. Data were analyzed using Python and the ARtool package [Kay and Wobbrock 2016] in R. The self-report data were based on 5-point Likert scale, so our data was analyzed using non-parametric methods. We used the aligned rank Table  3 . Human-agent Raport questionnaire and the category of each question. The categories include quality of interaction (QoI), degree of rapport (DoR), degree of liking the agent (DoL), and degree of social presence (DSP).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Question",
      "text": "Category The interaction with DH was smooth, natural, and relaxed.   et al. 2011]  to statistically analyze non-parametric data. A set of three-way repeated measures ANOVA with ART was conducted to examine the effect of DH type (neutral or empathetic) and emotional state of conversation on empathy, emotional state, and the appropriateness of DH responses and expressions. We transferred the reported arousal and valence levels to the low(-1) and high(1) values according to Equations 1 and 2 to reduce the complexity and increase the recognition process in real-time. Additionally, using continuous or multi-class labels can lead to data sparsity, complicating the training of robust models. Low and high labels can address this issue by consolidating data into broader categories, making the classification results more interpretable for end-users. We considered low and high for arousal and valence values as the emotional state of conversation for statistical analysis.\n\nSimilarly, to statistically analyze the effect of agent type on the human-agent report, we conducted a set of one-way repeated measures ANOVA with ART when the agent type was the condition.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Human Body Responses",
      "text": "To compare the effect of agent type on human body responses, we extracted some common and descriptive features from the cleaned EEG, PPG, and EDA signals as described in Table  4 . Since these data were not normally distributed, we used a one-way repeated measures ANOVA with ART when the agent type was the condition.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results",
      "text": "To evaluate the agent's empathy, we assessed the three components of empathy including cognitive empathy, affective empathy and somatic empathy  [Preston and De Waal 2002] . First, statistically show the human-agent empathy and rapport based on self-report data (Cognitive empathy). Second, we examine the differences in human neural and physiological responses when interacting with neutral versus empathetic avatars (Somatic empathy). Finally, we assess the quality of emotion recognition and real-time expressions (Affective empathy).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Human-Agent Empathy",
      "text": "We conducted a series of one-way repeated measure ANOVA with ART to examine the empathy between humans and both DHs. The assessment of empathy was based on three reported factors:\n\n(1) the overall empathy level between the human and agent (empathy), (  2 ) the agent's ability to identify emotions (cognitive empathy), and (3) the agent's capacity to reflect emotions (affective empathy). The results of the analysis revealed significant differences in all of these factors, overall empathy (F(1, 166) = 27, p <0.001), cognitive empathy (F(1, 166) = 4.7, p <0.03), and affective empathy (F(1, 166) = 5.4, p <0.02) during interactions with both neutral and empathetic DHs. Figure  4  illustrates the empathy factors based on the type of agent. It is apparent from the figure that the average scores of the empathy factors were significantly higher in conversations with the empathetic DH in comparison to the neutral DH. However, the ratings were not notably high on average. To understand the reasons behind these moderate ratings, we further evaluated the effect of participants' emotional state during the conversation on the empathy factors. Similar to the previous section, we used a set of two-way repeated measure ANOVA with ART to examine how the emotional state affects empathy factors. The results of the ANOVA test revealed significant differences in empathy factors across various emotional arousal and valence levels (Table  5 ). However, there were no significant interactions between arousal and valence levels.\n\nAs demonstrated in Figure  5 , the ratings for all three factors were remarkably higher during positive emotions and high arousal states. This result aligns with the result in the previous section, indicating that in high valence and arousal, because reflected emotions are more appropriate and at an appropriate time, people feel more empathy during their interaction with the DH.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Human-Agent Rapport",
      "text": "A series of one-way repeated measures ANOVA was conducted to examine the effect of agent type on human-agent rapport factors. Results showed that the type of agent led to statistically significant differences in the Degree of Rapport (DoR) (F(1, 42) = 8.38, p = 0.006) and DoL (F(1, 42) = 6.64, p <0.01). The other two factors, including Degree of Social Presence (DSP) and Quality of Interaction (QoI), were not significantly different. However, point plots in Figure  6  show a higher rating in the interaction with the empathetic agents. We also measured the percentage of participants who reported a higher level of DoR, DoL, DSP, and QoI in interaction with the empathetic agent compared to the neutral one. For all factors, a higher portion of participants reported higher levels of DoL, DoL, DSP, and QoI in interaction with the empathetic agent, which was 82%, 86%, 59%, and 63%, respectively. The higher percentage of participants who reported higher levels of DoR, DoL, DSP, and QoI in interaction with the empathetic agent suggests that the empathetic agent was able to create a more positive and engaging interaction experience for participants. This is likely due to the fact that the empathetic agent was able to understand better and respond to the emotional needs of participants.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Physiological Responses",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Eeg Signals.",
      "text": "The ANOVA test results indicated that the agent type did not significantly impact EEG features. One possible reason for this lack of significance could be the presence of considerable noise in the EEG signals caused by talking during the conversation. The high levels of noise might have made it challenging for the statistical model to identify any discernible patterns in the data. Furthermore, during a conversation, multiple activities such as decision-making, language processing, motor cortex activity, emotion processing, and memory and information processing occur. These activities are likely to be common during interactions with both types of virtual agents. As a result, any subtle differences in brain activity related to the agent type may have been overshadowed by the shared neural processes.\n\nFuture research could focus on developing more robust noise reduction methods for EEG signals to better explore the effect of agent type on brain activities. In addition, using more detailed features in the analysis could help uncover subtle variations in brain activity that might be associated with different types of virtual agents. By addressing these challenges, we can gain deeper insights into the neural mechanisms underlying human-agent interactions and their potential implications for improving virtual agent design and usability.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Ppg Signals.",
      "text": "The ANOVA test results revealed significant differences in almost all heart rate variability (HRV) features during interactions with neutral and empathetic DH (Table  6 ). The point plots in Figure  7  show participants exhibited higher levels of each HRV feature in interactions with the empathetic DH, indicating greater HRV. Figure  7  shows the point plot of the most important HRV features, and the trend of others is the same. HRV is linked to enhanced emotional well-being  [Mather and Thayer 2018] . It is also connected to reduced levels of stress, lower anxiety  [Chalmers et al. 2014] , and improved regulation of emotional responses  [Appelhans and Luecken 2006]  and empathy  [Jauniaux et al. 2020] . Therefore, the interactions with the empathetic agent appear to have helped individuals regulate their emotions more effectively than with the neutral DH. In addition, higher HRV is linked to increased attention  [Appelhans and Luecken 2006] , suggesting that the empathetic agent was more engaging and effectively captured participants' attention.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Eda Signals.",
      "text": "The EDA analysis results revealed that the type of agent had a significant impact on the EDA signals, leading to differences in the statistical characteristics of EDA peaks and the tonic component (Table  6 ). As depicted in Figure  8 , the average amplitude of EDA peaks was higher in interactions with the empathetic agent. A higher EDA peak amplitude indicates a greater increase in skin conductance, which is associated with higher emotional arousal  [Benedek and Kaernbach 2010] . Additionally, the higher positive derivatives of EDA peaks observed in interactions with the empathetic agent suggest that the skin conductance is increasing at a faster rate during these interactions, further indicating higher emotional arousal. While EDA signals may not directly reflect the valence state  [Melander et al. 2018] , they can serve as an indicator of emotional arousal, excitement, and engagement [Kreibig 2010]. In the current study, interactions with the empathetic agent resulted in an increase in skin conductance responses. This could be attributed to a more joyful and engaging experience during interactions with the empathetic agent, leading to heightened emotional arousal.\n\nHowever, it is important to note that some participants provided post-study verbal feedback indicating feelings of nervousness while interacting with the empathetic agent. They found the concept of empathy emanating from a machine to be peculiar and unappealing. for example, some of them mentioned, \"The empathic condition was weird and I could not be able to connect with it. I usually expect a neutral reaction from a stranger or a machine.\", or \"Empathetic Condition was realistic but weird.\". These feelings might have contributed to the observed increase in skin conductance responses during these interactions.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Real-Time Expressions",
      "text": "Throughout the interactions with the DH, three factors were collected to evaluate the emotions expressed by the DH in response to the human participants. These factors focused on assessing the appropriateness of reflected emotions, the expression of emotions, and the timing of these expressions. To understand the impact of the agent type on these factors, a set of one-way repeated measure ANOVA with ART was performed. The result of statistical analysis demonstrated that the agent type significantly affected the appropriateness (F(1, 166) = 10, p <0.002) and timing (F(1, 166) = 6, p <0.01) of the reflected expressions.\n\nIn Figure  9 , the point plot illustrates the effect of agent type on the appropriateness of real-time emotions, expressions, and their timing. It is evident from the figure that the empathetic agent tended to use reflected expressions that were more appropriate and at the correct timing. Moreover, the reflected emotion quality was better in the empathetic DH, although this improvement did not reach statistical significance. To explore the appropriateness of empathetic DH expressions in different emotional states, a series of two-way repeated measure ANOVA with ART was conducted on participants' ratings in the empathetic condition, with arousal and valence as the conditions. To consider the individual's actual emotional state during the conversation, we mapped the reported arousal and valence using Equations 1 and 2, respectively. These values served as the emotional state conditions in the statistical analysis.\n\nThe results revealed significant differences in the empathetic DH's effectiveness in displaying appropriate emotions, expressions, and timing across different valence levels. The empathetic DH's effectiveness in displaying appropriate emotions and expressions across arousal levels was also significant. However, there were no significant interactions between arousal and valence levels concerning emotion, expression, and timing appropriateness (Table  7 ).\n\nThe point plots presented in Figure  10  show that the DH was more successful in expressing positive and high-arousal emotions. One possible explanation could be the model's proficiency in detecting high arousal and valence emotions. Another potential factor might be that the agent struggled to effectively express negative emotions effectively.",
      "page_start": 15,
      "page_end": 17
    },
    {
      "section_name": "Emotion Recognition",
      "text": "We recorded the detected arousal and valence states using the EEG, PPG, and EDA signals and their fusion in real-time. Then, we compared them with the reported arousal and valence levels. Since the reported labels were based on a 5-point Likert scale, we converted them to low(-1) and high(1) according to Equations 1 and 2.\n\nTable  8  shows the percentage of how well the recognized arousal and valence levels using various modalities matches the reported binary arousal and valence levels. It shows the percentage of correct recognition in real-time using various modalities compared to the reported binary arousal and valence levels. It is evident that the percentage of similarity between the reported emotion and the emotion used for generating real-time expressions and selecting appropriate responses using the fusion method is higher compared to other modalities for both arousal and valence levels. Despite these significant percentages, future improvements in recognition results can be achieved by employing more robust models.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Emotional State",
      "text": "We conducted an assessment by comparing the reported arousal and valence levels with the emotional state of the conversation topics in order to estimate the effectiveness of conversations in inducing emotions. To achieve this, we utilized a one-way repeated measure ANOVA with ART, which allowed us to examine the influence of the emotional state of the conversation on individuals' emotions. The results indicated that the emotional state of the conversation was significantly affected by the target emotion of the conversation (p-value <0.05). As shown in Figure  11 , the depicted data reveals that the reported arousal and valence ratings correspond closely with the emotional state of the conversation topics. This indicates that the conversations were effective in eliciting emotions.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Discussion",
      "text": "In this paper, we created a real-time emotion recognition application that recognizes emotions using multimodal data. We combined neural and physiological signals to recognize emotion every 5 seconds in real-time and integrated it with a DH from Soul Machines company. The real-time emotions fed into the DH, and DH created its expressions and responses according to the detected emotions empathetically by mirroring the participant's emotional state according to table 1. Using the multimodal real-time emotion recognition system and the integrated DH, we designed a user study and compared human physiological responses and self-reports in an empathetic and neutral mode.\n\nThe results show that people had emotional conversations with neutral and empathetic agents. They felt stronger emotions during their interactions with the empathetic agents. However, based on self-report data, in negative conversations, the empathetic agent was not as successful in understanding or sharing the emotions. This is likely because the agent could not effectively identify and reflect negative emotions.\n\nThe empathetic agent's expressions and responses were more appropriate in positive conversations and at the appropriate time. However, it could not show appropriate expressions at the right time in the negative conversations. The inability to show appropriate expressions at the right time in negative conversations made it difficult for the agent to build rapport and trust with participants. If an agent is not able to effectively respond to negative emotions, it may come across as insensitive or uncaring. Or maybe it is not appropriate to mirror negative emotions like sadness, fear, and anger. It might be more appropriate to have neutral or concerned expressions in these circumstances. This could be explored in future research.\n\nOur analysis of physiological signals also revealed a significantly higher level of HRV and skin conductance responses in interaction with the empathetic agent. These findings show that people experienced more emotional arousal and engagement in interaction with empathetic agent. They also had better emotion regulation in interaction with the empathetic agent than the neutral agent.\n\nOverall, people enjoyed and engaged more and felt more empathy in interaction with the empathetic DH. This shows that responding based on neural and physiological signals effectively creates empathetic agents and increases the empathy between humans and agents. However, the findings of this study suggest that there is still room for improvement in the development of empathetic agents by improving emotion recognition accuracy.\n\nIn this study, we faced some challenges and limitations, as discussed below. We could create more empathetic agents by addressing these shortcomings in the future.\n\n‚Ä¢ Recognition Accuracy: The level of empathy between humans and the digital human agent was directly influenced by the accuracy of the emotion recognition method. However, achieving a robust model capable of accurately and swiftly detecting emotions using EEG and physiological signals requires further exploration and research. In the future, we can enhance recognition accuracy by using deep learning methods and adopting a more robust fusion strategy. It is essential to also consider the recognition speed while seeking to improve accuracy.\n\nIncorporating behavioral modalities such as facial expressions and speech recognition could also offer the benefits of both behavioral and physiological modalities. By integrating these modalities, we can potentially enhance the overall performance and effectiveness of the empathetic conversational agent. ‚Ä¢ Avatar gender: Due to the constraints in participant recruitment, such as the difficulty of the EEG setup, we considered only a female avatar to interact with participants. However, some individuals find it easier to interact with female interviewers, while others are more at ease with male interviewers  [Brophy 1985; Tam et al. 2020] . To mitigate the effect of the avatar's gender in future studies, increasing the sample size and considering a male avatar would be beneficial. ‚Ä¢ Emotional Transition: Different modalities reflect the emotional transition in different time scales. For instance, the EEG signal can swiftly predict emotional transitions, while the PPG signal takes some time to accurately reflect the real emotion, resulting in differing recognition outcomes across modalities. Although this study did not extensively explore emotional transitions, such transitions commonly occur in everyday conversations. Further research is required to investigate the impact of emotion transitions on different modalities and their recognition accuracy to better understand and leverage their potential in empathetic conversational agents. ‚Ä¢ Personality Effect: Individuals with varying personalities may experience differing levels of empathy with the same agent in similar situations. Some individuals might feel uneasy or awkward interacting with a machine and expressing themselves, while others may feel more at ease and relaxed interacting with the agent compared to a human. To further enhance the reliability of the findings, future studies could recruit a larger and more diverse participant pool, thereby reducing the potential influence of personality variations on the results.\n\n‚Ä¢ Voice Tone The version of the Soul Machines' DH used in this study lacked the capability to control the voice tone to reflect its emotional state. Incorporating an agent that can modulate its voice tone based on the user's emotional state would be beneficial to create a more empathetic interaction in the future. ‚Ä¢ Language model The version of Soul Machines' Digital Human (DH) utilized in this study did not support the use of Large Language Models (LLMs). As a result, the agent's responses were drawn from a predefined database tailored to the detected emotional conditions. In the future, integrating LLMs could significantly enhance the quality of interactions by enabling more dynamic, context-aware, and engaging conversations, thereby improving overall user experience and engagement.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Conclusions",
      "text": "This research aimed to develop and implement a real-time emotion recognition application integrated with a digital human (DH) agent to facilitate empathetic interactions between humans and agents. The novelty lies in the integration of neural and physiological signals into the perception module of conversational agents, enhancing empathetic interactions through real-time physiological and neural emotion recognition. This is one of the first examples of using physiological cues to enable a DH to identify and respond to a user's emotional state empathetically by just relying on neural and physiological cues.\n\nThe findings indicate that participants experienced stronger emotions and engagement during interactions with empathetic agents, as evidenced by physiological responses and self-reports. However, challenges and limitations were also encountered, highlighting areas for improvement in future research.\n\nKey areas for enhancement include using large language models (LLMs) for making the conversations and improving the accuracy of emotion recognition methods. LLMs, such as GPT-based models, can generate more natural, context-aware, and engaging dialogue. By incorporating these models, we can simulate human-like conversations that adapt dynamically to user input, making interactions feel more authentic and engaging.\n\nAdvanced deep learning techniques can be used to improve the accuracy of emotion detection. In addition, integrating behavioral modalities such as facial expression analysis and speech recognition enriches the understanding of user emotions. For example, combining data from facial expressions, vocal tone, and textual content can provide a more comprehensive emotional profile. This holistic approach ensures the system can detect and respond to emotions even when one modality is less informative or unavailable. However, we should consider the limitations of each modality in all situations.\n\nIncorporating voice tone modulation capabilities into digital human agents could considerably enhance the empathetic interaction experience, addressing a current limitation observed in the study.\n\nOverall, while the findings demonstrate the potential of real-time emotion recognition and empathetic digital agents, ongoing research is essential to address the identified challenges and further refine the development of empathetic conversational agents. By addressing these limitations, future studies can contribute to the advancement of human-agent interaction and foster more empathetic and meaningful connections between humans and artificial entities.",
      "page_start": 21,
      "page_end": 21
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) to create an empathetic",
      "page": 4
    },
    {
      "caption": "Figure 1: The Soul Machine‚Äôs DH that we used in this study.",
      "page": 5
    },
    {
      "caption": "Figure 2: The acquired EEG, PPG, and EDA signals were primarily used for real-time",
      "page": 5
    },
    {
      "caption": "Figure 3: shows the overall architecture of the",
      "page": 5
    },
    {
      "caption": "Figure 2: The experimental setup and the devices used for data collection.",
      "page": 6
    },
    {
      "caption": "Figure 3: The overall architecture of the Embodied Conversational Agent (ECA).",
      "page": 6
    },
    {
      "caption": "Figure 4: illustrates the empathy factors based on the type of agent. It is apparent from the figure",
      "page": 11
    },
    {
      "caption": "Figure 4: Effect of agent type on empathy factors including cognitive and affective empathy and overall empathy",
      "page": 11
    },
    {
      "caption": "Figure 5: , the ratings for all three factors were remarkably higher during",
      "page": 12
    },
    {
      "caption": "Figure 5: Effect of the emotional state of conversation topic on empathy factors.",
      "page": 12
    },
    {
      "caption": "Figure 6: show a higher",
      "page": 13
    },
    {
      "caption": "Figure 6: Point-plots of human-agent raport factors. DoL: Degree of Liking, DOR: Degree of Rapport, DSP:",
      "page": 13
    },
    {
      "caption": "Figure 7: show participants exhibited higher levels of each HRV feature in interactions with",
      "page": 14
    },
    {
      "caption": "Figure 7: shows the point plot of the most important",
      "page": 14
    },
    {
      "caption": "Figure 7: Point-plots of HRV features extracted from PPG signals",
      "page": 14
    },
    {
      "caption": "Figure 8: , the average amplitude of EDA peaks was higher",
      "page": 15
    },
    {
      "caption": "Figure 8: Point-plots of EDA features.",
      "page": 15
    },
    {
      "caption": "Figure 9: , the point plot illustrates the effect of agent type on the appropriateness of real-time",
      "page": 16
    },
    {
      "caption": "Figure 9: Effect of agent type on the appropriateness of emotion.",
      "page": 16
    },
    {
      "caption": "Figure 10: show that the DH was more successful in expressing",
      "page": 17
    },
    {
      "caption": "Figure 10: Point-plots of the effect of emotional state on the appropriateness of emotion, expression, and timing",
      "page": 18
    },
    {
      "caption": "Figure 11: Comparing the reported arousal and valence levels with the emotional state of the conversation",
      "page": 19
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Arousal": "1",
          "Valence": "1",
          "state": "HAHV",
          "DH Expression": "Strong Happiness"
        },
        {
          "Arousal": "1",
          "Valence": "-1",
          "state": "HALV",
          "DH Expression": "Anger/Scaredness"
        },
        {
          "Arousal": "-1",
          "Valence": "1",
          "state": "LAHV",
          "DH Expression": "Slight Happiness"
        },
        {
          "Arousal": "-1",
          "Valence": "-1",
          "state": "LALV",
          "DH Expression": "Sadness"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: , This questionnaire",
      "data": [
        {
          "Category": "Emotional state",
          "Question": "1- How negative or positive was the emotion that you felt?",
          "Variable": "Valence"
        },
        {
          "Category": "",
          "Question": "2- What was your arousal level: Calm to Excited?",
          "Variable": "Arousal"
        },
        {
          "Category": "Human-Agent\nEmpathy",
          "Question": "3- I felt that DH was empathetic toward me.",
          "Variable": "Empathic"
        },
        {
          "Category": "",
          "Question": "4- DH identified what I was feeling.",
          "Variable": "Cognitive"
        },
        {
          "Category": "",
          "Question": "5- DH reflected on my emotions.",
          "Variable": "Affective"
        },
        {
          "Category": "real-time\nExpressions",
          "Question": "6- The emotion that DH displayed was appropriate.",
          "Variable": "Appropriate_emotion"
        },
        {
          "Category": "",
          "Question": "7- DH displayed the best expression for the situation.",
          "Variable": "Best_expression"
        },
        {
          "Category": "",
          "Question": "8- DH‚Äôs emotional expressions occurred at appropriate times.",
          "Variable": "Appropriate_time"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Question": "The interaction with DH was smooth,\nnatural, and relaxed.",
          "Category": "QoI"
        },
        {
          "Question": "I felt accepted and respected by DH.",
          "Category": "DoR"
        },
        {
          "Question": "I think DH is likable.",
          "Category": "DoL"
        },
        {
          "Question": "I got along with DH pretty well.",
          "Category": "DoR"
        },
        {
          "Question": "I did not want to get along with DH.",
          "Category": "DoL"
        },
        {
          "Question": "The interaction with DH was forced,\nawkward, and strained.",
          "Category": "QoI"
        },
        {
          "Question": "I felt uncomfortable during the interaction.",
          "Category": "QoI"
        },
        {
          "Question": "I felt that DH was paying attention to my mood.",
          "Category": "DoR"
        },
        {
          "Question": "I was paying attention to the way that DH responded\nto me and I was adapting my own behavior to it.",
          "Category": "DoR"
        },
        {
          "Question": "I think DH finds me likable.",
          "Category": "DoL"
        },
        {
          "Question": "When interacting with DH, I felt\nlike interacting with a real person",
          "Category": "Sp"
        },
        {
          "Question": "I enjoyed the interaction.",
          "Category": "QoI"
        },
        {
          "Question": "I sometimes felt like DH was actually looking at me",
          "Category": "Sp"
        },
        {
          "Question": "I would like to interact more with DH in the future.",
          "Category": "DoL"
        },
        {
          "Question": "I can imagine DH as a living creature",
          "Category": "Sp"
        },
        {
          "Question": "DH often said things completely out of place.",
          "Category": "QoI"
        },
        {
          "Question": "The interaction with DH was pleasant and interesting.",
          "Category": "QoI"
        },
        {
          "Question": "I often realized DH is not a real living creature",
          "Category": "Sp"
        },
        {
          "Question": "Sometimes it seemed as if DH had real feelings",
          "Category": "Sp"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 5: ).However,therewerenosignificantinteractionsbetweenarousalandvalencelevels.",
      "data": [
        {
          "Modality": "EEG",
          "Features": "Power Spectral Density (PSD)\nof EEG band powers",
          "Feature Details": "psd_theta, psd_alpha, psd_beta\npsd_gamma, psd_delta"
        },
        {
          "Modality": "PPG",
          "Features": "Some HRV time domain features\nusing neurokit library",
          "Feature Details": "HRV_MeanNN, HRV_SDNN, HRV_RMSSD, HRV_SDSD, HRV_CVNN,\nHRV_MedianNN, HRV_CVSD, HRV_MadNN, HRV_MCVNN, HRV_IQRNN,\nHRV_pNN50, HRV_pNN20, HRV_HTI, HRV_TINN"
        },
        {
          "Modality": "EDA",
          "Features": "Some statistics of phasic and\ntonic components using neurokit library",
          "Feature Details": "mean, standard deviation, and variance of\nphasic and tonic components and peaks"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Variable": "Empathetic",
          "Condition": "Arousal",
          "F": "17",
          "Df": "1",
          "Df.res": "84",
          "p": "<0.001"
        },
        {
          "Variable": "Empathetic",
          "Condition": "Valence",
          "F": "23",
          "Df": "1",
          "Df.res": "87",
          "p": "<0.001"
        },
        {
          "Variable": "Cognitive",
          "Condition": "Arousal",
          "F": "29",
          "Df": "1",
          "Df.res": "91",
          "p": "<0.001"
        },
        {
          "Variable": "Cognitive",
          "Condition": "Valence",
          "F": "26",
          "Df": "1",
          "Df.res": "91",
          "p": "<0.001"
        },
        {
          "Variable": "Affective",
          "Condition": "Arousal",
          "F": "29",
          "Df": "1",
          "Df.res": "90",
          "p": "<0.001"
        },
        {
          "Variable": "Affective",
          "Condition": "Valence",
          "F": "25",
          "Df": "1",
          "Df.res": "91",
          "p": "<0.001"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 6: The result of statistical analysis for significantly different features for EDA and PPG signal in",
      "data": [
        {
          "Modality": "EDA",
          "Variable": "peaks_var",
          "F": "9.12",
          "Dof": "1",
          "Dof.res": "111",
          "p": "<0.004"
        },
        {
          "Modality": "",
          "Variable": "peaks_std",
          "F": "9.12",
          "Dof": "1",
          "Dof.res": "111",
          "p": "<0.004"
        },
        {
          "Modality": "",
          "Variable": "peaks_mean",
          "F": "5.71",
          "Dof": "1",
          "Dof.res": "111",
          "p": "<0.020"
        },
        {
          "Modality": "",
          "Variable": "peaks_pos_derivatives",
          "F": "8.23",
          "Dof": "1",
          "Dof.res": "111",
          "p": "<0.005"
        },
        {
          "Modality": "",
          "Variable": "tonic_var",
          "F": "6.78",
          "Dof": "1",
          "Dof.res": "111",
          "p": "<0.020"
        },
        {
          "Modality": "",
          "Variable": "tonic_std",
          "F": "6.78",
          "Dof": "1",
          "Dof.res": "111",
          "p": "<0.020"
        },
        {
          "Modality": "PPG",
          "Variable": "HRV_CVNN",
          "F": "11.14",
          "Dof": "1",
          "Dof.res": "103",
          "p": "<0.002"
        },
        {
          "Modality": "",
          "Variable": "HRV_SDNN",
          "F": "10.82",
          "Dof": "1",
          "Dof.res": "103",
          "p": "<0.002"
        },
        {
          "Modality": "",
          "Variable": "HRV_RMSSD",
          "F": "9.98",
          "Dof": "1",
          "Dof.res": "103",
          "p": "<0.003"
        },
        {
          "Modality": "",
          "Variable": "HRV_SDSD",
          "F": "9.92",
          "Dof": "1",
          "Dof.res": "103",
          "p": "<0.003"
        },
        {
          "Modality": "",
          "Variable": "HRV_CVSD",
          "F": "9.29",
          "Dof": "1",
          "Dof.res": "103",
          "p": "<0.003"
        },
        {
          "Modality": "",
          "Variable": "HRV_HTI",
          "F": "8.89",
          "Dof": "1",
          "Dof.res": "103",
          "p": "<0.004"
        },
        {
          "Modality": "",
          "Variable": "HRV_TINN",
          "F": "8.44",
          "Dof": "1",
          "Dof.res": "103",
          "p": "<0.005"
        },
        {
          "Modality": "",
          "Variable": "HRV_MadNN",
          "F": "6.25",
          "Dof": "1",
          "Dof.res": "103",
          "p": "<0.020"
        },
        {
          "Modality": "",
          "Variable": "HRV_MedianNN",
          "F": "5.48",
          "Dof": "1",
          "Dof.res": "103",
          "p": "<0.030"
        },
        {
          "Modality": "",
          "Variable": "HRV_MCVNN",
          "F": "5.16",
          "Dof": "1",
          "Dof.res": "103",
          "p": "<0.030"
        },
        {
          "Modality": "",
          "Variable": "HRV_IQRNN",
          "F": "4.76",
          "Dof": "1",
          "Dof.res": "103",
          "p": "<0.040"
        },
        {
          "Modality": "",
          "Variable": "HRV_MeanNN",
          "F": "4.13",
          "Dof": "1",
          "Dof.res": "103",
          "p": "<0.050"
        },
        {
          "Modality": "",
          "Variable": "HRV_pNN50",
          "F": "4.06",
          "Dof": "1",
          "Dof.res": "103",
          "p": "<0.050"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Variable": "Appropriate Emotion",
          "Condition": "Arousal",
          "F": "13",
          "Df": "1",
          "Df.res": "84",
          "p": "<0.001"
        },
        {
          "Variable": "Best Expression",
          "Condition": "Arousal",
          "F": "6",
          "Df": "1",
          "Df.res": "83",
          "p": "<0.01"
        },
        {
          "Variable": "Best Expression",
          "Condition": "Valence",
          "F": "6",
          "Df": "1",
          "Df.res": "86",
          "p": "<0.01"
        },
        {
          "Variable": "Appropriate Time",
          "Condition": "Valence",
          "F": "9",
          "Df": "1",
          "Df.res": "85",
          "p": "<0.003"
        },
        {
          "Variable": "Appropriate Emotion",
          "Condition": "Valence",
          "F": "12",
          "Df": "1",
          "Df.res": "86",
          "p": "<0.001"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Arousal": "64.8",
          "Valence": "57.1"
        },
        {
          "Arousal": "65.4",
          "Valence": "58.1"
        },
        {
          "Arousal": "58.2",
          "Valence": "52.7"
        },
        {
          "Arousal": "69.1",
          "Valence": "57.3"
        }
      ],
      "page": 17
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Empathic conversational agents for real-time monitoring and co-facilitation of patient-centered healthcare",
      "authors": [
        "Achini Adikari"
      ],
      "year": "2022",
      "venue": "Future Generation Computer Systems"
    },
    {
      "citation_id": "2",
      "title": "A systematic survey on multimodal emotion recognition using learning algorithms",
      "authors": [
        "Naveed Ahmed",
        "Zaher Al Aghbari",
        "Shini Girija"
      ],
      "year": "2023",
      "venue": "Intelligent Systems with Applications"
    },
    {
      "citation_id": "3",
      "title": "Emotions recognition using EEG signals: A survey",
      "authors": [
        "M Soraia",
        "Manuel Alarcao",
        "Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Conversational agents: Goals, technologies, vision and challenges",
      "authors": [
        "Merav Allouch",
        "Amos Azaria",
        "Rina Azoulay"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "5",
      "title": "Detecting naturalistic expression of emotions using physiological signals while playing video games",
      "authors": [
        "Omar Alzoubi",
        "Buthina Almakhadmeh",
        "Muneer Bani Yassein",
        "Wail Mardini"
      ],
      "year": "2023",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "6",
      "title": "Understanding conversational and expressive style in a multimodal embodied conversational agent",
      "authors": [
        "Deepali Aneja",
        "Rens Hoegen",
        "Daniel Mcduff",
        "Mary Czerwinski"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition-based mental healthcare chat-bots: a survey",
      "authors": [
        "Carol Antony",
        "Bestina Pariyath",
        "Seema Safar",
        "Aswin Sahil",
        "Akash R Nair"
      ],
      "year": "2020",
      "venue": "Emotion recognition-based mental healthcare chat-bots: a survey"
    },
    {
      "citation_id": "8",
      "title": "Heart rate variability as an index of regulated emotional responding",
      "authors": [
        "M Bradley",
        "Linda Appelhans",
        "Luecken"
      ],
      "year": "2006",
      "venue": "Review of general psychology"
    },
    {
      "citation_id": "9",
      "title": "A Multi-resolution Mechanism with Multiple Decoders for Empathetic Dialogue Generation",
      "authors": [
        "Mk Ayshabi",
        "Mary Sumam",
        "Idicula"
      ],
      "year": "2021",
      "venue": "2021 8th International Conference on Smart Computing and Communications (ICSCC)"
    },
    {
      "citation_id": "10",
      "title": "A continuous measure of phasic electrodermal activity",
      "authors": [
        "Mathias Benedek",
        "Christian Kaernbach"
      ],
      "year": "2010",
      "venue": "Journal of neuroscience methods"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition by analysis of EEG signals",
      "authors": [
        "Hayfa Blaiech",
        "Mohamed Neji",
        "Ali Wali",
        "Adel Alimi"
      ],
      "year": "2013",
      "venue": "13th International Conference on Hybrid Intelligent Systems"
    },
    {
      "citation_id": "12",
      "title": "Interactions of male and female students with male and female teachers",
      "authors": [
        "Jere Brophy"
      ],
      "year": "1985",
      "venue": "Gender influences in classroom interaction"
    },
    {
      "citation_id": "13",
      "title": "Conversational agents in therapeutic interventions for neurodevelopmental disorders: a survey",
      "authors": [
        "Fabio Catania",
        "Micol Spitale",
        "Franca Garzotto"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "14",
      "title": "How do you like your virtual agent?: Human-agent interaction experience through nonverbal features and personality traits",
      "authors": [
        "Aleksandra Cerekovic",
        "Oya Aran",
        "Daniel Gatica-Perez"
      ],
      "year": "2014",
      "venue": "Human Behavior Understanding: 5th International Workshop"
    },
    {
      "citation_id": "15",
      "title": "Anxiety disorders are associated with reduced heart rate variability: a meta-analysis",
      "authors": [
        "John A Chalmers",
        "Daniel Quintana",
        "J-Anne Maree",
        "Andrew Abbott",
        "Kemp"
      ],
      "year": "2014",
      "venue": "Frontiers in psychiatry"
    },
    {
      "citation_id": "16",
      "title": "Emotion Recognition from EEG and Facial Expressions: a Multimodal Approach",
      "authors": [
        "Valentina Chaparro",
        "Alejandro Gomez",
        "Alejandro Salgado",
        "Lucia Quintero",
        "Natalia Lopez",
        "Luisa Villa"
      ],
      "year": "2018",
      "venue": "2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)",
      "doi": "10.1109/EMBC.2018.8512407"
    },
    {
      "citation_id": "17",
      "title": "Human Emotion Recognition using Physiological Signals: A Survey",
      "authors": [
        "Abhishek Chunawale",
        "Dr Bedekar"
      ],
      "year": "2020",
      "venue": "Human Emotion Recognition using Physiological Signals: A Survey"
    },
    {
      "citation_id": "18",
      "title": "Big Five predictors of behavior and perceptions in initial dyadic interactions: Personality similarity helps extraverts and introverts, but hurts \"disagreeables",
      "authors": [
        "Ronen Cuperman",
        "William Ickes"
      ],
      "year": "2009",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "19",
      "title": "Toward an affect-sensitive AutoTutor",
      "authors": [
        "D' Sidney",
        "Rosalind Mello",
        "Arthur Picard",
        "Graesser"
      ],
      "year": "2007",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "20",
      "title": "Emma: An emotion-aware wellbeing chatbot",
      "authors": [
        "Asma Ghandeharioun",
        "Daniel Mcduff",
        "Mary Czerwinski",
        "Kael Rowan"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "21",
      "title": "Creating rapport with virtual agents",
      "authors": [
        "Jonathan Gratch",
        "Ning Wang",
        "Jillian Gerten",
        "Edward Fast",
        "Robin Duffy"
      ],
      "year": "2007",
      "venue": "Intelligent Virtual Agents: 7th International Conference"
    },
    {
      "citation_id": "22",
      "title": "Assessing Acceptance of Assistive Social Agent Technology by Older Adults: the Almere Model",
      "authors": [
        "Marcel Heerink",
        "Ben Kr√∂se",
        "Vanessa Evers",
        "Bob Wielinga"
      ],
      "year": "2010",
      "venue": "Assessing Acceptance of Assistive Social Agent Technology by Older Adults: the Almere Model"
    },
    {
      "citation_id": "23",
      "title": "Observers' physiological measures in response to videos can be used to detect genuine smiles",
      "authors": [
        "Md Zakir",
        "Tom Gedeon"
      ],
      "year": "2019",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "24",
      "title": "The acoustically emotion-aware conversational agent with speech emotion recognition and empathetic responses",
      "authors": [
        "Jiaxiong Hu",
        "Yun Huang",
        "Xiaozhu Hu",
        "Yingqing Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Combining facial expressions and electroencephalography to enhance emotion recognition",
      "authors": [
        "Yongrui Huang",
        "Jianhao Yang",
        "Siyu Liu",
        "Jiahui Pan"
      ],
      "year": "2019",
      "venue": "Future Internet"
    },
    {
      "citation_id": "26",
      "title": "Empathetic Conversational Agents: Utilizing Neural and Physiological Signals for Enhanced Empathetic Interactions 23",
      "venue": "Empathetic Conversational Agents: Utilizing Neural and Physiological Signals for Enhanced Empathetic Interactions 23"
    },
    {
      "citation_id": "27",
      "title": "Towards imaca: Intelligent multimodal affective conversational agent",
      "authors": [
        "Amir Hussain",
        "Erik Cambria",
        "Thomas Mazzocco",
        "Marco Grassi",
        "Qiu-Feng Wang",
        "Tariq Durrani"
      ],
      "year": "2012",
      "venue": "Neural Information Processing: 19th International Conference"
    },
    {
      "citation_id": "28",
      "title": "Emotion regulation of others' positive and negative emotions is related to distinct patterns of heart rate variability and situational empathy",
      "authors": [
        "Josiane Jauniaux",
        "Marie-H√©l√®ne Tessier",
        "Sophie Regueiro",
        "Florian Chouchou",
        "Alexis Fortin-C√¥t√©",
        "Philip Jackson"
      ],
      "year": "2020",
      "venue": "PloS one"
    },
    {
      "citation_id": "29",
      "title": "Driver's Facial Expression Recognition in Real-Time for Safe Driving",
      "authors": [
        "Mira Jeong",
        "Byoung Chul Ko ;",
        "Matthew Kay",
        "Jacob Wobbrock"
      ],
      "year": "2016",
      "venue": "Package 'ARTool'. \" CRAN Repository"
    },
    {
      "citation_id": "30",
      "title": "Service personalisation of assistive robot for autism care",
      "authors": [
        "Rajiv Khosla",
        "Khanh Nguyen",
        "Mei-Tai Chu"
      ],
      "year": "2015",
      "venue": "IECON 2015-41st Annual Conference of the IEEE Industrial Electronics Society"
    },
    {
      "citation_id": "31",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "32",
      "title": "Electrodermal activity measurements for detection of emotional arousal",
      "authors": [
        "M Ko≈Çodziej",
        "Tarnowski",
        "Majkowski",
        "Rak"
      ],
      "year": "2019",
      "venue": "Bull. Pol. Ac.: Tech"
    },
    {
      "citation_id": "33",
      "title": "Autonomic nervous system activity in emotion: A review",
      "authors": [
        "D Sylvia",
        "Kreibig"
      ],
      "year": "2010",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "34",
      "title": "The International Affective Picture System (IAPS) in the study of emotion and attention",
      "authors": [
        "Peter Lang",
        "Margaret Bradley"
      ],
      "year": "2007",
      "venue": "The International Affective Picture System (IAPS) in the study of emotion and attention"
    },
    {
      "citation_id": "35",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2018",
      "venue": "Deep facial expression recognition: A survey",
      "arxiv": "arXiv:1804.08348"
    },
    {
      "citation_id": "36",
      "title": "Comparing two emotion models for deriving affective states from physiological data",
      "authors": [
        "Antje Lichtenstein",
        "Astrid Oehme",
        "Stefan Kupschick",
        "Thomas J√ºrgensohn"
      ],
      "year": "2008",
      "venue": "Affect and emotion in human-computer interaction"
    },
    {
      "citation_id": "37",
      "title": "It's only a computer: Virtual humans increase willingness to disclose",
      "authors": [
        "Jonathan Gale M Lucas",
        "Aisha Gratch",
        "Louis-Philippe King",
        "Morency"
      ],
      "year": "2014",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "38",
      "title": "A critical review of state-of-the-art chatbot designs and applications",
      "authors": [
        "Bei Luo",
        "Y Raymond",
        "Chunping Lau",
        "Yain-Whar Li",
        "Si"
      ],
      "year": "2022",
      "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery"
    },
    {
      "citation_id": "39",
      "title": "How heart rate variability affects emotion regulation brain networks",
      "authors": [
        "Mara Mather",
        "Julian Thayer"
      ],
      "year": "2018",
      "venue": "Current opinion in behavioral sciences"
    },
    {
      "citation_id": "40",
      "title": "Measuring electrodermal activity to improve the identification of agitation in individuals with dementia",
      "authors": [
        "Catharina Melander",
        "Jesper Martinsson",
        "Silje Gustafsson"
      ],
      "year": "2018",
      "venue": "Dementia and geriatric cognitive disorders extra"
    },
    {
      "citation_id": "41",
      "title": "Towards emotion recognition for virtual environments: an evaluation of eeg features on benchmark dataset",
      "authors": [
        "Maria Luiza",
        "Recena Menezes",
        "Anas Samara",
        "Leo Galway",
        "Anita Sant'anna",
        "Antanas Verikas",
        "Fernando Alonso-Fernandez",
        "Hui Wang",
        "Raymond Bond"
      ],
      "year": "2017",
      "venue": "Personal and Ubiquitous Computing"
    },
    {
      "citation_id": "42",
      "title": "A review on sentiment analysis and emotion detection from text",
      "authors": [
        "Pansy Nandwani",
        "Rupali Verma"
      ],
      "year": "2021",
      "venue": "Social Network Analysis and Mining"
    },
    {
      "citation_id": "43",
      "title": "Empathy: Its ultimate and proximate bases",
      "authors": [
        "D Stephanie",
        "Frans Bm De Preston",
        "Waal"
      ],
      "year": "2002",
      "venue": "Behavioral and brain sciences"
    },
    {
      "citation_id": "44",
      "title": "Embodied conversational agents in clinical psychology: a scoping review",
      "authors": [
        "Simon Provoost",
        "Ming Ho",
        "Jeroen Lau",
        "Heleen Ruwaard",
        "Riper"
      ],
      "year": "2017",
      "venue": "Journal of medical Internet research"
    },
    {
      "citation_id": "45",
      "title": "An affectively aware virtual therapist for depression counseling",
      "authors": [
        "Lazlo Ring",
        "Timothy Bickmore",
        "Paola Pedrelli"
      ],
      "year": "2016",
      "venue": "ACM SIGCHI Conference on Human Factors in Computing Systems (CHI) workshop on Computing and Mental Health"
    },
    {
      "citation_id": "46",
      "title": "Deep Learning for Real Time Facial Expression Recognition in Social Robots",
      "authors": [
        "Ariel Ruiz-Garcia",
        "Nicola Webb",
        "Vasile Palade",
        "Mark Eastwood",
        "Mark Elshaw"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "47",
      "title": "Octopus Sensing: A Python library for human behavior studies",
      "authors": [
        "Nastaran Saffaryazdi",
        "Aidin Gharibnavaz",
        "Mark Billinghurst"
      ],
      "year": "2022",
      "venue": "Journal of Open Source Software"
    },
    {
      "citation_id": "48",
      "title": "Exploring the impact of computer-mediated emotional interactions on human facial and physiological responses",
      "authors": [
        "Nastaran Saffaryazdi",
        "Nikita Kirkcaldy",
        "Gun Lee",
        "Kate Loveys",
        "Elizabeth Broadbent",
        "Mark Billinghurst"
      ],
      "year": "2024",
      "venue": "Telematics and Informatics Reports"
    },
    {
      "citation_id": "49",
      "title": "Emotion detection from text and speech: a survey",
      "authors": [
        "Kashfia Sailunaz",
        "Manmeet Dhaliwal",
        "Jon Rokne",
        "Reda Alhajj"
      ],
      "year": "2018",
      "venue": "Social Network Analysis and Mining"
    },
    {
      "citation_id": "50",
      "title": "Coral: An Approach for Conversational Agents in Mental Health Applications",
      "authors": [
        "Harsh Sakhrani",
        "Saloni Parekh",
        "Shubham Mahajan"
      ],
      "year": "2021",
      "venue": "Coral: An Approach for Conversational Agents in Mental Health Applications",
      "arxiv": "arXiv:2111.08545"
    },
    {
      "citation_id": "51",
      "title": "A review on automatic facial expression recognition systems assisted by multimodal sensor data",
      "authors": [
        "Najmeh Samadiani",
        "Guangyan Huang",
        "Borui Cai",
        "Wei Luo",
        "Chi-Hung Chi",
        "Yong Xiang",
        "Jing He"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "52",
      "title": "Mitigating boredom using an empathetic conversational agent",
      "authors": [
        "Samiha Samrose",
        "Kavya Anbarasu",
        "Ajjen Joshi",
        "Taniya Mishra"
      ],
      "year": "2020",
      "venue": "Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents"
    },
    {
      "citation_id": "53",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "Lin Shu",
        "Jinyan Xie",
        "Mingyue Yang",
        "Ziyi Li",
        "Zhenqi Li",
        "Dan Liao",
        "Xiangmin Xu",
        "Xinyi Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "54",
      "title": "Automatic stress detection in car drivers based on non-invasive physiological signals using machine learning techniques",
      "authors": [
        "Samah Ali I Siam",
        "Fatma Gamel",
        "Talaat"
      ],
      "year": "2023",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "55",
      "title": "Interaction strategies for an affective conversational agent",
      "authors": [
        "Cameron Smith"
      ],
      "year": "2011",
      "venue": "Presence"
    },
    {
      "citation_id": "56",
      "title": "Towards Empathic Conversational Interaction",
      "authors": [
        "Micol Spitale",
        "Franca Garzotto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2nd Conference on Conversational User Interfaces"
    },
    {
      "citation_id": "57",
      "title": "Emotional human-machine conversation generation based on long short-term memory",
      "authors": [
        "Xiao Sun",
        "Xiaoqi Peng",
        "Shuai Ding"
      ],
      "year": "2018",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "58",
      "title": "Multimodal Affective State Assessment Using fNIRS+ EEG and Spontaneous Facial Expression",
      "authors": [
        "Yanjia Sun",
        "Hasan Ayaz",
        "Ali Akansu"
      ],
      "year": "2020",
      "venue": "Brain Sciences",
      "doi": "10.3390/brainsci10020085"
    },
    {
      "citation_id": "59",
      "title": "Female patient preferences regarding physician gender: a national survey",
      "authors": [
        "Tiffanie Y Tam",
        "Austin Hill",
        "Abigail Shatkin-Margolis",
        "Rachel Pauls"
      ],
      "year": "2020",
      "venue": "Minerva Ginecologica"
    },
    {
      "citation_id": "60",
      "title": "Embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders",
      "authors": [
        "Hiroki Tanaka",
        "Hideki Negoro",
        "Hidemi Iwasaka",
        "Satoshi Nakamura"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "61",
      "title": "Combined analysis of GSR and EEG signals for emotion recognition",
      "authors": [
        "Pawe≈Ç Tarnowski",
        "Marcin Ko≈Çodziej",
        "Andrzej Majkowski"
      ],
      "year": "2018",
      "venue": "International Interdisciplinary PhD Workshop"
    },
    {
      "citation_id": "62",
      "title": "Multimodal machine learning for interactive mental health therapy",
      "authors": [
        "Leili Tavabi"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "63",
      "title": "Wearable emotion recognition system based on GSR and PPG signals",
      "authors": [
        "Goran Udoviƒçiƒá",
        "Jurica √êerek",
        "Mladen Russo",
        "Marjan Sikora"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2nd International Workshop on Multimedia for Personal Health and Health Care"
    },
    {
      "citation_id": "64",
      "title": "Affective robot story-telling human-robot interaction: exploratory real-time emotion estimation analysis using facial expressions and physiological signals",
      "authors": [
        "Mikel Val-Calvo",
        "Jos√© Ram√≥n √Ålvarez-S√°nchez",
        "Jos√© Manuel Ferr√°ndez-Vicente",
        "Eduardo Fern√°ndez"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "65",
      "title": "The complex relationship between empathy, engagement and boredom",
      "authors": [
        "J Harry",
        "Carlos Witchel",
        "James Santos",
        "Julian Ackah",
        "Nachiappan Tee",
        "Carina Chockalingam",
        "Westling"
      ],
      "year": "2016",
      "venue": "Proceedings of the European Conference on Cognitive Ergonomics"
    },
    {
      "citation_id": "66",
      "title": "The aligned rank transform for nonparametric factorial analyses using only anova procedures",
      "authors": [
        "Leah Jacob O Wobbrock",
        "Darren Findlater",
        "James Gergle",
        "Higgins"
      ],
      "year": "2011",
      "venue": "Proceedings of the SIGCHI conference on human factors in computing systems"
    },
    {
      "citation_id": "67",
      "title": "Modeling empathy in embodied conversational agents",
      "authors": [
        "√ñzge Nilay"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "68",
      "title": "Empathy framework for embodied conversational agents",
      "authors": [
        "√ñzge Nilay"
      ],
      "year": "2020",
      "venue": "Cognitive Systems Research"
    },
    {
      "citation_id": "69",
      "title": "Understanding affective experiences with conversational agents",
      "authors": [
        "Xi Yang",
        "Marco Aurisicchio",
        "Weston Baxter"
      ],
      "year": "2019",
      "venue": "proceedings of the 2019 CHI conference on human factors in computing systems"
    },
    {
      "citation_id": "70",
      "title": "EmpBot: a t5-based empathetic chatbot focusing on sentiments",
      "authors": [
        "Emmanouil Zaranis",
        "Georgios Paraskevopoulos"
      ],
      "venue": "EmpBot: a t5-based empathetic chatbot focusing on sentiments",
      "arxiv": "arXiv:2111.00310"
    },
    {
      "citation_id": "71",
      "title": "Inferring emotion from conversational voice data: A semi-supervised multi-path generative neural network approach",
      "authors": [
        "Suping Zhou",
        "Jia Jia",
        "Qi Wang",
        "Yufei Dong",
        "Yufeng Yin",
        "Kehua Lei"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "72",
      "title": "Valence-Arousal Model based Emotion Recognition using EEG, peripheral physiological signals and Facial Expression",
      "authors": [
        "Qingyang Zhu",
        "Guanming Lu",
        "Jingjie Yan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 4th International Conference on Machine Learning and Soft Computing"
    }
  ]
}