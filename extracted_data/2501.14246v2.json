{
  "paper_id": "2501.14246v2",
  "title": "Adaptive Progressive Attention Graph Neural Network For Eeg Emotion Recognition",
  "published": "2025-01-24T05:14:21Z",
  "authors": [
    "Tianzhi Feng",
    "Chennan Wu",
    "Yi Niu",
    "Fu Li",
    "Yang Li",
    "Boxun Fu",
    "Zhifu Zhao",
    "Xiaotian Wang"
  ],
  "keywords": [
    "Progressive attention",
    "electroencephalography (EEG)",
    "graph neural network",
    "EEG emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In recent years, numerous neuroscientific studies demonstrate that specific areas of the brain are connected to human emotional responses, with these regions exhibiting variability across individuals and emotional states. To fully leverage these neural patterns, we propose an Adaptive Progressive Attention Graph Neural Network (APAGNN), which dynamically captures the spatial relationships among brain regions during emotional processing. The APAGNN employs three specialized experts that progressively analyze brain topology. The first expert captures global brain patterns, the second focuses on region-specific features, and the third examines emotion-related channels. This hierarchical approach enables increasingly refined analysis of neural activity. Additionally, a weight generator integrates the outputs of all three experts, balancing their contributions to produce the final predictive label. Extensive experiments conducted on SEED, SEED-IV and MPED datasets indicate that our method enhances EEG emotion recognition performance, achieving superior results compared to baseline methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotions significantly impact various aspects of human daily life and psychological health, influencing our decisionmaking, motivation, attention, memory, problem-solving abilities  [1] ,  [2] . Consequently, it is necessary to develop methods to objectively and accurately identify human emotions. The ability for machines to understand human feelings has become an important field of study, drawing significant attention from Human-Machine Interaction (HMI) and pattern recognition researchers lately  [3] ,  [4] ,  [5] ,  [6] . Most of these studies primarily utilize two typical signals, i.e., external and internal responses. External responses mainly include some behavioral data, such as facial expression  [7] , speech signals  [8] , conversational data on social media platforms  [9] . Internal responses are based on physiological signals, including electromyography (EMG)  [10] , electrocardiogram (ECG)  [11] , and electroencephalogram (EEG)  [12] . Neuroscience research suggests that physiological signals provide more direct access to emotional origins than behavioral indicators. Thus, more and more researchers focus on this field during the past several years.\n\nEEG is a widely used technique for recording the electrophysiological activity of neurons in the cerebral cortex via electrodes attached to the scalp  [13] . As a typical physiological Tianzhi Feng, Chennan Wu, Yi Niu, Fu Li, Yang Li, Boxun Fu, Zhifu Zhao and Xiaotian Wang are with the Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, the School of Artificial Intelligence, Xidian University, Xi'an, 710071, China.( * Corresponding author: Fu Li, Yang Li (E-mail: fuli@mail.xidian.edu.cn, liy@xidian.edu.cn).)\n\nsignal, EEG has shown considerable promise in decoding human emotions  [14] ,  [15] ,  [16] ,  [17] . Over the past decades, numerous approaches have emerged for efficiently decoding emotions from EEG signals. Among various methods, convolutional neural networks (CNNs) are extensively employed in recognizing emotions from EEG data. For example, Hasan et al.  [18]  utilize Fast Fourier Transform (FFT) and CNNs to classify 64 emotions, achieving notable accuracy in valence and arousal dimensions. Similarly, Ali et al.  [19]  utilize a Capsule Network (CapsNet) that outperforms traditional support vector machines (SVMs) and CNNs, achieving average accuracies of 80.22% and 85.41%, respectively. Ahmad et al.  [20]  focused on a CNN-based model for classifying emotions into positive, neutral, and negative categories, attaining over 93% accuracy across all categories. Recent advancements include He et al.'s  [21]  CNN architecture for complex EEG signal classification and Aldawsari et al.'s  [22]  optimized 1D-CNN-based process, which significantly improve efficiency. Hybrid models combining CNNs with other techniques, such as LSTM and ensemble learning, have also shown promising results. As an example, Ali et al.  [23]  and Yuvaraj et al.  [24]  demonstrate high accuracies, with Ali's model reaching an impressive 98%. Other notable innovations include the integration of differential entropy with CNN-BiLSTM by Cui et al.  [25] , which achieves over 94% accuracy on DEAP and SEED datasets, and the introduction of CIT-EmotionNet by Lu et al.  [26] , which combines CNN and Transformer models to outperform previous methods. Huang et al.  [27]  incorporate attention mechanisms within a CNN-BiLSTM framework, achieving near-perfect accuracies in multi-class tasks. Further studies, such as those by Saha et al.  [28] , Li et al.  [29] , and Tao et al.  [30] , explore wavelet decomposition, multiscale CNNs, and attention mechanisms, all contributing to improved emotion recognition performance from EEG signals. Wang et al.  [3]  introduces self-supervised learning to CNNs, enhancing both resource utilization and performance. Recent approaches include Jin et al.'s  [31]  CNN-Transformer network for fNIRS-based emotion recognition and Farokhah et al.'s  [32]  simplified 2D CNN with selective channel to improve inter-subject accuracy. Asif et al.  [33]  demonstrate the potential of combining various convolution layers within a CNN framework for subject-independent tasks.\n\nWhile these algorithms have shown impressive performance, they often overlook the inter-channel topological relationships inherent in EEG channel spatial features. This limitation arises from CNNs' inability to process non-Euclidean spaces, such as graphs and manifolds. To better capture the spatial relationships within EEG signals, several Graph Neural Net-work (GNN)-based approaches have been proposed, yielding promising results. Zheng et al.  [34]  propose a Hierarchy Graph Convolution Network (ERHGCN) that achieves classification performance of 90.56% for valence dimension and 88.79% for arousal dimension, demonstrating the potential of hierarchical GCN structures for emotion recognition. Saboksayr et al.  [35]  apply Graph Signal Processing (GSP) techniques, showing enhanced performance when compared with traditional methods and highlighting the efficacy of graph-based approaches. Gilakjani et al.  [36]  combine GNNs with contrastive learning and GAN-based data augmentation, leading to enhanced classification performance on both DEAP and MAHNOB-HCI datasets. Li et al.  [29]  develop a model that learns discriminative graph topologies in EEG networks, achieving an average accuracy of 84.56% in online experiments, which is particularly important for affective brain-computer interfaces. Klepl et al.  [37]  present a comprehensive survey on GNN applications in EEG signal classification, providing valuable insights into their advantages and limitations. In the realm of deep learning, Abdulrahman et al.  [38]  achieve notable accuracies of 70.89% in binary classification and 90.33% for multi-class emotion recognition tasks using advanced deep learning models. DGCNN, proposed by Song et al.  [39] , dynamically learns the graph's adjacency matrix to establish spatial relationships. Zhong et al.'s RGNN  [13]  enhances GNN model robustness against cross-subject variations and noisy labels by incorporating two regularization techniques. The IAG model by Song et al.  [40]  adaptively generates directed graph connections from input graphs, allowing for exploration of intrinsic relationships between EEG regions.\n\nAlthough considerable progress has been made in emotion recognition using EEG, current approaches still face challenges in dynamically capturing the complex relationships between emotional patterns and brain functional regions. One major challenge is the significant variation in emotion-related brain activity between individuals, which demands more adaptive recognition approaches. To address these challenges, we propose a novel Adaptive Progressive Attention Graph Neural Network (APAGNN) that finely screens out critical EEG channels across different subjects through three specialized experts. Specifically, three experts work collaboratively through a progressive analysis pipeline. The first expert analyzes global brain topology patterns, followed by the second expert that identifies emotion-specific regions, while the third expert focuses on critical channels. This hierarchical refinement process enables a comprehensive understanding of emotion-related neural patterns at multiple levels of granularity. The APAGNN model incorporates several innovative components to enhance its effectiveness. To encourage diversity in feature learning while preventing redundancy among experts, we develop a diversity-preserving training strategy that maximizes Jensen-Shannon (JS) Divergence among expert probability distributions. Additionally, we design a dynamic expert fusion method that optimally integrates the outputs from multiple experts for final classification.\n\nTo our knowledge, this is the first work to exploit the discrimination for emotional EEG expression from global to region brain. Our experimental results highlight the effective-ness of this progressive attention model. Besides, we also investigate the impact that varying numbers of experts have on the performance of emotion recognition, compare dynamic and static attention mechanisms, and analyze how these factors influence the overall accuracy of emotion classification.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Preliminaries",
      "text": "In this section, we provide the theoretical preliminaries on GNNs and attention mechanisms, which are the basis of our proposed APAGNN method.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Graph Neural Network",
      "text": "We represent an undirected and connected graph as G = (V, E), where V is the set of vertices (or nodes) and E corresponds to the set of edges, which are unordered pairs of vertices. For a graph with |V| = N nodes, its structure can be characterized by the adjacency matrix A ∈ R N ×N , where A ij = 1 if a connection exists between vertices i and j, and\n\nThe degree matrix D ∈ R N ×N is defined as a diagonal matrix where each diagonal element D ii represents the degree of vertex i, computed as D ii = N j=1 A ij . From these matrices, we can derive the normalized Laplacian matrix\n\n, where I N is the N × N identity matrix. The Laplacian matrix plays a fundamental role in graph signal processing, as it encodes the graph structure and admits an eigendecomposition L = UΛU ⊤ , where U contains the eigenvectors and Λ contains the corresponding eigenvalues.\n\nIn graph convolutional networks (GCNs), signals on the graph x ∈ R N can be transformed from the spatial domain to the spectral domain using the graph Fourier transform (GFT), defined as x = U ⊤ x. The convolution operation on the graph is then formulated in the spectral domain as a filtering operation y = Ug θ (Λ)U ⊤ x, where g θ (Λ) is a learnable filter in the spectral domain that applies spectral weighting based on the eigenvalues in Λ.\n\nTo address the challenges of localized filtering and computational complexity, Defferrard et al.  [41]  introduced Chebyshev polynomials to approximate the spectral filters. This approach leverages the properties of Chebyshev polynomials to construct an efficient filtering process by considering only a limited range of eigenvalues. Consequently, the convolution operation becomes more efficient, enabling the practical application of GCNs to larger graphs while maintaining the ability to capture localized characteristics.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Attention Mechanism",
      "text": "The concept of attention in deep learning was inspired by human visual attention behaviors. During visual processing, humans concentrate on salient regions that are pertinent for decision-making while disregarding less relevant portions  [42] . Similarly, in deep learning applications, certain parts of the input may hold more significance for decision-making than others. Over the past few years, various attention-based approaches have been designed to capture this selective processing, including Class Activation Mapping (CAM), Gradientweighted Class Activation Mapping (Grad-CAM), Saliency Maps, and both hard and soft attention strategies. The integration of these attention modules into neural networks has demonstrated significant performance improvements across numerous studies  [43] ,  [44] ,  [45] ,  [46] .\n\nIn computer vision tasks, feature maps from the final convolutional layer are known to encode rich semantic information. These feature maps are weighted, summed, and then upscaled to the original image size to obtain the CAM for a specific category. The resulting attention map highlights regions of the input that are most influential on classification decisions. Grad-CAM, proposed by Selvaraju et al.  [47] , extends the original CAM approach by leveraging class-specific gradient information as weights for the feature maps. Thus, Grad-CAM works with many different types of network structures without the need for architectural modifications. The Grad-CAM computation is formally expressed as\n\nGrad-CAM represents the corresponding Grad-CAM for category c, and F k denotes the k-th feature map from the final convolutional layer. The ReLU activation ensures that only features positively contributing to the class prediction are preserved in the attention map. The weight coefficient α c k for the k-th feature map is calculated by backpropagating the gradients of the specific class score y c with respect to the feature map j) , where W and H are the width and height of feature map F k , respectively, and F k (i, j) represents the activation value of the k-th feature map at spatial location (i, j). The score y c corresponds to the logit or probability for class c from the network's output. The weighting coefficient α c k signifies the importance of the k-th feature map F k concerning the target class c, effectively measuring its contribution to the prediction score y c .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "For clarity regarding our methodology, Fig.  1  presents the APAGNN model structure. Its goal is to capture more discriminative EEG representation for emotion recognition. We employ three steps to attain this objective. The first step focuses on building the spatial relationships among brain regions from global to local regions progressively. Subsequently, to encourage diversity in feature learning while preventing redundancy among experts, we implement a knowledge diversitypreserving operation. Third, a dynamic expert fusion strategy is designed to integrate the knowledge from multiple experts for final emotion prediction. We detail the complete procedure below.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Multi-Expert Progressive Learning",
      "text": "To model the complex spatial relationships among different neural regions at multiple scales, we employ three specialized experts that progressively analyze brain activity from global patterns to regional features and ultimately to electrodespecific characteristics. Each expert focuses on a different spatial scale, allowing for both broad and fine-grained feature extraction. The structure of each expert is presented in Fig.  2 . Each expert performs two primary tasks: learning discriminative features for emotion classification and generating attention maps to transfer knowledge. By utilizing three experts with this structure, we achieve the goal of progressive learning.\n\nGraph-based EEG representation Learning. To effectively capture the spatial dependencies in EEG signals, we leverage GNNs with Chebyshev filters to construct each expert. The process begins by transforming raw EEG samples into graph representations, where each EEG electrode serves as a vertex. Specifically, we first extract differential entropy (DE) features from five frequency bands. These features serve as the initial node attributes, yielding a feature matrix X ∈ R C×F , in which C represents the electrode count, and F indicates the frequency band count. To model the spatial relationships between electrodes, we construct an adjacency matrix A ∈ R C×C based on the physical arrangement of the electrodes  [48] . Through this process, each raw EEG sample is converted into a graph structure G = (V, E) = (X, A), with G denoting a graph with vertices V and edges E. The graph convolution operation, implemented using Chebyshev polynomial formulation, is given by:\n\nwhere θ k represents learnable parameters for the k-th order Chebyshev polynomial, and T k ( L) denotes the k-th order Chebyshev polynomial evaluated at the scaled Laplacian L.\n\nTo efficiently compute higher-order Chebyshev polynomials, we use the recurrence relation. Define Xk = T k ( L)X, with X0 = X, X1 = LX, and Xk = 2 L Xk-1 -Xk-2 . Thus, we can formulate the convolution operation on graphs as:\n\nFor a single Chebyshev filter, the learnable coefficients are represented by Θ i = [θ 0 , θ 1 , ..., θ K-1 ] T ∈ R K×1 . To learn diverse feature transformations, we implement D parallel filters, each learning a different transformation of the node features. The complete set of learnable parameters is denoted as Θ = [Θ 0 , Θ 1 , ..., Θ D-1 ] ∈ R K×D . After computing the outputs for each node and filter across all K Chebyshev orders, we concatenate the results and apply the weights Θ. Finally, through the ReLU activation function, the learned EEG representation H i ∈ R C×D for the i-th expert is obtained:\n\nThe learned features H i are then flattened to input to a fully connected layer to obtain class probability scores S i ∈ R E for each emotion class, with E indicating the total emotion categories. The i-th expert's classification loss is calculated using:\n\nwhere y l i represents the one-hot encoded actual label for class l, and S l i represents the probability of class l predicted by the i-th expert.\n\nAttention-guided Knowledge Transfer. To facilitate knowledge transfer across experts, we construct an attention map that transfers important electrodes identified by earlier experts to subsequent ones. This allows later experts to build upon patterns discovered by previous ones. We achieve this through dynamic attention method  [47] , which helps us identify the electrodes that contribute significantly to the final decision.\n\nSpecifically, for a target emotion class index l, we calculate the gradient between predicted probability S l i and feature representations H i through backpropagation. The gradients are processed through global average pooling to derive the importance score α l d for each feature dimension d:\n\nThis gradient-based approach reflects the significance of each feature dimension on the classification decision. Next, the weights α l d and the feature map H i are linearly combined to generate the attention map for class l. By taking the mean of attention weights α l d across feature dimensions, we derive the final channel importance I i :\n\nwhere ReLU ensures only positive contributions are preserved.\n\nFinally, using the attention map I i from each expert, we selectively prune weakly emotion-related connections. To ensure consistent thresholding across different attention scales, we first normalize I i to the range [0, 1]:\n\nBased on this normalized attention map, we establish a threshold η ∈ (0, 1). Any channel-node with an activation value below this threshold is considered for pruning. To maintain data structure integrity during training, the pruned node's value is set to zero, and its associated edges are removed by zeroing out the relevant rows and columns in the adjacency matrix. This process yields the attention map Φ i , highlighting the important electrodes learned by the i-th expert, which will be transferred to the next expert. Expert Progressive Learning. The three experts work in sequence to progressively refine the analysis of EEG signals.\n\nThe pipeline begins with the first expert conducting a globalscale analysis on the original graph G, generating both feature representation H 1 and attention map Φ 1 that identifies emotionally salient regions. Building upon this foundation, the second expert receives both the original graph G and the attention map Φ 1 from the first expert to perform region-scale analysis. By utilizing Φ 1 to mask irrelevant nodes in G, it focuses on the emotion-relevant areas. The second expert then applies the same graph convolution and attention computation process with its own learnable parameters, producing a refined feature representation H 2 and an attention map Φ 2 with important electrodes. The final stage of analysis is performed by the third expert, which combines G and Φ 2 to conduct electrode-scale analysis, yielding the feature representation H 3 that captures fine-grained spatial patterns within EEG signals.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "B. Knowledge Diversity Preserving",
      "text": "To encourage knowledge diversity among different experts and reduce redundancy in their focus areas, we introduce a diversity-preserving training method based on Jensen-Shannon (JS) Divergence. This approach aims to maximize the JS divergence between attention maps produced by the first two experts, thereby promoting the extraction of distinct EEG emotion-related attention patterns.\n\nSpecifically, we normalize the attention maps Φ 1 and Φ 2 from the first and second experts to ensure they represent valid probability distributions. This is achieved by applying the softmax function across the EEG channels for each attention map:\n\nwhere C represents the total number of EEG electrodes. The JS divergence between the two normalized attention distribu-tions Φ ′ 1 and Φ ′ 2 from the first and second experts is defined as:\n\nwhere KL(•∥•) denotes the Kullback-Leibler (KL) Divergence.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Dynamic Expert Fusion",
      "text": "To fully leverage the emotion-related features extracted by the three experts, we integrate their representations through a dynamic weight generator. The weight generator assigns importance coefficients ξ 1 , ξ 2 , ξ 3 to each expert's representation H 1 , H 2 , and H 3 , yielding the final output:\n\nThen the final data representation H o is flattened and passed to a fully-connected layer before applying a softmax transformation to generate the predicted probability distribution P (l | X), which indicates the probability that the EEG sample X is classified as the l-th emotional state:\n\nwhere E is the total number of emotion categories, z l represents the logit (pre-softmax activation) for the l-th emotion category.\n\nFinally, the cross-entropy loss, which quantifies the discrepancy between predicted probability outputs and ground-truth label, is defined as:\n\nwhere l gt represents the ground-truth label for the EEG sample X.\n\nFinally, we compute the total loss L total by averaging the combination of prediction loss, expert classification losses, and diversity loss across all training samples in a mini-batch:\n\nwhere N represents the sample count within each mini-batch, and λ and β function as hyperparameters that control the contributions of experts' losses and the JS divergence term, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiment",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Datasets",
      "text": "To validate the proposed APAGNN model, we conducte extensive experiments on three open-access databases, which record EEG data while subjects are viewing emotion-eliciting video clips.\n\nSEED: This dataset includes EEG signals from 15 participants, each participating in three sessions. During each session, the subjects watch 15 video clips, categorized into three emotional states: happy, neutral, and sad. The raw EEG signals are sampled every second and passed through a 0.3-50 Hz bandpass filter. For experimental consistency, we followed the same protocol as  [49] , using data from the initial nine trials for training and the final six trials for testing.\n\nSEED-IV: This dataset resembles SEED but includes four emotion types, with a total of 24 video clips per session (6 clips per emotion). We adhered to the experimental protocol in  [50] , where data from the first sixteen trials is used for training and the final eight trials are utilized for testing.\n\nMPED: This dataset comprises EEG signals from 30 subjects (one session per subject) and encompasses seven emotion types. For each emotion category, participants viewed four different video clips, yielding 28 trials per session. Following the protocol in  [51] , we first obtain Short-Time Fourier Transform (STFT) features from five frequency bands, and then use 21 trials for model training and reserve the final seven trials for testing purposes.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Implementation Details",
      "text": "For our experiment, the node-pruning threshold η is set to 0.5. The Chebyshev kernel size K is set to 3, and the number of convolution filters D is set to 32. Optimization is performed using the Adam optimizer. We train the model for 100 epochs using a batch size of 64 and a learning rate of 1e-3. Model performance is evaluated using mean accuracy (ACC) and standard deviation (STD). The model is trained with PyTorch on a GeForce RTX 2080Ti GPU.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Experiment Results",
      "text": "The first category includes GNN-based methods such as DGCNN  [39] , RGNN  [13] , IAG  [40] , and V-IAG  [52] . These methods use graph neural networks to analyze EEG data. The second category consists of approaches that focus on specific brain regions or important electrode channels. This group includes BiDANN  [53] , BiHDM  [54] , and DBN  [49] . Additionally, we compare our model with traditional machine learning methods, specifically SVM  [55]  and standard GNN  [41] , which are common baseline techniques in this field.\n\nAs shown in Table  I , the APAGNN model achieves superior performance compared to the existing methods on all three datasets. Specifically, it achieves accuracies of 96.38% in SEED, 86.64% in SEED-IV, and 41.58% in MPED. Notably, the APAGNN model surpasses the previous best method, IAG, by 0.94% and 1.2% in accuracy, while reducing the standard deviations by 1.29% and 4.14% on the SEED and MPED datasets, respectively. Remarkably, APAGNN consistently achieves the lowest standard deviation across all three datasets, demonstrating its superior stability and robustness.\n\nTo further confirm the progressive attention module's effectiveness, we conduct an extra experiment by replacing the progressive learning process with single attention. The resulting model can be denoted as APAGNN-2E. For better comparison, we denote our APAGNN as APAGNN-3E here.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Confusion Matrix",
      "text": "To better understand the performance of APAGNN, Fig.  3  displays the confusion matrices across SEED, SEED-IV, and MPED datasets. In these matrices, the columns show the predicted categories, while the rows represent the true categories. Analyzing these results reveals several important insights.\n\n1) In the SEED three-class classification task, our model performs exceptionally well, achieving over 90% accuracy for all three emotion categories. This high accuracy suggests that the EEG patterns for each emotion are distinct and easy to differentiate.\n\n2) In the SEED-IV four-class classification task, which includes the addition of the fear category, we observe a slight decrease in performance for the happy, neutral, and sad emotions. Among these, sad emotion is most affected. As shown in Fig.  3(b ), the highest confusion occurs between fear and sad emotions, with 9.4% of fear instances misclassified as sad. This suggests that negative emotions like fear and sadness may have similar underlying EEG patterns.\n\n3) In the more challenging MPED seven-class classification task, our model performs better in recognizing neutral, joy, fear, and sad emotions, but struggles more with funny, disgust, and angry emotions. Consistent with the SEED and SEED-IV tasks, the APAGNN achieves the highest accuracy in classifying neutral emotion.\n\nOverall, our model shows strong performance across different emotion categories, especially in recognizing happy, neutral, sad, and fear emotions, with particularly good results for neutral emotions. These findings demonstrate that our APAGNN model effectively captures the key EEG features that distinguish different emotional states.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "E. Attention Visualization",
      "text": "To validate whether our method can successfully identify important electrodes through progressive learning, we visualize the attention maps of the first three subjects from each dataset, as shown in Fig.  4 . These attention maps highlight important parts of the brain that play a role in processing emotions, including regions within the prefrontal and temporal lobes. These critical regions are consistent with established neuroscience findings on emotion recognition  [56] ,  [57] ,  [58] . Moreover, the activation in occipital lobe, which is associated with visual processing, may be related to the video-based stimulus material used in the datasets. Notably, the visualization outcomes confirm our APAGNN method achieves the goal of progressive attention. Taking the subjects in Fig.  4 (a) as an example, the first expert focuses on a broad range of emotionrelated brain regions, while the second expert, building upon the knowledge from the first expert, further concentrates on more specific regions of importance. Meanwhile, we can see subtle individual variations on different subjects. This observation demonstrates our model's ability to both learn generally important emotion-processing areas and adapt to individual-specific neural patterns.\n\nTo evaluate the advantage of our dynamic attention module in multi-expert progressive learning architecture, we modify our APAGNN framework to two variants that utilize predefined critical channel sets from previous studies  [49] ,  [13] , denoted as SPAGNN-v1 and SPAGNN-v2. For fair comparison, we carefully design the channel selection scheme according to important brain regions. Specifically, for the first expert, both variants use identical channel selections, including FPZ, FP1, FP2, AF3, AF4, F5, F6, F7, F8, FT7, FT8, C5, C6, T7, T8, CP5, CP6, TP7, TP8, P5, P7, P8, PO5, PO6, PO7, PO8, CB1, and CB2. For the second expert, the two variants utilize different channel sets based on prior research. SPAGNN-v1 employs 12 key channels identified by Zheng et al.  [49]  (FT7, FT8, C5, C6, T7, T8, CP5, CP6, TP7, TP8, P7, P8), while SPAGNN-v2 employs eight critical channels based on Zhong et al.  [13]  (FP1, FP2, AF3, AF4, F6, F8, CB2, PO8). We evaluate SPAGNN-v1 and SPAGNN-v2 using the SEED and SEED-IV datasets, with detailed results presented in Table  IV . SPAGNN-v1 and SPAGNN-v2 with predefined electrodes achieve classification accuracies of 93.37% and 92.38% on SEED, respectively. And the performance reaches 82.65% and 81.44% on SEED-IV, respectively. While our APAGNN with dynamic attention module demonstrates superior performance, improving accuracy by 3.01% on SEED and 3.99% on SEED-IV compared to the best results of both variants. These results demonstrate the advantages of dynamic attention module over static approaches.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "F. T-Sne Visualization",
      "text": "To further validate our proposed model's effectiveness, we employ t-SNE  [59]  to visualize APAGNN's feature learning process. Fig.  5  presents the t-SNE visualizations for the first three subjects in the MPED dataset, where distinct emotions",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Conclusion",
      "text": "This study introduces APAGNN, an innovative model for decoding emotional states from EEG signals. Our approach not only achieves leading performance but also demonstrates the capability to adaptively identify subject-specific emotionrelated EEG channel sets and corresponding brain region topological subgraphs. The model's adaptive nature enables the discovery of individual differences in the spatial distribution of emotion-related brain regions. Through APAGNN's interpretable architecture, we gain valuable insights into how various parts of the brain work together during emotional experiences. Our findings suggest that emotion-related neural activity patterns exhibit significant inter-subject variability rather than following a universal template, emphasizing the importance of personalized approaches in emotion recognition systems. This work contributes to both the theoretical understanding of emotion processing in different brain regions and the practical advancement of EEG-based emotion recognition technologies.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of APAGNN. In the multi-expert architecture, each expert learns discriminative features for emotion classification while generating",
      "page": 3
    },
    {
      "caption": "Figure 1: presents the",
      "page": 3
    },
    {
      "caption": "Figure 2: Each expert performs two primary tasks: learning discrimina-",
      "page": 3
    },
    {
      "caption": "Figure 2: Schematic illustration of the expert module. Each expert performs two parallel tasks: (a) graph convolution operations for feature learning and (b)",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion matrices for subject-dependent experiments on SEED,",
      "page": 7
    },
    {
      "caption": "Figure 3: displays the confusion matrices across SEED, SEED-IV,",
      "page": 7
    },
    {
      "caption": "Figure 3: (b), the highest confusion occurs between fear",
      "page": 7
    },
    {
      "caption": "Figure 4: These attention maps highlight",
      "page": 7
    },
    {
      "caption": "Figure 5: presents the t-SNE visualizations for the first",
      "page": 7
    },
    {
      "caption": "Figure 4: Attention maps generated by three experts across different subjects from three datasets: (a) SEED, (b) SEED-IV, and (c) MPED. Each row represents",
      "page": 8
    },
    {
      "caption": "Figure 5: T-SNE visualization of the feature learning process on the MPED dataset. Each row shows the feature evolution for an individual subject across four",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "86.6": "7.04",
          "1.98": "90.87",
          "5.88": "1.58",
          "5.53": "0.51"
        },
        {
          "86.6": "5.99",
          "1.98": "3.43",
          "5.88": "85.76",
          "5.53": "4.82"
        },
        {
          "86.6": "3.66",
          "1.98": "5.30",
          "5.88": "9.40",
          "5.53": "81.64"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "92.73": "1.63",
          "4.81": "97.94",
          "2.46": "0.43"
        },
        {
          "92.73": "0.99",
          "4.81": "0.81",
          "2.46": "98.2"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "44.69": "5.36",
          "16.58": "39.00",
          "2.11": "24.83",
          "7.92": "6.44",
          "9.00": "7.89",
          "11.25": "11.67",
          "8.44": "4.81"
        },
        {
          "44.69": "4.00",
          "16.58": "7.67",
          "2.11": "59.42",
          "7.92": "8.06",
          "9.00": "5.25",
          "11.25": "7.56",
          "8.44": "8.06"
        },
        {
          "44.69": "4.36",
          "16.58": "6.42",
          "2.11": "10.72",
          "7.92": "39.86",
          "9.00": "14.69",
          "11.25": "14.17",
          "8.44": "9.78"
        },
        {
          "44.69": "10.31",
          "16.58": "7.83",
          "2.11": "12.58",
          "7.92": "11.03",
          "9.00": "45.44",
          "11.25": "6.08",
          "8.44": "6.72"
        },
        {
          "44.69": "6.53",
          "16.58": "12.61",
          "2.11": "11.64",
          "7.92": "13.64",
          "9.00": "10.86",
          "11.25": "33.83",
          "8.44": "10.89"
        },
        {
          "44.69": "11.08",
          "16.58": "8.94",
          "2.11": "10.39",
          "7.92": "11.69",
          "9.00": "14.31",
          "11.25": "14.75",
          "8.44": "28.83"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Explicit and implicit emotion regulation: A dual-process framework",
      "authors": [
        "A Gyurak",
        "J Gross",
        "A Etkin"
      ],
      "year": "2011",
      "venue": "COGNITION AND EMOTION",
      "doi": "10.1080/02699931.2010.544160"
    },
    {
      "citation_id": "2",
      "title": "The emotion process: Event appraisal and component differentiation",
      "authors": [
        "K Scherer",
        "A Moors"
      ],
      "year": "2019",
      "venue": "Annual Review of Psychology",
      "doi": "10.1146/annurev-psych-122216-011854"
    },
    {
      "citation_id": "3",
      "title": "Selfsupervised eeg emotion recognition models based on cnn",
      "authors": [
        "X Wang",
        "Y Ma",
        "J Cammon",
        "F Fang",
        "Y Gao",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "4",
      "title": "Msfr-gcn: A multi-scale feature reconstruction graph convolutional network for eeg emotion and cognition recognition",
      "authors": [
        "D Pan",
        "H Zheng",
        "F Xu",
        "Y Ouyang",
        "Z Jia",
        "C Wang",
        "H Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "5",
      "title": "Fine-grained interpretability for eeg emotion recognition: Concat-aided grad-cam and systematic brain functional network",
      "authors": [
        "B Liu",
        "J Guo",
        "C Chen",
        "X Wu",
        "T Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Pr-pl: A novel prototypical representation based pairwise learning framework for emotion recognition using eeg signals",
      "authors": [
        "R Zhou",
        "Z Zhang",
        "H Fu",
        "L Zhang",
        "L Li",
        "G Huang",
        "F Li",
        "X Yang",
        "Y Dong",
        "Y Zhang",
        "Z Liang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition through facial expression analysis based on a neurofuzzy network",
      "authors": [
        "S Ioannou",
        "A Raouzaiou",
        "V Tzouvaras",
        "T Mailis",
        "K Karpouzis",
        "S Kollias"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition by speech signals",
      "authors": [
        "O.-W Kwon",
        "K Chan",
        "J Hao",
        "T.-W Lee"
      ],
      "year": "2003",
      "venue": "Eighth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "9",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition from surface emg signal using wavelet transform and neural network",
      "authors": [
        "B Cheng",
        "G Liu"
      ],
      "year": "2008",
      "venue": "Proceedings of the 2nd international conference on bioinformatics and biomedical engineering (ICBBE)"
    },
    {
      "citation_id": "11",
      "title": "Ecg pattern analysis for emotion detection",
      "authors": [
        "F Agrafioti",
        "D Hatzinakos",
        "A Anderson"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "12",
      "title": "Multichannel eeg-based emotion recognition via group sparse canonical correlation analysis",
      "authors": [
        "W Zheng"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "13",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Affective brain-computer interfaces (abcis): A tutorial",
      "authors": [
        "D Wu",
        "B Lu",
        "B Hu",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "15",
      "title": "A progressive learning classifier based on dynamic differential weighted network for feature identification of brain network series",
      "authors": [
        "W Xue",
        "H He"
      ],
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "16",
      "title": "Dualencoder vae-gan with spatiotemporal features for emotional eeg data augmentation",
      "authors": [
        "C Tian",
        "Y Ma",
        "J Cammon",
        "F Fang",
        "Y Zhang",
        "M Meng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "17",
      "title": "Cross-cultural emotion recognition with eeg and eye movement signals based on multiple stacked broad learning system",
      "authors": [
        "X Gong",
        "C Chen",
        "T Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "18",
      "title": "Fine-grained emotion recognition from eeg signal using fast fourier transformation and cnn",
      "authors": [
        "M Hasan",
        "S Rokhshana-Nishat-Anzum",
        "T Yasmin",
        "Pias"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Intelligent Vision and Computing (ICIEVic)",
      "doi": "10.1109/ICIEVicIVPR52578.2021.9564204"
    },
    {
      "citation_id": "19",
      "title": "Eeg emotion signal of artificial neural network by using capsule network",
      "authors": [
        "U Ali",
        "H Li",
        "R Yao",
        "Q Wang",
        "W Hussain",
        "S Duja",
        "M Amjad"
      ],
      "year": "2020",
      "venue": "International Journal of Advanced Computer Science and Applications (IJACSA)",
      "doi": "10.14569/ijacsa.2020.0110154"
    },
    {
      "citation_id": "20",
      "title": "Deep learning based on cnn for emotion recognition using eeg signal",
      "authors": [
        "I Ahmad",
        "S Zhang",
        "S Saminu",
        "L Wang",
        "A Isselmou",
        "Z Cai",
        "I Javaid",
        "S Kamhi",
        "U Kulsum"
      ],
      "year": "2021",
      "venue": "WSEAS Transactions on Systems and Control",
      "doi": "10.37394/232014.2021.17.4"
    },
    {
      "citation_id": "21",
      "title": "Hmt: An eeg signal classification method based on cnn architecture",
      "authors": [
        "M He",
        "Y Wu",
        "Z Li",
        "S Wang",
        "W Li",
        "W Zhou",
        "H Rong",
        "J Wang"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Multimedia Signal Processing (ICMSP)",
      "doi": "10.1109/ICMSP58539.2023.10170904"
    },
    {
      "citation_id": "22",
      "title": "Optimizing 1dcnn-based emotion recognition process through channel and feature selection from eeg signals",
      "authors": [
        "H Aldawsari",
        "S Al-Ahmadi",
        "F Muhammad"
      ],
      "year": "2023",
      "venue": "Diagnostics",
      "doi": "10.3390/diagnostics13162624"
    },
    {
      "citation_id": "23",
      "title": "Eeg-based emotion recognition using hybrid cnn and lstm classification",
      "authors": [
        "L Ali",
        "G Li",
        "B Chakravarthi",
        "S.-C Ng",
        "M Ezilarasan",
        "M.-F Leung"
      ],
      "year": "2022",
      "venue": "Frontiers in Computational Neuroscience",
      "doi": "10.3389/fncom.2022.1019776"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition from spatio-temporal representation of eeg signals via 3d-cnn with ensemble learning techniques",
      "authors": [
        "R Yuvaraj",
        "A Baranwal",
        "A Prince",
        "M Murugappan",
        "J Mohammed"
      ],
      "year": "2023",
      "venue": "Brain Sciences",
      "doi": "10.3390/brainsci13040685"
    },
    {
      "citation_id": "25",
      "title": "A novel de-cnn-bilstm multi-fusion model for eeg emotion recognition",
      "authors": [
        "F Cui",
        "R Wang",
        "W Ding",
        "Y Chen",
        "L Huang"
      ],
      "year": "2022",
      "venue": "Mathematics",
      "doi": "10.3390/math10040582"
    },
    {
      "citation_id": "26",
      "title": "Cit-emotionnet: Cnn interactive transformer network for eeg emotion recognition",
      "authors": [
        "W Lu",
        "H Ma",
        "T Tan"
      ],
      "year": "2023",
      "venue": "Cit-emotionnet: Cnn interactive transformer network for eeg emotion recognition",
      "doi": "10.48550/arXiv.2305.05548"
    },
    {
      "citation_id": "27",
      "title": "A model for eeg-based emotion recognition: Cnn-bi-lstm with attention mechanism",
      "authors": [
        "Z Huang",
        "Y Ma",
        "R Wang",
        "W Li",
        "Y Dai"
      ],
      "year": "2023",
      "venue": "Electronics",
      "doi": "10.3390/electronics12143188"
    },
    {
      "citation_id": "28",
      "title": "Automatic emotion recognition from eeg signal utilizing wavelet packet node reconstruction and a cnn classifier",
      "authors": [
        "O Saha",
        "M Mahmud",
        "S Fattah"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Computer and Information Technology (ICCIT)",
      "doi": "10.1109/ICCIT60459.2023.10440995"
    },
    {
      "citation_id": "29",
      "title": "Eeg-based emotion recognition using spatial-temporal-connective features via multi-scale cnn",
      "authors": [
        "T Li",
        "B Fu",
        "Z Wu",
        "Y Liu"
      ],
      "year": "2023",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2023.3270317"
    },
    {
      "citation_id": "30",
      "title": "Eeg-based emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.3025777"
    },
    {
      "citation_id": "31",
      "title": "Research on emotion recognition method of cerebral blood oxygen signal based on cnn-transformer network",
      "authors": [
        "Z Jin",
        "Z Xing",
        "Y Wang",
        "S Fang",
        "X Gao",
        "X Dong"
      ],
      "year": "2023",
      "venue": "Sensors",
      "doi": "10.3390/s23208643"
    },
    {
      "citation_id": "32",
      "title": "Simplified 2d cnn architecture with channel selection for emotion recognition using eeg spectrogram",
      "authors": [
        "L Farokhah",
        "R Sarno",
        "C Fatichah"
      ],
      "year": "2023",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2023.3275565"
    },
    {
      "citation_id": "33",
      "title": "Inter subject emotion recognition using spatio-temporal features from eeg signal",
      "authors": [
        "M Asif",
        "D Srivastava",
        "A Gupta",
        "U Tiwary"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Software Engineering and Computer Systems (ICSECS)",
      "doi": "10.1109/ICSEC59635.2023.10329776"
    },
    {
      "citation_id": "34",
      "title": "Eeg emotion recognition based on hierarchy graph convolution network",
      "authors": [
        "F Zheng",
        "B Hu",
        "S Zhang",
        "Y Li",
        "X Zheng"
      ],
      "venue": "2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), 2021",
      "doi": "10.1109/BIBM52615.2021.9669465"
    },
    {
      "citation_id": "35",
      "title": "Eeg-based emotion classification using graph signal processing",
      "authors": [
        "S Saboksayr",
        "G Mateos",
        "M ¸etin"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP39728.2021.9414342"
    },
    {
      "citation_id": "36",
      "title": "A graph neural network for eeg-based emotion recognition with contrastive learning and generative adversarial neural network data augmentation",
      "authors": [
        "S Gilakjani",
        "H Osman"
      ],
      "year": "2023",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2023.3344476"
    },
    {
      "citation_id": "37",
      "title": "Graph neural network-based eeg classification: A survey",
      "authors": [
        "D Klepl",
        "M Wu",
        "F He"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering",
      "doi": "10.1109/TNSRE.2024.3355750"
    },
    {
      "citation_id": "38",
      "title": "A novel approach for emotion recognition based on eeg signal using deep learning",
      "authors": [
        "A Abdulrahman",
        "M Baykara",
        "T Alakus"
      ],
      "year": "2022",
      "venue": "Applied Sciences",
      "doi": "10.3390/app121910028"
    },
    {
      "citation_id": "39",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Instance-adaptive graph for eeg emotion recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "41",
      "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
      "authors": [
        "M Defferrard",
        "X Bresson",
        "P Vandergheynst"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "42",
      "title": "Show, attend and tell: Neural image caption generation with visual attention",
      "authors": [
        "K Xu",
        "J Ba",
        "R Kiros",
        "K Cho",
        "A Courville",
        "R Salakhudinov",
        "R Zemel",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "43",
      "title": "Grad-cam guided channel-spatial attention module for fine-grained visual classification",
      "authors": [
        "S Xu",
        "D Chang",
        "J Xie",
        "Z Ma"
      ],
      "year": "2021",
      "venue": "2021 IEEE 31st International Workshop on Machine Learning for Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Sharpen focus: Learning with attention separability and consistency",
      "authors": [
        "L Wang",
        "Z Wu",
        "S Karanam",
        "K.-C Peng",
        "R Singh",
        "B Liu",
        "D Metaxas"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "45",
      "title": "Ts-cam: Token semantic coupled attention map for weakly supervised object localization",
      "authors": [
        "W Gao",
        "F Wan",
        "X Pan",
        "Z Peng",
        "Q Tian",
        "Z Han",
        "B Zhou",
        "Q Ye"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "46",
      "title": "Learning a mixture of granularity-specific experts for fine-grained categorization",
      "authors": [
        "L Zhang",
        "S Huang",
        "W Liu",
        "D Tao"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "47",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "48",
      "title": "Gmss: Graph-based multi-task self-supervised learning for eeg emotion recognition",
      "authors": [
        "Y Li",
        "J Chen",
        "F Li",
        "B Fu",
        "H Wu",
        "Y Ji",
        "Y Zhou",
        "Y Niu",
        "G Shi",
        "W Zheng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "49",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "50",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "51",
      "title": "Mped: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "52",
      "title": "Variational instance-adaptive graph for eeg emotion recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "Y Li",
        "X Zhou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "53",
      "title": "A novel neural network model based on cerebral hemispheric asymmetry for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Z Cui",
        "T Zhang",
        "Y Zong"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "54",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "55",
      "title": "Least squares support vector machine classifiers: a large scale algorithm",
      "authors": [
        "J Suykens",
        "L Lukas",
        "P Van Dooren",
        "B De Moor",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "European Conference on Circuit Theory and Design"
    },
    {
      "citation_id": "56",
      "title": "Frontal eeg asymmetry as a moderator and mediator of emotion",
      "authors": [
        "J Coan",
        "J Allen"
      ],
      "year": "2004",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "57",
      "title": "The amygdala: vigilance and emotion",
      "authors": [
        "M Davis",
        "P Whalen"
      ],
      "year": "2001",
      "venue": "Molecular psychiatry"
    },
    {
      "citation_id": "58",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "59",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}