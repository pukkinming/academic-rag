{
  "paper_id": "2008.12360v1",
  "title": "Language Models As Emotional Classifiers For Textual Conversation",
  "published": "2020-08-27T20:04:30Z",
  "authors": [
    "Connor T. Heaton",
    "David M. Schwartz"
  ],
  "keywords": [
    "Affective computing",
    "Natural Language Processing",
    "Language modeling"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotions play a critical role in our everyday lives by altering how we perceive, process and respond to our environment. Affective computing aims to instill in computers the ability to detect and act on the emotions of human actors. A core aspect of any affective computing system is the classification of a user's emotion. In this study we present a novel methodology for classifying emotion in a conversation. At the backbone of our proposed methodology is a pre-trained Language Model (LM), which is supplemented by a Graph Convolutional Network (GCN) that propagates information over the predicate-argument structure identified in an utterance. We apply our proposed methodology on the IEMOCAP and Friends data sets, achieving state-of-the-art performance on the former and a higher accuracy on certain emotional labels on the latter. Furthermore, we examine the role context plays in our methodology by altering how much of the preceding conversation the model has access to when making a classification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions play a critical role in our everyday lives. They can alter how we perceive, process and respond to our environment. For instance, psychological literature  [11]  reveals that people with depression interpret stimuli differently than those without depression. Furthermore, emotions influence how we express ourselves in conversation, revealing more information than just what was said. Additionally, in conversations, emotional responses can display empathy, communicating understanding and making two (or more) people feel closer together.\n\nAffective computing aims to give computers the ability to detect and act on human emotions. Understanding a user's emotional state provides many new opportunities for computer systems. Detecting frustration could allow a computer to identify when a user is having trouble performing a task and can suggest help, for example. Chatbots used in customer support can engage in more realistic conversations if they are able to understand the emotional content in received messages and incorporate that into a more accurate and meaningful response. Furthermore, affective computing systems can act as a safe guard for individuals with depression; a system that can understand the emotional content of items on the web and the current emotional state of the user can automatically determine when and what it should filter to prevent the user from feeling distressed.\n\nThe first step in all of the systems mentioned above is detecting emotion. This study focuses on the construction of an emotional classifier for textual conversations. Our method leverages pre-trained language models (BERT and XLNet in our experiments) in conjunction with a Graph Convolutional Network (GCN) which is used to process the predicate-argument structure of an utterance, identified through semantic role labeling (SRL). We achieved substantial improvements to the state-of-the-art when applying our method on the IEMOCAP data set. When applying it on the Friends data set, our method does not beat the state-of-the-art, but does a better job of identifying emotions that appear infrequently. We also analyze the importance of context in conversation by altering the amount of preceding utterances the LM's have access to while making their classification.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Literature",
      "text": "As our work deals with conversation transcribed as text, we describe literature pertaining to language modelling first. Then, related work in emotional classification is mentioned. Finally, the data sets being used in the study are discussed.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Language Modeling",
      "text": "Language modeling is a natural language processing (NLP) task in which a model is asked to learn underlying distribution and relation among word tokens in a specified vocabulary  [8] . It is common for a language model (LM) to be pre-trained on a large-scale, general purpose corpus before being fine-tuned for a specific NLP task. Leveraging a LM in such a fashion has shown to be effective for performing a variety of natural language understanding (NLU) and natural language inference (NLI) tasks including speech recognition  [12] , machine translation  [15] , and text summarization  [6] .\n\nUntil recently most LM architectures were based on recurrent neural networks  [8] . However, in late 2018, Devlin et al released a model they dubbed BERT (Bidirectional Encoder Representation from Transformers)  [4] . As the name suggests, BERT is a language model based on the transformer architecture which consists almost entirely of attention modules  [19] . BERT was pre-trained on the BooksCorpus  [22]  and English Wikipedia data sets using two pretraining tasks: 1) Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\n\nFor the MLM pre-training task 15% of tokens in the input sequence were randomly masked, and BERT was asked to predict the missing tokens. For the NSP pre-training task, two sentences were concatenated together and then processed by BERT as a single input sequence. The [CLS] token, a special token prepended to all input sequences, was then extracted from the output and used to make a classification as to whether or not the two sentences appeared next to one another in the source document. BERT achieved state-of-the-art performance on a variety of NLU tasks including the popular GLUE, MultiNLI, and SQuAD benchmarks  [4] .\n\nShortly after BERT was released, Yang et al proposed XLNet, a generalized autoregressive pre-training method for LM's  [21] . XL-Net was similar to BERT in that it was pre-trained on a large, general purpose corpus and based on the transformer architecture, bet differed from BERT on two key respects. First, XLNet was trained to make predictions over all permutations of the input sequence whereas BERT made predictions on the raw input sequence. Second, XLNet leveraged an autoregressive architecture whereas BERT employed an auto-encoder architecture. These modifications break the independence of the tokens in the input sequence assumed by BERT, theoretically allowing XLNet to learn more contextual knowledge. However, the autoregressive nature of XLNet means that generated tokens are only conditioned on tokens up to the current position in the input (to the left) whereas BERT is able to access context from both the left and right of the current position because of it's auto-encoder architecture.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Semantic Role Labeling",
      "text": "Semantic Role Labeling (SRL) is a fundamental NLP task in which a model is asked to identify the predicate-argument structure of a sentence, shedding light on \"who\" did \"what\" to \"whom\" in the input text. To this end, Shi et al proposed a BERT-based model for SRL in 2019  [16] . Their model was able to achieve state-of-theart performance in a variety of SRL benchmarks without the use of external features, such as part-of-speech tags and dependency trees. The choice to not use any external features was a significant departure from previous work on SRL and demonstrated the extent to which BERT is able to model human language.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Graph Neural Networks",
      "text": "Graph neural networks (GNN's) are a specialized group of deep learning architectures which are able to work with data represented in non-Euclidean domains, such as in a graph  [20] . While many specialized variants of the GNN have been proposed, one of the most popular GNN architectures is known as a Graph Convolutional Network (GCN)  [9] . A GCN is appealing for applications in which data is represented by a graph and embeddings for each node in the graph is desired. GCN's, and many GNN's in general, can be seen as a specialized message passing network in which the new embedding of a node is informed by it's previous embedding, it's neighbors, and it's neighbors' embeddings.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Classification",
      "text": "Emotional classification has been done over many modalities. Poria et al.  [14]  and Tripathi et al.  [18]  performed modality analyses for emotional classification to understand which modes of communication contain the most emotional signal. Their work involved creating many classifiers for different types of data such as video, audio, text, and motion capture. Each model was first trained on data from a single medium as well as different pairings of data from different modalities to compare the performance of unimodal and multimodal models. The two studies found that for unimodal models, performance from best to worst was as follows: text, audio, video, and motion capture. It is worth noting that textual data performed substantially better than the rest (with respect to models made by each researcher). As modalities were combined, model performance continued to increase, as did the computational complexity of the resulting system. A more detailed review of uni-and multimodal emotional models can be found in  [13] .\n\nBERT has been used in both text and auditory systems to classify emotion. The EmotionX 2019 challenge  [17]  demonstrates its application in a text-only domain. The challenge was to create an emotional classifier. Eleven teams partook in the competition and seven submitted reports documenting their models. Five of those seven teams used BERT to generate contextual embeddings in their emotion classification system and outperformed the two teams who did not incorporate BERT in their system.\n\nBERT has also been used to classify the emotion of audio in IEmoNet  [7] . IEmoNet was designed to be a modular system for classifying emotion, where each module can be trained (mostly) independent of the others. The system accepts an audio signal as input and extracts auditory emotional features from it. Concurrently, the audio is transcribed and fed into textual information system. The output from the text system is combined with the audio features extracted earlier and given to a classifier that determines the emotion of the original audio clip. BERT was incorporated into IEmoNet as it had the best performance when classifying based on text only. Furthermore, using BERT as a purely text-based emotion classifier achieved an accuracy only three percentage points lower than the complete IEmoNet model.\n\nOur research differs from these prior uses of pre-trained LM's towards emotion classification in two key regards. First, we analyze how changing the amount of context given to the model impacts classification accuracy. Second, we supplement the LM with a GCN which generates an additional representation of an utterance, informed by the predicate-argument structure of the utterance identified through SRL.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Data Sets",
      "text": "During this study, two data sets were used: interactive emotional dyadic motion capture (IEMOCAP)  [1]  and Friends  [2] . The data sets are described in more detail in the sections below.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iemocap",
      "text": "IEMOCAP  [1]  captures multimodal emotional data. Ten actors were recruited to perform one-on-one scripted and improvised scenarios designed to show a specific emotion. The dataset includes video and audio recordings, motion capture data of the hands and face, as well as the original scripts and transcribed audio. While scenarios were aimed to elicit specific emotions, the emotional labels in the data set were assigned by six evaluators.\n\nEvaluators categorized each utterance in the data set into one of ten emotions: neutral, happiness, sadness, anger, surprise, fear, disgust, frustration, excited, and other. Happiness, sadness, anger, disgust, fear, and surprise were used as labels because they are considered basic emotions according to  [5] . A neutral category was added because it was of interest to the creators. Lastly, frustration and excited labels were added because the creators believed they were important categories to accurately represent the data.\n\nEach utterance was seen by three different evaluators. The assessments of the displayed emotion from every judge are logged in the data. It is common practice for this data set to filter out data points for which the annotators were unable to reach a consensus. After filtering, about 74% of the data available for analysis. Figure  1  shows the distribution of labels in scripted and improvised scenes for utterances in the filtered data set.\n\nAlthough the IEMOCAP data set contains a total of nine possible emotional labels, researchers using IEMOCAP typically focus on four emotions: happiness, anger, sadness, and neutral.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Friends",
      "text": "EmotionLines  [2]  is a data set containing two smaller sets: Emo-tionPush and Friends. EmotionPush contains text conversations where each message is labeled with the emotion it projects. The Friends dataset contains scripts from the TV show Friends where each line is labeled with an emotional category. The EmotionX 2019  [17]  challenge had researchers develop emotional classifiers with EmotionLines. After the competition was over, the Friends portion of the data set was posted for others to use. The complete EmotionPush data set can only be obntained via request.\n\nThe Friends data set has eight emotional categories: non-neutral, neutral, joy, sadness, anger, disgust, fear, and surprise. The data was labeled using Amazon Mechanical Turk. Each utterance in the data set was seen by five Turkers. The final emotional label assigned to the utterance was the emotion that received a majority vote by the annotators. The non-neutral category contains all utterances that had no majority vote. Figure  2  shows the distribution of utterances and labels in the Friends data set. The figure demonstrates that the data is rather skewed, with the majority being neutral utterances.\n\nThe show Friends revolves around six main characters. Thus, unlike in IEMOCAP, the conversations involve more than two people. This makes capturing context in a conversation more difficult. However, it also tests the robustness of the model to generalize to situations involving more than two speakers.\n\nThe EmotionX competition asked researchers to create models that categorize an utterance into four categories: joy, sadness, anger, and neutral. Therefore, it mimics the outputs typically used by IEMOCAP. However, the EmotionLines paper (  [2] ), that we later compare our method to, performed an analysis on all output categories except non-neutral. As a result, we do not compare to the EmotionX results as we used more output categories.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Our Methodology",
      "text": "In this section, we describe our methodology in detail, highlighting the importance of each module. The backbone of our proposed methodology can be any pre-trained transformer-based LM, such as BERT  [4]  or XLNet  [21]  described above. Given the relatively small size of most data sets used for identifying emotion in conversation, we believe the use of a LM pre-trained on a general purpose corpus is crucial for achieving a high level of performance. We treat the task of classifying the emotion of an utterance in conversation as a one-versus-all binary classification task for each possible emotion label, and formulate it such that it closely resembles NSP, a task commonly used as a pre-training task for LM's. A visual depiction of our proposed methodology is presented in figure  3  and described in depth in the following sections.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Identifying Predicate-Argument Structure",
      "text": "The first step in our proposed methodology is the identification of the predicate-argument structure within an individual utterance. Here, we employ the BERT-based SRL model proposed by Shi et al and introduced above  [16] . We extract the predicates and verbs identified by the BERT-based model in each utterance, disregarding other identified entities. Next, a graph is constructed to represent the predicate-argument structure identified in each utterance. For each predicate-argument set in an utterance, a node is created for both the predicate and argument and an edge is added to connect the two nodes. To increase connectivity of the graph we add an edge between node a and node b if the set of tokens represented by node a are a subset of those represented by node b. We note that this process is only performed on the utterance being classified, not any of the utterances provided as context.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Constructing The Input Representation",
      "text": "To assess LM's ability to identify the emotion of an individual utterance in a conversation, as opposed to a single, isolated utterance, we explore the LM's performance when provided with a varying amount of preceding utterances as context. We frame the problem of emotion classification as a one-versus-all binary classification task for each emotion, so an auxiliary sentence must be constructed for all possible emotion labels. The utterance being classified and the preceding N context utterances are concatenated together, separated by the [SEP] token, and used as text A for the binary classification. The auxiliary sentence for each possible emotion label takes the form of \"That statement expressed [EMOTION]\" and is used as text B for the binary classification. The input sequence to be passed to the LM is formed by concatenating text A and text B as follows:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Fine-Tuning With Graph-Reasoning",
      "text": "The fine-tuning of vanilla transformer-based LM's such as BERT and XLNet is a relatively straightforward procedure. We utilize pre-trained models provided by Hugging Face 1  in our experiments. Although a multi-class classification at heart, the LM will first make E binary predictions as to whether or not an utterance should be labeled with each of the E emotion labels and takes the emotion corresponding to the highest binary prediction as the overall label.\n\nAfter the input sequence has been processed, but before each binary prediction is made, a graph is constructed in accordance with the predicate-argument structure identified above. The initial embedding of the i th node, h 0 i is obtained by averaging the embeddings of the corresponding tokens in the LM's output representation. This value is then projected to the graph embedding dimension, d GC N , via weight matrix W. This process is described below in equation 1.  where s i = {w 0 , ..., w t } are tokens represented by node i, h w j is the LM's contextual representation of the token w j , W ∈ R d LM xd GC N projects the embeddings, and σ is an activation function.\n\nHere, we use a GCN, described above, to process our predicateargument graph  [9] . As such, information propagates through the graph in two phases: aggregation and combination. During aggregation, an intermediate representation of node i's neighbors in layer l, z l i , is influenced by node i's neighbors, N i , and the embedding of those neighbors, h l j . The aggregation process is described in detail in equation 2, where V l is the adjacency matrix of the graph in layer l.\n\nThe combination phase is then executed to obtain a new embedding of node i in layer l + 1, h l +1 i . The new embedding is informed by node i's previous embedding, h l i , and the intermediate representation of i's neighbors, z l i . The combination phase is described in detail in equation 3 below where W l is a weight matrix and σ an activation function.\n\nThen, multiplicative attention is used to obtain a unified representation of nodes in the graph  [10] . The embedding of the [CLS] token is extracted from the output of the LM and the attention scores between the [CLS] token and each node in the graph are computed. The unified graph embedding, h д , is then computed as the weighted average of each node in the graph in accordance with the attention scores. This process is described in equations 4 and 5 below where h c is the embedding of the [CLS] token, W 1 is a weight matrix, h L i is the final embedding of node i, and N i is the neighborhood of node i.\n\nThe unified graph embedding, h д , is then concatenated with the embedding of the [CLS] token, h c , and passed through a dense layer to make the binary classification. The emotion corresponding to the binary prediction for each emotion is then taken as the predicted label for the utterance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "We apply our methodology on both the IEMOCAP and Friends data sets and present our results below. We explore the performance of both BERT-base-uncased and XLNet-base-cased, the most commonly used variant of each model, in our experiments. In all experiments, the learning rate was set to 5e -6 , a single GCN layer was used, and an Adam optimizer was used with β 1 = 0.9 and β 2 = 0.999.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iemocap",
      "text": "For experiments on the IEMOCAP data set, utterances for which less than two annotators agreed on a label were excluded. Furthermore, utterances labeled with an emotion other than anger, happiness, neutral, and sadness were disregarded as is common practice. Both BERT and XLNet were trained for 9 epochs. Our results obtained by providing BERT and XLNet with 0, 1, 2, 4, and 8 previous utterances as context are presented below in figure  4 .\n\nTable  1  compares our model with varying levels of context to prior models mentioned earlier.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model",
      "text": "WA UA bc-LSTM  [14]  73.6% LSTM (Text_Model3)  [18]  64.78% BERT (portion of IEmoNet)  [7]     1 : Performance comparison on IEMOCAP. WA represents weighted average accuracy. UA is unweighted average accuracy. Columns with one value reported either the same for WA and UA or did not disclose which metric was reported. The integer at the end of our presented models signifies the number of context utterances provided.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Friends",
      "text": "For experiments on the Friends data set all utterances were considered. Both BERT and XLNet were trained for 11 epochs. Our results obtained by providing BERT and XLNet with 0, 1, 2, 4, and 8 previous utterances as context are presented below in figure  5 .\n\nTables  2  and 4  compare our models with varying amounts of context over the entire dataset and per category with  [2] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model",
      "text": "WA UA CNN-BiLSTM  [2]  77.4% 39.4% BERT+SRL-GNN-1 70.02% 51.63% BERT+SRL-GNN-2 68.78% 49.89% BERT+SRL-GNN-4 68.78% 50.27% BERT+SRL-GNN-8 72.10% 53.71% XLNet+SRL-GNN-1 69.40% 47.68% XLNet+SRL-GNN-2 71.47% 48.23% XLNet+SRL-GNN-4 71.47% 51.36% XLNet+SRL-GNN-8 72.82% 53.41% Table  2 : Performance comparison on Friends dataset. WA is weighted accuracy. UA is unweighted accuracy.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Discussion",
      "text": "Upon inspecting the results of our experiments we generally see an increase in performance for both BERT and XLNet when applied to the IEMOCAP and Friends data sets when our methodology is used. However, the realized increase in performance varies by model and data set. When preceding utterances are not provided as context, models trained both with and without our methodology achieve similar levels of performance. We hypothesize that this is because  both models are able to identify the relevant predicate-argument structure in an utterance without the use of a GCN. Recent work presented by Clark et al suggests that different attention heads in BERT are able to attend to specific types of tokens, such as direct objects, noun modifiers, passive auxiliary verbs, and prepositions, among others, lending credence to our hypothesis  [3] . While a similar analysis has not been done for XLNet, due to their very similar architecture, we believe it is not unreasonable to assume the attention heads in XLNet perform a similar function.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iemocap",
      "text": "We see very strong performance gains when our methodology is applied to the IEMOCAP data set. Performance of the vanilla XLNet and BERT models peak when two and four utterances are provided as context, respectively, but degrades when eight utterances are provided as context. When our methodology is used, however, we see performance continue to increase as more utterances are provided as context. We hypothesize that the performance of the vanilla models degrades when more than four utterances are provided as context because the length of the input sequence approaches the maximum input length of both BERT and XLNet. As a result, we believe the models struggle to identify important syntactic and semantic information in each utterance as well as they can when dealing with shorter input sequences. The inclusion of our proposed GCN module appears to alleviate the issue encountered when dealing with longer sequences.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Friends",
      "text": "The results of applying our methodology to the Friends data set do not tell as definitive a story as the IEMOCAP results. We do not see a clear difference in the performance of either XLNet or BERT when our methodology is employed, nor do we see a trend in performance as the number of utterances provided as context increases. However, in general, our methodology does seem to provide an increase in performance for particular experimental settings. For example, when four utterances are provided as context, BERT sees roughly a 5% increase in unweighted average accuracy by using our methodology, but XLNet achieves the same level of performance whether our methodology is used or not. Furthermore, we observe improved unweighted accuracy metrics when our model is applied with both LM's compared to when it is not applied. This would suggest that our model is not as sensitive to the distribution of labels in the data set compared to previous methods  [17] .\n\nOne possible explanation for the inconsistent performance on the Friends data set is that the models struggled to handle conversations containing multiple speakers. All conversations in the IEMOCAP data set involve exactly two participants, while conversations in the Friends data set all contain at least two participants. Another likely cause for the inconsistent performance is the imbalanced nature of the data set. For example, utterances with the neutral label account for roughly 45% of the data set while only 2% of utterances are labeled as fear.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Future Work",
      "text": "While we showed that our methodology provides significant improvements over previous state-of-the-art on the IEMOCAP data set, we believe even higher levels of performance could be reached with a few modifications to our framework. We briefly describe these possible future directions in the sections below.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Fully Exploiting Lm'S",
      "text": "In our experiments we exclusively took the output from BERT and XLNet's final layer as the embedding for tokens in our input sequence. However, the output is actually the 13 t h representation of the input sequence produced by the LM's (recall that each LM is formed by stacking N transformer modules, N = 12 for the base models used in our experiments). Additionally, the models compute an attention matrix for the input sequence in each layer. We believe these 12 internal embeddings and attention matrices may contain additional emotional signal which could be used to further improve performance. Clark et al have recently demonstrated how particular layers in BERT, and specific attention heads in different layers, have different functionality, perhaps suggesting this is a fruitful path forward  [3] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Better Leveraging Context Utterances",
      "text": "Another interesting path forward could include identifying the predicate-argument structure in utterances in the context, as well as in the utterance being classified. Having shown that performance improves when SRL is performed on only the utterance being classified, it is not outlandish to assume that the performing the same process on context utterances could further improve performance.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "More Accurately Modeling Group Conversation",
      "text": "As mentioned above, we believe the multi-party nature (>2 participants) of the conversations in the Friends data set is partially to blame for the inconsistent performance we observed. A brief inspection of the data revealed that in some conversations, a subset of participants will go off on a tangent, straying from the rest of the group. Not only does the model need to handle multiple participants, but also multiple topics of conversation. To this end, we believe it would be interesting to explore the effect of introducing participant-specific special tokens to our model, one for each participant. We believe this may allow the LM's to better manage multiple participants, and perhaps even multiple topics of conversation.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the distribution of labels in scripted and improvised scenes",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the distribution of utterances",
      "page": 3
    },
    {
      "caption": "Figure 3: and described",
      "page": 3
    },
    {
      "caption": "Figure 1: Distribution of labels in datapoints that have a majority agreement on label in IEMOCAP. The pie chart’s (a) and (b)",
      "page": 4
    },
    {
      "caption": "Figure 2: Distribution of labels in the Friends data set.",
      "page": 4
    },
    {
      "caption": "Figure 3: Visual depiction of our proposed methodology. Before being processed by the LM, SRL is performed on the utterance",
      "page": 5
    },
    {
      "caption": "Figure 4: Table 1 compares our model with varying levels of context to",
      "page": 5
    },
    {
      "caption": "Figure 5: Tables 2 and 4 compare our models with varying amounts of",
      "page": 5
    },
    {
      "caption": "Figure 4: Weighted and un-weighted average accuracy of BERT and XLNet when applied to IEMOCAP data set.",
      "page": 6
    },
    {
      "caption": "Figure 5: Weighted and un-weighted average accuracy of BERT and XLNet when applied to Friends data set for the four emo-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "CNN-BiLSTM [2]",
          "WA": "77.4%",
          "UA": "39.4%"
        },
        {
          "Model": "BERT+SRL-GNN-1\nBERT+SRL-GNN-2\nBERT+SRL-GNN-4\nBERT+SRL-GNN-8",
          "WA": "70.02%\n68.78%\n68.78%\n72.10%",
          "UA": "51.63%\n49.89%\n50.27%\n53.71%"
        },
        {
          "Model": "XLNet+SRL-GNN-1\nXLNet+SRL-GNN-2\nXLNet+SRL-GNN-4\nXLNet+SRL-GNN-8",
          "WA": "69.40%\n71.47%\n71.47%\n72.82%",
          "UA": "47.68%\n48.23%\n51.36%\n53.41%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "CNN-BiLSTM [2]",
          "Neu": "87.0%",
          "Joy": "60.3%",
          "Sad": "28.7%",
          "Fea": "0.0%",
          "Ang": "32.4%",
          "Sur": "40.9%",
          "Dis": "26.7%"
        },
        {
          "Model": "BERT+SRL-GNN-1\nBERT+SRL-GNN-2\nBERT+SRL-GNN-4\nBERT+SRL-GNN-8",
          "Neu": "82.08%\n81.26%\n80.86%\n84.32%",
          "Joy": "73.98%\n67.48%\n70.73%\n69.92%",
          "Sad": "50.00%\n51.61%\n54.84%\n48.39%",
          "Fea": "37.93%\n27.59%\n20.69%\n31.03%",
          "Ang": "41.18%\n47.06%\n41.18%\n47.06%",
          "Sur": "67.55%\n65.56%\n66.23%\n73.51%",
          "Dis": "8.70%\n8.70%\n17.39%\n21.74%"
        },
        {
          "Model": "XLNet+SRL-GNN-1\nXLNet+SRL-GNN-2\nXLNet+SRL-GNN-4\nXLNet+SRL-GNN-8",
          "Neu": "81.67%\n85.74%\n83.30%\n85.34%",
          "Joy": "74.80%\n73.98%\n73.17%\n73.98%",
          "Sad": "46.77%\n48.39%\n59.68%\n67.74%",
          "Fea": "0.00%\n0.00%\n0.00%\n3.45%",
          "Ang": "50.59%\n48.24%\n54.12%\n60.00%",
          "Sur": "66.89%\n68.21%\n67.55%\n61.59%",
          "Dis": "13.04%\n13.04%\n21.74%\n21.74%"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "IDEA (BERT-based)[17]\nKU (BERT-based)[17]\nHSU (BERT-based)[17]\nAlexU (BERT-based)[17]",
          "Neu": "87.30%\n86.00%\n85.40%\n84.50%",
          "Joy": "75.50%\n72.00%\n73.60%\n72.30%",
          "Sad": "59.60%\n51.40%\n55.60%\n58.00%",
          "Ang": "68.90%\n65.6%\n65.00%\n59.70%"
        },
        {
          "Model": "BERT+SRL-GNN-0\nBERT+SRL-GNN-1\nBERT+SRL-GNN-2\nBERT+SRL-GNN-4\nBERT+SRL-GNN-8",
          "Neu": "93.08%\n91.45%\n91.45%\n90.84%\n88.80%",
          "Joy": "69.92%\n65.85%\n62.60%\n69.11%\n69.92%",
          "Sad": "48.39%\n53.23%\n53.23%\n46.77%\n53.23%",
          "Ang": "57.65%\n48.24%\n43.53%\n52.94%\n57.65%"
        },
        {
          "Model": "XLNet+SRL-GNN-0\nXLNet+SRL-GNN-1\nXLNet+SRL-GNN-2\nXLNet+SRL-GNN-4\nXLNet+SRL-GNN-8",
          "Neu": "87.37%\n86.35%\n86.56%\n87.78%\n86.76%",
          "Joy": "80.49%\n77.24%\n71.54%\n77.24%\n75.61%",
          "Sad": "45.16%\n59.68%\n59.68%\n62.90%\n69.35%",
          "Ang": "55.29%\n57.65%\n52.94%\n60.00%\n51.76%"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan",
        "Iemocap"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "2",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "S.-Y Chen",
        "C.-C Hsu",
        "C.-C Kuo",
        "L.-W Ku"
      ],
      "year": "2018",
      "venue": "Emotionlines: An emotion corpus of multi-party conversations",
      "arxiv": "arXiv:1802.08379"
    },
    {
      "citation_id": "3",
      "title": "What does bert look at? an analysis of bert's attention",
      "authors": [
        "K Clark",
        "U Khandelwal",
        "O Levy",
        "C Manning"
      ],
      "year": "2019",
      "venue": "What does bert look at? an analysis of bert's attention",
      "arxiv": "arXiv:1906.04341"
    },
    {
      "citation_id": "4",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "5",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "6",
      "title": "Sentence compression by deletion with lstms",
      "authors": [
        "K Filippova",
        "E Alfonseca",
        "C Colmenares",
        "Ł Kaiser",
        "O Vinyals"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Bimodal speech emotion recognition using pre-trained language models",
      "authors": [
        "V Heusser",
        "N Freymuth",
        "S Constantin",
        "A Waibel"
      ],
      "year": "2019",
      "venue": "Bimodal speech emotion recognition using pre-trained language models",
      "arxiv": "arXiv:1912.02610"
    },
    {
      "citation_id": "8",
      "title": "Exploring the limits of language modeling",
      "authors": [
        "R Jozefowicz",
        "O Vinyals",
        "M Schuster",
        "N Shazeer",
        "Y Wu"
      ],
      "year": "2016",
      "venue": "Exploring the limits of language modeling",
      "arxiv": "arXiv:1602.02410"
    },
    {
      "citation_id": "9",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2016",
      "venue": "Semi-supervised classification with graph convolutional networks",
      "arxiv": "arXiv:1609.02907"
    },
    {
      "citation_id": "10",
      "title": "Effective approaches to attentionbased neural machine translation",
      "authors": [
        "M.-T Luong",
        "H Pham",
        "C Manning"
      ],
      "year": "2015",
      "venue": "Effective approaches to attentionbased neural machine translation",
      "arxiv": "arXiv:1508.04025"
    },
    {
      "citation_id": "11",
      "title": "Abnormal recognition of facial expression of emotions in depressed patients with major depression disorder and schizotypal personality disorder",
      "authors": [
        "E Mikhailova",
        "T Vladimirova",
        "A Iznak",
        "E Tsusulkovskaya",
        "N Sushko"
      ],
      "year": "1996",
      "venue": "Biological psychiatry"
    },
    {
      "citation_id": "12",
      "title": "Recurrent neural network based language model",
      "authors": [
        "T Mikolov",
        "M Karafiát",
        "L Burget",
        "J Černockỳ",
        "S Khudanpur"
      ],
      "year": "2010",
      "venue": "Eleventh annual conference of the international speech communication association"
    },
    {
      "citation_id": "13",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "14",
      "title": "Multimodal sentiment analysis: Addressing key issues and setting up the baselines",
      "authors": [
        "S Poria",
        "N Majumder",
        "D Hazarika",
        "E Cambria",
        "A Gelbukh",
        "A Hussain"
      ],
      "year": "2018",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "15",
      "title": "Large, pruned or continuous space language models on a gpu for statistical machine translation",
      "authors": [
        "H Schwenk",
        "A Rousseau",
        "M Attik"
      ],
      "year": "2012",
      "venue": "Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT"
    },
    {
      "citation_id": "16",
      "title": "Simple bert models for relation extraction and semantic role labeling",
      "authors": [
        "P Shi",
        "J Lin"
      ],
      "year": "2019",
      "venue": "Simple bert models for relation extraction and semantic role labeling",
      "arxiv": "arXiv:1904.05255"
    },
    {
      "citation_id": "17",
      "title": "Socialnlp emotionx 2019 challenge overview: Predicting emotions in spoken dialogues and chats",
      "authors": [
        "B Shmueli",
        "L.-W Ku"
      ],
      "year": "2019",
      "venue": "Socialnlp emotionx 2019 challenge overview: Predicting emotions in spoken dialogues and chats",
      "arxiv": "arXiv:1909.07734"
    },
    {
      "citation_id": "18",
      "title": "Multi-modal emotion recognition on iemocap with neural networks",
      "authors": [
        "S Tripathi",
        "S Tripathi",
        "H Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on iemocap with neural networks",
      "arxiv": "arXiv:1804.05788"
    },
    {
      "citation_id": "19",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "20",
      "title": "A comprehensive survey on graph neural networks",
      "authors": [
        "Z Wu",
        "S Pan",
        "F Chen",
        "G Long",
        "C Zhang",
        "S Philip"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "21",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "22",
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "authors": [
        "Y Zhu",
        "R Kiros",
        "R Zemel",
        "R Salakhutdinov",
        "R Urtasun",
        "A Torralba",
        "S Fidler"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    }
  ]
}