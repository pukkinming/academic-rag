{
  "paper_id": "2308.08713v1",
  "title": "Decoding Emotions: A Comprehensive Multilingual Study Of Speech Models For Speech Emotion Recognition",
  "published": "2023-08-17T00:30:56Z",
  "authors": [
    "Anant Singh",
    "Akshat Gupta"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "HuBERT",
    "wav2vec2",
    "Edge Probing",
    "Feature Extraction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent advancements in transformer-based speech representation models have greatly transformed speech processing. However, there has been limited research conducted on evaluating these models for speech emotion recognition (SER) across multiple languages and examining their internal representations. This article addresses these gaps by presenting a comprehensive benchmark for SER with eight speech representation models and six different languages. We conducted probing experiments to gain insights into inner workings of these models for SER. We find that using features from a single optimal layer of a speech model reduces the error rate by 32% on average across seven datasets when compared to systems where features from all layers of speech models are used. We also achieve state-of-the-art results for German and Persian languages. Our probing results indicate that the middle layers of speech models capture the most important emotional information for speech emotion recognition. GitHub 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent years, several transformer-based speech representation models, trained on massive amounts of speech data, have been introduced  [1]  [2] [3]  [4] . These models have demonstrated remarkable improvements in performance across various downstream tasks. Although several benchmarks exist for evaluating their performance  [5]    [6] , insufficient attention has been given to evaluating these models across multiple languages. Previous large-scale works on speech emotion recognition either do not evaluate the performance of the latest speech representation models  [7]  or they only evaluate a subset of models or languages  [8]  [9]  [10] . Consequently, there is a gap in the literature when it comes to comprehensive evaluations of these models for speech emotion recognition across various languages.\n\nAnother critical aspect that has received limited attention is analyzing the inner workings of these models using probing techniques, which can provide valuable insights into how 1 https://github.com/95anantsingh/Decoding-Emotions these models process and encode various linguistic and acoustic features. By examining the responses of these models to specific linguistic or emotional cues, we can develop a deeper understanding of the strengths and limitations of these models. Recognizing emotion in speech requires an understanding of both phonetic and the prosodic content in the spoken utterance  [11] . While previous work has explored probing techniques to understand the phonetic content of speech representation models  [12]  [13]  [14] , we haven't seen studies doing the same for tasks that have a higher dependence on prosodic content.\n\nThis paper has a dual focus. Firstly, we present a comprehensive benchmark of multiple speech representation models for speech emotion recognition across a range of languages. This benchmark ensures that the models are just as applicable and relevant in diverse cultural and linguistic contexts. One of the problems in SER is the lack of standardized training and testing splits which allows different papers to report different performances for the task. To counter this, we adopt a standardized train-dev-test split of Scheidwasser et al. (2022)  [7]  to facilitate consistent comparisons. By providing a standardized evaluation framework, we enable effective comparisons and assessments of the performance of different speech emotion recognition models.\n\nSecondly, we conduct probing experiments to gain insights into the underlying mechanisms of speech emotion recognition across multiple languages. Unlike  [11] , we find that the most important layer for speech emotion recognition are the center layers, as can be seen in Figure  1 . This observation then inspired us to just utilize one single layer instead of the final layer or aggregate all layers  [8]  for SER. We surprisingly find that when features are extracted from the optimal layer, which are usually the center layers of the model, we achieve best performance. This is contrary to previous work where conventionally only final layer features or aggregating feature from different layers  [8]  have lead to best results. As a result, we report state-of-the-art results for German and Persian where only speech input is used for SER. Our findings challenge the prevailing notion and underscore the importance of selecting the appropriate layer to maximize the efficacy of speech representation models in SER tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background",
      "text": "Previous research on probing speech representation models have primarily focused on studying how speech models understand phonetic content  [12]  [13]  [14] . In order to probe each layer, these studies adopt a technique of obtaining the time-average of feature vectors from the target layer and then passing them through a linear layer for downstream tasks.\n\nThe idea is to use the least complex model for classification so that the descriptive power of the model can be studied. Notably, these investigations reveal that wav2vec2 exhibits an auto-encoder-like behavior, with the initial and final layers resembling the input, while the intermediate layers generate higher-level representations that encapsulate maximum contextual information. In a study conducted by  Lin et al. (2023)  regarding the utilization of speech representation models for prosodic tasks, it was discovered that only the initial layers encoded prosodic information. We find that speech emotion recognition is a task done best by the center layers of the model, producing state-of-the-art results with a very simple linear model when applied on the right layer.\n\nA large number of research articles exists in literature for making systems for speech emotion recognition. In this paper, we focus on doing speech emotion recognition with transformer-based speech representation models. We specifically focus on using three pre-trained speech models -wav2vec2  [1] , XLSR  [2]  and HUBERT  [3] . We do not finetune the weights of the speech representation models  [15]  [9][16]  [17]  and only use them as feature extractors as we believe such systems are more practical in the real world setting -where a single feature extractor is used to extract speech features and multiple systems perform different tasks using the same set of features. Unlike pre-trained language models in natural language processing where the final layer features are used to do most tasks, Pepino et al  [8]  showed that the best way to use speech models for downstream task is not to use the final layer representations. They took a weighted averaged the features of across all layers of wav2vec2 and then used an LSTM model to achieve state-of-the-art results for the IEMOCAP (English) dataset. In our paper, we show that better results can be achieved using information from the optimal layer.\n\nIn this paper, we also study speech emotion recognition across multiple languages. Most studies on speech emotion are centred around English  [8]  [18]  [19]  or use one or two additional languages  [20]  [10]  [21] . Our work also presents a benchmark for speech emotion recognition across multiple languages using pre-trained speech representation models. Previous multlingual SER works either don't evaluate these representation models  [7]  or only use one such model for multiple languages  [9] . While such benchmarks exist in lowresourced languages  [22]  [23]  [24]  for other tasks  [24]  [6], we do not yet have a comprehensive benchmark for speech emotion recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "This research explores seven speech emotion classification tasks conducted on six distinct language datasets. The study focuses on English, French, German, Greek, Italian, and Persian. Specifically, the English datasets used in this study consist of two widely recognized collections, namely IEMOCAP (IEM4)  [25]  and RAVDESS  [26] . Additionally, the other languages are represented by CaFE (French)  [27] , EmoDB (German)  [28] , AESDD (Greek)  [29] , EMOVO (Italian)  [30] , and ShEMO (Persian)  [31]  dataset.\n\nThe datasets exhibit variations in several aspects, including size (number of utterances), number of speakers, class distribution, and number of classes as described in Table  1 . While emotions such as anger, happiness, and sadness are present across all datasets, additional emotions such as disgust, fear, neutral emotion, surprise, calm, and boredom appear in at least one of the datasets. Each dataset consists of speech samples characterized by three crucial attributes: audio data represented as raw single channel waveforms, speaker identification, and emotion labels encompassing various emotions such as anger, happiness, and sadness. It is worth noting that all the datasets exhibit similar average utterance durations, which range from 2.5 to 4.5 seconds.\n\nThe selection of benchmark datasets for this study was primarily based on two key factors: dataset popularity and language diversity. The chosen benchmark datasets, includ-  ing EmoDB  [28] , IEMOCAP  [25] , and RAVDESS  [26] , are widely used in the field of speech emotion recognition. To address class imbalance, a subset of the IEMOCAP  [25]  dataset, specifically the four emotion classes (IEM4), was used. For the remaining tasks, all samples and classes from the original datasets were retained. To represent Italic languages, CaFE  [27]  and EMOVO  [30]  were chosen, while AESDD  [29]  and ShEMO  [31]  represented the Hellenic and Indo-Iranian branches of the Indo-European family, respectively. The majority of the benchmark datasets primarily comprise scripted and acted speech, with IEM4, RAVDESS  [26] , and ShEMO  [31]  also incorporating spontaneous utterances.\n\nTo train, optimize, and evaluate language-specific speech emotion classifiers, each dataset, following Scheidwasser et al.  [7] , was divided into training, validation, and testing sets. The standard split employed for most datasets involved allocating 60% of the data for training, 20% for validation, and 20% for testing purposes. Speaker independence was care-fully maintained in each partition, ensuring that the sets of speakers in each partition were mutually exclusive. This fixed data split design facilitated the assessment of the performance of experimental setups using different amounts of data, considering the variations in dataset sizes within the benchmark.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Models",
      "text": "In this paper, we work with three pre-trained speech models -wav2vec2  [1] , XLSR  [2]  and HUBERT  [3] . We use these models as feature extractors. A summary of these models can be found in Table  2 , presenting a concise overview of their key characteristics. For classification, we use to different heads on top of the features extracted from these speech representation models, as described in section 4.2.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extractors",
      "text": "We selected feature extraction models that vary in terms of pre-training data and the number of languages involved. By incorporating models with distinct pre-training data and linguistic coverage, we aimed to enhance the comprehensiveness and robustness of our research findings.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Wav2Vec2",
      "text": "A total of 3 versions of wav2vec2 were studied. Among them, two versions are pre-trained models, namely Wav2vec2 Base and Wav2vec2 Large. Additionally, there is one version of wav2vec2 finetuned for ASR called wav2vec2 ASR Large. These models differ in several aspects, including the number of layers in the transformer encoder and the training data hours, as well as the number of languages they were pretrained on. These statistics are shown in Table  2 . The pre-training of wav2vec2 base, wav2vec2 large, and wav2vec2 ASR large was performed using the LibriSpeech dataset, which consists of English speech data  [32] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Xls-R",
      "text": "We studied two versions of the XLS-R model: Wav2vec2 XLSR 53 and Wav2vec2 XLSR 300M. These models share the same architecture, with 24 encoder layers and 300M parameters.\n\nWav2vec2 XLSR 53 was pre-trained on a diverse dataset consisting of 53 languages. On the other hand, Wav2vec2 XLSR 300M was pre-trained on an even more extensive dataset, comprising 128 languages. The inclusion of these two models allowed us to explore the impact of different language coverage on the performance and capabilities of the XLS-R model in our research.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Hubert",
      "text": "In our research experiments, we explored HuBERT by utilizing two pretrained models and one finetuned for ASR version. The pretrained versions of HuBERT include HuBERT Base, which has 12 encoder layers, and HuBERT Large, which has 24 encoder layers. HuBERT Base was pretrained on 960 hours of the LibriSpeech English dataset  [32] . On the other hand, HuBERT Large was pretrained using a significantly larger dataset, specifically 60,000 hours of the Libri-Light English dataset  [33] . We also employed a finetuned version of HuBERT for ASR, called HuBERT ASR Large. This model consists of 24 layers in its encoder and it was pretrained on 60,000 hours of the Libri-Light dataset and then further finetuned on 960 hours of the LibriSpeech dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Classification Heads",
      "text": "Classification head is a model which takes in the learned features or representations of the input speech data from the preceding feature extractor and make predictions about the target class or emotion which a given input belongs. We used two classification heads which are described below.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Linear",
      "text": "The Linear classification head consists of two linear layers. First layer takes the averaged input features over the time sequence, and the rectified linear unit (ReLU) activation function is applied to the output with hidden dimension as 128. The resulting tensor is then passed through the second layer, which produces the final logits for each label. This classification head is based on the probing models used in  [12]  [13]  [14] . The idea here is to use the simplest feed forward neural network to be able to study the speech representation power of pre-trained models.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dense",
      "text": "Dense classification model uses piece-wise linear layers at every time step of a layer or a CNN layer with kernel size 1. Two such layers are used with the hidden dimensions of 256 hidden units. These features are then averaged across time and then passed through a final classification layer. This model is used with both single layer features and multilayer features. When using features from multiple layers, we aggregate the features following  [8]  before using the dense layer.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "We conducted two fundamental experiments by integrating all the feature extractors and classification heads. The first experiment involved a straightforward aggregation model, while the second experiment focused on edge probing to gain insights into the internal encoding schema of emotions in speech representation models. As part of our research, we treated the CNN layer that precedes the transformer layer as a separate layer, which we have denoted as the \"zeroth\" layer  [8] . To enhance the statistical validity and ensure the consistency of our findings, we conducted each individual trial five times.   [14]  83.8  [13]  100  [15]  62.8  [12]  72.5  [22]  76.3  [11]  93.5  [5]  HuBERT ASR LARGE 82.6  [13]  82.5  [10]  100  [10]  67.3  [12]  71.4  [10]  75.3  [12]  93.8  [8]  Table  3 . Maximum accuracies for Probing using the Dense classifier with various feature extraction models. The corresponding layer of the encoder in the feature extractor is denoted within square brackets.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Models",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Edge Probing -Linear",
      "text": "Edge probing involves conducting targeted analyses and experiments to investigate how the model's internal representations and attention mechanisms respond when an input is sent through the model. For a given model with layers L, we attached a separate classification head to each layer and trained them independently to predict a target label. The classifier solely relies on input from a single layer. We conducted this experiment using both a Linear classification head and a Dense classification head. Consequently, the classifier heavily relies on the encoder to provide meaningful information regarding the relationship between spans and their respective roles within a sentence. This approach empowered us to evaluate the encoder's proficiency in capturing and conveying emotional features from the high density speech input data.\n\nThe results of the probing experiments for five models are shown in Figure  1 . We find that the initial and final layers perform worst for the task of speech emotion recognition. The initial layers are unable to create a rich enough representation of speech to classify emotions and classifying emotions requires both the understanding of the phonetic as well as prosodic content. The final layers of these models, as shown in  [12]  [13]  [14]  are focused on reconstructing the input and lose the rich contextual representations for emotion recognition to be able to provide enough phonetic content for speech reconstruction. This can also be seen in Figure  1  where the performance of the final ASR layers are always worse than the non-ASR models, showing that models trained to convert speech to text lose out on a lot more prosodic information than their non-ASR counterparts. The center layers contain the richest contextual features that contain enough phonetic and prosodic content to do speech emotion recognition. This observation is true across six different languages.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Aggregation",
      "text": "The aggregation experiments follow the work in  [8] , where feature representations from all the layers are used to train an SER model. To combine features from different layers, we use a weighted average for each layer as done in  [8] , where these weights are learnable parameter. We then use the dense layer to work with these weighted multi-layer features. Upon performance evaluation of this model, we found that the aggregation based models were performing worse than the linear edge-probing models. This can also be seen in Figure  2 , where Agg. Dense performance if always worse than the Probing withLinear classifier performance except for the IEMOCAP (English) dataset. This motivated us to push the performance using a single layer of the pre-trained models using the same classification head.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Edge Probing -Dense",
      "text": "To extract even more out of the edge probing models, we use the dense classification head on the features extracted from a single layer. We find that the the dense probing models outperform both linear probing and dense aggregate models as shown in Figure  2 . This means that features extracted from a single layer are enough to achieve best performance on SER over using features extracted from all layers, even though the features are combined using learnable weights.\n\nThe improvement in performance over aggregation experiments is highlighted further in Figure  3 . The plots show the error reduction percentage over the aggregation models when using the linear and dense probing models. We observed that for 5 out of 7 datasets, the linear probing model performs better than the aggregation models, and the dense probing model is better than the aggregation model for all datasets. We also noticed that the dense probing model closes the error of the aggregation model by 5-100%, with an average error reduction about 32% across 7 datasets and 6 languages.\n\nFigure  3  also shows that the maximum improvements with dense probing models happen for the smallest datasets. This means that for low resourced scenarios, if the training dataset is small, the middle layers become even more crucial for accurately classifying emotions and aggregation models as proposed in  [8]  require a larger amount of data to achieve optimal performance. The exact values of classification accuracies for the dense probing head with the corresponding best layer for each model are shown in Table  3 . Unsurprisingly, we find the XLSR models trained on large multilingual data performs best for speech emotion recognition, including doing SER for English.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "We conducted a comprehensive evaluation of transformerbased speech representation models for speech emotion recognition (SER) across multiple languages. Our findings challenge prevailing notions and provide valuable insights for optimizing SER models in multilingual and low-resourced scenarios. Our probing experiments show that the middle layers of the speech representation models capture the most important features for SER. This is contrary to previous work  [11] , which shows that the early layers of speech models capture prosodic information, highlighting the importance of both phonetic and prosodic information for recognizing emotion in speech.\n\nWe also discovered that single-layer probing models consistently outperformed aggregation models in terms of accuracy and variability. The findings of this study challenge the previous work by Pepino et al.  [8] , as they demonstrate that utilizing features from a single layer of a speech representation model outperforms the approach of aggregating features from all layers of the model. The optimal layers were usually the center layers of the model, thus further highlighting the importance of the center layers for the task of speech emotion recognition. The optimal single layer although seems to differ from task to task and dataset to dataset, and finding the optimal layer is a part of our future investigations.\n\nAdditionally, dataset size played a crucial role, with aggregation models requiring more data to perform well. Furthermore, we observed that models trained on a larger number of languages exhibited better encoding of emotions, emphasizing the importance of linguistic diversity in pre-training.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Dataset-wise accuracy for Probing using a Linear classiﬁer with various feature extractors. The presented data is an",
      "page": 3
    },
    {
      "caption": "Figure 2: Maximum accuracies for all the classiﬁers models and",
      "page": 4
    },
    {
      "caption": "Figure 3: Error reduction percentage from Aggregate Dense",
      "page": 4
    },
    {
      "caption": "Figure 1: We ﬁnd that the initial and ﬁnal layers per-",
      "page": 5
    },
    {
      "caption": "Figure 1: where the",
      "page": 5
    },
    {
      "caption": "Figure 2: This means that features extracted from",
      "page": 5
    },
    {
      "caption": "Figure 3: The plots show the",
      "page": 5
    },
    {
      "caption": "Figure 3: also shows that the maximum improvements with",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "Ma\npr",
          "Column_3": "x\ne",
          "Column_4": "im\n-tr",
          "Column_5": "",
          "Column_6": "m\nnin",
          "Column_7": "acc\ng",
          "Column_8": "",
          "Column_9": "cies\nav2",
          "Column_10": "",
          "Column_11": "ra\nc2",
          "Column_12": "ll\nb",
          "Column_13": "",
          "Column_14": "cl\n,",
          "Column_15": "a\nw",
          "Column_16": "",
          "Column_17": "fie\n2v",
          "Column_18": "rs\nec2",
          "Column_19": "",
          "Column_20": "od\narg",
          "Column_21": "els\ne,",
          "Column_22": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "3",
      "title": "Unsupervised cross-lingual representation learning for speech recognition",
      "authors": [
        "Alexis Conneau",
        "Alexei Baevski",
        "Ronan Collobert",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Unsupervised cross-lingual representation learning for speech recognition",
      "arxiv": "arXiv:2006.13979"
    },
    {
      "citation_id": "4",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrah-Man Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "5",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "6",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Andy Yist Y Lin",
        "Jiatong Liu",
        "Xuankai Shi",
        "Guan-Ting Chang",
        "Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "7",
      "title": "Ml-superb: Multilingual speech universal performance benchmark",
      "authors": [
        "Jiatong Shi",
        "Dan Berrebbi",
        "William Chen",
        "Ho-Lam Chung",
        "En-Pei Hu",
        "Ping Huang",
        "Xuankai Chang",
        "Shang-Wen Li",
        "Abdelrahman Mohamed",
        "Hung-Yi Lee"
      ],
      "year": "2023",
      "venue": "Ml-superb: Multilingual speech universal performance benchmark",
      "arxiv": "arXiv:2305.10615"
    },
    {
      "citation_id": "8",
      "title": "Serab: A multi-lingual benchmark for speech emotion recognition",
      "authors": [
        "Neil Scheidwasser-Clow",
        "Mikolaj Kegler",
        "Pierre Beckmann",
        "Milos Cernak"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "10",
      "title": "Multi-lingual multi-task speech emotion recognition using wav2vec 2.0",
      "authors": [
        "Mayank Sharma"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Multilingual speech emotion recognition with multi-gating mechanism and neural architecture search",
      "authors": [
        "Zihan Wang",
        "Qi Meng",
        "Haifeng Lan",
        "Xinrui Zhang",
        "Kehao Guo",
        "Akshat Gupta"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "12",
      "title": "On the utility of self-supervised models for prosody-related tasks",
      "authors": [
        "Guan-Ting Lin",
        "Chi-Luen Feng",
        "Wei-Ping Huang",
        "Yuan Tseng",
        "Tzu-Han Lin",
        "Chen-An Li",
        "Hung-Yi Lee",
        "Nigel Ward"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "13",
      "title": "What all do audio transformer models hear? probing acoustic representations for language delivery and its structure",
      "authors": [
        "Jui Shah",
        "Yaman Kumar Singla",
        "Changyou Chen",
        "Rajiv Ratn Shah"
      ],
      "year": "2021",
      "venue": "What all do audio transformer models hear? probing acoustic representations for language delivery and its structure",
      "arxiv": "arXiv:2101.00387"
    },
    {
      "citation_id": "14",
      "title": "Layer-wise analysis of a self-supervised speech representation model",
      "authors": [
        "Ankita Pasad",
        "Ju-Chieh Chou",
        "Karen Livescu"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "15",
      "title": "Comparative layer-wise analysis of self-supervised speech models",
      "authors": [
        "Ankita Pasad",
        "Bowen Shi",
        "Karen Livescu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Yingzhi Wang",
        "Abdelmoumene Boumadane",
        "Abdelwahab Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "17",
      "title": "Why does self-supervised learning for speech recognition benefit speaker recognition?",
      "authors": [
        "Sanyuan Chen",
        "Yu Wu",
        "Chengyi Wang",
        "Shujie Liu",
        "Zhuo Chen",
        "Peidong Wang",
        "Gang Liu",
        "Jinyu Li",
        "Jian Wu",
        "Xiangzhan Yu"
      ],
      "year": "2022",
      "venue": "Why does self-supervised learning for speech recognition benefit speaker recognition?",
      "arxiv": "arXiv:2204.12765"
    },
    {
      "citation_id": "18",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "Li-Wei Chen",
        "Alexander Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition with multi-task learning",
      "authors": [
        "Xingyu Cai",
        "Jiahong Yuan",
        "Renjie Zheng",
        "Liang Huang",
        "Kenneth Church"
      ],
      "year": "2021",
      "venue": "Speech emotion recognition with multi-task learning"
    },
    {
      "citation_id": "20",
      "title": "Exploration of a self-supervised speech model: A study on emotional corpora",
      "authors": [
        "Yuanchao Li",
        "Yumnah Mohamied",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition in persian speech using deep neural networks",
      "authors": [
        "Ali Yazdani",
        "Hossein Simchi",
        "Yasser Shekofteh"
      ],
      "year": "2021",
      "venue": "2021 11th International Conference on Computer Engineering and Knowledge (ICCKE)"
    },
    {
      "citation_id": "22",
      "title": "A vector quantized masked autoencoder for speech emotion recognition",
      "authors": [
        "Samir Sadok",
        "Simon Leglaive",
        "Renaud Séguier"
      ],
      "year": "2023",
      "venue": "A vector quantized masked autoencoder for speech emotion recognition",
      "arxiv": "arXiv:2304.11117"
    },
    {
      "citation_id": "23",
      "title": "On building spoken language understanding systems for low resourced languages",
      "authors": [
        "Akshat Gupta"
      ],
      "year": "2022",
      "venue": "On building spoken language understanding systems for low resourced languages",
      "arxiv": "arXiv:2205.12818"
    },
    {
      "citation_id": "24",
      "title": "Acoustics based intent recognition using discovered phonetic units for low resource languages",
      "authors": [
        "Akshat Gupta",
        "Xinjian Li",
        "Sai Krishna Rallabandi",
        "Alan Black"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Intent recognition and unsupervised slot identification for low-resourced spoken dialog systems",
      "authors": [
        "Akshat Gupta",
        "Olivia Deng",
        "Akruti Kushwaha",
        "Saloni Mittal",
        "William Zeng",
        "Sai Krishna Rallabandi",
        "Alan Black"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "26",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Emily Ebrahim (abe) Kazemzadeh",
        "Samuel Provost",
        "Jeannette Kim",
        "Sungbok Chang",
        "Shrikanth Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "27",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "28",
      "title": "A canadian french emotional speech dataset",
      "authors": [
        "Philippe Gournay",
        "Olivier Lahaie",
        "Roch Lefebvre"
      ],
      "year": "2018",
      "venue": "A canadian french emotional speech dataset"
    },
    {
      "citation_id": "29",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "M Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "30",
      "title": "Speech emotion recognition for performance interaction",
      "authors": [
        "Nikolaos Vrysas",
        "Rigas Kotsakis",
        "A Liatsou",
        "Charalampos Dimoulas",
        "George Kalliris"
      ],
      "year": "2018",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "31",
      "title": "EMOVO corpus: an Italian emotional speech database",
      "authors": [
        "Giovanni Costantini",
        "Iacopo Iaderola",
        "Andrea Paoloni",
        "Massimiliano Todisco"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)"
    },
    {
      "citation_id": "32",
      "title": "Shemo -a large-scale validated database for persian speech emotion detection",
      "authors": [
        "Mohamad Omid",
        "Paria Jamshid Nezami",
        "Mansoureh Lou",
        "Karami"
      ],
      "year": "2019",
      "venue": "Shemo -a large-scale validated database for persian speech emotion detection"
    },
    {
      "citation_id": "33",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Libri-light: A benchmark for asr with limited or no supervision",
      "authors": [
        "J Kahn",
        "M Rivière",
        "W Zheng",
        "E Kharitonov",
        "Q Xu",
        "P Mazaré",
        "J Karadayi",
        "V Liptchinsky",
        "R Collobert",
        "C Fuegen",
        "T Likhomanenko",
        "G Synnaeve",
        "A Joulin",
        "A Mohamed",
        "E Dupoux"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}