{
  "paper_id": "2403.18635v1",
  "title": "Fusion Approaches For Emotion Recognition From Speech Using Acoustic And Text-Based Features",
  "published": "2024-03-27T14:40:25Z",
  "authors": [
    "Leonardo Pepino",
    "Pablo Riera",
    "Luciana Ferrer",
    "Agustin Gravano"
  ],
  "keywords": [
    "speech emotion recognition",
    "fusion",
    "deep learning",
    "BERT"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we study different approaches for classifying emotions from speech using acoustic and text-based features. We propose to obtain contextualized word embeddings with BERT to represent the information contained in speech transcriptions and show that this results in better performance than using Glove embeddings. We also propose and compare different strategies to combine the audio and text modalities, evaluating them on IEMOCAP and MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is beneficial on both datasets, though only subtle differences are observed across the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect that the criteria used to define the cross-validation folds have on results. In particular, the standard way of creating folds for this dataset results in a highly optimistic estimation of performance for the text-based system, suggesting that some previous works may overestimate the advantage of incorporating transcriptions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is an active research area with important applications in the field of human-computer interaction. SER is a complex task even for humans  [1] . In fact, in spite of recent advances enabled by deep learning models and the release of larger emotion datasets, the performance of SER systems is still relatively poor, with average recall rates usually well below 70% on the most realistic datasets, indicating that it remains an open problem.\n\nMost SER systems use low-level descriptors (LLD) extracted from the audio signal such as MFCCs, pitch and voice quality features  [2] , or features learned automatically from spectrograms using deep neural networks  [3, 4] . The excellent performance of current automatic speech recognition systems (ASR) also allows us to extract reliable text transcriptions from the speech without the need for human annotators. A few works have incorporated this information into SER systems. In some of these studies, emotional word-based vectors were computed from word occurrences in each emotion class  [5] , or using external lexicons  [6] . Similarly, emotional vectors can be extracted from phonemes  [7] . In some works  [5] , the word-based vectors are used as input to SVM classifiers together with high-level statistics of the acoustic LLDs. Another approach is to train textand audio-based classifiers separately and combine their outputs to make a final prediction  [8] . Recently, deep neural networks have been used to learn audio-linguistic embeddings  [9]  and to train emotion classifiers in an end-to-end framework combining text and audio modalities  [10, 11, 12, 13] .\n\nIn this paper, we study different ways of fusing audio and linguistic information, using early and late fusion techniques and comparing different training approaches, including  (1)  initializing two individual branches with models trained separately for audio and text and further fine-tuning the last few layers, (2) fixing the text and audio branches and training only the fusion parameters, and (3) training the whole combined neural network from scratch. For the audio branch, we use a standard approach based on MFCC, pitch, loudness, jitter, shimmer and logHNR features. For the text branch, we use contextualized word embeddings  [14]  instead of the standard word embeddings like Glove  [15]  used in most of the previous works  [12, 10] . Standard word embeddings like those obtained with Glove are extracted independently of the context in which the words are found. For example, the word \"sad\" would be assigned the same embedding whether the phrase was \"I am very sad\" or \"I am not sad at all\". On the other hand, contextualized word embeddings like those extracted by BERT take into account the whole phrase in which the word is found. As a consequence, the embedding corresponding to the word \"sad\" in those two phrases would most likely be different. We hypothesized that this characteristic should positively impact SER performance. To our knowledge,  [16]  is the only work in which word embeddings extracted with BERT have been used for SER. In that paper, authors propose a shared representation of audio, text and video modalities through deep canonical correlation analysis. A comparison with other types of embeddings is not shown in that work.\n\nThe proposed models are tested on the well-studied IEMOCAP dataset  [17] , as well as on the more challenging MSP-PODCAST dataset  [18] . Our first contribution is to show that linguistic information gives significant improvements in performance when combined with acoustic information on the MSP-PODCAST dataset. As far as we know, this is the first time that linguistic information has been used on this dataset. Second, we show that the use of contextualized word embeddings obtained with BERT results in significant improvements with respect to using standard word embeddings obtained with Glove. Third, we propose a novel way to fuse audio and text information by pretraining the neural network in audio and text modalities and then fine-tuning the fused model. Finally, we show that creating folds by speaker is not sufficient to obtain fair performance predictions on IEMOCAP, since the data contains scripted dialogues which greatly affect the performance of text-based systems when the same script is observed in training and testing. This being such a widely used dataset, we believe this observation is of great importance to the research community.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Models",
      "text": "This section describes the models used in the experiments. First, the individual models for each modality are introduced. Then, models that combine the text and audio information are described. All models are trained to optimize cross-entropy loss for four emotion classes: happy, sad, angry and neutral.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Text-Based Model",
      "text": "Recently, a language model called BERT  [14] , trained with large amounts of data has been released to the community. This model can be fine-tuned or used as a feature extractor for downstream tasks, achieving state-of-the-art results on many of them. BERT is based on the Transformer  [19]  -a network capable of modeling long contextual information, generating word embeddings that are conditioned on the phrase in which the word is found. In this study, a sequence of word embeddings is extracted from speech transcriptions using the pretrained BERT base uncased model, which consists of 12 layers, 12 attention heads and 110M parameters. The word embeddings are formed by adding the activations of the last 4 layers of the pretrained BERT model without fine-tuning. The resulting features are used as input to the text model shown on the left of Figure  1 .\n\nThe first layer (LT 1) in our text-based model operates on each embedding in the sequence reducing its dimensionality from 768 to 128. Then, 2 convolutional layers (LT 2 and LT 3) model relationships across neighboring elements of the sequence. Finally, an average over time is taken, resulting in an embedding that summarizes all the information in the sample. A final dense layer with softmax activation predicts emotion probabilities P (C k ). We applied batch normalization in all layers.\n\nTo make a comparison with non-contextualized word embeddings, we trained the same model using 300-dimensional Glove embeddings  [15] .  1",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio-Based Model",
      "text": "Each speaker utterance was divided into 32ms segments, using a hop length of 10ms. The following acoustic features were extracted from each window using openSMILE  [20] : pitch, jitter, shimmer, logHNR, loudness, and the first 13 MFCCs. These features were normalized to have a mean of 0 and standard deviation of 1, using the global statistics. Finally, first-order differences were added for all features to form a sequence of 36-dimensional feature vectors that are the input to the neural network shown on the right of Figure  1 .\n\nThe audio model consists of two convolutional layers LA1 and LA2 that model the temporal evolution of the input sequence followed by mean-pooling over time. A final dense layer with softmax activation returns the emotion probabilities P (C k ).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Fusion Models",
      "text": "In this section, we describe the strategies we implemented to combine audio and text information. In all cases, the fusion model consists of two parallel branches processing audio and text separately up to a layer where the information from the two branches is merged. The models differ on the location of the merging layer, on the network appended after merging, and on the training approach.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Early Fusion",
      "text": "In the early fusion (EF) approach, the fixed-size embeddings obtained after mean pooling in the audio and text models (Figure  1 , layers LT 4 and LA3) are concatenated resulting in a multi-modal embedding of 232 dimensions. This embedding is input to a feedforward neural network with a hidden dense layer of 128 units with ReLU activation and an output layer with 4 units and softmax activation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Late Fusion",
      "text": "In the late fusion model (LF), the logits (pre-softmax) of the audio and text models are concatenated (Figure  1 , layers LT 5 and LA4) resulting in an 8-dimensional vector that is used as input to a dense layer of 4 units with softmax activation. This dense layer learns to combine the logits of audio and text modalities to generate the final output probabilities P (C k ). We have also explored learning a scalar weight for each system instead of a full dense layer but the resulting performance was slightly worse.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training Strategies",
      "text": "We trained our fusion models in 3 different ways:\n\n• Cold-start (CS): Use Xavier uniform initialization  [21]  for all layers of the fusion model and train the model jointly from scratch. • Pre-trained (PT): Train the audio and text models separately and use the trained weights to initialize the corresponding layers of the audio and text branches in the fusion model. The layers after merging are initialized with Xavier uniform initialization. Only these layers are trained, keeping the layers up to the merging point frozen. • Warm-start (WS): Initialize all layers as in the PT approach but instead of training only the layers after merging, train also the layers right before pooling for each branch (LT 3 and LA2), keeping the first layers (LT 1, LT 2 and LA1) frozen, as in the PT approach. This procedure, in contrast to PT, allows the layers immediately before the pooling to change their weights.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setup And Datasets",
      "text": "Our experiments were performed on the IEMOCAP and MSP-PODCAST datasets. The Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset  [17]  has a length of approximately 12 hours and consists of scripted and improvised dialogues by 10 speakers. It is composed of 5 sessions, each including speech from an actor and an actress. Annotators were asked to label each sample choosing one or more labels from a pool of emotions. In this work, we used 4 emotional classes: anger, happiness, sadness and neutral, and following  [22] , we relabeled excitement samples as happiness.\n\nInstances from other classes and with no annotator agreement were discarded. 2 For this dataset, human transcriptions are used for the text-based system.\n\nTo test our models we used 5-fold cross-validation, organizing the folds so that training and test sets do not share actors or scripts. This last point is very important for the text-based model, as has been noted in  [7] , because dialogues from the same script are very similar. We show the effect of the criteria used for making the folds on both text and audio models in Section 4.1.\n\nThe MSP-PODCAST dataset v1.4  [18]  contains speech segments from podcast recordings, annotated using crowdsourcing. After discarding the instances not belonging to any of the 4 emotional classes under study, the training set contains 12078 speech segments from 601 speakers, while the test set contains 5557 utterances from 50 speakers not present in the training set. The training and test set definitions used in this paper are the ones provided with the dataset. The test set is gender balanced. Speech transcriptions were extracted using the Google Cloud Speech-to-Text API. 3  To counteract the effect of class imbalance present in both datasets, a cost-sensitive training strategy was applied by multiplying the loss of each instance with the inverse of the frequency of the class it belongs to. The models were optimized using Adam  [24]  with a learning rate of 0.0007, except for the fine-tuning case in which the learning rate was decreased to 0.0001, and the case of late fusion using pre-trained branches where learning rate was increased to 0.01. During the first 40 steps, the learning rate was linearly increased from 0 to the final value, except for the late fusion system with pretraining (LF-PT). We applied dropout with 0.5 probability at the input of layer LA2 only for the audio branch. As the input sequences have variable length, we padded them with zeros up to a maximum sequence length and then masked the padded values.\n\nWe report two different metrics: average recall (AvRec), and the average area under the ROC (AvAUC). Average recall is used instead of accuracy since both datasets have significant imbalance across classes. Both averages are computed over the four target emotions, considering a one-vs-all problem in order to compute individual recall and AUC values.\n\nWe observed that using early stopping in IEMOCAP led to inconsistent results as the data are scarce to generate a validation fold large enough. For this reason, the number of epochs for training each model was selected by optimizing the median AvAUC value on IEMOCAP over 5 seeds. The architectures and hyperparameters were also selected based on IEMOCAP results (sometimes using a single seed). The final results on IEMOCAP were obtained over 10 seeds, including the 5 used for the optimization of the epoch and the hyperparameters tuning. This leads to possibly optimistic results on this dataset. On the other hand, the results on MSP-PODCAST 2 Note that discarding no-agreement samples and samples from non-target emotions is not an ideal practice  [23] . Here, we decided to do this since it is standard practice in SER literature, facilitating comparisons across papers. were obtained using the same number of epochs and hyperparameters chosen for IEMOCAP, also averaging over 10 seeds. All of our models were trained using Keras  [25] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "In this section we report results for individual and fused systems. We start by showing the effect that the criteria used to define the folds for cross-validation on IEMOCAP has on the two individual systems.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Effect Of Partition Criteria For Iemocap Folds",
      "text": "Figure  2  shows the AvAUC for three different criteria used to define the folds on IEMOCAP. Results are computed on the merged test scores for all folds. In all cases, 5 folds are used. We compare:\n\n(1) random folds (RAND), where no information about speakers or scripts is used to define the folds; (2) folds by speaker (SP) where each fold contains the two speakers from one of the sessions; and\n\n(3) fold by speaker and script (SP&SC) where the folds are defined as in the previous case, but only script 3 is used for testing while all other scripts are used in training. Note that this last option includes less data for each fold. Finally, we compare two different sizes of models: one using half of the nodes in the models from Figure  1  (small), and one using twice the number of nodes (large). We note that most papers use by-speaker folds  [11, 12, 26] , while some use random folds  [10] . We are not aware of any work that splits by script, though some works discard the scripts altogether using only improvisation instances for testing  [7, 13] . Figure  2  clearly shows that both random and by-speaker folds result in optimistic performance for the text-based systems. For the audio-based system, both by-speaker and by-speaker-and-script options lead to similar performance (indicating that the effect of byspeaker-and-script folds having less data is limited), while the random splits result in an optimistic estimation of performance. Furthermore, the conclusion of which model size is optimal for BERT features changes depending on the fold criteria, as a large model is more likely to overfit, but this effect can only be observed when using folds that do not repeat speakers or scripts between training and test sets. Given these results, we believe it is essential to define the folds for IEMOCAP carefully, not allowing speakers and scripts seen in training to be repeated in testing. In the remaining experiments, we use folds by speaker and script.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Audio-And Text-Based Models",
      "text": "Figure  3  shows the performance obtained with the different systems on both datasets. Average AUC is reported using box and whiskers  where the text-based models significantly outperform audio models  [10, 11] . As we showed in Section 4.1 this is explained by the way we have defined the folds, preventing the text model from being trained in dialogues very similar or identical to the ones present in the test set and avoiding unrealistically good performance estimates for these systems.\n\nThe effect of using BERT versus Glove to represent word information can be seen in Figure  3  and Table  1 . BERT embeddings outperform Glove ones in both datasets with relative UAR improvements of 15.5% and 2.4% in IEMOCAP and MSP-PODCAST datasets, respectively. We attribute this performance gain to the contextual information imbued in the pretrained BERT model. While our text model could potentially learn contextual information from standard word embeddings like Glove, learning to represent negations or modification values would require a significant amount of data. We hypothesize that this is the reason why Glove performance is closer to BERT in MSP-PODCAST than in IEMOCAP, since the size and variability of dialogues in MSP-PODCAST may be allowing the text model to learn contextual information even from standard word embeddings.\n\nFinally, we note the large effect that the seed has on our systems. In many cases, the ranking of systems changes significantly depending on the seed (results not shown due to lack of space), which thus highlights the critical importance of using several seeds in order to reach more solid conclusions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fusion Models",
      "text": "As it has been noted in previous works, adding text information to audio-based SER systems gives significant performance improvements  [10, 12] . This is also observed in our fusion experiments where for both MSP-PODCAST and IEMOCAP datasets, the AvRec improves 16% relative to the best performing single model. All fusion approaches perform similarly, in agreement with previous results in the literature  [27, 11] . Only the late fusion approach with pre-training is shown here, due to space considerations. The other two training approaches gave similar results.\n\nA small advantage of the warm-start approach can be observed for both datasets with the early fusion architecture, indicating that this direction may be worth further exploration. In the future, we plan to explore approaches where the fusion is made before or at the pooling layer. We believe this has the potential to give additional benefits since the interaction between both modalities is most likely happening at short time intervals rather than at phrase level.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "We presented different approaches for emotion recognition from speech using audio features and transcriptions. We showed results on two publicly available datasets: IEMOCAP and MSP-PODCAST. We demonstrated the positive effect of representing linguistic information using contextualized word embeddings extracted with BERT compared to using standard word embeddings like those extracted with Glove. We also showed, in agreement with previous works, that the fusion of audio-and text-based information leads to significant improvements of approximately 16% on both datasets relative to using the best single modality. To our knowledge, these are the first published results using linguistic information on MSP-PODCAST, a very large, naturalistic and challenging emotion dataset.\n\nSeveral fusion strategies were tested, including early and late fusion using different training procedures. Results were not significantly different for the different methods, which again agrees with previous observations in the literature.\n\nAs an additional contribution, we highlighted the importance and impact of how folds are defined for the IEMOCAP dataset, showing how the standard procedure of splitting by session leads to highly optimistic results on our text-based system. We hope that our proposed criteria, which avoids repeating scripted dialogues between training and test sets, or the alternative of discarding scripted dialogues, will be adopted in future works on the IEMOCAP dataset, specially for text-based systems.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The first layer (LT 1) in our text-based model operates on each",
      "page": 2
    },
    {
      "caption": "Figure 1: The audio model consists of two convolutional layers LA1 and",
      "page": 2
    },
    {
      "caption": "Figure 1: Text-based and audio-based architectures. Ttext and Taudio",
      "page": 2
    },
    {
      "caption": "Figure 1: , layers LT 5 and LA4)",
      "page": 2
    },
    {
      "caption": "Figure 2: Effect of different criteria for defining the folds in IEMOCAP",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the AvAUC for three different criteria used to define",
      "page": 3
    },
    {
      "caption": "Figure 1: (small), and one using twice the number of nodes (large). We note",
      "page": 3
    },
    {
      "caption": "Figure 2: clearly shows that both random and by-speaker folds",
      "page": 3
    },
    {
      "caption": "Figure 3: shows the performance obtained with the different systems",
      "page": 3
    },
    {
      "caption": "Figure 3: Results for IEMOCAP and MSP-PODCAST dataset. Average AUC distributions for 10 different initialization seeds for different",
      "page": 4
    },
    {
      "caption": "Figure 3: and Table 1. BERT embeddings",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Median of evaluation metrics ± interquartile range ob- 4.3. Fusionmodels",
      "data": [
        {
          "IEMOCAP\nAvRec (%)\nAvAUC": "56,0±1,9\n.782±.006",
          "MSP-PODCAST\nAvRec (%)\nAvAUC": "45,7±1,4\n.726±.010"
        },
        {
          "IEMOCAP\nAvRec (%)\nAvAUC": "47,8±0.1\n.736±.007\n55,2±1.0\n.792±.003",
          "MSP-PODCAST\nAvRec (%)\nAvAUC": "49,8±0.4\n.736±.003\n51,0±0.9\n.749±.007"
        },
        {
          "IEMOCAP\nAvRec (%)\nAvAUC": "65,1±0.5\n.857±.002\n.863±.002\n64.7±1.6\n64.9±1.0\n.859±.004",
          "MSP-PODCAST\nAvRec (%)\nAvAUC": "58.2±2.4\n.817±.009\n59.1±1.8\n.823±.003\n56.5±0.3\n.817±.002"
        },
        {
          "IEMOCAP\nAvRec (%)\nAvAUC": "63.9±0.5\n.857±.006",
          "MSP-PODCAST\nAvRec (%)\nAvAUC": "58.0±0.7\n.819±.004"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Challenges in real-life emotion annotation and machine learning based detection",
      "authors": [
        "Laurence Devillers",
        "Laurence Vidrascu",
        "Lori Lamel"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "3",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "Christos-Nikolaos Anagnostopoulos",
        "Theodoros Iliou",
        "Ioannis Giannoukos"
      ],
      "year": "2015",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "4",
      "title": "An experimental study of speech emotion recognition based on deep convolutional neural networks",
      "authors": [
        "W Zheng",
        "J Yu",
        "Y Zou"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "5",
      "title": "Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms",
      "authors": [
        "Aharon Satt",
        "Shai Rozenberg",
        "Ron Hoory"
      ],
      "year": "2017",
      "venue": "Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Q Jin",
        "C Li",
        "S Chen",
        "H Wu"
      ],
      "year": "2015",
      "venue": "ICASSP"
    },
    {
      "citation_id": "7",
      "title": "Multi-modal emotion recognition from speech and text",
      "authors": [
        "Ze-Jing Chuang",
        "Chung-Hsien Wu"
      ],
      "year": "2004",
      "venue": "International Journal of Computational Linguistics & Chinese Language Processing: Special Issue on New Trends of Speech and Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Salience based lexical features for emotion recognition",
      "authors": [
        "Vidhyasaharan Kalani Wataraka Gamage",
        "Eliathamby Sethu",
        "Ambikairajah"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2004",
      "venue": "ICASSP"
    },
    {
      "citation_id": "10",
      "title": "Audio-linguistic embeddings for spoken sentences",
      "authors": [
        "Albert Haque",
        "Michelle Guo",
        "Prateek Verma",
        "Li Fei-Fei"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "11",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "12",
      "title": "Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts",
      "authors": [
        "Jilt Sebastian",
        "Piero Pierucci"
      ],
      "year": "2019",
      "venue": "Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts"
    },
    {
      "citation_id": "13",
      "title": "Multi-Modal Learning for Speech Emotion Recognition: An Analysis and Comparison of ASR Outputs with Ground Truth Transcription",
      "authors": [
        "Saurabh Sahu",
        "Vikramjit Mitra",
        "Nadee Seneviratne",
        "Carol Espy-Wilson"
      ],
      "year": "2019",
      "venue": "Multi-Modal Learning for Speech Emotion Recognition: An Analysis and Comparison of ASR Outputs with Ground Truth Transcription"
    },
    {
      "citation_id": "14",
      "title": "Exploiting Acoustic and Lexical Properties of Phonemes to Recognize Valence from Speech",
      "authors": [
        "Biqiao Zhang",
        "Soheil Khorram",
        "Emily Provost"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT"
    },
    {
      "citation_id": "16",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "EMNLP"
    },
    {
      "citation_id": "17",
      "title": "Multi-Modal Sentiment Analysis Using Deep Canonical Correlation Analysis",
      "authors": [
        "Zhongkai Sun",
        "K Prathusha",
        "William Sarma",
        "Erik Sethares",
        "Bucy"
      ],
      "year": "2019",
      "venue": "Multi-Modal Sentiment Analysis Using Deep Canonical Correlation Analysis"
    },
    {
      "citation_id": "18",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "19",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Opensmile: The munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "22",
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "authors": [
        "Xavier Glorot",
        "Yoshua Bengio"
      ],
      "year": "2010",
      "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS'10)"
    },
    {
      "citation_id": "23",
      "title": "Evaluating deep learning architectures for Speech Emotion Recognition",
      "authors": [
        "M Haytham",
        "Margaret Fayek",
        "Lawrence Lech",
        "Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "24",
      "title": "No sample left behind: Towards a comprehensive evaluation of speech emotion recognition system",
      "authors": [
        "Pablo Riera",
        "Luciana Ferrer",
        "Agustín Gravano",
        "Lara Gauder"
      ],
      "year": "2019",
      "venue": "Proc. Workshop on Speech, Music and Mind"
    },
    {
      "citation_id": "25",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference for Learning Representations"
    },
    {
      "citation_id": "26",
      "title": "Keras",
      "authors": [
        "Chollet Franc"
      ],
      "year": "2015",
      "venue": "Keras"
    },
    {
      "citation_id": "27",
      "title": "Learning alignment for multimodal emotion recognition from speech",
      "authors": [
        "Haiyang Xu",
        "Hui Zhang",
        "Kun Han",
        "Yun Wang",
        "Yiping Peng",
        "Xiangang Li"
      ],
      "year": "2019",
      "venue": "Learning alignment for multimodal emotion recognition from speech"
    },
    {
      "citation_id": "28",
      "title": "Deep Hierarchical Fusion with Application in Sentiment Analysis",
      "authors": [
        "Efthymios Georgiou",
        "Charilaos Papaioannou",
        "Alexandros Potamianos"
      ],
      "year": "2019",
      "venue": "Deep Hierarchical Fusion with Application in Sentiment Analysis"
    }
  ]
}