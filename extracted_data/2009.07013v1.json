{
  "paper_id": "2009.07013v1",
  "title": "Group-Level Emotion Recognition Using A Unimodal Privacy-Safe Non-Individual Approach Author Version",
  "published": "2020-09-15T12:25:33Z",
  "authors": [
    "Anastasia Petrova",
    "Dominique Vaufreydaz",
    "Philippe Dessus"
  ],
  "keywords": [
    "EmotiW 2020",
    "audio-video group emotion recognition",
    "Deep Learning",
    "affective computing",
    "privacy"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This article presents our unimodal privacy-safe and non-individual proposal for the audio-video group emotion recognition subtask at the Emotion Recognition in the Wild (EmotiW) Challenge 2020 1 . This sub challenge aims to classify in the wild videos into three categories: Positive, Neutral and Negative. Recent deep learning models have shown tremendous advances in analyzing interactions between people, predicting human behavior and affective evaluation. Nonetheless, their performance comes from individual-based analysis, which means summing up and averaging scores from individual detections, which inevitably leads to some privacy issues. In this research, we investigated a frugal approach towards a model able to capture the global moods from the whole image without using face or pose detection, or any individual-based feature as input. The proposed methodology mixes stateof-the-art and dedicated synthetic corpora as training sources. With an in-depth exploration of neural network architectures for group-level emotion recognition, we built a VGG-based model achieving 59.13% accuracy on the VGAF test set (eleventh place of the challenge). Given that the analysis is unimodal based only on global features and that the performance is evaluated on a real-world dataset, these results are promising and let us envision extending this model to multimodality for classroom ambiance evaluation, our final target application.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Since the emergence of the \"affective computing\" research trend at the beginning of the 2000s  [1] , more and more features describing people's affective states are calculable using machine learning on remote sensor signals. Deep learning has attracted significant attention of the research community thanks to its faculty to automatically extract features from images and videos, determining the best representations from raw data. Deep learning shows high performance in multiple domains, notably in computer vision and speech recognition. The literature presents a wide variety of Convolutional Neural Networks (CNNs) algorithms with extensive applications in many areas, such as human-computer interaction, image retrieval, surveillance, virtual reality, etc. These achievements impact other research areas, such as affective computing and behavioral modeling to detect and predict human actions, being one of the main multimodal perception instruments for analyzing interactions between people and groups  [2] . However, most of them cannot reach the human-level understanding of in the wild emotions  [3] .\n\nIn the Teaching Lab project, we aim at developing a smart pervasive classroom to analyze teacher-students social relationships from still images or video sequences coupled with acoustic features: namely current teacher activity, current teaching episodes, whole-class engagement, students' attention or engagement, classroom ambiance, etc. The system's goal is to analyze the underlying teaching processes (e.g., teacher-students interaction, misbehavior management), not to monitor individual behaviors per se, even if they are inadequate. The multimodal perception system will thus monitor the whole classroom at a glance to help teachers enhance their pedagogical practices afterward. The underlying research question is: is it possible to use deep learning to capture global moods only from the whole scene?\n\nWhile perceiving groups, current state-of-the-art perception systems focus on people as individuals, i.e., each individual is detected and processed as one entity. Group level perception is then computed by iterating over the group. For instance, to detect which emotion is carried by a photograph, systems typically try to locate faces accurately and then identify emotions from facial expressions with techniques such as facial action coding. Averaging over results leads to the final estimation  [4] . However, this approach can raise problems for privacy and ethical concerns. An ambient classroom has \"ears and eyes\" and any attendee's behavior is subject to be recorded and further analyzed  [5, 6]  for monitoring or surveillance purposes, thus hampering their so-called \"ambient privacy\". Since the overall goal of Teaching Lab systems is to obtain global information about pedagogical episodes, a question arises about the possibility to calculate it without individual sub-calculations. This kind of approach exists using audio processing  [7] , but, as far as we know, no research on pure global group-level emotions features on images/videos, again without including individual computations, is reported in the literature. This paper addresses our first investigation about perceiving the mood of groups of people with an ethical and privacy-safe frugal approach, i.e. an ethical approach excluding individual-based features. The EmotiW 2020 challenge  [8]  and its Audio-video Group Emotion Recognition sub-challenge are an opportunity for the challenge team (OnlyGlobalFeatures) to confront this problem using data gathered \"in the wild\" and sustain its ethical proposal using deep neural network using only global features. The current proposed unimodal model achieved 59.13% of accuracy on the VGAF test set  [9]  and the eleventh place in the challenge. Using only the video stream, this promising preliminary result is an opening on future evolutions of such perception systems and will be further extended to audio signal processing. This article is organized as follows. The related work section provides insights into current researches on group-level perception. Then, the proposed approach and the system pipeline are depicted. The next sections describe information about the training datasets and conducted experiments. Results are discussed and conclusions are drawn in the last sections.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "A significant contribution to automatic computer recognition of human emotions was made by the previous EmotiW challenges participants. Audio-video Group Emotion Recognition is a new problem introduced this year, aiming to detect and classify the emotion of a group of people among three classes: positive, neutral and negative. This new task is the combination of two sub-challenges that were held in previous years. Despite the considerable progress that has been made in the field of computer vision, predicting the emotions of a group of people in the wild is still a difficult task, due to the number of faces in the image, various illumination conditions, face deformations, occlusions, non-frontal faces, etc. For this challenge, the organizers proposed a baseline based on the Inception-V3 architecture  [10] . Their work includes the analysis of visual and audio cues  [9] .\n\nOne of these sub-challenges is Group-level emotion recognition (GReco) 2018, which consists in predicting the mood of a group of people from images. Participants combined the top-down (the usage of the global tone of a photo for analysis) and bottom-up (performing analysis for each of the extracted faces, and then the subsequent combination of the results) approaches to design their prediction algorithm. The winner proposed a hybrid deep learning network  [11]  obtained by fusing four individually trained models for faces, scenes, skeletons and salient regions detection using VGGNet  [12] , ResNet  [13] , Inception  [14]  and LSTM  [15]  networks. The adjustment of each model's prediction weights by grid-search strategy allowed achieving a 68.08% classification accuracy on the testing set. The second-place system proposed a cascade attention network for the face cues and CNNs (VGG19  [12] , ResNet101  [13] , SE-Net154  [16] ) for body, face and image cues  [17] . Participants used the cascade attention network to solve the partial discrepancy of some faces to the photograph's general atmosphere, which allowed them to capture the importance of each face in the image and achieve 67.48% classification accuracy. For face and body detection, the researchers used MTCNN  [18]  and OPENPOSE  [19] , respectively. The third-place proposed a four-stream hybrid network, consisting of the face-location aware global stream, the multi-scale face stream, the global blurred stream and the global stream, which achieved 65.59% accuracy on the test set  [20] . For face detection, this team also used MTCNN  [18] . The face stream consisted of two networks designed for high and low-resolution face images. To learn features from the whole image, they fine-tuned a pre-trained VGG16  [12]  network on the ImageNet dataset  [21] . The face-location aware global stream was specially implemented to capture features from the whole image together with face-relative location information. To learn only scene related features, the participants proposed a global blurred stream, where original images with blurred faces were used as input. Finally, all four streams were fused by tuning fusion weights.\n\nThe second sub-challenge, Audio-video Emotion Recognition task (VReco), aims to predict six basic emotions plus neutral from videos, was organized seven times since 2013. In the 2019 EmotiW challenge, Li et al.  [22]  proposed a bimodality fusion framework containing two models. The first one is the facial image model, where four different types of neural networks (VGG-Face, Restnet18, Densenet121, VGG16) were used for feature extraction, and the obtained features were integrated into a Bi-Directional Long Short Term Memory (Bi-LSTM) model  [23]  to capture dynamical temporal information. The second one is the audio model based on two methods for feature extraction and classification, using deep learning feature extraction and the openSMILE toolbox  [24]  to compute LLD features. The fusion of the two models achieved 62.78% accuracy on the test set and ranked 2nd. Zhou et al.  [25]  explored emotion features (revealing the most suitable CNNs) and three types of fusion strategies (self-attention, relation-attention, and transformer), that are usually used to highlight the essential emotional features. Their model obtained 62.48% accuracy and ranked at the 3rd place.\n\nFigure  1 : The pipeline for the proposed approach to group emotion recognition. This scheme reflects the key points of our analysis, namely the process of model selection, training and adjustment.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "As already stated, this research voluntarily positions itself in a frugal approach excluding the prior detection of faces or the body poses, and more generally, the detection of individuals. It differs from usual approaches that combine global features with individual ones. As traditional CNNs are by nature not explainable, i.e. one does not know what has actually been learned, our global proposal takes advantage of this fact to increase the privacy of monitored users. Recent studies use individualized visual cues coupled with these global visual features in classrooms, some also combining them with global acoustic features  [20, 11, 17, 26] . In this first investigation about pure global group mood recognition, we will focus only on visual information, trying to reach the best performance before mixing with audio information. This study and our participation in the challenge is the first step towards a more comprehensive classroom ambiance recognition.\n\nThat said, as depicted in Figure  1 , our methodology explores different convolutional neural network architectures, focusing those that have shown superior results for computer vision tasks over the past years. These systems were trained on the training data and compared by evaluating their score on the challenge's validation set, revealing the most suitable architecture to such a global approach. We then selected the best initial model and gradually modified and refined it to increase the prediction accuracy on the validation set. The rationale is to assess the baseline performance of a global approach.\n\nThe available data to train group emotion recognition systems is limited. We decided to augment it with synthetic data generated on-the-fly during the training. This allows us to add a variety in terms of facial expressions, positions of faces and background types. This generation mechanism is described in section 4.2.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "This section presents the EmotiW 2020 dataset for the Audio-video Group Emotion Recognition sub-challenge and our synthetic dataset generation dedicated to group mood detection within images.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Vgaf Dataset",
      "text": "The VGAF dataset  [9]  is the set of videos for group-level emotion recognition collected in the wild. It contains 1,004 videos collected from the Web with different real-world scenarios, faces, illuminations, occlusions. The data is distributed into three parts: Train, Validation, and Test with 587, 215 and 202 samples respectively. These videos were collected from different social events, such as protests, TV shows, interviews, parties, meetings, sport events, etc. There are three categories in this dataset: Positive, Negative, Neutral. The duration of each video is between 8 and 25 seconds. The number of people present in each video ranges from 2 to 62. Data annotation was done manually by three different persons. The final label has been agreed upon by all annotators.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Synthetic Dataset",
      "text": "Besides VGAF, we created a synthetically generated dataset to enlarge the number of training samples. We used an approach similar to those described in  [27] . The process consists in creating images using real faces showing different emotions and superimpose them on an arbitrary background. The underlying idea is to guide the neural network while training to focus on faces in images while ignoring background pixels.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Generation Algorithm",
      "text": "The generation process is depicted in Figure  3 . It consists of several key steps. First, the generator selects a background and N faces randomly. Analyzing statistics from the VGAF dataset, the number of people present on each video ranges from 2 to 62, while the mode equals 9. Most videos contain 4 to 18 faces. Knowing that, we decided to set N ∈  [1, 9] . After data augmentation on selected faces, the generation algorithm places them on the image at random positions but, using a binary mask, the algorithm prevents occlusions between faces. Then, it computes the image label. All negative emotions (anger, fear, disgust, sadness) were combined into the class 'Negative', happiness and neutral respectively into 'Positive' and 'Neutral' classes. Each of the generated images contains emotions from different classes. The emotion label is set as the most common class presented in the image. If none, it is set to neutral. Figure  6  shows examples of generated images for each class. The next sections detail these generation steps.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Background Images",
      "text": "The first step in creating synthetic images is the background selection. The goal of the generation process is to feed the neural network with many different environments to learn to ignore them: extracting only emotions, regardless of background. Our expectation is that the system trained with such images would learn to focus on people. The dataset LSUN  [28]  is a good background provider as it contains 10 different scene categories: classroom, bedroom, bridge, church outdoor, conference room, dining room, kitchen, living room, restaurant, tower. Figure  4  shows some examples of background images.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Face Images Augmentations",
      "text": "To create synthetic images related to group emotion, the generator needs photographs of faces showing different emotions. For this, two facial expression databases are available: FACES  [29]  and KDEF  [30] . These datasets contain frontal facial photos of young, middle-aged, and old people that bring an essential variety to the image collection. As one can see in Figure  5 , the FACES image contains the entire head, the neck and part of the shoulders. In our generation process, these images are mixed with a cropped version of the KDEF frontal views 2  focused on the face  [31, 32] , from the chin to the top of the forehead (see Figure  3 ). The emotion annotation in both datasets is available in discrete emotion labels, namely the universal facial expressions plus neutral. Due to the presence of the background color on the face images, before overlaying them on the background image, the algorithm gets rid of it using color filtering with OpenCV  [33] .  To enrich the synthetic corpus, data augmentation applies to face images. Wang and al.  [34]  collected an extensive number of state-of-the-art approaches. From the proposed set of augmentation methods, we identified the most suitable, i.e. the augmentations that distort the initial data in a non-destructive way. Geometric and photometric data transformations are considered. The first type includes transformations that alter the pixels' position in the original image, and the second type encompasses changes in the RGB channels. Figure  5  shows examples of the retain photometric and geometric transformations used for face augmentation.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Validation",
      "text": "After finalizing the generation process, the first stage was to validate the potential of this data as a training resource. Indeed, before using the synthetic corpus in the challenge system, we had to verify that it is suitable to learn group emotions only with global features. We conducted several tests. As a starting point, we defined a two-class experiment (happiness or not) with only one face within the images. We implemented a model similar to Smile-CNN proposed by Chen & al.  [35] . The authors presented a deep learning architecture focused on detecting one of the most frequent human emotions  [36] , joy (aka happiness in corpora's labels). This architecture intends to detect smiles under both laboratory-controlled and wild data, on single face images. This straightforward architecture was designed to work on rather small face images (64 × 64), not on faces in context. The result was equivalent to random.\n\nWe decided to evaluate deeper neural networks. Even complexifying the task with up to 9 faces on the image, modified versions of ResNet 18  [13]  and Inception V3  [10]  achieved respectively 83.4% and 88.5% of accuracy. These results validate that the synthetic dataset can serve as training corpus in group emotion recognition and confirm the potentiality to detect emotions from the whole image with multiple faces. We started our work on the EmotiW Challenge from this point.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "This section describes the implementation details, the evaluation of different approaches, the description of the proposed model and the challenge results. For all our experiments, we used PyTorch  [37]  as a deep learning framework. We compared different models by training them using VGAF and our synthetic datasets, validating them on the VGAF validation set. The challenge organizers were in charge of computing challenge results.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Selection Of Underlying Architecture",
      "text": "Starting from validation results (section 4.2.4), we decided to explore exhaustively different state-of-the-art neural network architectures. Indeed, for some computer vision tasks, the advantages and drawbacks of these architectures have been explored. As far as we know, it is not the case for group-level emotion recognition from images. The accuracies of 13 widely employed deep learning architectures were compared on a challenge validation set. Results using these architectures as backbone, modifying them to provide a 3-class classification are presented in Table  5 .1. Each architecture was modified by changing the number of neurons in the last fully-connected layer to 3, as we have a ternary classification task. All of the considered models were pretrained on the 1000-class ImageNet dataset  [21] , which contains more than one million high-resolution images. Since these images are significantly different from the challenge's task, these models were then fine-tuned using the VGAF training dataset.\n\nThe computed results are quite surprising. First, unlike first results on binary classification (section 4.2.4), Inception V3 has a lower accuracy than ResNet18. Second, the deeper architectures have not the best accuracies. Among all of the evaluated algorithms, VGG-19 showed the best performance (52.36% of accuracy). Therefore, this architecture is the basis of the proposed model. The implementation details, as well as challenge results, are described in the next sections.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Underlying Architecture Validation Accuracy",
      "text": "Wide ResNet50  [38]  43.60% ResNet34  [13]  43.60% MobileNet v2  [39]  45.04% DenseNet  [40]  45.95% ResNeXt  [41]  47.39% Inception V3  [10]  47.91% ResNet50  [13]  48.17% GoogLeNet  [14]  48.43% VGG-11  [12]  48.69% ResNet18  [13]  49.22% VGG-16  [12]  50.91% SqueezeNet  [42]  51.44% VGG19  [12]  52.36%\n\nTable  1 : Performance of different network architectures on the VGAF validation set in order of accuracy",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Challenge Model",
      "text": "This section presents the optimization of the challenge model, its official accuracy and ranking of the team in the Audio-video Group Emotion Recognition sub-challenge.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Optimization",
      "text": "For the challenge, starting from the initial VGG19 model, we further explored several options to improve the model accuracy:\n\n• Train the model from scratch, allowing the network to learn more specific deep features, closer to the task;\n\n• Vary the size of linear layers;\n\n• Add complementary linear layers;\n\n• Fine-tune only last layer of our model;\n\n• Combine the above options.\n\nComparing all of the aforementioned options, the best performance on the validation set is observed by implementing the following procedure on the model presented in Figure  7 . First, some fully-connected layers, dropout layers and ReLU activation functions are added at the end of the network, finishing by a ternary tensor. As training from scratch does not provide improvement, the model is first trained on ImageNet. Using these weights, the model is then fine-tuned on both synthetic and VGAF corpora. For each epoch, we generated 10k images using our data generation algorithm and sampled 10k frames from VGAF videos, each frame having an equal probability for being selected. For all images  The learning rate equals 0.001. The training script uses stochastic gradient descent with 0.9 momentum as optimizer, and cross-entropy as loss function.\n\nTo obtain video-level labels from frame-level classification, we applied two techniques: score averaging and score accumulation. The best results were observed by predicting labels for all frames and then averaging the scores. The final system has a 57.18% accuracy on the validation set.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Challenge Results",
      "text": "The challenge organizers allowed to make a total of five submissions to evaluate the method on the test set. The team made three attempts, and managed to achieve an accuracy of 59.127%, reaching the eleventh place out of nineteenth. The best team achieved 76.85%, potentially with a multimodal approach. One can consider that the achieved accuracy is reasonable, given the small size of the training set, the wild data conditions and the fact that our team employed a frugal and unimodal approach with a fairly basic temporal integration.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "In order to go deeper into the results, we did several analyses. First, the class-wise accuracy matrix is shown in Table  5 .2.1. As can be seen from this table, the model shows better performance for the Positive and Negative classes on the testing set. As expected, the neutral class is the most difficult to classify. To analyze further the misclassifications, information is provided by the confusion matrix on the VGAF validation set (Figure  8 ) and in    Hypotheses in our methodology are two-fold. The first hypothesis is that using the image at a glance, the neural network will learn to focus on people. Moreover, using our synthetic dataset, the expectation was that it would mainly consider people's faces. The second hypothesis is that the network will ignore the background. To evaluate these aspects, we employed Class Activation Map (CAM) techniques to highlight the contribution of each part of the image to the current class classification. More precisely, we use Score-CAM  [43, 44]  to compute images shown in Figure  9 .\n\nThese three images illustrate Score-CAM results on Negative, Neutral and Positive images. Red parts of the heatmap represent less important pixels, while green, blue and purple highlight more and more relevant ones. Looking at these representations, one can conclude that our preliminary hypotheses are not fulfilled. Among these three examples, the best heatmap is for the neutral class. The network actually focuses on both people, ignoring the background. In contrast, in the Negative heatmap, the important pixels are located in storefronts while the network overlooks most people. Last, the positive heatmap reflects that the network concentrates on some people but completely ignores others as if they belonged to the background. Unexpectedly, the neural network does not focus on all people nor their faces. It also does not totally ignore the background pixels. We need to further explore these visualizations to better understand what is relevant and modify the network training accordingly. Nevertheless, these visualizations provide a bag of pixels, but not the underlying visual features (color, shape, texture...). More investigations have to be conducted in order to identify the substance of these features.\n\nThis paper proposes a deep learning model for group emotion detection using a privacy-safe non-individual approach.\n\nThe main novelty of this research is the frugal modeling, i.e. processing only global image cues instead of including more and more individual-based features, as well as mixing available datasets with a dedicated synthetic one. We compared thirteen state-of-the-art architectures and created a pure unimodal model able to recognize the mood of groups of people. The final architecture is based on the VGG-19 model with the addition of several fully-connected layers, dropout layers and ReLU activation functions. The experiments showed the promising result of 59.13% on the EmotiW Challenge test set, in a challenging 3-class classification task in the wild.\n\nIn the future, some improvements need to be investigated. First, several methods can be used to enhance our synthetic corpus generation: face swapping or replacement in available datasets, facial expression generation  [45, 46]  or even more complex methods targeting body language. The way our proposal tackles temporal classification is relatively basic and could be improved. This could be done using Recurrent Neural Networks (RNN) or Long Short-Term Memory (LSTM) networks, for instance. Another way of improvement relies on an in-depth analysis of the proposed network to identify possible enhancements in the network architecture or in the training process. Finally, as most other participants of the challenge did, including the audio stream will be obviously of interest.\n\nTo conclude, this research on a unimodal privacy-safe non-individual approach to group-level emotion recognition is promising. The results reinforce the team's ambition to identify classroom ambiance applying ethical rules prohibiting individual computation. Within the context of the Teaching Lab project, the final goal of such a perception system is to provide clues to teacher trainers to help young teachers improve their pedagogical skills.",
      "page_start": 8,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The pipeline for the proposed approach to group emotion recognition. This scheme reﬂects the key points of",
      "page": 3
    },
    {
      "caption": "Figure 1: , our methodology explores different convolutional neural network architectures,",
      "page": 3
    },
    {
      "caption": "Figure 2: highlights the difﬁculty of the task through examples of extracted frames from the VGAF [9] train dataset for",
      "page": 4
    },
    {
      "caption": "Figure 2: Examples of frames from VGAF [9] dataset, from left to right: Positive, Neutral and Negative images. Three",
      "page": 4
    },
    {
      "caption": "Figure 3: It consists of several key steps. First, the generator selects a background",
      "page": 4
    },
    {
      "caption": "Figure 3: Synthetic Dataset Generation Scheme. This scheme represents the key steps of data generation process",
      "page": 5
    },
    {
      "caption": "Figure 4: Background images from LSUN [28] dataset, from left to right: tower, restaurant, classroom. Three examples",
      "page": 5
    },
    {
      "caption": "Figure 4: shows some examples",
      "page": 5
    },
    {
      "caption": "Figure 5: , the FACES image contains the entire head, the neck and part of the shoulders. In our generation",
      "page": 5
    },
    {
      "caption": "Figure 3: ). The emotion annotation in both datasets is available in discrete",
      "page": 5
    },
    {
      "caption": "Figure 5: Examples of face augmentations, from top to bottom, from left to right: source image from [29], source image",
      "page": 6
    },
    {
      "caption": "Figure 6: Examples of synthetically generated data. Labels from left to right: Positive, Negative, Neutral. These images",
      "page": 6
    },
    {
      "caption": "Figure 5: shows examples of the retain",
      "page": 6
    },
    {
      "caption": "Figure 7: First, some fully-connected layers, dropout layers and",
      "page": 7
    },
    {
      "caption": "Figure 7: Scheme of the modiﬁed VGG-19 architecture. The scheme is a sequence of different layers: convolutional,",
      "page": 8
    },
    {
      "caption": "Figure 8: ) and in Table 5.2.1 that contain",
      "page": 8
    },
    {
      "caption": "Figure 8: Normalized confusion matrix for the validation set for 3-class classiﬁcation task for the ﬁnal model. It shows",
      "page": 9
    },
    {
      "caption": "Figure 9: Negative, Neutral and Positive visualization using Score-cam algorithm [43].",
      "page": 9
    },
    {
      "caption": "Figure 9: These three images illustrate Score-CAM results on Negative, Neutral and Positive images. Red parts of the heatmap",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Performance of different network architectures on the VGAF",
      "page": 7
    },
    {
      "caption": "Table 2: Precision, Recall and F1-score for Neutral,",
      "page": 8
    },
    {
      "caption": "Table 3: Accuracy for our best challenge submission.",
      "page": 8
    },
    {
      "caption": "Table 5: 2.1. As can be seen from this table, the model shows better performance for the Positive and Negative classes on",
      "page": 8
    },
    {
      "caption": "Table 5: 2.1 that contain",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing: challenges",
      "authors": [
        "R Picard"
      ],
      "year": "2003",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "2",
      "title": "Group affect",
      "authors": [
        "S Barsade",
        "A Knight"
      ],
      "year": "2015",
      "venue": "Annu. Rev. Organ. Psychol. Organ. Behav"
    },
    {
      "citation_id": "3",
      "title": "A performance comparison of eight commercially available automatic classifiers for facial affect recognition",
      "authors": [
        "D Dupré",
        "E Krumhuber",
        "D Küster",
        "G Mckeown"
      ],
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "4",
      "title": "Emotiw 2016: Video and group-level emotion recognition challenges",
      "authors": [
        "A Dhall",
        "R Goecke",
        "J Joshi",
        "J Hoey",
        "T Gedeon"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "5",
      "title": "Edusense: Practical classroom sensing at scale",
      "authors": [
        "K Ahuja",
        "D Kim",
        "F Xhakaj",
        "V Varga",
        "A Xie",
        "S Zhang",
        "J Townsend",
        "C Harrison",
        "A Ogan",
        "Y Agarwal"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "6",
      "title": "Ethical Teaching Analytics in a Context-Aware Classroom: A Manifesto",
      "authors": [
        "Romain Laurent",
        "Dominique Vaufreydaz",
        "Philippe Dessus"
      ],
      "year": "2020",
      "venue": "ERCIM News"
    },
    {
      "citation_id": "7",
      "title": "Automated classification of classroom climate by audio analysis",
      "authors": [
        "A James",
        "Y Chua",
        "T Maszczyk",
        "A Núñez",
        "R Bull",
        "K Lee",
        "J Dauwels"
      ],
      "year": "2019",
      "venue": "th International Workshop on Spoken Dialogue System Technology"
    },
    {
      "citation_id": "8",
      "title": "Emotiw 2020: Driver gaze, group emotion, student engagement and physiological signal based challenges",
      "authors": [
        "A Dhall",
        "G Sharma",
        "R Goecke",
        "T Gedeon"
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "9",
      "title": "Automatic group level affect and cohesion prediction in videos",
      "authors": [
        "G Sharma",
        "S Ghosh",
        "A Dhall"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"
    },
    {
      "citation_id": "10",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "C Szegedy",
        "V Vanhoucke",
        "S Ioffe",
        "J Shlens",
        "Z Wojna"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "11",
      "title": "Group-level emotion recognition using hybrid deep models based on faces, scenes, skeletons and visual attentions",
      "authors": [
        "X Guo",
        "B Zhu",
        "L Polania",
        "C Boncelet",
        "K Barner"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "12",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "13",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "14",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "15",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "16",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "17",
      "title": "Cascade attention networks for group emotion recognition with face, body and image cues",
      "authors": [
        "K Wang",
        "X Zeng",
        "J Yang",
        "D Meng",
        "K Zhang",
        "X Peng",
        "Y Qiao"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "18",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "19",
      "title": "Realtime multi-person 2d pose estimation using part affinity fields",
      "authors": [
        "Z Cao",
        "T Simon",
        "S.-E Wei",
        "Y Sheikh"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Group-level emotion recognition using deep models with a four-stream hybrid network",
      "authors": [
        "A Khan",
        "Z Li",
        "J Cai",
        "Z Meng",
        "J O'reilly",
        "Y Tong"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "21",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "22",
      "title": "Bi-modality fusion for emotion recognition in the wild",
      "authors": [
        "S Li",
        "W Zheng",
        "Y Zong",
        "C Lu",
        "C Tang",
        "X Jiang",
        "J Liu",
        "W Xia"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "23",
      "title": "Bidirectional recurrent neural networks",
      "authors": [
        "M Schuster",
        "K Paliwal"
      ],
      "year": "1997",
      "venue": "IEEE transactions on Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "Exploring emotion features and fusion strategies for audio-video emotion recognition",
      "authors": [
        "H Zhou",
        "D Meng",
        "Y Zhang",
        "X Peng",
        "J Du",
        "K Wang",
        "Y Qiao"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "26",
      "title": "Toward automated classroom observation: Predicting positive and negative climate",
      "authors": [
        "A Ramakrishnan",
        "E Ottmar",
        "J Locasale-Crouch",
        "J Whitehill"
      ],
      "year": "2019",
      "venue": "Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "27",
      "title": "Learning from synthetic humans",
      "authors": [
        "G Varol",
        "J Romero",
        "X Martin",
        "N Mahmood",
        "M Black",
        "I Laptev",
        "C Schmid"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop",
      "authors": [
        "F Yu",
        "A Seff",
        "Y Zhang",
        "S Song",
        "T Funkhouser",
        "J Xiao"
      ],
      "year": "2015",
      "venue": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop",
      "arxiv": "arXiv:1506.03365"
    },
    {
      "citation_id": "29",
      "title": "Faces-a database of facial expressions in young, middle-aged, and older women and men: Development and validation",
      "authors": [
        "N Ebner",
        "M Riediger",
        "U Lindenberger"
      ],
      "year": "2010",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "30",
      "title": "The karolinska directed emotional faces: a validation study",
      "authors": [
        "E Goeleven",
        "R De Raedt",
        "L Leyman",
        "B Verschuere"
      ],
      "year": "2008",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "31",
      "title": "Perceived emotion genuineness: normative ratings for popular facial expression stimuli and the development of perceived-as-genuine and perceived-as-fake sets",
      "authors": [
        "A Dawel",
        "L Wright",
        "J Irons",
        "R Dumbleton",
        "R Palermo",
        "R O'kearney",
        "E Mckone"
      ],
      "year": "2017",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "32",
      "title": "Facial emotion detection using modified eyemap-mouthmap algorithm on an enhanced image and classification with tensorflow",
      "authors": [
        "A Joseph",
        "P Geetha"
      ],
      "year": "2020",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "33",
      "title": "OpenCV computer vision with python",
      "authors": [
        "J Howse"
      ],
      "year": "2013",
      "venue": "OpenCV computer vision with python"
    },
    {
      "citation_id": "34",
      "title": "A survey on face data augmentation",
      "authors": [
        "X Wang",
        "K Wang",
        "S Lian"
      ],
      "year": "2019",
      "venue": "A survey on face data augmentation",
      "arxiv": "arXiv:1904.11685"
    },
    {
      "citation_id": "35",
      "title": "Smile detection in the wild with deep convolutional neural networks",
      "authors": [
        "J Chen",
        "Q Ou",
        "Z Chi",
        "H Fu"
      ],
      "year": "2017",
      "venue": "Machine vision and applications"
    },
    {
      "citation_id": "36",
      "title": "Emotions in everyday life",
      "authors": [
        "D Trampe",
        "J Quoidbach",
        "M Taquet"
      ],
      "year": "2015",
      "venue": "PloS one"
    },
    {
      "citation_id": "37",
      "title": "Automatic differentiation in pytorch",
      "authors": [
        "A Paszke",
        "S Gross",
        "S Chintala",
        "G Chanan",
        "E Yang",
        "Z Devito",
        "Z Lin",
        "A Desmaison",
        "L Antiga",
        "A Lerer"
      ],
      "year": "2017",
      "venue": "Automatic differentiation in pytorch"
    },
    {
      "citation_id": "38",
      "title": "Wide residual networks",
      "authors": [
        "S Zagoruyko",
        "N Komodakis"
      ],
      "year": "2016",
      "venue": "Wide residual networks",
      "arxiv": "arXiv:1605.07146"
    },
    {
      "citation_id": "39",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "M Sandler",
        "A Howard",
        "M Zhu",
        "A Zhmoginov",
        "L.-C Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "40",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "41",
      "title": "Aggregated residual transformations for deep neural networks",
      "authors": [
        "S Xie",
        "R Girshick",
        "P Dollár",
        "Z Tu",
        "K He"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "42",
      "title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size",
      "authors": [
        "F Iandola",
        "S Han",
        "M Moskewicz",
        "K Ashraf",
        "W Dally",
        "K Keutzer"
      ],
      "year": "2016",
      "venue": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size",
      "arxiv": "arXiv:1602.07360"
    },
    {
      "citation_id": "43",
      "title": "Score-cam: Score-weighted visual explanations for convolutional neural networks",
      "authors": [
        "H Wang",
        "Z Wang",
        "M Du",
        "F Yang",
        "Z Zhang",
        "S Ding",
        "P Mardziel",
        "X Hu"
      ],
      "year": "2019",
      "venue": "Score-cam: Score-weighted visual explanations for convolutional neural networks"
    },
    {
      "citation_id": "44",
      "title": "Pytorch cnn visualizations",
      "authors": [
        "U Ozbulak"
      ],
      "year": "2019",
      "venue": "Pytorch cnn visualizations"
    },
    {
      "citation_id": "45",
      "title": "Generative cooperative net for image generation and data augmentation",
      "authors": [
        "Q Xu",
        "Z Qin",
        "T Wan"
      ],
      "year": "2017",
      "venue": "Generative cooperative net for image generation and data augmentation"
    },
    {
      "citation_id": "46",
      "title": "U-net conditional gans for photo-realistic and identity-preserving facial expression synthesis",
      "authors": [
        "X Wang",
        "Y Wang",
        "W Li"
      ],
      "year": "2019",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)"
    }
  ]
}