{
  "paper_id": "2409.11906v1",
  "title": "Fusion In Context: A Multimodal Approach To Affective State Recognition",
  "published": "2024-09-18T12:06:23Z",
  "authors": [
    "Youssef Mohamed",
    "Severin Lemaignan",
    "Arzu Guneysu",
    "Patric Jensfelt",
    "Christian Smith"
  ],
  "keywords": [
    "Human detection",
    "computer vision",
    "social human-robot interaction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Accurate recognition of human emotions is a crucial challenge in affective computing and human-robot interaction (HRI). Emotional states play a vital role in shaping behaviors, decisions, and social interactions. However, emotional expressions can be influenced by contextual factors, leading to misinterpretations if context is not considered. Multimodal fusion, combining modalities like facial expressions, speech, and physiological signals, has shown promise in improving affect recognition. This paper proposes a transformerbased multimodal fusion approach that leverages facial thermal data, facial action units, and textual context information for context-aware emotion recognition. We explore modalityspecific encoders to learn tailored representations, which are then fused using additive fusion and processed by a shared transformer encoder to capture temporal dependencies and interactions. The proposed method is evaluated on a dataset collected from participants engaged in a tangible tabletop Pacman game designed to induce various affective states. Our results demonstrate the effectiveness of incorporating contextual information and multimodal fusion for affective state recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Accurately perceiving and interpreting human emotions is a fundamental challenge in affective computing and humanrobot interaction (HRI) fields. Since emotions significantly influence our behavior, decisions, and social interactions, creating systems that can reliably recognize and understand these emotional states is essential. Such advancements would enable robots to engage in more natural, effective interactions by better comprehending and responding to human needs and preferences.\n\nOne of the key challenges in affective state recognition lies in accounting for the contextual information surrounding affect expressions. In other words, the same facial expression or physiological signal may convey different affective meanings depending on the context in which it occurs. For instance, a smile in a social setting may indicate enjoyment, while a similar smile in a different context could signify sarcasm or discomfort. Failing to consider these contextual factors can lead to misinterpretations and inaccurate affect recognition, hindering the effectiveness of affective computing systems.\n\nMultimodal fusion, which integrates diverse data streams such as facial expressions, speech, and physiological signals, has shown promise in enhancing affect recognition performance compared to unimodal methods  [2] ,  [20] ,  [19] ,  [3] . However, effectively fusing these modalities and incorporating contextual information remains a significant challenge in the field.\n\nTo address this challenge, we propose a transformerbased multimodal fusion approach for context-aware emotion recognition. Our method leverages recent advances in deep learning, particularly transformer architectures, which have demonstrated remarkable capabilities in capturing temporal dependencies and modeling complex interactions between modalities  [25] ,  [17] .\n\nOur approach utilizes modality-specific encoders to extract tailored representations from facial thermal data  [19] , action units  [4] , and textual context information  [14] . These representations are then combined and processed by a shared transformer encoder. This architecture enables effective integration of contextual cues and leverages the complementary information from multiple modalities, providing a more comprehensive understanding of affective states.\n\nWe evaluate our method on a dataset collected from participants in a tabletop Pacman game  [10]  re-designed to elicit various affective states, including enjoyment, boredom, and frustration. By doing so, we aim to demonstrate the potential of our approach in real-world emotion recognition scenarios.\n\nThe primary contributions of this paper are as follows.\n\n• Contextual Information Integration: Demonstrates the importance of incorporating contextual information as a separate modality to enhance affect recognition accuracy when added to other physiological and visual modalities.\n\n• Transformer-based Multimodal Architecture: Proposes a transformer architecture with additive fusion of modality-specific representations, effectively capturing temporal dependencies and interactions for improved affective state detection. Our experimental results on the Pacman game dataset demonstrate the relative effectiveness of different modalities and their combinations in our transformer-based multimodal fusion approach. By examining various configurations of thermal data, action units, and contextual information, we provide insights into the contributions of each modality to affective state recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Background",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Context-Aware Emotion Recognition",
      "text": "Context-aware emotion recognition is crucial for improving affective computing systems' accuracy  [11] ,  [28] . Various approaches have been proposed to incorporate context, each with its own limitations.\n\nMittal et al.  [18]  developed a multimodal and contextaware model using multiplicative fusion to combine facial expressions, speech, and physiological signals. It employs a graph-based attention mechanism to weight modalities based on context. However, this approach may struggle with complex, non-linear relationships between modalities and context.\n\nWang et al.  [28]  proposed a context-aware network with a hierarchical attention mechanism for video data. It learns to focus on salient emotional cues, considering facial expressions, body language, voice tone, and environmental context simultaneously. The reliance on predefined hierarchical structures, however, may limit its adaptability to diverse scenarios.\n\nKim et al.  [13]  introduced a deep semantic feature fusion approach for video emotion recognition, combining facial expressions, audio features, and textual context using hierarchical fusion. While innovative, their method may not fully capture the nuanced interplay between different contextual elements and emotional expressions.\n\nThese studies demonstrate the importance of incorporating context in affect recognition systems, but often treat context as a fixed set of features, potentially overlooking its dynamic nature.\n\nWe propose a transformer-based architecture for more effective multimodal integration, overcoming limitations of fixed fusion strategies. By leveraging natural language processing and transformers, our method achieves a more comprehensive and adaptable integration of context in emotion recognition, potentially leading to more accurate and robust affective computing systems.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Multimodal Machine Learning",
      "text": "Multimodal machine learning integrates multiple modalities like text, audio, images, and videos  [15] . Key principles driving innovations include modality heterogeneity, connections, and interactions  [15] . These principles are fundamental to our work, integrating facial thermal data, action units, and textual context.\n\nCore technical challenges include representation and alignment  [1] . Representation involves encoding diverse modalities with distinct statistical properties, while alignment concerns mapping corresponding elements across modalities. These challenges are particularly relevant in emotion recognition, where facial expressions, thermal, and contextual information must be coherently integrated.\n\nJiang et al.  [12]  highlighted the importance of constructing meaningful latent modality structures, suggesting that exact modality alignment may not be optimal for tasks like emotion recognition.\n\nOur approach addresses these principles through modalityspecific encoders and a shared transformer encoder capturing temporal dependencies and interactions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Transformer Multimodal Fusion",
      "text": "Transformer-based architectures have gained popularity for multimodal fusion due to their ability to capture intermodality interactions and model temporal dependencies.\n\nA recent advancement in this field is the work of Faye et al.  [8] , who proposed the Context-Based Multimodal Fusion (CBMF) model. This approach combines modality fusion and data distribution alignment using context vectors fused with modality embeddings. The CBMF model is particularly relevant to our work as it shares our focus on integrating contextual information directly into the fusion process.\n\nOur method builds upon recent advancements in multimodal analysis for manipulation detection. While some approaches use complex interaction mechanisms  [27] , we employ separate encoding processes for each modality followed by additive fusion, allowing effective integration without intricate cross-modal attention mechanisms.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Dataset",
      "text": "This study utilizes a dataset collected by  [20] , which captures participants' affective states during a tangible Pacman game designed with multiple configurations and can induce four affects: frustration, enjoyment, boredom and neutral  [20] ,  [10] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Data Modalities",
      "text": "The dataset comprises three main modalities: thermal data from facial regions of interest (ROIs), visual data in the form of Action Units (AUs) extracted from RGB images, and text data from game-play logs and settings (see Table  I ).\n\n1) Thermal and Visual Features: Figure  1  illustrates the facial landmarks and AUs extracted using OpenFace (left), and the thermal ROIs (right).\n\nTable  I  summarizes the extracted features for each modality.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Dataset Composition",
      "text": "Our dataset includes four distinct affective states: baseline (neutral), enjoyment, boredom, and frustration. We use a 7second window for analysis, aligning with the methodology of  [20] . This approach is grounded in the well-established understanding that physiological signals typically manifest on the face within a 5-15 second timeframe, while facial expressions can take up to 4 seconds to appear and often persist for longer  [22] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Context Extraction And Classification",
      "text": "We have gained access to the raw video data, the videos of each participant were then processed to capture their interactions within the game environment. The contextual data was categorized into two types: Game-Only Context (GOC) and Full Context (FC)\n\nFor both types of context, we first generated descriptive sentences, which were then transformed into highdimensional vector representations using the OpenAI embedding model embedding-large  [21] . This model converted each descriptive sentence into a 3072-dimensional vector, capturing the semantic nuances of the context.\n\n1) Game-Only Context (GOC) Embedding: For the GOC, we included only game-related information. An example sentence might be:\n\n\"The person is playing a Pacman game with difficulty level: easy\"\n\nThe description of the game difficulty was based on the speed of the robots, their number, and the amount of rotation needed to collect the points, which was classified into three settings: easy, medium, and hard  [20] .\n\n2) Full Context (FC) Embedding: For the FC, we combined the game settings and difficulty level with facial expression descriptions. To capture the temporal dynamics of facial expressions, we implemented a sliding window on the raw video data. We sub-sampled the video stream to 1 frame per second, as we do not expect the signals to move at a faster rate  [22] .\n\nFor each instance, we extracted a pair of consecutive frames: the current frame at time t and its predecessor at time t -1. This two-frame window slides throughout the duration of the video, allowing us to capture the evolution of facial expressions over time  [16] .\n\nWe used GPT-4(V) model, accessed through its API, for facial expression analysis. Each frame pair was submitted to the model using the following prompt:\n\nGiven two images, the first of the face at time t-1 and the second at time t, describe the current emotional state of the person in one brief sentence, considering the presence and intensities of facial expressions.\n\nAn example of a full context sentence combining game information and facial expression data would be:",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Game Context",
      "text": "The person is playing a Pacman game with difficulty level: easy GPT-4(V) Output (Facial Description) with a look of wonder or amazement with raised eyebrows",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Transformer",
      "text": "In this section, we present a transformer-based model for multimodal affect recognition. The three inputs comprises thermal features, visual features (action units), and contextual data. The raw input is discretized using a 7-second sliding window  [19]  stepping 1 second at a time to match the classification into one of the four affective states provided by  [20] . The latter is the target output for the transformer network.\n\nFig.  1 : Facial landmarks and Action Units extracted using OpenFace (left) and thermal regions of interest (right)  [20] .\n\nIn the following subsections, we present our transformer model, discussing its architecture, key components, and the hyperparameters used for optimization.\n\n1) Architecture: The proposed multimodal fusion method uses a hierarchical transformer-based architecture with three components: modality-specific encoding, additive fusion, and shared transformer encoder. Each input modality is processed by a dedicated transformer encoder branch, learning representations tailored to their unique characteristics. An additive fusion mechanism combines these representations, followed by a shared transformer encoder for further processing.\n\nLet X (1) , X (2) , . . . , X (M ) denote input tensors for M modalities, where X (i) ∈ R N ×Di represents the i-th modality with N samples and D i features. Each modality-specific encoder f i processes input X (i) to produce representation Z (i) :\n\nAdditive fusion combines modality-specific representations:\n\nA shared transformer encoder g processes Z fused to produce final encoded representation H:\n\nH undergoes positional encoding and is fed into a classification head (linear layer) for affective state recognition.\n\nThis method captures interactions between diverse modalities while leveraging the transformer architecture's ability to model temporal dependencies and complex relationships within the fused representation  [15] .\n\n2) Hyper-parameters: The model's hyperparameters were selected using a gridsearch algorithm, testing various combinations on the full dataset. For each hyperparameter, we explored four different options, with numerical parameters varied by factors of 10. This search revealed that the model's performance was most sensitive to the learning rate and the choice of optimizer, while other parameters exhibited robust performance across a range of values.\n\nThe final hyperparameter configuration for the transformer model was as follows:\n\n• Transformer networks: We used k-fold (k = 29), training on 28 groups, and testing on the remaining one, resulting in 29 total evaluations. This approach was applied across all models, with multiple runs using different random seeds for robustness.\n\nTo mitigate overfitting, we implemented early stopping during the training process with a patience value of 5. This approach ensured that the model's training was halted when no improvement was observed in the validation loss for five consecutive epochs, thereby optimizing the model's generalization.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "V. Results",
      "text": "We evaluated our proposed transformer model using various input modality configurations. Table  II  presents an ablation study, showing F1 scores for different combinations of input modalities.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Thermal + Action Units + Full Context",
      "text": "The combination of Thermal, Action Units (AU), and Full Context (FC) modalities yielded an F1 score of 89%. Fig.  3  presents the normalized confusion matrix for this configuration.\n\nThe model detected the neutral (baseline) state with 91.1% accuracy. Enjoyment recognition achieved the highest accuracy at 96.9%. Boredom detection showed an accuracy of 78.3%, which is lower compared to other emotional states. Frustration detection reached an accuracy of 85.8%.\n\nMisclassifications were relatively low across categories. Notably, 10.83% of boredom instances were misclassified  as neutral, and 10.45% of frustration instances were misclassified as enjoyment. Other misclassification rates remained below 10%.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Thermal + Action Units + Game-Only Context",
      "text": "Excluding facial descriptions while retaining thermal data, action units, and game-only context resulted in an F1 score of 76% and an average accuracy of 79.53%. Baseline and enjoyment states were detected with high accuracy (84% and 89%, respectively), while boredom exhibited some confusion with enjoyment and baseline states.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Thermal + Full Context",
      "text": "The Thermal + FC configuration achieved an F1 score of 57.51% and an average accuracy of 63.78% (see Figure ??. The confusion matrix for this setup is as follows: This configuration showed moderate performance, with the baseline state being the most accurately detected. However, there was notable confusion between other affective states, particularly between boredom and enjoyment, and between frustration and baseline.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Individual Modalities 1) Gpt-4V:",
      "text": "We have also used GPT-4V on each frame of the video with the prompt \"Given the facial expressions and the context of playing a pacman game. Detect one output of four emotional states of the person in the image: Baseline, Boredom, Frustration and Enjoyment. Only write the output\" This resulted in an f1 score of 23%, with major misclassification between all classes.\n\n2) Full Context Only: Using only FC resulted in a low F1 score of 25.76% and an average accuracy of 32.14%, indicating significant misclassification across all affective states.\n\n3) Action Units Only: The AU-only configuration achieved an F1 score of 64.85% and an average accuracy of 67.38%. It showed high accuracy for the baseline state (93.34%) but exhibited confusion between frustration and enjoyment states.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Discussion",
      "text": "The results of our study underscore the importance of multimodal input configurations for affective state detection. The proposed transformer model demonstrated an improved performance when combining thermal data, action units (AU), and contextual information, achieving an F1 score of 89%. This finding aligns with previous studies  [6] ,  [9]  that emphasize the effectiveness of multimodal approaches in affect recognition.\n\nThe configuration using thermal data and AU alone yielded an F1 score of 84%, which is a substantial improvement over the use of either modality independently (30% for thermal data alone and 65% for AU alone). This result corroborates earlier research by  [23]  and  [7]  on the value of thermal imagery and facial action units in emotion recognition. However, our results indicate that the combination of these modalities is more effective than using them in isolation, supporting the hypothesis that thermal and facial action data capture distinct but complementary aspects of affective expressions  [20] ,  [19] .\n\nIncorporating contextual information further enhanced the model's performance, as evidenced by the increase in the F1 score to 89%. This improvement is particularly noteworthy in the recognition of enjoyment and frustration states, which showed significant gains in detection accuracy. This finding is supported by  [24] , who demonstrated that context-aware models could significantly enhance emotion recognition by providing additional situational cues that help disambiguate similar affective states.\n\nInterestingly, the addition of Game Only-Context (GOC) led to a decreased F1 score compared to using only thermal and AU modalities, suggesting that GOC may introduce noise rather than providing sufficient context for the transformer. In contrast, the addition of Full-Context (FC) improved the accuracy of thermal data from 30% to 58% and AU data from 65% to 75%. These findings align with previous research by  [2]  and  [5] , emphasizing that the quality of added modalities is crucial, not just their quantity.\n\nThe reduction in F1 score when incorporating certain additional modalities is not unprecedented.  [26]  observed similar effects in multimodal affect recognition, attributing such decreases to potential inter-modality conflicts or insufficient integration strategies.\n\nUnlike the results in  [20] , our transformer-based model with FC data (Figure  3 ) nearly eliminates confusion between enjoyment and frustration, dramatically improving their classification (95.15% and 85.82% respectively).\n\nThe FC-only configuration performed poorly, with an F1 score of 25.76%, indicating that contextual information alone is insufficient for accurate affective state detection. This is consistent with the findings of  [6] , who reported that while context can enhance emotion recognition, it cannot replace direct physiological or facial cues. Similarly, the single-modality configurations (thermal data or AU alone) showed limitations, particularly in differentiating between enjoyment and frustration or boredom and other states. These results underscore the necessity of multimodal approaches for affective state detection, as single modalities lack the comprehensive coverage needed to capture the full spectrum of affective state expressions.\n\nDespite using a 7-second analysis window, our system is designed for real-time implementation on robots. This approach is effective because the system's primary goal is to detect the user's affective state during specific tasks, which typically takes 5-15 seconds.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "Our study demonstrates the efficacy of multimodal integration for affective state detection. By fusing thermal data, action units, and contextual information, our transformerbased model achieved an impressive F1 score of 89%, outperforming GPT-4(V)'s 23%. This performance difference can be attributed to two key factors: First, our multimodal approach provides a more comprehensive view of affective states, capturing nuances that may be missed in purely visual analysis. Second, unlike GPT-4(V)'s general-purpose design, our model is specifically tailored for affective state detection in our experimental context.\n\nThese findings underscore the importance of diverse data sources and advanced fusion techniques in developing accurate affective state detection systems. Moreover, they show the potential for more advancements in affective computing through targeted multimodal approaches and specialized model architectures.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the",
      "page": 3
    },
    {
      "caption": "Figure 1: Facial landmarks and Action Units extracted using OpenFace (left) and thermal regions of interest (right)[20].",
      "page": 4
    },
    {
      "caption": "Figure 3: presents the normalized confusion matrix for this",
      "page": 4
    },
    {
      "caption": "Figure 2: Multimodal Transformer Architecture: Integrates action units (16), facial thermal data (144),",
      "page": 5
    },
    {
      "caption": "Figure 3: Confusion Matrix for Thermal + AU + FC configu-",
      "page": 5
    },
    {
      "caption": "Figure 3: ) nearly eliminates confusion between",
      "page": 6
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Challenges and applications in multimodal machine learning. The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition",
      "authors": [
        "T Baltru Šaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Challenges and applications in multimodal machine learning. The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition"
    },
    {
      "citation_id": "2",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltru Šaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Multimodal machine learning: A survey and taxonomy"
    },
    {
      "citation_id": "3",
      "title": "Multimodal egocentric analysis of focused interactions",
      "authors": [
        "S Bano",
        "T Suveges",
        "J Zhang",
        "S Mckenna"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "4",
      "title": "Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements",
      "authors": [
        "L Barrett",
        "R Adolphs",
        "S Marsella",
        "A Martinez",
        "S Pollak"
      ],
      "year": "2019",
      "venue": "Psychological science in the public interest"
    },
    {
      "citation_id": "5",
      "title": "Multimodal affect detection in the wild: Accuracy, availability, and generalizability",
      "authors": [
        "N Bosch"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "6",
      "title": "Analysis of emotion recognition using facial expressions, speech and multimodal information",
      "authors": [
        "C Busso",
        "Z Deng",
        "S Yildirim",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "S Lee",
        "U Neumann",
        "S Narayanan"
      ],
      "year": "2004",
      "venue": "Proceedings of the 6th International Conference on Multimodal Interfaces (ICMI"
    },
    {
      "citation_id": "7",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "8",
      "title": "Contextbased multimodal fusion",
      "authors": [
        "B Faye",
        "H Azzag",
        "M Lebbah",
        "D Bouchaffra"
      ],
      "year": "2024",
      "venue": "Contextbased multimodal fusion",
      "arxiv": "arXiv:2403.04650"
    },
    {
      "citation_id": "9",
      "title": "Automatic, dimensional and continuous emotion recognition",
      "authors": [
        "H Gunes",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "International Journal of Synthetic Emotions (IJSE)"
    },
    {
      "citation_id": "10",
      "title": "Iterative design of an upper limb rehabilitation game with tangible robots",
      "authors": [
        "A Guneysu Ozgur",
        "M Wessel",
        "W Johal",
        "K Sharma",
        "A Özg Ür",
        "P Vuadens",
        "F Mondada",
        "F Hummel",
        "P Dillenbourg"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "11",
      "title": "Contextaware emotion recognition based on visual relationship detection",
      "authors": [
        "M.-H Hoang",
        "S.-H Kim",
        "H.-J Yang",
        "G.-S Lee"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "12",
      "title": "Understanding and constructing latent modality structures in multi-modal representation learning",
      "authors": [
        "Q Jiang",
        "C Chen",
        "H Zhao",
        "L Chen",
        "Q Ping",
        "S Tran",
        "Y Xu",
        "B Zeng",
        "T Chilimbi"
      ],
      "year": "2023",
      "venue": "Understanding and constructing latent modality structures in multi-modal representation learning",
      "arxiv": "arXiv:2303.05952"
    },
    {
      "citation_id": "13",
      "title": "Emotion in context: Deep semantic feature fusion for video emotion recognition",
      "authors": [
        "Y Kim",
        "H Lee",
        "B Kim"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM on Multimedia Conference"
    },
    {
      "citation_id": "14",
      "title": "Social embeddings: Concept and initial investigation",
      "authors": [
        "S Lemaignan",
        "A Andriella",
        "L Ferrini",
        "L Juricic",
        "Y Mohamed",
        "R Ros"
      ],
      "year": "2024",
      "venue": "Open Research Europe"
    },
    {
      "citation_id": "15",
      "title": "Foundations and trends in multimodal machine learning: Principles, challenges, and open questions",
      "authors": [
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2022",
      "venue": "Foundations and trends in multimodal machine learning: Principles, challenges, and open questions",
      "arxiv": "arXiv:2209.03430"
    },
    {
      "citation_id": "16",
      "title": "Gpt as psychologist? preliminary evaluations for gpt-4v on visual affective computing",
      "authors": [
        "H Lu",
        "X Niu",
        "J Wang",
        "Y Wang",
        "Q Hu",
        "J Tang",
        "Y Zhang",
        "K Yuan",
        "B Huang",
        "Z Yu",
        "Et Al"
      ],
      "year": "2024",
      "venue": "Gpt as psychologist? preliminary evaluations for gpt-4v on visual affective computing",
      "arxiv": "arXiv:2403.05916"
    },
    {
      "citation_id": "17",
      "title": "Facial expression recognition with visual transformers and attentional selective fusion",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Multimodal and contextaware emotion perception model with multiplicative fusion",
      "authors": [
        "T Mittal",
        "A Bera",
        "D Manocha"
      ],
      "year": "2021",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "19",
      "title": "Automatic frustration detection using thermal imaging",
      "authors": [
        "Y Mohamed",
        "G Ballardini",
        "M Parreira",
        "S Lemaignan",
        "I Leite"
      ],
      "year": "2022",
      "venue": "2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
    },
    {
      "citation_id": "20",
      "title": "Multi-modal affect detection using thermal and optical imaging in a gamified robotic exercise",
      "authors": [
        "Y Mohamed",
        "A Üneysu",
        "S Lemaignan",
        "I Leite"
      ],
      "year": "2023",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "21",
      "title": "Openai embeddings: Language models as apis",
      "authors": [
        "Openai"
      ],
      "year": "2021",
      "venue": "Openai embeddings: Language models as apis"
    },
    {
      "citation_id": "22",
      "title": "What the face reveals : basic and applied studies of spontaneous expression using the facial action coding system (FACS)",
      "authors": [
        "E Paul",
        "E Rosenberg"
      ],
      "year": "2005",
      "venue": "What the face reveals : basic and applied studies of spontaneous expression using the facial action coding system (FACS)"
    },
    {
      "citation_id": "23",
      "title": "Seeing through the face of deception",
      "authors": [
        "I Pavlidis",
        "N Eberhardt",
        "J Levine"
      ],
      "year": "2002",
      "venue": "Nature"
    },
    {
      "citation_id": "24",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "25",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "26",
      "title": "Exploring fusion methods for multimodal emotion recognition with missing data",
      "authors": [
        "J Wagner",
        "E Andre",
        "F Lingenfelser",
        "J Kim"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Exploiting modality-specific features for multi-modal manipulation detection and grounding",
      "authors": [
        "J Wang",
        "B Liu",
        "C Miao",
        "Z Zhao",
        "W Zhuang",
        "Q Chu",
        "N Yu"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Y Wang",
        "M Li",
        "S Liu",
        "M Li"
      ],
      "year": "2019",
      "venue": "Context-aware emotion recognition networks",
      "arxiv": "arXiv:1908.05913"
    }
  ]
}