{
  "paper_id": "2509.05617v1",
  "title": "From Joy To Fear: A Benchmark Of Emotion Estimation In Pop Song Lyrics",
  "published": "2025-09-06T06:28:28Z",
  "authors": [
    "Shay Dahary",
    "Avi Edana",
    "Alexander Apartsin",
    "Yehudit Aperstein"
  ],
  "keywords": [
    "Multi-Label Emotion Classification",
    "Large Language Models (LLMs)",
    "Annotated Song Lyrics",
    "Sentiment Analysis in Lyrics",
    "Affective Computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The emotional content of song lyrics plays a pivotal role in shaping listener experiences and influencing musical preferences. This paper investigates the task of multi-label emotional attribution of song lyrics by predicting six emotional intensity scores corresponding to six fundamental emotions. A manually labeled dataset is constructed using a mean opinion score (MOS) approach, which aggregates annotations from multiple human raters to ensure reliable ground-truth labels. Leveraging this dataset, we conduct a comprehensive evaluation of several publicly available large language models (LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model specifically for predicting multi-label emotion scores. Experimental results reveal the relative strengths and limitations of zero-shot and fine-tuned models in capturing the nuanced emotional content of lyrics. Our findings highlight the potential of LLMs for emotion recognition in creative texts, providing insights into model selection strategies for emotion-based music information retrieval applications. The labeled dataset is available at https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Music, as a universal form of human expression, evokes a rich spectrum of emotional experiences. While musical elements such as melody and rhythm contribute significantly to emotional perception, the lyrical content often carries the most explicit and interpretable emotional cues. Understanding and quantifying the emotional attribution of song lyrics is a critical task in fields such as music information retrieval, affective computing, and human-computer interaction. Applications of this research span personalized music recommendation systems, emotional playlist generation, mental health interventions, and cultural studies of music trends.\n\nDespite the importance of this task, automatically attributing emotions to lyrics remains a challenging problem. Lyrics often contain abstract language, metaphors, and cultural references that complicate the direct inference of emotions. Traditional sentiment analysis methods, typically designed for binary or ternary sentiment classification, fall short in capturing the multi-dimensional and overlapping emotional content present in lyrical texts. Moreover, the subjective nature of emotional perception necessitates robust labelling methodologies and models that can handle ambiguity and intensity variation across multiple emotional categories.\n\nIn this work, we address these challenges by constructing a high-quality, manually labeled dataset for lyric emotion attribution using the Mean Opinion Score (MOS) method. This approach aggregates multiple human annotations to derive reliable emotion intensity scores across six fundamental emotions: joy, sadness, anger, fear, surprise, and disgust. Using this dataset, we systematically evaluate the capabilities of several publicly available large language models (LLMs) under zero-shot settings. Additionally, we develop and fine-tune a BERT-based model tailored explicitly for multilabel emotion score prediction.\n\nOur research provides a comprehensive analysis of model performance across various learning scenarios, highlighting the trade-offs between zero-shot generalization and fine-tuned specialization. By quantifying model effectiveness in the complex task of emotional attribution, this study provides valuable insights into the deployment of language models for affective analysis of creative texts, supporting future advancements in emotionally aware AI systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Literature Review",
      "text": "Many modern systems frame emotion categorizations as a multi-label classification problem, recognizing that texts often evoke multiple concurrent emotions. A common approach adopts Ekman's six basic emotions-anger, disgust, fear, happiness (also known as joy), sadness, and surprise-as target labels  (Ekman, 1992)  The analysis of emotions in creative texts, including song lyrics and poetry, has garnered increasing attention. Mihalcea and Strapparava (2008) created an early lyrics dataset labeled with Ekman's emotion categories, demonstrating the complexity of affective analysis in artistic language. Edmonds and Sedoc (2021) expanded this work by introducing the Edmonds Dance dataset, using both Ekman's six emotions and Plutchik's eight-emotion model. They showed that fine-tuning on in-domain lyric data consistently outperforms transferring models trained on social media text. Song and Beck (2022) modelled emotion dynamics in lyrics by treating each song as a time series of sentences. They trained a sentence-level emotion predictor based on Ekman categories and refined predictions using a state-space expectation-maximization algorithm, achieving improved accuracy without requiring fully labeled songs. Addressing the challenge of data scarcity, Sakunkoo and Sakunkoo (2024) proposed cross-domain transfer learning, where a CNN is pre-trained on Reddit comments and then finetuned on a small dataset of lyrics. Their results demonstrated that cross-domain knowledge transfer can produce effective emotion models even with limited annotated lyric data.\n\nLarge pre-trained transformers, such as BERT and GPT models, are increasingly used for emotion recognition with minimal supervision.   2024 ) directly compared finetuned GPT-3 variants to DeBERTa v3 on standard emotion datasets, concluding that fine-tuned BERT-based models generally outperformed LLM prompting approaches. However, the performance gap narrowed with larger models, such as GPT-4. Current research suggests that while LLMs can achieve competitive results with effective prompting strategies, finetuned task-specific models still offer superior performance when labeled data is available.\n\nBuilding high-quality emotion datasets requires reconciling subjective differences in annotator judgments. The most common strategy for categorical emotion labels is majority voting, as seen in the EmotionLines dialogue corpus, which aggregated annotations from five raters per utterance (Hsu et al., 2018). However, majority voting can obscure individual variations in emotional perception. When emotions are assessed on a continuous scale, researchers often employ the Mean Opinion Score (MOS) method, which involves averaging the ratings of annotators to produce a final intensity score  (Chen et al., 2020) . Bagdon et al. (2024) argue that direct rating scales can suffer from inconsistency and propose Best-Worst Scaling (BWS) as a more reliable alternative for capturing the intensity of emotions. In speech emotion recognition, the MOS approach remains standard for evaluating perceived emotional strength  (Truong & van Leeuwen, 2007) . Recent studies also explore probabilistic label models and soft-label aggregation methods to capture better annotator uncertainty and disagreement (Sarumi et al., 2024). Overall, while majority voting and MOS remain dominant strategies, there is growing interest in more nuanced aggregation techniques that preserve the diversity of emotional interpretations across annotators.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Dataset Construction And Annotation",
      "text": "To establish a reliable ground truth for emotion attribution in song lyrics, we employed a manual annotation process based on the Mean Opinion Score (MOS) methodology. A diverse committee of human annotators, each with prior experience in affective text analysis or music interpretation, participated in the labelling process. Annotators were asked to read each song lyric and independently assign an intensity score ranging from 0 (not present) to 5 (highly present) for each of Ekman's six basic emotions: joy, sadness, anger, fear, surprise, and disgust. For each lyric and each emotion category, the individual scores were aggregated by computing the arithmetic mean across all annotators, resulting in continuous-valued emotional intensity scores. These MOS values effectively captured the subjective variability among annotators while providing a consistent numerical representation of emotional attribution. This approach also allowed the dataset to reflect the degree of emotional presence rather than forcing binary or categorical labels, which are often inadequate for representing the nuanced emotional content found in song lyrics.\n\nIn the resulting dataset, the MOS scores for all emotions are clustered near mid-range values as shown in Figure  1 , with the highest mean score corresponding to joy (0.95) and the lowest mean values corresponding to anger (0.40).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Figure 1: Distribution Of Scores For Each Emotion Type",
      "text": "There is also no significant correlation between music genre and emotion scores, as shown in Figure  2 , with the highest noticeable exception of anger emotion being prominent for rap songs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Model Evaluation",
      "text": "Using the constructed dataset, we evaluated both finetuned and zero-shot models for predicting multi-label emotion scores. 1. Fine-Tuned BERT-Based Models: We fine-tuned a pretrained BERT model for regression-based multi-label prediction, directly predicting the six continuous emotion scores for each lyric. The model architecture consisted of the standard BERT encoder followed by a fully connected regression head with six output nodes, one for each emotion. The loss function used was Mean Squared Error (MSE), which is appropriate for continuous-valued target variables. The model was trained and validated on the annotated dataset to optimize its predictive accuracy across all six emotions.\n\n2. Zero-Shot Pretrained Models: In addition to fine-tuned models, we investigated the zero-shot emotion classification capabilities of large language models without requiring further task-specific training using the Grok 3 model.\n\nFor zero-shot evaluation, each model was prompted using carefully crafted instructions to predict the emotional intensity scores directly from the raw lyrics. The models were asked to output numeric scores corresponding to the six target emotions. This experimental setup enabled us to evaluate the generalization capabilities of large pre-trained models and their ability to perform fine-grained emotional analysis without requiring specialized fine-tuning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Results",
      "text": "To evaluate model performance, we report both aggregated results across all emotions and detailed results for each emotion. Two main evaluation metrics were used: Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). Lower values indicate better predictive accuracy in estimating the emotional intensity scores.  Notably, the fine-tuned models demonstrate strong predictive accuracy for emotions such as surprise and fear, with MAE values as low as 0.10 and 0.13, respectively. By contrast, the zero-shot model exhibits higher error rates across all categories, particularly for joy and anger. These results confirm that while zero-shot LLMs provide a baseline capability for lyric emotion attribution, fine-tuned transformer-based models offer far superior performance, making them more suitable for music information retrieval and affective computing applications.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusions And Future Directions",
      "text": "In this study, we addressed the complex task of emotional attribution in song lyrics by constructing a highquality, manually labeled dataset using the Mean Opinion Score (MOS) method. This approach enabled us to capture the nuanced and subjective emotional content present in lyrics across Ekman's six basic emotions. Using this dataset, we conducted a comprehensive evaluation of both fine-tuned and zero-shot language models for predicting multi-label emotional intensity.\n\nOur results demonstrate that fine-tuning a BERTbased model significantly outperforms a large zero-shot model such as Grok 3. While zero-shot models offer a practical solution when labeled data is unavailable, their performance remains inferior to models explicitly trained for the task. Among the zero-shot models, DeepSeek-R1 demonstrated stronger generalization capabilities, indicating its enhanced alignment with affective reasoning tasks.\n\nFuture work will explore expanding the dataset to include a broader range of lyrical genres and languages, which may improve model generalization and robustness. Additionally, we plan to investigate the effectiveness of advanced prompting strategies, such as chain-of-thought reasoning and structured output prompts, to enhance zero-shot performance. Ultimately, we aim to investigate the application of larger and more specialized language models, as well as multimodal approaches that integrate both lyrics and audio features, to further advance the field of emotion recognition in music.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. References",
      "text": "",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , with the highest mean score corresponding to joy",
      "page": 3
    },
    {
      "caption": "Figure 1: Distribution of scores for each emotion type",
      "page": 3
    },
    {
      "caption": "Figure 2: , with the",
      "page": 3
    },
    {
      "caption": "Figure 2: Correlation between music genre and emotion score",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "Joy",
          "1": "2",
          "2": "1",
          "3": "1",
          "4": "1",
          "Mean \nScore \n(MOS)": "1.25"
        },
        {
          "Emotion": "Sadness",
          "1": "0",
          "2": "1",
          "3": "1",
          "4": "2",
          "Mean \nScore \n(MOS)": "1"
        },
        {
          "Emotion": "Anger",
          "1": "0",
          "2": "0",
          "3": "0",
          "4": "0",
          "Mean \nScore \n(MOS)": "0"
        },
        {
          "Emotion": "Fear",
          "1": "0",
          "2": "0",
          "3": "0",
          "4": "0",
          "Mean \nScore \n(MOS)": "0"
        },
        {
          "Emotion": "Surprise",
          "1": "1",
          "2": "1",
          "3": "2",
          "4": "2",
          "Mean \nScore \n(MOS)": "1.5"
        },
        {
          "Emotion": "Disgust",
          "1": "0",
          "2": "0",
          "3": "0",
          "4": "0",
          "Mean \nScore \n(MOS)": "0"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "Joy",
          "Zero-shot \nGrok \nMAE/RMSE": "0.41/0.55",
          "Fine-tuned \nRoBERTa \nMAE/RMSE": "0.17/0.36",
          "Fine-Tuned \nBERT \nMAE/RMSE": "0.18/0.37"
        },
        {
          "Emotion": "Sadness",
          "Zero-shot \nGrok \nMAE/RMSE": "0.42/0.54",
          "Fine-tuned \nRoBERTa \nMAE/RMSE": "0.17/0.37",
          "Fine-Tuned \nBERT \nMAE/RMSE": "0.17/0.37"
        },
        {
          "Emotion": "Anger",
          "Zero-shot \nGrok \nMAE/RMSE": "0.45/0.45",
          "Fine-tuned \nRoBERTa \nMAE/RMSE": "0.14/0.32",
          "Fine-Tuned \nBERT \nMAE/RMSE": "0.13/0.31"
        },
        {
          "Emotion": "Fear",
          "Zero-shot \nGrok \nMAE/RMSE": "0.35/0.47",
          "Fine-tuned \nRoBERTa \nMAE/RMSE": "0.13/0.33",
          "Fine-Tuned \nBERT \nMAE/RMSE": "0.14/0.32"
        },
        {
          "Emotion": "Surprise",
          "Zero-shot \nGrok \nMAE/RMSE": "0.31/0.48",
          "Fine-tuned \nRoBERTa \nMAE/RMSE": "0.10/0.26",
          "Fine-Tuned \nBERT \nMAE/RMSE": "0.10/0.27"
        },
        {
          "Emotion": "Disgust",
          "Zero-shot \nGrok \nMAE/RMSE": "0.45/057",
          "Fine-tuned \nRoBERTa \nMAE/RMSE": "0.16/0.35",
          "Fine-Tuned \nBERT \nMAE/RMSE": "0.16/0.35"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multi-label emotion classification in texts using transfer learning",
      "authors": [
        "I Ameer",
        "N Bölücü",
        "M Siddiqui",
        "B Can",
        "G Sidorov",
        "A Gelbukh"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "2",
      "title": "English prompts are better for NLI-based zero-shot emotion classification than target-language prompts",
      "authors": [
        "P Bareiß",
        "R Klinger",
        "J Barnes"
      ],
      "year": "2024",
      "venue": "WWW '24: Companion Proceedings of the ACM on Web Conference 2024"
    },
    {
      "citation_id": "3",
      "title": "Datastories at SemEval-2018 Task 1: Predicting emotion intensity in tweets with deep attentive RNN",
      "authors": [
        "C Baziotis",
        "N Pelekis",
        "C Doulkeridis"
      ],
      "year": "2018",
      "venue": "Proceedings of SemEval-2018"
    },
    {
      "citation_id": "4",
      "title": "Towards a generative approach for emotion detection and reasoning",
      "authors": [
        "A Bhaumik",
        "T Strzalkowski"
      ],
      "year": "2024",
      "venue": "Towards a generative approach for emotion detection and reasoning"
    },
    {
      "citation_id": "5",
      "title": "A comparative analysis of GPT-3 and BERT models for text-based emotion recognition: Performance, efficiency, and robustness",
      "authors": [
        "E Boitel",
        "A Mohasseb",
        "E Haig"
      ],
      "year": "2024",
      "venue": "Advances in Intelligent Systems and Computing, 1453"
    },
    {
      "citation_id": "6",
      "title": "Emotion representation learning: bridging lexical and distributional semantics",
      "authors": [
        "S Buechel",
        "U Hahn"
      ],
      "year": "2017",
      "venue": "Proceedings of NAACL-HLT 2017"
    },
    {
      "citation_id": "7",
      "title": "Recent advancement of emotion cognition in large language models",
      "authors": [
        "Y Chen",
        "Y Xiao"
      ],
      "year": "2024",
      "venue": "Recent advancement of emotion cognition in large language models"
    },
    {
      "citation_id": "8",
      "title": "GoEmotions: A dataset of fine-grained emotions",
      "authors": [
        "D Demszky",
        "D Movshovitz-Attias",
        "J Ko",
        "A Cowen",
        "G Nemade",
        "J Mcauley"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "9",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT"
    },
    {
      "citation_id": "10",
      "title": "Multi-emotion classification for song lyrics",
      "authors": [
        "D Edmonds",
        "J Sedoc"
      ],
      "year": "2021",
      "venue": "Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "11",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "12",
      "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion, and sarcasm",
      "authors": [
        "L Felbo",
        "A Mislove",
        "A Søgaard",
        "I Rahwan",
        "S Lehmann"
      ],
      "year": "2017",
      "venue": "Proceedings of the 11th International AAAI Conference on Web and Social Media"
    },
    {
      "citation_id": "13",
      "title": "Emotion classification in poetry text using deep neural network. Multimedia Tools and Applications",
      "authors": [
        "A Khattak",
        "M Asghar",
        "H Khalid",
        "H Ahmad"
      ],
      "year": "2022",
      "venue": "Emotion classification in poetry text using deep neural network. Multimedia Tools and Applications"
    },
    {
      "citation_id": "14",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of EMNLP 2014"
    },
    {
      "citation_id": "15",
      "title": "Large language models understand and can be enhanced by emotional stimuli",
      "authors": [
        "C Li",
        "J Wang",
        "Y Zhang",
        "K Zhu",
        "W Hou",
        "J Lian",
        "F Luo",
        "Q Yang",
        "X Xie"
      ],
      "year": "2023",
      "venue": "Large language models understand and can be enhanced by emotional stimuli"
    },
    {
      "citation_id": "16",
      "title": "Emotion classification for short texts: an improved multi-label method",
      "authors": [
        "X Liu",
        "T Shi",
        "G Zhou",
        "M Liu",
        "Z Yin",
        "L Yin",
        "W Zheng"
      ],
      "year": "2023",
      "venue": "Humanities and Social Sciences Communications"
    },
    {
      "citation_id": "17",
      "title": "",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen"
      ],
      "venue": ""
    },
    {
      "citation_id": "18",
      "title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "authors": [
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "19",
      "title": "EmoLLMs: A series of emotional large language models and annotation tools for comprehensive affective analysis",
      "authors": [
        "Z Liu",
        "K Yang",
        "Q Xie",
        "T Zhang",
        "S Ananiadou"
      ],
      "year": "2024",
      "venue": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "20",
      "title": "Lyrics, music, and emotions",
      "authors": [
        "R Mihalcea",
        "C Strapparava"
      ],
      "year": "2012",
      "venue": "Proceedings of EMNLP 2012"
    },
    {
      "citation_id": "21",
      "title": "SemEval-2018 Task 1: Affect in Tweets",
      "authors": [
        "S Mohammad",
        "F Bravo-Marquez",
        "M Salameh",
        "S Kiritchenko"
      ],
      "year": "2018",
      "venue": "Proceedings of the 12th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "22",
      "title": "Improving emotion recognition with LLMs: Adaptation of large language models for emotion recognition",
      "authors": [
        "L Peng",
        "Z Zhang",
        "T Pang",
        "J Han",
        "Y Xiao"
      ],
      "year": "2023",
      "venue": "Improving emotion recognition with LLMs: Adaptation of large language models for emotion recognition"
    },
    {
      "citation_id": "23",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        ". Liu"
      ],
      "year": "2020",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "24",
      "title": "Modeling emotion dynamics in song lyrics with state space models",
      "authors": [
        "Y Song",
        "D Beck"
      ],
      "year": "2023",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition in poetry using an ensemble of classifiers",
      "authors": [
        "P Sreeja",
        "G Mahalakshmi"
      ],
      "year": "2019",
      "venue": "Next Generation Computing Technologies on Computational Intelligence"
    },
    {
      "citation_id": "26",
      "title": "A parallel corpus of music and lyrics annotated with emotions",
      "authors": [
        "C Strapparava",
        "R Mihalcea",
        "A Battocchi"
      ],
      "year": "2012",
      "venue": "Proceedings of LREC 2012"
    },
    {
      "citation_id": "27",
      "title": "GPT-4 emulates averagehuman emotional cognition from a third-person perspective",
      "authors": [
        "A Tak",
        "J Gratch"
      ],
      "year": "2024",
      "venue": "GPT-4 emulates averagehuman emotional cognition from a third-person perspective"
    },
    {
      "citation_id": "28",
      "title": "Exploring large language models' emotion detection abilities: Use cases from the Middle East",
      "authors": [
        "R Venkatakrishnan",
        "M Goodarzi"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 IEEE Conference on Artificial Intelligence"
    },
    {
      "citation_id": "29",
      "title": "Bias in emotion recognition with ChatGPT",
      "authors": [
        "N Wake",
        "A Kanehira",
        "K Sasabuchi",
        "J Takamatsu",
        "K Ikeuchi"
      ],
      "year": "2023",
      "venue": "Bias in emotion recognition with ChatGPT"
    },
    {
      "citation_id": "30",
      "title": "XLNet: A Generalized Autoregressive Pretraining Approach for Language Understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    }
  ]
}