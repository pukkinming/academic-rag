{
  "paper_id": "2208.11346v2",
  "title": "Icanet: A Method Of Short Video Emotion Recognition Driven By Multimodal Data",
  "published": "2022-08-24T07:54:03Z",
  "authors": [
    "Xuecheng Wu",
    "Mengmeng Tian",
    "Lanhang Zhai"
  ],
  "keywords": [
    "Multimodal Deep Learning",
    "Emotion Recognition",
    "Attention Mechanism",
    "Human-Computer Interaction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With the fast development of artificial intelligence and short videos, emotion recognition in short videos has become one of the most important research topics in human-computer interaction. At present, most emotion recognition methods still stay in a single modality. However, in daily life, human beings will usually disguise their real emotions, which leads to the problem that the accuracy of single modal emotion recognition is relatively terrible. Moreover, it is not easy to distinguish similar emotions. Therefore, we propose a new approach denoted ICANet to achieve multimodal short video emotion recognition by employing three different modalities of audio, video and optical flow, making up for the lack of a single modality and then improving the accuracy of emotion recognition in short videos. ICANet has a better accuracy of 80.77% on the IEMOCAP benchmark.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "People's life always accompanies emotion, which is an essential part of the dynamic mechanism in people's psychological activities. The \"pure cognition\" research methods, separated from the emotional factors, can not entirely investigate and simulate human behaviors. Therefore, the fast development of emotion recognition based on neural networks is very significant. At present, emotion recognition methods based on the non-physiological signals mainly include the recognition of facial expression and voice intonation.\n\nHowever, the faeture information extracted by the single modality emotion recognition method is usually one-sided, resulting in the low accuracy of the recognition. In the contrary, the multimodal fusion method combined with various feature information can more accurately capture and identify human emotions. This paper propose a new method, which combines the three modalities of audio, video and optical flow for multimodal emotion recognition in short videos, making up for the lack of a single modality and improving the accuracy of emotion recognition in short videos. In this paper, we select the videos, optical flow and LFCC spectrograms as inputs.\n\nThe three feature extraction networks, I3D (RGB/FLOW) and CA-VGG16, are deployed to model the three modality feature information, respectively, and then the prediction scores corresponding to the three modalities are calculated.  The decision level feature fusion is carried out according to a certain modality weight ratio. Finally, the specific feature fusion results are transferred to the SoftMax classifier, and then we get the final emotion recognition results in short videos.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Methodology",
      "text": "The overall structure of ICANet is divided into three parts: data preprocessing module, multimodal feature extraction module and fusion classification module. The overall network structure of ICANet is shown as Fig.  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Data Preprocessing Module",
      "text": "1) LFCC Spectrogram: In this paper, the audio signal is preprocessed by generating the LFCC spectrograms. LFCC spectrogram is a three-dimensional spectrum representing the voice frequency graph changing with time. Its abscissa is time, its ordinate is frequency, and the coordinate point value is voice data energy. Since the spectrogram deploys a twodimensional plane to express three-dimensional information, the energy value is expressed by the depth of the colors. Firstly, this paper preprocesses the Wav files of the audio section of the IEMOCAP dataset, using the scipy voice processing tool and python speech features library to read the speech information and extract the LFCC speech features, and finally converts the ordinary Wav speech signals into LFCC spectrograms.\n\n2) Optical Flow Extraction: Optical Flow is apparent motion mode of two consecutive interframe images caused by the motion of an object or a camera. It is a 2D vector field, where each vector is a displacement vector, representing the motion of the point from frame X to frame X + 1. In this paper, we deploy the interface of the optical flow estimation algorithm provided in the visual algorithm library OpenCV, including the sparse flow estimation algorithm cv2.Calcopticalflowpyrlk() and dense flow estimation algorithm cv2.calcOpticalFlowFarneback(). Specifically, we deploy the sparse flow estimation algorithm, which is the Lucas Kanade algorithm  [1] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Multimodal Feature Extraction Module 1) Rgb And Flow Feature Extraction Module:",
      "text": "In this paper, the I3D  [2]  is deployed to extract the specific features of the characters in the videos, and the model training method of separating the video stream and the optical flow stream is adopted. I3D (two-stream inflated 3D CNN)  [2]  is a video action recognition model proposed by Google DeepMind in 2017, using Inception-V1 as the backbone. Its specific network structure is shown in Fig.  2 . In Inception-V1, the stride of the first convolution transformation layer is 2. In addition to the parallel maximum pooling layer, there are four maximum pooling layers with a stride of 2 × 2 and 7 × 7. The structure of the initial space sub module \"Inc.\" is shown as Fig.  3 .\n\nThis paper adopts the form of separate training. One I3D network training takes RGB (video) stream as input, and the other takes FLOW (optical flow) stream as input, carrying optimized and smooth stream information. In this paper, the two I3D networks are first trained, and the pre-trained weights are then loaded during model validation. The models are trained on the IEMOCAP multimodal dataset, and all the video frames have been adjusted to the appropriate size, number and channels before input into the network. Seventy-nine video frames are extracted at uniform time intervals, and the entire  video content is captured. Finally, the pre-trained weights of RGB and FLOW modalities are obtained.\n\n2) Audio Feature Extraction Module: This paper first converts audio files into the LFCC spectrograms, and then evolve it into an image classification task. We deploy the independently improved VGG16 denoted as CA-VGG16. We mainly introduce the Coordinate Attention Module (CA module) to improve the performance of VGG16, adding the CA Module after the first, fourth and fifth convolution transformation blocks of VGG16 to improve the model performance. The overall network structure of CA-VGG16 is shown in Fig.  4 .\n\nThe Coordinate Attention is a novel attention mechanism proposed by Hou et al.  [3] . The Coordinate Attention innovatively embeds the precise positional information into the inter-channel attention and then improves the capability of the backbone network to capture the detailed object structures, enrich the semantic information of shallow feature maps and obtain the precise positional information of larger areas.\n\nVGG16  [4]  mainly deploys the small convolution filters to build a new convolutional neural network, including the convolution transformation layers, the pooling layers, and the full connection layers.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Convolution Transformation Layer",
      "text": "In the VGG16, it proposes to utilize two 3 × 3 convolution kernels to replace one specific 5 × 5 convolution kernel, and deploy three 3 × 3 convolution kernels to replace one 7 × 7 convolution kernel. In this approach, we can significantly reduce the parameters of whole network, increase the depth and improve the feature expression ability of the overall network to a certain extent. The calculation formula of the convolution transformation layer are as follows:\n\nPooling Layer VGG16 deploys the maximum pooling layers, followed by a maximum pooling layer after several convolution transformation layers to form a block for compressing the shape of feature maps. The general formula for the pooling layer are shown in the following:\n\nFrom the Eq. 1 to the Eq. 4, where W and H represent the width and height, respectively. P , K, and S represent the filter size, the convolution kernel size, and the stride, respectively.\n\nFull Connection Layer In the CNN, the full connection layer is responsible for the task of \"Classifier\", which classifies the feature tensors. VGG16 deploys three full connection layers at the end, and the last full connection layer uses the SoftMax layer for classification.\n\nIn this paper, we add the CA modules after the first, fourth and fifth blocks of VGG16, denoted as CA-VGG16-3. At the same time, we also attempt to add the CA modules after each block, denoted as CA-VGG16-5. However, the ablation experimental results show that CA-VGG16-3 performs better than CA-VGG-5. Therefore, we select CA-VGG16-3 as the audio feature extraction network, denoted as CA-VGG16. The detailed network structure of CA-VGG16 is shown in Fig.  4 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Fusion Classification Module",
      "text": "The ICANet proposed in this paper adopts the decision level feature fusion module to combine the highest level of pretrained weights and carry out feature fusion on the prediction score, that is, to take the weights of three modalities, carry out weighted average fusion on them, and then output the final prediction results from a SoftMax classifier.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experiments A. Iemocap Dataset Introduction",
      "text": "In detail, the IEMOCAP  [5]  multimodal emotion recognition dataset is deployed to conduct model training and validation. IEMOCAP records the three selected scripts and fictional scene dialogues designed to trigger specific emotions by ten actors. We filter the unbalanced data in the original dataset and finally form an emotion recognition dataset composed of four emotion categories: happiness, sadness, neutrality and anger, with a total number of 5531. The distribution for each category in the IEMOCAP dataset is shown as Tab. I.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Evaluation Indicator",
      "text": "In the task of multimodal emotion recognition in short videos, we mainly deploy accuracy (ACC) as the evaluation indicator. The formula of ACC is shown as Eq. 5 below:\n\nIn the abovementioned formula, T P represents a positive sample detected as correct; T N represents a negative sample detected as wrong; F P represents a negative sample detected as a positive sample; F N represents the positive sample detected as a negative sample  [6] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Ablation Studies",
      "text": "We conduct four sets of ablation experiments for the audio branch to determine the effectiveness of the CA modules in improving the model performance. The models of these four ablation experiments are original VGG16, SE-VGG16 based on SE Attention, CBAM-VGG16 based on CBAM Attention, CA-VGG16-3 and CA-VGG16-5, respectively. The original VGG16 is set as the control group to determine the effectiveness of the proposed improvement components. The specific ablation results are shown in Fig.  5 .\n\nWe can clearly observe that the CA-VGG16-3 achieves the highest accuracy of 58.96%, which is a practical improvement on the original VGG16. We conclude the reason is that adding the CA modules strengthens the feature extraction capability of the model for LFCC spectrograms and improves the processing of specific features. We have demonstrated that the CA-VGG16 with three CA modules is more feasible and practical in extracting the audio features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Comparison With Other Sota Methods",
      "text": "To explore the best recognition effect of ICANet, this paper gives different modalities different weight ratios for fusion to obtain the optimal weight ratio. When the weight ratios of RGB, FLOW, and Audio are 1:1:1, 3:2:5, 4:3:3, 5:3:2, respectively, the ACC are respectively 77.50%, 78.85%, 79.04%, 80.77%, 67.66%. We can observe that blindly increasing the weight of RGB stream will not achieve better results. On the contrary, moderately increasing the weight of FLOW stream or Audio stream will also improve the overall model performance to a certain extent. The reason is that in this approach, the overall network can not achieve good information complementarity and the better effect of multimodal fusion. According to the experimental results, when RGB:FLOW:Audio = 4:2:4, we can get the best model performance. Therefore, we selects 4:2:4 as the optimal weight ratio of three different modalities for later decision level feature fusion.\n\nTo further verify the effectiveness of the emotion recognition method in short videos driven by multimodal data proposed in this paper, we compare ICANet with the current mainstream emotion recognition methods. The results of the performance comparison are shown in Tab. II. It can be seen from the abovementioned table that the accuracy of the new emotion recognition method ICANet based on RGB, FLOW and Audio modalities is significantly higher than the above five mainstream emotion recognition baseline methods. Furthermore, the accuracy of ICANet proposed in this paper reaches 80.77%, exceeding the existing CNN-based state-ofthe-art methods by 15.89%.\n\nThe analysis shows that the main reason for the model performance improvement of ICANet is that it integrates the feature information of RGB, FLOW and Audio modalities, realizes the complementarity of feature information, makes up for the deficiency of a single modality, and deploys three sub feature extraction networks with higher accuracy to extract richer emotional feature information. To sum up, the validity of the emotion recognition method in short videos driven by the typical features of multimodal data is verified.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusions",
      "text": "This paper propose a new emotion recognition method in short videos denoted as ICANet driven by the typical features of multimodal data. The experimental results on the IEMOCAP multimodal emotion recognition dataset show that",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall illustration of ICANet. It consists of the Data Prepro-",
      "page": 1
    },
    {
      "caption": "Figure 1: A. Data Preprocessing Module",
      "page": 2
    },
    {
      "caption": "Figure 2: In Inception-V1, the stride of",
      "page": 2
    },
    {
      "caption": "Figure 3: This paper adopts the form of separate training. One I3D",
      "page": 2
    },
    {
      "caption": "Figure 2: The overall network structure of Inﬂated Inception-V1. Here,",
      "page": 2
    },
    {
      "caption": "Figure 3: The overall illustration of initial space sub module “Inc.”. The strides",
      "page": 2
    },
    {
      "caption": "Figure 4: The Coordinate Attention is a novel attention mechanism",
      "page": 2
    },
    {
      "caption": "Figure 4: C. Fusion Classiﬁcation Module",
      "page": 3
    },
    {
      "caption": "Figure 4: The overall network structure of CA-VGG16. Speciﬁcally, there are",
      "page": 3
    },
    {
      "caption": "Figure 5: We can clearly observe that the CA-VGG16-3 achieves the",
      "page": 4
    },
    {
      "caption": "Figure 5: The ablation experiments of the speciﬁc audio branch of ICANet in",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "Abstract—With the fast development of artiﬁcial\nintelligence",
          "zhailhang@163.com": "(cid:68)(cid:437)(cid:367)(cid:410)(cid:349)(cid:373)(cid:381)(cid:282)(cid:258)(cid:367)(cid:3)(cid:38)(cid:286)(cid:258)(cid:410)(cid:437)(cid:396)(cid:286)\n(cid:3)(cid:3)(cid:3)(cid:24)(cid:258)(cid:410)(cid:258)(cid:3)(cid:87)(cid:396)(cid:286)(cid:393)(cid:396)(cid:381)(cid:272)(cid:286)(cid:400)(cid:400)(cid:349)(cid:374)(cid:336)(cid:3)(cid:68)(cid:381)(cid:282)(cid:437)(cid:367)(cid:286)"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:38)(cid:437)(cid:400)(cid:349)(cid:381)(cid:374)(cid:3)(cid:18)(cid:367)(cid:258)(cid:400)(cid:400)(cid:349)(cid:296)(cid:349)(cid:272)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:68)(cid:381)(cid:282)(cid:437)(cid:367)(cid:286)\n(cid:3)(cid:28)(cid:454)(cid:410)(cid:396)(cid:258)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:68)(cid:381)(cid:282)(cid:437)(cid:367)(cid:286)"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "and short videos, emotion recognition in short videos has become",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "one of\nthe most\nimportant research topics\nin human-computer",
          "zhailhang@163.com": "(cid:4)(cid:437)(cid:282)(cid:349)(cid:381)(cid:3)"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "(cid:100)(cid:396)(cid:258)(cid:374)(cid:400)(cid:296)(cid:381)(cid:396)(cid:373)\n(cid:4)(cid:296)(cid:296)(cid:286)(cid:396)(cid:286)(cid:374)(cid:410)\n(cid:62)(cid:38)(cid:18)(cid:18)"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "interaction. At present, most\nemotion recognition methods\nstill",
          "zhailhang@163.com": "(cid:38)(cid:396)(cid:286)(cid:395)(cid:437)(cid:286)(cid:374)(cid:272)(cid:455)(cid:3)\n(cid:18)(cid:4)(cid:882)(cid:115)(cid:39)(cid:39)(cid:1005)(cid:1010)\n(cid:94)(cid:393)(cid:286)(cid:272)(cid:410)(cid:396)(cid:381)(cid:336)(cid:396)(cid:258)(cid:373)"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "(cid:116)(cid:258)(cid:448)(cid:3)(cid:38)(cid:349)(cid:367)(cid:286)"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "stay in a single modality. However,\nin daily life, human beings will",
          "zhailhang@163.com": "(cid:24)(cid:286)(cid:272)(cid:349)(cid:400)(cid:349)(cid:381)(cid:374)"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "(cid:28)(cid:454)(cid:410)(cid:396)(cid:258)(cid:272)(cid:410)\n(cid:28)(cid:373)(cid:381)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "usually disguise their real emotions, which leads to the problem",
          "zhailhang@163.com": "(cid:4)(cid:296)(cid:296)(cid:286)(cid:396)(cid:286)(cid:374)(cid:410)\n(cid:94)(cid:381)(cid:296)(cid:410)(cid:68)(cid:258)(cid:454)\n(cid:3)(cid:3)(cid:62)(cid:286)(cid:448)(cid:286)(cid:367)"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "(cid:38)(cid:367)(cid:381)(cid:449)(cid:3)(cid:38)(cid:349)(cid:367)(cid:286)\n(cid:47)(cid:1007)(cid:24)(cid:3)(cid:38)(cid:62)(cid:75)(cid:116)\n(cid:115)(cid:349)(cid:282)(cid:286)(cid:381)(cid:3)(cid:38)(cid:349)(cid:367)(cid:286)\n(cid:90)(cid:286)(cid:272)(cid:381)(cid:336)(cid:374)(cid:349)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "(cid:18)(cid:367)(cid:258)(cid:400)(cid:400)(cid:349)(cid:296)(cid:349)(cid:286)(cid:396)\n(cid:3)(cid:38)(cid:286)(cid:258)(cid:410)(cid:437)(cid:396)(cid:286)"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "that the accuracy of single modal emotion recognition is relatively",
          "zhailhang@163.com": "(cid:90)(cid:286)(cid:400)(cid:437)(cid:367)(cid:410)(cid:400)\n(cid:3)(cid:38)(cid:437)(cid:400)(cid:349)(cid:381)(cid:374)"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "terrible. Moreover,\nit\nis not easy to distinguish similar emotions.",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "Therefore, we\npropose\na\nnew approach\ndenoted\nICANet\nto",
          "zhailhang@163.com": "(cid:4)(cid:296)(cid:296)(cid:286)(cid:396)(cid:286)(cid:374)(cid:410)\n(cid:47)(cid:1007)(cid:24)(cid:3)(cid:90)(cid:39)(cid:17)"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "achieve multimodal short video emotion recognition by employing",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "three different modalities of audio, video and optical ﬂow, making",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "up for\nthe\nlack of\na\nsingle modality\nand then improving\nthe",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "Fig. 1.\nThe overall\nillustration of\nICANet.\nIt consists of\nthe Data Prepro-"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "accuracy of emotion recognition in short videos.\nICANet has a",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "cessing Module,\nthe Multimodal Feature Extraction Module, and the Fusion"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "better accuracy of 80.77% on the IEMOCAP benchmark.",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "Classiﬁcation Module. Speciﬁcally, we fuse the three different\nfeature tensors"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "Index Terms—Multimodal Deep Learning, Emotion Recogni-",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "of\nfeature extraction networks in the decision level\nfeature fusion module."
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "tion, Attention Mechanism, Human-Computer Interaction.",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "I.\nINTRODUCTION",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "The decision level\nfeature fusion is carried out according to"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "a\ncertain modality weight\nratio. Finally,\nthe speciﬁc\nfeature"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "People’s\nlife\nalways\naccompanies\nemotion, which\nis\nan",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "essential\npart\nof\nthe\ndynamic mechanism in\npeople’s\npsy-",
          "zhailhang@163.com": "fusion results are transferred to the SoftMax classiﬁer, and then"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "we get\nthe ﬁnal emotion recognition results in short videos."
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "chological activities. The “pure cognition” research methods,",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "separated\nfrom the\nemotional\nfactors,\ncan\nnot\nentirely\nin-",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "II. RELATED WORK"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "vestigate and simulate human behaviors. Therefore,\nthe\nfast",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "development of emotion recognition based on neural networks",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "Currently,\nthe\nresearch\non\nemotion\nrecognition\nin\nshort"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "is very signiﬁcant. At present, emotion recognition methods",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "videos\nis mainly\ndivided\ninto\nphysiological\nsignals\nbased"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "based\non\nthe\nnon-physiological\nsignals mainly\ninclude\nthe",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "emotion recognition methods\nand non-physiological\nsignals"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "recognition of\nfacial expression and voice intonation.",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "based\nemotion\nrecognition methods. The\nemotion\nrecogni-"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "However,\nthe\nfaeture\ninformation extracted by the\nsingle",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "tion methods based on physiological\nsignals mainly deploy"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "modality emotion\nrecognition method is\nusually\none-sided,",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "physiological\nsignals\nsuch as EEG, EMG, and ECG to pre-"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "resulting\nin\nthe\nlow accuracy\nof\nthe\nrecognition.\nIn\nthe",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "dict. Although physiological signals can not be camouﬂaged"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "contrary, the multimodal fusion method combined with various",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "and have the capability of getting more objective results,\nit"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "feature information can more accurately capture and identify",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "is\nrelatively\ndifﬁcult\nto\ncollect\nthese\nphysiological\nsignals."
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "human emotions. This paper propose a new method, which",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "Furthermore,\nit\nlacks\nreasonable evaluation standards, which"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "combines the three modalities of audio, video and optical ﬂow",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "are unsuitable for practical application. Emotion recognition"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "for multimodal emotion recognition in short videos, making up",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "based\non\nnon-physiological\nsignals mainly\nemploys\nnon-"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "for the lack of a single modality and improving the accuracy of",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "physiological\nsignals\nsuch\nas\nfacial\nexpression,\nvoice,\nand"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "emotion recognition in short videos.\nIn this paper, we select",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "gestures to predict. These signals are relatively easy to collect"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "the videos, optical ﬂow and LFCC spectrograms as\ninputs.",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "and do not require much data preprocessing. They can utilize"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "The\nthree\nfeature\nextraction\nnetworks,\nI3D (RGB/FLOW)",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "the existing deep learning technologies to predict quickly. The"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "and CA-VGG16,\nare\ndeployed\nto model\nthe\nthree modal-",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "basic\nidea of multimodal\nemotion recognition is\nto predict"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "ity feature information,\nrespectively, and then the prediction",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "the emotion category expressed by the detected objects based"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "scores\ncorresponding to the\nthree modalities are\ncalculated.",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "on\nthe\ninformation\nof\ntext,\nspeech,\nfacial\nexpression\nand"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "other multimodal inputs. The multimodal emotion recognition"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "Xuecheng Wu, Mengmeng Tian,\nand Lanhang Zhai\nare\nall with school",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "",
          "zhailhang@163.com": "method\nhas\nhigher\naccuracy than\nprevious\nsingle modality"
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "(Corresponding\nof Cyber Science\nand Engineering, Zhengzhou University.",
          "zhailhang@163.com": ""
        },
        {
          "tmm@stu.zzu.edu.cn\nwuxc@stu.zzu.edu.cn": "authors: Xuecheng Wu.)",
          "zhailhang@163.com": "emotion recognition methods."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:3)(cid:1011)(cid:853)(cid:1005)(cid:1005)(cid:853)(cid:1005)(cid:1005)\n(cid:1005)(cid:1005)(cid:853)(cid:1006)(cid:1011)(cid:853)(cid:1006)(cid:1011)"
        },
        {
          "III. METHODOLOGY": "The overall structure of ICANet\nis divided into three parts:",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:1005)(cid:104)(cid:1007)(cid:104)(cid:1007)\n(cid:1011)(cid:104)(cid:1011)(cid:104)(cid:1011)\n(cid:1005)(cid:104)(cid:1007)(cid:104)(cid:1007)"
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:1007)(cid:104)(cid:1007)(cid:104)(cid:1007)\n(cid:1005)(cid:104)(cid:1005)(cid:104)(cid:1005)"
        },
        {
          "III. METHODOLOGY": "data\npreprocessing module, multimodal\nfeature\nextraction",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:115)(cid:349)(cid:282)(cid:286)(cid:381)\n(cid:68)(cid:258)(cid:454)(cid:882)(cid:87)(cid:381)(cid:381)(cid:367)\n(cid:18)(cid:381)(cid:374)(cid:448)\n(cid:68)(cid:258)(cid:454)(cid:882)(cid:87)(cid:381)(cid:381)(cid:367)\n(cid:47)(cid:374)(cid:272)(cid:856)\n(cid:18)(cid:381)(cid:374)(cid:448)\n(cid:18)(cid:381)(cid:374)(cid:448)"
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:400)(cid:410)(cid:396)(cid:349)(cid:282)(cid:286)(cid:3)(cid:1005)(cid:853)(cid:1006)(cid:853)(cid:1006)\n(cid:400)(cid:410)(cid:396)(cid:349)(cid:282)(cid:286)(cid:3)(cid:1006)\n(cid:400)(cid:410)(cid:396)(cid:349)(cid:282)(cid:286)(cid:3)(cid:1005)(cid:853)(cid:1006)(cid:853)(cid:1006)"
        },
        {
          "III. METHODOLOGY": "module and fusion classiﬁcation module. The overall network",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "structure of\nICANet\nis shown as Fig. 1.",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)"
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:1006)(cid:1007)(cid:853)(cid:1011)(cid:1009)(cid:853)(cid:1011)(cid:1009)"
        },
        {
          "III. METHODOLOGY": "A. Data Preprocessing Module",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:1007)(cid:104)(cid:1007)(cid:104)(cid:1007)"
        },
        {
          "III. METHODOLOGY": "1) LFCC Spectrogram:\nIn this paper,\nthe audio signal\nis",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:68)(cid:258)(cid:454)(cid:882)(cid:87)(cid:381)(cid:381)(cid:367)\n(cid:47)(cid:374)(cid:272)(cid:856)\n(cid:47)(cid:374)(cid:272)(cid:856)\n(cid:47)(cid:374)(cid:272)(cid:856)\n(cid:47)(cid:374)(cid:272)(cid:856)\n(cid:47)(cid:374)(cid:272)(cid:856)"
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:400)(cid:410)(cid:396)(cid:349)(cid:282)(cid:286)(cid:3)(cid:1006)"
        },
        {
          "III. METHODOLOGY": "preprocessed by generating the LFCC spectrograms. LFCC",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "spectrogram is a three-dimensional spectrum representing the",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:3)(cid:3)(cid:3)(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:3)(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)"
        },
        {
          "III. METHODOLOGY": "voice\nfrequency\ngraph\nchanging with\ntime.\nIts\nabscissa\nis",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:3)(cid:3)(cid:1013)(cid:1013)(cid:853)(cid:1009)(cid:1007)(cid:1013)(cid:853)(cid:1009)(cid:1007)(cid:1013)\n(cid:1009)(cid:1013)(cid:853)(cid:1006)(cid:1005)(cid:1013)(cid:853)(cid:1006)(cid:1005)(cid:1013)"
        },
        {
          "III. METHODOLOGY": "time,\nits ordinate is frequency, and the coordinate point value",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:1006)(cid:104)(cid:1006)(cid:104)(cid:1006)"
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:1005)(cid:104)(cid:1005)(cid:104)(cid:1005)\n(cid:1006)(cid:104)(cid:1011)(cid:104)(cid:1011)"
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:68)(cid:258)(cid:454)(cid:882)(cid:87)(cid:381)(cid:381)(cid:367)\n(cid:47)(cid:374)(cid:272)(cid:856)\n(cid:47)(cid:374)(cid:272)(cid:856)\n(cid:47)(cid:374)(cid:272)(cid:856)\n(cid:87)(cid:396)(cid:286)(cid:282)(cid:349)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)(cid:400)"
        },
        {
          "III. METHODOLOGY": "is voice data\nenergy. Since\nthe\nspectrogram deploys a\ntwo-",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:18)(cid:381)(cid:374)(cid:448)\n(cid:4)(cid:448)(cid:336)(cid:882)(cid:87)(cid:381)(cid:381)(cid:367)\n(cid:400)(cid:410)(cid:396)(cid:349)(cid:282)(cid:286)(cid:3)(cid:1006)"
        },
        {
          "III. METHODOLOGY": "dimensional plane to express\nthree-dimensional\ninformation,",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "the energy value is expressed by the depth of the colors. Firstly,",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "Fig.\n2.\nThe\noverall\nnetwork\nstructure\nof\nInﬂated\nInception-V1. Here,"
        },
        {
          "III. METHODOLOGY": "this paper preprocesses the Wav ﬁles of the audio section of the",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "“Rec.Fields” represents\nthe receptive ﬁelds for speciﬁc feature tensors."
        },
        {
          "III. METHODOLOGY": "IEMOCAP dataset, using the scipy voice processing tool and",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "python speech features library to read the speech information",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:69)(cid:286)(cid:454)(cid:410)(cid:3)(cid:62)(cid:258)(cid:455)(cid:286)(cid:396)"
        },
        {
          "III. METHODOLOGY": "and extract the LFCC speech features, and ﬁnally converts the",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "ordinary Wav speech signals into LFCC spectrograms.",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "2) Optical Flow Extraction: Optical Flow is\nthe apparent",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:18)(cid:381)(cid:374)(cid:272)(cid:258)(cid:410)(cid:286)(cid:374)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)"
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:47)(cid:374)(cid:272)(cid:856)"
        },
        {
          "III. METHODOLOGY": "motion mode of\ntwo consecutive\ninterframe\nimages\ncaused",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "by the motion of\nan object or\na\ncamera.\nIt\nis\na 2D vector",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:1007)(cid:104)(cid:1007)(cid:104)(cid:1007)\n(cid:1005)(cid:104)(cid:1005)(cid:104)(cid:1005)\n(cid:1007)(cid:104)(cid:1007)(cid:104)(cid:1007)\n(cid:1005)(cid:104)(cid:1005)(cid:104)(cid:1005)"
        },
        {
          "III. METHODOLOGY": "ﬁeld, where\neach\nvector\nis\na\ndisplacement\nvector,\nrepre-",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:18)(cid:381)(cid:374)(cid:448)\n(cid:18)(cid:381)(cid:374)(cid:448)\n(cid:18)(cid:381)(cid:374)(cid:448)\n(cid:18)(cid:381)(cid:374)(cid:448)"
        },
        {
          "III. METHODOLOGY": "senting\nthe motion\nof\nthe\npoint\nframe\nfrom frame X to",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "In this paper, we deploy the interface of\nthe optical\nX + 1.",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:1005)(cid:104)(cid:1005)(cid:104)(cid:1005)\n(cid:1005)(cid:104)(cid:1005)(cid:104)(cid:1005)\n(cid:1007)(cid:104)(cid:1007)(cid:104)(cid:1007)"
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:18)(cid:381)(cid:374)(cid:448)\n(cid:18)(cid:381)(cid:374)(cid:448)\n(cid:68)(cid:258)(cid:454)(cid:882)(cid:87)(cid:381)(cid:381)(cid:367)"
        },
        {
          "III. METHODOLOGY": "ﬂow estimation algorithm provided in\nthe\nvisual\nalgorithm",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "library OpenCV,\nincluding the\nsparse ﬂow estimation algo-",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "rithm cv2.Calcopticalﬂowpyrlk()\nand\ndense ﬂow estimation",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "(cid:87)(cid:396)(cid:286)(cid:448)(cid:349)(cid:381)(cid:437)(cid:400)(cid:3)(cid:62)(cid:258)(cid:455)(cid:286)(cid:396)"
        },
        {
          "III. METHODOLOGY": "algorithm cv2.calcOpticalFlowFarneback().\nSpeciﬁcally, we",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "deploy\nthe\nsparse ﬂow estimation\nalgorithm, which\nis\nthe",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "Fig. 3.\nThe overall\nillustration of initial space sub module “Inc.”. The strides"
        },
        {
          "III. METHODOLOGY": "Lucas Kanade algorithm [1].",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "of convolution and pooling operators are 1, which are not speciﬁced."
        },
        {
          "III. METHODOLOGY": "B. Multimodal Feature Extraction Module",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": ""
        },
        {
          "III. METHODOLOGY": "1) RGB and Flow Feature Extraction Module:\nIn this paper,",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "video content\nis captured. Finally,\nthe pre-trained weights of"
        },
        {
          "III. METHODOLOGY": "the I3D [2]\nis deployed to extract\nthe speciﬁc features of\nthe",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "RGB and FLOW modalities are obtained."
        },
        {
          "III. METHODOLOGY": "characters\nin the videos,\nand the model\ntraining method of",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "2) Audio Feature Extraction Module: This paper ﬁrst con-"
        },
        {
          "III. METHODOLOGY": "separating the video stream and the optical ﬂow stream is",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "verts audio ﬁles into the LFCC spectrograms, and then evolve"
        },
        {
          "III. METHODOLOGY": "adopted.\nI3D (two-stream inﬂated 3D CNN)\n[2]\nis\na video",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "it\ninto an image classiﬁcation task. We deploy the indepen-"
        },
        {
          "III. METHODOLOGY": "action recognition model proposed by Google DeepMind in",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "dently improved VGG16 denoted as CA-VGG16. We mainly"
        },
        {
          "III. METHODOLOGY": "2017, using Inception-V1 as the backbone. Its speciﬁc network",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "introduce the Coordinate Attention Module (CA module)\nto"
        },
        {
          "III. METHODOLOGY": "structure\nis\nshown in Fig. 2.\nIn Inception-V1,\nthe\nstride of",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "improve the performance of VGG16, adding the CA Module"
        },
        {
          "III. METHODOLOGY": "the ﬁrst convolution transformation layer\nis 2.\nIn addition to",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "after\nthe\nﬁrst,\nfourth\nand\nﬁfth\nconvolution\ntransformation"
        },
        {
          "III. METHODOLOGY": "the parallel maximum pooling layer,\nthere are four maximum",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "blocks of VGG16 to improve\nthe model performance. The"
        },
        {
          "III. METHODOLOGY": "pooling layers with a stride of 2 × 2 and 7 × 7. The structure",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "overall network structure of CA-VGG16 is shown in Fig. 4."
        },
        {
          "III. METHODOLOGY": "of\nthe initial space sub module “Inc.” is shown as Fig. 3.",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "The Coordinate Attention is a novel attention mechanism"
        },
        {
          "III. METHODOLOGY": "This paper adopts\nthe form of\nseparate training. One I3D",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "proposed by Hou et al.\n[3]. The Coordinate Attention inno-"
        },
        {
          "III. METHODOLOGY": "network training takes RGB (video) stream as input, and the",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "vatively embeds\nthe precise positional\ninformation into the"
        },
        {
          "III. METHODOLOGY": "other\ntakes FLOW (optical ﬂow)\nstream as\ninput,\ncarrying",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "inter-channel attention and then improves the capability of the"
        },
        {
          "III. METHODOLOGY": "optimized and smooth stream information.\nIn this paper,\nthe",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "backbone network to capture\nthe detailed object\nstructures,"
        },
        {
          "III. METHODOLOGY": "two I3D networks are ﬁrst\ntrained, and the pre-trained weights",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "enrich the semantic information of shallow feature maps and"
        },
        {
          "III. METHODOLOGY": "are\nthen\nloaded\nduring model\nvalidation. The models\nare",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "obtain the precise positional\ninformation of\nlarger areas."
        },
        {
          "III. METHODOLOGY": "trained on the IEMOCAP multimodal dataset, and all the video",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "VGG16 [4] mainly\ndeploys\nthe\nsmall\nconvolution ﬁlters"
        },
        {
          "III. METHODOLOGY": "frames have been adjusted to the appropriate size, number and",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "to build a new convolutional neural network,\nincluding the"
        },
        {
          "III. METHODOLOGY": "channels before\ninput\ninto the network. Seventy-nine video",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "convolution transformation layers,\nthe pooling layers, and the"
        },
        {
          "III. METHODOLOGY": "frames are extracted at uniform time intervals, and the entire",
          "(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:90)(cid:286)(cid:272)(cid:856)(cid:38)(cid:349)(cid:286)(cid:367)(cid:282)(cid:400)\n(cid:47)(cid:374)(cid:296)(cid:367)(cid:258)(cid:410)(cid:286)(cid:282)(cid:3)(cid:47)(cid:374)(cid:272)(cid:286)(cid:393)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:115)(cid:1005)": "full connection layers."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": ""
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": "MULTIMODAL EMOTION RECOGNITION DATASET."
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": ""
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": "Happy"
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": ""
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": "278"
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": ""
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": "327"
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": ""
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": "286"
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": ""
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": "303"
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": ""
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": "442"
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": ""
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": "1636"
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": ""
        },
        {
          "THE DISTRIBUTION FOR FOUR CATEGORIES IN THE IEMOCAP": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": "TRAIN ALL THE MODELS FOR THE SAME EPOCH FOR A FAIR COMPARISON."
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": ""
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": "Method\nModality\nACC (%)"
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": ""
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": "I3D\nRGB\n64.88"
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": ""
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": ""
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": "I3D\nFLOW\n60.12"
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": ""
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": "C3D\nRGB\n61.33"
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": ""
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": "1D Music CNN\nRGB\n53.04"
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": ""
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": "ResNet50+LSTM\nRGB\n53.37"
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": ""
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": "Ours\n80.77\nRGB+FLOW+Audio"
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": ""
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": ""
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": ""
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": "the\naccuracy\nof\nthis\nnew short\nvideo\nemotion\nrecognition"
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": ""
        },
        {
          "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE": "can\nreach\n80.77%. Compared with\nthe\ncurrent mainstream"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "FLOW and Audio modalities is\nsigniﬁcantly higher\nthan the"
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "above ﬁve mainstream emotion recognition baseline methods."
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "Furthermore,\nthe accuracy of\nICANet proposed in this paper"
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "reaches 80.77%, exceeding the existing CNN-based state-of-"
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "the-art methods by 15.89%."
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "The\nanalysis\nshows\nthat\nthe main\nreason\nfor\nthe model"
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "performance improvement of\nICANet\nis that\nit\nintegrates the"
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "feature\ninformation of RGB, FLOW and Audio modalities,"
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "realizes the complementarity of feature information, makes up"
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "for the deﬁciency of a single modality, and deploys three sub"
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "feature\nextraction networks with higher\naccuracy to extract"
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "richer emotional\nfeature information. To sum up,\nthe validity"
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "of\nthe emotion recognition method in short videos driven by"
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "the typical\nfeatures of multimodal data is veriﬁed."
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "V. CONCLUSIONS"
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "This\npaper\npropose\na\nnew emotion\nrecognition method"
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "in\nshort\nvideos\ndenoted\nas\nICANet\ndriven\nby\nthe\ntypical"
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": ""
        },
        {
          "new emotion\nrecognition method\nICANet\nbased\non RGB,": "features of multimodal data. The experimental\nresults on the"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "original VGG16 is\nset as\nthe control group to determine the": "effectiveness of\nthe proposed improvement components. The",
          "(cid:1010)(cid:1004)": "(cid:1009)(cid:1012)(cid:856)(cid:1013)(cid:1010)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "(cid:1009)(cid:1013)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "speciﬁc ablation results are shown in Fig. 5.",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "We can clearly observe that\nthe CA-VGG16-3 achieves the",
          "(cid:1010)(cid:1004)": "(cid:1009)(cid:1012)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "(cid:1009)(cid:1011)(cid:856)(cid:1004)(cid:1007)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "highest accuracy of 58.96%, which is a practical improvement",
          "(cid:1010)(cid:1004)": "(cid:1009)(cid:1010)(cid:856)(cid:1013)(cid:1012)\n(cid:1009)(cid:1011)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "on the original VGG16. We conclude the reason is that adding",
          "(cid:1010)(cid:1004)": "(cid:1009)(cid:1009)(cid:856)(cid:1012)(cid:1007)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "(cid:1009)(cid:1010)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "the CA modules strengthens the feature extraction capability of",
          "(cid:1010)(cid:1004)": "(cid:4)(cid:18)(cid:18)(cid:894)(cid:1081)(cid:895)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "the model for LFCC spectrograms and improves the processing",
          "(cid:1010)(cid:1004)": "(cid:1009)(cid:1009)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "of\nspeciﬁc\nfeatures. We\nhave\ndemonstrated\nthat\nthe CA-",
          "(cid:1010)(cid:1004)": "(cid:1009)(cid:1007)(cid:856)(cid:1011)(cid:1010)\n(cid:1009)(cid:1008)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "VGG16 with three CA modules is more feasible and practical",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "(cid:1009)(cid:1007)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "in extracting the audio features.",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "(cid:1009)(cid:1006)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "D. Comparison with other SOTA Methods",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "(cid:1009)(cid:1005)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "(cid:115)(cid:39)(cid:39)(cid:1005)(cid:1010)\n(cid:94)(cid:28)(cid:882)(cid:115)(cid:39)(cid:39)(cid:1005)(cid:1010)\n(cid:18)(cid:17)(cid:4)(cid:68)(cid:882)(cid:115)(cid:39)(cid:39)(cid:1005)(cid:1010)\n(cid:18)(cid:4)(cid:882)(cid:115)(cid:39)(cid:39)(cid:1005)(cid:1010)(cid:882)(cid:1007)\n(cid:18)(cid:4)(cid:882)(cid:115)(cid:39)(cid:39)(cid:1005)(cid:1010)(cid:882)(cid:1009)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "To explore the best recognition effect of ICANet,\nthis paper",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "gives different modalities different weight\nratios for\nfusion to",
          "(cid:1010)(cid:1004)": "Fig. 5.\nThe ablation experiments of\nthe speciﬁc audio branch of\nICANet\nin"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "obtain the optimal weight\nratio. When the weight\nratios of",
          "(cid:1010)(cid:1004)": "terms of ACC(%) on the IEMOCAP dataset."
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "RGB, FLOW,\nand Audio are 1:1:1, 3:2:5, 4:3:3, 5:3:2,\nre-",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "spectively, the ACC are respectively 77.50%, 78.85%, 79.04%,",
          "(cid:1010)(cid:1004)": "TABLE II"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "PERFORMANCE COMPARISON OF ICANET AND DIFFERENT EMOTION"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "80.77%, 67.66%. We can observe that blindly increasing the",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "RECOGNITION METHODS ON THE IEMOCAP MULTIMODAL DATASET. WE"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "weight of RGB stream will not achieve better\nresults. On the",
          "(cid:1010)(cid:1004)": "TRAIN ALL THE MODELS FOR THE SAME EPOCH FOR A FAIR COMPARISON."
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "contrary, moderately increasing the weight of FLOW stream or",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "Audio stream will also improve the overall model performance",
          "(cid:1010)(cid:1004)": "Method\nModality\nACC (%)"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "to a certain extent. The reason is that in this approach, the over-",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "I3D\nRGB\n64.88"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "all network can not achieve good information complementarity",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "and the better effect of multimodal\nfusion. According to the",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "I3D\nFLOW\n60.12"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "experimental results, when RGB:FLOW:Audio = 4:2:4, we can",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "C3D\nRGB\n61.33"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "get\nthe best model performance. Therefore, we selects 4:2:4",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "as\nthe optimal weight\nratio of\nthree different modalities\nfor",
          "(cid:1010)(cid:1004)": "1D Music CNN\nRGB\n53.04"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "later decision level\nfeature fusion.",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "ResNet50+LSTM\nRGB\n53.37"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "To further verify the\neffectiveness of\nthe\nemotion recog-",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "nition method\nin\nshort\nvideos\ndriven\nby multimodal\ndata",
          "(cid:1010)(cid:1004)": "Ours\n80.77\nRGB+FLOW+Audio"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "proposed in this paper, we compare ICANet with the current",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "mainstream emotion recognition methods. The results of\nthe",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "performance\ncomparison\nare\nshown\nin Tab.\nII.\nIt\ncan\nbe",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "the\naccuracy\nof\nthis\nnew short\nvideo\nemotion\nrecognition"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "seen from the abovementioned table that\nthe accuracy of\nthe",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "can\nreach\n80.77%. Compared with\nthe\ncurrent mainstream"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "new emotion\nrecognition method\nICANet\nbased\non RGB,",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "emotion recognition methods,\nthe\nrecognition accuracy has"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "FLOW and Audio modalities is\nsigniﬁcantly higher\nthan the",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "been signiﬁcantly improved."
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "above ﬁve mainstream emotion recognition baseline methods.",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "Furthermore,\nthe accuracy of\nICANet proposed in this paper",
          "(cid:1010)(cid:1004)": "REFERENCES"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "reaches 80.77%, exceeding the existing CNN-based state-of-",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "[1]\nLucas, Bruce D and Kanade, Takeo and others. 1981. An iterative image"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "the-art methods by 15.89%.",
          "(cid:1010)(cid:1004)": "registration technique with an application to stereo vision.\nIn Proceed-"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "ings of 7th International Joint Conference on Artiﬁcial Intelligence. 674-"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "The\nanalysis\nshows\nthat\nthe main\nreason\nfor\nthe model",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "679."
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "performance improvement of\nICANet\nis that\nit\nintegrates the",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "[2] Carreira,\nJ. and Zisserman, A. 2017. Quo Vadis, Action Recognition?"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "feature\ninformation of RGB, FLOW and Audio modalities,",
          "(cid:1010)(cid:1004)": "A New Model and the Kinetics Dataset.\nIn 2017 IEEE Conference on"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "Computer Vision and Pattern Recognition (CVPR)."
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "realizes the complementarity of feature information, makes up",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "[3] Qibin Hou, Daquan Zhou, and Jiashi Feng. 2021. Coordinate attention"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "for the deﬁciency of a single modality, and deploys three sub",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "the IEEE/CVF\nfor efﬁcient mobile network design.\nIn Proceedings of"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "feature\nextraction networks with higher\naccuracy to extract",
          "(cid:1010)(cid:1004)": "Conference on Computer Vision and Pattern Recognition. 13713–13722."
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "[4]\nSimonyan, Karen and Zisserman, Andrew. 2014. Very deep convolu-"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "richer emotional\nfeature information. To sum up,\nthe validity",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "arXiv\npreprint\ntional\nnetworks\nfor\nlarge-scale\nimage\nrecognition.\nIn"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "of\nthe emotion recognition method in short videos driven by",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "arXiv:1409.1556."
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "the typical\nfeatures of multimodal data is veriﬁed.",
          "(cid:1010)(cid:1004)": "[5] Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh,"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "Lee, Sungbok and Narayanan, Shrikanth S. 2008. IEMOCAP: Interactive"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "V. CONCLUSIONS",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "emotional dyadic motion capture database.\nIn Language resources and"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "This\npaper\npropose\na\nnew emotion\nrecognition method",
          "(cid:1010)(cid:1004)": "evaluation. 335–359."
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "[6] Yang, Lingxiao and Zhang, Ru-Yuan and Li, Lida and Xie, Xiaohua."
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "in\nshort\nvideos\ndenoted\nas\nICANet\ndriven\nby\nthe\ntypical",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "2021. Simam: A simple, parameter-free\nattention module for convolu-"
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "features of multimodal data. The experimental\nresults on the",
          "(cid:1010)(cid:1004)": ""
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "",
          "(cid:1010)(cid:1004)": "tional neural networks. In International conference on machine learning."
        },
        {
          "original VGG16 is\nset as\nthe control group to determine the": "IEMOCAP multimodal emotion recognition dataset show that",
          "(cid:1010)(cid:1004)": "11863–11874."
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An iterative image registration technique with an application to stereo vision",
      "authors": [
        "Bruce Lucas",
        "Takeo Kanade",
        "Others"
      ],
      "year": "1981",
      "venue": "Proceedings of 7th International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "3",
      "title": "Coordinate attention for efficient mobile network design",
      "authors": [
        "Qibin Hou",
        "Daquan Zhou",
        "Jiashi Feng"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "5",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun And Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Lee",
        "Sungbok Narayanan",
        "S Shrikanth"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "6",
      "title": "Simam: A simple, parameter-free attention module for convolutional neural networks",
      "authors": [
        "Lingxiao Yang",
        "Ru-Yuan And Zhang",
        "Lida Li",
        "Xiaohua Xie"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    }
  ]
}