{
  "paper_id": "2307.14637v1",
  "title": "Htnet For Micro-Expression Recognition",
  "published": "2023-07-27T06:04:20Z",
  "authors": [
    "Zhifeng Wang",
    "Kaihao Zhang",
    "Wenhan Luo",
    "Ramesh Sankaranarayana"
  ],
  "keywords": [
    "Hierarchical transformer",
    "Micro-expression recognition",
    "Deep learning",
    "Facial muscle"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial expression is related to facial muscle contractions and different muscle movements correspond to different emotional states. For micro-expression recognition, the muscle movements are usually subtle, which has a negative impact on the performance of current facial emotion recognition algorithms. Most existing methods use self-attention mechanisms to capture relationships between tokens in a sequence, but they do not take into account the inherent spatial relationships between facial landmarks. This can result in sub-optimal performance on micro-expression recognition tasks.Therefore, learning to recognize facial muscle movements is a key challenge in the area of micro-expression recognition. In this paper, we propose a Hierarchical Transformer Network (HTNet) to identify critical areas of facial muscle movement. HTNet includes two major components: a transformer layer that leverages the local temporal features and an aggregation layer that extracts local and global semantical facial features. Specifically, HTNet divides the face into four different facial areas: left lip area, left eye area, right eye area and right lip area. The transformer layer is used to focus on representing local minor muscle movement with local self-attention in each area. The aggregation layer is used to learn the interactions between eye areas and lip areas. The experiments on four publicly available micro-expression datasets show that the proposed approach outperforms previous methods by a large margin. The codes and models are available at: https://github.com/wangzhifengharrison/HTNet",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Micro-expression refers to subtle muscle movements that last only for approximately 0.04 seconds. In recent years, extensive research has been conducted on utilizing computer vision-based methods to analyze micro-expressions  [1] . However, the accuracy of recognizing micro-expressions still requires improvement. Even though the micro-expression datasets are collected in well-controlled laboratory environments, the current results are still unsatisfactory  [2] . Utilizing computer vision for this task remains challenging due to the presence of subtle muscle movements that often accompany micro-expressions, making them difficult for both humans and computers to detect. On the other hand, the recognition of normal macro expressions has achieved high accuracy rates (over 95%)  [3] . This stark difference in performance can be attributed to the fact that micro-expressions are incredibly hard to detect due to their fleeting nature and subtle characteristics. As a result, researchers are still working to improve the precision and reliability of micro-expression recognition using computer vision techniques.\n\nIn the domain of micro-expression recognition, several researchers have proposed the adoption of the Local Binary Pattern (LBP) method to extract facial features. The LBP technique has demonstrated commendable discrimination ability through its texture-based feature extraction approach, while maintaining a low computational complexity  [4] . Moreover, other scholars have explored the use of optical flow features as inputs for estimating muscle motion  [4] . Optical flow is derived from the differences in brightness between consecutive frames, enabling the estimation of subtle facial movements. Several optical flow-based methodologies have been investigated, including Bi-WOOF  [5] , MDMO  [6] , FHOFO  [7] ,\n\nOptical Strain Weight, and Optical Strain Feature  [8] .Furthermore, notable deep learning models such as VGG16  [9] , GoogleNet  [10] , AlexNet  [11] , and OFF-Apex  [2]  have been utilized to process the TV-L1 optical flow. This optical flow is extracted from selected apex and onset frames of the image sequences. The peak frame, capturing the most salient information about the micro-expression, proves to be instrumental in accurately recognizing the expression.\n\nThe recognition of micro-expressions involves two primary stages: feature classification of facial features and the extraction of relevant facial information from images. Traditional classification methods often rely on artificially generated features, while deep learning techniques automatically detect features from facial images. To address the challenge of understanding subtle facial movements and the limited availability of training data, Liu et al.  [4]  propose using domain adaptation methods to transfer macro-expression knowledge for micro-expression recognition. This involves magnifying micro-expressions and augmenting the training dataset with more generated images, known as expression magnification and reduction (EMR). The system's performance using this approach proves to be competitive in the Micro-Expression Grand Challenge. Another approach suggested by Liong et al. is the use of STSTNet, a three shallow stream CNN, to recognize facial expressions. The optical strain, vertical optical flow, and horizontal optical flow fields are fed into these three shallow stream networks  [12] . For recognizing micro-expressions, Xia et al. propose adopting a recurrent convolutional network, which has a shallower design and combines the strengths of both RNNs and CNNs  [13] . Detecting micro-expressions is challenging due to the small and subtle nature of the facial movements. This makes it difficult to identify the specific facial muscles involved in the expression and monitor the pixel's motion over time. To address these issues, Zhou et al.  [14]  suggest using a feature refinement network, specific expression feature learning network, and fusion techniques for expression recognition. By employing self-attention on global image features, their method can extract distinguishing characteristics for micro-expression recognition. However, global self-attention between image features will lack of fine-grained features. To deal with this issue, our objective is to maintain selfattention in the local blocks at each hierarchy level to capture fine-grained features. We use aggregation blocks to incorporate both local fine-grained and global coarse-grained interactions at different image scales. With this new mechanism, each pixel in same-level blocks is treated at a fine granularity, while the pixels in upper-level blocks are at a coarse granularity. This allows our model to effectively capture both short-and long-range visual dependencies.\n\nIn this study, we present a novel self-attention method utilizing Transformer layers to effectively capture both local and global interactions within a hierarchical structure. The low-level self-attention in the Transformer layers aims to capture fine-grained features within local regions. On the other hand, the high-level self-attention in these layers is designed to capture coarse-grained features spanning global regions. To facilitate interactions between different blocks at the same level, we propose an aggregation block. The overall architecture of our proposed method, referred to as HTNet, is illustrated in Fig.  1  This approach helps to mitigate the impact of noticeable background noise that may be picked up by the lab camera due to possible light flickering. The use of Transformer layers allows us to concentrate on modeling small-scale, subtle muscle motions within each area through local self-attention. Additionally, the aggregation layer learns the interactions between the eye areas and lip areas. We conduct experiments to investigate how varying block sizes affect the accuracy of micro-expression recognition. c) Through experiments conducted on four available datasets, we demonstrate that our proposed method outperforms previous approaches noticeably. This highlights the effectiveness and superiority of our model for micro-expression recognition tasks.",
      "page_start": 2,
      "page_end": 5
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conventional Methods",
      "text": "In conventional techniques for expression recognition, appearance-based characteristics are commonly utilized. One prevalent pattern is the local binary pattern from three orthogonal planes (LBP-TOP)  [15] . In some LBP-TOP works, LBP-TOP is transferred to the tensor-independent RGB space, which enhances robustness  [16] . To achieve lower computational complexity, LBP-SIP effectively minimizes redundancy in LBP-TOP patterns, providing a lightweight representation  [17] . LSDF  [18]  utilizes the 16 interest regions (ROIs) extracted by the Facial Action Coding System for micro-expression recognition. These 16\n\nRegions of Interest represent 16 muscle motion areas, while irrelevant areas are eliminated. STCLQP  [19]  incorporates additional features such as orientation, shape attributes, and magnitude. By integrating sign, magnitude, and orientation components, STCLQP expands from LSDF and reduces the number of interest zones from 16 to 3. A codebook is created to extract discriminative and salient features from codebooks for different facial emotions.\n\nIn traditional methods, geometric-based features are extracted using facial landmarks or optical flow. These geometric-based features can identify motion deformations. Li et al.\n\n[20] explore a deep learning technique for localizing face landmarks and dividing the facial area into interest zones. Since facial micro-expressions are produced by facial muscular contractions, assessing the orientation of these contractions is crucial for identifying emotions.\n\nUsing this technique, they categorize the face into different areas of concern that correlate to different muscle action patterns. Furthermore, the facial area's activity can be adequately reflected by the optical flow. Liu et al.  [6]  propose the use of a powerful MDMO feature extraction network for micro-expression identification. They calculate the optical flow for each image in the video sequence and divide the facial zones into various intriguing portions.\n\nMean optical flow features are then calculated from each image, and an SVM classifier is employed on these mean optical flow features for emotion identification. Their method effectively considers both geographical position and regional statistical movement information, proving to be straightforward and efficient. Rather than using the LBP histogram, Liong et al.  [5]  suggest weighting the histogram of LBP and averaging the value of optical flow features for recognizing emotions on the face, namely Bi-WOOF.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Deep Learning Methods",
      "text": "In recent years, an array of deep learning techniques has emerged for extracting facial characteristics in expression identification  [21] [22] [23] [24] . Xia et al.  [25]  proposed the application of recurrent convolutional networks to establish connections between facial position information and record facial muscle contractions across various regions. This model incorporates multiple recurrent convolutional layers and a classification layer to capture visual characteristics for facial emotion recognition. Meanwhile, Lei et al.  [21]  introduced the use of Transformer as an encoder to model the connections between nodes and edges of the face, constructing a face graph based on the edges and nodes of facial features. Although prior methods often relied on handcrafted features to represent subtle facial contractions, such features often come with higher computational costs. In response, Gan et al.  [2]  proposed an automated approach to localize the apex frame and employ optical flow as input for their OFF-ApexNet. Leveraging CNN, their network extracts new feature descriptors from optical flow. On the other hand, some researchers have explored the adoption of shallow CNNs, which effectively derive high-level visual attributes from three components of movement estimation -horizontal, vertical optical flow fields, and optical strain -for inferring emotional states  [12] . To address the issue of composite-database domain shift, Xia et al.  [13]  developed an RCN model to examine how a smaller model affects the detection of micro-expressions. Within the RCN, they designed three parameter-free modules, including an attention unit, shortcut connection, and wide expansion, to prevent an increase in the number of learnable parameters. Additionally, researchers have acknowledged the significance of hierarchical structures in this context  [26] .",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Transformer In Computer Vision",
      "text": "The standard attention mechanism, initially introduced in the Transformer model, is known as scaled dot-product attention, which has become a fundamental method in Nat-ural Language Processing (NLP) tasks. The Vision Transformer (ViT)  [27]  extends the Transformer architecture from NLP tasks to computer vision tasks. Specifically, for image classification, ViT applies the Transformer to non-overlapping image blocks. However, a significant drawback of ViT is its reliance on large training datasets to improve network performance. Recently, new training methodologies introduced by DeiT have allowed ViT to perform effectively with lower training datasets. Nevertheless, ViT is not well-suited as a backbone network for dense sub-vision tasks due to the quadratic increase in computing complexity with the increasing image size.To address this limitation and apply the Transformer architecture to high-resolution images, Liu et al.  [28]  proposed the Swin Transformer. The Swin Transformer achieves this by employing shifted windows, which restrict self-attention calculation to a local window while enabling cross-window communication between different image blocks. This approach enhances the model's efficiency and scalability when dealing with high-resolution images. Other related Transformer works  [29, 30]  explore convolutional-free and simpler backbone networks for deep prediction tasks, offering alternative approaches to address the computational complexity and resource requirements in vision tasks.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Proposed Method",
      "text": "As illustrated in Fig.  2 , the proposed HTNet consists of transformer layers and block aggregation at each level of the hierarchical structure. The transformer layer performs selfattention on each image block independently. At the low-level network, the self-attention function in the transformer layer captures fine-grained features. Subsequently, the block aggregation process aggregates small image blocks into larger ones, allowing for the creation of interactions between different blocks at the same level. This aggregation leads to the capture of coarse-grained features after each block aggregation. It is important to note that all the blocks within the same level share the same set of parameters. Finally, the MLP block in our model is applied to the final feature maps for micro-expression classification. This modular and hierarchical design enables HTNet to effectively extract and integrate features at different scales and levels of granularity for improved micro-expression recognition performance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Optical Flow Map Extraction",
      "text": "Optical flows, generated from the commencement and peak frames, serve as a valuable means to describe movement displacements in facial areas. These optical flows have demonstrated promising results in micro-expression recognition datasets  [2, 12] .\n\nIn order to obtain an optical feature map, it is essential to determine the onset and apex frame indices. However, in the case of micro-expression datasets, the onset frame index is already provided, necessitating the determination of only the apex frame index from video sequences. To achieve this, we adopt the D&C-RoIs method, which has been widely used in prior research on micro-expression  [2] . The D&C-RoIs approach effectively establishes the relationship between the commencement frame and subsequent frames, allowing for the ac-curate identification of the apex frame index. This ensures the reliable extraction of optical flow features for subsequent micro-expression recognition tasks.\n\n,where B is the number of bins in histograms, h 1 is the first frame, and h 2 is the other frames except the first frame. The highest rate of difference in ROIs will be selected, which can represent the apex frame that has maximum facial muscle changes.\n\nThen, we obtain optical flow feature maps using the commencement and peak frames. It is possible to formulate the optical flow feature map as follows:\n\n, where X and Y represent the frame's width W and height H respectively, u(x, y) and v(x, y) is the horizontal and vertical component of optical flow feature map\n\nwhere\n\nIn our approach, we employ the first-order derivatives of the optical flow field to calculate the variations in the optical flow fields, commonly known as optical strain. The optical strain provides an estimation of the degree of facial displacement, thereby offering valuable insights into the subtle movements that occur during micro-expressions. This computation of optical strain allows us to capture and analyze the intricate facial dynamics, contributing to the accurate recognition of micro-expressions:\n\n,where ∂Vx\n\nIn our network, we adopt a region-specific approach by focusing on four specific facial areas -the left eye region, left lip region, right eye region, and right lip region -instead of considering the entire facial region. This strategic selection aims to mitigate the impact of audible background noise that may be present in the lab recordings due to potential flickering lights.\n\nTo extract the facial optical flow feature maps from the entire optical flow feature maps V m , we employ the Multi-task Cascaded Convolutional Networks (MTCNN)  [31]  to obtain the face landmark coordinates from the apex images. The four facial optical flow feature maps are subsequently cropped from V m , representing the left-eye optical flow feature map, left-lip visual feature map, right-eye optical flow feature map, and right-lip feature map. Each of these four feature maps has a size of W 2 × H 2 × 3, which is half the size of the whole optical flow image. Following the extraction of the four optical flow features, we combine them and feed the combined features into our HTNet for micro-expression recognition. The entire process is visually depicted in Fig.  2 .",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Transformer Layer",
      "text": "In our approach, each image block has a size of P × P when processing an input optical flow image with dimensions H × W × 3. After linear projection and partitioning on the optical flow images, each patch has a feature dimension of P × P × 3. Subsequently, the patches are flattened, resulting in an input for our model denoted as X ∈ R b×Hn×n×d , where H n represents the number of blocks in each level, b is the batch size, n is the sequence length, and H n × n = H×W P 2 . Multiple transformer layers are applied within each block, and the hierarchy determines the number of transformer layers utilized. Each transformer layer consists of Layer Normalization (LN), Multi-Head Self-Attention (MSA) layer, and Feed-Forward Fully Connected Network (FFN). To encode spatial information, a trainable positional embedding vector is incorporated into all sequence vectors in R d . This ensures that spatial relationships and positional information are effectively captured and encoded within the feature representations:\n\n,where l = 1, 2, ..., L, l is the index of the l -th block in each hierarchy layer i, and L is the overall number of blocks in each hierarchy layer.\n\nThe FFN includes two layers: max(0, xW 1 +b)W 2 +b. At each block i within the same level, the multi-head self-attention mechanism is applied. In this self-attention component, the input X ∈ R n×d is transformed into three parts, namely queries Q, keys K, and values V , where n represents the sequence length and d is the dimension of the inputs. Subsequently, the scaled dot-product attention is employed on Q, K, and V :\n\nLN will be applied in each block as follows:\n\n,where µ is the mean of features and δ is standard deviation of the feature, o is the elementwise dot and λ and β are learnable parameters.\n\nFollowing the transformer layer, we employ block aggregation to merge the output of the transformer layer. Specifically, we group every four small blocks into one larger block through the block aggregation process.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Block Aggregation",
      "text": "The block aggregation function utilized in our HTNet shares similarities with several Pyramid designs. However, a notable distinction is that our model employs local attention on each image block, rather than global attention on the entire image. This approach proves to be beneficial for enhancing the model's performance since micro-expression recognition heavily relies on localized facial muscle motion areas. By focusing on specific face regions and leveraging local attention, our model effectively captures the essential features for inferring micro-expression states while disregarding irrelevant facial areas.\n\nIn our HTNet model, each block independently processes the optical flow map, and block",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Loss Function",
      "text": "In this paper, we employ the cross-entropy loss function to train our model. The computation of the cross-entropy loss L can be expressed by the following formula:\n\n,where w i is the weight of samples in the dataset, y is the label and y i ∈ 0, 1.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Datasets",
      "text": "The experiments are conducted on four databases: SAMM  [32] , SMIC  [33] , CASME II  [34]  and CASME III  [35]  databases. To ensure consistency and comparability, SAMM, SMIC, and CASME II are merged into a composite dataset, where the same emotion labels from these three datasets are adopted for the micro-expression recognition tasks. In these datasets, the emotion categories are divided as follows: the \"positive\" emotion category includes the \"happiness\" emotion class, and the \"negative\" emotion category includes \"sadness\",\"disgust\", \"contempt\", \"fear\" and \"anger\" emotion classes while \"surprise\" emotion category only includes \"surprise\" class.\n\nTable  1 : The experiments are implemented on SAMM  [32] , SMIC  [33] , CASME II  [34]  and CASME III  [35]  databases. SAMM, SMIC, and CASME II are merged into one composite dataset, and the same labels in these three datasets are adopted for micro-expression tasks. in face images with a resolution of 28 × 28 pixels. Emotion categories in images in SAMM are divided into categories including \"disgust\", \"fear\", \"contempt\", \"angry\", \"repression\", \"surprise\", \"happiness\", and \"others\". After classifying into three emotion categories, the number of \"negative\", \"positive\", and \"surprise\" is 92, 26,15.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Database",
      "text": "CASME II  [34] : The CASME II dataset consists of data from 24 subjects, totaling 145 samples corresponding to 145 emotions. All samples are captured using lab cameras, with a frame rate set at 200 frames per second. The original size of the samples is 640 × 480 pixels. For our experiment, we cropped and resized the face images to a resolution of 28 × 28 pixels. Samples in CASME II are divided into categories such as \"happiness\", \"surprise\", \"disgust\", \"sadness\", \"fear\", \"repression\", and \"others\". After merging into three emotion categories, the number of \"negative\", \"positive\", and \"surprise\" is 88, 32,25. The onset, offset, and apex index are annotated in CASME II.\n\nCASME III  [35] :CASME III, the third generation of the Facial Spontaneous Micro-Expression database, is distinguished by its inclusion of depth information and high ecological validity, making it a valuable resource for micro-expression recognition. CASME III Part A comprises data from 100 subjects, totaling 943 samples corresponding to 943 emotions.\n\nThe samples are captured using a lab camera, with a frame rate of 30 frames per second, and have an original resolution of 1280 × 720 pixels. Samples in CASME III part A are categorized into \"happiness\", \"anger\",\"fear\",\"disgust\", \"surprise\", \"others\" and \"sadness\".\n\nThe total number of \"negative\", \"positive\", and \"surprise\" is 508, 64, and 201.\n\nSMIC  [33] : The SMIC-HS dataset comprises data from 16 subjects, totaling 164 samples corresponding to 164 emotions. All samples are captured using a lab camera with a frame rate of 100 frames per second. The original image size of the samples is 640 × 480 pixels. To focus on the facial region of interest and maintain a consistent input size for our micro-expression recognition task, we crop the face images to a resolution of 28 × 28 pixels.\n\nSamples in SMIC are categorized into \"negative\", \"surprise\" and \"positive\". The number of \"negative\", \"positive\", and \"surprise\" is 70, 51,43. The onset and offset are given in SMIC, but the apex index is not given in SMIC. The detailed information of these three datasets can be summarised in Table  1  4",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": ".2. Implementation Details",
      "text": "Initially, in the case of the SMIC dataset, where the peak frame index is missing, we employed the D&C-RoIs technique  [36]  to determine the index of the peak frame. For the SAMM, CASME II, and CASME III datasets, the ground truth for the peak frame is available, which simplifies the process of obtaining the crucial micro-expression moment. After obtaining the onset and apex images from the dataset, we utilized Gunnar Farneback's algorithm  [37]  to extract optical flow from these images at both the commencement and peak The experiments are conducted using PyTorch and Python 3.9 on Ubuntu 22.04 operating system. We set the learning rate for the training parameters to 5 × 10 -5 and use a maximum of 800 epochs for training.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Setup:",
      "text": "The standard evaluation method for micro-expression tasks is the leave-onesubject-out (LOSO) cross-validation. LOSO cross-validation is preferred as it allows for a fair comparison between different models and ensures that the model's performance is not biased by specific subject characteristics. This approach closely simulates real-world scenarios, where individuals with diverse backgrounds and expressions are encountered in various settings and locations.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Performance Metrics",
      "text": "The class distribution in the composite micro-expression dataset is unbalanced, with different emotions having varying frequencies. Specifically, the rate of surprise, positive, and negative emotions is approximately 1:1.3:3, respectively.To address this issue, we employ the Unweighted Average Recall (UAR) and Unweighted F1 score (UF1) to report our results.\n\n1) The Unweighted F1-score (UF1), also known as the macro-average F1-score, is a metric that is commonly used to evaluate performance in multi-class classification tasks with imbalanced class distributions. To calculate the UF1, we need to compute the False Positives (FP), True Positives (TP), and False Negatives (FN) for each class c in all folds of the leave-one-subject-out (LOSO) cross-validation. Then, the F1-score for each class F 1 c can be computed using the formula:\n\nIn the formula 8, the C is the number of classes.\n\n2) Unweighted Average Recall (UAR): UAR is a metric that is particularly useful when evaluating the effectiveness of a model in the presence of imbalanced class ratios.\n\nTo calculate the UAR, we first compute the True Positives (T P c ) for each emotion class c. The T P c represents the number of correctly classified samples in each class.\n\nAdditionally, we calculate the total number of samples (n c ) in each class. The number of emotion classes is denoted as C.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Comparison To State Of The Arts",
      "text": "In Table  2  and Table  3 , we present the performance of our proposed method in compari- OFF-ApexNet  [2] : Because implementing optimal feature extraction methods for the subtle motion of facial expressions is complex, and many approaches extract the features of the subtle motion of facial expressions by using handcraft features.For micro-expression tasks, they suggested utilising optical flow fields from the beginning and peak frames. In the optical flow fields, horizontal and vertical features are obtained and fed into CNN-based network for further feature enhancement. After that, the extracted features from OFF-ApexNet will be used for micro-expression classification.\n\nSTSTNet  [12] : The authors proposed employing a three shallow CNN-based model to obtain high-level discriminative representations for classifying micro-expression emotions. The CNN-based network consists of several layers, including convolutional layers, fully connected layers, and pooling layers. These layers work together to extract relevant muscle motion features from two directions: the horizontal and vertical optical flow fields, as well as optical strain information.\n\nDual-Inception  [39] : To address the challenges of cross-database micro-expression recognition, the authors proposed a novel approach using two inception networks to extract horizontal and vertical features from the optical flow maps. By feeding the horizontal component of the optical flow into one inception network and the vertical component into another inception network, they can independently capture relevant patterns and information from both directions.\n\nEMR  [4] : Given the finite number of training samples in facial emotion datasets, it becomes essential to enhance both the quantity and quality of the available training images. To address this, the proposed method in EMR incorporates two domain adaptation strategies.The first strategy involves adversarial training methods.The second strategy is the expression magnification method RCN  [13] : In the context of the composite dataset, the subtle facial motion necessary for micro-expression recognition may be lost due to domain shift, leading to a decline in the model's performance. To address this issue, the proposed approach suggests using a lower-size image as input and adopting a smaller-architecture model, which has shown to be beneficial for improving the model's performance on composite dataset tasks.\n\nFeatRef  [14] : In the FeatRef network, it includes two stages, in the first stage, horizontal inception network and vertical inception network will extract horizontal and vertical muscle motion features. After that, the horizontal and vertical features will be merged and fed into three attention based network for classifying these extracted features into different microexpression categories. Finally, the classification branch is used to fuse salient and discriminative features obtained from the inception module for inferring micro-expression. Their experiments demonstrated that Feature Refinement (FeatRef) could extract discriminative and salient representation for micro-expression recognition and obtained good performance in micro-expression datasets.  Approaches CASME III Part A UF1 UAR AlexNet  [11]  0.257 0.2634 STSTNet  [12]  0.3795 0.3792 RCN  [13]  0.3928 0.3893 FeatRef  [14]  0.3493 0.3413 HTNet(Ours) 0.5767 0.5415",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Compared To Handcrafted Methods",
      "text": "",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Compared To Deep Learning Methods",
      "text": "In Table  2  and Table  3 , our HTNet outperforms most deep learning methods by a considerable margin. According to Table  2 , HTNet achieved an UF1 and UAR of 0.8603 and 0.8475, respectively, on the full composite dataset, which represents an improvement of approximately 7% compared to previous state-of-the-art methods. Analyzing Table  2 , we observe that ALexNet, GoogLeNet, and VGG16 achieved UF1 and UAR of 0.6933 and 0.7154, 0.5573 and 0.6049, and 0.6425 and 0.6516, respectively, on the full composite dataset. In Table  3 , we conducted experiments on CASME III Part A and reported the Unweighted F1-score (UF1) and Unweighted Average Recall (UAR) of deep learning methods,",
      "page_start": 21,
      "page_end": 23
    },
    {
      "section_name": "Ablation Study",
      "text": "This section presents an in-depth analysis of the impact of various parameters in our HTNet model. We investigate the influence of block size, hidden dimension, number of heads in the transformer, and the number of transformer layers at each hierarchy level.\n\nThe initial experimental settings include a block size of 7 × 7 at the bottom-level, a hidden dimension of 256, three heads in the transformer, and (2, 2, 8) transformer layers in each hierarchy level. For each ablation experiment, we modify a specific parameter while keeping other settings constant. The evaluation of each approach is conducted using the UAR metric and UF1 metric.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Impacts Of Block Size",
      "text": "We conducted experiments to study the impact of different block sizes on the overall accuracy in composite micro-expression datasets, including SMIC, SAMM, and CASME II. The block size refers to the size of the facial regions considered in the HTNet model.  We varied the bottom-level block size from 5 to 10, with the middle-level block size being twice the bottom-level size, and the top-level block size being four times the bottom-level size. The results, reported in Table  4 , indicate that the choice of block size significantly affects the model's performance. Smaller block sizes may lead to subpar performance as they might miss some crucial facial parts, while larger block sizes generally perform better.\n\nHowever, if the block size becomes too large, some of the facial areas may overlap, such as the left-eye area overlapping with the right-eye area, which can negatively impact the model's performance. Therefore, finding an optimal balance between block sizes is crucial to ensure the best performance in micro-expression recognition tasks.",
      "page_start": 24,
      "page_end": 26
    },
    {
      "section_name": "Impacts Of Dimensions",
      "text": "We research how the number of dimensions affects the accuracy of composite datasets -SMIC, SAMM and CASME II. The Unweighted F1-score (UF1) and Unweighted Average Recall (UAR) performance of composite datasets are reported in Fig.  5 . The smaller hidden dimension has worse performance because the small hidden dimension is hard to encode the optical flow feature map. However, using a larger hidden dimension will lead to overfitting problems. The full UF1 score for hidden dimension 256 has the best performance, around 0.86.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Impact Of Number Of Heads",
      "text": "We investigate the effects of the transformer layer's head count on accuracy in composite datasets-SMIC, SAMM and CASME II. The Unweighted F1-score (UF1) and Unweighted Average Recall (UAR) performance of composite datasets are reported in Fig.  6 . In Fig.  6 (a), it demonstrates that three heads in transformer layers will have the best performance.  Figure  7  (e) presents the confusion matrix obtained from this cross-validation. From the confusion matrix, we observe that the negative emotion category achieves the highest accuracy among the three emotion categories. This result is consistent with the performance observed in the composite datasets, where the negative category also exhibited the highest accuracy.On the other hand, the positive emotion category obtains the lowest accuracy, approximately 0.15. Most of the positive samples are misclassified as negative. In contrast, in the composite datasets, only a small portion of samples are misclassified as negative.",
      "page_start": 26,
      "page_end": 28
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a hierarchical transformer architecture to extract important",
      "page_start": 29,
      "page_end": 29
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed HTNet paradigm. The proposed framework focuses on four facial areas instead of",
      "page": 4
    },
    {
      "caption": "Figure 1: Our contributions in",
      "page": 5
    },
    {
      "caption": "Figure 2: , the proposed HTNet consists of transformer layers and block",
      "page": 8
    },
    {
      "caption": "Figure 2: HTNet: Overall architectures of hierarchical transformer network for micro-expression recognition.",
      "page": 9
    },
    {
      "caption": "Figure 2: 3.2. Transformer layer",
      "page": 11
    },
    {
      "caption": "Figure 3: Several transformer layers will be applied in each block in parallel.Hierarchy determines the",
      "page": 12
    },
    {
      "caption": "Figure 4: The block aggregation include 3×3 convolutional layer and followed by LN and 3×3 max pooling.",
      "page": 14
    },
    {
      "caption": "Figure 4: At hierar-",
      "page": 14
    },
    {
      "caption": "Figure 5: We research how the number of dimensions affects the accuracy of composite datasets-SMIC,",
      "page": 25
    },
    {
      "caption": "Figure 6: We investigate the effects of the transformer layer’s head count on accuracy in composite datasets-",
      "page": 25
    },
    {
      "caption": "Figure 7: The confusion matrix for the proposed HTNet on the composite database-SMIC, SAMM and",
      "page": 27
    },
    {
      "caption": "Figure 7: (a) displays the confusion matrices of HTNet, presenting the accuracy achieved",
      "page": 28
    },
    {
      "caption": "Figure 7: (e) presents the confusion matrix obtained from this cross-validation. From",
      "page": 28
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The experiments are implemented on SAMM[32], SMIC[33], CASME II[34] and CASME III [35]",
      "page": 16
    },
    {
      "caption": "Table 2: and Table 3, we present the performance of our proposed method in compari-",
      "page": 20
    },
    {
      "caption": "Table 2: The Unweighted F1-score (UF1) and Unweighted Average Recall (UAR) performance of handcraft",
      "page": 21
    },
    {
      "caption": "Table 2: presents a comparative analysis of different methods, including LBP-TOP and",
      "page": 22
    },
    {
      "caption": "Table 3: The Unweighted F1-score (UF1) and Unweighted Average Recall (UAR) performance of deep",
      "page": 23
    },
    {
      "caption": "Table 2: and Table 3, our HTNet outperforms most deep learning methods by a",
      "page": 23
    },
    {
      "caption": "Table 2: , HTNet achieved an UF1 and UAR of 0.8603",
      "page": 23
    },
    {
      "caption": "Table 3: , we conducted experiments on CASME III Part A and reported the Un-",
      "page": 23
    },
    {
      "caption": "Table 4: Study the impacts of different block sizes on composite datasets-SMIC, SAMM and CASME II.",
      "page": 24
    },
    {
      "caption": "Table 4: , indicate that the choice of block size significantly",
      "page": 25
    },
    {
      "caption": "Table 5: Study the impacts of the number of transformer layers on composite datasets. The composite",
      "page": 26
    },
    {
      "caption": "Table 5: presents an investigation into the impact of the number of transformer layers on",
      "page": 27
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Ferv39k: A large-scale multiscene dataset for facial expression recognition in videos",
      "authors": [
        "Y Wang",
        "Y Sun",
        "Y Huang",
        "Z Liu",
        "S Gao",
        "W Zhang",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "2",
      "title": "Off-apexnet on micro-expression recognition system",
      "authors": [
        "Y Gan",
        "S.-T Liong",
        "W.-C Yau",
        "Y.-C Huang",
        "L.-K Tan"
      ],
      "year": "2019",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "3",
      "title": "Needle in a haystack: Spotting and recognising micro-expressions \"in the wild",
      "authors": [
        "Y Gan",
        "J See",
        "H.-Q Khor",
        "K.-H Liu",
        "S.-T Liong"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "4",
      "title": "A neural micro-expression recognizer",
      "authors": [
        "Y Liu",
        "H Du",
        "L Zheng",
        "T Gedeon"
      ],
      "year": "2019",
      "venue": "14th IEEE international conference on automatic face & gesture recognition (FG 2019)"
    },
    {
      "citation_id": "5",
      "title": "Less is more: Micro-expression recognition from video using apex frame",
      "authors": [
        "S.-T Liong",
        "J See",
        "K Wong",
        "-W Phan"
      ],
      "year": "2018",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "6",
      "title": "A main directional mean optical flow feature for spontaneous micro-expression recognition",
      "authors": [
        "Y.-J Liu",
        "J.-K Zhang",
        "W.-J Yan",
        "S.-J Wang",
        "G Zhao",
        "X Fu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Fuzzy histogram of optical flow orientations for micro-expression recognition",
      "authors": [
        "S Happy",
        "A Routray"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Optical strain based recognition of subtle emotions",
      "authors": [
        "S.-T Liong",
        "R -W. Phan",
        "J See",
        "Y.-H Oh",
        "K Wong"
      ],
      "year": "2014",
      "venue": "2014 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS)"
    },
    {
      "citation_id": "9",
      "title": "Going deeper in spiking neural networks: Vgg and residual architectures",
      "authors": [
        "A Sengupta",
        "Y Ye",
        "R Wang",
        "C Liu",
        "K Roy"
      ],
      "year": "2019",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "10",
      "title": "On the performance of googlenet and alexnet applied to sketches",
      "authors": [
        "P Ballester",
        "R Araujo"
      ],
      "year": "2016",
      "venue": "Thirtieth AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "11",
      "title": "A review of micro-expression recognition based on deep learning",
      "authors": [
        "H Zhang",
        "H Zhang"
      ],
      "year": "2022",
      "venue": "2022 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "12",
      "title": "Shallow triple stream three-dimensional cnn (ststnet) for micro-expression recognition",
      "authors": [
        "S.-T Liong",
        "Y Gan",
        "J See",
        "H.-Q Khor",
        "Y.-C Huang"
      ],
      "year": "2019",
      "venue": "14th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "13",
      "title": "Revealing the invisible with model and data shrinking for composite-database micro-expression recognition",
      "authors": [
        "Z Xia",
        "W Peng",
        "H.-Q Khor",
        "X Feng",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "14",
      "title": "Feature refinement: An expression-specific feature learning and fusion method for micro-expression recognition",
      "authors": [
        "L Zhou",
        "Q Mao",
        "X Huang",
        "F Zhang",
        "Z Zhang"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "16",
      "title": "Micro-expression recognition using dynamic textures on tensor independent color space",
      "authors": [
        "S.-J Wang",
        "W.-J Yan",
        "X Li",
        "G Zhao",
        "X Fu"
      ],
      "year": "2014",
      "venue": "nd international conference on pattern recognition"
    },
    {
      "citation_id": "17",
      "title": "Lbp with six intersection points: Reducing redundant information in lbp-top for micro-expression recognition",
      "authors": [
        "Y Wang",
        "J See",
        "R -W. Phan",
        "Y.-H Oh"
      ],
      "year": "2014",
      "venue": "Asian conference on computer vision"
    },
    {
      "citation_id": "18",
      "title": "Micro-expression recognition using robust principal component analysis and local spatiotemporal directional features",
      "authors": [
        "S.-J Wang",
        "W.-J Yan",
        "G Zhao",
        "X Fu",
        "C.-G Zhou"
      ],
      "year": "2014",
      "venue": "European Conference on computer vision"
    },
    {
      "citation_id": "19",
      "title": "Spontaneous facial micro-expression analysis using spatiotemporal completed local quantized patterns",
      "authors": [
        "X Huang",
        "G Zhao",
        "X Hong",
        "W Zheng",
        "M Pietikäinen"
      ],
      "year": "2016",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "20",
      "title": "Spontaneous facial micro-expression detection based on deep learning",
      "authors": [
        "X Li",
        "J Yu",
        "S Zhan"
      ],
      "year": "2016",
      "venue": "IEEE 13th International Conference on Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Micro-expression recognition based on facial graph representation learning and facial action unit fusion",
      "authors": [
        "L Lei",
        "T Chen",
        "S Li",
        "J Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Facial smile detection based on deep learning features",
      "authors": [
        "K Zhang",
        "Y Huang",
        "H Wu",
        "L Wang"
      ],
      "year": "2015",
      "venue": "2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)"
    },
    {
      "citation_id": "23",
      "title": "Facial expression recognition based on deep evolutional spatialtemporal networks",
      "authors": [
        "K Zhang",
        "Y Huang",
        "Y Du",
        "L Wang"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "24",
      "title": "Four-player groupgan for weak expression recognition via latent expression magnification",
      "authors": [
        "W Niu",
        "K Zhang",
        "D Li",
        "W Luo"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "25",
      "title": "Spatiotemporal recurrent convolutional networks for recognizing spontaneous micro-expressions",
      "authors": [
        "Z Xia",
        "X Hong",
        "X Gao",
        "X Feng",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "26",
      "title": "Hierarchical structure correlation inference for pose estimation",
      "authors": [
        "G Zheng",
        "S Wang",
        "B Yang"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "27",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "28",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "29",
      "title": "Transformers and cnns fusion network for salient object detection",
      "authors": [
        "C Yao",
        "L Feng",
        "Y Kong",
        "L Xiao",
        "T Chen"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "30",
      "title": "Nested hierarchical transformer: Towards accurate, data-efficient and interpretable visual understanding",
      "authors": [
        "Z Zhang",
        "H Zhang",
        "L Zhao",
        "T Chen",
        "S Arik",
        "T Pfister"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Face recognition based surveillance system using facenet and mtcnn on jetson tx2",
      "authors": [
        "E Jose",
        "M Greeshma",
        "M Haridas",
        "M Supriya"
      ],
      "year": "2019",
      "venue": "2019 5th International Conference on Advanced Computing & Communication Systems (ICACCS)"
    },
    {
      "citation_id": "32",
      "title": "Samm: A spontaneous micro-facial movement dataset",
      "authors": [
        "A Davison",
        "C Lansley",
        "N Costen",
        "K Tan",
        "M Yap"
      ],
      "year": "2016",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "33",
      "title": "A spontaneous micro-expression database: Inducement, collection and baseline",
      "authors": [
        "X Li",
        "T Pfister",
        "X Huang",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic face and gesture recognition (fg)"
    },
    {
      "citation_id": "34",
      "title": "Casme ii: An improved spontaneous micro-expression database and the baseline evaluation",
      "authors": [
        "W.-J Yan",
        "X Li",
        "S.-J Wang",
        "G Zhao",
        "Y.-J Liu",
        "Y.-H Chen",
        "X Fu"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "35",
      "title": "Cas (me) 3: A third generation facial spontaneous micro-expression database with depth information and high ecological validity",
      "authors": [
        "J Li",
        "Z Dong",
        "S Lu",
        "S.-J Wang",
        "W.-J Yan",
        "Y Ma",
        "Y Liu",
        "C Huang",
        "X Fu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence"
    },
    {
      "citation_id": "36",
      "title": "Automatic apex frame spotting in micro-expression database",
      "authors": [
        "S.-T Liong",
        "J See",
        "K Wong",
        "A Le Ngo",
        "Y.-H Oh",
        "R Phan"
      ],
      "year": "2015",
      "venue": "2015 3rd IAPR Asian conference on pattern recognition (ACPR)"
    },
    {
      "citation_id": "37",
      "title": "A fpga implementation of farneback optical flow by high-level synthesis, FPGA '19",
      "authors": [
        "C.-W Chang",
        "Z.-Q Zhong",
        "J.-J Liou"
      ],
      "year": "2019",
      "venue": "A fpga implementation of farneback optical flow by high-level synthesis, FPGA '19"
    },
    {
      "citation_id": "38",
      "title": "Capsulenet for micro-expression recognition",
      "authors": [
        "N Van Quang",
        "J Chun",
        "T Tokuyama"
      ],
      "year": "2019",
      "venue": "14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "39",
      "title": "Dual-inception network for cross-database micro-expression recognition",
      "authors": [
        "L Zhou",
        "Q Mao",
        "L Xue"
      ],
      "year": "2019",
      "venue": "14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "40",
      "title": "Ramesh Sankaranarayana received the ME degree from the Indian Institute of Science, India, and the PhD degree from the University of Alberta, Canada. He is currently the associate director",
      "authors": [
        "Cvpr Icml",
        "Iccv",
        "Eccv",
        "Aaai",
        "Acmmm Acl",
        "Iclr",
        "A Tpami",
        "Ijcv"
      ],
      "venue": "top-tier conferences and journals"
    }
  ]
}