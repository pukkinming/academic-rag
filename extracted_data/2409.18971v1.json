{
  "paper_id": "2409.18971v1",
  "title": "Early Joint Learning Of Emotion Information Makes Multimodal Model Understand You Better",
  "published": "2024-09-12T05:05:34Z",
  "authors": [
    "Mengying Ge",
    "Mingyang Li",
    "Dongkai Tang",
    "Pengbo Li",
    "Kuo Liu",
    "Shuhao Deng",
    "Songbai Pu",
    "Long Liu",
    "Yang Song",
    "Tao Zhang"
  ],
  "keywords": [
    "Emotion ViT",
    "Joint Audio-Text representation",
    "Data mining",
    "Audio denoise"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we present our solutions for emotion recognition in the sub-challenges of Multimodal Emotion Recognition Challenge (MER2024). For the tasks MER-SEMI and MER-NOISE, participants are required to recognize discrete emotions. Particularly, in MER-NOISE, the test videos are corrupted with noise, necessitating the consideration of modality robustness. We developed our Emotion ViT based on large-scale data pre-training and fine-tune, a vision feature extractor adapted to emotion recognition tasks. In addressing the modal competition between audio and text, we implemented an early fusion methodology underpinned by a large language model. This design facilitates full interaction between audio and text, thereby harmonizing their contributions. The joint Audio-Text representation can be late-fused with other features extracted from our specific unimodal encoders. To solve the problems of data insufficiency and class imbalance, we employed multiple rounds of multi-model voting for data mining. To ensure the high quality of audio features, we introduce a speech source separation method to denoise the audios.Our model secured 2nd in both MER2024-SEMI and MER2024-NOISE categories, affirming that the robustness and validity of our strategies in advancing the field of multimodal emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In the rapidly advancing field of human-machine interaction, emotion recognition stands as a pivotal bridge facilitating emotional communication between humans and machines. Previous works cues from facial expressions, vocal intonations, and textual content to deeply analyze the emotional states of individuals  [1] . Among these approaches, multimodal technology with its capability to integrate different modal information sources for comprehensive analysis, has garnered significant attention  [2]   [3] . However, the emotional complexity, scarcity of high-quality annotated data and inevitable environmental noise in practical applications pose substantial challenges to the accuracy and robustness of multimodal emotion recognition. Efficient and precise feature extractors have emerged as a crucial means to overcome these bottlenecks. Traditional handcrafted features and machine learning classifiers often falter when confronted with complex nonlinear relationships, while the rise of deep learning and large-scale pre-trained models has infused new vitality into the domain of emotion recognition, exemplified by models such as VideoMAE-large  [4] , CLIP-large  [5]  for vision, wav2vec2.0  [6] , HUBERT  [7]  for audio, and RoBERTa  [8] , Baichuan-13B  [9]  for text, which have all demonstrated remarkable performance. Although training of feature extractors and classifiers simultaneously can enhance results further, the large scale parameters in multimodal contexts typically entail prohibitive computational costs. Hence, a more efficient approach involves optimizing feature extractors during pre-training and subsequently freezing their parameters while only training fusion layers and classifier layers  [10] .\n\nWithin the realm of multimodal information processing, the information from different modalities complements one another yet may also contain redundancy or conflicts. Effectively fusing these multiple sources of information remains a core challenge in the field  [11]   [12] . Notably, late fusion strategies incorporating attention mechanisms have shown exceptional performance in semantic-level feature integration  [13] . Nonetheless, it is often observed in practice that even with improvements in individual modality models, the performance of fused models may fall short of expectations, potentially due to modality conflicts, overfitting of unimodal models, and an overreliance on a single modality at the expense of exploiting complementary advantages across modalities  [14] . Thus, exploring effective joint training strategies to optimize multimodal feature representations becomes imperative.\n\nThe MER-SEMI and MER-NOISE tracks specifically address the challenges of semi-supervised learning in multimodal emotion recognition and enhancing robustness under noisy conditions. The organizers have generously provided meticulously annotated highquality datasets and abundant unlabeled data resources, laying a solid foundation for participants and researchers to delve into these critical areas. Additionally, comprehensive research materials have been released, offering invaluable insights into understanding and addressing these challenges  [15] . Initially, extensive baseline experiments were conducted to systematically assess the performance of unimodal models in emotion recognition tasks, conclusively demonstrating the substantial performance boost from more powerful feature extractors. Subsequently, in pursuit of further exploiting the potential of multimodal information, feature-level late fusion strategies were attempted to consolidate features from different modalities and enhance overall recognition capabilities. However, reliance solely on late fusion strategies often fails to adequately address potential conflicts and redundancies among modalities, thereby limiting the optimization of fusion effectiveness. In response to this challenge, we propose a novel joint training framework, particularly focusing on early fusion exploration of speech and text modalities.\n\nThis framework incorporates attention mechanisms and employs a hybrid fusion approach to handle multimodal features with greater finesse, effectively alleviating modality conflicts, and fully leveraging the complementary strengths between modalities, ultimately realizing efficient and precise integration and recognition of multimodal emotion features. Our key contributions are summarized as follows:\n\nEmotion ViT Model: We have trained a Vision Transformer (ViT) model proficient in characterizing human emotional features.\n\nLeveraging self-supervised learning, this model efficiently harnesses vast amounts of unlabeled data during pre-training and, through subsequent fine-tuning on a broad range of emotional data, significantly enhances its capacity for emotion feature expression.\n\nAudio-Text Joint Training Architecture: To tackle potential expression conflicts in multimodal emotion recognition, we introduce an innovative joint training structure for speech and text. By adopting early fusion, this structure effectively integrates information from both speech and text modalities, circumventing information loss associated with late fusion and surpassing simplistic dual-modal late fusion methods in performance.\n\nNoise Robustness Enhance: Addressing the interference of noise in complex environments on emotion recognition performance, audio denoise strategies and optimizations to ASR system's noise resilience have been implemented. These measures collectively elevate the model's stability and recognition precision in noisy environments, thereby reinforcing its noise robustness.\n\nEfficient Utilization of Unlabeled Data: To fully exploit the potential of unlabeled data, we incorporate a cyclic boosting data mining method. Through iterative utilization of unlabeled data for model training, this approach significantly enhances model accuracy and generalization capabilities, achieving efficient utilization of unlabeled data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Framework Overview",
      "text": "As shown in Figure  1 , we employ audio, text, and vision encoders to extract features from input audio, text, and facial sequence data, respectively. These encoders have been pre-trained or fine-tuned on emotion datasets. Further, we introduce a novel joint Audio-Text modality feature extraction module, specifically designed to harness the synergistic information between audio and text, thereby enhancing the expressivity of cross modalities. Subsequently, we leverage the distinct characteristics of these extracted features by combining them based on their differential properties. A cross-modal attention mechanism is then employed to fuse these combined features, allowing the system to dynamically weigh and focus on the most salient aspects for emotion recognition across each modality. Each fusion branch yields its independent prediction. Finally, a tailored ensemble strategy is invoked to ascertain the ultimate predicted result.\n\nAs shown in formulas (1)-  (7)  , where ğ‘“ ğ‘¡ ğ‘– , ğ‘“ ğ‘ ğ‘– , and ğ‘“ ğ‘£ ğ‘– respectively represents the features extracted from text, audio, and visual modalities.\n\nğ‘“ ğ‘— ğ‘– indicates the early joint Audio-Text modality feature, and ğ‘“ * + ğ‘– signifies the fused features based on multimodal attention of different feature groups. Each ğ‘¦ ğ‘– signifies the prediction outcome from individual fusion branch. Å·ğ‘œ signifies the final sentiment prediction after our specific ensemble strategy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Unimodal Emotional Encoder",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Vit.",
      "text": "For MER2024, a large amount of labeled emotion data is not available. Considering this limitation, we adopt a self supervised training approach and propose EmotionViT, which mainly includes two parts:\n\nâ€¢ ViT-Base  [16] : We employ the MAE  [17]  self-supervision methodology for pre-training,utilizing ViT-Base as the backbone architecture. Our training dataset comprises a substantial 8 million images, including facial recognition, facial attributions, facial occlusion, and facial expression datasets.\n\nIn particular, we have trained the ViT from scratch without leveraging any publicly available pre-trained models, such as models pre-trained on ImageNet1K  [18] . We believe that although ImageNet1K covers most scenarios, facial expressions are the most important basis for emotion recognition. Therefore, our entire dataset for pre-training contains a large amount of facial data. After pre-trained, the ViT-Base model was fine-tuned on the labeled data of MER2024 and public emotion recognition datasets. Add a classification head to the vit base structure for training facial expression classification, and finally use the fine-tuned model to extract facial features as input for subsequent model fusion. During the process of creating data, we found that, The dataset for fine-tuning comes from the training set of MER2024, which consists of segments of video with video level labels instead of image level labels. For example, the annotated video is labeled as happy, but after the video is divided into frames, although the main expression is happy, not every frame is happy. Therefore, when this data is added to the training, it becomes a negative sample. Based on this, we innovatively distinguish the training data. For the source of a single image, we still use the original input method, as shown in Figure  2   â€¢ InternVIT-6B: In addition to self-supervised pre-training methods such as MAE, the CLIP leverages image-text pairs for contrastive learning, is widely adopted in multi-modal models as a visual encoder. For our visual feature fusion, we introduced vit-base pre-trained model and CLIP pre-trained model for feature extraction. The InternVL-chat-V1.5  [19] , outstanding on the opencompass multi-modal benchmark, employs InternVIT-6B, a visual encoder pre-trained on a large-scale, high-quality image-text dataset with a substantial 6 billion parameters, exhibiting exceptional feature representation capabilities. We enhanced InternVIT-6B with a classification head and fine-tuned it on labeled data of MER2024 and public emotion datasets. Given the large scale parameters of InternVIT-6B, we only trained the projector layers. In summary, EmotionViT mainly includes ViT-Base and InternViT-6B, which are pre trained and fintuned on a large amount of human centered data and integrated for use on the MER-SEMI and MER-NOISE tracks.Then the facial frames from a video is transformed into a sequence of high-dimensional video embeddings through Emotion ViT, denoted as",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Fine-Tuned Chinese_Hubert_Large.",
      "text": "To extract features from raw audio inputs, we employ a fine-tuned variant of the large-scale Chinese version of HuBERT_large. This model has been meticulously adapted to the MSA and Multimodal Emotional Recognition 2023 Audio Datasets, enhancing its capability to comprehend nuanced linguistic and emotional content in the Chinese language. The feature extraction process harnesses the concatenated hidden representations averaged over the last four layers of the Chinese_HuBERT_large architecture. Consequently, each audio input is transformed into a sequence of high-dimensional audio embeddings, denoted as F ğ‘ âˆˆ R ğ‘ ğ‘ Ã—1024 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fine-Tuned Baichuan13B_Chat.",
      "text": "We extract features for a text input by leveraging a large language model Baichuan13b_Chat, fine-tuned on 200w Chinese texts related to emotion which contains WeiBo, ECB and MER2023  [20]  Dataset, and use the averaged hidden representations across the last four layers of the LLM, resulting in a sequence of text representations F ğ‘¡ âˆˆ R ğ‘ ğ‘¡ Ã—5120 for each text input.  In most cases, the emotion expressed by the individual in videos can be discerned through their auditory signals, encompassing not only the words but also the tone. Emotion is a product of both textual content and acoustic characteristics. For multimodal models, the crux lies in effectively integrating these modalities-textual and vocal features. Our experimental findings revealed that while features extracted using audio encoder in Section 2.2.2 and text encoder in Section 2.2.3 performed exceptionally well in their respective unimodal settings, the fusion of these two modalities based on attention mechanisms resulted in slight improvement.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Joint Audio-Text Module",
      "text": "We posit that sometimes there are discrepancies between audio and text representations, bringing additional competition between modalities, which potentially leads to the model under-fitting. This suggests that relying solely on feature-level fusion of cross-modal vocal-textual data might be insufficient to fully harness their complementary strengths.\n\nTo this end, we have augmented the QwenAudio  [21]  framework with the inclusion of a text modality, implementing an early fusion strategy tailored for emotion-laden data. Upon training this joint audio-text framework, it evolves into a novel, unified feature extractor for audio-text modalities based on a Large Language Model, aimed at more accurately capturing and representing the intricate interplay of emotions conveyed through both audio and text. This approach endeavors to transcend the limitations of conventional multimodal fusion by fostering a deeper integration at the very onset of the processing pipeline, thereby enhancing the model's capacity to interpret emotional expressions in a multimodal context.\n\nAs shown in Figure  3 , The whisper-large-v2 model is used as Audio Encoder and the decoder-only Qwen-7B model is the Large Language Model(LLM). In our methodology, audio inputs are processed through an audio feature extractor to yield audio embeddings.\n\nConcurrently, the transcribed text from the audio is transformed into text embeddings. These embeddings, along with the prompt's embedding, are then concatenated and fed into the LLM for joint training across the audio and text modalities. The high-quality dataset comprising 5,000 samples from MER2023 serves as our training data, with a notable preprocessing step that filters out instances lacking sound in an audio. Full parameter fine-tuning is adopted as the training strategy, and our experiments affirm its superiority over the LoRA method.\n\nDespite the multitask-adapted QwenAudio model's inherent capability for speech recognition, it encounters inaccuracies when confronted with background noise or ambient sounds. Furthermore, the model's transcription output lacks punctuation, which is vital for effective emotion recognition in the textual modality as MERBench  [13]  paper asserts. Consequently, furnishing the model with the correct, punctuated text aligned with the audio not only compensates for this limitation but also preserves and does not interfere with the model's innate speech recognition functionality, thereby enhancing its overall performance in multimodal emotion recognition tasks. We use our own ASR model in Section 2.4.1 to extract text from audios and punctuate the text by CT-Transformer  [22]  model.\n\nAs a joint audio-text modality feature extractor, the last four layers of the large language model's decoder are employed to derive feature representations for each audio-text pair denoted as F ğ‘ğ‘¡ âˆˆ R ğ‘ ğ‘ğ‘¡ Ã—4096 .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Noise Robustness Enhance",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Noise-Robust Asr.",
      "text": "Our noise-robust ASR model is improved based on Paraformer  [23] . We optimized the ASR model from following three aspects.\n\nFirstly, we added noise to the training set of the ASR model for data augmentation. We employ the MUSAN  [24]  noise database as the environmental noise to expand the training set, with signal-tonoise ratios ranging from 5db to 12db. The MUSAN noise database contains various types of environmental noise, including laughter, music sounds, etc., which are very suitable for simulating noise situations in daily life. In addition, we also used the training set combined with room impulse response(RIR)  [25]  to produce background human voice noise to further augment our training set, with signal-to-noise ratios ranging from 10db to 30db. We train our ASR model in a 2:1:1 ratio of clean speech, ambient noise speech, and background human voice noise speech. Secondly, we use semisupervised method to further improve the robustness of the ASR model in noisy environments. Specifically, we use the ASR model to infer large-scale unlabeled noisy speech data, obtaining pseudo labels and beam search scores for each segment of speech. By calculating the average beam search score of each speech segment, we selected pseudo labeled data with high confidence for model self training. We iteratively trained the model until its performance did not improve. Finally, we further improved the performance of the ASR model by using a language model to avoid recognition errors. We trained an n-gram language model using a large amount of text data. Combining with Weighted Finite-State Transducers (WFST) decoding, we rescore the beams earch path of the ASR acoustic model to obtain text that is more in line with people's daily expressions.\n\nIn this way, the performance of our ASR model can be further improved and it also has strong robustness to noisy environments.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Speech Source Separation Denoise.",
      "text": "For multimodal emotion recognition, audio denoising can elevate model performance significantly. Audio noise is broadly categorized into background noise and vocal noise, with separate handling of each potentially complicating the process and risking compounded audio degradation through successive denoising steps. To this end, we propose a denoising method based on MossFormer2  [26] , an attention-based speech separation model capable of decomposing a noisy audio into two distinct monaural audio.\n\nThe workflow commences by subjecting the noisy audio to noiserobust ASR model(2.4) to obtain a template text, designated as ğ‘‡ ğ‘’ğ‘¥ğ‘¡ ğ‘¡ğ‘’ğ‘šğ‘ . Subsequently, the noisy audio is processed through Moss-Former2, yielding two separated audios. Each of the audios is then passed through the ASR model above to get transcripts ğ‘‡ ğ‘’ğ‘¥ğ‘¡ ğ‘–ğ‘‘0 and ğ‘‡ ğ‘’ğ‘¥ğ‘¡ ğ‘–ğ‘‘1 .\n\nA comparative analysis ensues, calculating the similarity between Text_id0 and Text_temp against Text_temp, quantified as ğ‘†ğ‘–ğ‘š ğ‘–ğ‘‘0 and ğ‘†ğ‘–ğ‘š ğ‘–ğ‘‘1 , respectively. If the absolute difference |ğ‘†ğ‘–ğ‘š ğ‘–ğ‘‘0 -ğ‘†ğ‘–ğ‘š ğ‘–ğ‘‘1 | exceeds 0.1, the audio corresponding to the higher similarity score is selected; otherwise, the original audio is the chosen one. This strategy ensures a reduction in audio degradation after denoised.\n\nThe adoption of a speech source separation denoising scheme effectively mitigates the detrimental impact of noise on audio features, thereby enhancing multimodal emotion recognition model's performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Mining And Model Ensemble",
      "text": "This study proposes an innovative semi-supervised learning framework based on difference, aimed at effectively mining high confidence pseudo labeled samples from a large unlabeled dataset ğ· ğ‘¢ , thereby promoting iterative improvement of model performance. Specifically, we first use a labeled dataset ğ· ğ‘™ to train an initial weak learner through supervised learning. In order to enhance the diversity and complementarity among learners, we carefully designed four multimodal fusion models ğ‘€ 1 , ğ‘€ 2 , ğ‘€ 3 , and ğ‘€ 4 with different feature combinations, and trained four independent weak learners ğ‘€ ğ‘™1 , ğ‘€ ğ‘™2 , ğ‘€ ğ‘™3 , and ğ‘€ ğ‘™4 based on these models.\n\nSubsequently, these four weak learners were synergistically applied to the unlabeled dataset ğ· ğ‘¢ to predict each unlabeled sample. In order to generate reliable pseudo labels, we adopted a majority voting strategy: only when the prediction results of at least three learners are completely consistent, the predicted label is adopted as the pseudo label of the sample, thus constructing the pseudo label dataset ğ· ğ‘¤ .\n\nNext, in order to fully utilize these newly generated pseudo labeled data, we evenly divide ğ· ğ‘¤ into four subsets and merge them with the original labeled dataset ğ· ğ‘™ to form four extended and differentiated labeled datasets ğ· ğ‘™ ğ‘¤1 , ğ· ğ‘™ ğ‘¤2 , ğ· ğ‘™ ğ‘¤3 , ğ· ğ‘™ ğ‘¤4 .\n\nSubsequently, the four extended datasets were used as training sets for supervised and refined training of models M1 to M4, resulting in four more powerful learners ğ‘€ ğ‘™ ğ‘¤1 , ğ‘€ ğ‘™ ğ‘¤2 , ğ‘€ ğ‘™ ğ‘¤3 , ğ‘€ ğ‘™ ğ‘¤4 . We used the updated learner to vote on the remaining unlabeled data to generate new pseudo labeled samples and repeat the training and enhancement steps above for N iterations. As the number of iterations increases, the model's performance improve gradually driven by the continuous enrichment of data.\n\nAfter N iterations, we obtained 4 strong learners. These learners adopt different strategies in feature extraction and different combinations of datasets during training, resulting in significant differences in model characteristics and knowledge representation.\n\nBased on four strong learners, we designed a multimodel ensemble strategy, as shown in Figure  4 . We rank four models from high to low according to the indicators on the test set, and then count the mode(most common categories) of the output results of multiple models. If the number of modes is 1, we choose the mode as the voting result, otherwise we ignore the last model's result, and repeat the above steps until the mode is unique.  The data details of MER-SEMI and MER-NOISE tracks are shown in Table  1 . This dataset includes 5030 carefully labeled data samples and 115595 unlabeled data samples, which together form the foundation of the training and validation stages. Participants can fully utilize this data to optimize and improve the performance of their models. In the testing phase, in order to encourage participants to pay more attention to the generalization ability of the model, it is required to make predictions on all submitted models on a sample set of 20000. However, for the MER-SEMI track, we only evaluated 1169 samples; For the MER-NOISE track, 1170 noisy samples were evaluated to comprehensively test the performance of the model in complex noise environments.\n\nWe have trained two vision feature extractors, based on VIT-Base and InternVIT-6B respectively. Training VIT-Base encoder includes pre-train and fine-tune stages. In pre-train stage, we collected CelebA  [27] , VoxCeleb2  [28] , LFW  [29] , wideface  [30] , FFHQ  [31] , IMDB-WIKI  [32] , CH-SIMS  [33] , CMU-MOSI  [34] , and RAF-DB  [35] datasets, with a total number of 9M. After filtering out blurry faces, incomplete faces, and large profile faces, the remaining 8M is used as our pre-train dataset. In the fine-tune stage, we use the labeled dataset ExpW  [36]  and RAF-DB dataset related to emotion and labeled data of MER2024. InternVIT-6B shares the same fine-tune data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "For ViT-Base, we choose mmpre-train[? ] framework for largescale pre-training and fine-tune it by adding a classification header. In the pre-train stage, the parameters are set as follows: input resolution 224, batch size 1024, initial learning rate 2e-4, maximum training rounds 300, mixed precision training enabled, gradient accumulation 4, and training on 8*A800. In the finuetune stage, the final learning is 2e-3, the maximum number of training rounds is 100, and the other settings are the same as pre-train stage.\n\nFor InternViT-6B, the experimental settings are as follows, batch size 512, input resolution 224, learning rate 2e-5, optimizer SGD, attenuation parameter 0.9, maximum training rounds 50. Table  2  shows the details of ViT-Base and InternViT-6B in finetuning. The model on the left is ViT Base, which uses the Adamw optimizer. \"single\" represents a single image as the input to the training set, as shown in Figure  2 (a) . \"Multi\" represents the use of multi image stitching as the input to the video data in the training set, as shown in Figure  2 (b) . Non video data still uses a single input. \"8classes\" represents 8 categories when fine-tuning, which are \"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\", and \"worried\". \"3classes\" represents 3 categories, which are \"positive\", \"negative\" and \"neutral\". The model on the right is the experimental result of fine-tuning InternViT-6B.  25    2 's left, which shows that the more classification categories, the more the model improves. In addition, it is also verified that multi-frames input can improve the model's video recognition capabilities.\n\nThe results show that full-parameter training has the worst effect, and only a few parameters of ğ¶ğ¿ğ¼ğ‘ƒ ğ‘ğ‘Ÿğ‘œ ğ‘— are released, which has the best effect on downstream tasks.\n\nIn addition, we conducted experiments to verify the performance of the joint Audio-Text feature extractor. The training dataset is labeled data over 3k in MER2023. The test dataset includes test dataset in MER2023's multi-label track and the test dataset in both tracks of MER2024. As shown in Table  3 , Audio is the audio feature extracted from fine-tuned ğ¶â„ğ‘–ğ‘›ğ‘’ğ‘ ğ‘’_ğ»ğ‘¢ğ‘ğ‘’ğ‘Ÿğ‘¡_ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’, Text refers to the text feature extracted from fine-tuned ğµğ‘ğ‘–ğ‘â„ğ‘¢ğ‘_13ğµ, Joint Audio-Text is the joint Audio-Text feature based on enhanced QwenAudio (ğ‘„ğ‘¤ğ‘’ğ‘›_ğ½ğ´ğ‘‡ ), and 'Denoise' represents denoising preprocessing.\n\nFrom the table, we can find that Joint Audio-Text feature fused with ğµğ‘ğ‘–ğ‘â„ğ‘¢ğ‘ğ‘›_13ğµ ğ‘“ ğ‘¡ and ğ¶â„ğ‘–ğ‘›ğ‘’ğ‘ ğ‘’_ğ»ğ‘¢ğ‘ğ‘’ğ‘Ÿğ‘¡_ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ ğ‘“ ğ‘¡ reached 0.8606 score in MER2024-semi track. Tri-feature fusion can maximize the performance of audio and text modality. In addition, using Joint Audio-Text features after audio denoising can simultaneously improve our model's performance in both tracks of MER2024.However, due to the reduction of the speech sampling rate after denoising (16k to 8k), the feature quality of ğ¶â„ğ‘–ğ‘›ğ‘’ğ‘ ğ‘’_ğ»ğ‘¢ğ‘ğ‘’ğ‘Ÿğ‘¡_ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ ğ‘“ ğ‘¡ (pretrained on audios sampling rate 16k) decreased, which cause the score in noise-track dropped after the tri-feature fusion. Next, we conduct features fusion experiments extracted from all our modal feature encoder based on MER2024 labeled dataset sized 5030. The Table  4  shows the comparison of different modality feature groups with the baseline in the multi-modal fusion model. It's clear that our fine-tuned audio encoder (ğ¶â„ğ‘–ğ‘›ğ‘’ğ‘ ğ‘’_ğ»ğ‘¢ğ‘ğ‘’ğ‘Ÿğ‘¡_ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ * ), fine-tuned text encoder (ğµğ‘ğ‘–ğ‘â„ğ‘¢ğ‘ğ‘›_13ğµ * ), ğ¸ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›ğ‘‰ ğ‘–ğ‘‡ (collective name for ğ‘£ğ‘–ğ‘¡_ğ‘ğ‘ğ‘ ğ‘’_ğ‘’ğ‘šğ‘œ and ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘›ğ‘£ğ‘–ğ‘¡_6ğµ_ğ‘’ğ‘šğ‘œ) and ğ‘„ğ‘¤ğ‘’ğ‘›_ğ½ğ´ğ‘‡ all have significantly improved the overall performance of multi-modal models respectively. Specifically, the encoder groups including ğµğ‘ğ‘–ğ‘â„ğ‘¢ğ‘ğ‘›_13ğµ * , ğ‘£ğ‘–ğ‘¡_ğ‘ğ‘ğ‘ ğ‘’_ğ‘’ğ‘šğ‘œ and ğ‘„ğ‘¤ğ‘’ğ‘›_ğ½ğ´ğ‘‡ increase the ğ‘Š ğ´ğ¹ by up to 3.41% compared to the baseline, which is a very considerable improvement.\n\nMoreover, our denoise preprocess method for audios before ğ‘„ğ‘¤ğ‘’ğ‘›_ğ½ğ´ğ‘‡ showed outstanding results on the MER2024-NOISE track, with a WAF increase of 2.9%, proving the effectiveness and robustness of our approach. What's even more gratifying is that in the MER2024-SEMI track, our denoise method also brings a 0.39% increasement.\n\nIn order to amplify the diversity among models, we designed four distinct feature combination schemes. Just as shown in Figure  5 , through multi-turns data mining , we elevated the Weighted Average F1-score (WAF) of the four individual models above 0.88, with the optimal combination approaching 0.89. Finally, by ensemble voting on these four models with remarkable differences, we effectively raised the WAF to 0.9001.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion And Limitations",
      "text": "In this paper, we present our solutions for emotion recognition in the sub-challenges of MER2024. Firstly, We launched our Emotion ViT based on large-scale data pre-training and fine-tune, a vision feature extractor adapted to emotion recognition tasks. To mitigate the modal competition issue between audio and text, we adopt an early fusion strategy based on a large language model, where joint training of audio and text is conducted initially. And the joint Audio-Text modal feature will be late-fused with other unimodal features. In order to solve the problems of data insufficiency and class imbalance, We use multiple turns of multi-model voting for data mining. Moreover, to enhance the quality of audio features, we employ speech source separation to preprocess audios. The efficacy of our methods has been demonstrated through outstanding performances in both MER2024-SEMI and MER2024-NOISE, achieving scores of 0.9001 and 0.8383 respectively.\n\nOur research into the competition among visual modality and other modalities is insufficient, and furthermore, the ambiguity inherent in Chinese text poses a particular challenge for emotion recognition. These issues above may need the utilization of large language models for resolution. In the future, we will investigate emotion recognition based on Multimodal Large Language Models.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Multi-Modal Emotion Recognition Framework",
      "page": 2
    },
    {
      "caption": "Figure 1: , we employ audio, text, and vision encoders",
      "page": 2
    },
    {
      "caption": "Figure 2: (a), while for the training data of a video, we change the",
      "page": 3
    },
    {
      "caption": "Figure 2: (b), and this modification can",
      "page": 3
    },
    {
      "caption": "Figure 2: ViT-Base fine-tune of different inputs",
      "page": 3
    },
    {
      "caption": "Figure 3: Illustration of Joint Audio-Text Module",
      "page": 4
    },
    {
      "caption": "Figure 3: , The whisper-large-v2 model is used as",
      "page": 4
    },
    {
      "caption": "Figure 4: We rank four models from",
      "page": 5
    },
    {
      "caption": "Figure 4: MuiltiModel Ensemble Strategy",
      "page": 5
    },
    {
      "caption": "Figure 2: (a). \"Multi\" represents the use of",
      "page": 6
    },
    {
      "caption": "Figure 2: (b). Non video data still uses a single",
      "page": 6
    },
    {
      "caption": "Figure 5: , through multi-turns data mining , we elevated the Weighted",
      "page": 7
    },
    {
      "caption": "Figure 5: The trend of model indicators with data mining",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Table 1: MER2024 Dataset Distribution",
      "page": 5
    },
    {
      "caption": "Table 2: Emotion ViT fine-tune",
      "page": 6
    },
    {
      "caption": "Table 2: shows the details of ViT-Base and InternViT-6B in fine-",
      "page": 6
    },
    {
      "caption": "Table 3: , Audio is the audio feature",
      "page": 6
    },
    {
      "caption": "Table 3: Comparison results around Joint Audio-Text modal-",
      "page": 6
    },
    {
      "caption": "Table 4: shows the comparison of different modality fea-",
      "page": 6
    },
    {
      "caption": "Table 4: Comparison results in MER2023 and MER2024 test",
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "Guimin Hu",
        "Ting-En Lin",
        "Yi Zhao",
        "Guangming Lu",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "year": "2022",
      "venue": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "arxiv": "arXiv:2211.11256"
    },
    {
      "citation_id": "3",
      "title": "Misa: Modalityinvariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "4",
      "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "authors": [
        "Zhan Tong",
        "Yibing Song",
        "Jue Wang",
        "Limin Wang"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "5",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "6",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "7",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed",
        "Hubert"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "8",
      "title": "A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov",
        "Roberta"
      ],
      "year": "2019",
      "venue": "A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "9",
      "title": "Open large-scale language models",
      "authors": [
        "Aiyuan Yang",
        "Bin Xiao",
        "Bingning Wang",
        "Borong Zhang",
        "Ce Bian",
        "Chenxu Chao Yin",
        "Da Lv",
        "Dian Pan",
        "Dong Wang",
        "Yan"
      ],
      "year": "2023",
      "venue": "Open large-scale language models",
      "arxiv": "arXiv:2309.10305"
    },
    {
      "citation_id": "10",
      "title": "Jointly fine-tuning\" bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "authors": [
        "Shamane Siriwardhana",
        "Andrew Reis"
      ],
      "year": "2020",
      "venue": "Jointly fine-tuning\" bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "arxiv": "arXiv:2008.06682"
    },
    {
      "citation_id": "11",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas BaltruÅ¡aitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "12",
      "title": "A systematic review on affective computing: Emotion models, databases, and recent advances",
      "authors": [
        "Yan Wang",
        "Wei Song",
        "Wei Tao",
        "Antonio Liotta",
        "Dawei Yang",
        "Xinlei Li",
        "Shuyong Gao",
        "Yixuan Sun",
        "Weifeng Ge",
        "Wei Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "13",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Yong Ren",
        "Hao Gu",
        "Haiyang Sun",
        "Lan Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "14",
      "title": "Modality competition: What makes joint training of multi-modal network fail in deep learning?(provably)",
      "authors": [
        "Yu Huang",
        "Junyang Lin",
        "Chang Zhou",
        "Hongxia Yang",
        "Longbo Huang"
      ],
      "year": "2022",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "15",
      "title": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Hao Gu",
        "Jinming Zhao",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "arxiv": "arXiv:2404.17113"
    },
    {
      "citation_id": "16",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "17",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr DollÃ¡r",
        "Ross Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "18",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "19",
      "title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks",
      "authors": [
        "Zhe Chen",
        "Jiannan Wu",
        "Wenhai Wang",
        "Weijie Su",
        "Guo Chen",
        "Sen Xing",
        "Muyan Zhong",
        "Qinglong Zhang",
        "Xizhou Zhu",
        "Lewei Lu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Multi-label learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Kang Chen",
        "Mngyu Xu",
        "Kexin Wang",
        "Ke Xu",
        "Yu He",
        "Ying Li",
        "Jinming Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "22",
      "title": "A fundamental end-to-end speech recognition toolkit",
      "authors": [
        "Zhifu Gao",
        "Zerui Li",
        "Jiaming Wang",
        "Haoneng Luo",
        "Xian Shi",
        "Mengzhe Chen",
        "Yabin Li",
        "Lingyun Zuo",
        "Zhihao Du",
        "Zhangyu Xiao"
      ],
      "year": "2023",
      "venue": "A fundamental end-to-end speech recognition toolkit",
      "arxiv": "arXiv:2305.11013"
    },
    {
      "citation_id": "23",
      "title": "Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition",
      "authors": [
        "Zhifu Gao",
        "Shiliang Zhang",
        "Ian Mcloughlin",
        "Zhijie Yan"
      ],
      "year": "2022",
      "venue": "Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition",
      "arxiv": "arXiv:2206.08317"
    },
    {
      "citation_id": "24",
      "title": "A Music, Speech, and Noise Corpus",
      "authors": [
        "David Snyder",
        "Guoguo Chen",
        "Daniel Povey",
        "Musan"
      ],
      "year": "2015",
      "venue": "A Music, Speech, and Noise Corpus",
      "arxiv": "arXiv:1510.08484v1"
    },
    {
      "citation_id": "25",
      "title": "Building and evaluation of a real room impulse response dataset",
      "authors": [
        "Igor SzÃ¶ke",
        "Miroslav SkÃ¡cel",
        "Ladislav MoÅ¡ner",
        "Jakub Paliesek",
        "Jan ÄŒernocká»³"
      ],
      "year": "2019",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Mossformer2: Combining transformer and rnn-free recurrent network for enhanced time-domain monaural speech separation",
      "authors": [
        "Shengkui Zhao",
        "Yukun Ma",
        "Chongjia Ni",
        "Chong Zhang",
        "Hao Wang",
        "Trung Hieu Nguyen",
        "Kun Zhou",
        "Jia Qi Yip",
        "Dianwen Ng",
        "Bin Ma"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "27",
      "title": "Deep learning face attributes in the wild",
      "authors": [
        "Ziwei Liu",
        "Ping Luo",
        "Xiaogang Wang",
        "Xiaoou Tang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "28",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "Son Joon",
        "Arsha Chung",
        "Andrew Nagrani",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "Voxceleb2: Deep speaker recognition",
      "arxiv": "arXiv:1806.05622"
    },
    {
      "citation_id": "29",
      "title": "Labeled faces in the wild: A database forstudying face recognition in unconstrained environments",
      "authors": [
        "Marwan Gary B Huang",
        "Tamara Mattar",
        "Eric Berg",
        "Learned-Miller"
      ],
      "year": "2008",
      "venue": "Workshop on faces in'Real-Life'Images: detection, alignment, and recognition"
    },
    {
      "citation_id": "30",
      "title": "Wider face: A face detection benchmark",
      "authors": [
        "Shuo Yang",
        "Ping Luo",
        "Chen-Change Loy",
        "Xiaoou Tang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "31",
      "title": "A style-based generator architecture for generative adversarial networks",
      "authors": [
        "Tero Karras",
        "Samuli Laine",
        "Timo Aila"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "32",
      "title": "Imdb-wiki-500k+ face images with age and gender labels",
      "authors": [
        "Radu Rasmus Rothe",
        "Timofte",
        "Gool"
      ],
      "year": "2015",
      "venue": "Imdb-wiki-500k+ face images with age and gender labels"
    },
    {
      "citation_id": "33",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Fanyang Meng",
        "Yilin Zhu",
        "Yixiao Ma",
        "Jiele Wu",
        "Jiyun Zou",
        "Kaicheng Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "34",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Prateek Vij",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "35",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "36",
      "title": "Learning social relation traits from face images",
      "authors": [
        "Zhanpeng Zhang",
        "Ping Luo",
        "Chen-Change Loy",
        "Xiaoou Tang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "37",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    }
  ]
}