{
  "paper_id": "2508.11889v1",
  "title": "In-Context Examples Matter: Improving Emotion Recognition In Conversation With Instruction Tuning",
  "published": "2025-08-16T03:23:48Z",
  "authors": [
    "Hui Ma",
    "Bo Zhang",
    "Jinpeng Hu",
    "Zenglin Shi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversation (ERC) aims to identify the emotion of each utterance in a conversation, playing a vital role in empathetic artificial intelligence. With the growing of large language models (LLMs), instruction tuning has emerged as a critical paradigm for ERC. Existing studies mainly focus on multi-stage instruction tuning, which first endows LLMs with speaker characteristics, and then conducts context-aware instruction tuning to comprehend emotional states. However, these methods inherently constrains the capacity to jointly capture the dynamic interaction between speaker characteristics and conversational context, resulting in weak alignment among speaker identity, contextual cues, and emotion states within a unified framework. In this paper, we propose InitERC, a simple yet effective one-stage In-context instruction tuning framework for ERC. InitERC adapts LLMs to learn speaker-contextemotion alignment from context examples via in-context instruction tuning. Specifically, InitERC comprises four components, i.e., demonstration pool construction, in-context example selection, prompt template design, and in-context instruction tuning. To explore the impact of in-context examples, we conduct a comprehensive study on three key factors: retrieval strategy, example ordering, and the number of examples. Extensive experiments on three widely used datasets demonstrate that our proposed InitERC achieves substantial improvements over the state-of-the-art baselines.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversation (ERC) is a fundamental task in natural language processing (NLP) that aims to identify the emotion of each utterance in a conversation. The task has been popularly explored due to its potential applications in recommendation  (Song et al. 2024) , social media analysis  (Brambilla et al. 2022) , and empathetic dialogue systems  (Zhou et al. 2020) . Unlike sentence-level emotion recognition  (Kim 2014; Deng et al. 2023; Zhang et al. 2024) , ERC necessitates modeling speaker characteristics and contextual dependencies, as both are essential for understanding emotional states within the hierarchical and interactive structure of conversations.\n\nPrevious works on ERC can be generally divided into two categories  (Li et al. 2023 ): sequence-based methods and graph-based methods. Sequence-based methods, such as Hi-Trans  (Li et al. 2020) , MVN  (Ma et al. 2022) , BERT-ERC  (Qin et al. 2023) , and SDT  (Ma et al. 2024) , typically model conversational utterances as temporal sequences and leverage recurrent neural networks (RNNs) or transformer-based architectures to capture contextual dependencies. Graphbased methods, including SKAIG  (Li et al. 2021) , DAG-ERC  (Shen et al. 2021b) , DualGATs  (Zhang, Chen, and Chen 2023) , and ESIHGNN  (Zha, Zhao, and Zhang 2024) , construct conversational graphs and employ graph neural networks (GNNs) to capture dependencies and interactions within the dialogue structure. Despite significantly enhancing emotion recognition by leveraging contextual and speaker-specific information, these methods heavily rely on manually crafted graph structures and intricate model designs.\n\nMore recently, the advent of large language models (LLMs) has marked a paradigm shift in NLP research. Pretrained on massive and diverse corpora, LLMs have demonstrated impressive performance on a wide range of NLP tasks  (Lin et al. 2024; Li et al. 2025b; Yuan et al. 2025 ), including ERC. A mainstream strategy for effectively adapting LLMs to ERC is instruction tuning, which integrates task-specific rules during fine-tuning to achieve significant performance improvements. BiosERC  (Xue et al. 2024)  extracts speaker biographical information using LLMs as supplementary knowledge to fine-tune LLMs. To enhance emotion understanding, InstructERC  (Lei et al. 2023 ) and LaERC-s  (Fu et al. 2025 ) adopt a two-stage instruction tuning framework. In the initial stage, speaker identification or characteristic instruction tuning is performed to endow LLMs with speaker characteristics. In the second stage, context-aware instruction tuning is conducted to comprehend emotional states within conversational contexts. However, these sequential methods inherently constrain the capacity to jointly capture the dynamic interplay between speaker characteristics and conversational context, leading to weak alignment among speaker identity, contextual cues, and emotion states.\n\nIn this paper, we propose InitERC, a simple yet effective one-stage in-context instruction tuning framework for ERC. InitERC enables LLMs to learn the alignment among speaker identity, conversational context, and emotional state from context examples via instruction tuning. Specifically, InitERC consists of four components: demonstration pool construction creates a candidate set of context-and speakeraware examples; in-context example selection retrieves the most relevant demonstration examples for each target input; prompt template design integrates the task description, target input, and selected in-context examples into a unified prompt format; and in-context instruction tuning adapts the LLM to capture speaker-context-emotion alignment through a single-stage instruction tuning with the constructed prompts. Furthermore, we conduct a comprehensive study on retrieval strategy, example ordering, and the number of examples to explore the effects of in-context examples, and we hope our findings will offer useful insights for future research on in-context instruction tuning for ERC. Our main contributions are summarized as follows:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work Emotion Recognition In Conversation",
      "text": "Conventional ERC studies are mainly divided into sequencebased methods and graph-based methods  (Li et al. 2023  With the development of LLMs, several works have explored their potential for ERC. BiosERC  (Xue et al. 2024)  fine-tunes LLMs using extracted speaker biographical information as supplementary knowledge. InstructERC  (Lei et al. 2023 ) and LaERC-S  (Fu et al. 2025 ) adopt a two-stage instruction tuning framework, where first stage equips LLMs with speaker-specific characteristics, followed by context-aware instruction tuning to interpret emotional states. However, such sequential frameworks limit the ability to jointly model the dynamic interaction between speaker characteristics and conversational context, leading to suboptimal alignment among speaker identity, contextual cues, and emotional states. In this paper, we propose InitERC, a one-stage instruction tuning framework, to learn speakercontext-emotion alignment from context examples via instruction tuning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Instruction Tuning",
      "text": "Instruction tuning  (Shengyu et al. 2023) , also known as supervised fine-tuning, is an effective technique for enabling LLMs to follow natural language instructions and perform a wide range of tasks. It involves further training LLMs to generate certain outputs given instruction prompts. Recent studies, including InstructionGPT  (Ouyang et al. 2022) , OPT-IML  (Iyer et al. 2022) , and FLAN  (Chung et al. 2024) , have systematically explored instruction tuning methods to enhance the capabilities and controllability of LLMs. In addition to serving as generalists, instruction tuning can also function as specialists  (Lee et al. 2024) , excelling in specific tasks rather than achieving proficiency across a broad range of tasks. For example,  Chen et al. (2024)  employed a modified LLaMA model with instruction tuning to boost not only the accuracy of emotion recognition but also the depth of emotional reasoning.  Jian et al. (2025)  addressed the aspect sentiment quad prediction task by designing prompts with syntactic and semantically similar demonstrations. This word constructs effective instruction prompt templates and focuses on instruction tuning for ERC task.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "In-Context Learning",
      "text": "In-context learning (ICL)  (Dong et al. 2024)  has recently gained increasing attention as a novel training-free paradigm for enhancing the performance of LLMs. The approach enables LLMs to perform diverse tasks using a few inputoutput examples as demonstrations, without requiring retraining or fine-tuning. For instance, GPT-3  (Brown et al. 2020)  demonstrates competitive performance on unseen tasks when provided with few-shot examples as input prefixes, achieving results comparable to fine-tuned models. To further improve the effectiveness of ICL, some studies have investigated integrating it with fine-tuning.  Chen et al. (2022)  proposed in-context tuning, which metatrains language models with a few examples. DiSTRICT  (Venkateswaran, Duesterwald, and Isahagian 2023)   corresponding speaker. The ERC task aims to predict the emotion label y t of each utterance u t based on its historical context c t = (s u1 , u 1 ) , (s u1 , u 2 ) , • • • , s ut-1 , u t-1 and its speaker s ut . The emotion label belongs to the predefined emotion category set {e 1 , e 2 , • • • , e M } and M is the number of emotion categories.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Initerc Overview",
      "text": "The architecture of our proposed InitERC framework is shown in Figure  1",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Demonstration Pool Construction",
      "text": "Given an ERC dataset, we construct a demonstration pool P using its training set. In ERC, each utterance's emotional state is not only influenced by its content but also highly dependent on the preceding conversational context and the identity of the speaker. The historical context provides essential background information, such as conversational flow and emotional dependencies, while speaker identity helps to understand personalized emotional expressions and discourse patterns. Therefore, it is critical that each demonstration example includes the conversational history, current utterance and its speaker identity to ensure context-and speaker-aware emotion recognition. Formally, the demonstration pool is defined as\n\n, where c i is the historical context of utterance u i , x i = (s ui , u i ) contains utterance u i and its speaker s ui , y i is the corresponding emotion label of utterance u i , and K is the total number of examples. This carefully constructed demonstration pool serves as the candidate set for subsequent in-context example selection, ensuring that each example provides rich contextual and speaker-specific cues to guide instruction tuning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "In-Context Example Selection",
      "text": "For a given target input comprising the query utterance u t and its speaker s ut , denotes as x t , along with its historical context c t , we employ the off-the-shelf retriever to select the k most relevant examples P e = {(c i , x i , y i )} k i=1 from the demonstration pool P:\n\nwhere ⊕ denotes the concatenation operation.\n\nIn this work, we adopt Contriever-MS MARCO (Izacard et al. 2022) as our retriever that is a dual transformer encoder architecture and calculates the similarity between target input and candidates via dot product. Contriever-MS MARCO is pre-trained using MoCo contrastive loss  (He et al. 2020 ) on unsupervised data and further fine-tuned on the MS MARCO dataset.\n\nDuring retrieval, we exclude from the demonstration pool any examples originating from the same dialogue as the query utterance to prevent information leakage. Moreover, emotion labels are not used during retrieval and are incorporated afterward when constructing the final in-context examples, thereby ensuring that retrieval remains label-agnostic and mitigating potential bias toward specific emotional categories.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Prompt Template Design",
      "text": "Prompt template serves as a critical bridge between LLMs and the ERC task. In our InitERC framework, the instruction prompt not only defines the task in a clear and explicit manner but also effectively guides the LLM to align speaker identity, conversational context, and emotional state within a unified framework. The instruction prompt template comprises three parts, i.e., task description T , target input I, and in-context examples E.\n\n• Task Description T provides explicit instructions that define the objective of the task and the expected output format. The task description T is designed as:\n\nYou are an expert in emotion recognition in conversations. Given an utterance and its conversation history (if available), classify the emotion of the utterance based on the history. Your output must be exactly one of the following categories: {Candidate labels}.\n\nPlease output only the label without any other text.\n\n• Input I embeds the recognized speaker-aware utterance x t = (s ut , u t ) into the prompt together with its historical context c t , so that LLMs can leverage both \"who said what\" and \"what was said before\". The target input is defined as:\n\n, and we adopt a similarity-descending order, i.e., similar-first, for example ordering. The incontext examples is constructed as:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "In-Context Instruction Tuning",
      "text": "To overcome the limitations of multi-stage tuning and enable unified learning of speaker traits, contextual cues, and emotion states, we introduce in-context instruction tuning.\n\nwhere θ denotes the trainable parameters in LLM.\n\nInitERC enables LLMs learn the speaker-context-emotion alignment from context examples in a single learning stage. During inference, the demonstration examples for the recognized utterance are also selected from the constructed demonstration pool.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Settings Datasets And Evaluation Metrics",
      "text": "We evaluate our framework on three benchmark datasets. Statistics of these datasets are listed in Table  1 .\n\n• IEMOCAP  (Busso et al. 2008 ) consists of dyadic conversations between pairs of ten speakers, containing 153 conversations and 7, 433 utterances. Each utterance is labeled as one of six emotion categories: happy, sad, neutral, angry, excited, and frustrated. • MELD  (Poria et al. 2019 ) is a multiparty conversation dataset from Friends TV series, containing 1, 433 conversations, 13, 708 utterances. Each utterance is labeled as one of seven emotion categories: anger, disgust, fear, joy, neutral, sadness, and surprise. • EmoryNLP (Zahiri and Choi 2018) is also collected from Friends TV series, containing 97 episodes, 897 scenes, and 12, 606 utterances. Each utterance is annotated as one of seven emotion types: neutral, joyful, peaceful, powerful, scared, mad, and sad.\n\nFollowing prior studies (Zhang, Chen, and Chen 2023;  Qin et al. 2023; Fu et al. 2025) , we use the weighted average F1 score (w-F1) to evaluate performance on all three datasets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "We implement the proposed InitERC using LLaMA-Factory framework  (Zheng et al. 2024) . In this paper, we adopt LLaMA3.1-8B-Instruct  (Grattafiori et al. 2024)  as our LLM backbone. For in-context example selection, we set the number of examples k to 5. To enable parameter-efficient fine-tuning, we apply LoRA  (Hu et al. 2022 ) by inserting low-rank adapters in all linear layers, with the rank set to 8.\n\nWe employ AdamW optimizer  (Kingma and Ba 2015) , with the learning rate of 1e-4, and the batch size of 8. The maximum input length is set to 2048 for IEMOCAP, and 1024 for both MELD and EmoryNLP datasets. During inference, we use greedy decoding strategy to predict emotion labels. All experiments are conducted on a single NVIDIA RTX 4090 GPU.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baseline Methods",
      "text": "To demonstrate the effectiveness of our InitERC framework, we compare InitERC with a range of conventional and LLMbased ERC baselines. For conventional ERC methods, we adopt both graph-and sequence-based methods. The graphbased methods include SKAIG  (Li et al. 2021) , DAG-ERC  (Shen et al. 2021b) , and DualGATs (Zhang, Chen, and Chen 2023), which leverage psychological-knowledge-aware interaction graphs, directed acyclic graph structures, or dual graph attention networks to recognize emotion in conversation. The sequence-based methods include HiTrans  (Li et al. 2020) , DialogXL  (Shen et al. 2021a) , SPCL+CL  (Song et al. 2022) , and BERT-ERC  (Qin et al. 2023) , which utilize hierarchical transformers, memory-augmented attention, contrastive learning, and pretrained language models to capture sequential and structural information within dialogues. For LLM-based methods, we compare with ChatGPT  (Zhao et al. 2023 ) using 3-shot prompting, InstructERC  (Lei et al. 2023)  with two-stage instruction tuning, BiosERC  (Xue et al. 2024 ) which integrates speaker biographical knowledge to fine-tune LLMs, and LaERC-S  (Fu et al. 2025 ) that fine-tunes LLaMA2-7B using a two-stage learning to capture emotion dynamics.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Main Results",
      "text": "Table  2  reports the performance of our InitERC and other baseline methods on three datasets. Given that current LLMbased approaches utilize LLaMA2-7B as their backbone, we also adopt this LLM in InitERC for fair and consistent comparison. From the results, we have the following observations: (1) Methods based on instruction-tuned LLMs, such as BiosERC, InstructERC, and LaERC-S, outperform conventional graph-based models (e.g., DAG-ERC, Dual-GATs) and sequence-based models (e.g., DialogXL, Hi-Trans). This indicates the superior capacity of instruction tuning in modeling conversational dynamics compared to conventional architectures. (2) ChatGPT exhibits a notable performance gap compared to fine-tuned LLMs and even performs worse than most traditional methods. These results highlight that without fine-tuning, general-purpose LLMs are less capable of modeling the nuanced emotional dynamics required for ERC. (3) Our InitERC framework achieves new state-of-the-art performance, surpassing the strongest baseline (LaERC-S) by 20.25%, 9.44%, and 14.42% on IEMOCAP, MELD, and EmoryNLP respectively. These significant improvements validate our one-stage in-context instruction-tuning framework can capture the complex interplay between speaker characteristics, dialogue history, and emotional dynamics more effectively. (4) Even when adopting LLaMA2-7B as backbone, InitERC still demonstrates strong performance, outperforming all other baselines. This validates the rationality and superiority of our framework in harnessing LLMs potential for specialized ERC tasks.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct an ablation study to investigate the impact of in-context examples and instruction tuning in our proposed InitERC framework.  Figure  2 : Cross-dataset experimental results on three datasets. \"Single\" and \"Mix\" denote training on a single and mixed dataset, respectively. For both settings, we select data from each dataset at proportions of 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, and 1.\n\nthree datasets. This demonstrates that fine-tuning alone cannot adequately learn the alignment among speaker characteristics, conversational context, and emotion states without demonstration examples. Overall, the results clearly demonstrate that InitERC-which jointly incorporates in-context examples and instruction tuning-significantly outperforms all ablation variants, which validates the design rationality of our framework.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cross-Dataset Robustness Analysis",
      "text": "To validate our InitERC's robustness capability, we perform cross-dataset evaluation experiments. First, we sample equal proportions from the training sets of IEMOCAP, MELD, and EmoryNLP, and merge them into a mixed dataset. We then fine-tune InitERC on the mixed dataset and inference separately on the test sets of the three original datasets. For comparison, we also report the results of InitERC when both training and inference on the original datasets. Following previous work  (Lei et al. 2023; Fu et al. 2025) , we map the emotion labels of the three datasets into a unified label space before mixing to ensure label consistency across datasets. Figure  2  presents cross-dataset experimental results on three datasets. We can observe that InitERC generally benefits from training on the mixed dataset under lowresource conditions (≤ 1/8), demonstrating robust generalization when data is scarce due to the supply of complementary conversational cues. Moreover, as the training size increases, the gap between InitERC trained on mixedand single-dataset narrows on MELD and EmoryNLP, yet the mix-trained InitERC remains competitive, indicating its robustness across varying data scales and heterogeneous distributions. In contrast, mixed training underperforms single-dataset training on IEMOCAP at 1/2 and full data scales, likely stems from the mismatch between IEMO-CAP's dyadic, acted dialogues and the multi-party interactions in MELD and EmoryNLP. Overall, InitERC shows strong cross-dataset robustness, particularly in low-resource scenarios and on structurally compatible datasets. However, when the target domain has distinctive discourse structures, indiscriminate mixing can undermine robustness once sufficient in-domain data is available.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Generalization Analysis Across Llms",
      "text": "To evaluate the generalization capability of InitERC across different LLMs, we conduct experiments with another open source LLM, namely Qwen2.5-7B-Instruct  (Yang et al. 2025) . Furthermore, we instruction-tune each LLM using LoRA without any in-context examples, serving as comparison baselines.    , we can observe that across all three datasets, as k increases, the performance of InitERC generally shows an upward trend, although the rate of improvement gradually saturates. This result demonstrates that increasing the number of in-context examples contributes positively to our InitERC performance, particularly in the early phase, but with diminishing returns beyond a certain point. This finding highlights the practical trade-off between computational efficiency and performance gains when scaling in-context example size, and further implies that a moderate number of well-retrieved examples is sufficient for effective instruction tuning in ERC.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose InitERC, a one-stage instruction tuning framework for ERC that enables LLMs to learn the alignment among speaker identity, conversational context, and emotional state from in-context examples. InitERC contains demonstration pool construction, in-context example selection, prompt template design, and in-context instruction tuning: it first creates a candidate set of examples, retrieves the most relevant demonstrations, integrates the task description, target input, and selected examples into a unified prompt format, and finally fine-tunes LLMs using the constructed instruction prompts. Experimental results on three benchmark datasets show that the proposed framework outperforms both conventional and LLM-based ERC baselines, demonstrating the effectiveness of one-stage in-context instruction tuning. Moreover, our comprehensive analysis of retrieval strategy, example ordering, and the number of incontext examples provides useful suggestions for optimizing example selection in InitERC. We hope these findings can offer some insights for future research on in-context instruction tuning for ERC.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Framework of the proposed InitERC.",
      "page": 3
    },
    {
      "caption": "Figure 1: InitERC integrates four components:",
      "page": 3
    },
    {
      "caption": "Figure 2: Cross-dataset experimental results on three datasets. “Single” and “Mix” denote training on a single and mixed dataset,",
      "page": 6
    },
    {
      "caption": "Figure 2: presents cross-dataset experimental results on",
      "page": 6
    },
    {
      "caption": "Figure 3: Impact of in-context example ordering on InitERC",
      "page": 7
    },
    {
      "caption": "Figure 4: Impact of number of in-context examples on",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Task Description": "Target Input"
        },
        {
          "Task Description": "In-Context Examples"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Task Description"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Target Input"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 5: Impact of retrieval strategy on InitERC perfor-",
      "data": [
        {
          "Column_1": "",
          "IE M O C A P\nM E L D": "E m oryN L P\n81.0",
          "Column_3": "86.\n3",
          "90.": "89",
          "92.\n87": "",
          "65 92.": "",
          "79": ""
        },
        {
          "Column_1": "73.",
          "IE M O C A P\nM E L D": "14",
          "Column_3": "72.",
          "90.": "76.\n99",
          "92.\n87": "78.\n88",
          "65 92.": "71 79.",
          "79": "33"
        },
        {
          "Column_1": "69.",
          "IE M O C A P\nM E L D": "88 70.0",
          "Column_3": "7",
          "90.": "",
          "92.\n87": "",
          "65 92.": "",
          "79": ""
        },
        {
          "Column_1": "",
          "IE M O C A P\nM E L D": "50.2",
          "Column_3": "53.\n0",
          "90.": "55.\n46",
          "92.\n87": "68 56.",
          "65 92.": "50 56.",
          "79": "89"
        },
        {
          "Column_1": "40.",
          "IE M O C A P\nM E L D": "87",
          "Column_3": "",
          "90.": "",
          "92.\n87": "",
          "65 92.": "",
          "79": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: , the per-",
      "data": [
        {
          "S im ilar-F irst\nS im ilar-L ast\n92.65 92.42 92.09 R andom": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "78.71 77.52 77.98",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "S im ilar-F irst\nS im ilar-L ast": ""
        },
        {
          "S im ilar-F irst\nS im ilar-L ast\n92.65 92.42 92.09 R andom": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "56.50 55.26 56.23",
          "Column_10": "",
          "S im ilar-F irst\nS im ilar-L ast": ""
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Graph-based conversation analysis in social media",
      "authors": [
        "M Brambilla",
        "A Javadian Sabet",
        "K Kharmale",
        "A Sulistiawati"
      ],
      "year": "2022",
      "venue": "Big Data and Cognitive Computing"
    },
    {
      "citation_id": "2",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "3",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "4",
      "title": "Meta-learning via Language Model In-context Tuning",
      "authors": [
        "Y Chen",
        "R Zhong",
        "S Zha",
        "G Karypis",
        "H He"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "5",
      "title": "ReactGPT: Understanding of Chemical Reactions via In-Context Tuning",
      "authors": [
        "Z Chen",
        "Z Fang",
        "W Tian",
        "Z Long",
        "C Sun",
        "Y Chen",
        "H Yuan",
        "H Li",
        "M Lan"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Z Cheng",
        "Z.-Q Cheng",
        "J.-Y He",
        "K Wang",
        "Y Lin",
        "Z Lian",
        "X Peng",
        "A Hauptmann"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "H Chung",
        "L Hou",
        "S Longpre",
        "B Zoph",
        "Y Tay",
        "W Fedus",
        "Y Li",
        "X Wang",
        "M Dehghani",
        "S Brahma"
      ],
      "year": "2024",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "8",
      "title": "Llms to the moon? reddit market sentiment analysis with large language models",
      "authors": [
        "X Deng",
        "V Bashlovkina",
        "F Han",
        "S Baumgartner",
        "M Bendersky"
      ],
      "year": "2023",
      "venue": "Companion Proceedings of the ACM Web Conference 2023"
    },
    {
      "citation_id": "9",
      "title": "A Survey on Incontext Learning",
      "authors": [
        "Q Dong",
        "L Li",
        "D Dai",
        "C Zheng",
        "J Ma",
        "R Li",
        "H Xia",
        "J Xu",
        "Z Wu",
        "B Chang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "10",
      "title": "LaERC-S: Improving LLM-based Emotion Recognition in Conversation with Speaker Characteristics",
      "authors": [
        "Y Fu",
        "J Wu",
        "Z Wang",
        "M Zhang",
        "L Shan",
        "Y Wu",
        "B Liu"
      ],
      "year": "2025",
      "venue": "Proceedings of the 31st International Conference on Computational Linguistics"
    },
    {
      "citation_id": "11",
      "title": "The llama 3 herd of models",
      "authors": [
        "A Grattafiori",
        "A Dubey",
        "A Jauhri",
        "A Pandey",
        "A Kadian",
        "A Al-Dahle",
        "A Letman",
        "A Mathur",
        "A Schelten",
        "A Vaughan"
      ],
      "year": "2024",
      "venue": "The llama 3 herd of models",
      "arxiv": "arXiv:2407.21783"
    },
    {
      "citation_id": "12",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "K He",
        "H Fan",
        "Y Wu",
        "S Xie",
        "R Girshick"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": [
        "E Hu",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "14",
      "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "authors": [
        "S Iyer",
        "X Lin",
        "R Pasunuru",
        "T Mihaylov",
        "D Simig",
        "P Yu",
        "K Shuster",
        "T Wang",
        "Q Liu",
        "P Koura"
      ],
      "year": "2022",
      "venue": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "arxiv": "arXiv:2212.12017"
    },
    {
      "citation_id": "15",
      "title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
      "authors": [
        "G Izacard",
        "M Caron",
        "L Hosseini",
        "S Riedel",
        "P Bojanowski",
        "A Joulin",
        "E Grave"
      ],
      "year": "2022",
      "venue": "Transactions on Machine Learning Research"
    },
    {
      "citation_id": "16",
      "title": "SimRP: Syntactic and Semantic Similarity Retrieval Prompting Enhances Aspect Sentiment Quad Prediction",
      "authors": [
        "Z Jian",
        "Y Chen",
        "J Li",
        "S Wang",
        "X Zeng",
        "J Yao",
        "X An",
        "Q Wu"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Convolutional Neural Networks for Sentence Classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "19",
      "title": "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks",
      "authors": [
        "C Lee",
        "J Han",
        "S Ye",
        "S Choi",
        "H Lee",
        "K Bae"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Instructerc: Reforming emotion recognition in conversation with multi-task retrieval-augmented large language models",
      "authors": [
        "S Lei",
        "G Dong",
        "X Wang",
        "K Wang",
        "R Qiao",
        "S Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with multi-task retrieval-augmented large language models",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "21",
      "title": "HiTrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "J Li",
        "D Ji",
        "F Li",
        "M Zhang",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "22",
      "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "J Li",
        "Z Lin",
        "P Fu",
        "W Wang"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "23",
      "title": "Large Language Models are in-Context Molecule Learners",
      "authors": [
        "J Li",
        "W Liu",
        "Z Ding",
        "W Fan",
        "Y Li"
      ],
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "24",
      "title": "Exploring Model Editing for LLM-based Aspect-Based Sentiment Classification",
      "authors": [
        "S Li",
        "Z Wang",
        "Z Zhao",
        "Y Zhang",
        "P Li"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "SKIER: A symbolic knowledge integrated model for conversational emotion recognition",
      "authors": [
        "W Li",
        "L Zhu",
        "R Mao",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "26",
      "title": "Data-efficient Fine-tuning for LLM-based Recommendation",
      "authors": [
        "X Lin",
        "W Wang",
        "Y Li",
        "S Yang",
        "F Feng",
        "Y Wei",
        "T.-S Chua"
      ],
      "year": "2024",
      "venue": "Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "27",
      "title": "A multi-view network for real-time emotion recognition in conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "X Pan",
        "Y Zhang",
        "Z Yang"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "28",
      "title": "A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "B Zhang",
        "Y Zhang",
        "B Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "30",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "31",
      "title": "Bert-erc: Fine-tuning bert is enough for emotion recognition in conversation",
      "authors": [
        "X Qin",
        "Z Wu",
        "T Zhang",
        "Y Li",
        "J Luan",
        "B Wang",
        "L Wang",
        "J Cui"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "32",
      "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
      "authors": [
        "N Reimers",
        "I Gurevych"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "33",
      "title": "The probabilistic relevance framework: BM25 and beyond",
      "authors": [
        "S Robertson",
        "H Zaragoza"
      ],
      "year": "2009",
      "venue": "Foundations and Trends® in Information Retrieval"
    },
    {
      "citation_id": "34",
      "title": "2021a. DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion Recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "35",
      "title": "2021b. Directed Acyclic Graph Network for Conversational Emotion Recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "36",
      "title": "Instruction tuning for large language models: A survey",
      "authors": [
        "Z Shengyu",
        "D Linfeng",
        "L Xiaoya",
        "Z Sen",
        "S Xiaofei",
        "W Shuhe",
        "L Jiwei",
        "R Hu",
        "Z Tianwei",
        "F Wu"
      ],
      "year": "2023",
      "venue": "Instruction tuning for large language models: A survey",
      "arxiv": "arXiv:2308.10792"
    },
    {
      "citation_id": "37",
      "title": "Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation",
      "authors": [
        "X Song",
        "L Huang",
        "S Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "38",
      "title": "Cagk: Collaborative aspect graph enhanced knowledge-based recommendation",
      "authors": [
        "X Song",
        "H Lin",
        "J Zhu",
        "X Gong"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024"
    },
    {
      "citation_id": "39",
      "title": "DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning",
      "authors": [
        "P Venkateswaran",
        "E Duesterwald",
        "V Isahagian"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "40",
      "title": "BiosERC: Integrating Biography Speakers Supported by LLMs for ERC Tasks",
      "authors": [
        "J Xue",
        "M.-P Nguyen",
        "B Matheny",
        "L.-M Nguyen"
      ],
      "year": "2024",
      "venue": "International Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "41",
      "title": "Qwen3 technical report",
      "authors": [
        "A Yang",
        "A Li",
        "B Yang",
        "B Zhang",
        "B Hui",
        "B Zheng",
        "B Yu",
        "C Gao",
        "C Huang",
        "C Lv"
      ],
      "year": "2025",
      "venue": "Qwen3 technical report",
      "arxiv": "arXiv:2505.09388"
    },
    {
      "citation_id": "42",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "43",
      "title": "Query Understanding in LLM-based Conversational Information Seeking",
      "authors": [
        "Y Yuan",
        "Z Abbasiantaeb",
        "M Aliannejadi",
        "Y Deng"
      ],
      "year": "2025",
      "venue": "Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "44",
      "title": "Emotion Detection on TV Show Transcripts with Sequence-Based Convolutional Neural Networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "Workshops at the AAAI Conference on Artifi-cial Intelligence"
    },
    {
      "citation_id": "45",
      "title": "Esihgnn: Event-state interactions infused heterogeneous graph neural network for conversational emotion recognition",
      "authors": [
        "X Zha",
        "H Zhao",
        "Z Zhang"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "DualGATs: Dual Graph Attention Networks for Emotion Recognition in Conversations",
      "authors": [
        "D Zhang",
        "F Chen",
        "X Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "47",
      "title": "Sentiment Analysis in the Era of Large Language Models: A Reality Check",
      "authors": [
        "W Zhang",
        "Y Deng",
        "B Liu",
        "S Pan",
        "L Bing"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: NAACL 2024"
    },
    {
      "citation_id": "48",
      "title": "LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu",
        "S Wang",
        "Y Tong",
        "B Qin",
        "Y Zheng",
        "R Zhang",
        "J Zhang",
        "Y Yeyanhan",
        "Z Luo"
      ],
      "year": "2023",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
      "arxiv": "arXiv:2304.09582"
    },
    {
      "citation_id": "49",
      "title": "The design and implementation of xiaoice, an empathetic social chatbot",
      "authors": [
        "L Zhou",
        "J Gao",
        "D Li",
        "H.-Y Shum"
      ],
      "year": "2020",
      "venue": "Computational Linguistics"
    }
  ]
}