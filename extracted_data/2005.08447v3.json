{
  "paper_id": "2005.08447v3",
  "title": "Augmenting Generative Adversarial Networks For Speech Emotion Recognition",
  "published": "2020-05-18T04:10:12Z",
  "authors": [
    "Siddique Latif",
    "Muhammad Asim",
    "Rajib Rana",
    "Sara Khalifa",
    "Raja Jurdak",
    "Björn W. Schuller"
  ],
  "keywords": [
    "speech emotion recognition",
    "mixup",
    "data augmentation",
    "generative adversarial networks",
    "feature learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Generative adversarial networks (GANs) have shown potential in learning emotional a ributes and generating new data samples. However, their performance is usually hindered by the unavailability of larger speech emotion recognition (SER) data. In this work, we propose a framework that utilises the mixup data augmentation scheme to augment the GAN in feature learning and generation. To show the e ectiveness of the proposed framework, we present results for SER on (i) synthetic feature vectors, (ii) augmentation of the training data with synthetic features, (iii) encoded features in compressed representation. Our results show that the proposed framework can e ectively learn compressed emotional representations as well as it can generate synthetic samples that help improve performance in within-corpus and cross-corpus evaluation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is an active area of research with potential applications in healthcare  [1] , call centres  [2] , and designing naturalistic voice-based human-computer interfaces  [3] . Despite signi cant progress in machine learning, the performance of state-of-the-art SER systems is quite low. Data scarcity is one of the major reasons in this eld  [4] . Available SER datasets are relatively small in size compared to other speech-related applications such as speaker identi cation and speech recognition  [3] .\n\nis limits the performance of SER systems by causing the curse of the dimensionality problem  [5] . Dimensionality reduction techniques are considered as a popular solution to resolve this issue  [6] . However, features extracted in low dimension using these techniques are not always guaranteed to provide the best performance in SER  [7] .\n\nAnother promising approach is to generate synthetic samples using generative models for augmentation of training data. Generative adversarial networks (GANs)  [8]  have gained a lot of a ention in the machine learning (ML) community due to their ability to learn and mimic data distributions. ey have shown great performance in image generation  [9] , image translation  [10] , and enhancement  [11] , and also in speech generation  [12]  and conversion  [13] . However, the lack of availability of larger labelled datasets causes convergence issues in vanilla GANs while generating the synthetic feature vector to augment SER systems  [14] . To solve this issue, we propose to use a data augmentation technique combined with a GAN to improve the generation of synthetic samples. Particularly, we utilise a recently proposed data augmentation technique called \"mixup\"  [15]  to train a GAN for synthetic emotional feature generation and also for learning compressed emotional representation. To the best of our knowledge, this paper is the rst to investigate mixup to augment GANs.\n\ne key contribution of this paper is the proposed framework that can e ectively utilise mixup while training a GAN, which augments the representation learning as well as synthetic feature vector generation by a GAN. We present a detailed analysis by evaluating the SER performance on (i) a compressed representation, (ii) synthetic samples, and (iii) by using generated samples to augment the training data. Results for withincorpus and cross-corpus se ing using two emotional datasets show that the proposed framework performs be er compared to recent studies.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "GANs have already successfully been applied in SER. Bao et al.  [16] , utilised larger unlabelled data in a Cycle consistent adversarial networks (CycleGANs)  [17]  based model to generate synthetic features by transferring an emotion feature vector from an unlabelled speech corpus. ey were able to improve the SER performance by utilising synthetic data. Sahu et al.  [14]  investigated two networks including vanilla GAN and a conditional GAN to generate a high-dimensional (1582-d) emotional feature vectors from a low-dimensional (2-d) space. ey used support vector machines (SVMs) for emotion classi cation on real and synthetic data. It was shown in  [14] , that the vanilla GAN could not achieve convergence due to the limited size of data. ey were able to generate synthetic feature vectors by conditioning a GAN on class labels. However, the performance on synthetic features vector was quite low. To address this issue, we are using the mixup strategy on the training data to augment generating abilities in the GAN.\n\nSome studies also utilised generative models for emotional representation learning  [18] . Chang and Scherer  [19]  utilised a deep convolutional GAN in a multi-task se ing to learn the emotional representation from speech. ey utilised unlabelled data in a semi-supervised way to improve the performance of the system. In  [20] , the authors utilised the GAN based framework for multi-lingual emotion recognition. Based on the results, they showed that a GAN can help in learning language invariant features. To learn emotional features in lower dimensions, the authors in  [7]  utilised adversarial autoencoders (AAEs) in SER. Based on their results, the authors showed that AAEs can e ciently encode emotional a ributes in lower dimensions. Similarly, the authors in  [6]  explored di erent low-rank representations learning algorithms for SER. ey showed that low dimensional emotional representations can achieve comparable performance to the high dimensional features. To further improve performance on compressed features, we utilise a GAN based framework to learn emotional representation from augmented data. Beyond, GANs have in SER also been used on audio-level for augmentation, e. g., by emotional voice conversion  [21] . An overview on GANs in SER is further found in  [22] .\n\ne mixup data augmentation strategy has been applied in various vision-related tasks and also in speech-related studies. In  [23] , the authors use mixup strategies in a deep neural network (DNN)-based text-independent speaker veri cation system. ey were able to signi cantly improve performance while using mixup. Tomashenko et al.  [24]  utilised mixup for regularisation of DNN-based acoustic models in automatic speech recognition (ASR). ey found that mixup provides an additional gain in ASR performance. However, no study has utilised mixup in conjunction with GANs to augment feature learning and generation.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Framework",
      "text": "Our proposed framework consists of two components: mixup and GANs. We brie y explain both components rst, and then present the details of the proposed technique.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mixup Augmentation",
      "text": "Mixup  [15]  is a simple data augmentation technique which trains a neural network on convex combinations of pairs of examples and their labels. In this way, it regularises the neural network to favour simple linear behaviour in-between training examples. It constructs virtual training examples as follows:\n\nwhere (xi, yi) and (xj, yj) are randomly selected two examples from training data, and λ ∈ [0, 1]. erefore, mixup extends the training distribution by augmenting the data with linear interpolations of training samples and their targets. Despite the simplicity of mixup, it can improve the performance of various state-of-the-art systems in computer vision and the audio domain  [23] . As outlined, mixup is an essential part of our proposed framework, and it is used to augment the training data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Generative Adversarial Networks (Gans)",
      "text": "Generative adversarial networks (GANs)  [8]  include two neural networks-a generator, G, and a discriminator, D, which play a min-max adversarial game to contest each other. Given a random sample z from some known prior, pz (e. g., Gaussian), G is responsible for generating a fake or synthetic data point G(z). e discriminator, D, a empts to di erentiate between generated samples, G(z), and real data samples, x, (drawn from data distribution, p data ). e objective of a GAN is to train generator network, G(z) that can mimic real data such that the discriminator becomes incapable of discriminating between real and synthetic samples. is makes the GANs very powerful in feature learning  [3]  and generation  [25] . In SER, their performance is hindered by the de facto unavailability of larger datasets. We aim to address this issue by proposing a framework that can utilise mixup in an e ective way to augment GANs both in feature learning as well as in feature generation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Augmenting Gans",
      "text": "As outlined, our model combines mixup with GAN to augment feature learning and generation in SER. e model is shown in Figure  1 . We use mixup to linearly interpolate the input samples before providing them to the proposed GAN network. e samples (xn, yn) and (xm, ym) are randomly selected from the training data to create mixup samples xin and their labels using the equations 1 and 2. Due to the unsupervised nature of our proposed framework, only xin samples are given to the encoder (Ee) network. Here, we modi ed the GAN architecture and use an encoder (Ee) network along with a generator (Ge) and a discriminator (De). e encoder network Ee generates the compressed encoded feature vector ze. Instead of a random sample, the generator (Ge) uses encoded features ze to generate synthetic (or fake) samples (Ge(ze)). e generator (Ge) also acts as the decoder of the autoencoder network. e parameters of the encoder and decoder are optimised by minimising the following cost function:\n\ne discriminator (De) is tasked to classify between real and synthetic (Ge(ze)) samples using a binary cross-entropy loss function. Here, we consider real samples with λ = 0, 1 . erefore, the discriminator (De) network is tasked to classify the real sample with λ = 0, 1, and the synthetic one. is enables the generator (Ge) to generate samples close to real samples (λ = 0, 1) instead of confusions arising from augmented samples with mixup. It also helps the encoder network to encode important emotional a ributes that can help Ge in synthetic feature generation. Overall, the proposed model is trained using the following optimisation: (4) e generator (Ge) a empts to minimise the optimisation in Equation 4 by generating a synthetic sample that can fool the discriminator in the classi cation of real samples (λ= 0, 1) and generated ones. We train the overall model iteratively. First, we update the autoencoder network. en, the generator network is updated. Finally, the discriminator network is updated for samples with λ = 0, 1.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "We use the following datasets for evaluations. IEMOCAP: Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [26]  is a multimodal corpus that contains English dyadic conversations of ten actors over ve sessions. Each session has recordings from one male and one female speaker. Overall, u erances in IEMOCAP are annotated in 10 emotions by 3-4 assessors based on both video and audio streams. To be consistent with previous studies  [27, 28] , we use four emotions including angry, happy, neutral, and sad, where the excitement class is merged into the happiness class.\n\nis results in a total of 5 531 samples. MSP-IMPROV: For cross-corpus evaluation, we select the MSP-IMPROV  [29]  dataset as target data.\n\nis corpus also contains the recordings of English dyadic interactions between actors.\n\nere are six sessions, where each session has the u erances from two speakers (one male, and one female). Overall, 7 798 u erances from 12 speakers are annotated across four emotions: angry, neutral, sad, happy. We use all u erances of this corpus.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Features",
      "text": "We use the openSMILE toolkit  [30]  for extracting features from speech u erances. We use emobase2010 as reference feature set which consists of 1 582 features. is feature set is based on the Interspeech 2010 Paralinguistics Challenge feature set (IS10)  [31]  and contains the combination of prosody, spectral, and energy-based features. We use these features as real samples in our experiments. Mixup is applied directly on these features of training samples and their labels.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Con Guration And Training",
      "text": "We implement our model using feed-forward neural network layers. Our encoder and decoder network consist of two layers with hidden units of 1 000 and 500 each. We vary the dimension of the encoder feature vectors to compare the results with different studies. Our discriminator consists of two hidden layers with 1 000 neurons each.\n\ne autoencoder network is regularised by a dropout layer with a value of 0.5 for between two feed-forward layers. Leaky Recti ed Linear Units (leaky ReLUs)  [32]  are selected as activation function in all hidden layers.\n\nAs described, we employ mixup on the IS10 features vectors and their respective labels of training data. Augmented training data is then given to the proposed model. We pre-train the autoencoder network before initialising the generator. e generator is updated for all input samples, however, the discriminator is only updated for input samples with λ = 0, 1. It is important to note that we only use mixup on training data. A er training the model, we use it to compute the encoded feature vectors and synthetic data for training as well as the testing set. We consider u erance-level speaker-independent SER for our experiments. Speci cally, we use leave-one-sessionout cross-validation to be consistent with previous studies. We use the unweighted average recall (UAR) as the performance metric. We repeat all experiments ve times and mean and standard deviation are reported. We apply min-max normalisation in the synthetic features generation experiments. For cross-corpus evaluation, we apply z-normalisation separately, as it provides be er results compared to min-max normalisation for cross-corpus classi cation  [33] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "We perform two types of experiments to evaluate the performance of the proposed framework: (1) a within-corpus experiment, and (2) a cross-corpus experiment. Each experiment is presented separately below.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Within-Corpus Experiments",
      "text": "In this experiment, we evaluate the proposed model on both synthetic and encoded features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Synthetic Features",
      "text": "In this experiment, we perform analysis on synthetic features. We build DNN classi ers for emotion classi cation using: (i) only real features, (ii) only synthetic features, and (iii) both real and synthetic features. Here, real features show the openSMILE ones with the mixup scheme. Our classi ers consist of two hidden layers with 400 hidden units for the experiments (i) and (ii), and 1000 hidden units for the experiment (iii). We use the dropout layer with a dropout value of 0.5. We use a learning rate of 10 -5 in all these experiments. Results are reported in Table  1 . We perform a comparison of our results to recent studies  [16]  and  [14] . In  [14] , Sahu et al. investigated GAN architectures to generate the synthetic feature vectors (1582-d) using a low dimensional (2-d) representation for SER and to improve the performance in exploiting both real and synthetic features. Similar to  [14] , we also select ze = 2 and generate a synthetic vector (1582-d). We are achieving be er results compared to this study for the classi cation of real, synthetic, and real+synthetic se ings. Bao et al.  [16]  apply a CycleGAN based model to augment SER by transferring feature vectors extracted from a large unlabelled speech data into synthetic features for the given target emotions. We compare their best results for real+synthetic features when they used the classi cation loss in Table  1 . In contrast to  [16] , we are achieving be er results for real and real+synthetic features. However, our classi cation results on synthetic features are slightly lower. To gain a deeper understanding of the performance di erences, we analyse prediction errors in Figure  2a-2c . We also plot the prediction results on synthetic data achieved by  [16]  in Figure  2d .\n\nIt can be noted from the confusion matrices that the prediction performance is improved using real+synthetic features compared to using only real features (see Figure  2a  and 2c ). Our results on synthetic data (Figure  2b ) are comparable to the results achieved using real data (Figure  2a ) for the angry and sad classes. However, we are achieving lower results for the classes happy and neutral. We also compare the prediction errors on synthetic data with Bao et al.  [16] . e proposed model in  [16]  improved the prediction on the sad class, however, performed Figure  2 : Results on the IEMOCAP data using: (2a) real features, (2b) synthetic features, and (2c) real+synthetic. 2d shows the results of  [16]  on synthetic features.\n\npoor on happy and neutral (see Figure  2d ). In contrast, we are achieving results closer to Figure  2a  for all classes, which shows that the proposed framework is generating synthetic feature vectors similar to real samples.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Encoded Features",
      "text": "To evaluate the performance of encoded features by our proposed model, we use encoded features (ze = Ee(xin)) from the autoencoder component as the input to the classi er for classi cation. In this experiment, we compare our results with a recent study  [6]  in which the authors used di erent non-linear dimensionality reduction algorithms for extracting low-rank feature representations for SER. We select three top-performing dimensionality reduction algorithms in  [6] . ese methods include SMACOF multidimensional scaling (MDS)  [34] , Principal Component Analysis (PCA)  [35] , and an autoencoder  [36] . Results are presented in Table  2 . In  [6] , the authors used SVMs for classi cation on the features learnt by each dimensionality reduction algorithm. However, they did not use any data augmentation technique. erefore, we also implemented these dimensionality reduction methods with mixup to have a fair comparison with our proposed model. To be consistent with  [6] , we reduce the dimension of the IS10 features from 1 582 to 25 dimensions and compute the results. In our proposed model, we use ze = Ee(xin) features for classi cation with SVMs. We select an RBF kernel and perform a grid search on validation data to select the optimal hyper-parameters for classi cation. e standard autoencoder applied in this experiment is trained with 3 fully connected encoder layers, 3 decoder layers and 1 hidden layer. ReLU activation is chosen in these layers. It can be noted from Table  2  that the proposed model performs be er than the other non-linear dimension reduction techniques. is shows that the proposed model e ciently encodes features in lower dimension while keeping emotional information.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Cross-Corpus Evaluation",
      "text": "To investigate the proposed model in a cross-corpus se ing, we also perform the same experiments (as in Section 5.1.1) using real, synthetic, and real+ synthetic data. Here, we have MSP-IMPROV as the target data. erefore, we randomly select 30 % of the samples from MSP-IMPROV as the development set for hyper-parameter selection and the remaining 70 % as test data, as done in  [16] . We keep the class proportions equal in both sets. For classi cation, we choose a DNN model with two fully connected layers with 400 hidden units in each layer. e values for the learning rate and dropout are 10 -5 and 0.8, respectively. e results are compared with  [14]  and  [16]  in Table  3 . Both of these studies augmented the training data with synthetic samples to help SER in a cross-corpus se ing. Compared to these studies, we are achieving improved results. is shows that the proposed model improves the performance of SER in a cross-corpus se ing using synthetic data and also when training data is augmented with these synthetic samples.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "A major challenge in speech emotion recognition (SER) is the lack of availability of larger datasets.\n\nis limits the performance of representation learning algorithms and generative models. We address this issue by proposing a framework that utilises a data augmentation technique called mixup to augment GANs in representation learning as well as synthetic feature vector generation. Compared to recent studies, our proposed framework was able to learn be er emotional representations in compressed form and also to generate synthetic features vectors that can be e ectively utilised to augment the training size of SER for performance improvement. In future e orts, we aim to design an extended version of the proposed framework for domain adaptation in cross-lingual SER.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Block diagram of the proposed framework. Mixup is",
      "page": 2
    },
    {
      "caption": "Figure 1: We use mixup to linearly interpolate the input",
      "page": 2
    },
    {
      "caption": "Figure 2: a-2c. We also plot the prediction results on",
      "page": 3
    },
    {
      "caption": "Figure 2: a and 2c). Our",
      "page": 3
    },
    {
      "caption": "Figure 2: b) are comparable to the re-",
      "page": 3
    },
    {
      "caption": "Figure 2: a) for the angry and sad",
      "page": 3
    },
    {
      "caption": "Figure 2: Results on the IEMOCAP data using: (2a) real features, (2b) synthetic features, and (2c) real+synthetic. 2d shows the results of",
      "page": 4
    },
    {
      "caption": "Figure 2: d). In contrast, we are",
      "page": 4
    },
    {
      "caption": "Figure 2: a for all classes, which shows",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Comparison of results using different dimensionality Sahuetal.[14] 45.14 33.96 45.40",
      "data": [
        {
          "Method\nUAR (%)": "SMACOF MDS [6]\n58.5"
        },
        {
          "Method\nUAR (%)": "PCA [6]\n57.7"
        },
        {
          "Method\nUAR (%)": "Autoencoder [6]\n57.8"
        },
        {
          "Method\nUAR (%)": "with mixup"
        },
        {
          "Method\nUAR (%)": "SMACOF MDS\n58.9"
        },
        {
          "Method\nUAR (%)": "PCA\n58.3"
        },
        {
          "Method\nUAR (%)": "Autoencoder\n58.5"
        },
        {
          "Method\nUAR (%)": "59.6\nProposed"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Automated screening for distress: A perspective for the future",
      "authors": [
        "R Rana",
        "S Latif",
        "R Gururajan",
        "A Gray",
        "G Mackenzie",
        "G Humphris",
        "J Dunn"
      ],
      "year": "2019",
      "venue": "European Journal of Cancer Care"
    },
    {
      "citation_id": "3",
      "title": "Detecting anger in automated voice portal dialogs",
      "authors": [
        "F Burkhardt",
        "J Ajmera",
        "R Englert",
        "J Stegmann",
        "W Burleson"
      ],
      "year": "2006",
      "venue": "Ninth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "4",
      "title": "Deep representation learning in speech processing: Challenges, recent advances, and future trends",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Deep representation learning in speech processing: Challenges, recent advances, and future trends",
      "arxiv": "arXiv:2001.00378"
    },
    {
      "citation_id": "5",
      "title": "Paralinguistics in speech and language state-of-the-art and the challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "F Burkhardt",
        "L Devillers",
        "C Müller",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "6",
      "title": "Adaptive control processes: a guided tour",
      "authors": [
        "R Bellman"
      ],
      "year": "2015",
      "venue": "Adaptive control processes: a guided tour"
    },
    {
      "citation_id": "7",
      "title": "Unsupervised low-rank representations for speech emotion recognition",
      "authors": [
        "G Paraskevopoulos",
        "E Tzinis",
        "N Ellinas",
        "T Giannakopoulos",
        "A Potamianos"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "8",
      "title": "Adversarial auto-encoders for speech based emotion recognition",
      "authors": [
        "S Sahu",
        "R Gupta",
        "G Sivaraman",
        "W Abdalmageed",
        "C Espy-Wilson"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "10",
      "title": "High-resolution image synthesis and semantic manipulation with conditional gans",
      "authors": [
        "T.-C Wang",
        "M.-Y Liu",
        "J.-Y Zhu",
        "A Tao",
        "J Kautz",
        "B Catanzaro"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pa ern recognition"
    },
    {
      "citation_id": "11",
      "title": "Dualgan: Unsupervised dual learning for image-to-image translation",
      "authors": [
        "Z Yi",
        "H Zhang",
        "P Tan",
        "M Gong"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "12",
      "title": "Retrospective motion correction in multishot mri using generative adversarial network",
      "authors": [
        "M Usman",
        "S Latif",
        "M Asim",
        "B.-D Lee",
        "J Qadir"
      ],
      "year": "2020",
      "venue": "Scienti c Reports"
    },
    {
      "citation_id": "13",
      "title": "Wgansing: A multi-voice singing voice synthesizer based on the wassersteingan",
      "authors": [
        "P Chandna",
        "M Blaauw",
        "J Bonada",
        "E Gómez"
      ],
      "year": "2019",
      "venue": "2019 27th European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "14",
      "title": "Voice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks",
      "authors": [
        "C.-C Hsu",
        "H.-T Hwang",
        "Y.-C Wu",
        "Y Tsao",
        "H.-M Wang"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "15",
      "title": "On enhancing speech emotion recognition using generative adversarial networks",
      "authors": [
        "S Sahu",
        "R Gupta",
        "C Espy-Wilson"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2017",
      "venue": "mixup: Beyond empirical risk minimization",
      "arxiv": "arXiv:1710.09412"
    },
    {
      "citation_id": "17",
      "title": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition",
      "authors": [
        "F Bao",
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition"
    },
    {
      "citation_id": "18",
      "title": "Unpaired image-toimage translation using cycle-consistent adversarial networks",
      "authors": [
        "J.-Y Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "19",
      "title": "Towards Conditional Adversarial Training for Prediciting Emotions from Speech",
      "authors": [
        "J Han",
        "Z Zhang",
        "Z Ren",
        "F Ringeval",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings ICASSP"
    },
    {
      "citation_id": "20",
      "title": "Learning representations of emotional speech with deep convolutional generative adversarial networks",
      "authors": [
        "J Chang",
        "S Scherer"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "authors": [
        "S Latif",
        "J Qadir",
        "M Bilal"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on A ective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "22",
      "title": "StarGAN for Emotional Speech Conversion: Validated by Data Augmentation of End-to-End Emotion Recognition",
      "authors": [
        "G Rizos",
        "A Baird",
        "M Ellio",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proceedings ICASSP"
    },
    {
      "citation_id": "23",
      "title": "Adversarial Training in A ective Computing and Sentiment Analysis: Recent Advances and Perspectives",
      "authors": [
        "J Han",
        "Z Zhang",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Computational Intelligence Magazine, Special Issue on Computational Intelligence for A ective Computing and Sentiment Analysis"
    },
    {
      "citation_id": "24",
      "title": "Mixup learning strategies for textindependent speaker veri cation",
      "authors": [
        "Y Zhu",
        "T Ko",
        "B Mak"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "25",
      "title": "Speaker adaptive training and mixup regularization for neural network acoustic models in automatic speech recognition",
      "authors": [
        "N Tomashenko",
        "Y Khokhlov",
        "Y Estève"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "26",
      "title": "Wav2pix: speech-conditioned face generation using generative adversarial networks",
      "authors": [
        "A Duarte",
        "F Roldan",
        "M Tubau",
        "J Escur",
        "S Pascual",
        "A Salvador",
        "E Mohedano",
        "K Mcguinness",
        "J Torres",
        "X Giro-I Nieto"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "28",
      "title": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "29",
      "title": "Transfer learning for improving speech emotion classi cation accuracy",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "30",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on A ective Computing"
    },
    {
      "citation_id": "31",
      "title": "Recent developments in opensmile, the munich open-source multimedia feature extractor",
      "authors": [
        "F Eyben",
        "F Weninger",
        "F Gross",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM international conference on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "e interspeech 2010 paralinguistic challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "F Burkhardt",
        "L Devillers",
        "C Müller",
        "S Narayanan"
      ],
      "year": "2010",
      "venue": "Eleventh Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "33",
      "title": "Empirical evaluation of recti ed activations in convolutional network",
      "authors": [
        "B Xu",
        "N Wang",
        "T Chen",
        "M Li"
      ],
      "year": "2015",
      "venue": "Empirical evaluation of recti ed activations in convolutional network",
      "arxiv": "arXiv:1505.00853"
    },
    {
      "citation_id": "34",
      "title": "Unsupervised learning in cross-corpus acoustic emotion recognition",
      "authors": [
        "Z Zhang",
        "F Weninger",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "2011 IEEE Workshop on Automatic Speech Recognition & Understanding"
    },
    {
      "citation_id": "35",
      "title": "Multidimensional scaling: I. theory and method",
      "authors": [
        "W Torgerson"
      ],
      "year": "1952",
      "venue": "Psychometrika"
    },
    {
      "citation_id": "36",
      "title": "LIII. on lines and planes of closest t to systems of points in space",
      "authors": [
        "K Pearson"
      ],
      "year": "1901",
      "venue": "London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science"
    },
    {
      "citation_id": "37",
      "title": "Autoencoders, minimum description length and helmholtz free energy",
      "authors": [
        "G Hinton",
        "R Zemel"
      ],
      "year": "1994",
      "venue": "Advances in neural information processing systems"
    }
  ]
}