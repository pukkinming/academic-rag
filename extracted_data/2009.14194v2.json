{
  "paper_id": "2009.14194v2",
  "title": "Deep Evolution For Facial Emotion Recognition",
  "published": "2020-09-29T17:58:09Z",
  "authors": [
    "Emmanuel Dufourq",
    "Bruce A. Bassett"
  ],
  "keywords": [
    "convolutional neural networks",
    "evolutionary algorithms",
    "facial expression recognition",
    "image classification",
    "neural network compression"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deep facial expression recognition faces two challenges that both stem from the large number of trainable parameters: long training times and a lack of interpretability. We propose a novel method based on evolutionary algorithms, that deals with both challenges by massively reducing the number of trainable parameters, whilst simultaneously retaining classification performance, and in some cases achieving superior performance. We are robustly able to reduce the number of parameters on average by 95% (e.g. from 2M to 100k parameters) with no loss in classification accuracy. The algorithm learns to choose small patches from the image, relative to the nose, which carry the most important information about emotion, and which coincide with typical human choices of important features. Our work implements a novel form attention and shows that evolutionary algorithms are a valuable addition to machine learning in the deep learning era, both for reducing the number of parameters for facial expression recognition and for providing interpretable features that can help reduce bias.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Humans convey their emotions across in various forms, one of which is expressed through changes in a person's face as a consequence of their emotional state  [1] . There are countless numbers of interactions between humans each day given that humans are a social species. The human face provides a lot of information in these interactions  [2] . Facial Expression Recognition (FER) is the ability to recognise the expressions that are being conveyed through the changes in the face.\n\nThere has been a lot of work in this field of research given the progress in computer vision research, see  [3]  for a review of works from 2003 to 2012. Amongst the vast available set of machine learning algorithms, there has been a tremendous amount of recent work using deep learning  [4] . Convolutional neural networks (CNN)  [5]  are widely used in image classification tasks. Researchers focusing on FER have since been using CNNs as they achieve higher classification performance in comparison to earlier methods.\n\nThere are existing literature reviews on the topic of machine learning and FER. Corneanu et al.  [6]  provides a useful taxonomy of FER and computer vision research areas such as face localisation, feature extraction, classification and multimodal fusion -these are discussed in the context of RGB, thermal and 3D images. In terms of the classification areas not a lot of CNN studies were analysed. Pramerdorfer and Martin  [7]  compared 6 CNN studies of depths 5 to 11 against VGG, Inception, ResNet and an ensemble on a single dataset. Pantic and Rothkrantz  [8]  discussed some neural network studies but did not focus on CNNs, similarly, Sariyanidi et al. and Wu et al.  [9] ,  [10]  did not describe CNN related works. Martinez et al.  [11]  reviewed a number of studies and described challenges and opportunities in FER research such as medical, marketing and audience monitoring as well as human computer interaction studies. They did not review studies that implemented CNNs. Ghayoumi  [12]  discussed a few CNN studies. Zhang  [13]  discussed a few deep belief networks and CNNs. Ko  [14]  describe a number of long short term memory CNN approaches which have been applied to FER. Latha and Priya  [15]  discuss 17 CNN studies which have been applied to FER.\n\nIt is clear from the existing work in the literature that CNNs achieve state-of-the-art performance on FER tasks as opposed to other machine learning methods. Training CNNs often result in a large number of trainable parameters. This in turn implies that long training times are to be expected on limited hardware. In this study, we explore a novel idea which attempts to optimise the predictive performance of CNNs for FER and simultaneously, reduce the number of neural network trainable parameters without compromising on the classification accuracy. We ask the following, can we achieve similar predictive performance when training a model on small image patches (extracted from an image) as opposed to the entire image of the face? Are certain facial features more discriminative for FER and can an evolutionary algorithm  [16]  discover these areas? Can an evolutionary algorithm optimise the size of small image patches to reduce the input image size and consequently the number of trainable CNN parameters?",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Proposed Approach",
      "text": "Based on the literature surveyed, there have been no prior attempts at using an evolutionary algorithm to extract patches from images with the objective of reducing the number of trainable CNN parameters and to retain (or improve) classification performance. We propose Evolutionary Facial Expression Recognition (EvoFER) in this section. EvoFER extracts a number of patches from the original image and stacks the patches to form a new image. Figure  1  illustrates an example whereby an image is input into a CNN for classification, and below, an example of four stacked patches (extracted from EvoFER) being inputted into the same CNN. After converting to greyscale, the resolution of the original image is 281×381×1 whereas the new stacked image is 50×50×4. Thus, the number of trainable CNN parameters using the original image is greater than the number of parameters when using the new stacked image made up of several patches. The patches are stacked in a similar manner to a colour image having 3 colour channels (RGB). The objective function for EvoFER is multi-objective. Firstly, EvoFER attempts to reduce the number of trainable parameters as opposed to the number of parameters which would be used when training on the full image. The second objective is to achieve the highest possible classification accuracy using the patches as opposed to using the full image. The following subsections describe EvoFER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Chromosome",
      "text": "EvoFER chromosomes contain 2 fixed genes and then allows for a number of pair of genes to be added. The first two genes denote values α and β which represent the width and height of the patches to be extracted. The remainder of the chromosome denotes (x, y) pairs. Each x and y pair are the coordinates of the top left point of the corresponding patch to be extracted from the original image. The coordinates x and y are relative to the location of the nose. The nose is used as a reference point and was obtained using OpenCV and DLIB 1 . The patch is extracted by obtaining the coordinate (x, y) and using α and β to obtain the entire patch. A chromosome thus encodes the location and size of the patches to be extracted from the original images. A chromosome must have at least one (x, y) pair. User-defined parameters specify the maximum number of (x, y) pairs allowed in each chromosome. The patches are stacked on top of each other vertically which in turn creates a new image. The order of the patches is not  important. Figure  2  illustrates an example of a chromosome with two patches. The width of the patches to be extracted is 10 and the height is 20. The green dotted line is the first patch and the solid blue line is the second patch. The location of the top left corner of each patch to extract relative to the coordinates of the nose are  (10, 30)  and (-10, -5). The figure illustrates the unstacked exacted patches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Initial Population Generation",
      "text": "We use the standard initial population generation algorithm. We propose algorithm 1 to generate the EvoFER chromosomes. This algorithm is executed a number of times based on the number of chromosomes to create. A random value for α and β is assigned to each chromosome based on a corresponding bound which is pre-defined by the experimenter. The values of α and β are not modified during the evolutionary process.\n\nUpon the creation of a chromosome, the number of patches is randomly assigned based on a user specified bound. For each patch to be created, a random value for x and y is created. These values are randomly generated based on the image width and height as is illustrated from lines 11 to 14 in algorithm 1. This was done so that the patches remain as closely as possible within in the bounds of the image. Initially, the evolutionary algorithm can create patches which contain redundant pixels (for example the background, hair or clothing). We do not bias the algorithm towards initialising on patches of interest, such as the mouth or eyes.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Mutation",
      "text": "We propose two mutation operators to traverse through the search space. The first is a variation of the standard mutation operator and the second is tailored to the problem domain. When the mutation function is applied one of four things can be executed depending on the number of patches in the Algorithm 1: Creating an EvoFER chromosome.\n\ninput: min alpha: minimum size for alpha input: min beta: minimum size for beta input: max alpha: maximum size for alpha input: max beta: maximum size for beta input: min patches: minimum number of patches allowed input: max patches: maximum number of patches allowed 1 begin 2 Initialise an empty chromosome. Function GenerateY (beta)\n\nchromosomes, namely adding, changing, removing or shifting.\n\nThe shift operation is a novel method which we describe below. Changing and shifting is always allowed, however adding and removing is only allowed under the following conditions:\n\n• a patch can be added if the number of patches is less than the user-defined predefined maximum • a patch can be removed if the number of patches is greater than the user-defined predefined minimum\n\nAdding a new patch simply generates a value for x and y in a similar way to the functions presented in algorithm 1. The new patch is appended to the chromosome. Removing a patch consists of randomly selecting a (x, y) pair to be removed. Changing a patch randomly selects one and then replaces the (x, y) pair with new values.\n\nAlgorithm 2 presents the pseudocode for applying shift mutation to a chromosome. A patch is randomly selected from the chromosome and then the x and y values are extracted from the patch. The pair is then perturbed by values ranging between [-alpha, alpha] and [-beta, beta]. This enables the algorithm to shift the patch in a random direction. The standard mutation operator allows the algorithm to take a large jump in the pixel space, whereas shift mutation restricts the jump. Figure  3  shows an example of the shift mutation and standard mutation operators being applied to a EvoFER chromosome. Algorithm 2: Applying shift mutation.\n\ninput: alpha: patch width input: beta: patch height input: chromosome: the chromosome which will be mutated 1 begin 2 mutation patch ← randomly select a patch from chromosome",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "D. Crossover",
      "text": "The standard crossover operator is applied to the (x, y) pairs between two parent chromosomes. Specifically, let (x 1 , y 1 ) and (x 2 , y 2 ) be a pair of coordinates within parents P 1 and P 2 . Then offspring C 1 is created by copying all of the genes in P 1 however, x 1 is replaced with x 2 and similarly, y 1 is replaced with y 2 . Chromosome C 2 is created by copying all of the genes in P 2 however, x 2 is replaced with x 1 and similarly, y 2 is replaced with y 1 . The offspring are evaluated and the one with the highest fitness is returned. Figure  4  shows an example of the crossover operator being applied to two EvoFER chromosomes. The first patch from each parent chromosome is swapped to create the two offspring.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Chromosome Evaluation",
      "text": "Before executing the evolutionary process, we run a CNN model M on the image dataset and record the validation accuracy and the number of neural network trainable parameters. Once the model is trained we apply the model to the test images and record the test performance -we denote this as the baseline model.\n\nEach chromosome C i is evaluated on every image X j in the dataset. When determining the fitness for C i , the corresponding image patches E k are extracted from X j based on the number of (x, y) pairs in C i . All the extracted patches E k are stacked upon each other which produce a new image N j . That is, each chromosome will be applied to each X j which creates a corresponding image N j . The new images N j are input into the CNN model M . When computing the fitness score, we make use of the baseline validation accuracy and number of trainable CNN parameters. This is done to assign a fitness score that compares relative performance of chromosomes to the baseline CNN with the ultimate goal of improving the chromosome accuracy and reducing the number of parameters.\n\nWe propose the fitness function presented in equation 1. The function considers both the effect of the validation accuracy and the number of trainable parameters between the network produced on images N and the images X. The objective is to maximise the fitness. When the validation accuracy of the chromosome is larger than the baseline then Sc S b is a large number. Similarly, when the number of parameters obtained by the network as a result of the chromosome is smaller than the number of parameters obtained by the baseline then P b -Pc P b is a large number. We allow a parameter W S to fine-tune the weight allocated to the validation accuracy. Small values (¡ 1.0) of W S will allocate greater importance to the number of parameters. This way, the experimenter can decide on the importance of the validation performance. The value of W S was determined through trial runs and we explored values ranging from {0.1 to 5}.\n\nwhere We provide an example of the fitness computation of a baseline and chromosome. Suppose that a CNN is trained on a FER dataset and the validation accuracy averaged over R runs is computed to be 0.65. Assume that the number of trainable parameters for the network is 12,189,447. Now assume that the proposed algorithm is executed and that the validation accuracy of a chromosome is 0.31 and that the number of trainable parameters is 123,063 (a smaller value is obtained since the input images consist of smaller stacked patches in comparison to the larger original images). Then we have 0.31 0.65 ≈ 0.48 and 12,189,447-123,063\n\n12,189,447 ≈ 0.99. Let W S = 5. The final calculation for the fitness of the chromosome is exp (5 × 0.48 + 0.99) ≈ 29.67. Since the objective is to maximise the fitness then a larger value denotes a better chromosome.\n\nFor each chromosome, we execute a CNN and allow it to train on the transformed images (made up of extracted patches). We use Keras and Tensorflow to train the CNN. The pipeline of constructing patches and training the CNN is illustrated in figure  5 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Experimental Setup",
      "text": "In this section we describe the experimental setup which was used to evaluate the performance of EvoFER. The rationale behind the decisions made to guide the experiments were based on preliminary implementations of CNNs to general FER.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Datasets",
      "text": "To evaluate the performance of EvoFER we conducted a number of experiments on the datasets listed below. These datasets were selected since they are commonly used in the literature and represent different characteristics (gender, age, ethnic diversity and image resolution).\n\n• JAFFE  [18]  • KDEF  [17]  • MUG  [19]  • RAFD  [20]  B. Pre-processing Pre-processing is a vital step when dealing with image data. This can be described as some method to transform original input images using some image manipulation function. The function renders new images with the ultimate goal being that the new images will help the predictive performance of the classifier. Applying a CNN directly to images which have not been pre-processed in some way can yield weaker predictive performance. For example, consider a dataset containing images of people for which the lighting conditions across the face are not consistent. It would be more suitable to attempt to correct the variation in illumination in such a way to have the same amount of lighting exposure across the entire face so that the CNN can learn useful features across the entire face. Preprocessing was shown to improve classification performance in a number of studies  [21] ,  [22] ,  [23] ,  [24] ,  [25] ,  [26] ,  [27] .\n\nWe implemented histogram equalisation on each of the images. Additionally, we converted each image into greyscale as the additional information available in colour images does not impact the performance of FER. To achieve this, we made use of OpenCV as it is commonly used in literature, for examples see  [28] ,  [29] ,  [30] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Data Augmentation",
      "text": "The application of CNNs often require large datasets to enable good predictive performance and has been successfully used in literature, for examples see  [31] ,  [32] ,  [33] ,  [27] ,  [22] . We implemented a number of augmentation techniques using the literature to guide our decisions. We augment our training images using the following techniques:\n\n• horizontal flipping • blurring • noise • rotate by -5 and -10 degrees • rotate by 5 and 10 degrees These augmentation techniques were only applied to the training images. The test images were kept in their original form. For each training image we generated an additional 9 images by applying two levels of blurring and noise. These were achieved by using imgaug 2  -a software package for image augmentation. Table  I  presents the number of images which were used for training for each dataset after we applied the augmentation techniques. Figure  6  presents the augmentation techniques applied to each image.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Network Architecture",
      "text": "Figure  7  presents the CNN architecture which we propose to use for the EvoFER experiments. The architecture was inspired by our findings from existing literature, for examples see  [33] ,  [27] ,  [34] ,  [35] . More specifically, the reviewed works used on average 3 convolutional layers with max pooling in between them, and two fully connected layers at the end of the network. The ReLU activation function was used the most frequently in all layers except for the last where softmax was used. We thus used the literature to guide our decisions, and furthermore various modifications of the architecture were explored using preliminary runs on the JAFFE dataset by varying the depth, activation function and parameters. The last fully connected layer takes on a value of 'C' for the number of units as this is dependant on the number of expression classes in each dataset.\n\nThe proposed architecture consists of three convolutional and max pooling layers, along with two fully connected layers. Dropout was applied after each of the layers except for the last one. The ReLU activation function was used for all the layers except for the last fully connected layer whereby the softmax function was used. It would also be possible to optimise the network using machine learning, such as EDEN, as described in  [36] , however, we chose to use the findings from the literature to guide our decisions. Studies in literature (for examples see  [37] ,  [38] ,  [39] ) reveal that using an ensemble  will result in superior classification performance, however, we wanted to examine the effect of using patches as input to enhance the performance.\n\nWe ran two sets of experiments. In the first, we ran EvoFER using the literature inspired network and used the patches as input to the CNN. In the second set of experiments, we ran the same literature inspired network and used the full image as input to the CNN. We denote experiments conducted on the full images as the baseline.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Training And Testing",
      "text": "Lopes et al.  [40]  describe that a fair comparison is one whereby the same subjects should not be found in the training and testing sets. In their work they split the images into 8 groups containing a number of subjects each, and that the same subject is not found in more than one group. From all the studies reviewed, it was found that their work contained the most emphasis on fairness. We implemented this approach in our experiments, for which 30% of the data was used for testing and the remaining for training. We repeated the execution of EvoFER ten times and averaged our results. This was conducted for both the evaluation of the baseline and for the evolutionary process. The choice of optimiser was selected based on a literature survey. We selected the Adam optimiser and a batch size of 8.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "F. Evofer Parameters",
      "text": "Table  II  provides the details of the parameters associated with the evolutionary algorithm. The parameters associated with the training of the CNN are presented in table III. Finally, table IV presents the parameters associated with the initialisation of the chromosomes during the initial population generation and mutation operation. These parameters were chosen by performing a search on a number of values which were found in the literature and by performing a random hyper-parameter search.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Iv. Results And Discussion",
      "text": "Table V presents the average test classification accuracy results. The baseline accuracy (literature inspired CNN model with original images) and EvoFER accuracy (the same literature inspired CNN model with extracted patches from best chromosome) are presented. The CNN architecture and hyperparameters were the same for both results. The findings reveal that on all of the datasets EvoFER was able to outperform the same CNN model which had been trained on the original images. The smallest improvement in accuracy was observed in the KDEF dataset with an improvement of 1.3%. The largest improvement in classification accuracy was obtained on the MUG dataset with a value of 20.6%. This indicates that given  a fixed architecture, an EA can be applied in such a way to extract patches from an image and to train the CNN on those patches and achieve competitive performance. Why did EvoFER achieve superior performance to the baseline method? It is hypothesised that EvoFER is able to obtain better predictive performance given that it inputs extracted patches to the CNN and consequently the number of neural network parameters are reduced. The average number of neural network trainable parameters obtained in the experiments are presented in table VI. For each dataset the number of neural network parameters for EvoFER is significantly less than for the baseline CNN. The baseline CNN inputs larger images of the faces compared to EvoFER which inputs smaller extracted patches. The percentage difference achieved by EvoFER for KDEF, JAFFE, RAFD and MUG are 88%, 97%, 97% and 98% respectively. The largest difference in parameters was obtained on the JAFFE dataset with a reduction of over 7 million parameters. It is observed on the JAFFE dataset that the standard deviation for the number of parameters is zero, which indicates that in each execution of the algorithm, EvoFER extracted the same number of patches of the same size. Despite the large standard deviation on the KDEF dataset the number of parameters is still distant from the baseline parameters.\n\nFigures 8 to 10 illustrate the patches which were extracted from the best chromosome in generation 0 and the last generation for the various datasets. Each row presents the patches which were extracted from a random test example.\n\nFor the JAFFE dataset, the best chromosome from the initial and final population extracted two patches. For generation 0, both patches were around the left eye region. This of course is sub-optimal as that does not provide the CNN with enough information to discriminate between the various emotion classes. The best chromosome extracts patches around the left eye region and includes the eyebrow. The second patch extracts pixels around the center and right region of the mouth.\n\nFigure  8  illustrates the patches for the KDEF dataset. In generation 0 the chromosome extracts four patches. The first and last patch are redundant as the first extracts pixels around the hair and the other around the neck. This does not help discriminate between expressions. The second and third patch are better since they extract pixels between the eyes and near the left corner of the mouth. In the case of the best chromosome in generation 15, two patches are extracted. Combined, they extract pixels near the left eye and half of the mouth.\n\nThe extracted patches for the MUG dataset are presented in figure  9 . The best chromosome from generation 0 extracts two patches, one near the left ear and the other contains pixels around the nose and mouth. The first patch does not assist in distinguishing between expressions. The best chromosome from the final generation extracts three patches. The second and the third patch are similar since they both extract pixels around the left eye. The first patch extracts pixels around the center of the mouth.\n\nFinally, figure  10  presents the patches extracts for the RAFD dataset. For generation 0, the best chromosome extracts four patches of which the first, second and last contain redundant information. Those patches cannot help the CNN distinguish between the various expression classes. The third patch extracts pixels near the right eye and eyebrow. For the last generation, the best chromosome extracts two patches. The first contains pixels around the mouth and chin, and the second contains pixels capturing the mouth, nose and eyes (no information about the eyebrows).\n\nFrom the figures it is observed that the best predictive performance is achieved when pixels around the eyes and mouth are obtained. The best chromosome from each of the last generations extracted two patches on 3 of the 4 datasets. We can deduce that those areas are the most salient when distinguishing between expressions on the datasets examined. We did not have sufficient computing resources to compare the results to state-of-the-art CNNs which use much deeper network with a larger number of parameters. The findings do reveal that on a fixed architecture which was inspired by the literature, EvoFER is able to enhance the performance by using stacked patches as input.\n\nWhat is the processing time for EvoFER given that the nose has to be located prior to the application of the chromosome? We examined the time it took to perform the necessary preprocessing steps for EvoFER and in the case of the baseline CNN. These findings are presented in table VIII. The time, in seconds, is presented for the two methods to pre-process and predict the expression for 10 images. In the case of KDEF, RAFD and MUG, EvoFER took approximately twice as long as the baseline method. This is because EvoFER has the extra overhead of needing to locate the nose to establish a reference point for the chromosome. EvoFER took less time than the baseline on the JAFFE dataset. It can be hypothesised that this is the case since the resolution of the images in the JAFFE dataset is much smaller than the other datasets. It would be of interest to reduce the resolution of images in the other datasets  to determine if this could result in faster execution times for EvoFER. Despite the fact that EvoFER has an additional overhead, the processing time is not significantly large and could thus still be implemented in a real-world setting. In terms of training, EvoFER took up to 5 hours whereas training the equivalent network in a traditional setting (i.e. the baseline) took less than a minute. Once the location of the patches have been optimised, EvoFER trains faster than the baseline approach (i.e. the same network on the original dataset) -these findings are presented in table VII.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Conclusion",
      "text": "This study proposes a novel evolutionary algorithm to extract patches from images with the goal of transforming images into more compact representations whilst retaining the predictive performance (or increasing it) and to reduce the number of trainable parameters. We propose a chromosome Generation 0 Generation 15 representation which allows the algorithm to encode locations relative to the nose. The chromosomes also encode the width and height of the patches to be extracted at each location. We introduce a fitness function which combines the relative performance of each chromosome to a baseline execution.\n\nThe baseline execution consists of running a CNN on the original images. The multi-objective fitness function attempts to optimise the validation accuracy and number of trainable parameters. To enable us to interface each chromosome to the training of a CNN we use Keras and Tensorflow. We evaluated EvoFER and the findings revealed that it can achieve superior performance to the exact CNN architecture trained on the entire image. Furthermore, the findings revealed that EvoFER can reduce the number of trainable parameters, on average, up to 95%. EvoFER was able to optimise the search for optimal patch locations and size to enable the CNN to distinguish between the various expressions (without any insights hardcoded into the algorithm). Initially EvoFER could select patches which did not contain parts of the face at all. In several cases there was patches which were located near people's hair. Through the evolutionary process, EvoFER extracted patches generally around the eyes and mouth which enabled the CNN to achieve better predictive performance. We hypothesise that superior performance could be achieved by increasing the number of epochs; in this study we imposed a limit due to available computational time.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates an example whereby an image is input",
      "page": 1
    },
    {
      "caption": "Figure 1: Illustrating the primary idea behind EvoFER. Instead of inputting",
      "page": 2
    },
    {
      "caption": "Figure 2: Illustrating a EvoFER chromosome which extracts two patches. The",
      "page": 2
    },
    {
      "caption": "Figure 2: illustrates an example of a chromosome",
      "page": 2
    },
    {
      "caption": "Figure 3: shows an example of the shift mutation and standard",
      "page": 3
    },
    {
      "caption": "Figure 3: Illustrating shift mutation on the left and standard mutation on the",
      "page": 3
    },
    {
      "caption": "Figure 4: shows an example of the crossover operator",
      "page": 3
    },
    {
      "caption": "Figure 4: Illustrating the crossover operator swapping the ﬁrst patch between",
      "page": 4
    },
    {
      "caption": "Figure 5: Illustrating the EvoFER pipeline. Each chromosome is applied to the",
      "page": 4
    },
    {
      "caption": "Figure 6: presents the augmenta-",
      "page": 5
    },
    {
      "caption": "Figure 6: Illustrating the different augmentation techniques which were used to",
      "page": 5
    },
    {
      "caption": "Figure 7: presents the CNN architecture which we propose to",
      "page": 5
    },
    {
      "caption": "Figure 7: The CNN architecture which is proposed for the experiments. This",
      "page": 6
    },
    {
      "caption": "Figure 8: illustrates the patches for the KDEF dataset. In",
      "page": 7
    },
    {
      "caption": "Figure 8: Illustrating the patches extracted from the best chromosome in",
      "page": 8
    },
    {
      "caption": "Figure 9: Illustrating the patches extracted from the best chromosome in",
      "page": 8
    },
    {
      "caption": "Figure 10: Illustrating the patches extracted from the best chromosome in",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "KDEF",
          "Baseline with original\ndataset\ninput": "247",
          "EvoFER with extracted\npatches input": "42"
        },
        {
          "Dataset": "JAFFE",
          "Baseline with original\ndataset\ninput": "61",
          "EvoFER with extracted\npatches input": "10"
        },
        {
          "Dataset": "RAFD",
          "Baseline with original\ndataset\ninput": "112",
          "EvoFER with extracted\npatches input": "30"
        },
        {
          "Dataset": "MUG",
          "Baseline with original\ndataset\ninput": "43",
          "EvoFER with extracted\npatches input": "10"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Facial expression recognition",
      "authors": [
        "Y Tian",
        "T Kanade",
        "J Cohn"
      ],
      "year": "2011",
      "venue": "Handbook of face recognition"
    },
    {
      "citation_id": "2",
      "title": "The human face as a dynamic tool for social communication",
      "authors": [
        "R Jack",
        "P Schyns"
      ],
      "year": "2015",
      "venue": "Current Biology"
    },
    {
      "citation_id": "3",
      "title": "Face expression recognition: A brief overview of the last decade",
      "year": "2013",
      "venue": "2013 IEEE 8th International Symposium on Applied Computational Intelligence and Informatics"
    },
    {
      "citation_id": "4",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "Nature"
    },
    {
      "citation_id": "5",
      "title": "Handwritten digit recognition with a backpropagation network",
      "authors": [
        "Y Lecun",
        "B Boser",
        "J Denker",
        "D Henderson",
        "R Howard",
        "W Hubbard",
        "L Jackel"
      ],
      "year": "1990",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "6",
      "title": "Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications",
      "authors": [
        "C Corneanu",
        "M Simon",
        "J Cohn",
        "S Guerrero"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Facial expression recognition using convolutional neural networks: State of the art",
      "authors": [
        "C Pramerdorfer",
        "M Kampel"
      ],
      "year": "2016",
      "venue": "Facial expression recognition using convolutional neural networks: State of the art",
      "arxiv": "arXiv:1612.02903"
    },
    {
      "citation_id": "8",
      "title": "Automatic analysis of facial expressions: the state of the art",
      "authors": [
        "M Pantic",
        "L Rothkrantz"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "9",
      "title": "Automatic analysis of facial affect: A survey of registration, representation, and recognition",
      "authors": [
        "E Sariyanidi",
        "H Gunes",
        "A Cavallaro"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Survey of the facial expression recognition research",
      "authors": [
        "T Wu",
        "S Fu",
        "G Yang"
      ],
      "year": "2012",
      "venue": "Advances in Brain Inspired Cognitive Systems"
    },
    {
      "citation_id": "11",
      "title": "Advances, Challenges, and Opportunities in Automatic Facial Expression Recognition",
      "authors": [
        "B Martinez",
        "M Valstar"
      ],
      "year": "2016",
      "venue": "Advances, Challenges, and Opportunities in Automatic Facial Expression Recognition"
    },
    {
      "citation_id": "12",
      "title": "A quick review of deep learning in facial expression",
      "authors": [
        "M Ghayoumi"
      ],
      "year": "2017",
      "venue": "Journal of Communication and Computer"
    },
    {
      "citation_id": "13",
      "title": "Facial Expression Recognition Based on Deep Learning: A Survey",
      "authors": [
        "T Zhang"
      ],
      "year": "2018",
      "venue": "Facial Expression Recognition Based on Deep Learning: A Survey"
    },
    {
      "citation_id": "14",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "15",
      "title": "A review on deep learning algorithms for speech and facial emotion recognition",
      "authors": [
        "C Latha",
        "M Priya"
      ],
      "year": "2016",
      "venue": "APTIKOM Journal on Computer Science and Information Technologies"
    },
    {
      "citation_id": "16",
      "title": "Genetic algorithms, selection schemes, and the varying effects of noise",
      "authors": [
        "B Miller",
        "D Goldberg"
      ],
      "year": "1996",
      "venue": "Evolutionary Computation"
    },
    {
      "citation_id": "17",
      "title": "The karolinska directed emotional faces ? kdef",
      "authors": [
        "D Lundqvist",
        "A Ohman"
      ],
      "year": "1998",
      "venue": "The karolinska directed emotional faces ? kdef"
    },
    {
      "citation_id": "18",
      "title": "Coding facial expressions with gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "19",
      "title": "The mug facial expression database",
      "authors": [
        "N Aifanti",
        "C Papachristou",
        "A Delopoulos"
      ],
      "year": "2010",
      "venue": "11th International Workshop on Image Analysis for Multimedia Interactive Services"
    },
    {
      "citation_id": "20",
      "title": "Presentation and validation of the radboud faces database",
      "authors": [
        "O Langner",
        "R Dotsch",
        "G Bijlstra",
        "D Wigboldus",
        "S Hawk",
        "A Van Knippenberg"
      ],
      "year": "2010",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "21",
      "title": "Image based static facial expression recognition with multiple deep network learning",
      "authors": [
        "Z Yu",
        "C Zhang"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, ser. International Conference on Multimodal Interaction '15"
    },
    {
      "citation_id": "22",
      "title": "Fusing aligned and non-aligned face information for automatic affect recognition in the wild: A deep learning approach",
      "authors": [
        "B Kim",
        "S Dong",
        "J Roh",
        "G Kim",
        "S Lee"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "23",
      "title": "Baseline cnn structure analysis for facial expression recognition",
      "authors": [
        "M Shin",
        "M Kim",
        "D Kwon"
      ],
      "year": "2016",
      "venue": "25th IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "24",
      "title": "Facial emotion detection using deep learning",
      "authors": [
        "D Spiers"
      ],
      "year": "2016",
      "venue": "Facial emotion detection using deep learning"
    },
    {
      "citation_id": "25",
      "title": "Head-pose invariant facial expression recognition using convolutional neural networks",
      "authors": [
        "B Fasel"
      ],
      "year": "2002",
      "venue": "Fourth IEEE International Conference on Multimodal Interfaces"
    },
    {
      "citation_id": "26",
      "title": "Deep learning using linear support vector machines",
      "authors": [
        "Y Tang"
      ],
      "year": "2013",
      "venue": "Deep learning using linear support vector machines",
      "arxiv": "arXiv:1306.0239"
    },
    {
      "citation_id": "27",
      "title": "Facial expression recognition using deep convolutional neural networks",
      "authors": [
        "D Sang",
        "N Dat",
        "D Thuan"
      ],
      "year": "2017",
      "venue": "International Conference on Knowledge and Systems Engineering"
    },
    {
      "citation_id": "28",
      "title": "Facial expression recognition based on transfer learning from deep convolutional networks",
      "authors": [
        "M Xu",
        "W Cheng",
        "Q Zhao",
        "L Ma",
        "F Xu"
      ],
      "year": "2015",
      "venue": "11th International Conference on Natural Computation"
    },
    {
      "citation_id": "29",
      "title": "Development of deep learning-based facial expression recognition system",
      "authors": [
        "H Jung",
        "S Lee",
        "S Park",
        "B Kim",
        "J Kim",
        "I Lee",
        "C Ahn"
      ],
      "year": "2015",
      "venue": "21st Korea-Japan Joint Workshop on Frontiers of Computer Vision"
    },
    {
      "citation_id": "30",
      "title": "Cross-database facial expression recognition based on fine-tuned deep convolutional network",
      "authors": [
        "M Zavarez",
        "R Berriel",
        "T Oliveira-Santos"
      ],
      "year": "2017",
      "venue": "30th SIBGRAPI Conference on Graphics, Patterns and Images"
    },
    {
      "citation_id": "31",
      "title": "Automatic facial expression recognition",
      "authors": [
        "H Valero"
      ],
      "year": "2016",
      "venue": "Automatic facial expression recognition"
    },
    {
      "citation_id": "32",
      "title": "A deep neural network-driven feature learning method for multi-view facial expression recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "J Yan",
        "K Yan"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Convolutional neural networks for facial expression recognition",
      "authors": [
        "S Alizadeh",
        "A Fazel"
      ],
      "year": "2017",
      "venue": "Convolutional neural networks for facial expression recognition",
      "arxiv": "arXiv:1704.06756"
    },
    {
      "citation_id": "34",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ser. International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "35",
      "title": "Facial expression recognition with convolutional neural networks",
      "authors": [
        "A Raghuvanshi",
        "V Choksi"
      ],
      "year": "2016",
      "venue": "Facial expression recognition with convolutional neural networks"
    },
    {
      "citation_id": "36",
      "title": "Eden: Evolutionary deep networks for efficient machine learning",
      "authors": [
        "E Dufourq",
        "B Bassett"
      ],
      "year": "2017",
      "venue": "2017 Pattern Recognition Association of South Africa and Robotics and Mechatronics"
    },
    {
      "citation_id": "37",
      "title": "Facial Expression Recognition Based on Multi-scale CNNs",
      "authors": [
        "S Zhou",
        "Y Liang",
        "J Wan",
        "S Li"
      ],
      "year": "2016",
      "venue": "Facial Expression Recognition Based on Multi-scale CNNs"
    },
    {
      "citation_id": "38",
      "title": "Facial expression recognition using a hybrid cnn-sift aggregator",
      "authors": [
        "M Al-Shabi",
        "W Cheah",
        "T Connie"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "39",
      "title": "Group-level emotion recognition using transfer learning from face identification",
      "authors": [
        "A Rassadin",
        "A Gruzdev",
        "A Savchenko"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction, ser. International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "40",
      "title": "Facial expression recognition with convolutional neural networks: Coping with few data and the training sample order",
      "authors": [
        "A Lopes",
        "E De Aguiar",
        "A Souza",
        "T Oliveira-Santos"
      ],
      "year": "2017",
      "venue": "Pattern Recognition"
    }
  ]
}