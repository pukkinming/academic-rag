{
  "paper_id": "2411.10087v3",
  "title": "Pfml: Self-Supervised Learning Of Time-Series Data Without Representation Collapse",
  "published": "2024-11-15T10:16:38Z",
  "authors": [
    "Einari Vaaras",
    "Manu Airaksinen",
    "Okko Räsänen"
  ],
  "keywords": [
    "EEG data",
    "embedding masking",
    "human activity recognition",
    "multi-sensor inertial measurement unit data",
    "representation collapse",
    "self-supervised learning",
    "sleep stage classification",
    "speech emotion recognition",
    "statistical functionals",
    "time-series data"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Self-supervised learning (SSL) is a data-driven learning approach that utilizes the innate structure of the data to guide the learning process. In contrast to supervised learning, which depends on external labels, SSL utilizes the inherent characteristics of the data to produce its own supervisory signal. However, one frequent issue with SSL methods is representation collapse, where the model outputs a constant input-invariant feature representation. This issue hinders the potential application of SSL methods to new data modalities, as trying to avoid representation collapse wastes researchers' time and effort. This paper introduces a novel SSL algorithm for time-series data called Prediction of Functionals from Masked Latents (PFML). Instead of predicting masked input signals or their latent representations directly, PFML operates by predicting statistical functionals of the input signal corresponding to masked embeddings, given a sequence of unmasked embeddings. The algorithm is designed to avoid representation collapse, rendering it straightforwardly applicable to different time-series data domains, such as novel sensor modalities in clinical data. We demonstrate the effectiveness of PFML through complex, real-life classification tasks across three different data modalities: infant posture and movement classification from multi-sensor inertial measurement unit data, emotion recognition from speech data, and sleep stage classification from EEG data. The results show that PFML is superior to a conceptually similar SSL method and a contrastive learning-based SSL method. Additionally, PFML is on par with the current state-of-the-art SSL method, while also being conceptually simpler and without suffering from representation collapse. The code is freely available at https://github.com/SPEECHCOG/PFML.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "Self-supervised learning (SSL) can be described as a datadriven learning paradigm where the training process is guided by the inherent structure of the data itself. Unlike supervised learning that relies on externally provided labels, SSL exploits the intrinsic properties of the data to generate its own supervisory signal  [1] ,  [2] . SSL enables the model to learn rich feature representations from large amounts of unlabeled data that can be used as a starting point for downstream tasks, either as such or by fine-tuning the feature extractor to be better suited for solving some specific task  [2] ,  [3] . Since typically there is an abundance of unlabeled data but a scarcity of labeled data, the use of SSL has been shown to reduce the need for large, manually annotated datasets  [4] ,  [5] ,  [6] . In addition to SSL algorithms that have been developed for a single data modality, SSL algorithms that can be applied to multiple different data modalities have gained popularity in recent years  [2] ,  [4] ,  [7] ,  [8] ,  [9] . These methods and their extensions have shown great success in e.g. audio, image, and text data  [4] ,  [7] ,  [8] ,  [9] ,  [10] ,  [11] ,  [12] ,  [13] ,  [14] .\n\nHowever, many SSL algorithms suffer from two issues: First, SSL algorithms are usually complex, with a plethora of hyperparameters that need careful tuning for the algorithm to work properly. This hinders the ability of SSL algorithms to be applied to new data domains, where the selection of these hyperparameters is not self-evident. For example, in contrastive learning-based SSL, the selection of positive and negative samples during training is essential for the algorithm to work properly. However, deciding which samples should be assigned to positive and negative categories is not always apparent  [1] ,  [15] ,  [16] . As another example, determining the number of clusters for clustering-based SSL algorithms (such as Caron et al.  [17]  and Hsu et al.  [18] ) in a new data domain or task can be difficult. Examples of such domains could include, for instance, different types of medical time-series data (e.g. electroencephalography (EEG), electrocardiography (ECG), or electromyography (EMG) recordings) that come in various dataset sizes and from various recording configurations. Second, a common failure mode during SSL pre-training is representation collapse, where the model ends up outputting a constant, time-invariant feature representation. Representation collapse is very common in SSL pre-training  [1] ,  [19] ,  [20] ,  [21] , and many SSL methods apply different countermeasures to tackle the problem (see Section III-A).\n\nIn the present study, we propose a new SSL algorithm for time-series data called Prediction of Functionals from Masked Latents (PFML). In PFML, the aim is to predict statistical functionals of the input signal corresponding to masked embeddings, given a sequence of unmasked embeddings. The overall methodological aim of our method is to have an SSL algorithm that would be as straightforward as possible to apply to various time-series data domains with minimal hyperparameter optimization, and without the risk of representation collapse. The contributions of the present study are as follows:\n\n1) We propose a novel SSL algorithm for time-series data, PFML, that does not suffer from representation collapse, rendering the method straightforward to apply to new time-series data domains. To the best of our knowledge, PFML is the first work within the field of SSL for time-series data where the central idea of reconstructing statistical functionals is utilized. 2) We demonstrate the effectiveness of PFML using three different data modalities with complex, real-life classification tasks: infant posture and movement classification from multi-sensor inertial measurement unit (IMU) data, emotion recognition from speech data, and sleep stage classification from EEG data. 3) We show that PFML obtains superior results against both a conceptually similar SSL method and a contrastive learning-based SSL method. Additionally, PFML achieves results that are on par with the current state-of-the-art data modality-agnostic SSL method, while also being conceptually simpler and without suffering from representation collapse.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Most of the advances in SSL have focused on developing new, better-performing algorithms with some specific data modality in mind. For speech data, Baevski et al.  [5]  presented an SSL algorithm where the basic idea is to mask speech embeddings and then solve a contrastive task that is defined over a quantization of the embeddings which are simultaneously learned during the pre-training task. Hsu et al.  [18]  proposed that instead of solving a contrastive task, they predict cluster targets of masked embeddings. Furthermore, the SSL method by Chen et al.  [22]  also uses masking of embeddings, but the authors simulate noisy speech inputs and predict pseudo-labels of the original speech from the masked embeddings. Similar to the advances in SSL for speech data, there have been significant developments in SSL for image data as well  [6] ,  [23] ,  [24] ,  [25] ,  [26] ,  [27] ,  [28] ,  [29] ,  [30] . Grill et al.  [26]  presented an SSL method that uses two neural networks that learn from each other's representations of differently augmented views of the same image. He et al.  [28]  proposed masked autoencoders (MAE) that try to reconstruct masked patches of input images using an asymmetric encoder-decoder architecture. The SSL algorithm by Bao et al.  [29]  tokenizes images into visual tokens, followed by masking some image patches and then trying to recover the original tokens from the masked patches.\n\nSSL has also excelled in natural language processing  [31] ,  [32] ,  [33] ,  [34] . Devlin et al.  [31]  introduced an SSL method which obtains bidirectional feature representations from unlabeled text by conditioning on both the left and right textual context. The method by Brown et al.  [32]  uses an autoregressive model which alternates dense and locally banded sparse attention patterns in their Transformer model. OpenAI  [34]  proposed an expanded version of the method by Brown et al.  [32]  by making the model not only larger, but also capable of handling image inputs in addition to text inputs.\n\nMore recently, SSL literature has seen a growing number of work towards SSL algorithms capable of running the pretraining task on multiple different data modalities. The authors of van den Oord et al.  [4]  developed an SSL approach that predicts future embeddings based on previous context using contrastive learning. They showed that their method was able to learn useful feature representations for audio, image, text, and reinforcement learning in 3D environments. The SSL method by Akbari et al.  [7]  also uses contrastive learning, but their method simultaneously takes audio, video, and text data as input and creates multimodal feature representations. These features were shown to work well with multiple different downstream tasks, i.e. video action recognition, audio event classification, image classification, and text-to-video retrieval. Baevski et al.  [8]  proposed data2vec, an SSL method for audio, image, and text data. In their approach, the model tries to predict masked latent features of an older version of itself that are both normalized and averaged over multiple Transformer layers. Their results in downstream tasks demonstrate the effectiveness of the method in all three data modalities. Yue et al.  [35]  presented a universal framework, TS2Vec, for learning time-series representations at different semantic levels using hierarchical contrastive learning over augmented context views. Their approach enables robust contextual representations for each timestamp and allows for simple aggregation to obtain representations of arbitrary sub-sequences. They demonstrated that their method obtained state-of-the-art performance in timeseries classification, forecasting, and anomaly detection tasks on multiple datasets, most of which were small in terms of the number of training samples. Notably, TS2Vec achieved these results by adding very simple classifiers after the pre-trained model, which has fixed constraints on the model architecture to model the hierarchical representations. Wang et al.  [9]  proposed an SSL method that performs prediction of masked tokens in a unified manner on images, texts, and image-text pairs. Their experiments showed that their method achieves state-of-the-art performance on various vision and visionlanguage tasks.\n\nFor modality agnostic SSL algorithms, objective functions play a crucial role in guiding the learning process. These functions can be broadly categorized into three types: instance discrimination, clustering, and masked prediction. Instance discrimination aims to distinguish between different instances of data, thereby encouraging the model to learn unique features for each instance and enhancing the discriminative power of the learned representations. Contrastive learning methods, such as  [4] ,  [5] ,  [7] ,  [35] ,  [36] ,  [37] , are an example of instance discrimination-based SSL methods. Clustering, on the other hand, groups similar instances together in the feature space, fostering the model to learn common features among instances belonging to the same group. Methods like  [17] ,  [18] ,  [38]  are examples of clustering-based SSL methods. Lastly, masked prediction involves the task of predicting masked parts of the input data based on the unmasked parts, thereby encouraging the model to learn contextual relationships within the data. Examples of such SSL methods include  [8] ,  [28] ,  [31] ,  [39] ,  [40] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Method A. Motivation",
      "text": "One key issue with many SSL methods is the problem of representation collapse, where the model outputs a constant, input-invariant feature representation, leading to a trivial solution of the pre-training task  [1] ,  [20] . This considerably slows down the development process for novel data domains and/or tasks due to the necessity of operating in uncertainty, when it is not clear whether the representation collapse is caused by an ill-posed task or by the SSL algorithm. To avoid this, SSL methods have taken several different countermeasures: Baevski et al.  [5]  use the same target representations in their contrastive learning task in a dual manner, i.e. both as a positive and a negative example. Grill et al.  [26]  both add an additional predictor to their training regime and use a moving average of their so-called online neural network to avoid representation collapse. Bardes et al.  [41]  add a regularization term to their loss function that both maintains variance of the embeddings and decorrelates each pair of variables. In data2vec  [8] , the authors tackle representation collapse by carefully selecting their model hyperparameters and promoting target representation variance through feature normalization. Also, in the code implementation of data2vec if the learning objective is too complicated, the model fails to converge to a useful solution. For time-series data, i.e. a waveform (e.g. audio) or a set of waveforms (e.g. multichannel EEG), trying to reconstruct masked parts of the input signal given the unmasked parts of the signal (as in e.g. MAE  [28] ) is a very complex task. This is due to the fact that a time-series signal can have large temporal variation even between short periods of time. While joint learning of a priori unspecified latent representations and their prediction allows discarding of this irrelevant variation (as in, e.g.,  [4]  or  [5] ), the problem requires learning algorithms that become susceptible to representation collapse and/or may require careful tuning of the training process. We hypothesize that for SSL pretraining with time-series data, a model would learn more useful features for downstream tasks if the complex setting of MAE would be alleviated slightly. Hence, we propose PFML, a novel SSL algorithm for time-series data. Our method builds on the concept of MAE and reduces the complexity of the pre-training task of MAE in two ways:\n\n1) Instead of aiming to reconstruct the input signal, the model tries to predict a set of statistical functionals computed from the input signal. 2) Instead of masking the input signal directly, PFML borrows the idea of e.g. wav2vec 2.0  [5]  and data2vec  [8]  and masks the embeddings created by the encoder model.\n\nRegarding point  (1) , by making the model predict statistical functionals of masked latent features instead of predicting the input signal x itself, we relieve the model from the complex task of modelling the high-dimensional distribution of x in detail. We validate this argument of generating better features for downstream tasks by reducing the computational complexity of the pre-training task in Section IV, where we compare our proposed method against MAE. In theory, the set of statistical functionals can be chosen so that the desired and deterministically calculated statistical properties of the data, and thereby their variance, are preserved in the target features. Furthermore, regarding point (2), we show in our experiments in Section IV that it is more beneficial during pre-training to mask the latent features instead of masking the input directly. This further alleviates the complexity of the learning task in particular for the encoder module. First, a single-or multi-channel signal x is framed into a sequence of short-term frames {x 0 , x 1 , ...}, x n = {x t , x t+1 , ..., x t+N -1 }, of N samples each. Then, a set of m functionals, F = {F 0 , F 1 , ..., F m-1 }, is computed for each frame x n to produce corresponding functional values Some of these embeddings are masked randomly at time steps M (for example, M ∈ {1, 2} in Figure  1 ), after which all z n are used as an input for a Transformer-based model to obtain outputs y n . Finally, a prediction loss is computed between the outputs of masked time steps y M and their functional counterparts f M . As a result, PFML pre-training optimizes the prediction of functionals of input signal frames corresponding to the masked embeddings, given the unmasked embeddings from the temporal context of these frames.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Prediction Of Functionals From Masked Latents",
      "text": "In PFML, predicting only one or a few functionals of a framed signal can be a trivial task, and will most probably lead to learning feature representations that are not very useful for downstream tasks. However, as the number of functionals that each describe some property of the framed signal grows, a more accurate description of the signal can be obtained (see e.g. McDermott & Simoncelli  [42]  for representing perceptual properties of sound textures with functionals). Therefore, as the number of different functionals grows, the PFML algorithm is getting closer to predicting all of the nuances of the input signal.\n\nLet us assume the following in PFML pre-training:\n\n• Assumption 1: There is temporal variability across the frames x n . This assumption is reasonable as real-world data typically exhibits temporal variability. • Assumption 2: Given Assumption 1, a set of non-trivial functionals F computed from x n also contains variance across the frames. This follows naturally since nonconstant functionals derived from variable data also exhibit variability.\n\nUnder these assumptions, as the model is trying to predict the computed functionals f n given the embeddings z n , good model predictions y n that lead to low prediction loss values also inherently contain variance. On the contrary, if y n were to contain zero variance across the frames while f n contains variance, the prediction loss would be high. Consequently, PFML pre-training does not converge to collapsed feature representations, as long as Assumptions 1 and 2 hold true. For a more detailed formulation, see Appendix A in the supplementary material. Empirical results (see Section V) support this theoretical claim, showing that PFML maintains variance in predictions across various datasets.\n\nIn the present study, we selected 11 mathematical operations as our set of functionals: mean, variance, skewness, kurtosis, minimum value, maximum value, zero-crossing rate (ZCR), and the mean, variance, skewness, and kurtosis of the autocorrelation function (ACF). The ZCR for a signal\n\nwhere sgn denotes the sign function  [43] . The ACF for a signal x at lag τ is defined as\n\nwhere τ < N , µ is the mean of x, and σ 2 is the variance of x  [43] . Note that Equation 2 returns a vector of measurements when applied to all lags τ < N .\n\nFor masking the embeddings, in each training and validation minibatch we randomly select frames with a probability of p m to be mask starting indices, and we mask the embedding of that frame and l m -1 subsequent frames, resulting in a minimum mask length of l m frames. We replace each embedding that is selected for masking with a vector of ones. Masks can overlap, enabling longer mask spans than l m frames (especially with high p m ). Furthermore, we also define that each training and validation sequence needs to have at least one mask starting index during PFML pre-training.\n\nNote that the PFML pre-training process is not restricted to any specific type of neural networks. In the present study, we used convolutional neural networks (CNNs) as our encoder model, and T Transformer encoder blocks as the temporal model. However, any type of encoder could be used for PFML, as long as the encoder can convert time-series data into a sequence of embeddings. Furthermore, other temporal models, such as conformer-based models  [44]  or bidirectional recurrent neural networks  [45] ,  [46] , could also be used for PFML, as long as the model is able to take contextual information into account.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments",
      "text": "We evaluate our PFML method using three different datasets of time-series data with complex classification tasks: infant posture and movement classification from multi-sensor IMU data, emotion recognition from speech data, and sleep stage classification from EEG data. For each dataset, we first run SSL pre-training with unlabeled data using PFML, after which we fine-tune our models for downstream classification tasks using labeled data. We compare PFML against four different baselines: MAE  [28] , data2vec  [8] , TS2Vec  [35] , and not using pre-training at all. We selected MAE for our experiments since it is conceptually very similar to PFML, and we chose data2vec since it is the current state-of-the-art data-modalityagnostic SSL method. We also included the conceptually different TS2Vec in the present experiments due to its reported state-of-the-art performance on multiple different datasets of single-and multi-channel time-series data. In order to make the prediction of functionals directly comparable with predicting the input signal, we use a slightly modified version of MAE where we mask embeddings instead of masking inputs.\n\nThis section is organized as follows: First, Section IV-A gives general-level information on the pre-training and finetuning experiments regarding PFML, MAE, and data2vec methods that utilize identical neural network architecture for the end-use system. This is followed by Sections IV-B, IV-C, and IV-D, which give modality-specific experimental details for IMU, speech, and EEG data, respectively. Finally, Section IV-E explains the key differences between TS2Vec-related experiments and the experiments described in Section IV-A.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Pfml, Mae, And Data2Vec Experiments",
      "text": "For PFML pre-training, our models consist of a modalityspecific frame-level encoder (detailed in Sections IV-B, IV-C, and IV-D for IMU, speech, and EEG data, respectively) and a Transformer network consisting of T Transformer encoder blocks. Between the encoder and Transformer networks there is a CNN-based relative positional encoder followed by a Gaussian error linear unit (GELU)  [47]  activation and layer normalization  [48] . We frame our input signals before feeding the data into an encoder model, and we compute functionals from these frames as our training targets. For multi-channel data, we compute functionals separately for each channel. The functionals are then z-score normalized across the entire pretraining dataset. For computational efficiency, we pre-compute the functionals of each signal frame before the pre-training process. After the Transformer encoder blocks, we add a linear projection to convert the Transformer outputs into predicted functionals. After pre-training, this linear projection is discarded. Pre-training is run until validation loss convergence, and we use the model with the lowest validation loss as our pre-trained model. Starting from an initial learning rate, we gradually reduce the learning rate during model training with a reduction factor of 0.5 based on the plateauing of the validation loss.\n\nWe pre-train our models using MAE and data2vec in a similar manner as for PFML, and we use the same model architecture for all three pre-training algorithms. MAE pretraining is run in a similar manner as PFML pre-training, with the only exception of predicting the input signal frames instead of functionals. For data2vec pre-training, we used the instancenormalized  [49]  and averaged outputs of each feed-forward part of all Transformer encoder blocks as our training targets. If we observed that a representation collapse occurred during data2vec pre-training, we restarted the pre-training process. For further details on the data2vec algorithm, see Baevski et al.  [8] . We used mean squared error loss for all pre-training processes except for PFML with speech data, where we found L1 loss to work better.\n\nWe fine-tune our pre-trained models in two stages. In the first stage, two randomly initialized fully-connected GELU layers followed by a softmax function are added after the Transformer model. Then, these layers are fine-tuned separately as the weights of the encoder and Transformer are frozen. In the second stage, the entire model is fine-tuned with the same hyperparameters as in the first fine-tuning stage with one exception: The learning rate η is linearly increased from 0.001 • η to η during a warm-up period of 20 training epochs, followed by reduction by a factor of 0.5 based on validation loss plateauing. We use weighted categorical cross-entropy loss by weighting the loss of each sample by its class' inverse frequency.\n\nWe also test the linear separability of the features learned by our pre-trained models. In this case, we only add one linear layer followed by a softmax function after the Transformer model, and we fine-tune this single layer while the weights of the encoder and Transformer are frozen. As a baseline, we perform the same linear evaluation for a randomly initialized model without any pre-training.\n\nIn order to demonstrate the superiority of PFML against the state-of-the-art SSL method for multiple data modalities, data2vec, in terms of representation collapse, we ran each of the pre-training algorithms of the present experiments 10 times using the best hyperparameter combinations for each SSL method and for each data modality. We defined representation collapse to have occurred if the variance of either the embeddings or model outputs fell below 0.01 for 10 consecutive pre-training epochs, during which the validation loss was decreasing. In our preliminary experiments, we found that this condition was a good indicator of an upcoming representation collapse: A systematic decrease in the variance of a model's embeddings or outputs indicates impending representation collapse in SSL methods where the model can invent its own training targets.\n\nFor pre-training, we use the RAdam  [50] , AdamW  [51] , and Adam  [52]  optimizers for PFML, MAE, and data2vec, respectively. For fine-tuning, we use the Adam optimizer. For the ''no pre-training'' condition, we simply omit pre-training, the first fine-tuning stage, and the learning rate warm-up period of the second fine-tuning stage. To demonstrate fair comparison, we carefully select the pre-training and fine-tuning hyperparameters for each SSL method separately in order to minimize the number of representation collapses during SSL pre-training, and to maximize the fine-tuning performance. For a complete list of pre-training and fine-tuning hyperparameters, refer to Appendix B in the supplementary material. We used an NVIDIA Tesla V100 GPU to train our models, and we implemented the code using PyTorch version 1.13.1. Our implementation is publicly available on GitHub. 2",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Infant Posture And Movement Classification",
      "text": "For infant posture and movement classification, we use the multi-sensor IMU data from Airaksinen et al.  [53] . The data contains 24-channel signals from infants (three gyroscope and three accelerometer channels, four limbs) with a sampling rate of 52 Hz. We window the signals into 120-sample frames (approx. 2.3 seconds) with 50% overlap. For further details about the dataset, see Airaksinen et al.  [53] .\n\nFor model pre-training, we use a 387-hour subset of a 1333hour dataset of unlabeled IMU data  [54] . This subset contains infant free-form play that has been automatically screened for signal quality, and it was selected for the present experiments based on the findings of Vaaras et al.  [54] , where using the subset yielded the best results in terms of SSL pre-training. This subset contains 4669 sequences of 260 consecutive frames, each corresponding to five minutes of data. As the encoder, we use the same four-layer CNN-based encoder architecture as in Airaksinen et al.  [53]  with three minor modifications that were found to improve training efficiency and system performance when replicating the experiments of Airaksinen et al.  [53] : We added layer normalization after the last two convolutions to make the pre-training process more stable, the kernel size of the second convolutional layer of the CNN encoder was changed from  [4, 5]  to  [3, 5] , and the originally temporally asymmetrical padding was set to  [1, 2]  to make it symmetric. The pre-training data is randomly split into a training and validation set in a ratio of 80:20 sequences, and we input 260-frame sequences into the model.\n\nFor fine-tuning our pre-trained models, we use a 29-hour (91,449 frames) labeled dataset of IMU data  [53]  (41 recordings and distinct participants) for two separate tasks: posture classification and movement classification. The data contains nine annotated movement categories (still, roll left/right, pivot left/right, proto/elementary/fluent movement, transition) and seven annotated posture categories (prone, supine, left/right 2 https://github.com/SPEECHCOG/PFML side, crawl posture, sitting, standing) for each 2.3-second frame. For model training, we use all annotated data, but we only use the frames in which all annotators agreed on the label for model testing. We train our models separately for both classification tasks using the so-called iterative annotation refinement labels from Airaksinen et al.  [55] .\n\nModel fine-tuning is run using recording-level 10-fold cross-validation on the 41 distinct recordings of the labeled dataset. We",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Speech Emotion Recognition",
      "text": "We use the 56-hour subset of Finnish speech of the NICU-A corpus  [56]  for our speech emotion recognition experiments. This subset contains 129,007 utterances with a sampling rate of 16 kHz, of which 5198 and 345 belong to annotated training and testing sets, respectively. Each annotated utterance in NICU-A contains binary labels for emotional valence (positive/non-positive) and arousal (high/low). We window each speech signal into 30-ms frames with a 20-ms overlap. Each sequence is z-score normalized, and we zero-pad or truncate each normalized sequence into 3-second segments (301 frames). See Vaaras et al.  [56]  for further details on NICU-A.\n\nFor model pre-training, we use all 129,007 utterances, and we input 301-frame sequences to our model. We use a fourlayer CNN encoder (slightly modified audio encoder of van den Oord et al.  [4] ) with output channels [128, 128, 128, 128], kernel sizes  [10, 8, 4, 4] , strides of [5, 4, 2, 2], and paddings of [3, 2, 1, 1]. Each layer is followed by layer normalization, a GELU nonlinearity, and dropout. The last CNN layer is followed by average pooling with a kernel size of 6 before dropout. The pre-training utterances are randomly split into a training and validation set in a ratio of 80:20 sequences.\n\nWe fine-tune and test our models separately for both classification tasks (valence/arousal) using the labeled 5198and 345-utterance training and testing sets, respectively. The training set is randomly split into a training and validation set in a ratio of 80:20 utterances, and we select the best-performing model of the fine-tuning process based on the unweighted average recall (UAR) performance score on the validation set. This model is then used to compute the UAR performance score of the test set. See Appendix B in the supplementary material for further details regarding the pre-training and finetuning hyperparameters.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "D. Sleep Stage Classification",
      "text": "For sleep stage classification, we use the pre-processed expanded Sleep-EDF Database  [57] ,  [58]  from a study by Eldele et al.  [59] . The dataset contains 30-second segments of the Fpz-Cz channel with a sampling rate of 100 Hz, comprising a total of 195,479 segments of EEG data. Each 30-second segment belongs to one of five annotated categories: wake, rapid eye movement (REM), non-REM stage 1, non-REM stage 2, or non-REM stages 3 and 4 combined. We z-score normalize each 30-second segment, and we window each segment into 4-second frames with 2 seconds of overlap, resulting into 14 frames for each segment.\n\nWe pre-train our models using all 195,479 EEG segments. We use the 14-frame sequences as our input for a threelayer CNN encoder with output channels [128, 128, 128, 128], kernel sizes  [10, 8, 4] , strides of  [5, 5, 3] , and paddings of [3, 2, 1]. Each convolution is followed by layer normalization, a GELU nonlinearity, and dropout. The third CNN layer is followed by average pooling with a kernel size of 5 before dropout. We randomly split the EEG segments for pre-training into a training and validation set in a ratio of 80:20 segments.\n\nWe fine-tune our models for sleep stage classification using 10-fold cross-validation at the test subject-level on the 78 test subjects of the dataset. Each training fold is split into training and validation sets at the test subject-level in a ratio of 80:20 test subjects. Similar to Sec. IV-B, we use the validation UAF1 score as our training criterion, and the testing UAF1 score is computed from an aggregate confusion matrix across all test folds. For further details on the training hyperparameters, see Appendix B in the supplementary material.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "E. Ts2Vec Experiments",
      "text": "We also conducted a similar set of experiments as described in Section IV-A for the TS2Vec method. TS2Vec stands out from the other three SSL methods used in the present experiments due to its hierarchical contrastive learning algorithm, which is integrated into the pre-defined CNN encoder architecture. Therefore, TS2Vec does not allow the use of the same modalityspecific encoders used with PFML, MAE, and data2vec in our experiments. Additionally, it is not possible to pretrain the classification model (i.e. Transformer) in TS2Vec. Consequently, for all three data modalities (IMU, speech, and EEG data), we use the pre-defined TS2Vec encoder, and we do not pre-train the Transformer network. In order to compare the linear separability of the SSL features, we add one linear layer directly after the TS2Vec pre-trained encoder, and we finetune this single linear layer while the weights of the encoder are frozen.\n\nFor TS2Vec pre-training, we used the original TS2Vec implementation 3 with default hyperparameter settings, except for two modifications: First, we observed that the pre-defined number of minibatch updates was insufficient for our pretraining experiments. We found it more effective to run TS2Vec pre-training similarly to PFML, MAE, and data2vec, i.e. until 3 https://github.com/zhihanyue/ts2vec validation loss convergence, and then use the model with the lowest validation loss as the final pre-trained model. Second, we adjusted the output dimensionality of the TS2Vec encoder to match the modality-specific input dimensionalities of the Transformer (see Table  5  of Appendix B in the supplementary material).\n\nFor fine-tuning the TS2Vec pre-trained models, we added a randomly initialized Transformer model and two randomly initialized fully-connected GELU layers followed by a softmax function after the TS2Vec pre-trained encoder. The models were then fine-tuned similarly to the process described in Section IV-A, except that in the first fine-tuning stage, both the Transformer and the GELU layers were fine-tuned while the weights of the encoder were frozen. The fine-tuning hyperparameters were the same as those used for the other pre-training algorithms (see Table  6  of Appendix B in the supplementary material).\n\nTo demonstrate fair comparison, we also conducted three additional experiments using TS2Vec: First, similar to Section IV-A, we tested the ''no pre-training'' condition of the neural network architecture combining the TS2Vec encoder and Transformer classifier by fine-tuning the entire model at once using a randomly initialized model. Second, we tested the linear separability of a randomly initialized TS2Vec encoder by adding a single linear layer after the encoder and finetuning only this layer. Third, since the authors of the TS2Vec paper  [35]  used a support vector machine (SVM) classifier with a radial basis function kernel for the SSL features in classification tasks, we adopted the same procedure for our experimental pipeline. All of the TS2Vec-related results are presented in Appendix F in the supplementary material.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Results",
      "text": "Table  1  presents the fine-tuning results of the comparison of our PFML method against MAE, data2vec, TS2Vec, and not using pre-training at all. Across all three data modalities and five classification tasks, the end-use results show that PFML outperformed MAE and TS2Vec, and achieved results that are on par with data2vec. Using pre-training with any SSL method other than TS2Vec provided superior results as opposed to not using pre-training at all. Furthermore, for the classification of posture from IMU data, which is considered an easy task  [53] , there were only minor differences in performance between all SSL methods other than TS2Vec. The comparably weak results for TS2Vec are most probably due to the fact that TS2Vec cannot utilize modality-specific encoders, and also since the Transformer model is not pre-trained during TS2Vec pre-training. Using SVM models instead of Transformers in TS2Vec classification, as in the original article of Yue et al.  [35] , did not improve the results (see Appendix F in the supplementary material). In sleep stage classification from EEG data, both MAE and PFML outperformed data2vec by a large margin. Also, the comparison between PFML and MAE showcases that it is more beneficial to predict functionals than to predict the input signal.\n\nTable  2  shows the results of the linear evaluation experi-VOLUME 13, 2025   1 , PFML outperformed MAE and was comparable to data2vec when using the pretrained models as feature extractors for linear classifiers. Again, both MAE and PFML outperformed data2vec by a large margin in sleep stage classification from EEG data. In valence classification from speech data and sleep stage classification from EEG data, the TS2Vec method excelled. However, for other classification tasks, the SSL features provided by TS2Vec were the weakest performance-wise. In the case of using a randomly initialized model as a feature extractor for linear classifiers, the classification accuracy was at chance-level in all cases except when classifying posture for IMU data. The results of representation collapse experiments are shown in Table  3 . As can be seen from the results, it is very common for representation collapse to occur with data2vec across all data modalities. On the contrary, the results indicate that PFML, MAE, and TS2Vec do not suffer from representation collapse: PFML and TS2Vec did not experience representation collapses at all, and MAE had a representation collapse only once. Furthermore, we attribute this single representation collapse of MAE to bad luck in model weight initialization, as in this particular case the model loss started diverging from the beginning of the pre-training process. The results showcase that methods like PFML, MAE, and TS2Vec, whose training targets inherently contain variance, are less prone to representation collapse compared to methods like data2vec that learn their own prediction targets.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vi. Additional Hyperparameter Experiments",
      "text": "In order to demonstrate that it is more beneficial during pre-training to mask the latent features instead of masking the input directly, we ran PFML pre-training for all three datasets twice: either by masking the inputs or by masking the embeddings. Subsequently, we fine-tuned our models for all five classification tasks, and the results are shown in Table  7  of Appendix C in the supplementary material. As can be observed from the results, it is more beneficial for downstream tasks if we alleviate the complexity of the pre-training task for the encoder by masking the embeddings instead of masking the inputs. The only exception was with EEG data, where it did not make a difference whether inputs or embeddings were masked.\n\nFor each data modality, we also experimented with different configurations of masking probability p m and the length of the masks l m . We ran PFML pre-training using different configurations of p m and l m , and then we fine-tuned the pre-trained models. For IMU and speech data, we only experimented with one classification task each, namely classification of movement from IMU data and classification of valence from speech data. The results for different configurations of p m and l m for IMU, speech, and EEG data are shown in Appendix C in the supplementary material in Tables  8, 9 , and 10, respectively. For IMU data, the differences between different masking strategies are rather small, whereas for speech and EEG data a TS2Vec uses a different encoder compared to other experiments, since the TS2Vec algorithm is built into the pre-defined CNN encoder architecture.\n\nb In TS2Vec, the linear layer is added directly after the pre-trained encoder.\n\nthe selection of masking hyperparameters has a notable effect on fine-tuning performance. We also experimented with the effect of discarding some of the functionals in PFML pre-training for IMU data. After pretraining, we fine-tuned our model for movement classification, and the results are presented in Table  11  of Appendix C in the supplementary material. The results indicate that using the full set of 11 functionals during PFML pre-training provides the best outcome. As the number of discarded functionals increases, the prediction task becomes simpler and the training targets are able to capture less information of the input signal frames, leading to worse fine-tuning performance.\n\nFinally, we tested different mask types for PFML pretraining using IMU data. We either replaced the masked embeddings with a fixed vector of zeros, ones, random Gaussian noise (as in e.g. Baevski et al.  [11] ), or a learnable mask token (as in e.g. Baevski et al.  [8] ). After PFML pre-training using the four different mask types, we fine-tuned the pre-trained models for movement classification. Table  12  of Appendix C in the supplementary material presents the comparison results for different mask types. As can be observed, the choice between a mask of ones or random Gaussian noise does not have a notable impact on the performance. However, using a learnable mask token yielded slightly worse results than a vector of ones or random Gaussian noise, and a vector of zeros yielded the worst results. We observed that either using a vector of ones, random Gaussian noise, or learnable mask tokens for masking the embeddings promoted embedding variance, whereas using a vector of zeros provided a smaller level of variance for the embedding representations during pre-training. This lower level of variance for embeddings might potentially hinder the fine-tuning process, resulting into a lower performance in downstream tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, we presented PFML, a novel SSL algorithm for time-series data that avoids the common SSL issue of representation collapse. PFML operates by predicting statistical functionals of the input signal corresponding to masked embeddings, given a sequence of unmasked embeddings. We demonstrated the effectiveness of PFML using five different classification tasks across three different data modalities: infant posture and movement classification from multi-sensor IMU data, emotion recognition from speech data, and sleep stage classification from EEG data. Our results show that PFML is superior to both a conceptually similar SSL method, MAE, and a contrastive learning-based SSL method, TS2Vec. Our results also show that PFML is on par with the current state-of-the-art data modality agnostic SSL method, data2vec, while being conceptually simpler and without suffering from representation collapse. The pros and cons for each of the SSL methods used in the present study are shown in Table  4 . The fact that PFML matches the performance of data2vec while also avoiding the issue of representation collapse renders PFML more straightforward to apply to new time-series data domains, such as in the case of clinical time-series data. The present work may also be extended to other domains than time-series data, such as images where functionals could be computed of, e.g., image patches.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Limitations",
      "text": "We selected the present set of 11 functionals for their effectiveness across the three data modalities used in the present study, aiming for potential generalizability and a robust starting point to other data domains and downstream tasks. However, carefully selecting the number and type of functionals specifically for different modalities may lead to better results than presented here. Also, we did not include data augmentation in our pre-training processes to save computational time for PFML pre-training, as we wanted to pre-compute the functionals before the model training. As shown in e.g.  [1] ,  [6] ,  [26] ,  [28] , data augmentation during pre-training may lead to improved performance on downstream tasks. Nonetheless, performing masking for randomly sampled frames is already a form of data augmentation in itself. Furthermore, other model architectures besides CNN-based encoders or Transformer encoder blocks could also be used, and this may improve PFML pre-training performance. Lastly, we acknowledge that typically SSL pre-training is run with very large minibatch sizes using multiple GPUs, and the results of the present experiments might improve with larger minibatch sizes. However, to promote reproducibility and encourage other researchers to try PFML, we deliberately pre-trained our models using relatively small minibatches so that the pre-training processes could be run on a single GPU with 16 GB of VRAM. As detailed in Appendix E in the supplementary material, our method used only a moderate amount of computational resources.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Broader Impacts",
      "text": "Since the main goal of PFML is to make the algorithm straightforwardly applicable to different time-series data domains, our method makes it easier to apply SSL pre-training for time-  VOLUME 13, 2025  series data without complex tuning of hyperparameters or the need to profoundly understand the target data domain. As an example, properties of different medical time-series data, such as those obtained with EEG, ECG, or EMG, can be dependent on the clinical environment, the specific measurement equipment and setup, or clinical population being measured  [60] . This limits the applicability of ''universal'' pre-trained models predominant in computer vision and speech technology. In a similar manner, various industrial sensor setups, such as those for system monitoring and predictive maintenance (accelerometers, magnetometers etc.), can result in data unique to a particular environment or machine type. In these cases, the use of PFML pre-training can be practical, since applying modality-specific SSL algorithms or fine-tuning pre-trained models from other data modalities might not generalize well to novel time-series data domains. Hence, PFML may promote the use of machine learning as an assisting tool in e.g. clinical healthcare or other limited-data domains. However, as with all classifiers, machine-learning models trained using PFML might make errors. Incorrect model-based decisions, such as incorrect diagnoses, may be detrimental in some cases. Lastly, any bias, private information, or harmful content in the pre-training data can, in theory, be reflected to the feature representations that are learned by PFML.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Appendix B Pre-Training And Fine-Tuning Hyperparameters",
      "text": "This section provides details on the pre-training (Table  5 ) and fine-tuning (Table  6 ) hyperparameters of the present experiments.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Appendix C Results On Additional Hyperparameter Experiments",
      "text": "This section provides the result tables for the additional hyperparameter experiments in Section VI. Table  7  presents the fine-tuning results for the comparison between masking either the model inputs or embeddings during PFML pre-training. Tables  8, 9 , and 10 show the fine-tuning results for different configurations of masking probabilities (p m ) and mask lengths (l m ) for multi-sensor IMU, speech, and EEG data, respectively. Table  11  shows the fine-tuning results for discarding some of the 11 functionals in PFML pre-training for multi-sensor IMU data. Finally, Table  12  presents the fine-tuning results for different mask types for PFML pre-training for multi-sensor IMU data.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Appendix F Additional Ts2Vec Results",
      "text": "This section provides all TS2Vec-related results of the present experiments. Table  15  shows a summary of all of the results regarding TS2Vec for IMU, speech, and EEG data, further described in Section IV-E.",
      "page_start": 18,
      "page_end": 18
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: depicts an overview of the PFML pre-training",
      "page": 3
    },
    {
      "caption": "Figure 1: An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be",
      "page": 4
    },
    {
      "caption": "Figure 1: ), after which all zn",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "This work was supported in part by the Research Council of Finland under Grant 343498, and in part by the Sigrid Juselius Foundation."
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "Self-supervised learning (SSL)\nis a data-driven learning approach that utilizes the innate\nABSTRACT"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "structure of the data to guide the learning process. In contrast\nto supervised learning, which depends on"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "external labels, SSL utilizes the inherent characteristics of the data to produce its own supervisory signal."
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "However, one frequent\nissue with SSL methods is representation collapse, where the model outputs a"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "constant input-invariant feature representation. This issue hinders the potential application of SSL methods"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "to new data modalities, as trying to avoid representation collapse wastes researchers’ time and effort. This"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "paper introduces a novel SSL algorithm for time-series data called Prediction of Functionals from Masked"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "Latents (PFML). Instead of predicting masked input signals or their latent representations directly, PFML"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "operates by predicting statistical functionals of the input signal corresponding to masked embeddings, given"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "a sequence of unmasked embeddings. The algorithm is designed to avoid representation collapse, rendering"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "it straightforwardly applicable to different\ntime-series data domains, such as novel sensor modalities in"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "clinical data. We demonstrate the effectiveness of PFML through complex, real-life classification tasks"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "across three different data modalities: infant posture and movement classification from multi-sensor inertial"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "measurement unit data, emotion recognition from speech data, and sleep stage classification from EEG data."
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "The results show that PFML is superior to a conceptually similar SSL method and a contrastive learning-based"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "SSL method. Additionally, PFML is on par with the current state-of-the-art SSL method, while also being"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "conceptually simpler and without suffering from representation collapse. The code is freely available at"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "https://github.com/SPEECHCOG/PFML."
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "ment unit data, representation collapse, self-supervised learning, sleep stage classification, speech emotion"
        },
        {
          "Corresponding author: Einari Vaaras (e-mail: einari.vaaras@tuni.fi).": "recognition, statistical functionals, time-series data."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "ment unit data, representation collapse, self-supervised learning, sleep stage classification, speech emotion"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "recognition, statistical functionals, time-series data."
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "I.\nINTRODUCTION"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": ""
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "Self-supervised learning (SSL) can be described as a data-"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": ""
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "driven learning paradigm where the training process is guided"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": ""
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "by the inherent structure of the data itself. Unlike supervised"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "learning that relies on externally provided labels, SSL exploits"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "the intrinsic properties of the data to generate its own supervi-"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "sory signal [1], [2]. SSL enables the model to learn rich feature"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "representations from large amounts of unlabeled data that can"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "be used as a starting point for downstream tasks, either as such"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "or by fine-tuning the feature extractor to be better suited for"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "solving some specific task [2], [3]. Since typically there is"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "an abundance of unlabeled data but a scarcity of labeled data,"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "the use of SSL has been shown to reduce the need for large,"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "manually annotated datasets [4], [5], [6]. In addition to SSL"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "algorithms that have been developed for a single data modality,"
        },
        {
          "INDEX TERMS EEG data, embedding masking, human activity recognition, multi-sensor inertial measure-": "SSL algorithms that can be applied to multiple different data"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "minimal hyperparameter optimization, and without the risk of": "representation collapse. The contributions of the present study",
          "banded sparse attention patterns in their Transformer model.": "OpenAI [34] proposed an expanded version of the method by"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "are as follows:",
          "banded sparse attention patterns in their Transformer model.": "Brown et al. [32] by making the model not only larger, but also"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "capable of handling image inputs in addition to text inputs."
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "1) We propose a novel SSL algorithm for time-series data,",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "More recently, SSL literature has seen a growing number"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "PFML, that does not suffer from representation collapse,",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "of work towards SSL algorithms capable of running the pre-"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "rendering the method straightforward to apply to new",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "training task on multiple different data modalities. The authors"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "time-series data domains. To the best of our knowledge,",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "of van den Oord et al. [4] developed an SSL approach that"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "PFML is\nthe first work within the field of SSL for",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "predicts future embeddings based on previous context using"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "time-series data where the central idea of reconstructing",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "contrastive learning. They showed that their method was able"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "statistical functionals is utilized.",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "to learn useful feature representations for audio, image, text,"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "2) We demonstrate the effectiveness of PFML using three",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "and reinforcement\nlearning in 3D environments. The SSL"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "different data modalities with complex, real-life classifi-",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "method by Akbari et al.\n[7] also uses contrastive learning,"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "cation tasks: infant posture and movement classification",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "but their method simultaneously takes audio, video, and text"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "from multi-sensor inertial measurement unit (IMU) data,",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "data as input and creates multimodal feature representations."
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "emotion recognition from speech data, and sleep stage",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "These features were shown to work well with multiple different"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "classification from EEG data.",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "downstream tasks, i.e. video action recognition, audio event"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "3) We show that PFML obtains superior\nresults against",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "classification, image classification, and text-to-video retrieval."
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "both a conceptually similar SSL method and a con-",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "Baevski et al. [8] proposed data2vec, an SSL method for audio,"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "trastive\nlearning-based\nSSL method. Additionally,",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "image, and text data.\nIn their approach,\nthe model\ntries to"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "PFML achieves results that are on par with the current",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "predict masked latent features of an older version of itself that"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "state-of-the-art data modality-agnostic SSL method,",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "are both normalized and averaged over multiple Transformer"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "while\nalso being conceptually simpler\nand without",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "layers. Their\nresults in downstream tasks demonstrate the"
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "suffering from representation collapse.",
          "banded sparse attention patterns in their Transformer model.": ""
        },
        {
          "minimal hyperparameter optimization, and without the risk of": "",
          "banded sparse attention patterns in their Transformer model.": "effectiveness of the method in all three data modalities. Yue et"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "authors simulate noisy speech inputs and predict pseudo-labels"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "of the original speech from the masked embeddings."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Similar to the advances in SSL for speech data, there have"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "been significant developments in SSL for image data as well"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[6], [23], [24], [25], [26], [27], [28], [29], [30]. Grill et al."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[26] presented an SSL method that uses two neural networks"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "that\nlearn from each other’s\nrepresentations of differently"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "augmented views of the same image. He et al. [28] proposed"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "masked autoencoders (MAE) that try to reconstruct masked"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "patches of input images using an asymmetric encoder-decoder"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "architecture. The SSL algorithm by Bao et al. [29] tokenizes"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "images into visual tokens, followed by masking some image"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "patches and then trying to recover the original tokens from the"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "masked patches."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "SSL has\nalso excelled in natural\nlanguage processing"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[31], [32], [33], [34]. Devlin et al. [31] introduced an SSL"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "method which obtains bidirectional feature representations"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "from unlabeled text by conditioning on both the left and"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "right textual context. The method by Brown et al. [32] uses"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "an autoregressive model which alternates dense and locally"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "banded sparse attention patterns in their Transformer model."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "OpenAI [34] proposed an expanded version of the method by"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Brown et al. [32] by making the model not only larger, but also"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "capable of handling image inputs in addition to text inputs."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "More recently, SSL literature has seen a growing number"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "of work towards SSL algorithms capable of running the pre-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "training task on multiple different data modalities. The authors"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "of van den Oord et al. [4] developed an SSL approach that"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "predicts future embeddings based on previous context using"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "contrastive learning. They showed that their method was able"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "to learn useful feature representations for audio, image, text,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "and reinforcement\nlearning in 3D environments. The SSL"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "method by Akbari et al.\n[7] also uses contrastive learning,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "but their method simultaneously takes audio, video, and text"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "data as input and creates multimodal feature representations."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "These features were shown to work well with multiple different"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "downstream tasks, i.e. video action recognition, audio event"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "classification, image classification, and text-to-video retrieval."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Baevski et al. [8] proposed data2vec, an SSL method for audio,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "image, and text data.\nIn their approach,\nthe model\ntries to"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "predict masked latent features of an older version of itself that"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "are both normalized and averaged over multiple Transformer"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "layers. Their\nresults in downstream tasks demonstrate the"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "effectiveness of the method in all three data modalities. Yue et"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "al. [35] presented a universal framework, TS2Vec, for learning"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "time-series representations at different semantic levels using hi-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "erarchical contrastive learning over augmented context views."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Their approach enables robust contextual representations for"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "each timestamp and allows for simple aggregation to obtain"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "representations of arbitrary sub-sequences. They demonstrated"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "that their method obtained state-of-the-art performance in time-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "series classification, forecasting, and anomaly detection tasks"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "on multiple datasets, most of which were small in terms of the"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "number of training samples. Notably, TS2Vec achieved these"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "results by adding very simple classifiers after the pre-trained"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "model, which has fixed constraints on the model architecture"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "to model\nthe hierarchical\nrepresentations. Wang et al.\n[9]"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "proposed an SSL method that performs prediction of masked"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "tokens in a unified manner on images, texts, and image-text"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "pairs. Their experiments showed that their method achieves"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "state-of-the-art performance on various vision and vision-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "language tasks."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "For modality agnostic SSL algorithms, objective functions"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "play a crucial\nrole in guiding the learning process. These"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "functions can be broadly categorized into three types: instance"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "discrimination, clustering, and masked prediction. Instance"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "discrimination aims to distinguish between different instances"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "of data, thereby encouraging the model to learn unique features"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "for each instance and enhancing the discriminative power"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "of the learned representations. Contrastive learning methods,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "such as [4], [5], [7], [35], [36], [37], are an example of instance"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "discrimination-based SSL methods. Clustering, on the other"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "hand, groups similar instances together in the feature space,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "fostering the model to learn common features among instances"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "belonging to the same group. Methods like [17], [18], [38] are"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "examples of clustering-based SSL methods. Lastly, masked"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "prediction involves the task of predicting masked parts of the"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "input data based on the unmasked parts, thereby encouraging"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "the model\nto learn contextual relationships within the data."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Examples of such SSL methods include [8], [28], [31], [39],"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[40]."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "III. METHOD"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "A. MOTIVATION"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "One key issue with many SSL methods is the problem of"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "representation collapse, where the model outputs a constant,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "input-invariant feature representation, leading to a trivial solu-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "tion of the pre-training task [1], [20]. This considerably slows"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "down the development process for novel data domains and/or"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "tasks due to the necessity of operating in uncertainty, when"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "it\nis not clear whether the representation collapse is caused"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "by an ill-posed task or by the SSL algorithm. To avoid this,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "SSL methods have taken several different countermeasures:"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Baevski et al. [5] use the same target representations in their"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "contrastive learning task in a dual manner,\ni.e. both as a"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "positive and a negative example. Grill et al. [26] both add an"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "additional predictor to their training regime and use a moving"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "average of\ntheir\nso-called online neural network to avoid"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "representation collapse. Bardes et al. [41] add a regularization"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "term to their\nloss function that both maintains variance of"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "the embeddings and decorrelates each pair of variables. In"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "data2vec [8],\nthe authors tackle representation collapse by"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "carefully selecting their model hyperparameters and promoting"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "target representation variance through feature normalization."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Also, in the code implementation of data2vec1, pre-training is"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "stopped if the variance of either model predictions or training"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "targets falls below a predefined threshold."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Intuitively, given a trivial\ntask,\nthe model does not\nlearn"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "useful feature representations during pre-training. In contrast,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "1https://github.com/facebookresearch/fairseq/tree/main/examples/"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "data2vec"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "y0": "",
          "y1": "",
          "y2": "",
          "y3": "",
          "y4": ""
        },
        {
          "y0": "",
          "y1": "",
          "y2": "",
          "y3": "",
          "y4": ""
        },
        {
          "y0": "",
          "y1": "Transformer network",
          "y2": "",
          "y3": "",
          "y4": ""
        },
        {
          "y0": "",
          "y1": "",
          "y2": "",
          "y3": "",
          "y4": ""
        },
        {
          "y0": "z0",
          "y1": "z1",
          "y2": "z2",
          "y3": "z3",
          "y4": "z4"
        },
        {
          "y0": "",
          "y1": "",
          "y2": "",
          "y3": "",
          "y4": ""
        },
        {
          "y0": "",
          "y1": "",
          "y2": "",
          "y3": "",
          "y4": ""
        },
        {
          "y0": "",
          "y1": "",
          "y2": "Encoder",
          "y3": "",
          "y4": ""
        },
        {
          "y0": "",
          "y1": "",
          "y2": "",
          "y3": "",
          "y4": ""
        },
        {
          "y0": "",
          "y1": "",
          "y2": "",
          "y3": "",
          "y4": ""
        },
        {
          "y0": "x0",
          "y1": "x1",
          "y2": "x2",
          "y3": "x3",
          "y4": "x4"
        },
        {
          "y0": "",
          "y1": "",
          "y2": "",
          "y3": "",
          "y4": ""
        },
        {
          "y0": "",
          "y1": "",
          "y2": "",
          "y3": "",
          "y4": ""
        },
        {
          "y0": "",
          "y1": "",
          "y2": "",
          "y3": "",
          "y4": ""
        },
        {
          "y0": "",
          "y1": "",
          "y2": "",
          "y3": "",
          "y4": ""
        },
        {
          "y0": "",
          "y1": "",
          "y2": "",
          "y3": "",
          "y4": ""
        },
        {
          "y0": "",
          "y1": "",
          "y2": "",
          "y3": "",
          "y4": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "applied to multi-channel time-series data."
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "Some of these embeddings are masked randomly at time steps"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "M (for example, M\n1, 2\nin Figure 1), after which all zn"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "∈ {\n}"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "are used as an input for a Transformer-based model to obtain"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "outputs yn. Finally, a prediction loss is computed between"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "functional\nthe outputs of masked time steps yM and their"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "counterparts fM . As a result, PFML pre-training optimizes the"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "prediction of functionals of input signal frames corresponding"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "to the masked embeddings, given the unmasked embeddings"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "from the temporal context of these frames."
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "In PFML, predicting only one or a few functionals of a"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "framed signal can be a trivial\ntask, and will most probably"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "lead to learning feature representations that are not very useful"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "for downstream tasks. However, as the number of functionals"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": ""
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "that each describe some property of the framed signal grows,"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": ""
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "a more accurate description of the signal can be obtained (see"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": ""
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": ""
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "e.g. McDermott & Simoncelli [42] for representing perceptual"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": ""
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "properties of sound textures with functionals). Therefore, as"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": ""
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "the number of different functionals grows, the PFML algorithm"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": ""
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "is getting closer to predicting all of the nuances of the input"
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": ""
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": ""
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "signal."
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": ""
        },
        {
          "FIGURE 1. An overview of the PFML pre-training pipeline. Note that in the figure, the input signal has only a single channel, whereas PFML can also be": "Let us assume the following in PFML pre-training:"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": "is getting closer to predicting all of the nuances of the input"
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": "signal."
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": "•"
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": "•"
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": ""
        },
        {
          "the number of different functionals grows, the PFML algorithm": "Under these assumptions, as the model"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Note that the PFML pre-training process is not restricted to"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "any specific type of neural networks. In the present study, we"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "used convolutional neural networks (CNNs) as our encoder"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "model, and T Transformer encoder blocks as the temporal"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "model. However, any type of encoder could be used for PFML,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "as long as the encoder can convert\ntime-series data into a"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "sequence of embeddings. Furthermore, other temporal models,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "such as conformer-based models [44] or bidirectional recurrent"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "neural networks [45], [46], could also be used for PFML, as"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "long as the model is able to take contextual information into"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "account."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "IV. EXPERIMENTS"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "We evaluate our PFML method using three different datasets"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "of time-series data with complex classification tasks: infant"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "posture and movement classification from multi-sensor IMU"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "data, emotion recognition from speech data, and sleep stage"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "classification from EEG data. For each dataset, we first run"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "SSL pre-training with unlabeled data using PFML, after which"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "we fine-tune our models for downstream classification tasks"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "using labeled data. We compare PFML against four different"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "baselines: MAE [28], data2vec [8], TS2Vec [35], and not"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "using pre-training at all. We selected MAE for our experiments"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "since it is conceptually very similar to PFML, and we chose"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "data2vec since it is the current state-of-the-art data-modality-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "agnostic SSL method. We also included the conceptually"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "different TS2Vec in the present experiments due to its reported"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "state-of-the-art performance on multiple different datasets of"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "single- and multi-channel time-series data. In order to make the"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "prediction of functionals directly comparable with predicting"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "the input signal, we use a slightly modified version of MAE"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "where we mask embeddings instead of masking inputs."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "This section is organized as follows: First, Section IV-A"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "gives general-level information on the pre-training and fine-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "tuning experiments regarding PFML, MAE, and data2vec"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "methods that utilize identical neural network architecture for"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "the end-use system. This is followed by Sections IV-B, IV-C,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "and IV-D, which give modality-specific experimental details"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "for IMU, speech, and EEG data, respectively. Finally, Section"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "IV-E explains the key differences between TS2Vec-related"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "experiments and the experiments described in Section IV-A."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "A. PFML, MAE, AND DATA2VEC EXPERIMENTS"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "For PFML pre-training, our models consist of a modality-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "specific frame-level encoder (detailed in Sections IV-B, IV-C,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "and IV-D for IMU, speech, and EEG data, respectively) and"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "a Transformer network consisting of T Transformer encoder"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "blocks. Between the encoder and Transformer networks there"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "is a CNN-based relative positional encoder\nfollowed by a"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Gaussian error linear unit (GELU) [47] activation and layer"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "normalization [48]. We frame our input signals before feeding"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "the data into an encoder model, and we compute functionals"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "from these frames as our training targets. For multi-channel"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "data, we compute functionals separately for each channel. The"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "functionals are then z-score normalized across the entire pre-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "training dataset. For computational efficiency, we pre-compute"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "VOLUME 13, 2025"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "side, crawl posture, sitting, standing)\nfor each 2.3-second"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "frame. For model training, we use all annotated data, but we"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "only use the frames in which all annotators agreed on the label"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "for model\ntesting. We train our models separately for both"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "classification tasks using the so-called iterative annotation"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "refinement labels from Airaksinen et al. [55]."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Model fine-tuning is\nrun using recording-level 10-fold"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "cross-validation on the 41 distinct recordings of the labeled"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "dataset. We split each training fold into separate training and"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "validation sets in a ratio of 80:20 recordings. Due to large class"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "imbalances in the labeled dataset of IMU data (see Airaksinen"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "et al. [53]), the unweighted average F1 score (UAF1) is used"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "as our performance metric. We use UAF1 on the validation"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "set as the training criterion, and we select the best-performing"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "model based on validation set UAF1 score. We use random"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "sensor dropout\nfor data augmentation during\n(p = 0.3)"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "model fine-tuning. The final UAF1 score of fine-tuning is"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "computed from an aggregate confusion matrix across all test"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "folds. For further details regarding the pre-training and fine-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "tuning hyperparameters, see Appendix B in the supplementary"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "material."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "C. SPEECH EMOTION RECOGNITION"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "We use the 56-hour subset of Finnish speech of the NICU-A"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "corpus [56] for our speech emotion recognition experiments."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "This\nsubset contains 129,007 utterances with a sampling"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "rate of 16 kHz, of which 5198 and 345 belong to annotated"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "training and testing sets, respectively. Each annotated utterance"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "in NICU-A contains binary labels\nfor emotional valence"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "(positive/non-positive) and arousal (high/low). We window"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "each speech signal into 30-ms frames with a 20-ms overlap."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Each sequence is z-score normalized, and we zero-pad or"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "truncate each normalized sequence into 3-second segments"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "(301 frames). See Vaaras et al.\n[56]\nfor\nfurther details on"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "NICU-A."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "For model pre-training, we use all 129,007 utterances, and"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "we input 301-frame sequences to our model. We use a four-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "layer CNN encoder (slightly modified audio encoder of van"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "den Oord et al. [4]) with output channels [128, 128, 128, 128],"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "kernel sizes [10, 8, 4, 4], strides of [5, 4, 2, 2], and paddings"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "of [3, 2, 1, 1]. Each layer is followed by layer normalization,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "a GELU nonlinearity, and dropout. The last CNN layer\nis"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "followed by average pooling with a kernel size of 6 before"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "dropout. The pre-training utterances are randomly split into a"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "training and validation set in a ratio of 80:20 sequences."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "We fine-tune\nand test our models\nseparately for both"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "classification tasks (valence/arousal) using the labeled 5198-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "and 345-utterance training and testing sets, respectively. The"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "training set is randomly split into a training and validation set in"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "a ratio of 80:20 utterances, and we select the best-performing"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "model of the fine-tuning process based on the unweighted"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "average recall (UAR) performance score on the validation set."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "This model\nis then used to compute the UAR performance"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "score of the test set. See Appendix B in the supplementary"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "material for further details regarding the pre-training and fine-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "tuning hyperparameters."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: of Appendix B in the",
      "data": [
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "D. SLEEP STAGE CLASSIFICATION"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "For sleep stage classification, we use the pre-processed ex-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "panded Sleep-EDF Database [57], [58] from a study by Eldele"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "et al. [59]. The dataset contains 30-second segments of the Fpz-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Cz channel with a sampling rate of 100 Hz, comprising a total"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "of 195,479 segments of EEG data. Each 30-second segment"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "belongs to one of five annotated categories: wake, rapid eye"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "movement (REM), non-REM stage 1, non-REM stage 2, or"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "non-REM stages 3 and 4 combined. We z-score normalize"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "each 30-second segment, and we window each segment into"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "4-second frames with 2 seconds of overlap, resulting into 14"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "frames for each segment."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "We pre-train our models using all 195,479 EEG segments."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "We use the 14-frame sequences as our\ninput\nfor a three-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "layer CNN encoder with output channels [128, 128, 128, 128],"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "kernel\nsizes\nstrides of\n[10, 8, 4],\n[5, 5, 3], and paddings of"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[3, 2, 1]. Each convolution is followed by layer normalization,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "a GELU nonlinearity, and dropout. The third CNN layer is"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "followed by average pooling with a kernel size of 5 before"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "dropout. We randomly split the EEG segments for pre-training"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "into a training and validation set in a ratio of 80:20 segments."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "We fine-tune our models for sleep stage classification using"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "10-fold cross-validation at the test subject-level on the 78 test"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "subjects of the dataset. Each training fold is split into training"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "and validation sets at the test subject-level in a ratio of 80:20"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "test subjects. Similar to Sec. IV-B, we use the validation UAF1"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "score as our training criterion, and the testing UAF1 score is"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "computed from an aggregate confusion matrix across all test"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "folds. For further details on the training hyperparameters, see"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Appendix B in the supplementary material."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "E. TS2VEC EXPERIMENTS"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "We also conducted a similar set of experiments as described in"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Section IV-A for the TS2Vec method. TS2Vec stands out from"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "the other three SSL methods used in the present experiments"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "due to its hierarchical contrastive learning algorithm, which"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "is integrated into the pre-defined CNN encoder architecture."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Therefore, TS2Vec does not allow the use of the same modality-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "specific\nencoders used with PFML, MAE,\nand data2vec"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "in our experiments. Additionally,\nit\nis not possible to pre-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "train the classification model (i.e. Transformer) in TS2Vec."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Consequently, for all three data modalities (IMU, speech, and"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "EEG data), we use the pre-defined TS2Vec encoder, and we do"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "not pre-train the Transformer network. In order to compare the"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "linear separability of the SSL features, we add one linear layer"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "directly after the TS2Vec pre-trained encoder, and we fine-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "tune this single linear layer while the weights of the encoder"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "are frozen."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "For TS2Vec pre-training, we used the original TS2Vec"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "implementation3 with default hyperparameter settings, except"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "for two modifications: First, we observed that the pre-defined"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "number of minibatch updates was insufficient\nfor our pre-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "training experiments. We found it more effective to run TS2Vec"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "pre-training similarly to PFML, MAE, and data2vec, i.e. until"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "3https://github.com/zhihanyue/ts2vec"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "VOLUME 13, 2025"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE 1. Downstream task fine-tuning results for PFML, data2vec, MAE, TS2Vec, and not using pre-training at all for the five different classification tasks": "across the three different data modalities (IMU, speech, and EEG data)."
        },
        {
          "TABLE 1. Downstream task fine-tuning results for PFML, data2vec, MAE, TS2Vec, and not using pre-training at all for the five different classification tasks": ""
        },
        {
          "TABLE 1. Downstream task fine-tuning results for PFML, data2vec, MAE, TS2Vec, and not using pre-training at all for the five different classification tasks": ""
        },
        {
          "TABLE 1. Downstream task fine-tuning results for PFML, data2vec, MAE, TS2Vec, and not using pre-training at all for the five different classification tasks": ""
        },
        {
          "TABLE 1. Downstream task fine-tuning results for PFML, data2vec, MAE, TS2Vec, and not using pre-training at all for the five different classification tasks": ""
        },
        {
          "TABLE 1. Downstream task fine-tuning results for PFML, data2vec, MAE, TS2Vec, and not using pre-training at all for the five different classification tasks": ""
        },
        {
          "TABLE 1. Downstream task fine-tuning results for PFML, data2vec, MAE, TS2Vec, and not using pre-training at all for the five different classification tasks": ""
        },
        {
          "TABLE 1. Downstream task fine-tuning results for PFML, data2vec, MAE, TS2Vec, and not using pre-training at all for the five different classification tasks": "No pre-training"
        },
        {
          "TABLE 1. Downstream task fine-tuning results for PFML, data2vec, MAE, TS2Vec, and not using pre-training at all for the five different classification tasks": "MAE"
        },
        {
          "TABLE 1. Downstream task fine-tuning results for PFML, data2vec, MAE, TS2Vec, and not using pre-training at all for the five different classification tasks": "data2vec"
        },
        {
          "TABLE 1. Downstream task fine-tuning results for PFML, data2vec, MAE, TS2Vec, and not using pre-training at all for the five different classification tasks": "TS2Veca"
        },
        {
          "TABLE 1. Downstream task fine-tuning results for PFML, data2vec, MAE, TS2Vec, and not using pre-training at all for the five different classification tasks": "PFML (ours)"
        },
        {
          "TABLE 1. Downstream task fine-tuning results for PFML, data2vec, MAE, TS2Vec, and not using pre-training at all for the five different classification tasks": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "UAF1 (%)": "",
          "UAR (%)": ""
        },
        {
          "UAF1 (%)": "EEG data",
          "UAR (%)": ""
        },
        {
          "UAF1 (%)": "",
          "UAR (%)": "Speech data"
        },
        {
          "UAF1 (%)": "(sleep stage",
          "UAR (%)": ""
        },
        {
          "UAF1 (%)": "",
          "UAR (%)": "(speech emotion recognition)"
        },
        {
          "UAF1 (%)": "classification)",
          "UAR (%)": ""
        },
        {
          "UAF1 (%)": "Sleep stage",
          "UAR (%)": ""
        },
        {
          "UAF1 (%)": "22.8",
          "UAR (%)": ""
        },
        {
          "UAF1 (%)": "56.0",
          "UAR (%)": ""
        },
        {
          "UAF1 (%)": "53.9",
          "UAR (%)": ""
        },
        {
          "UAF1 (%)": "57.0",
          "UAR (%)": ""
        },
        {
          "UAF1 (%)": "56.3",
          "UAR (%)": ""
        },
        {
          "UAF1 (%)": "UAF1 (%)",
          "UAR (%)": "UAR (%)"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "PFML (ours) \n0/10 \n0/10 \n0/10"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "ments. Similar to the results of Table 1, PFML outperformed"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "MAE and was comparable to data2vec when using the pre-"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "trained models as\nfeature extractors\nfor\nlinear classifiers."
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "Again, both MAE and PFML outperformed data2vec by a large"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "margin in sleep stage classification from EEG data. In valence"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "classification from speech data and sleep stage classification"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "from EEG data, the TS2Vec method excelled. However, for"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "other classification tasks, the SSL features provided by TS2Vec"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "were the weakest performance-wise. In the case of using a"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "randomly initialized model as a feature extractor for linear"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "classifiers, the classification accuracy was at chance-level in"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "all cases except when classifying posture for IMU data."
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "The\nresults of\nrepresentation collapse\nexperiments\nare"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "shown in Table 3. As can be seen from the results, it is very"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "common for representation collapse to occur with data2vec"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "across all data modalities. On the contrary, the results indicate"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "that PFML, MAE, and TS2Vec do not\nsuffer\nfrom repre-"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "sentation collapse: PFML and TS2Vec did not experience"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "representation collapses at all, and MAE had a representation"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "collapse only once. Furthermore, we attribute this\nsingle"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "representation collapse of MAE to bad luck in model weight"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "initialization, as in this particular case the model loss started"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "diverging from the beginning of the pre-training process. The"
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": ""
        },
        {
          "TS2Vec \n0/10 \n0/10 \n0/10": "results showcase that methods like PFML, MAE, and TS2Vec,"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 11: of Appendix C in SSL methods used in the present study are shown in Table",
      "data": [
        {
          "MAE \n± \n+": "data2vec \n+ \n−",
          "+ \n+ \n± \n+": "− \n+ \n± \n−"
        },
        {
          "MAE \n± \n+": "PFML (ours) \n+ \n+",
          "+ \n+ \n± \n+": "+ \n+ \n+ \n±"
        },
        {
          "MAE \n± \n+": "the selection of masking hyperparameters has a notable effect",
          "+ \n+ \n± \n+": "MAE, and a contrastive learning-based SSL method, TS2Vec."
        },
        {
          "MAE \n± \n+": "on fine-tuning performance.",
          "+ \n+ \n± \n+": "Our results also show that PFML is on par with the current"
        },
        {
          "MAE \n± \n+": "We also experimented with the effect of discarding some of",
          "+ \n+ \n± \n+": "state-of-the-art data modality agnostic SSL method, data2vec,"
        },
        {
          "MAE \n± \n+": "the functionals in PFML pre-training for IMU data. After pre-",
          "+ \n+ \n± \n+": "while being conceptually simpler and without suffering from"
        },
        {
          "MAE \n± \n+": "training, we fine-tuned our model for movement classification,",
          "+ \n+ \n± \n+": "representation collapse. The pros and cons for each of\nthe"
        },
        {
          "MAE \n± \n+": "and the results are presented in Table 11 of Appendix C in",
          "+ \n+ \n± \n+": "SSL methods used in the present study are shown in Table"
        },
        {
          "MAE \n± \n+": "the supplementary material. The results indicate that using the",
          "+ \n+ \n± \n+": "4. The fact that PFML matches the performance of data2vec"
        },
        {
          "MAE \n± \n+": "full set of 11 functionals during PFML pre-training provides",
          "+ \n+ \n± \n+": "while also avoiding the issue of representation collapse renders"
        },
        {
          "MAE \n± \n+": "the best outcome. As the number of discarded functionals",
          "+ \n+ \n± \n+": "PFML more straightforward to apply to new time-series data"
        },
        {
          "MAE \n± \n+": "increases, the prediction task becomes simpler and the training",
          "+ \n+ \n± \n+": "domains, such as in the case of clinical time-series data. The"
        },
        {
          "MAE \n± \n+": "targets are able to capture less information of the input signal",
          "+ \n+ \n± \n+": "present work may also be extended to other domains than"
        },
        {
          "MAE \n± \n+": "frames, leading to worse fine-tuning performance.",
          "+ \n+ \n± \n+": "time-series data, such as images where functionals could be"
        },
        {
          "MAE \n± \n+": "Finally, we tested different mask types\nfor PFML pre-",
          "+ \n+ \n± \n+": "computed of, e.g., image patches."
        },
        {
          "MAE \n± \n+": "training using IMU data. We either replaced the masked em-",
          "+ \n+ \n± \n+": ""
        },
        {
          "MAE \n± \n+": "beddings with a fixed vector of zeros, ones, random Gaussian",
          "+ \n+ \n± \n+": "A. LIMITATIONS"
        },
        {
          "MAE \n± \n+": "noise (as in e.g. Baevski et al. [11]), or a learnable mask token",
          "+ \n+ \n± \n+": "We selected the present set of 11 functionals for their effec-"
        },
        {
          "MAE \n± \n+": "(as in e.g. Baevski et al. [8]). After PFML pre-training using",
          "+ \n+ \n± \n+": "tiveness across the three data modalities used in the present"
        },
        {
          "MAE \n± \n+": "the four different mask types, we fine-tuned the pre-trained",
          "+ \n+ \n± \n+": "study, aiming for potential generalizability and a robust starting"
        },
        {
          "MAE \n± \n+": "models for movement classification. Table 12 of Appendix C in",
          "+ \n+ \n± \n+": "point to other data domains and downstream tasks. However,"
        },
        {
          "MAE \n± \n+": "the supplementary material presents the comparison results for",
          "+ \n+ \n± \n+": "carefully selecting the number and type of functionals specifi-"
        },
        {
          "MAE \n± \n+": "different mask types. As can be observed, the choice between",
          "+ \n+ \n± \n+": "cally for different modalities may lead to better results than"
        },
        {
          "MAE \n± \n+": "a mask of ones or random Gaussian noise does not have a",
          "+ \n+ \n± \n+": "presented here. Also, we did not include data augmentation"
        },
        {
          "MAE \n± \n+": "notable impact on the performance. However, using a learnable",
          "+ \n+ \n± \n+": "in our pre-training processes\nto save computational\ntime"
        },
        {
          "MAE \n± \n+": "mask token yielded slightly worse results than a vector of ones",
          "+ \n+ \n± \n+": "for PFML pre-training, as we wanted to pre-compute the"
        },
        {
          "MAE \n± \n+": "or random Gaussian noise, and a vector of zeros yielded the",
          "+ \n+ \n± \n+": "functionals before the model training. As shown in e.g. [1],"
        },
        {
          "MAE \n± \n+": "worst results. We observed that either using a vector of ones,",
          "+ \n+ \n± \n+": "[6], [26], [28], data augmentation during pre-training may lead"
        },
        {
          "MAE \n± \n+": "random Gaussian noise, or learnable mask tokens for masking",
          "+ \n+ \n± \n+": "to improved performance on downstream tasks. Nonetheless,"
        },
        {
          "MAE \n± \n+": "the embeddings promoted embedding variance, whereas using",
          "+ \n+ \n± \n+": "performing masking for randomly sampled frames is already a"
        },
        {
          "MAE \n± \n+": "a vector of zeros provided a smaller level of variance for the",
          "+ \n+ \n± \n+": "form of data augmentation in itself. Furthermore, other model"
        },
        {
          "MAE \n± \n+": "embedding representations during pre-training. This lower",
          "+ \n+ \n± \n+": "architectures besides CNN-based encoders or Transformer"
        },
        {
          "MAE \n± \n+": "level of variance for embeddings might potentially hinder the",
          "+ \n+ \n± \n+": "encoder blocks could also be used, and this may improve"
        },
        {
          "MAE \n± \n+": "fine-tuning process,\nresulting into a lower performance in",
          "+ \n+ \n± \n+": "PFML pre-training performance. Lastly, we acknowledge that"
        },
        {
          "MAE \n± \n+": "downstream tasks.",
          "+ \n+ \n± \n+": "typically SSL pre-training is run with very large minibatch"
        },
        {
          "MAE \n± \n+": "",
          "+ \n+ \n± \n+": "sizes using multiple GPUs, and the results of the present exper-"
        },
        {
          "MAE \n± \n+": "VII. CONCLUSION",
          "+ \n+ \n± \n+": "iments might improve with larger minibatch sizes. However, to"
        },
        {
          "MAE \n± \n+": "In this paper, we presented PFML, a novel SSL algorithm",
          "+ \n+ \n± \n+": "promote reproducibility and encourage other researchers to try"
        },
        {
          "MAE \n± \n+": "for\ntime-series data that avoids the common SSL issue of",
          "+ \n+ \n± \n+": "PFML, we deliberately pre-trained our models using relatively"
        },
        {
          "MAE \n± \n+": "representation collapse. PFML operates by predicting statis-",
          "+ \n+ \n± \n+": "small minibatches so that the pre-training processes could be"
        },
        {
          "MAE \n± \n+": "tical functionals of the input signal corresponding to masked",
          "+ \n+ \n± \n+": "run on a single GPU with 16 GB of VRAM. As detailed in"
        },
        {
          "MAE \n± \n+": "embeddings, given a sequence of unmasked embeddings. We",
          "+ \n+ \n± \n+": "Appendix E in the supplementary material, our method used"
        },
        {
          "MAE \n± \n+": "demonstrated the effectiveness of PFML using five different",
          "+ \n+ \n± \n+": "only a moderate amount of computational resources."
        },
        {
          "MAE \n± \n+": "classification tasks across\nthree different data modalities:",
          "+ \n+ \n± \n+": ""
        },
        {
          "MAE \n± \n+": "infant posture and movement classification from multi-sensor",
          "+ \n+ \n± \n+": "B. BROADER IMPACTS"
        },
        {
          "MAE \n± \n+": "IMU data, emotion recognition from speech data, and sleep",
          "+ \n+ \n± \n+": "Since the main goal of PFML is to make the algorithm straight-"
        },
        {
          "MAE \n± \n+": "stage classification from EEG data. Our\nresults show that",
          "+ \n+ \n± \n+": "forwardly applicable to different time-series data domains, our"
        },
        {
          "MAE \n± \n+": "PFML is superior to both a conceptually similar SSL method,",
          "+ \n+ \n± \n+": "method makes it easier to apply SSL pre-training for time-"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "BEIT Pretraining for Vision and Vision-Language Tasks,’’ in Proc. IEEE"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "CVPR, 2023, pp. 19 175–19 186."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[10] O. J. Hénaff, A. Srinivas, J. De Fauw, A. Razavi, C. Doersch, S. M. A."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Eslami, and A. Van Den Oord,\n‘‘Data-efficient\nimage recognition with"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "contrastive predictive coding,’’ in Proc. ICML, 2020, pp. 4182–4192."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[11] A. Baevski, A. Babu, W.-N. Hsu, and M. Auli, ‘‘Efficient self-supervised"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "learning with contextualized target representations for vision, speech and"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "language,’’ in Proc. ICML, 2023, pp. 1416–1429."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[12]\nJ. W. Yoon, S. M. Kim, and N. S. Kim, ‘‘MCR-Data2vec 2.0: Improving"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Self-supervised Speech Pre-training via Model-level Consistency Regular-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "ization,’’ in Proc. INTERSPEECH, 2023, pp. 2833–2837."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[13] Q.-S. Zhu, L. Zhou, J. Zhang, S.-J. Liu, Y.-C. Hu, and L.-R. Dai, ‘‘Robust"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Data2VEC: Noise-Robust Speech Representation Learning for ASR by"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Combining Regression and Improved Contrastive Learning,’’ in Proc. IEEE"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "ICASSP, 2023, pp. 1–5."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[14]\nJ. Lian, A. Baevski, W.-N. Hsu,\nand M. Auli,\n‘‘Av-Data2Vec: Self-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Supervised Learning of Audio-Visual Speech Representations with Contex-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "tualized Target Representations,’’ in IEEE ASRU, 2023, pp. 1–8."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[15] Y. Kalantidis, M. B. Sariyildiz, N. Pion, P. Weinzaepfel, and D. Larlus,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "‘‘Hard negative mixing for contrastive learning,’’ in Proc. NeurIPS, 2020,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "pp. 21 798—-21 809."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[16]\nJ. Robinson, C.-Y. Chuang, S. Sra, and S. Jegelka, ‘‘Contrastive Learning"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "with Hard Negative Samples,’’ in Proc. ICLR, 2021."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[17] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, ‘‘Un-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "supervised learning of visual features by contrasting cluster assignments,’’"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "in Proc. NeurIPS, 2020, pp. 9912—-9924."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "A. Mohamed, ‘‘HuBERT: Self-Supervised Speech Representation Learning"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "by Masked Prediction of Hidden Units,’’ IEEE/ACM Trans. Audio, Speech"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "and Lang. Proc., vol. 29, p. 3451–3460, 2021."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "T. Hua, W. Wang, Z. Xue, S. Ren, Y. Wang, and H. Zhao, ‘‘On Feature\n[19]"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Decorrelation in Self-Supervised Learning,’’ in Proc. IEEE ICCV, 2021, pp."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "9578–9588."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[20]\nL. Jing, P. Vincent, Y. LeCun, and Y. Tian, ‘‘Understanding Dimensional"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Collapse in Contrastive Self-supervised Learning,’’ in Proc. ICLR, 2022."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[21] Q. Garrido, R. Balestriero, L. Najman, and Y. LeCun, ‘‘RankMe: Assessing"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "the Downstream Performance of Pretrained Self-Supervised Representations"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "by Their Rank,’’ in Proc. ICML, 2023, p. 10929–10974."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[22]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "T. Yoshioka, X. Xiao, J. Wu, L. Zhou, S. Ren, Y. Qian, Y. Qian, J. Wu,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "M. Zeng, X. Yu, and F. Wei, ‘‘WavLM: Large-Scale Self-Supervised Pre-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Training for Full Stack Speech Processing,’’ IEEE Journal of Selected Topics"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[23] H.-Y. Lee,\nJ.-B. Huang, M. Singh,\nand M.-H. Yang,\n‘‘Unsupervised"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Representation Learning by Sorting Sequences,’’\nin Proc.\nICCV, 2017,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "pp. 667–676."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[24]\nS. Gidaris, P. Singh, and N. Komodakis, ‘‘Unsupervised Representation"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Learning by Predicting Image Rotations,’’ in Proc. ICLR, 2018."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[25] M. Caron, P. Bojanowski, A. Joulin, and M. Douze,\n‘‘Deep Clustering"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "for Unsupervised Learning of Visual Features,’’ in Proc. ECCV, 2018, pp."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "132—-149."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[26]\nJ.-B. Grill, F. Strub, F. Altché, C. Tallec, P. H. Richemond, E. Buchatskaya,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar, B. Piot, K. Kavukcuoglu,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "R. Munos, and M. Valko, ‘‘Bootstrap Your Own Latent a New Approach to"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Self-Supervised Learning,’’ in Proc. NeurIPS, 2020, pp. 21 271–21 284."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[27] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sas-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "try, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, ‘‘Learning"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Transferable Visual Models From Natural Language Supervision,’’ in Proc."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "ICML, 2021, pp. 8748–8763."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[28] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. B. Girshick, ‘‘Masked"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Autoencoders Are Scalable Vision Learners,’’ in Proc. IEEE CVPR, 2022,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "pp. 15 979–15 988."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[29] H. Bao, L. Dong, S. Piao, and F. Wei, ‘‘BEiT: BERT Pre-Training of Image"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Transformers,’’ in Proc. ICLR, 2022."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[30] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "P. Fernandez, D. HAZIZA, F. Massa, A. El-Nouby, M. Assran, N. Bal-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "las, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li,\nI. Misra, M. Rabbat,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "V\n. Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "and P. Bojanowski, ‘‘DINOv2: Learning Robust Visual Features without"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Supervision,’’ Transactions on Machine Learning Research, 2024."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[31]\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training of"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Deep Bidirectional Transformers for Language Understanding,’’ in Proc."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "NAACL-HLT, 2019, pp. 4171–4186."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[37]": "",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": "Supervised Descriptor for Image Copy Detection,’’ in Proc. IEEE CVPR,"
        },
        {
          "[37]": "",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": "2022, pp. 14 512–14 522."
        },
        {
          "[37]": "[38] Y. M. Asano, C. Rupprecht, and A. Vedaldi, ‘‘Self-labelling via simultaneous",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": ""
        },
        {
          "[37]": "",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": "clustering and representation learning,’’ in Proc. ICLR, 2020."
        },
        {
          "[37]": "",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": ""
        },
        {
          "[37]": "[39] W. Wang, Q. Tang,",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": "and K. Livescu,\n‘‘Unsupervised Pre-Training of"
        },
        {
          "[37]": "",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": ""
        },
        {
          "[37]": "",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": "Bidirectional Speech Encoders via Masked Reconstruction,’’ in Proc. IEEE"
        },
        {
          "[37]": "",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": ""
        },
        {
          "[37]": "",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": "ICASSP, 2020, pp. 6889–6893."
        },
        {
          "[37]": "",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": ""
        },
        {
          "[37]": "[40]",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": "Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu,"
        },
        {
          "[37]": "",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": ""
        },
        {
          "[37]": "",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": "‘‘SimMIM: a Simple Framework for Masked Image Modeling,’’ in Proc."
        },
        {
          "[37]": "",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": "IEEE CVPR, 2022, pp. 9643–9653."
        },
        {
          "[37]": "[41] A. Bardes,",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": "J. Ponce,\nand Y. LeCun,\n‘‘VICReg: Variance-Invariance-"
        },
        {
          "[37]": "",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": "Covariance Regularization for Self-Supervised Learning,’’ in Proc. ICLR,"
        },
        {
          "[37]": "",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": "2022."
        },
        {
          "[37]": "[42]",
          "E. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-": "J. H. McDermott and E. P. Simoncelli,\n‘‘Sound Texture Perception via"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[32]\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "‘‘Language Models are Few-Shot Learners,’’ in Proc. NeurIPS, 2020, pp."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "1877–1901."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[33] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "D. Bahri, T. Schuster, S. Zheng, D. Zhou, N. Houlsby, and D. Metzler,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "‘‘UL2: Unifying Language Learning Paradigms,’’ in Proc. ICLR, 2023."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[34] OpenAI, ‘‘GPT-4 Technical Report,’’ arXiv preprint arXiv: 2303.08774,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "2023."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[35]\nZ. Yue, Y. Wang, J. Duan, T. Yang, C. Huang, Y. Tong, and B. Xu, ‘‘TS2Vec:"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Towards Universal Representation of Time Series,’’ in Proc. AAAI, 2022,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "pp. 8980–8987."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[36] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, ‘‘Momentum Contrast for"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Unsupervised Visual Representation Learning,’’ in Proc. IEEE CVPR, 2020,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "pp. 9726–9735."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[37]\nE. Pizzi, S. D. Roy, S. N. Ravindra, P. Goyal, and M. Douze, ‘‘A Self-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Supervised Descriptor for Image Copy Detection,’’ in Proc. IEEE CVPR,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "2022, pp. 14 512–14 522."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[38] Y. M. Asano, C. Rupprecht, and A. Vedaldi, ‘‘Self-labelling via simultaneous"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "clustering and representation learning,’’ in Proc. ICLR, 2020."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "and K. Livescu,\n‘‘Unsupervised Pre-Training of\n[39] W. Wang, Q. Tang,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Bidirectional Speech Encoders via Masked Reconstruction,’’ in Proc. IEEE"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "ICASSP, 2020, pp. 6889–6893."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[40]\nZ. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "‘‘SimMIM: a Simple Framework for Masked Image Modeling,’’ in Proc."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "IEEE CVPR, 2022, pp. 9643–9653."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[41] A. Bardes,\nJ. Ponce,\nand Y. LeCun,\n‘‘VICReg: Variance-Invariance-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Covariance Regularization for Self-Supervised Learning,’’ in Proc. ICLR,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "2022."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[42]\nJ. H. McDermott and E. P. Simoncelli,\n‘‘Sound Texture Perception via"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Statistics of\nthe Auditory Periphery: Evidence from Sound Synthesis,’’"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Neuron, vol. 71, no. 5, pp. 926–940, 2011."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[43]\nL. R. Rabiner and R. W. Schafer, ‘‘Introduction to Digital Speech Process-"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "ing,’’ Foundations and Trends Signal Processing, vol. 1, no. 1-2, pp. 1–194,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "2007."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[44] A. Gulati, C.-C. Chiu, J. Qin, J. Yu, N. Parmar, R. Pang, S. Wang, W. Han,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Y\n. Wu, Y. Zhang, and Z. Zhang,\n‘‘Conformer: Convolution-augmented"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Transformer for Speech Recognition,’’ in Proc. INTERSPEECH, 2020, pp."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "5036–5040."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[45]\nS. Hochreiter and J. Schmidhuber, ‘‘Long Short-Term Memory,’’ Neural"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Computation, vol. 9, no. 8, pp. 1735–1780, 1997."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[46] K. Cho, B. Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Y\n. Bengio, ‘‘Learning Phrase Representations using RNN Encoder-Decoder"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "for Statistical Machine Translation,’’ in Proc. EMNLP, 2014, pp. 1724–"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "1734."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[47] D. Hendrycks and K. Gimpel, ‘‘Gaussian Error Linear Units (GELUs),’’"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "arXiv preprint arXiv: 1606.08415, 2016."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[48]\nJ. L. Ba, J. R. Kiros, and G. E. Hinton,\n‘‘Layer Normalization,’’ arXiv"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "preprint arXiv: 1607.06450, 2016."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[49] D. Ulyanov, A. Vedaldi, and V. Lempitsky, ‘‘Instance Normalization: The"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Missing Ingredient for Fast Stylization,’’ arXiv preprint arXiv: 1607.08022,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "2016."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han,\n‘‘On the\n[50]"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Variance of the Adaptive Learning Rate and Beyond,’’ in Proc. ICLR, 2020."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[51]\nI. Loshchilov and F. Hutter, ‘‘Decoupled Weight Decay Regularization,’’ in"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Proc. ICLR, 2019."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[52] D. P. Kingma and J. Ba, ‘‘Adam: A Method for Stochastic Optimization,’’"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "in Proc. ICLR, 2015."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[53] M. Airaksinen, A. Gallen, A. Kivi, P. Vijayakrishnan, T. Häyrinen, E. Ilen,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "O. Räsänen, L. M. Haataja,\nand S. Vanhatalo,\n‘‘Intelligent wearable"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "allows out-of-the-lab tracking of developing motor abilities in infants,’’"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Communications Medicine, vol. 2, no. 69, 2022."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[54]\nE. Vaaras, M. Airaksinen, S. Vanhatalo, and O. Räsänen, ‘‘Evaluation of"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "self-supervised pre-training for automatic infant movement classification"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "using wearable movement sensors,’’ in Proc. IEEE EMBC, 2023, pp. 1–6."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[55] M. Airaksinen, O. Räsänen, E.\nIlén, T. Häyrinen, A. Kivi, V. Marchi,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "A. Gallen, S. Blom, A. Varhe, N. Kaartinen, L. Haataja, and S. Vanhatalo,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "‘‘Automatic Posture and Movement Tracking of\nInfants with Wearable"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": ""
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "Movement Sensors,’’ Scientific Reports, vol. 10, no. 169, 2020."
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "[56]\nE. Vaaras, S. Ahlqvist-Björkroth, K. Drossos, L. Lehtonen, and O. Räsänen,"
        },
        {
          "E. Vaaras et al.: PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse": "‘‘Development of a speech emotion recognizer for large-scale child-centered"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "APPENDIX A": "PROOF OF NON-COLLAPSED FEATURE REPRESENTATIONS IN PFML PRE-TRAINING"
        },
        {
          "APPENDIX A": "This section provides a more detailed mathematical\nformulation for\nthe proof\nthat PFML pre-training does not converge to"
        },
        {
          "APPENDIX A": "collapsed feature representations."
        },
        {
          "APPENDIX A": "Let x be a single- or multi-channel\ntime-series signal, framed into a sequence of short-term frames\nof N samples\nx0, x1, ..."
        },
        {
          "APPENDIX A": "{\n}"
        },
        {
          "APPENDIX A": "=\n. We define a set of m functionals,\n,\nto be computed for each\neach, where xn =\nxt, xt+1, ..., xt+N\nF0, F1, ..., Fm"
        },
        {
          "APPENDIX A": "−\n−\n{\n1}\nF\n{\n1}"
        },
        {
          "APPENDIX A": "to functionals as mathematical operations which map\nframe xn to produce a set of computed functionals fn. Here, we refer"
        },
        {
          "APPENDIX A": "a time series of arbitrary length into a single value, such as the mean or variance of\nthe signal. Also,\nlet zn be the output"
        },
        {
          "APPENDIX A": "embeddings of an encoder model given the input xn, and let yn denote the output predictions of a Transformer-based model"
        },
        {
          "APPENDIX A": "let us define the following functions:\ngiven the input zn. To formalize the relationships between inputs and outputs,"
        },
        {
          "APPENDIX A": "Let\nbe the set of\nfunctionals\nthat maps\nthe input\ni.e.,\nframes xn to the computed functionals\nfn,\nfn =\n(xn) ="
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "−\n−\n{\n1}\nF\n{\n1}": "to functionals as mathematical operations which map\nframe xn to produce a set of computed functionals fn. Here, we refer"
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "a time series of arbitrary length into a single value, such as the mean or variance of\nthe signal. Also,\nlet zn be the output"
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "embeddings of an encoder model given the input xn, and let yn denote the output predictions of a Transformer-based model"
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "let us define the following functions:\ngiven the input zn. To formalize the relationships between inputs and outputs,"
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "Let\nbe the set of\nfunctionals\nthat maps\nthe input\ni.e.,\nframes xn to the computed functionals\nfn,\nfn =\n(xn) ="
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "•\nF\nF"
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": ".\nF0(xn), F1(xn), ..., Fm\n1(xn)"
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "−\n{\n}"
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "Let g be the function that maps the embeddings zn to the predictions yn,\ni.e., yn = g(zn)."
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "•"
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "Let us assume the following in PFML pre-training:"
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "Assumption 1: There is temporal variability across the frames xn. Formally,\nlet σ2(xn) denote the variance of xn across"
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "•"
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "the frames, and σ2(xn) > 0."
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "Assumption 2: Given Assumption 1,\nthe set of non-trivial\nfunctionals\ncomputed from xn also contains variance across"
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "•\nF"
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "the frames. Formally,\nlet σ2(fn) denote the variance of fn across the frames, and σ2(fn) > 0."
        },
        {
          "−\n−\n{\n1}\nF\n{\n1}": "Under\nthese assumptions, we aim to show that\nIn\nthe predictions yn also contain variance across the frames,\ni.e., σ2(yn) > 0."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": "MAE"
        },
        {
          "TABLE 5": "100"
        },
        {
          "TABLE 5": "1e-4"
        },
        {
          "TABLE 5": "40"
        },
        {
          "TABLE 5": "0.5"
        },
        {
          "TABLE 5": "AdamW"
        },
        {
          "TABLE 5": "64"
        },
        {
          "TABLE 5": "MSE"
        },
        {
          "TABLE 5": "0.1"
        },
        {
          "TABLE 5": "0.2"
        },
        {
          "TABLE 5": "GELU"
        },
        {
          "TABLE 5": "160"
        },
        {
          "TABLE 5": "6"
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": "640"
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": "10"
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": "13"
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": "6"
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": "1"
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": "0.15"
        },
        {
          "TABLE 5": "3"
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": "N/A"
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": "N/A"
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": ""
        },
        {
          "TABLE 5": "N/A"
        },
        {
          "TABLE 5": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE 6": ""
        },
        {
          "TABLE 6": "DATA)."
        },
        {
          "TABLE 6": "Multi-sensor"
        },
        {
          "TABLE 6": "IMU data"
        },
        {
          "TABLE 6": "100"
        },
        {
          "TABLE 6": "4e-5"
        },
        {
          "TABLE 6": "30"
        },
        {
          "TABLE 6": "0.5"
        },
        {
          "TABLE 6": "Adam"
        },
        {
          "TABLE 6": "1"
        },
        {
          "TABLE 6": "Weighted"
        },
        {
          "TABLE 6": ""
        },
        {
          "TABLE 6": "cross-entropy"
        },
        {
          "TABLE 6": "0.3"
        },
        {
          "TABLE 6": "0.4"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 7: presents the",
      "data": [
        {
          "TABLE 7": ""
        },
        {
          "TABLE 7": "Multi-sensor IMU data"
        },
        {
          "TABLE 7": "Movement \nPosture"
        },
        {
          "TABLE 7": "81.4 \n95.6"
        },
        {
          "TABLE 7": "81.8 \n95.7"
        },
        {
          "TABLE 7": "UAF1 (%)"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 7: presents the",
      "data": [
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        },
        {
          "TABLE 9": ""
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        },
        {
          "TABLE 10": ""
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE 11": "THE FINE-TUNING RESULTS FOR DISCARDING SOME OF THE FUNCTIONALS IN PFML PRE-TRAINING FOR IMU DATA."
        },
        {
          "TABLE 11": "Functionals left out"
        },
        {
          "TABLE 11": "‒"
        },
        {
          "TABLE 11": "min, max"
        },
        {
          "TABLE 11": "min, max, ACF skewness, ACF kurtosis"
        },
        {
          "TABLE 11": "min, max, ACF variance, ACF skewness, ACF kurtosis, ZCR"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE 12": "THE FINE-TUNING RESULTS FOR DIFFERENT MASK TYPES FOR PFML PRE-TRAINING FOR IMU DATA."
        },
        {
          "TABLE 12": ""
        },
        {
          "TABLE 12": "Mask type"
        },
        {
          "TABLE 12": ""
        },
        {
          "TABLE 12": "zeros"
        },
        {
          "TABLE 12": "ones"
        },
        {
          "TABLE 12": "Gaussian noise"
        },
        {
          "TABLE 12": "learnable mask"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 4: Table 13 presents the numerical values behind the",
      "data": [
        {
          "TABLE 13": "A LISTING OF PROS AND CONS FOR EACH OF THE SSL METHODS USED IN THE PRESENT EXPERIMENTS. UP ARROW (\n) INDICATES THAT A HIGHER SCORE"
        },
        {
          "TABLE 13": "↑"
        },
        {
          "TABLE 13": "IS BETTER, WHEREAS A DOWN ARROW (\n) INDICATES THAT A LOWER SCORE IS BETTER."
        },
        {
          "TABLE 13": "↓"
        },
        {
          "TABLE 13": "Robust to \nFlexibility of \nLinear \nNumber of"
        },
        {
          "TABLE 13": "End-use \nExpected pre-"
        },
        {
          "TABLE 13": "separability of \nhyperparameters \n \nrepresentation \nneural network"
        },
        {
          "TABLE 13": "performance (↑) \ntraining time (↓)"
        },
        {
          "TABLE 13": "collapse \narchitecture \nSSL features (↑) \n(↓)"
        },
        {
          "TABLE 13": "TS2Vec \n73.04 \n78.95 \n59.18 \n3 \n+ \n−"
        },
        {
          "TABLE 13": "MAE \n77.02 \n25.23 \n60.56 \n2 \n+ \n+"
        },
        {
          "TABLE 13": "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+"
        },
        {
          "TABLE 13": "PFML (ours) \n77.60 \n23.47 \n61.66 \n3 \n+ \n+"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 4: Table 13 presents the numerical values behind the",
      "data": [
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "PFML (ours) \n77.60 \n23.47 \n61.66 \n3 \n+ \n+"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "The values for\nthe columns in Table 13 have been defined as follows:"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "End-use performance: These results are the average of\nthe UAF1 (%) and UAR (%) values in Table 1 across all five"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "•"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "classification tasks."
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "Robust\nto representation collapse: This is a “Yes/No” decision based on Table 3."
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "•"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "Expected pre-training time: The expected pre-training time tpt(Pm) for pre-training method Pm is computed as"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "•"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "avg\ntraining\ntime ,\n(5)\ntpt(Pm) = (avg num collapses + 1)"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "·"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "where avg num collapses is the average number of\nrepresentation collapses across all\nthree data modalities (Table 3)"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "training\nand avg\ntime is the average pre-training time (in hours) across all\nthree data modalities. Note that\nfor MAE,"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "we considered the average number of\nrepresentation collapses to be zero, since there was only a single representation"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "collapse which was most probably due to bad luck in model weight\ninitialization. Also note that model\ntraining time can"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "vary significantly depending on the amount and type of\ntraining data,\nthe software implementation, and the computational"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "hardware used. However, when taking the expected number of\nrepresentation collapses during pre-training into account,"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "the SSL methods used in the present experiments can be categorized into three distinct groups."
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "Flexibility of neural network architecture: This is a “Yes/No” decision based on whether\nthe SSL algorithm has a free"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "•"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "choice of\nthe encoder and classifier architecture.\nIn TS2Vec,\nthe pre-training algorithm is built\ninto the pre-defined CNN"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "encoder architecture, and therefore it\nis not possible select\nthe architecture of\nthe encoder\nfreely."
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "Linear separability of SSL features: These results are the average of\nthe UAF1 (%) and UAR (%) values in Table 2 across"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "•"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "all five classification tasks."
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "Number\nof\nhyperparameters: These\nvalues\nare\na\nsimple\ncount\nof\nthe\nnumber\nof\ndifferent SSL algorithm-specific"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "•"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "hyperparameters that affect\nthe performance of\nthe SSL algorithm. These algorithm-specific hyperparameters for each SSL"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "algorithm are:"
        },
        {
          "data2vec \n77.34 \n305.09 \n60.76 \n7 \n− \n+": "– TS2Vec: 1) Minibatch size, 2) Number of negative samples in minibatch, 3) Data augmentation method"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 14: shows the pre-training durations for each data modality (multi-sensor IMU data, speech data, and EEG data). Note that due to",
      "data": [
        {
          "into RAM would speed up the pre-training process substantially.": "TABLE 14"
        },
        {
          "into RAM would speed up the pre-training process substantially.": ""
        },
        {
          "into RAM would speed up the pre-training process substantially.": ""
        },
        {
          "into RAM would speed up the pre-training process substantially.": "Data modality"
        },
        {
          "into RAM would speed up the pre-training process substantially.": ""
        },
        {
          "into RAM would speed up the pre-training process substantially.": "Multi-sensor IMU data"
        },
        {
          "into RAM would speed up the pre-training process substantially.": "Speech data"
        },
        {
          "into RAM would speed up the pre-training process substantially.": "EEG data"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table 15: shows a summary of all of the results",
      "data": [
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": ""
        },
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": ""
        },
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": ""
        },
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": ""
        },
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": ""
        },
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": ""
        },
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": ""
        },
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": ""
        },
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": ""
        },
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": "No pre-training"
        },
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": "TS2Vec pre-trained"
        },
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": "No pre-training, linear classifier"
        },
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": "TS2Vec pre-trained, linear classifier"
        },
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": "TS2Vec pre-trained, SVM classifier"
        },
        {
          "RESULTS ARE WITHOUT THE TRANSFORMER: FIRST, LINEAR EVALUATION RESULTS FOR THE TS2VEC ENCODER WITHOUT PRE-TRAINING, FOLLOWED BY": ""
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A Cookbook of Self-Supervised Learning",
      "authors": [
        "R Balestriero",
        "M Ibrahim",
        "V Sobal",
        "A Morcos",
        "S Shekhar",
        "T Goldstein",
        "F Bordes",
        "A Bardes",
        "G Mialon",
        "Y Tian",
        "A Schwarzschild",
        "A Wilson",
        "J Geiping",
        "Q Garrido",
        "P Fernandez",
        "A Bar",
        "H Pirsiavash",
        "Y Lecun",
        "M Goldblum"
      ],
      "year": "2023",
      "venue": "A Cookbook of Self-Supervised Learning",
      "arxiv": "arXiv:2304.12210"
    },
    {
      "citation_id": "2",
      "title": "A Survey on Self-Supervised Learning: Algorithms, Applications, and Future Trends",
      "authors": [
        "J Gui",
        "T Chen",
        "J Zhang",
        "Q Cao",
        "Z Sun",
        "H Luo",
        "D Tao"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Why Does Unsupervised Pre-training Help Deep Learning?",
      "authors": [
        "D Erhan",
        "Y Bengio",
        "A Courville",
        "P.-A Manzagol",
        "P Vincent",
        "S Bengio"
      ],
      "year": "2010",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "4",
      "title": "Representation Learning with Contrastive Predictive Coding",
      "authors": [
        "A Van Den Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation Learning with Contrastive Predictive Coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "5",
      "title": "'wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "6",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "7",
      "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
      "authors": [
        "H Akbari",
        "L Yuan",
        "R Qian",
        "W.-H Chuang",
        "S.-F Chang",
        "Y Cui",
        "B Gong"
      ],
      "venue": "Proc. NeurIPS, 2021"
    },
    {
      "citation_id": "8",
      "title": "'data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
      "authors": [
        "A Baevski",
        "W Hsu",
        "Q Xu",
        "A Babu",
        "J Gu",
        "M Auli"
      ],
      "year": "2022",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "9",
      "title": "Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks",
      "authors": [
        "W Wang",
        "H Bao",
        "L Dong",
        "J Bjorck",
        "Z Peng",
        "Q Liu",
        "K Aggarwal",
        "O Mohammed",
        "S Singhal",
        "S Som",
        "F Wei"
      ],
      "year": "2023",
      "venue": "Proc. IEEE CVPR"
    },
    {
      "citation_id": "10",
      "title": "Data-efficient image recognition with contrastive predictive coding",
      "authors": [
        "O Hénaff",
        "A Srinivas",
        "J De Fauw",
        "A Razavi",
        "C Doersch",
        "S Eslami",
        "A Van Den",
        "Oord"
      ],
      "year": "2020",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "11",
      "title": "Efficient self-supervised learning with contextualized target representations for vision, speech and language",
      "authors": [
        "A Baevski",
        "A Babu",
        "W.-N Hsu",
        "M Auli"
      ],
      "year": "2023",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "12",
      "title": "MCR-Data2vec 2.0: Improving Self-supervised Speech Pre-training via Model-level Consistency Regularization",
      "authors": [
        "J Yoon",
        "S Kim",
        "N Kim"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "13",
      "title": "Robust Data2VEC: Noise-Robust Speech Representation Learning for ASR by Combining Regression and Improved Contrastive Learning",
      "authors": [
        "Q.-S Zhu",
        "L Zhou",
        "J Zhang",
        "S.-J Liu",
        "Y.-C Hu",
        "L.-R Dai"
      ],
      "year": "2023",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "14",
      "title": "Av-Data2Vec: Self-Supervised Learning of Audio-Visual Speech Representations with Contextualized Target Representations",
      "authors": [
        "J Lian",
        "A Baevski",
        "W.-N Hsu",
        "M Auli"
      ],
      "year": "2023",
      "venue": "IEEE ASRU"
    },
    {
      "citation_id": "15",
      "title": "Hard negative mixing for contrastive learning",
      "authors": [
        "Y Kalantidis",
        "M Sariyildiz",
        "N Pion",
        "P Weinzaepfel",
        "D Larlus"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "16",
      "title": "Contrastive Learning with Hard Negative Samples",
      "authors": [
        "J Robinson",
        "C.-Y Chuang",
        "S Sra",
        "S Jegelka"
      ],
      "year": "2021",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "17",
      "title": "Unsupervised learning of visual features by contrasting cluster assignments",
      "authors": [
        "M Caron",
        "I Misra",
        "J Mairal",
        "P Goyal",
        "P Bojanowski",
        "A Joulin"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "18",
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc"
    },
    {
      "citation_id": "19",
      "title": "On Feature Decorrelation in Self-Supervised Learning",
      "authors": [
        "T Hua",
        "W Wang",
        "Z Xue",
        "S Ren",
        "Y Wang",
        "H Zhao"
      ],
      "year": "2021",
      "venue": "Proc. IEEE ICCV"
    },
    {
      "citation_id": "20",
      "title": "Understanding Dimensional Collapse in Contrastive Self-supervised Learning",
      "authors": [
        "L Jing",
        "P Vincent",
        "Y Lecun",
        "Y Tian"
      ],
      "year": "2022",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "21",
      "title": "RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank",
      "authors": [
        "Q Garrido",
        "R Balestriero",
        "L Najman",
        "Y Lecun"
      ],
      "year": "2023",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "22",
      "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao",
        "J Wu",
        "L Zhou",
        "S Ren",
        "Y Qian",
        "Y Qian",
        "J Wu",
        "M Zeng",
        "X Yu",
        "F Wei"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Unsupervised Representation Learning by Sorting Sequences",
      "authors": [
        "H.-Y Lee",
        "J.-B Huang",
        "M Singh",
        "M.-H Yang"
      ],
      "year": "2017",
      "venue": "Proc. ICCV"
    },
    {
      "citation_id": "24",
      "title": "Unsupervised Representation Learning by Predicting Image Rotations",
      "authors": [
        "S Gidaris",
        "P Singh",
        "N Komodakis"
      ],
      "year": "2018",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "25",
      "title": "Deep Clustering for Unsupervised Learning of Visual Features",
      "authors": [
        "M Caron",
        "P Bojanowski",
        "A Joulin",
        "M Douze"
      ],
      "year": "2018",
      "venue": "Proc. ECCV"
    },
    {
      "citation_id": "26",
      "title": "Bootstrap Your Own Latent a New Approach to Self-Supervised Learning,'' in Proc. NeurIPS",
      "authors": [
        "J.-B Grill",
        "F Strub",
        "F Altché",
        "C Tallec",
        "P Richemond",
        "E Buchatskaya",
        "C Doersch",
        "B Pires",
        "Z Guo",
        "M Azar",
        "B Piot",
        "K Kavukcuoglu",
        "R Munos",
        "M Valko"
      ],
      "year": "2020",
      "venue": "Bootstrap Your Own Latent a New Approach to Self-Supervised Learning,'' in Proc. NeurIPS"
    },
    {
      "citation_id": "27",
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "28",
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2022",
      "venue": "Proc. IEEE CVPR"
    },
    {
      "citation_id": "29",
      "title": "BEiT: BERT Pre-Training of Image Transformers",
      "authors": [
        "H Bao",
        "L Dong",
        "S Piao",
        "F Wei"
      ],
      "year": "2022",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "30",
      "title": "Learning Robust Visual Features without Supervision",
      "authors": [
        "M Oquab",
        "T Darcet",
        "T Moutakanni",
        "H Vo",
        "M Szafraniec",
        "V Khalidov",
        "P Fernandez",
        "D Haziza",
        "F Massa",
        "A El-Nouby",
        "M Assran",
        "N Ballas",
        "W Galuba",
        "R Howes",
        "P.-Y Huang",
        "S.-W Li",
        "I Misra",
        "M Rabbat",
        "V Sharma",
        "G Synnaeve",
        "H Xu",
        "H Jegou",
        "J Mairal",
        "P Labatut",
        "A Joulin",
        "P Bojanowski"
      ],
      "year": "2024",
      "venue": "Transactions on Machine Learning Research"
    },
    {
      "citation_id": "31",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. NAACL-HLT"
    },
    {
      "citation_id": "32",
      "title": "Language Models are Few-Shot Learners,'' in Proc. NeurIPS",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Language Models are Few-Shot Learners,'' in Proc. NeurIPS"
    },
    {
      "citation_id": "33",
      "title": "Unifying Language Learning Paradigms,'' in Proc. ICLR",
      "authors": [
        "Y Tay",
        "M Dehghani",
        "V Tran",
        "X Garcia",
        "J Wei",
        "X Wang",
        "H Chung",
        "D Bahri",
        "T Schuster",
        "S Zheng",
        "D Zhou",
        "N Houlsby",
        "D Metzler"
      ],
      "year": "2023",
      "venue": "Unifying Language Learning Paradigms,'' in Proc. ICLR"
    },
    {
      "citation_id": "34",
      "title": "",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "35",
      "title": "'TS2Vec: Towards Universal Representation of Time Series",
      "authors": [
        "Z Yue",
        "Y Wang",
        "J Duan",
        "T Yang",
        "C Huang",
        "Y Tong",
        "B Xu"
      ],
      "year": "2022",
      "venue": "Proc. AAAI"
    },
    {
      "citation_id": "36",
      "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
      "authors": [
        "K He",
        "H Fan",
        "Y Wu",
        "S Xie",
        "R Girshick"
      ],
      "year": "2020",
      "venue": "Proc. IEEE CVPR"
    },
    {
      "citation_id": "37",
      "title": "A Self-Supervised Descriptor for Image Copy Detection",
      "authors": [
        "E Pizzi",
        "S Roy",
        "S Ravindra",
        "P Goyal",
        "M Douze"
      ],
      "year": "2022",
      "venue": "Proc. IEEE CVPR"
    },
    {
      "citation_id": "38",
      "title": "Self-labelling via simultaneous clustering and representation learning",
      "authors": [
        "Y Asano",
        "C Rupprecht",
        "A Vedaldi"
      ],
      "year": "2020",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "39",
      "title": "Unsupervised Pre-Training of Bidirectional Speech Encoders via Masked Reconstruction",
      "authors": [
        "W Wang",
        "Q Tang",
        "K Livescu"
      ],
      "year": "2020",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "40",
      "title": "SimMIM: a Simple Framework for Masked Image Modeling",
      "authors": [
        "Z Xie",
        "Z Zhang",
        "Y Cao",
        "Y Lin",
        "J Bao",
        "Z Yao",
        "Q Dai",
        "H Hu"
      ],
      "year": "2022",
      "venue": "Proc. IEEE CVPR"
    },
    {
      "citation_id": "41",
      "title": "VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning",
      "authors": [
        "A Bardes",
        "J Ponce",
        "Y Lecun"
      ],
      "year": "2022",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "42",
      "title": "Sound Texture Perception via Statistics of the Auditory Periphery: Evidence from Sound Synthesis",
      "authors": [
        "J Mcdermott",
        "E Simoncelli"
      ],
      "year": "2011",
      "venue": "Neuron"
    },
    {
      "citation_id": "43",
      "title": "Introduction to Digital Speech Processing",
      "authors": [
        "L Rabiner",
        "R Schafer"
      ],
      "year": "2007",
      "venue": "Foundations and Trends Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
      "authors": [
        "A Gulati",
        "C.-C Chiu",
        "J Qin",
        "J Yu",
        "N Parmar",
        "R Pang",
        "S Wang",
        "W Han",
        "Y Wu",
        "Y Zhang",
        "Z Zhang"
      ],
      "year": "2020",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "45",
      "title": "Long Short-Term Memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "46",
      "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "authors": [
        "K Cho",
        "B Merrienboer",
        "C Gulcehre",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Proc. EMNLP"
    },
    {
      "citation_id": "47",
      "title": "Gaussian Error Linear Units (GELUs)",
      "authors": [
        "D Hendrycks",
        "K Gimpel"
      ],
      "year": "2016",
      "venue": "Gaussian Error Linear Units (GELUs)",
      "arxiv": "arXiv:1606.08415"
    },
    {
      "citation_id": "48",
      "title": "Layer Normalization",
      "authors": [
        "J Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2016",
      "venue": "Layer Normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "49",
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "authors": [
        "D Ulyanov",
        "A Vedaldi",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "arxiv": "arXiv:1607.08022"
    },
    {
      "citation_id": "50",
      "title": "On the Variance of the Adaptive Learning Rate and Beyond",
      "authors": [
        "L Liu",
        "H Jiang",
        "P He",
        "W Chen",
        "X Liu",
        "J Gao",
        "J Han"
      ],
      "year": "2020",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "51",
      "title": "Decoupled Weight Decay Regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "52",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "53",
      "title": "Intelligent wearable allows out-of-the-lab tracking of developing motor abilities in infants",
      "authors": [
        "M Airaksinen",
        "A Gallen",
        "A Kivi",
        "P Vijayakrishnan",
        "T Häyrinen",
        "E Ilen",
        "O Räsänen",
        "L Haataja",
        "S Vanhatalo"
      ],
      "year": "2022",
      "venue": "Communications Medicine"
    },
    {
      "citation_id": "54",
      "title": "Evaluation of self-supervised pre-training for automatic infant movement classification using wearable movement sensors",
      "authors": [
        "E Vaaras",
        "M Airaksinen",
        "S Vanhatalo",
        "O Räsänen"
      ],
      "year": "2023",
      "venue": "Proc. IEEE EMBC"
    },
    {
      "citation_id": "55",
      "title": "Automatic Posture and Movement Tracking of Infants with Wearable Movement Sensors",
      "authors": [
        "M Airaksinen",
        "O Räsänen",
        "E Ilén",
        "T Häyrinen",
        "A Kivi",
        "V Marchi",
        "A Gallen",
        "S Blom",
        "A Varhe",
        "N Kaartinen",
        "L Haataja",
        "S Vanhatalo"
      ],
      "year": "2020",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "56",
      "title": "Development of a speech emotion recognizer for large-scale child-centered audio recordings from a hospital environment",
      "authors": [
        "E Vaaras",
        "S Ahlqvist-Björkroth",
        "K Drossos",
        "L Lehtonen",
        "O Räsänen"
      ],
      "year": "2023",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "57",
      "title": "Analysis of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the EEG",
      "authors": [
        "B Kemp",
        "A Zwinderman",
        "B Tuk",
        "H Kamphuisen",
        "J Oberye"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "58",
      "title": "PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals",
      "authors": [
        "A Goldberger",
        "L Amaral",
        "L Glass",
        "J Hausdorff",
        "P Ivanov",
        "R Mark",
        "J Mietus",
        "G Moody",
        "C.-K Peng",
        "H Stanley"
      ],
      "year": "2000",
      "venue": "PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals"
    },
    {
      "citation_id": "59",
      "title": "An Attention-Based Deep Learning Approach for Sleep Stage Classification With Single-Channel EEG",
      "authors": [
        "E Eldele",
        "Z Chen",
        "C Liu",
        "M Wu",
        "C.-K Kwoh",
        "X Li",
        "C Guan"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "60",
      "title": "Clinical Applications of Machine Learning Algorithms: Beyond the Black Box",
      "authors": [
        "D Watson",
        "J Krutzinna",
        "I Bruce",
        "C Griffiths",
        "I Mcinnes",
        "M Barnes",
        "L Floridi"
      ],
      "year": "2019",
      "venue": "BMJ"
    }
  ]
}