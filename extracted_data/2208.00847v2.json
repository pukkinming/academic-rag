{
  "paper_id": "2208.00847v2",
  "title": "Mafw: A Large-Scale, Multi-Modal, Compound Affective Database For Dynamic Facial Expression Recognition In The Wild",
  "published": "2022-08-01T13:34:33Z",
  "authors": [
    "Yuanyuan Liu",
    "Wei Dai",
    "Chuanxu Feng",
    "Wenbin Wang",
    "Guanghao Yin",
    "Jiabei Zeng",
    "Shiguang Shan"
  ],
  "keywords": [
    "Dynamic compound affective dababase",
    "single and multiple expressions",
    "multi-modal",
    "Transformer",
    "in the wild"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Dynamic facial expression recognition (FER) databases provide important data support for affective computing and applications. However, most FER databases are annotated with several basic mutually exclusive emotional categories and contain only one modality, e.g., videos. The monotonous labels and modality cannot accurately imitate human emotions and fulfill applications in the real world. In this paper, we propose MAFW, a large-scale multi-modal compound affective database with 10,045 video-audio clips in the wild. Each clip is annotated with a compound emotional category and a couple of sentences that describe the subjects' affective behaviors in the clip. For the compound emotion annotation, each clip is categorized into one or more of the 11 widely-used emotions, i.e., anger, disgust, fear, happiness, neutral, sadness, surprise, contempt, anxiety, helplessness, and disappointment. To ensure high quality of the labels, we filter out the unreliable annotations by an Expectation Maximization (EM) algorithm, and then obtain 11 single-label emotion categories and 32 multi-label emotion categories. To the best of our knowledge, MAFW is the first in-the-wild multi-modal database annotated with compound emotion annotations and emotionrelated captions. Additionally, we also propose a novel Transformerbased expression snippet feature learning method to recognize the",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent years, facial expression recognition (FER) has become a hot research topic in the fields of human-computer interaction (HCI) systems, multimedia analysis and processing, intelligent robots, and so on  [4, 11, 14, 33] . Despite the progress, most the existing methods and databases are developed based on six basic emotions (i.e., happiness, sadness, fear, surprise, disgust, and anger) proposed by P. Ekman  [13]  and contain only a single modality, e.g., videos. Since the monotonous labels and modality are significantly different from the real-world human emotions in the wild, FER techniques are Table  1 : Summary of existing dynamic facial expression databases.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Database #Sample Source Expression Annotation",
      "text": "Is in-the-wild? #Annotation Modality Times CK+  [28]  327 Lab 6 expressions+neutral and contempt No -Video MMI  [31]  2900 Lab 6 expressions+neutral No -Video BP4D  [39]  328 Lab 6 expressions+embarrassment and pain No -Video&Audio Aff-Wild2  [8]  84\n\nWeb & YouTube 6 expressions+neutral Yes 3 Video&Audio AFEW 7.0  [7]  1,809 54 movies 6 expressions+neutral Yes 2 Video&Audio CAER  [22]  13,201 79 TV dramas 6 expressions+neutral Yes 3 Video&Audio EmoVoxCeleb  [1]  22,496 Interview videos from YouTube 6 expressions+neutral and contempt Yes Auto Video&Audio DFEW  [20]  16 Video 20,000 short videos from reality shows, talk shows, news, etc 32 multiple expressions Audio 2,045 clips from  [7] ,  [20] , and  [1]  emotional descriptive text Text still far from the real-world applications  [15, 24] . In order to enhance the real-world use of FER technology, it is essential to construct a sizable, in-the-wild dynamic affective database encompassing compound emotions and modalities.\n\nExisting dynamic databases are classified into two categories based on the method of collection: laboratory-collected constrained databases and in-the-wild databases  [24] . Table  1  reports existing dynamic FER databases and their information. Through event induction, the constrained databases, including CK+  [28] , MMI  [31] , etc., record films of facial expression changes in the lab. These databases with single, limited, and consistent expression changes have seen substantial breakthroughs in FER technology, but they fall short in simulating the complex real-world human emotions. The in-the-wild databases, such as AFEW 7.0  [7]  and DFEW  [20] , are constructed by crawling videos from movies and TV dramas. These databases closely reflect actual life, including a variety of contextual factors and spontaneous expressions. However, they still have the following limitations:\n\nâ€¢ The labels of the data are monotonous. As shown in Table  1 , most existing databases are composed of seven or eight basic mutually exclusive emotional categories, e.g., six basic expressions plus neutral or contempt. Many studies  [10, 12, 32, 35, 40]  have shown that people usually express multiple emotions simultaneously in real life, along with gestures and vocal changes. â€¢ Video sources are relatively homogeneous and repetitive. As shown in Table  1 , videos in CAER  [22]  and DFEW  [20]  are from 79 TV dramas and 1,500 movies, respectively, while EmoVoxCeleb  [1]  is collected from interview programs. â€¢ The modality of the data is relatively monotonous. As shown in Table  1 , most existing FER databases contain only video and audio modalities, and very few contain text modalities.\n\nTo overcome the above problems, we construct a large-scale compound affective database called MAFW with multiple modalities in the wild, which contains 10,045 video-audio clips. MAFW can be used as a new benchmark for researchers to develop and evaluate their methods for several FER tasks, such as multi-modal emotion recognition, cross-domain FER, emotion captioning, selfsupervision FER, etc. Fig.  1  gives typical examples and the corresponding annotations in our MAFW database. Our MAFW has the following three advantages over the existing databases:\n\nâ€¢ Our MAFW is the first large-scale, multi-modal, multi-label affective database with 11 single expression categories, 32 multiple expression categories, and emotional descriptive texts. To obtain reliable and objective annotation, each clip in MAFW is independently labeled enough often as one or more of the 11 expression categories. Unreliable labels are then removed by an Expectation Maximization (EM) based reliability evaluation algorithm. â€¢ Unlike most existing multi-modal FER databases that are only labeled with expression category tags, we also provide bilingual descriptive texts on facial expressions and emotions for videos in English and Chinese. The descriptive texts include the information on the environment, body movements, facial unit action, and other emotional elements that can be used for both video emotion captioning and FER. â€¢ Compared to existing databases whose sources are mostly movies and TV shows, MAFW also includes short videos of reality TV, talk shows, news, variety shows, etc.\n\nIn addition to MAFW, we also propose a novel Transformerbased expression snippet feature learning method (T-ESFL) to effectively model subtle intra-snippet and inter-snippet expression movements for discovering movement-sensitive emotion representation, thus obtaining robust uni-and multi-modal FER. Furthermore, we establish four benchmark evaluation protocols for MAFW. Extensive experiments show the advantages of T-ESFL over other state-of-the-art deep learning methods, for both uni-and multimodal FER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Constrained dynamic FER databases The constrained databases are usually captured from a small group of individuals in a fixed indoor setting, with emotion frequently occurring during video viewing and event elicitation. For example, CK+  [28]  collected six basic expressions from 123 individuals under laboratory conditions. BP4D  [39]  collected eight expressions from 41 individuals in eight different scenarios, including one-to-one interviews (evoking pleasure), suddenly hearing a voice (evoking surprise), and so on. Despite being spontaneous, the constrained expression databases are limited by a single environment, simple settings, the number of individuals, and the cost of production, making it difficult to simulate the real-world human emotions.\n\nIn-the-wild dynamic FER databases Dynamic FER databases in the wild are usually collected from online sources like TV episodes, movies, and other media. AFEW 7.0  [7]  and DFEW  [20]  collect 1,800 and 16,372 facial expression video clips from movies, respectively.  13,201 facial expression video clips from TV dramas are included in CAER  [22] . Although these databases are created using real-world films, they all have the same restrictions, such as just offering basic and single expression labels and using movie or television clips as their sources.\n\nCompound FER databases Recent studies in psychology and cognition have revealed that people frequently express compound emotions at once  [12, 35] . This suggests that the existing FER databases with single, basic expression labels are not conducive to understand human emotions. In CVPR2017, Deng et al.  [25]  presented the first static compound FER database, namely RAF-DB, that contains 7-class single expressions and 12-class multiple expressions. In ACL2018, Zadeh et al.  [2]  presented a dynamic database, CMU-MOSEI, supporting multiple labels consisting of six basic expressions. Compared to these compound FER databases, our MAFW has more basic emotion categories, reliable multi-label emotion categories, and richer modalities.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Mafw Database 3.1 Data Collection",
      "text": "The pipeline of data collection in MAFW is shown in Fig.  2 . The MAFW has two main data sources. The first data sources are movies, TV dramas, and short videos from some reality shows, talk shows, news, variety shows, etc., on BiliBili and Youtube websites. We develop a crawler program to crawl over 1,600 HD movies, TV dramas, and over 20,000 short videos. These videos come from China, Japan, Korea, Europe, America, and India and cover various themes, e.g., variety, family, science fiction, suspense, love, comedy, and interviews, encompassing a wide range of human emotions. To ensure the diversity of the data, we only randomly download one episode of the same TV series, as well as select no more than three facial expression clips in an episode or short video. The second data source, inspired by  [38] , uses videos from already-existing public databases to supplement some unusual categories, including 1,097 videos from DFEW  [20] , 98 videos from AFEW 7.0  [7] , and 850 videos from EmoVoxCeleb  [1] .\n\nWith the crawled audio-video clips, we first use FaceDetector  [18, 23]  to detect the clips containing faces, then manually remove the unqualified clips to obtain 10,045 usable clips.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Annotation",
      "text": "Unlike other databases that only give the basic and single emotion annotation, our database offers three different types of emotion annotations for video clips: (1) single expression label, i.e., each clip is assigned to a predominant and exclusive expression label, namely anger(AN), disgust(DI), fear(FE), happiness(HA), sadness(SA), surprise(SU), contempt(CO), anxiety(AX), helplessness(HL), disappointment(DS), and neutral(NE) (see Fig.  1(a) ); (2) multiple expression label, i.e., a clip can be annotated as a multi-label multiple expression category when it is determined to contain multiple emotions (such as \"Anger+Disgust\" in Fig.  1(b )); (3) emotional descriptive text, i.e., each clip is bilingually annotated with a couple of sentences describing the subjects' affective behaviors in the clip. The following details the annotator selection, compound emotion category annotation, and descriptive text annotation, respectively.\n\nAnnotator selection Our annotators are college students from different degrees, majors, countries, and genders. To help annotate the emotion category and emotional descriptive text of each video, each annotator is initially trained to recognize expressions using the expression training tool mett 1 proposed by Paul Ekman to gain knowledge of facial action units and emotions. Following the instruction, the experts evaluate each annotator. Finally, for the annotation, 11 skilled annotators are used, each of whom had a test accuracy of at least 90%. Compound emotion category annotation To facilitate effective annotation, we create the ExpreLabelTool labeling tool. Each clip is categorized into one or more of the 11 complex emotions using the tool and is labeled by the 11 annotators. On a scale of 0 to 1, the annotators evaluate the self-confidence scores of their annotations (including 11 levels in ExpreLabelTool). The more certain the annotation is, the higher the score. After that, each clip can be obtained as an 11-dimensional vector, where each dimension represents the score of the labeled emotion. We describe later how to select single and multiple expressions based on this vector.\n\nDescriptive text annotation For each video, except for the neutral emotion, the annotators are required to watch the video carefully and write down the bilingual emotional description according to the pre-established rules. Fig.  1  shows examples of the descriptive texts for emotion captioning in MAFW. The atmosphere, body movements, facial action units, and other emotional details are included in the captions. To ensure the complementarity of the emotional descriptive text, the descriptive text cannot directly use terms with emotional labels, such as \"she is angry\".",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Metadata",
      "text": "The MAFW is a multi-modal database with text, audio, and video modalities. Each clip data is provided with a single or multiple expression label, an average confidence score for each emotion annotation, and several descriptive sentences (texts) for emotion captioning. We additionally offer three automatic annotations: the frame-level 68 facial landmarks, face regions, and gender. The gender of each person is identified by a CNN model that has been pre-trained on CelebA  [26] , and the facial landmarks and regions are detected by  [3] . After identification and counting, 58.1% of the MAFW database is male and 41.9% is female.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Annotation Reliability Estimation",
      "text": "Due to the subjectivity difference of annotators, annotation reliability may be highly variable and inconsistent. To get rid of the labels with lower reliability, motivated by  [34]  and  [5] , we employ an Expectation Maximization (EM) algorithm to assess each annotator's reliability to achieve high-reliability labels. The algorithm of EM for reliability estimation is shown in Algorithm 1.\n\nGiven the labels of ğ‘ videos annotated by ğ‘€ annotators, we first binarize their labels into a zero-one matrix ğ» ğ‘˜ ğ‘€ğ‘ on the emotion category ğ‘˜ as:\n\nwhere â„ ğ‘˜ ğ‘– ğ‘— will be \"1\", if the ğ‘–th annotator labels the ğ‘—th video with emotion category ğ‘˜, otherwise it will be \"0\".\n\nOur goal is to estimate each annotator' reliability by optimizing the likelihood of their labels. The reliability is formulated as two M-dimensional probability vectors: {ğ›¼ ğ‘˜ ğ‘– } and {ğ›½ ğ‘˜ ğ‘– },\n\nwhere ğ›¼ ğ‘˜ ğ‘– is the reliability probability that the ğ‘–th annotator correctly labels the emotion category ğ‘˜ and ğ›½ ğ‘˜ ğ‘– is the reliability probability that the ğ‘–th annotator does not label the emotion category ğ‘˜. Note that ğ›¼ ğ‘˜ ğ‘– and ğ›½ ğ‘˜ ğ‘– are independent of each other. ğ‘£ ğ‘˜ ğ‘— = {0, 1} denotes whether the ğ‘—th video has the label of the emotion category ğ‘˜. We initialize the ğ‘£ ğ‘˜ ğ‘— via annotation majority voting. With the above definitions, in the E-step of the EM, the reliability probabilities are used to estimate the posterior probability ğœ‘ ğ‘˜ ğ‘— that the ğ‘—th video correctly is labeled with the emotion category ğ‘˜:\n\nwhere ğ‘ ğ‘˜ is the expected probability of the emotion category ğ‘˜ and initialized by 1\n\nğ‘£ ğ‘˜ ğ‘— . ğœ‡ ğ‘˜ ğ‘— and ğœ‚ ğ‘˜ ğ‘— are calculated as:\n\nIn the M-step of the EM, we first update ğ‘ ğ‘˜ as:\n\nThen, we update ğ›¼ ğ‘˜ ğ‘– and ğ›½ ğ‘˜ ğ‘– by Maximum Likelihood Estimation:\n\nFinally, we set ğ‘„ (ğ‘ ğ‘˜ , ğ›¼ ğ‘˜ , ğ›½ ğ‘˜ ) as the convergence objective in EM algorithm as:\n\nWe can further determine whether ğ‘„ (ğ‘ ğ‘˜ , ğ›¼ ğ‘˜ , ğ›½ ğ‘˜ ) converges:\n\nwhere ğ‘¡ denotes the number of iterations and ğœ€ is the convergence threshold that is set as 0.000001 empirically. If ğ‘„ (ğ‘ ğ‘˜ , ğ›¼ ğ‘˜ , ğ›½ ğ‘˜ ) converges, we can obtain the reliability of all annotators, otherwise return the E-step. With the reliability estimation, for each emotion category, we retain five high-reliability labels at least. We use Cronbach's Alpha  [6]  scores to measure the consistency of the retained labels. The results in Table  2  show that the retained labels have high consistency and reliability, with an average score of 0.823 on the  11    3 )-  (5) .\n\nM-step: update ğ‘ ğ‘˜ , ğ›¼ ğ‘˜ ğ‘– , and ğ›½ ğ‘˜ ğ‘– based on {ğœ‘ ğ‘˜ ğ‘— } ğ‘ ğ‘—=1 through the maximum likelihood algorithm as Eq. (  6 )-  (8) .\n\nCalculate ğ‘„ (ğ‘ ğ‘˜ , ğ›¼ ğ‘˜ , ğ›½ ğ‘˜ ) as Eq. (  9 ). until ğ‘„ (ğ‘ ğ‘˜ , ğ›¼ ğ‘˜ , ğ›½ ğ‘˜ ) converges",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Single And Multiple Expression Selection",
      "text": "Using the retained high-reliability labels with their self-confidence scores, we can naturally divide the MAFW into two sets, namely the single expression set and the multiple expression set. Fig.  1(a  Given the self-confidence scores from a high-reliability labeled clip, if no less than half of the annotators have labeled the ğ‘˜th emotion category ğ¶ ğ‘˜ = (ğ‘ ğ‘˜ 1 , ğ‘ ğ‘˜ 2 , . . . , ğ‘ ğ‘˜ ğ‘š ), we then calculate the mean value of the self-confidence scores ğ‘ ğ‘˜ ğ‘šğ‘’ğ‘ğ‘› = ğ‘š ğ‘–=1 ğ‘ ğ‘˜ ğ‘– /ğ‘š on the emotion category, and pick out the emotion label ğ‘˜ w.r.t ğ‘ ğ‘˜ ğ‘šğ‘’ğ‘ğ‘› â‰¥ 0.5 as the valid label.\n\nSingle expression set For valid-labeled clips with single expression labels, we directly classify them into the single expression set; for clips with multiple expression labels, we select the labels with the highest average confidence score as its predominant single expressions and also classify them into the single expression set, so that the single expression set consists of all 9,172 valid-labeled clips with 11-class emotions. Table  3  reports the distribution of clip amount and clip length per expression category on the single expression set.\n\nMultiple expression set Similarly, we create the multiple expression set from the valid-labeled clips with multiple expression",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Expression Snippet Feature Learning With Transformer",
      "text": "In-the-wild FER is a difficult task due to subtle facial expression movements within videos that can be too difficult to be modeled properly by existing methods. In this paper, we propose a novel Transformer-based expression snippet feature learning method (T-ESFL) that can model intra-snippet and inter-snippet expression movements and relations, to obtain movement-sensitive emotion representation. In particular, for intra-snippet modeling, we decompose the modeling of facial movements of the entire video into the modeling of a series of small expression snippets so that enhance the encoding of subtle facial movements of each snippet by gradually attending to more salient information. Meanwhile, for inter-snippet modeling, we introduce a snippet order shuffling and reconstruction learning (SOSR) head and its loss to improve the modeling of subtle motion changes across snippets by training the Transformer to identify shuffled snippet orders. To this end, the T-ESFL consists of three main components, i.e., expression snippet decomposition, Transformer, and SOSR, as illustrated in Fig.  4 . Expression snippet decomposition Formally, given an input FER video clip S, we first decompose the input into a series of small expression snippets S = {ğ‘† 1 , ğ‘† 2 , ......ğ‘† ğ‘› }, where ğ‘† ğ‘– represents the ğ‘–-th snippet and ğ‘› is the total number of snippets. All the snippets have the same length, and they follow consecutive orders along time. To model subtle expression changes within each snippet, we employ a pre-trained CNN  [29]  and attention learning to extract snippet features ğ‘… ğ‘– from each ğ‘† ğ‘– , thus augmenting the Transformer's ability to model intra-snippet expression changes.\n\nTransformer architecture With the snippet features ğ‘… ğ‘– , a Transformer is applied here to model the expression movements across snippets and discover a unified emotion feature for FER. We follow the typical Transformer  [37]  and apply a multi-head attention-based encoder-decoder pipeline for the processing. In general, the multihead attention estimates the correlation between a query tensor and a key tensor and then aggregates a value tensor according to correlation results to obtain an attended output.\n\nSOSR learning To make the output representation of the Transformer more sensitive to subtle expression movements, SOSR shuffles the snippet order and makes T-ESFL reconstruct the correct order in a self-supervision learning manner. The order of frames/audio within each snippet is retained. We follow a Jigsaw permutation  [30]  and shuffle the order pure randomly to deconstruct the normal temporal dependency between the snippets. The shuffled snippets are sent to T-ESFL and predicted the permutation type by using a reconstruction loss ğ¿ ğ‘Ÿğ‘’ğ‘ . Based on this, we can achieve movementsensitive emotion representation ğ‘‡ for robust FER.\n\nOptimization Objective The total objective function of T-ESFL includes two joint cross-entropy losses and is expressed as ğ¿ = ğ¿ ğ‘ğ‘™ğ‘  + 1 ğ‘› â€¢ ğ¿ ğ‘Ÿğ‘’ğ‘ . The first one ğ¿ ğ‘ğ‘™ğ‘  is a FER classification loss, and the second one ğ¿ ğ‘Ÿğ‘’ğ‘ is the snippet order reconstruction loss. Note that, ğ‘› is the number of the decomposed snippets.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Multi-Modal Emotion Prediction",
      "text": "The T-ESFL is easily extended for multi-modal FER, achieving the state-of-the-art performance on both uni-and multi-modal FER. Specifically, we use the ResNet_LSTM network and DPCNN  [21]  to extract audio and text emotion features, respectively. Then, we concatenate the audio, text, and movement-sensitive visual representations to identify the final emotion category via a simple fully-connected layer and Softmax operation. We experimentally verified that the use of multi-modal fusion features effectively improves FER in the wild.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiments",
      "text": "The experimental setup of the benchmarks, including experiment protocols, data preprocessing, assessment measures, and implementation information, are first presented in the section. Then, using a variety of labels and modalities, we conducted comprehensive benchmarks and comparison studies on our MAFW.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "Data&Protocol To facilitate the FER research from laboratory environments to the real world, we performed four challenging benchmark experiments on MAFW: 11-class uni-modal single expression classification, 11-class multi-modal single expression classification, Similar to the evaluation protocol of existing FER databases  [20, 25] , we adopt a 5-fold cross-validation protocol for these benchmarks on our MAFW database.\n\nPreprocessing First, we extracted frame pictures for each clip using OpenCV. Then, after deleting any frames without faces, we used the face-alignment-master program  [3]  to collect face areas and 68 landmarks on all frames. Finally, we performed face alignment using affine transform and matrix rotation in OpenCV.\n\nEvaluation Metrics Consistent with the previous research  [20, 25] , we chose four widely-used validation metrics, i.e., the unweighted average recall (UAR), weighted average recall (WAR), F-score (F1), and Area under the ROC curve (AUC), to evaluate the uni-modal and multi-modal FER tasks, respectively. The UAR is the average accuracy of all expression categories, regardless of the number of samples per class. The WAR is the recognition accuracy of overall expressions, which is related to the number of samples in each category. The F1 is regarded as the weighted harmonic mean value of the accuracy and recall, and here we simply calculate the average of the F1 on all categories. AUC generically refers to the area under the receiver operating characteristic (ROC) curve, and here we calculate the average AUC for all categories. We expect the proposed model to gain improvements in UAR, WAR, F1, and AUC metrics.\n\nImplementation Details In this paper, we employed the Py-Torch framework to implement all models. We conducted experiments in the uni-modal and multi-modal FER tasks, while each task contained single and compound expression classification, respectively. The key training parameters involved in the work are presented in Table  4 . All models were trained on NVIDIA GeForce RTX 3090 and GTX1080, with an initial learning rate of 0.0001 provided by the grid search strategy. During training, the learning rate decreased at a rate of 0.2 when the loss was saturated.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "11-Class Uni-Modal Single Expression Classification.",
      "text": "To evaluate uni-modal single expression classification, we compared our T-ESFL model with existing state-of-the-art FER models including three static frame-based methods ( i.e., Resnet18  [17] , VIT  [9] , and EmotionClassifier  [18, 23] ) and four dynamic sequence-based methods (i.e., C3D  [36] , Resnet18  [16, 17, 19] , VIT_LSTM  [9, 16, 19] , and C3D_LSTM  [16, 19, 36] ). The comparison results are shown in Table  5 . For these static frame-based methods, we first selected five frames from a video evenly as input and then fused the prediction probabilities of the five frames in the output layer of the models to obtain the final prediction result. For these dynamic sequencebased methods, we used all frames in a video for emotion prediction. Compared to other state-of-the-art methods, the proposed T-ESFL achieved the best WAR of 48.18%. Moreover, our approach improved the WAR by 3.43% compared to the commercial model Emotion-Classifier  [18, 23] , and also improved the WAR by 2.62% compared to the second best sequence-based method VIT_LSTM.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "11-Class Multi-Modal Single Expression Classification.",
      "text": "For multi-modal FER, we compared our T-ESFL model with two spatiotemporal neural network methods, i.e., Resnet18_LSTM  [16, 17, 19]  and C3D_LSTM  [16, 19, 36] , as shown in Table  6 . Obviously, the multiple modalities effectively improved the performance of FER. Compared to the other methods, our T-ESFL model obtained the best results in the fusion of different modalities, e.g., 4.09% boost in UAR on video and audio modalities. Moreover, continuously adding the descriptive text modality obtained a relative 3.26% boost in WAR.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "43-Class Uni-Modal Compound Expression",
      "text": "Classification. Table  7  shows the comparison results of 43-class uni-modal compound expression classification. Similar to the above single expression classification, the same six models except for the EmotionClassifier were used for 43-class uni-modal compound expression recognition, with the four evaluation metrics (WAR, UAR, F1, and AUC). Compared to the other methods, the proposed T-ESFL achieved the best WAR of 34.35% and the best AUC of 75.63%.   [9]  frame-based 8.62 31.76 7.46 74.9 C3D  [36]  sequence-based 9.51 28.12 6.73 74.54 Resnet18_LSTM  [16, 17, 19]  sequence-based 6.93 26.6 5.56 68.86 VIT_LSTM  [9, 16, 19]  sequence-based 8.72 32.24 7.59 75.33 C3D_LSTM  [16, 19, 36]  sequence-based 7.  34",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "43-Class Multi-Modal Compound Expression Classification.",
      "text": "For 43-class multi-modal compound expression classification, we also compared our T-ESFL with Resnet18_LSTM and C3D_LSTM, as shown in Table  8 . Compared to the two methods on the multimodal task, the proposed T-ESFL on video and audio modalities achieved the best UAR of 9.93% and WAR of 34.67%, respectively. Moreover, the results of T-ESFL kept achieving improvements after adding the descriptive text modality, i.e., with a relative increase of 1% in WAR, 2.5% in F1, and 0.3% in AUC.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In this paper, we propose a large-scale, multi-label, multi-modal affective database called MAFW in the wild, which contains 10,045 video-audio clips. Each clip is annotated with a high-reliability compound emotional category and a couple of sentences that describe the subjects' affective behaviors in the clip. Therefore, MAFW is the first affective database that provides three types of emotion annotations, i.e., single expression labels (11 class), multiple expression labels (32 class), and bilingual emotion captions. Moreover, we also propose a novel Transformer-based expression snippet feature learning method to obtain movement-sensitive emotion representation, thus achieving state-of-the-art performance on both uni-modal and multi-modal FER in the wild. In the future, we will continue to maintain the MAFW and hope that the release of this database can encourage more research on dynamic FER under unconstrained conditions, e.g., multi-modal emotion recognition, self-supervision FER, video emotion caption, zero-shot AU detection, etc.\n\nTable  1  shows the distributions of 32-class multiple emotion categories on the multiple expression set, including the clip length and clip amount.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A.2 Annotation Format Of The Compound Emotion",
      "text": "To efficiently annotate compound emotions, we developed an annotation tool called ExprLabelTool to generate and save annotation files for each annotator. Fig.  1  shows an annotated file format of a video-audio clip in MAFW. The \"video_id\" represents the index of the video-audio clip, the \"labels\" represents the expression categories labeled by an annotator for the clip, and the \"scores\" represents the self-confidence scores corresponding to the expression categories.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A.3 Annotation Format Of The Emotional Descriptive Text",
      "text": "We carefully design our caption annotation task for emotional descriptive texts and develop several rules to ensure the sentences are of high syntactic and semantic quality in MAFW. Table . 2 shows the annotation instructions given to the annotators for the emotional description text.\n\nTable  2 : The annotation instructions given to the annotator for the emotional description text.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Task",
      "text": "The task is to describe the emotional elements and the movements of the five facial features of the only main character in the video. The emotional elements include the body actions, the environment, the persons the character is speaking to, the tone of voice, and the the events' context. DOs 1. Each emotional description text is available in both Chinese and English. 2. Use a personal pronoun as the subject of the sentence to refer to the main character in the video, such as \"an old man\", \"a boy\", etc., rather than their names (either the character's name or the actor's name). 3. Use the simple present tense. 4. Try to describe the part of the emotional elements in one sentence and modify the verb with an appropriate adverb to emphasize the sentiment state of the character. This part should be no less than eight words. 5. Use predefined sentences to describe the part of the five facial features without adding new descriptions arbitrarily. 6. Each sentence should be grammatically correct.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C Experiments For Video Emotional Captioning",
      "text": "We further discuss the application of our database to another task, e.g., video emotional captioning. To this end, we used two off-theshelf video captioning models, namely Reconstruction network  [10]  and Video paragraph captioning model  [8] , to perform the video emotional captioning task and generate emotional text descriptions. We used three widely-used standard metrics in video captioning to evaluate the generated emotional text descriptions, namely BLEU-4  [7] , METEOR  [1] , and CIDEr  [9] . Table  3  shows the experimental results of video emotional captioning using these two models in our database. Additionally, qualitative examples for video emotional captioning are shown in Figure  3 .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "D Ethical Statement",
      "text": "Although this is a purely academic investigation, the potential sensitivity of facial information necessitates an explicit statement of the ethics involved. Privacy. Our method is used to capture features of facial expressions shared by many individuals, which are related to the common human perception of expressions. Therefore our method does not produce individual-specific facial expression analysis. Our MAFW database is used for academic research only and is compliant with GDPR 1 principles. The copyright of the original and cropped versions of the video remains with the original owner. No commercialization, secondary distribution or alteration of MAFW is allowed by any applicant.\n\nDatabase Bias. During the data collection process, we did not differentiate any factors like gender, race, geography, age, etc. However, some data bias may occur in our MAFW database due to objective limitations such as data sources, the difficulty of collecting different emotions, etc.\n\nMetadata. In our MAFW metadata, we use only the gender statistics automatically inferred from the model pre-trained on CelebA  [6] . We only use this information to evaluate the distribution of data in our MAFW database and do not make use of it in our experiments or elsewhere.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: gives typical examples and the corre-",
      "page": 2
    },
    {
      "caption": "Figure 1: Examples of the compound expressions and the bilingual descriptive texts from MAFW. (a) The single expressions",
      "page": 3
    },
    {
      "caption": "Figure 2: Overview of the construction of MAFW.",
      "page": 4
    },
    {
      "caption": "Figure 1: (a)); (2) multiple expression",
      "page": 4
    },
    {
      "caption": "Figure 1: (b)); (3) emotional descriptive",
      "page": 4
    },
    {
      "caption": "Figure 1: shows examples of the",
      "page": 4
    },
    {
      "caption": "Figure 1: (b) show some typical examples from the 11-class single",
      "page": 5
    },
    {
      "caption": "Figure 3: shows the distribution",
      "page": 6
    },
    {
      "caption": "Figure 3: The distribution of the number of multiple expres-",
      "page": 6
    },
    {
      "caption": "Figure 4: Expression snippet decomposition Formally, given an input",
      "page": 6
    },
    {
      "caption": "Figure 4: The architecture of T-ESFL for movement-sensitive emotion representation learning. Using untrimmed video clips,",
      "page": 7
    },
    {
      "caption": "Figure 1: shows an annotated file format of",
      "page": 11
    },
    {
      "caption": "Figure 1: An example of the emotion annotation file in",
      "page": 11
    },
    {
      "caption": "Figure 2: The framework with the T-ESFL for multi-modal emotion recognition.",
      "page": 12
    },
    {
      "caption": "Figure 2: The multi-modal T-ESFL consists of",
      "page": 12
    },
    {
      "caption": "Figure 3: Visualization examples of video emotional captioning. The words in red are the predicted results of each model close",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotions Alpha": "Neutral 0.878\nSadness 0.948\nSurprise 0.920\nContempt 0.731"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion Recognition in Speech Using Cross-Modal Transfer in the Wild",
      "authors": [
        "Arsha Samuel Albanie",
        "Andrea Nagrani",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM International Conference on Multimedia",
      "doi": "10.1145/3240508.3240578"
    },
    {
      "citation_id": "2",
      "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1208"
    },
    {
      "citation_id": "3",
      "title": "How Far are We from Solving the 2D & 3D Face Alignment Problem? (and a Dataset of 230,000 3D Facial Landmarks)",
      "authors": [
        "Adrian Bulat",
        "Georgios Tzimiropoulos"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2017.116"
    },
    {
      "citation_id": "4",
      "title": "Intelligent Teaching Evaluation System Integrating Facial Expression and Behavior Recognition in Teaching Video",
      "authors": [
        "Zheng Chen",
        "Meiyu Liang",
        "Wanying Yu",
        "Yongkang Huang",
        "Xiaoxiao Wang"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Big Data and Smart Computing (BigComp)",
      "doi": "10.1109/BigComp51126.2021.00019"
    },
    {
      "citation_id": "5",
      "title": "Crowdsourcing quality control model protecting location privacy of workers",
      "authors": [
        "Xiang Chu",
        "Qiuyan Zhong"
      ],
      "year": "2016",
      "venue": "Systems Engineering -Theory & Practice",
      "doi": "10.12011/1000-6788(2016)08-2047-09"
    },
    {
      "citation_id": "6",
      "title": "Coefficient alpha and the internal structure of tests",
      "authors": [
        "Lee Cronbach"
      ],
      "year": "1951",
      "venue": "Psychometrika",
      "doi": "10.1007/BF02310555"
    },
    {
      "citation_id": "7",
      "title": "From Individual to Group-Level Emotion Recognition: EmotiW 5.0",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Shreya Ghosh",
        "Jyoti Joshi",
        "Jesse Hoey",
        "Tom Gedeon"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction",
      "doi": "10.1145/3136755.3143004"
    },
    {
      "citation_id": "8",
      "title": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace",
      "authors": [
        "Kollias Dimitrios",
        "Zafeiriou Stefanos"
      ],
      "year": "2019",
      "venue": "30th British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "9",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In 9th International Conference on Learning Representations (ICLR)",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In 9th International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "10",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "Shichuan Du",
        "Yong Tao",
        "Aleix Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": "10.1073/pnas.1322355111"
    },
    {
      "citation_id": "11",
      "title": "EMOPAIN Challenge 2020: Multimodal Pain Evaluation from Facial and Bodily Expressions",
      "authors": [
        "Joy Egede",
        "Siyang Song",
        "Temitayo Olugbade",
        "Chongyang Wang",
        "Amanda De",
        "C Williams",
        "Hongying Meng",
        "Min Aung",
        "Nicholas Lane",
        "Michel Valstar",
        "Nadia Bianchi-Berthouze"
      ],
      "year": "2020",
      "venue": "15th IEEE International Conference on Automatic Face and Gesture Recognition (FG)",
      "doi": "10.1109/FG47880.2020.00078"
    },
    {
      "citation_id": "12",
      "title": "Expression and the nature of emotion",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1984",
      "venue": "Approaches to Emotion"
    },
    {
      "citation_id": "13",
      "title": "Facial expression and emotion",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1993",
      "venue": "American Psychologist",
      "doi": "10.1037/0003-066X.48.4.384"
    },
    {
      "citation_id": "14",
      "title": "Fusing Body Posture With Facial Expressions for Joint Recognition of Affect in Child-Robot Interaction",
      "authors": [
        "Panagiotis Paraskevas Filntisis",
        "Niki Efthymiou",
        "Petros Koutras"
      ],
      "year": "2019",
      "venue": "Gerasimos Potamianos, and Petros Maragos",
      "doi": "10.1109/LRA.2019.2930434"
    },
    {
      "citation_id": "15",
      "title": "Why is Facial Expression Analysis in the Wild Challenging?",
      "authors": [
        "Tobias Gehrig",
        "HazÄ±m Kemal"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 on Emotion Recognition in the Wild Challenge and Workshop",
      "doi": "10.1145/2531923.2531924"
    },
    {
      "citation_id": "16",
      "title": "Learning to forget: continual prediction with LSTM",
      "authors": [
        "F Gers",
        "J Schmidhuber",
        "F Cummins"
      ],
      "year": "1999",
      "venue": "9th International Conference on Artificial Neural Networks (ICANN)",
      "doi": "10.1049/cp:19991218"
    },
    {
      "citation_id": "17",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2016.90"
    },
    {
      "citation_id": "18",
      "title": "A fully end-to-end cascaded cnn for facial landmark detection",
      "authors": [
        "Zhenliang He",
        "Meina Kan",
        "Jie Zhang",
        "Xilin Chen",
        "Shiguang Shan"
      ],
      "year": "2017",
      "venue": "12th IEEE International Conference on Automatic Face Gesture Recognition (FG)",
      "doi": "10.1109/FG.2017.33"
    },
    {
      "citation_id": "19",
      "title": "Long Short-Term Memory",
      "authors": [
        "Sepp Hochreiter",
        "JÃ¼rgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation",
      "doi": "10.1162/neco.1997.9.8.1735"
    },
    {
      "citation_id": "20",
      "title": "DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions in the Wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia",
      "doi": "10.1145/3394171.3413620"
    },
    {
      "citation_id": "21",
      "title": "Deep Pyramid Convolutional Neural Networks for Text Categorization",
      "authors": [
        "Rie Johnson",
        "Tong Zhang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1052"
    },
    {
      "citation_id": "22",
      "title": "Context-Aware Emotion Recognition Networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2019.01024"
    },
    {
      "citation_id": "23",
      "title": "A convolutional neural network cascade for face detection",
      "authors": [
        "Haoxiang Li",
        "Zhe Lin",
        "Xiaohui Shen",
        "Jonathan Brandt",
        "Gang Hua"
      ],
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2015.7299170"
    },
    {
      "citation_id": "24",
      "title": "Deep Facial Expression Recognition: A Survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.2981446"
    },
    {
      "citation_id": "25",
      "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2017.277"
    },
    {
      "citation_id": "26",
      "title": "Deep Learning Face Attributes in the Wild",
      "authors": [
        "Ziwei Liu",
        "Ping Luo",
        "Xiaogang Wang",
        "Xiaoou Tang"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2015.425"
    },
    {
      "citation_id": "27",
      "title": "Mel Frequency Cepstral Coefficients for Music Modeling",
      "authors": [
        "Beth Logan"
      ],
      "year": "2000",
      "venue": "1st International Symposium on Music Information Retrieval (ISMIR)"
    },
    {
      "citation_id": "28",
      "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops",
      "doi": "10.1109/CVPRW.2010.5543262"
    },
    {
      "citation_id": "29",
      "title": "Frame Attention Networks for Facial Expression Recognition in Videos",
      "authors": [
        "Debin Meng",
        "Xiaojiang Peng",
        "Kai Wang",
        "Yu Qiao"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Image Processing",
      "doi": "10.1109/ICIP.2019.8803603"
    },
    {
      "citation_id": "30",
      "title": "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles",
      "authors": [
        "Mehdi Noroozi",
        "Paolo Favaro"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "31",
      "title": "Web-based database for facial expression analysis",
      "authors": [
        "M Pantic",
        "M Valstar",
        "R Rademaker",
        "L Maat"
      ],
      "year": "2005",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME)",
      "doi": "10.1109/ICME.2005.1521424"
    },
    {
      "citation_id": "32",
      "title": "A general psychoevolutionary theory of emotion",
      "authors": [
        "R Plutchik"
      ],
      "year": "2000",
      "venue": "Emotion Theory Research & Experience"
    },
    {
      "citation_id": "33",
      "title": "Facial Expression Modeling and Synthesis for Patient Simulator Systems: Past, Present, and Future",
      "authors": [
        "Maryam Pourebadi",
        "Laurel Riek"
      ],
      "year": "2022",
      "venue": "ACM Trans. Comput. Healthcare",
      "doi": "10.1145/3483598"
    },
    {
      "citation_id": "34",
      "title": "Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm",
      "authors": [
        "A Dawida",
        "Skene"
      ],
      "year": "1979",
      "venue": "Journal of the Royal Statistical Society"
    },
    {
      "citation_id": "35",
      "title": "The recognition of pure and blended facial expressions of emotion from still photographs",
      "authors": [
        "Nummenmaa Tapio"
      ],
      "year": "1988",
      "venue": "Scandinavian Journal of Psychology",
      "doi": "10.1111/j.1467-9450.1988.tb00773.x"
    },
    {
      "citation_id": "36",
      "title": "Learning Spatiotemporal Features with 3D Convolutional Networks",
      "authors": [
        "Du Tran",
        "Lubomir Bourdev",
        "Rob Fergus",
        "Lorenzo Torresani",
        "Manohar Paluri"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2015.510"
    },
    {
      "citation_id": "37",
      "title": "Attention is All You Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "38",
      "title": "AP-10K: A Benchmark for Animal Pose Estimation in the Wild",
      "authors": [
        "Hang Yu",
        "Yufei Xu",
        "Jing Zhang",
        "Wei Zhao",
        "Ziyu Guan",
        "Dacheng Tao"
      ],
      "year": "2021",
      "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "39",
      "title": "BP4D-Spontaneous: a highresolution spontaneous 3D dynamic facial expression database",
      "authors": [
        "Xing Zhang",
        "Lijun Yin",
        "Jeffrey Cohn",
        "Shaun Canavan",
        "Michael Reale",
        "Andy Horowitz",
        "Peng Liu",
        "Jeffrey Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing",
      "doi": "10.1016/j.imavis.2014.06.002"
    },
    {
      "citation_id": "40",
      "title": "Emotion Distribution Recognition from Facial Expressions",
      "authors": [
        "Ying Zhou",
        "Hui Xue",
        "Xin Geng"
      ],
      "year": "2015",
      "venue": "Proceedings of the 23rd ACM International Conference on Multimedia",
      "doi": "10.1145/2733373.2806328"
    },
    {
      "citation_id": "41",
      "title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language",
      "authors": [
        "J Michael",
        "Alon Denkowski",
        "Lavie"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014",
      "doi": "10.3115/v1/w14-3348"
    },
    {
      "citation_id": "42",
      "title": "Learning to forget: continual prediction with LSTM",
      "authors": [
        "F Gers",
        "J Schmidhuber",
        "F Cummins"
      ],
      "year": "1999",
      "venue": "9th International Conference on Artificial Neural Networks (ICANN)",
      "doi": "10.1049/cp:19991218"
    },
    {
      "citation_id": "43",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2016.90"
    },
    {
      "citation_id": "44",
      "title": "Long Short-Term Memory",
      "authors": [
        "Sepp Hochreiter",
        "JÃ¼rgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation",
      "doi": "10.1162/neco.1997.9.8.1735"
    },
    {
      "citation_id": "45",
      "title": "Deep Pyramid Convolutional Neural Networks for Text Categorization",
      "authors": [
        "Rie Johnson",
        "Tong Zhang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1052"
    },
    {
      "citation_id": "46",
      "title": "Deep Learning Face Attributes in the Wild",
      "authors": [
        "Ziwei Liu",
        "Ping Luo",
        "Xiaogang Wang",
        "Xiaoou Tang"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2015.425"
    },
    {
      "citation_id": "47",
      "title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
      "authors": [
        "Kishore Papineni",
        "Salim Roukos",
        "Todd Ward",
        "Wei-Jing Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.3115/1073083.1073135"
    },
    {
      "citation_id": "48",
      "title": "Towards Diverse Paragraph Captioning for Untrimmed Videos",
      "authors": [
        "Yuqing Song",
        "Shizhe Chen",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual"
    },
    {
      "citation_id": "49",
      "title": "CIDEr: Consensus-based image description evaluation",
      "authors": [
        "C Ramakrishna Vedantam",
        "Devi Zitnick",
        "Parikh"
      ],
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015",
      "doi": "10.1109/CVPR.2015.7299087"
    },
    {
      "citation_id": "50",
      "title": "Reconstruction Network for Video Captioning",
      "authors": [
        "Bairui Wang",
        "Lin Ma",
        "Wei Zhang",
        "Wei Liu"
      ],
      "year": "2018",
      "venue": "2018 IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00795"
    }
  ]
}