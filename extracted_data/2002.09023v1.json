{
  "paper_id": "2002.09023v1",
  "title": "Audio-Video Emotion Recognition In The Wild Using Deep Hybrid Networks",
  "published": "2020-02-20T21:18:17Z",
  "authors": [
    "Xin Guo",
    "Luisa F. Polanía",
    "Kenneth E. Barner"
  ],
  "keywords": [
    "Audio-video emotion recognition",
    "multimodal fusion",
    "long short-term memory networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents an audiovisual-based emotion recognition hybrid network. While most of the previous work focuses either on using deep models or hand-engineered features extracted from images, we explore multiple deep models built on both images and audio signals. Specifically, in addition to convolutional neural networks (CNN) and recurrent neutral networks (RNN) trained on facial images, the hybrid network also contains one SVM classifier trained on holistic acoustic feature vectors, one long short-term memory network (LSTM) trained on short-term feature sequences extracted from segmented audio clips, and one Inception(v2)-LSTM network trained on image-like maps, which are built based on short-term acoustic feature sequences. Experimental results show that the proposed hybrid network outperforms the baseline method by a large margin.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is relevant in many computing areas that take into account the affective state of the user, such as human-computer interaction  [1] , human-robot interaction  [2] , music and image recommendation  [3] , affective video summarization  [4] , and personal wellness and assistive technologies  [5] . Although emotion recognition is an interesting problem, it is also very challenging unless the recording conditions are well controlled. Emotion recognition \"in the wild\" suffers from many issues that need to be overcome, such as cluttered backgrounds, large variances in face pose and illumination, video and audio noise, and occlusions.\n\nRecently, hybrid neural networks combining CNNs and RNNs  [6, 7, 8]  have become the state-of-the-art for emotion recognition. Of particular interest are the top-performing works of the EmotiW Challenge, whose goal is to advance\n\nThe work is supported by the National Science Foundation under Grant No. 1319598. emotion recognition in unconstrained conditions by providing researchers with a platform to benchmark the performance of their algorithms on \"in-the-wild\" datasets. One of the sub-challenges of the EmotiW challenge is the audio-video emotion recognition sub-challenge, which is based on an augmented version of the AFEW dataset  [9]  that contains short video clips extracted from movies that have been annotated for seven different emotions.\n\nDeep learning has played an important role in most of the sub-challenge winning submissions. In 2013, the winners presented a method that combines CNNs for static faces, an auto-encoder for human action recognition, a deep belief network for audio information, and a shallow network architecture for feature extraction of the mouth region  [10] . The winners of the 2014 sub-challenge used CNNs for feature extraction of the aligned faces provided by the challenge organizers  [11] , while in 2016, the winners of the sub-challenge proposed a hybrid network architecture that combines 3D CNNs and a CNN-RNN in a late-fusion fashion  [6] .\n\nWhile a variety of methods based on images have been proposed, the audio channel has been explored in a less extent. Existing approaches that exploit the audio channel for emotion recognition include the use of support vector machines (SVM)  [12, 6]  , random forests  [13, 14]  and CNNs trained on comprehensive acoustic vectors extracted by openSmile  [15] . We propose to fully exploit the audiochannel information in this paper. Inspired by the recurrent support vector machines designed by Yun et al.  [16]  on event detection, we propose an LSTM  [17]  trained on short-term audio features extracted from segmented audio clips. Furthermore, a CNN-RNN network trained on image-like maps, formed by stacking short-term audio features, is also presented in this paper. The proposed hybrid network (Figure  1 ) results in the combination of a CNN-RNN network trained on images and audio-based models, and achieves an overall accuracy of 55.61% and 51.15% on the validation and testing sets, respectively, surpassing the audio-video emotion recognition sub-challenge baseline of 38.81% on the validation set with significant gains.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "The Proposed Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Vgg-Lstm Based On Faces",
      "text": "A traditional CNN-LSTM neural network  [6, 14]  is explored to learn emotion from faces. Video frames are extracted at a frequency of 60 fps. Faces and facial landmarks are first detected within each frame using the method described in  [18] , then a 2D affine transformation where the left and right eye corners of all the images are aligned to the same positions is performed (the code of the face detection and alignment algorithms is developed based on  [19] ).\n\nAligned faces are used as input to the VGG-16 convolutional neural network  [20] . The VGG architecture is modified by changing the number of neurons in the last layer to 7, indicating 7 emotion classes. This modified VGG architecture is initialized with the parameters of the VGG-FACE model, except for the last fully-connected layer which is initialized with weights sampled from a Gaussian distribution of zero mean and variance 1×10 -4 , and trained from scratch with the learning rate for the weight and bias filters set to be 10 times larger than the overall learning rate. The VGG-FACE model was presented as the result of training the 16-layer VGG architecture on a large-scale dataset containing 2.6M images of 2.6K celebrities and public figures for face recognition in  [21]  The training procedure is three-fold. First, the modified VGG network is trained on the facial expression recognition 2013 (FER-2013) database  [22] . The FER-2013 database contains 35887 images corresponding to 7 basic emotions. The idea of this step is to transfer the knowledge from face recognition to face emotion recognition. Second, the resulting model is fine-tuned on the detected faces of the AFEW dataset. Third, the layers of the fine-tuned model after the \"fc6\" layer are replaced by a one-layer LSTM and a final fully connected layer with 7 output units. The weights of the LSTM are initialized with values drawn from a uniform distribution over [-0.01, 0.01] and the bias terms are initialized to 0. The combined VGG-LSTM is trained end-to-end.\n\nThe LSTM layer has 128 nodes and the length of the input sequence is 16. Face images extracted at every 8 frames of the original video sequence are selected as input to the VGG-LSTM network. Experimental results show that the frame gap helps improve the classification accuracy since facial change is more visible in this way.\n\nUnlike some existing works that first train the CNN and use their \"fc6\" features as input vector sequences for the LSTM network, the proposed structure connects the VGG and LSTM networks end-to-end and learns all the parameters simultaneously. Experimental results show that our VGG-LSTM outperforms the results of the winner of the audio-video emotion recognition sub-challenge in 2016.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Acoustic Svm Classifier",
      "text": "An SVM classifier, which is learned based on the 1582dimensional acoustic features extracted using openSMILE, is incorporated into the hybrid network. Acoustic features include low level descriptors, such as energy, melfrequency cepstral coefficients (MFCCs), linear predictive coding (LPC), zero-crossing rate (ZCR), spectral flux, spectral roll-off, chroma vector, and statistical features summarized by functions, such as mean and standard deviation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Lstm Based On Audio Clips",
      "text": "Instead of extracting holistic features, each audio is divided into segments of length 100 ms, using an overlapping factor of 50%, and then segment-level features are extracted to form a sequence of vectors. Specifically, 34 short-term features are extracted for each segment using pyAudioAnalysis  [23] . Features include ZCR, energy, entropy of energy, spectral centroid, spectral spread, spectral flux, spectral roll-off, MFCCs, chroma vector, and chroma deviation. Assume that the audio signal has length m, then the number of sequences n is (m -50)/50. Therefore, this feature extraction process results in n sequences of dimension 34 × 1. Since each audio is of different length, a sequence length converter is applied to make the number of sequences be at least 16 by copying the last feature vector of the sequence 16-n times when the number of sequences is less than 16. A one-layer LSTM with 512 neurons is trained on the sequence of feature vectors. Unlike the audio SVM model which focuses on the holistic properties of the signal, the audio LSTM model focuses on learning the dynamic temporal behavior of the audio signals.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Inception(V2)-Lstm Based On Audio Maps",
      "text": "In this section, the sequence of feature vectors from Section 2.3 is converted into sequential image-like maps. Specifically, the feature vectors are organized in matrix form to build an image-like map of dimension 34 × n. The next step is to segment this image-like map into smaller maps of size 34 × 34 using an overlapping factor of 50%. For the architecture proposed in this section, the sequence length n must be a multiple of 17 and greater or equal than 34. If this condition is not satisfied, then the last column of the 34 × n image-like map is replicated n -n times, where n is the closest multiple of 17 larger than n. This approach results in a sequence of 34 × 34 image-like maps, whose sequence length is (n -17)/17.\n\nA network similar to the VGG-LSTM network described in section 2.1, Inception(v2)-LSTM, is developed to train on image-like maps. Instead of using the VGG architecture, we use Inception-v2  [24]  to train the image-like maps first. The number of output units of the last layer is changed to 7, and the training parameters, such as the learning rate and the weight decay are set the same as the ones used in Inception-v2 on ImageNet  [25] . After the training of the modified Inception-v2 on the individual image-like maps, the layers after the \"global pool\" layer of the Inception-v2 architecture are replaced by a onelayer LSTM with 128 neurons and a fully connected layer with 7 outputs. The resulting network is referred to as Inception(v2)-LSTM. This network takes a sequence of 8 image-like maps at a time and learns the features end-toend to model the dynamic temporal properties of the sequence. Since the sequence length of the image-like maps is (n -17)/17 and needs to be greater than 8 to serve as input to the Inception(v2)-LSTM architecture, the last 34 × 34 imagelike map of the sequence is copied 8 -(n -17)/17 times to make the sequence satisfy the minimum length requirement. The initial learning rate is set to 0.001 and decreases every 3000 iterations by a factor of 10. The batch size, the weight decay and the maximum number of iterations are set to 16, 0.002 and 10000, respectively.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Database",
      "text": "The AFEW database used in EmotiW 2017 contains 773, 383 and 653 audio-video movie clips in the training, validation and testing datasets, respectively. The task is to assign a single emotion label to a video clip from the 7 basic emotions, namely, anger, disgust, fear, happiness, neutral, sad and surprise. Participants compete on the accuracy of the testing data 1  .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results Of The Proposed Models",
      "text": "Confusion matrices for each model are shown in Tables  1  through 5 . One VGG-LSTM model is trained on the aligned faces, which are obtained as described in Section 2.1, and another VGG-LSTM model is trained on the faces provided by the challenge organizers. Our best VGG-LSTM model achieves an overall classification accuracy of 49.61%, outperforming the 45.43% accuracy obtained by the winner of the 2016 audio-video emotion recognition sub-challenge, which suggests that the frame gap introduced by the proposed VGG-LSTM model is a better way to represent the dynamics of face expression in video. The second VGG-LSTM model trained on the faces provided by the challenge organizers complements the proposed model, and the combination of the two achieves a classification accuracy of 51.02% on the validation set.\n\nAudio models, including audio SVM, audio LSTM and audio Inception(v2)-LSTM have lower accuracy than the VGG-LSTM models trained on faces. However, they perform well on the anger class, and therefore, it improves the overall accuracy of the hybrid network.\n\nThe aforementioned deep models are combined using decision fusion. Grid search is employed to find the model weights that maximize the classification accuracy on the validation set. Fused hybrid network achieves a classification accuracy of 55.61% on the validation set, while the challenge baseline is of accuracy 38.81%. When trained on a combination of the training and validation sets, the accuracy on the testing set of the proposed hybrid network is 51.15%. The corresponding confusion matrix is shown in Table  6 .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we proposed an audiovisual-based hybrid network that combines the predictions of 5 models for emotion recognition in the wild, with an emphasis on exploring audio channels. The overall accuracy of the proposed method achieves 55.61% and 51.15% classification accuracy on the validation and testing dataset, respectively, surpassing the audio-video emotion recognition sub-challenge baseline of 38.81% on the validation set with significant gains.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall structure of the proposed hybrid network. For visualization purposes, only one VGG-LSTM on faces is",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Confusion matrix results of the VGG-LSTM net-",
      "data": [
        {
          "AN": "53.12",
          "DI": "6.25",
          "FE": "7.81",
          "HA": "0",
          "NE": "17.19",
          "SA": "3.12",
          "SU": "12.50"
        },
        {
          "AN": "17.50",
          "DI": "27.50",
          "FE": "7.50",
          "HA": "2.50",
          "NE": "25.00",
          "SA": "15.00",
          "SU": "5.00"
        },
        {
          "AN": "21.74",
          "DI": "4.35",
          "FE": "23.91",
          "HA": "13.04",
          "NE": "6.52",
          "SA": "17.39",
          "SU": "13.04"
        },
        {
          "AN": "7.94",
          "DI": "1.59",
          "FE": "0",
          "HA": "84.13",
          "NE": "0",
          "SA": "4.76",
          "SU": "1.59"
        },
        {
          "AN": "11.11",
          "DI": "11.11",
          "FE": "7.94",
          "HA": "6.35",
          "NE": "53.97",
          "SA": "6.35",
          "SU": "3.17"
        },
        {
          "AN": "8.20",
          "DI": "4.92",
          "FE": "1.64",
          "HA": "6.56",
          "NE": "22.95",
          "SA": "55.74",
          "SU": "0"
        },
        {
          "AN": "21.74",
          "DI": "6.52",
          "FE": "17.39",
          "HA": "4.35",
          "NE": "17.39",
          "SA": "4.35",
          "SU": "28.26"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Confusion matrix results of the VGG-LSTM net-",
      "data": [
        {
          "AN": "64.06",
          "DI": "1.56",
          "FE": "7.81",
          "HA": "1.56",
          "NE": "12.50",
          "SA": "6.25",
          "SU": "6.25"
        },
        {
          "AN": "22.50",
          "DI": "15.00",
          "FE": "5.00",
          "HA": "10.00",
          "NE": "25.00",
          "SA": "20.00",
          "SU": "2.50"
        },
        {
          "AN": "32.61",
          "DI": "8.70",
          "FE": "26.09",
          "HA": "4.35",
          "NE": "13.04",
          "SA": "8.70",
          "SU": "6.52"
        },
        {
          "AN": "9.52",
          "DI": "3.17",
          "FE": "0",
          "HA": "73.02",
          "NE": "6.35",
          "SA": "6.35",
          "SU": "1.59"
        },
        {
          "AN": "14.29",
          "DI": "11.11",
          "FE": "1.59",
          "HA": "3.17",
          "NE": "63.49",
          "SA": "6.35",
          "SU": "0"
        },
        {
          "AN": "16.39",
          "DI": "11.48",
          "FE": "6.56",
          "HA": "8.20",
          "NE": "13.11",
          "SA": "40.98",
          "SU": "3.28"
        },
        {
          "AN": "32.61",
          "DI": "6.52",
          "FE": "17.39",
          "HA": "0",
          "NE": "15.22",
          "SA": "8.70",
          "SU": "19.57"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Confusion matrix results of the VGG-LSTM net-",
      "data": [
        {
          "AN": "76.56",
          "DI": "0",
          "FE": "3.12",
          "HA": "9.38",
          "NE": "7.81",
          "SA": "3.12",
          "SU": "0"
        },
        {
          "AN": "25.00",
          "DI": "0",
          "FE": "0",
          "HA": "42.50",
          "NE": "20.00",
          "SA": "12.50",
          "SU": "0"
        },
        {
          "AN": "23.91",
          "DI": "0",
          "FE": "30.43",
          "HA": "23.91",
          "NE": "13.04",
          "SA": "8.70",
          "SU": "0"
        },
        {
          "AN": "15.87",
          "DI": "1.59",
          "FE": "9.52",
          "HA": "42.86",
          "NE": "20.63",
          "SA": "9.52",
          "SU": "0"
        },
        {
          "AN": "12.70",
          "DI": "1.59",
          "FE": "3.17",
          "HA": "34.92",
          "NE": "46.03",
          "SA": "1.59",
          "SU": "0"
        },
        {
          "AN": "11.48",
          "DI": "0",
          "FE": "11.48",
          "HA": "26.23",
          "NE": "21.31",
          "SA": "27.87",
          "SU": "1.64"
        },
        {
          "AN": "19.57",
          "DI": "0",
          "FE": "17.39",
          "HA": "36.96",
          "NE": "13.04",
          "SA": "13.04",
          "SU": "0"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Confusion matrix results of the VGG-LSTM net-",
      "data": [
        {
          "AN": "48.44",
          "DI": "1.56",
          "FE": "0",
          "HA": "15.62",
          "NE": "15.62",
          "SA": "18.75",
          "SU": "0"
        },
        {
          "AN": "15.00",
          "DI": "2.50",
          "FE": "0",
          "HA": "35.00",
          "NE": "32.50",
          "SA": "15.00",
          "SU": "0"
        },
        {
          "AN": "30.43",
          "DI": "0",
          "FE": "0",
          "HA": "19.57",
          "NE": "28.26",
          "SA": "21.74",
          "SU": "0"
        },
        {
          "AN": "12.70",
          "DI": "4.76",
          "FE": "0",
          "HA": "33.33",
          "NE": "30.16",
          "SA": "19.05",
          "SU": "0"
        },
        {
          "AN": "6.35",
          "DI": "3.17",
          "FE": "0",
          "HA": "17.46",
          "NE": "52.38",
          "SA": "20.63",
          "SU": "0"
        },
        {
          "AN": "11.48",
          "DI": "6.56",
          "FE": "0",
          "HA": "22.95",
          "NE": "32.79",
          "SA": "26.23",
          "SU": "0"
        },
        {
          "AN": "17.39",
          "DI": "0",
          "FE": "2.17",
          "HA": "30.43",
          "NE": "36.96",
          "SA": "13.04",
          "SU": "0"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 5: Confusion matrix of the audio Incetion(v2)-",
      "data": [
        {
          "AN": "56.25",
          "DI": "0",
          "FE": "0",
          "HA": "29.69",
          "NE": "10.94",
          "SA": "3.12",
          "SU": "0"
        },
        {
          "AN": "12.50",
          "DI": "0",
          "FE": "0",
          "HA": "57.50",
          "NE": "27.50",
          "SA": "2.50",
          "SU": "0"
        },
        {
          "AN": "13.04",
          "DI": "0",
          "FE": "2.17",
          "HA": "45.65",
          "NE": "36.96",
          "SA": "2.17",
          "SU": "0"
        },
        {
          "AN": "11.11",
          "DI": "0",
          "FE": "0",
          "HA": "52.38",
          "NE": "26.98",
          "SA": "9.52",
          "SU": "0"
        },
        {
          "AN": "6.35",
          "DI": "0",
          "FE": "0",
          "HA": "52.38",
          "NE": "39.68",
          "SA": "1.59",
          "SU": "0"
        },
        {
          "AN": "8.20",
          "DI": "0",
          "FE": "0",
          "HA": "63.93",
          "NE": "16.39",
          "SA": "11.48",
          "SU": "0"
        },
        {
          "AN": "6.52",
          "DI": "2.17",
          "FE": "2.17",
          "HA": "56.52",
          "NE": "26.09",
          "SA": "6.52",
          "SU": "0"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 5: Confusion matrix of the audio Incetion(v2)-",
      "data": [
        {
          "AN": "77.55",
          "DI": "0",
          "FE": "4.08",
          "HA": "8.16",
          "NE": "7.14",
          "SA": "2.04",
          "SU": "1.02"
        },
        {
          "AN": "32.50",
          "DI": "10.00",
          "FE": "2.50",
          "HA": "12.50",
          "NE": "20.00",
          "SA": "20.00",
          "SU": "2.50"
        },
        {
          "AN": "31.43",
          "DI": "0",
          "FE": "50.00",
          "HA": "1.43",
          "NE": "5.71",
          "SA": "7.14",
          "SU": "4.29"
        },
        {
          "AN": "20.83",
          "DI": "0",
          "FE": "1.39",
          "HA": "63.89",
          "NE": "10.42",
          "SA": "3.47",
          "SU": "0"
        },
        {
          "AN": "16.69",
          "DI": "1.04",
          "FE": "7.77",
          "HA": "6.22",
          "NE": "50.78",
          "SA": "11.92",
          "SU": "2.59"
        },
        {
          "AN": "22.50",
          "DI": "1.25",
          "FE": "11.25",
          "HA": "11.25",
          "NE": "16.25",
          "SA": "36.25",
          "SU": "1.25"
        },
        {
          "AN": "10.71",
          "DI": "3.57",
          "FE": "35.71",
          "HA": "10.71",
          "NE": "14.29",
          "SA": "25.00",
          "SU": "0"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "3",
      "title": "Affective state estimation for human-robot interaction",
      "authors": [
        "D Kulic",
        "E Croft"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Robotics"
    },
    {
      "citation_id": "4",
      "title": "Emotionbased music recommendation by affinity discovery from film music",
      "authors": [
        "M Shan",
        "F Kuo",
        "M Chiang",
        "S Lee"
      ],
      "year": "2009",
      "venue": "Expert systems with applications"
    },
    {
      "citation_id": "5",
      "title": "Exploiting facial expressions for affective video summarisation",
      "authors": [
        "H Joho",
        "J Jose",
        "R Valenti",
        "N Sebe"
      ],
      "year": "2009",
      "venue": "Proceedings of the ACM international conference on image and video retrieval"
    },
    {
      "citation_id": "6",
      "title": "Human computing and machine understanding of human behavior: A survey",
      "authors": [
        "M Pantic",
        "A Pentland",
        "A Nijholt",
        "T Huang"
      ],
      "year": "2007",
      "venue": "Artifical Intelligence for Human Computing"
    },
    {
      "citation_id": "7",
      "title": "Video-based emotion recognition using CNN-RNN and C3D hybrid networks",
      "authors": [
        "Y Fan",
        "X Lu",
        "D Li",
        "Y Liu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "8",
      "title": "How deep neural networks can improve emotion recognition on video data",
      "authors": [
        "P Khorrami",
        "T Paine",
        "K Brady",
        "C Dagli",
        "T Huang"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "9",
      "title": "Recurrent neural networks for emotion recognition in video",
      "authors": [
        "S Kahou",
        "V Michalski",
        "K Konda",
        "R Memisevic",
        "C Pal"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "10",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2012",
      "venue": "Collecting large, richly annotated facial-expression databases from movies"
    },
    {
      "citation_id": "11",
      "title": "Combining modality specific deep neural networks for emotion recognition in video",
      "authors": [
        "S Kahou"
      ],
      "year": "2013",
      "venue": "Proceedings of the 15th International conference on multimodal interaction"
    },
    {
      "citation_id": "12",
      "title": "Combining multiple kernel methods on Riemannian manifold for emotion recognition in the wild",
      "authors": [
        "M Liu",
        "R Wang",
        "S Li",
        "S Shan",
        "Z Huang",
        "X Chen"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "13",
      "title": "Support-vector networks",
      "authors": [
        "C Cortes",
        "V Vapnik"
      ],
      "year": "1995",
      "venue": "Mach. Learn"
    },
    {
      "citation_id": "14",
      "title": "Random forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Mach. Learn"
    },
    {
      "citation_id": "15",
      "title": "Lstm for dynamic emotion and group emotion recognition in the wild",
      "authors": [
        "B Sun",
        "Q Wei",
        "L Li",
        "Q Xu",
        "J He",
        "L Yu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "16",
      "title": "Multi-clue fusion for emotion recognition in the wild",
      "authors": [
        "J Yan"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "17",
      "title": "Recurrent support vector machines for audio-based multimedia event detection",
      "authors": [
        "Y Wang",
        "F Metze"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval"
    },
    {
      "citation_id": "18",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Comput"
    },
    {
      "citation_id": "19",
      "title": "One millisecond face alignment with an ensemble of regression trees",
      "authors": [
        "V Kazemi",
        "Josephine Sullivan"
      ],
      "year": "2014",
      "venue": "CVPR"
    },
    {
      "citation_id": "20",
      "title": "Effective face frontalization in unconstrained images",
      "authors": [
        "T Hassner",
        "S Harel",
        "E Paz",
        "R Enbar"
      ],
      "year": "2015",
      "venue": "CVPR"
    },
    {
      "citation_id": "21",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "22",
      "title": "Deep face recognition",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "BMVC"
    },
    {
      "citation_id": "23",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow"
      ],
      "year": "2013",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "24",
      "title": "pyaudioanalysis: An open-source python library for audio signal analysis",
      "authors": [
        "T Giannakopoulos"
      ],
      "year": "2015",
      "venue": "PloS one"
    },
    {
      "citation_id": "25",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "CoRR"
    },
    {
      "citation_id": "26",
      "title": "ImageNet: A Large-Scale Hierarchical Image Database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "ImageNet: A Large-Scale Hierarchical Image Database"
    }
  ]
}