{
  "paper_id": "2412.16444v1",
  "title": "Effective Context Modeling Framework For Emotion Recognition In Conversations",
  "published": "2024-12-21T02:22:06Z",
  "authors": [
    "Cuong Tran Van",
    "Thanh V. T. Tran",
    "Van Nguyen",
    "Truong Son Hy"
  ],
  "keywords": [
    "Emotion Recognition in Conversations",
    "Graph Neural Network",
    "Hypergraph",
    "Multimodal"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversations (ERC) facilitates a deeper understanding of the emotions conveyed by speakers in each utterance within a conversation. Recently, Graph Neural Networks (GNNs) have demonstrated their strengths in capturing data relationships, particularly in contextual information modeling and multimodal fusion. However, existing methods often struggle to fully capture the complex interactions between multiple modalities and conversational context, limiting their expressiveness. To overcome these limitations, we propose ConxGNN, a novel GNN-based framework designed to capture contextual information in conversations. ConxGNN features two key parallel modules: a multi-scale heterogeneous graph that captures the diverse effects of utterances on emotional changes, and a hypergraph that models the multivariate relationships among modalities and utterances. The outputs from these modules are integrated into a fusion layer, where a cross-modal attention mechanism is applied to produce a contextually enriched representation. Additionally, ConxGNN tackles the challenge of recognizing minority or semantically similar emotion classes by incorporating a re-weighting scheme into the loss functions. Experimental results on the IEMOCAP and MELD benchmark datasets demonstrate the effectiveness of our method, achieving state-of-the-art performance compared to previous baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion Recognition in Conversations (ERC) has gained significant attention as a research field for its broad practical applications. Traditional ERC approaches primarily focus on classifying emotions within individual utterances using conversational text  [1] ,  [2] . Leveraging the continuous nature of utterances in a conversation, some ERC methods model both the semantic features of utterances and the contextual information of conversations. Early approaches like ICON  [3] , CMN  [4] , and DialogueRNN  [5]  employ RNNs to model the conversation as a sequential flow of utterances. Meanwhile, Ada2I  [6]  tackled the challenge of modality imbalances and modality-fusion learning. However, these methods struggle to effectively balance long-and short-term dependencies for each utterance. In contrast, Graph Neural Networks (GNNs) have gained popularity due to their ability to efficiently aggregate information in conversational contexts. DialogueGCN  [7]  and RGAT  [8]  employed GNNs to model inter-utterance and inter-speaker relationships. DAG  [9]  leveraged the strengths of both traditional graph-based and recurrent neural networks. Recent advancements, such as CORECT  [10]  and M3GAT  [11] , integrated modality-specific representations with cross-modal interactions to create more comprehensive models. Additionally, approaches like graph contrastive learning  [12] ,  [13]  and knowledgeaware GNNs  [14]  further demonstrated the potential of GNNs to boost performance, setting a new benchmark for future ERC systems.\n\nHowever, current GNN-based approaches still face limitations in fully capturing conversational context. First, they rely on a fixed window size to model contextual information for all utterances, overlooking the variability in emotional shifts across a dialogue. This fixed setting struggles to account for the different emotional influences of each utterance, as the range of emotional impact varies throughout conversations. Second, traditional GNNs assume pairwise relationships between nodes, while in ERC, the emotional tone of one utterance can influence multiple subsequent utterances, which cannot be effectively captured through pairwise connections alone. Third, the integration of fine-grained multimodal features into emotional state prediction has not been thoroughly explored, limiting the potential performance improvements. Finally, current state-of-the-art (SOTA) methods overlook the issue of class imbalance, where majority classes significantly outnumber minority classes. This imbalance results in suboptimal performance, particularly when predicting emotions from minority classes.\n\nTo address these issues, we introduce ConxGNN, a novel framework designed to fully capture contextual information in conversations. At its core, ConxGNN consists of two parallel components: the Inception Graph Module (IGM) and the Hypergraph Module (HM). Recognizing the varying impact of utterances across conversations, and inspired by the use of multiple filter sizes in  [15] , IGM is built with multiple branches, each using a different window size to model interaction distances between utterances, enabling multiscale context modeling. Simultaneously, we capture multivariate relationships within conversations by constructing a hypergraph neural network. The outputs of these two modules are then passed through an attention mechanism, where attention weights are learned to complement emotional information across modalities. Additionally, to mitigate class imbalance, we introduce a re-weighting term to the loss functions, including InfoNCE and cross-entropy loss. Experiments on two popular ERC datasets demonstrate that ConxGNN achieves best performance compared to SOTA methods. The contribution of this paper can be summarized as follows:  (1)  We propose ConxGNN, which effectively models both multi-scale and multivariate interactions among modalities and utterances;  (2)  We design an attention mechanism to integrate fine-grained features from both graph modules into a unified representation; (3) We address class imbalance with a re-weighting scheme in the loss functions; (4) We conduct experiments on the IEMOCAP and MELD datasets, demonstrating that our proposed method achieves SOTA performance across both benchmarks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Proposed Approach",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Problem Formulation",
      "text": "Given a conversation consisting of L utterances U = {u1, u2, . . . , uL}, where each utterance ui is spoken by speaker si ∈ S and consists of multi-sensory data: textual (u t i ), visual (u v i ), and acoustic (u a i ) modalities: in which u τ i ∈ R dτ , τ ∈ {t, a, v} with dτ is the dimension size of raw modaltity features. The ERC task aims to predict the label for each utterance ui ∈ U from a set of C predefined emotional labels Y = {y1, y2, . . . , yC }.\n\nOur proposed architecture is illustrated in Figure  1A . In general, ConxGNN contains five main components: a unimodal encoder, an inception graph module, a hypergraph module, a fusion module, and an emotion classifier.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Unimodal Encoder",
      "text": "Following  [10] , we first capture the utterance-level features of each modality. Specifically, we utilize a Transformer encoder  [16]  for textual modality and a fully-connected network for visual and acoustic modalities as follows:\n\nwhere\n\nAdditonally, considering the impact of speakers information in a conversation, we incorporate the embedding of speakers' identity and produce the respective latent representations si = Embedding(S), in which si ∈ R d h . We then add speaker embedding to obtain speaker-and context-aware unimodal representation h τ i ∈ R d h at the i-th conversation turn:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "C. Inception Graph Module (Igm)",
      "text": "1) Graph Construction: We define G(VG, RG, EG) as the multimodal graph constructed from conversations.\n\nNodes. Each utterance is modeled as three distinct nodes, corresponding to the representations h t i , h v i and h a i , resulting in a total of |V| = 3L nodes.\n\nRelations. To capture both inter-and intra-dependencies among modalities, we define two types of relations: Rinter denotes the connections between the three modalities within the same utterance, while Rintra represents the connections between utterances of the same modality within a given time window. To capture this temporal aspect, we introduce a sliding window [p, f ] to control the number of past and future utterances that connected to the current node u τ i . Therefore, the two groups of relations can be expressed as follows:\n\nEdges. The edge (h τ i , h ν j , rij) ∈ EG; τ, ν ∈ {t, a, v} represents the interaction between h τ i and h ν j with the relation type rij ∈ RG. Following  [17] , we utilize the angular similarity to represent the edge weight between two nodes: Aij = 1-arccos(sim(h τ i , h ν j ))/π, where sim(•) is cosine similarity function.\n\n2) Inception Graph Module: The range of emotional influence varies between utterances across different conversations. In contexts with significant fluctuations, shorter interaction distances exert a stronger emotional impact, whereas in more stable conversations, longer distances also contribute to the target utterance's emotional tone. Consequently, determining the optimal sliding window [p, f ] for graph construction poses a significant challenge. Drawing inspiration from the usage of multiple filter sizes as proposed in  [15] , we design multiple graph structures corresponding to n distinct window slides P = {[p1, f1], . . . , [pn, fn]}. Each graph utilizes a different slide, enabling the parallel learning of multi-scale features, which are subsequently combined to form a comprehensive and rich representation. Figure  1A  illustrates the module, while Figure  1B  depicts the graph structure of an individual block.\n\n3) Graph Learning: With the objective of leveraging the variations of heterogeneous interactions between utterances and modalities as well as the structure diversity of multiple graph blocks, we employ k-dimensional GNNs (k-GNNs)  [18] . Specifically, the representation for the i-th utterance at layer ℓ (0 < ℓ ≤ Ninc) is inferred as follows:\n\nwhere g τ i,(0) = h τ i ; Nr(i) is the set of the node i's neighbors with the relation r ∈ R and |N (i)| = r∈R |Nr(i)|; W r 0 , W r 1 ∈ R d h ×d h are learnable parameters. After ℓ = Ninc iterations, we feed the output g τ i = g τ i,(N inc ) into a Graph Transformer model  [19]  to further extract rich representations. The representation is then transformed into:\n\nwhere W2, W3 ∈ R d h ×d h are learnable parameters, and || H h=1 represents the concatenation of outputs from H attention heads. The attention coefficient α τ ij is determined by:\n\nwhere W4, W5 ∈ R d h ×d h are learnable parameters. Finally, we aggregate the representation across every branch of the module, to create a unified representation that capable of capturing multi-scale interactions among modalities and utterances. As a result, we obtain new representation vectors:\n\nwhere\n\n1) Graph Construction: We construct a hypergraph H = (VH, EH, ω) from a sequence of L utterances. Similar to the ones in G, each node v ∈ VH (|VH| = 3L) represents a unimodal utterance. We intialize the node embeddings {q t i,(0) , q a i,(0) , q v i,(0) } with encoded representations {h t i , h a i , h v i } respectively. Different from G, every hyperedges e ∈ EH (|EH| = 3 + L) are designed to capture the combined effect of modalities and conversational context, connecting every nodes within the same modality and across different modalities in a same utterance. In this fashion, the constructed hypergraph is able to capture high-order and multivariate messages that are beyond pairwise formulation. Additionally, we introduce learnable edge weight ω(e) for every hyperedge e, enhancing the representation of complex multivariate relationships.\n\n2) Graph Learning: We employ hypergraph convolution operation  [20]  to propagate multivariate embeddings. Mathematically,\n\nwhere\n\nare the node degree matrices and hyperedge degree matrix, respectively. After completing Nhyp iterations, the final iteration's outputs are obtained as the multivariate representations:\n\nin which q τ i = q τ i,(N hyp ) .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "E. Fusion Module And Classifier",
      "text": "After utilizing the two mentioned modules, we combine their outputs by concatenating them to form the final feature representation\n\n, where W6 ∈ R da×2d h and b6 ∈ R da are learnable parameters. Given that the textual modality carries more sentiment information  [10] , we propose a cross-modal attention mechanism to align the other two modalities with the textual features, resulting in fused representations for the text-vision and text-audio modalities. Specifically, we define the cross-modal attention mechanism as follows:\n\nwhere τ ∈ {a, v}. WQ, WK , WV ∈ R da×da are the query, key, and value weights, respectively. Then, the two fused results were added to the textual features to form a new textual representation after the crossmodal attention:\n\nWe then aggregate the feature representations of the three modalities to create a unified, low-dimensional feature representation using a fully-connected layer and ReLU function:\n\nFinally, zi is then fed to a classifier, which is a fully-connected layer, to predict the emotion label yi for the utterance ui:\n\nwhere W7, b7 are trainable parameters.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "F. Training Objectives",
      "text": "To address the challenge of classifying minority classes during training, it is crucial to mitigate the impact of imbalanced class distributions. Inspired from the approach in  [21] , we introduce a reweighting strategy that uses the effective number of samples for each class to adjust the loss, resulting in a class-balanced loss. Specifically, for a sample from class ci with ni total samples, a weighting factor wc(i) = (1 -β)/(1 -β n i ) is applied to the loss function, where β ∈ [0, 1) is a hyperparameter. Given a batch of N dialogues, where the i-th dialogue contains Li utterances, the class-balanced (CB) training objectives are defined as follows:\n\nFocal Contrastive Loss. To address the challenge of classifying minority classes, we introduce a novel loss function called Class-Balanced Focal Contrastive (CBFC) loss, which extends the focal contrastive loss  [22]  by incorporating a class-weight term. This loss aligns pairs with the same emotional labels and maximizes inter-class distances by pushing apart pairs with different labels. The CBFC loss is formulated as follows:\n\nexp(z ⊤ i,j z i,s /τ ) , in which Pi,j, Ai,j denote the anchor's positive and full pair sets.\n\nCross-Entropy Loss. We adopt a weighted Cross-Entropy (CE) loss to measure the difference between predicted probabilities and true labels:\n\nwhere y c j is the one-hot vector of the true label. Full Loss. We linearly combine focal contrastive loss and Crossentropy loss as follows:\n\nwhere µ ∈ (0, 1] is a tunable hyperparameter.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Experiments And Analysis",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dataset",
      "text": "We conducted experiments on two multimodal datasets: IEMOCAP  [23]  and MELD  [24] . The IEMOCAP dataset consists of 12 hours of two-way conversations involving 10 speakers, comprising a total of 7,433 utterances and 151 dialogues, categorized into six emotion classes: happy, sad, neutral, angry, excited, and frustrated. The MELD dataset includes 1,433 conversations and 13,708 utterances, each labeled with one of seven emotion categories: angry, disgusted, fearful, happy, sad, surprised, and neutral. To ensure a fair comparison, we utilized the predefined train/validation/test splits provided by each dataset. As IEMOCAP lacks a validation set, we followed the split used in recent work  [10]  for training and validating all methods.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Experimental Setups",
      "text": "For both datasets, we use the Adam optimizer  [25]  with a learning rate of 0.0004 over 40 epochs. The number of layers in IGM and HM for both datasets is set to 2 and 4, respectively. We set β = 0.999 and µ = 0.8 across both datasets. The IGM architecture comprises 3 GNN branches, with window sizes set to [  (10, 9) ,  (5, 3) ,  (3, 2) ] for IEMOCAP and [  (11, 11) ,  (7, 4) ,  (6, 4) ] for MELD. Hyperparameters, including window sizes and the number of layers in each module, are set using the validation set. All reported results represent the mean of five independent runs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Experimental Results",
      "text": "Table  I  presents a performance comparison between our proposed method and other SOTA approaches. The results demonstrate that ConxGNN achieves superior performance across both datasets. Specifically, ConxGNN surpasses the previous best method, CORECT  [10] , by 2.32% in accuracy and 2.25% in weighted-F1 score on the IEMOCAP dataset. On the MELD dataset, our model shows slightly improvements of 0.19% in accuracy and 0.69% in weighted-F1 score compared to MM-DFN  [27]  and M 3 Net  [21] , respectively. These findings empirically validate the effectiveness of our proposed architecture.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Ablation Study 1) Components Analysis:",
      "text": "We conduct ablation study to evaluate the contribution of each module within our framework. Table  II  represents the model's performance when specific components are removed. Of the four modules, we can see that IGM has the greatest impact, as its removal leads to a significant performance decline across both datasets, with a drop in (accuracy, weighted-F1) of (27.8%, 42.96%) on IEMOCAP and (15.44%, 25.48%) on  MELD. The second key module, HM, also plays a critical role, especially on IEMOCAP, where its absence results in approximately a 4.5% reduction in performance, though its effect on MELD is minimal, causing around 1% degradation. The removal of other components, such as the cross-modal attention mechanism and the re-weighting scheme, also results in slight performance reductions. These findings collectively confirm the importance and effectiveness of each component in our architecture.\n\n2) Impact of Multi-scale Extractor: To highlight the significance of the IGM, we conduct an ablation study by varying the number of inception graph blocks/branches within the module. Table  III  presents the best average results for each number of blocks. In the 2-block analysis, we explore different combinations of three sliding windows to evaluate performance. The results are fairly consistent for the same number of blocks. Additionally, performance improves steadily as more blocks are added, with an approximate increase of 1% per block. Compared to the single-scale approach (i.e., a single block), our multi-scale strategy leads to notable performance gains, with accuracy increasing by 3.15% on IEMOCAP and 2.83% on MELD, and weighted F1 improving by 3.09% on IEMOCAP and 2.83% on MELD. These findings underscore the importance of the proposed IGM, which captures multi-scale interactions between modalities and utterances.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "We propose ConxGNN, a novel framework specifically designed for contextual modeling in conversations for the ERC task. ConxGNN is composed of two primary modules: the IGM, which extracts multiscale relationships using varying interactive window sizes, and HM, which captures the multivariate relationships among utterances and modalities. These modules operate in parallel, and their outputs are combined using an attention mechanism, resulting in contextually enriched information. Additionally, ConxGNN addresses the issue of class imbalance by incorporating a re-weighting scheme into the loss functions. Experimental results on the IEMOCAP and MELD datasets demonstrate that our approach achieves SOTA performance, highlighting its efficacy and advantages.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Detailed architecture of (A) the proposed ConxGNN, (B) Inception Graph Block, and (C) HyperBlock.",
      "page": 2
    },
    {
      "caption": "Figure 1: A. In general,",
      "page": 2
    },
    {
      "caption": "Figure 1: A illustrates the module, while Figure 1B depicts the graph",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "Abstract—Emotion Recognition in Conversations\n(ERC)\nfacilitates a"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "deeper understanding\nof\nthe\nemotions\nconveyed by\nspeakers\nin each"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "utterance within\na\nconversation. Recently, Graph Neural Networks"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "(GNNs) have demonstrated their\nstrengths\nin capturing data relation-"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "ships, particularly in contextual\ninformation modeling and multimodal"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "fusion. However,\nexisting methods\noften struggle\nto\nfully\ncapture\nthe"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "complex\ninteractions\nbetween multiple modalities\nand\nconversational"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "context,\nlimiting their expressiveness. To overcome these limitations, we"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "propose ConxGNN, a novel GNN-based framework designed to capture"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "contextual\ninformation\nin\nconversations. ConxGNN features\ntwo\nkey"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "parallel modules: a multi-scale heterogeneous graph that\ncaptures\nthe"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "diverse effects of utterances on emotional changes, and a hypergraph that"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "models\nthe multivariate relationships among modalities and utterances."
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "The outputs from these modules are integrated into a fusion layer, where"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "a cross-modal attention mechanism is applied to produce a contextually"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "enriched representation. Additionally, ConxGNN tackles\nthe\nchallenge"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "of\nrecognizing minority\nor\nsemantically\nsimilar\nemotion\nclasses\nby"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "incorporating a re-weighting scheme into the loss functions. Experimental"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "results on the IEMOCAP and MELD benchmark datasets demonstrate"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "the effectiveness of our method, achieving state-of-the-art performance"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "compared to previous baselines."
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "Index Terms—Emotion Recognition in Conversations, Graph Neural"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "Network, Hypergraph, Multimodal."
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "I.\nINTRODUCTION"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "Emotion Recognition in Conversations\n(ERC) has gained signifi-"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "cant attention as a research field for\nits broad practical applications."
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "Traditional ERC approaches primarily focus on classifying emotions"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "within individual utterances using conversational\ntext [1], [2]. Lever-"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "aging the\ncontinuous nature of utterances\nin a\nconversation,\nsome"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "ERC methods model both the\nsemantic\nfeatures of utterances\nand"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "the\ncontextual\ninformation of\nconversations. Early approaches\nlike"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "ICON [3], CMN [4], and DialogueRNN [5] employ RNNs to model"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "the conversation as a sequential flow of utterances. Meanwhile, Ada2I"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "[6] tackled the challenge of modality imbalances and modality-fusion"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "learning. However,\nthese methods\nstruggle\nto\neffectively\nbalance"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "long-\nand short-term dependencies\nfor\neach utterance.\nIn contrast,"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "Graph Neural Networks\n(GNNs)\nhave\ngained\npopularity\ndue\nto"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "their\nability\nto\nefficiently\naggregate\ninformation\nin\nconversational"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "contexts. DialogueGCN [7] and RGAT [8] employed GNNs to model"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "inter-utterance\nand\ninter-speaker\nrelationships. DAG [9]\nleveraged"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "the\nstrengths\nof\nboth\ntraditional\ngraph-based\nand\nrecurrent\nneural"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "networks. Recent advancements, such as CORECT [10] and M3GAT"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "[11],\nintegrated modality-specific\nrepresentations with cross-modal"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "interactions\nto\ncreate more\ncomprehensive models. Additionally,"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "approaches like graph contrastive learning [12], [13] and knowledge-"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "aware GNNs\n[14]\nfurther demonstrated the potential of GNNs\nto"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "boost performance, setting a new benchmark for future ERC systems."
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "However, current GNN-based approaches\nstill\nface limitations\nin"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "fully\ncapturing\nconversational\ncontext. First,\nthey\nrely\non\na fixed"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "window size\nto model\ncontextual\ninformation\nfor\nall\nutterances,"
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": ""
        },
        {
          "1FPT Software AI Center, Hanoi, Vietnam": "∗ These authors contributed equally to this work."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "in which uτ\nis the dimension size of\ni ∈ Rdτ , τ ∈ {t, a, v} with dτ",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "modality within a given time window. To capture this temporal aspect,"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "raw modaltity features. The ERC task aims\nto predict\nthe label\nfor",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "we introduce a sliding window [p, f ] to control the number of past and"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "labels\neach utterance ui ∈ U from a set of C predefined emotional",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "future utterances\nthat connected to the current node uτ\ni . Therefore,"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "Y = {y1, y2, . . . , yC }.",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "the two groups of\nrelations can be expressed as follows:"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "Our proposed architecture is\nillustrated in Figure 1A.\nIn general,",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "(5)\nRinter = (cid:8)(hτ\ni , hν\ni )| τ, ν ∈ {t, a, v}(cid:9),"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "ConxGNN contains five main components: a unimodal encoder, an",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "past"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "inception graph module, a hypergraph module, a fusion module, and",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "(cid:40)(cid:8)(hτ\n−\n−\n→ hτ\nj )| i − p < j < i, τ ∈ {t, a, v}(cid:9)"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "an emotion classifier.",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "(6)\nRintra ="
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "future−−−→ hτ\n(cid:8)(hτ\nj )| i < j < i + f, τ ∈ {t, a, v}(cid:9)"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "B. Unimodal Encoder",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "Edges.\nThe edge (hτ\nj , rij) ∈ EG; τ, ν ∈ {t, a, v} represents\ni , hν"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "Following\n[10], we first\ncapture\nthe\nutterance-level\nfeatures\nof",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "the interaction between hτ\nand hν\nj with the relation type rij ∈ RG."
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "each modality. Specifically, we utilize\na Transformer\nencoder\n[16]",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "Following\n[17], we\nutilize\nthe\nangular\nsimilarity\nto\nrepresent\nthe"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "for\ntextual modality and a\nfully-connected network for visual\nand",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "edge weight between two nodes: Aij = 1−arccos(sim(hτ\nj ))/π,\ni , hν"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "acoustic modalities as follows:",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "where sim(·) is cosine similarity function."
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "xt\n(2)\ni = Transformer(ut\ni, θt\ntrans),",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "2)\nInception Graph Module:\nThe\nrange of\nemotional\ninfluence"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "xτ\nτ ∈ {a, v},\n(3)\ni = Wτ uτ\ni + bτ ,",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "varies between utterances across different conversations.\nIn contexts"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "with\nsignificant\nfluctuations,\nshorter\ninteraction\ndistances\nexert\na"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "where θt\ntrans, Wτ ∈ Rdh×dτ , bτ ∈ Rdh are trainable parameters and",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "stronger\nemotional\nimpact, whereas\nin more\nstable\nconversations,"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "xt\nspeakers\ni ∈ Rdh . Additonally, considering the impact of\ni , xa\ni, xv",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "longer distances also contribute to the target utterance’s emotional"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "information\nin\na\nconversation, we\nincorporate\nthe\nembedding\nof",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "tone. Consequently, determining the optimal sliding window [p, f ] for"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "speakers’\nidentity and produce the respective latent\nrepresentations",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "graph construction poses a significant challenge. Drawing inspiration"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "si = Embedding(S),\nin which si ∈ Rdh . We then add speaker em-",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "from the usage of multiple filter sizes as proposed in [15], we design"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "bedding to obtain speaker- and context-aware unimodal representation",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "multiple graph structures corresponding to n distinct window slides"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "hτ\nat\nthe i-th conversation turn:\ni ∈ Rdh",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "slide,\nP = {[p1, f1], . . . , [pn, fn]}. Each graph utilizes a different"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "hτ",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "enabling the parallel\nlearning of multi-scale features, which are sub-"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "τ ∈ {t, a, v}.\n(4)\ni = si + xτ\ni ,",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "sequently combined to form a comprehensive and rich representation."
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "C.\nInception Graph Module (IGM)",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "Figure 1A illustrates the module, while Figure 1B depicts the graph"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "the multi-\n1) Graph Construction: We define G(VG, RG, EG) as",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "structure of an individual block."
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "modal graph constructed from conversations.",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "3) Graph Learning: With the objective of leveraging the variations"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "Nodes.\nEach utterance is modeled as three distinct nodes, corre-",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "of heterogeneous\ninteractions between utterances and modalities as"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "sponding to the representations ht\nand ha\ni, hv\ni , resulting in a total of",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "well as the structure diversity of multiple graph blocks, we employ"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "|V| = 3L nodes.",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "k-dimensional GNNs (k-GNNs) [18]. Specifically,\nthe representation"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "Relations.\nTo capture both inter- and intra-dependencies among",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "for the i-th utterance at\nlayer ℓ (0 < ℓ ≤ Ninc) is inferred as follows:"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "modalities, we\ndefine\ntwo\ntypes\nof\ndenotes\nthe\nrelations: Rinter",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "(cid:17)\n(cid:16)\n1\n(cid:88)"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "connections between the three modalities within the same utterance,",
          "(B)\nInception Graph Block, and (C) HyperBlock.": ""
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "(cid:88) r\ngτ\n,\nWr\n(7)\nAjigν\n0gτ\ni,(ℓ) =\nj,(ℓ−1)\ni,(ℓ−1)+Wr"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "|N (i)|"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "while Rintra represents the connections between utterances of the same",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "∈R"
        },
        {
          "Fig. 1. Detailed architecture of\n(A)\nthe proposed ConxGNN,": "",
          "(B)\nInception Graph Block, and (C) HyperBlock.": "j∈Nr (i)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "relation r ∈ R and |N (i)| = (cid:80)\n1 ∈ Rdh×dh",
          "modalities. Specifically, we define the cross-modal attention mecha-": "nism as follows:"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "r∈R |Nr(i)|; Wr",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "iterations, we feed the output",
          "modalities. Specifically, we define the cross-modal attention mecha-": "(cid:19)"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "are learnable parameters. After ℓ = Ninc",
          "modalities. Specifically, we define the cross-modal attention mecha-": "(cid:18) (WQf τ\ni )⊤(WK f t\ni )"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "√\nCAτ→t\n= Softmax\n(13)\nWV f t\ni ,"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "gτ\ninto a Graph Transformer model [19] to further extract\ni = gτ",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "i,(Ninc)",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "dh"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "rich representations. The representation is then transformed into:",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "where τ ∈ {a, v}. WQ, WK , WV ∈ Rda×da are the query, key, and"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "(cid:88)",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "oτ\nατ\n(cid:3),\n(8)\n(cid:2)W2gτ\ni = ||H\ni +\nijW3gτ",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "h=1",
          "modalities. Specifically, we define the cross-modal attention mecha-": "value weights,\nrespectively. Then,\nthe two fused results were added"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "j∈N (i)",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "to the textual\nfeatures to form a new textual\nrepresentation after\nthe"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "∈ Rdh×dh\n||H\nare\nlearnable\nparameters,\nand\nwhere W2, W3",
          "modalities. Specifically, we define the cross-modal attention mecha-": "crossmodal attention:"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "h=1",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "represents the concatenation of outputs from H attention heads. The",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "f t\n+ CAa→t\n(14)\ni = f t\ni + CAv→t"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "attention coefficient ατ\nis determined by:\nij",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "We then aggregate the feature representations of the three modali-"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "(cid:19)",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "(cid:18) (W4gτ\ni )⊤(W5gτ\nj )",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "√\n,\nατ\n(9)\nij = softmax",
          "modalities. Specifically, we define the cross-modal attention mecha-": "ties to create a unified,\nlow-dimensional\nfeature representation using"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "dh",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "a fully-connected layer and ReLU function:"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "are\nlearnable parameters. Finally, we\nwhere W4, W5 ∈ Rdh×dh",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "|| f a\n|| f v\n(15)\nzi = ReLU(Wz[ˆf t\n] + bz) ∈ Rdz"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "aggregate the representation across every branch of\nthe module,\nto",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "create a unified representation that capable of capturing multi-scale",
          "modalities. Specifically, we define the cross-modal attention mecha-": "is\nthen fed to a classifier, which is a fully-connected\nFinally, zi"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "interactions among modalities and utterances. As a result, we obtain",
          "modalities. Specifically, we define the cross-modal attention mecha-": "layer,\nto predict\nfor\nthe emotion label yi\nthe utterance ui:"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "new representation vectors:",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "(16)\npi = Softmax(W7zi + b7)"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "Pτ = [pτ\nτ ∈ {t, a, v},\n(10)\n1 , pτ\n2 , . . . , pτ\nL],",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "(17)\nyi = arg max(pi)"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "(cid:3)",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "(cid:80)n\n(cid:2)oτ\nwhere pτ\nand pτ\ni = 1\ni ∈ Rdh .",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "j=1\nn",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "j",
          "modalities. Specifically, we define the cross-modal attention mecha-": "where W7, b7 are trainable parameters."
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "D. Hypergraph Module (HM)",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "F\n. Training Objectives"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "1) Graph\n=\nConstruction: We\nconstruct\na\nhypergraph H",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "To address\nthe\nchallenge of\nclassifying minority classes during"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "from a sequence of L utterances. Similar\nto the ones\n(VH, EH, ω)",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "training,\nit\nis\ncrucial\nto mitigate\nthe\nimpact\nof\nimbalanced\nclass"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "in G,\neach\nnode\nrepresents\na\nunimodal\nv ∈ VH (|VH| = 3L)",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "distributions.\nInspired from the approach in [21], we introduce a re-"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "utterance. We\nintialize\nthe node\nembeddings {qt\ni,(0), qa\ni,(0), qv\ni,(0)}",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "weighting strategy that uses the effective number of samples for each"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "{ht\nwith\nencoded\nrepresentations\nrespectively. Different\ni, ha\ni , hv\ni }",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "class to adjust the loss, resulting in a class-balanced loss. Specifically,"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "from G, every hyperedges e ∈ EH (|EH| = 3 + L) are designed to",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "total samples, a weighting factor\nfor a sample from class ci with ni"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "capture the combined effect of modalities and conversational context,",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "is applied to the loss\nfunction, where\nwc(i) = (1 − β)/(1 − βni )"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "connecting every nodes within the same modality and across different",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "β\n∈ [0, 1)\nis\na\nhyperparameter. Given\na\nbatch\nof N dialogues,"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "modalities\nin\na\nsame\nutterance.\nIn\nthis\nfashion,\nthe\nconstructed",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "where\nthe\ni-th dialogue\nutterances,\nthe\nclass-balanced\ncontains Li"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "hypergraph is able to capture high-order and multivariate messages",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "(CB)\ntraining objectives are defined as follows:"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "that\nare\nbeyond\npairwise\nformulation. Additionally, we\nintroduce",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "Focal Contrastive Loss.\nTo address the challenge of classifying"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "learnable\nedge weight ω(e)\nfor\nevery hyperedge e,\nenhancing the",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "minority classes, we\nintroduce\na novel\nloss\nfunction called Class-"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "representation of complex multivariate relationships.",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "Balanced Focal Contrastive\n(CBFC)\nloss, which extends\nthe\nfocal"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "2) Graph Learning: We employ hypergraph convolution operation",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "contrastive loss [22] by incorporating a class-weight\nterm. This loss"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "[20]\nto propagate multivariate embeddings. Mathematically,",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "aligns pairs with the same emotional labels and maximizes inter-class"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "(11)\nQ(l) = σ(D−1HWeB−1H⊤Q(l−1)Θ),",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "distances by pushing apart pairs with different\nlabels. The CBFC loss"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "is formulated as follows:"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "∈ {t, a, v}(cid:9) ∈ R|VH|×dh\nwhere Q(l) = (cid:8)qτ\ni,(l)| i ∈ [1, L], τ",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "σ\nis\nthe\ninput\nat\nlayer\nl.\nis\na\nnon-linear\nactivation\nfunction.",
          "modalities. Specifically, we define the cross-modal attention mecha-": "1\nwc(j)\n(cid:88)\nLi(cid:88)"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "N(cid:88) i\n(1 − t(i)\nLCBFC = −\nj,k) log t(i)\nj,k,"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "H ∈\n{0, 1}|VH|×|EH|\n=\nrepresents\nthe\nincidence matrix, We",
          "modalities. Specifically, we define the cross-modal attention mecha-": "(cid:80)N"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "|Pi,j|"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "i=1 Li\n=1\nj=1\nzi,k∈Pi,j"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "is\nthe\nlearnable\ndiagonal\nhyperedge\ndiag(ω(e1), . . . , ω(e|EH|))",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "(18)"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "weight matrix,\nand D ∈ R|VH|×|VH|\nand B ∈ R|EH|×|EH|\nare",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "exp(z⊤\ni,j zi,k/τ )"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "where t(i)\n,\ndenote\nin which Pi,j, Ai,j\nj,k ="
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "the node degree matrices and hyperedge degree matrix,\nrespectively.",
          "modalities. Specifically, we define the cross-modal attention mecha-": "(cid:80)\nexp(z⊤"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "i,j zi,s/τ )\nzi,s∈Ai,j"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "After\niterations,\nthe final\niteration’s\noutputs\nare\ncompleting Nhyp",
          "modalities. Specifically, we define the cross-modal attention mecha-": "the anchor’s positive and full pair sets."
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "obtained as the multivariate representations:",
          "modalities. Specifically, we define the cross-modal attention mecha-": "Cross-Entropy Loss.\nWe adopt a weighted Cross-Entropy (CE)"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "loss\nto measure\nthe difference between predicted probabilities\nand"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "Qτ = [qτ\nτ ∈ {t, a, v},\n(12)\n1 , qτ\n2 , . . . , qτ\nL],",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "true labels:"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "in which qτ\ni = qτ",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "i,(Nhyp).",
          "modalities. Specifically, we define the cross-modal attention mecha-": "|C|"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "1\nLi(cid:88)"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "N(cid:88) i\nyc\n(19)\nwc(j)\nLCBCE = −\nj log pc\nj,"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "E. Fusion Module and Classifier",
          "modalities. Specifically, we define the cross-modal attention mecha-": "(cid:88) c\n(cid:80)N"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "i=1 Li\n=1\n=1\nj=1"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "After\nutilizing\nthe\ntwo mentioned modules, we\ncombine\ntheir",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "where yc\nis the one-hot vector of\nthe true label."
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "outputs by concatenating them to form the final feature representation",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "Full Loss.\nWe linearly combine focal contrastive loss and Cross-"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "f τ\n|| qτ\nand b6 ∈ Rda\ni = W6[pτ\ni ] + b6, where W6 ∈ Rda×2dh",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "entropy loss as follows:"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "are\nlearnable\nparameters. Given\nthat\nthe\ntextual modality\ncarries",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "more sentiment\ninformation [10], we propose a cross-modal attention",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "",
          "modalities. Specifically, we define the cross-modal attention mecha-": "(20)\nL = LCBCE + µLCBFC,"
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "mechanism to align the other two modalities with the textual features,",
          "modalities. Specifically, we define the cross-modal attention mecha-": ""
        },
        {
          "where gτ\ni ; Nr(i) is the set of the node i’s neighbors with the\ni,(0) = hτ": "resulting in fused representations\nfor\nthe text-vision and text-audio",
          "modalities. Specifically, we define the cross-modal attention mecha-": "where µ ∈ (0, 1]\nis a tunable hyperparameter."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": "",
          "TABLE II": ""
        },
        {
          "TABLE I": "Network",
          "TABLE II": "IEMOCAP"
        },
        {
          "TABLE I": "",
          "TABLE II": ""
        },
        {
          "TABLE I": "GNN-based",
          "TABLE II": "w-F1 (%)"
        },
        {
          "TABLE I": "Non-GNN",
          "TABLE II": ""
        },
        {
          "TABLE I": "",
          "TABLE II": "68.64"
        },
        {
          "TABLE I": "Non-GNN",
          "TABLE II": ""
        },
        {
          "TABLE I": "",
          "TABLE II": "25.68"
        },
        {
          "TABLE I": "GNN-based",
          "TABLE II": ""
        },
        {
          "TABLE I": "",
          "TABLE II": "63.92"
        },
        {
          "TABLE I": "GNN-based",
          "TABLE II": ""
        },
        {
          "TABLE I": "",
          "TABLE II": "64.31"
        },
        {
          "TABLE I": "GNN-based",
          "TABLE II": "63.90"
        },
        {
          "TABLE I": "GNN-based",
          "TABLE II": ""
        },
        {
          "TABLE I": "Non-GNN",
          "TABLE II": ""
        },
        {
          "TABLE I": "",
          "TABLE II": "TABLE III"
        },
        {
          "TABLE I": "GNN-based",
          "TABLE II": ""
        },
        {
          "TABLE I": "",
          "TABLE II": ""
        },
        {
          "TABLE I": "GNN-based",
          "TABLE II": ""
        },
        {
          "TABLE I": "GNN-based",
          "TABLE II": ""
        },
        {
          "TABLE I": "",
          "TABLE II": ""
        },
        {
          "TABLE I": "",
          "TABLE II": "w-F1 (%)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "M3Net\n[28]\nGNN-based\n65.75\n65.00"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "66.28\n65.69\nConxGNN (ours)\nGNN-based"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "III. EXPERIMENTS AND ANALYSIS"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "A. Dataset"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "We conducted experiments on two multimodal datasets: IEMOCAP"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "[23] and MELD [24]. The IEMOCAP dataset consists of 12 hours"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "of\ntwo-way conversations involving 10 speakers, comprising a total"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "of 7,433 utterances and 151 dialogues, categorized into six emotion"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "classes: happy, sad, neutral, angry, excited, and frustrated. The MELD"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "dataset\nincludes 1,433 conversations and 13,708 utterances, each la-"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "beled with one of seven emotion categories: angry, disgusted, fearful,"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "happy, sad, surprised, and neutral. To ensure a fair comparison, we"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "utilized the predefined train/validation/test\nsplits provided by each"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "dataset. As IEMOCAP lacks a validation set, we followed the split"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "used in recent work [10]\nfor\ntraining and validating all methods."
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "B. Experimental Setups"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "For both datasets, we use the Adam optimizer [25] with a learning"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "rate of 0.0004 over 40 epochs. The number of layers in IGM and HM"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "for both datasets\nis\nset\nto 2 and 4,\nrespectively. We set β = 0.999"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "and µ = 0.8 across both datasets. The IGM architecture comprises 3"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "GNN branches, with window sizes set\nto [(10, 9),\n(5, 3),\n(3, 2)]\nfor"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "IEMOCAP and [(11, 11), (7, 4), (6, 4)] for MELD. Hyperparameters,"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "including window sizes and the number of layers in each module, are"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "set using the validation set. All\nreported results\nrepresent\nthe mean"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "of five independent\nruns."
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "C. Experimental Results"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "Table\nI\npresents\na\nperformance\ncomparison\nbetween\nour\npro-"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "posed method\nand\nother\nSOTA approaches. The\nresults\ndemon-"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "strate\nthat ConxGNN achieves\nsuperior\nperformance\nacross\nboth"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "datasets. Specifically, ConxGNN surpasses the previous best method,"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "CORECT [10], by 2.32% in accuracy and 2.25% in weighted-F1"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "score on the IEMOCAP dataset. On the MELD dataset, our model"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "shows\nslightly improvements of 0.19% in accuracy and 0.69% in"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "weighted-F1\nscore\ncompared\nto MM-DFN [27]\nand M3Net\n[21],"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "respectively. These findings empirically validate the effectiveness of"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "our proposed architecture."
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "D. Ablation Study"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": ""
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "1) Components Analysis: We conduct ablation study to evaluate"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "the\ncontribution\nof\neach module within\nour\nframework. Table\nII"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "represents\nthe model’s performance when specific\ncomponents\nare"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "removed. Of\nthe\nfour modules, we\ncan\nsee\nthat\nIGM has\nthe"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "greatest\nimpact,\nas\nits\nremoval\nleads\nto a\nsignificant performance"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "decline\nacross\nboth\ndatasets, with\na\ndrop\nin\n(accuracy, weighted-"
        },
        {
          "MELD\nMM-DFN [27]\nGNN-based\n66.09\n64.16": "F1) of\n(27.8%, 42.96%) on IEMOCAP and (15.44%, 25.48%) on"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "ciation for Computational Linguistics and the 11th International Joint"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "Conference on Natural Language Processing (Volume 1: Long Papers),"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "Zong et al., Eds., Online, Aug. 2021, pp. 1551–1560, Association for"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "Computational Linguistics."
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "[10] Nguyen et al.,\n“Conversation understanding using relational\ntemporal"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "graph neural networks with auxiliary cross-modality interaction,”\nin"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "Proceedings of\nthe 2023 Conference on Empirical Methods in Natural"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "Language Processing, Bouamor et al., Eds., Singapore, Dec. 2023, pp."
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "15154–15167, Association for Computational Linguistics."
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "[11] Zhang\net\nal.,\n“M3gat: A multi-modal, multi-task\ninteractive\ngraph"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "attention\nnetwork\nfor\nconversational\nsentiment\nanalysis\nand\nemotion"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "recognition,” ACM Trans.\nInf. Syst., vol. 42, no. 1, Aug. 2023."
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "[12] Li et al.,\n“Joyful: Joint modality fusion and graph contrastive learning"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "for multimoda emotion recognition,” in Proceedings of the 2023 Confer-"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "ence on Empirical Methods in Natural Language Processing, Bouamor"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "et\nal., Eds., Singapore, Dec. 2023, pp. 16051–16069, Association for"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "Computational Linguistics."
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "[13] Yi et al., “Multimodal fusion via hypergraph autoencoder and contrastive"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "learning for\nemotion recognition in conversation,”\nin Proceedings of"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "the\n32nd ACM International Conference\non Multimedia, New York,"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "NY, USA, 2024, MM ’24, p. 4341–4348, Association for Computing"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "Machinery."
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "[14] Zhang\net\nal.,\n“Knowledge-aware\ngraph\nconvolutional\nnetwork with"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "utterance-specific window search\nfor\nemotion\nrecognition\nin\nconver-"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "sations,”\nin ICASSP 2023 - 2023 IEEE International Conference on"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "Acoustics, Speech and Signal Processing (ICASSP), 2023, pp. 1–5."
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "the\n[15]\nSzegedy et al., “Going deeper with convolutions,” in Proceedings of"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "June 2015."
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "in Neural\n[16] Vaswani\net\nal.,\n“Attention is\nall you need,”\nin Advances"
        },
        {
          "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of": "Information Processing Systems, Guyon et al., Eds. 2017, vol. 30, Curran"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "sentations with word embeddings for text classification,” in Proceedings"
        },
        {
          "REFERENCES": "[1] Hu et al.,\n“DialogueCRN: Contextual\nreasoning networks for emotion",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "of the Twelfth Workshop on Graph-Based Methods for Natural Language"
        },
        {
          "REFERENCES": "of\nthe\n59th Annual\nrecognition\nin\nconversations,”\nin Proceedings",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "Processing (TextGraphs-12), Glavaˇs et al., Eds., New Orleans, Louisiana,"
        },
        {
          "REFERENCES": "Meeting of\nthe Association for Computational Linguistics and the 11th",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "USA, June 2018, pp. 49–58, Association for Computational Linguistics."
        },
        {
          "REFERENCES": "International Joint Conference on Natural Language Processing (Volume",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "[18] Morris\net\nal.,\n“Weisfeiler\nand\nleman go\nneural: Higher-order\ngraph"
        },
        {
          "REFERENCES": "1: Long Papers), Zong et al., Eds., Online, Aug. 2021, pp. 7042–7052,",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "Proceedings of\nthe AAAI Conference on Artificial\nneural networks,”"
        },
        {
          "REFERENCES": "Association for Computational Linguistics.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "Intelligence, vol. 33, no. 01, pp. 4602–4609, Jul. 2019."
        },
        {
          "REFERENCES": "[2] Chandola et al., “Serc-gcn: Speech emotion recognition in conversation",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "[19]\nShi et al.,\n“Masked label prediction: Unified message passing model"
        },
        {
          "REFERENCES": "using graph convolutional networks,”\nin ICASSP 2024 - 2024 IEEE",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "of\nthe\nThirtieth\nfor\nsemi-supervised\nclassification,”\nin Proceedings"
        },
        {
          "REFERENCES": "International Conference on Acoustics, Speech and Signal Processing",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "International Joint Conference on Artificial Intelligence, IJCAI-21, Zhi-"
        },
        {
          "REFERENCES": "(ICASSP), 2024, pp. 76–80.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "Hua Zhou, Ed. 8 2021, pp. 1548–1554,\nInternational Joint Conferences"
        },
        {
          "REFERENCES": "[3] Hazarika et al., “ICON:\nInteractive conversational memory network for",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "on Artificial\nIntelligence Organization, Main Track."
        },
        {
          "REFERENCES": "the 2018 Conference\nmultimodal emotion detection,” in Proceedings of",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "[20] Bai et al.,\n“Hypergraph convolution and hypergraph attention,” Pattern"
        },
        {
          "REFERENCES": "on Empirical Methods\nin Natural Language Processing, Riloff\net\nal.,",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "Recognition, vol. 110, pp. 107637, 2021."
        },
        {
          "REFERENCES": "Eds., Brussels, Belgium, Oct.-Nov. 2018, pp. 2594–2604, Association",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "[21] Cui et al., “Class-balanced loss based on effective number of samples,”"
        },
        {
          "REFERENCES": "for Computational Linguistics.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "the IEEE/CVF Conference on Computer Vision and\nin Proceedings of"
        },
        {
          "REFERENCES": "[4] Hazarika et al.,\n“Conversational memory network for emotion recogni-",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "Pattern Recognition (CVPR), June 2019."
        },
        {
          "REFERENCES": "the 2018 Conference\ntion in dyadic dialogue videos,” in Proceedings of",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "[22] Zhang et al., “Unleashing the power of contrastive self-supervised visual"
        },
        {
          "REFERENCES": "of\nthe North American Chapter of\nthe Association for Computational",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "in Neural\nmodels\nvia\ncontrast-regularized fine-tuning,”\nin Advances"
        },
        {
          "REFERENCES": "Linguistics: Human Language Technologies, Volume 1 (Long Papers),",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "Information Processing Systems, Ranzato et al., Eds. 2021, vol. 34, pp."
        },
        {
          "REFERENCES": "Walker et al., Eds., New Orleans, Louisiana, June 2018, pp. 2122–2132,",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "29848–29860, Curran Associates,\nInc."
        },
        {
          "REFERENCES": "Association for Computational Linguistics.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "[23] Busso et\nal.,\n“Iemocap:\ninteractive\nemotional dyadic motion capture"
        },
        {
          "REFERENCES": "[5] Majumder et al.,\n“Dialoguernn: An attentive rnn for emotion detection",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "database,” Language Resources and Evaluation, vol. 42, no. 4, pp. 335–"
        },
        {
          "REFERENCES": "Proceedings of\nthe AAAI Conference on Artificial\nin conversations,”",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "359, Dec 2008."
        },
        {
          "REFERENCES": "Intelligence, vol. 33, no. 01, pp. 6818–6825, Jul. 2019.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "[24]\nPoria\net\nal.,\n“MELD: A multimodal multi-party dataset\nfor\nemotion"
        },
        {
          "REFERENCES": "[6] Nguyen et\nal.,\n“Ada2i: Enhancing modality balance\nfor multimodal",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "recognition in conversations,” in Proceedings of the 57th Annual Meeting"
        },
        {
          "REFERENCES": "the 32nd ACM\nconversational emotion recognition,”\nin Proceedings of",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "of\nthe Association for Computational Linguistics, Korhonen et al., Eds.,"
        },
        {
          "REFERENCES": "International Conference on Multimedia, New York, NY, USA, 2024,",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "Florence,\nItaly, July 2019, pp. 527–536, Association for Computational"
        },
        {
          "REFERENCES": "MM ’24, p. 9330–9339, Association for Computing Machinery.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "Linguistics."
        },
        {
          "REFERENCES": "[7] Ghosal et al.,\n“DialogueGCN: A graph convolutional neural network",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "[25] Diederik P. Kingma and Jimmy Ba,\n“Adam: A method for\nstochastic"
        },
        {
          "REFERENCES": "the 2019\nfor emotion recognition in conversation,”\nin Proceedings of",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "optimization,” 2017."
        },
        {
          "REFERENCES": "Conference on Empirical Methods in Natural Language Processing and",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "[26]\nJoshi\net\nal.,\n“COGMEN: COntextualized GNN based multimodal"
        },
        {
          "REFERENCES": "the 9th International Joint Conference on Natural Language Processing",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "of\nthe\n2022 Conference\nof\nemotion\nrecognitioN,”\nin Proceedings"
        },
        {
          "REFERENCES": "(EMNLP-IJCNLP), Inui et al., Eds., Hong Kong, China, Nov. 2019, pp.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "the North American Chapter\nof\nthe Association\nfor Computational"
        },
        {
          "REFERENCES": "154–164, Association for Computational Linguistics.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "Linguistics: Human Language Technologies, Carpuat et al., Eds., Seattle,"
        },
        {
          "REFERENCES": "[8]\nIshiwatari et al.,\n“Relation-aware graph attention networks with rela-",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "United States, July 2022, pp. 4148–4164, Association for Computational"
        },
        {
          "REFERENCES": "tional position encodings for emotion recognition in conversations,”\nin",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "Linguistics."
        },
        {
          "REFERENCES": "Proceedings of\nthe 2020 Conference on Empirical Methods in Natural",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "[27] Hu et al.,\n“Mm-dfn: Multimodal dynamic fusion network for emotion"
        },
        {
          "REFERENCES": "Language Processing (EMNLP), Webber et al., Eds., Online, Nov. 2020,",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "recognition in conversations,”\nin ICASSP 2022 - 2022 IEEE Interna-"
        },
        {
          "REFERENCES": "pp. 7360–7370, Association for Computational Linguistics.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "tional Conference on Acoustics, Speech and Signal Processing (ICASSP),"
        },
        {
          "REFERENCES": "[9]\nShen et al., “Directed acyclic graph network for conversational emotion",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "2022, pp. 7037–7041."
        },
        {
          "REFERENCES": "the 59th Annual Meeting of\nthe Asso-\nrecognition,”\nin Proceedings of",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "[28] Chen et al., “Multivariate, multi-frequency and multimodal: Rethinking"
        },
        {
          "REFERENCES": "ciation for Computational Linguistics and the 11th International Joint",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "graph neural networks for emotion recognition in conversation,” in 2023"
        },
        {
          "REFERENCES": "Conference on Natural Language Processing (Volume 1: Long Papers),",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "IEEE/CVF Conference\non Computer Vision\nand Pattern Recognition"
        },
        {
          "REFERENCES": "Zong et al., Eds., Online, Aug. 2021, pp. 1551–1560, Association for",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": "(CVPR), 2023, pp. 10761–10770."
        },
        {
          "REFERENCES": "Computational Linguistics.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "[10] Nguyen et al.,\n“Conversation understanding using relational\ntemporal",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "graph neural networks with auxiliary cross-modality interaction,”\nin",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "Proceedings of\nthe 2023 Conference on Empirical Methods in Natural",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "Language Processing, Bouamor et al., Eds., Singapore, Dec. 2023, pp.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "15154–15167, Association for Computational Linguistics.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "[11] Zhang\net\nal.,\n“M3gat: A multi-modal, multi-task\ninteractive\ngraph",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "attention\nnetwork\nfor\nconversational\nsentiment\nanalysis\nand\nemotion",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "recognition,” ACM Trans.\nInf. Syst., vol. 42, no. 1, Aug. 2023.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "[12] Li et al.,\n“Joyful: Joint modality fusion and graph contrastive learning",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "for multimoda emotion recognition,” in Proceedings of the 2023 Confer-",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "ence on Empirical Methods in Natural Language Processing, Bouamor",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "et\nal., Eds., Singapore, Dec. 2023, pp. 16051–16069, Association for",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "Computational Linguistics.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "[13] Yi et al., “Multimodal fusion via hypergraph autoencoder and contrastive",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "learning for\nemotion recognition in conversation,”\nin Proceedings of",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "the\n32nd ACM International Conference\non Multimedia, New York,",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "NY, USA, 2024, MM ’24, p. 4341–4348, Association for Computing",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "Machinery.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "[14] Zhang\net\nal.,\n“Knowledge-aware\ngraph\nconvolutional\nnetwork with",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "utterance-specific window search\nfor\nemotion\nrecognition\nin\nconver-",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "sations,”\nin ICASSP 2023 - 2023 IEEE International Conference on",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "Acoustics, Speech and Signal Processing (ICASSP), 2023, pp. 1–5.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "the\n[15]\nSzegedy et al., “Going deeper with convolutions,” in Proceedings of",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "June 2015.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "in Neural\n[16] Vaswani\net\nal.,\n“Attention is\nall you need,”\nin Advances",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "Information Processing Systems, Guyon et al., Eds. 2017, vol. 30, Curran",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        },
        {
          "REFERENCES": "Associates,\nInc.",
          "[17]\nSkianis et al., “Fusing document, collection and label graph-based repre-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "DialogueCRN: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Hu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "2",
      "title": "Serc-gcn: Speech emotion recognition in conversation using graph convolutional networks",
      "authors": [
        "Chandola"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Hazarika"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "4",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Hazarika"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "5",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Ada2i: Enhancing modality balance for multimodal conversational emotion recognition",
      "authors": [
        "Nguyen"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "7",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Ghosal"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Ishiwatari"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Shen"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Conversation understanding using relational temporal graph neural networks with auxiliary cross-modality interaction",
      "authors": [
        "Nguyen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "M3gat: A multi-modal, multi-task interactive graph attention network for conversational sentiment analysis and emotion recognition",
      "authors": [
        "Zhang"
      ],
      "year": "2023",
      "venue": "ACM Trans. Inf. Syst"
    },
    {
      "citation_id": "12",
      "title": "Joyful: Joint modality fusion and graph contrastive learning for multimoda emotion recognition",
      "authors": [
        "Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Multimodal fusion via hypergraph autoencoder and contrastive learning for emotion recognition in conversation",
      "authors": [
        "Yi"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Knowledge-aware graph convolutional network with utterance-specific window search for emotion recognition in conversations",
      "authors": [
        "Zhang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Going deeper with convolutions",
      "authors": [
        "Szegedy"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "16",
      "title": "Attention is all you need",
      "authors": [
        "Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "Fusing document, collection and label graph-based representations with word embeddings for text classification",
      "authors": [
        "Skianis"
      ],
      "year": "2018",
      "venue": "Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Weisfeiler and leman go neural: Higher-order graph neural networks",
      "authors": [
        "Morris"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Masked label prediction: Unified message passing model for semi-supervised classification",
      "authors": [
        "Shi"
      ],
      "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21"
    },
    {
      "citation_id": "20",
      "title": "Hypergraph convolution and hypergraph attention",
      "authors": [
        "Bai"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "21",
      "title": "Class-balanced loss based on effective number of samples",
      "authors": [
        "Cui"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "22",
      "title": "Unleashing the power of contrastive self-supervised visual models via contrast-regularized fine-tuning",
      "authors": [
        "Zhang"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "23",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Busso"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "24",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Poria"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "25",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2017",
      "venue": "Adam: A method for stochastic optimization"
    },
    {
      "citation_id": "26",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "authors": [
        "Joshi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "27",
      "title": "Mm-dfn: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "Hu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recognition in conversation",
      "year": "2023",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    }
  ]
}