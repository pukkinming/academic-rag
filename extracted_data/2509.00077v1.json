{
  "paper_id": "2509.00077v1",
  "title": "Amplifying Emotional Signals: Data-Efficient Deep Learning For Robust Speech Emotion Recognition",
  "published": "2025-08-26T19:08:54Z",
  "authors": [
    "Tai Vu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) presents a significant yet persistent challenge in humancomputer interaction. While deep learning has advanced spoken language processing, achieving high performance on limited datasets remains a critical hurdle. This paper confronts this issue by developing and evaluating a suite of machine learning models, including Support Vector Machines (SVMs), Long Short-Term Memory networks (LSTMs), and Convolutional Neural Networks (CNNs), for automated emotion classification in human speech. We demonstrate that by strategically employing transfer learning and innovative data augmentation techniques, our models can achieve impressive performance despite the constraints of a relatively small dataset. Our most effective model, a ResNet34 architecture, establishes a new performance benchmark on the combined RAVDESS and SAVEE datasets, attaining an accuracy of 66.7% and an F1 score of 0.631. These results underscore the substantial benefits of leveraging pre-trained models and data augmentation to overcome data scarcity, thereby paving the way for more robust and generalizable SER systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The ability to perceive and interpret human emotion is a cornerstone of intelligent interaction, yet it remains a significant frontier in the quest to build truly symbiotic human-computer interfaces. While deep learning has catalyzed remarkable progress in spoken language processing  (LeCun et al., 1995; Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017) , LSTMs, particularly in speech recognition and synthesis, the domain of Speech Emotion Recognition (SER) presents a more nuanced and formidable challenge. Unlike transcribing words, recognizing emotion requires decoding subtle, complex acoustic cues, variations in pitch, tone, and en-ergy, that are often ambiguous and highly contextdependent. This complexity is magnified by a persistent bottleneck in the field: the scarcity of large-scale, emotionally annotated speech datasets. Consequently, even sophisticated neural network architectures frequently struggle to generalize from limited training data, leading to overfitting and unreliable performance in real-world applications.\n\nThis paper directly confronts the critical challenge of data scarcity in SER. We propose and validate a methodology that demonstrates how stateof-the-art performance can be achieved even when training data is limited. By systematically evaluating a range of models, from traditional Support Vector Machines (SVMs) to deep learning architectures like LSTMs and CNNs, we identify the most effective approaches for this constrained environment. Our core contribution lies in the strategic application of transfer learning and data augmentation. We treat log-mel spectrograms, 2D visual representations of audio, as images, allowing us to leverage the power of a ResNet34 model pretrained on the vast ImageNet database. This crossdomain knowledge transfer, combined with a suite of augmentation techniques, empowers our model to learn robust, generalizable features from a small dataset. Our experiments, conducted on a composite of the RAVDESS and SAVEE datasets, culminate in a model that not only achieves a new benchmark with 66.7% accuracy and a 0.631 F1 score but also provides a clear blueprint for developing powerful, data-efficient SER systems.\n\nbust performance in data-constrained, audio-only environments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Traditional And Feature-Based Approaches",
      "text": "Early explorations into SER relied on engineered acoustic features to classify emotions. For instance, Hidden Markov Models (HMMs) were employed to extract and model features from speech signals for emotion detection  (Schuller et al., 2003) .\n\nA pivotal advancement was the adoption of Mel Frequency Cepstral Coefficients (MFCCs), which proved highly effective for speech-related tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "A parallel stream of research has focused on enhancing SER by incorporating information from additional modalities, such as video or text. For example, some studies have combined hand-crafted speech features like pitch and energy with facial landmark features extracted from videos  Kim et al. (2013) .  Tzirakis et al. (2017)  developed a more advanced multimodal system using 1D convolutional layers for speech and a ResNet50 for visual information from video frames, feeding the combined features into an LSTM for final classification.\n\nWhile these multimodal approaches have shown improved accuracy, they are contingent on the availability of visual or textual data, which is often not the case. This limitation underscores the importance of developing high-performing, audio-only systems. Our work is therefore motivated by the need for a practical solution that can accurately infer emotion based solely on audio inputs, without reliance on supplementary data streams.\n\n3 Approach",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Models",
      "text": "Our machine learning system included an encoder, which was followed by a classifier. The encoder received an audio clip and then produced a vector representation of the input data. Subsequently, this encoding was fed into the classifier, which outputted an emotion label.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model 1: Mfcc And Svm",
      "text": "As a starting point, we implemented the feature extractor using the Mel Frequency Cepstral Coefficients (MFCC). Afterwards, we took the averages of these MFCC input features across the time dimension and then used them to train a Support Vector Machines (SVM) model  (Boser et al., 1992)  to classify different emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model 2: Log Mel Spectrograms And Lstm",
      "text": "Our second model encoded each data point by computing a mel-scaled spectrogram and then converting it to log space. We built an LSTM neural network as our classifier. This network contained 2 bidirectional LSTM layers, followed by a dropout layer, a linear layer, and a softmax layer.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model 3: Log Mel Spectrograms And Cnn",
      "text": "In this model, we also extracted log-scaled mel spectrograms for the input speech data. Since these features were similar to 2D image arrays (shown in Figure  1 ), we then fed them into a CNN classifier in order to obtain emotion labels. Previously, we intended to put raw waveforms directly through the CNN model. However, during our experiments, we found out that training the CNN on log-scaled mel spectrograms was easier and more stable. We chose ResNet34  (He et al., 2016)  as our CNN architecture. Additionally, we experimented with two different approaches: training a ResNet34 network from scratch and using transfer learning to finetune a ResNet34 model that was pretrained on the ImageNet database  (Russakovsky et al., 2015; Vu et al., 2020) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data Augmentation",
      "text": "As we developed and trained our models on a small speech dataset, data augmentation would be helpful in generating more training data and dealing with overfitting problems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Image-Based Data Augmentation",
      "text": "In particular, since our CNN models were trained on image-like 2D arrays of log-scaled mel spectrograms, we applied several data augmentation methods on these input data, which include rotating by a small degree, zooming in, and changing brightness. Although such image-based augmentation techniques were more common in computer vision tasks and were not directly applied to audio data, we will demonstrate in Section 5.4 that these techniques indeed helped prevent overfitting and improve model performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Progressive Resizing",
      "text": "Another augmentation method that we used was progressive resizing  (Colangelo et al., 2021; Vu and Yang, 2025a) . Specifically, we first trained the CNN models on smaller versions of the logscaled mel spectrogram arrays (128 × 128), and then finetuned the networks on arrays of larger sizes (256 × 256). This approach not only augmented the training data, but also allowed the models to train much faster.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Mixup",
      "text": "In addition, we harnessed Mixup, a data augmentation technique that generated convex combinations of pairs of training examples and their labels  (Zhang et al., 2018) . Particularly, for two randomly sampled data points (x i , y i ) and (x j , y j ), this method constructed a new example of the form:\n\nHere, x i , x j are input vectors, y i , y j are one-hot label encodings, and λ ∈ [0, 1]. In this way, Mixup acted as a regularizer that encouraged the linear behaviors of the models, reduced their variance, and enhanced their generalization powers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Implementation",
      "text": "I implemented the code for this project in Python using PyTorch  (Paszke et al., 2019) , FastAI  (Howard and Gugger, 2020) , Scikit-learn  (Pedregosa et al., 2011 ), Librosa (McFee et al., 2015) . All the code can be found here.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data",
      "text": "In this project, we used the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) database  (Livingstone and Russo, 2018)  and the Surrey Audio-Visual Expressed Emotion (SAVEE) database  (Jackson and Haq, 2014) . We combined them into a single dataset for training and testing our models.\n\nRAVDESS is an English language database that contains 1440 utterances. This dataset was made by 24 actors (12 female and 12 male), who said two sentences \"Kids are talking by the door\" and \"Dogs are sitting by the door\" with various emotions. Meanwhile, the SAVEE database consists of 480 audio clips created by 4 male actors, and each of them recorded 15 sentences. There are 8 different emotion classes, including neutral, calm, happy, sad, angry, fearful, disgust, and surprised.\n\nThe duration of each utterance ranges from 3 to 5 seconds. The total duration of audio recordings is roughly 2 hours. In addition, we can see in Figure  2  that most of the emotional classes are relatively well balanced. The neutral and calm labels contain slightly fewer audio clips than the other 6 classes.\n\nWe split the dataset into 90% for training, 5% for validation, and 5% for testing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiment Details",
      "text": "For the SVM model, we produced 20 MFCCs for each input audio clip. We chose an RBF kernel for the SVM algorithm. For the LSTM and CNN models, we generated 128 mel bands when converting input speeches to mel spectrograms. We trained the LSTM model and the vanilla CNN model (with no pretraining) for 200 epochs. Meanwhile, for the ResNet34 model that was pretrained on ImageNet, we finetuned its weights for 30 epochs. We used a batch size of 64 and a learning rate of 0.001, with a decay rate of 0.9. We trained the above neural networks using the Cross Entropy loss and the Adam optimization algorithm (Kingma and Ba, 2014).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Evaluation Methods",
      "text": "Since this project tackled a classification problem, we used classification accuracy scores and F1 scores for evaluating model performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Results",
      "text": "As shown in Table  1 , the SVM algorithm produced an accuracy of 51.7% and an F1 score of 0.509. This result was better than we expected, because the model only took into account the mean values of the MFCC features across the time dimension. In other words, the SVM algorithm did not get access to useful temporal dependencies amongst the input MFCC features, but still learned to predict emotions with more than 50% accuracy.\n\nAfter that, the LSTM model performed slightly better than the SVM algorithm, with a higher accuracy of 52.8% and a comparable F1 score of 0.497. When investigating its training process, we can see that the performance was still quite low because the LSTM network was overfitting to the training data. In particular, the model learned to decrease training losses to a small value (around 0.5), but the validation losses were still high (around 2.9).\n\nA similar pattern occurred for the vanilla CNN model (with no pretraining), as it only produced 45.8% accuracy. In this case, another issue is that because the training set was too small, the Resnet34 network was not able to learn good representations of the speech contents, so it could not generalize well to unseen data.\n\nIn fact, when we finetuned the ResNet34 model with pretrained weights from ImageNet, the performance went up significantly (57.3% in accuracy and 0.528 in F1 score). Therefore, we can see that the neural network learned useful feature representations of the speech data after being pretrained on a large database like ImageNet. When it was finetuned on our small dataset, the model was able to transfer its prior knowledge about images to reading and extracting information from image-like log-scaled mel spectrogram arrays. The finetuning process then helped the model to adapt to the domain of our dataset even better, which enhanced its performance.\n\nFinally, the ResNet34 model with both transfer learning and data augmentation achieved the best performance, with an accuracy of 66.7% and an F1 score of 0.631. This illustrates the effectiveness of data augmentation techniques in boosting our model performance. Indeed, as we can see in the upper plot of Figure  3 , the ResNet34 network without data augmentation was still overfitting, with low training loss values and high validation loss values. This means that the gap between the training losses and the validation losses was still very large. However, this problem was alleviated with the support of data augmentation, as shown in the lower plot of Figure  3 . Both the training losses and the validation losses decreased gradually, and the gap between them was significantly narrowed.\n\nMeanwhile, because the accuracy of our final model was less than 70%, there is still a lot of room for improvement. One of the main challenges faced by our models was that RAVDESS and SAVEE were two simulated datasets, which consisted of several actors repeating the same sentences with various emotions. Hence, the speech contents in these datasets were not diverse enough for our machine learning programs to learn proper representations of input audio data and detect correlations between human speeches and emotions. In addition, we can observe in Figure  4      more, there was some confusion between certain pairs of emotion labels, such as neutral and calm. This issue is understandable, because the audio clips from these two classes in our dataset often sound similar. Two examples from those two classes are shown in Figure  5 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we successfully addressed the critical challenge of developing a high-performing Speech Emotion Recognition (SER) system in a data-constrained environment. We systematically developed and evaluated a series of models, including SVMs, LSTMs, and CNNs, culminating in a ResNet34 architecture that achieved a promising accuracy of 66.7% and an F1 score of 0.631 on a combined dataset of RAVDESS and SAVEE utterances.\n\nThe key to this achievement was a strategic methodology rooted in two powerful techniques. First, transfer learning proved instrumental in overcoming the limitations of our small dataset. By leveraging a ResNet34 model pre-trained on Im-ageNet, we effectively transferred rich, hierarchical feature-extraction capabilities to the domain of audio-spectrogram analysis, providing a robust foundation for learning. Second, data augmentation was critical for improving generalization and mitigating overfitting-a significant issue observed in our initial deep learning models. The combination of these methods demonstrates that it is possible to build effective SER systems without relying on massive, annotated speech corpora. While acknowledging the limitations inherent in using simulated datasets and the potential for further accuracy improvements, our work provides a clear and effective blueprint for data-efficient learning in the field of SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Future Work",
      "text": "Building on the promising results of this study, we have identified several key directions for future research that aim to further advance the capabilities and robustness of our SER models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model And Training Refinements",
      "text": "Our immediate next step involves more exhaustive hyperparameter tuning to optimize the current ResNet34 architecture. Beyond this, we plan to explore hybrid network architectures, such as combining CNN layers for efficient feature extraction from spectrograms with LSTM layers to better model the temporal dependencies inherent in speech. Such models may offer a more holistic understanding of the acoustic and sequential nature of emotional expression.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Advanced Audio-Based Augmentation",
      "text": "While our image-based augmentation on spectrograms was effective, we plan to implement more domain-specific, audio-based data augmentation techniques. Methods such as pitch shifting, timestretching, and altering loudness can create more realistic variations in the training data, better preparing the model for real-world acoustic conditions. Furthermore, we intend to integrate advanced tech-niques like SpecAugment, which has shown exceptional results in speech recognition by masking frequency channels and time steps directly on the spectrogram, thereby forcing the model to learn more robust and redundant features  (Park et al., 2019; Vu and Yang, 2025b) .\n\n7.3 Exploring State-of-the-Art Pre-trained Speech Models\n\nFinally, recognizing the immense potential of transfer learning, our most ambitious future work will involve fine-tuning large-scale models that have been pre-trained specifically on vast unlabeled speech corpora. Models like wav2vec  (Schneider et al., 2019)  and SpeechBERT  (Chuang et al., 2019; Sun et al., 2025)  have learned fundamental representations of human speech that are potentially far more powerful than those learned from general-purpose image datasets. Adapting these state-of-the-art speech foundation models to the downstream task of emotion recognition could unlock significant performance gains and push the boundaries of what is achievable in SER.",
      "page_start": 1,
      "page_end": 1
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ), we then fed them into a CNN classifier",
      "page": 2
    },
    {
      "caption": "Figure 1: Log mel spectrogram features of an example.",
      "page": 3
    },
    {
      "caption": "Figure 2: that most of the emotional classes are relatively",
      "page": 3
    },
    {
      "caption": "Figure 2: Distribution of emotion labels in the dataset.",
      "page": 4
    },
    {
      "caption": "Figure 3: , the ResNet34 network with-",
      "page": 4
    },
    {
      "caption": "Figure 3: Both the training losses and",
      "page": 4
    },
    {
      "caption": "Figure 3: Training and validation losses across 30",
      "page": 5
    },
    {
      "caption": "Figure 4: Confusion matrix for the best CNN model.",
      "page": 5
    },
    {
      "caption": "Figure 5: The waveforms of a neutral utterance (upper)",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , the SVM algorithm produced",
      "page": 4
    },
    {
      "caption": "Table 1: Performance of different models on the validation set.",
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A training algorithm for optimal margin classifiers",
      "authors": [
        "Bernhard Boser",
        "Isabelle Guyon",
        "Vladimir Vapnik"
      ],
      "year": "1992",
      "venue": "Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory"
    },
    {
      "citation_id": "2",
      "title": "Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering",
      "authors": [
        "Yung-Sung Chuang",
        "Chi-Liang Liu",
        "Hung-Yi Lee",
        "Lin-Shan Lee"
      ],
      "year": "2019",
      "venue": "Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering",
      "arxiv": "arXiv:1910.11559"
    },
    {
      "citation_id": "3",
      "title": "Progressive training of convolutional neural networks for acoustic events classification",
      "authors": [
        "Federico Colangelo",
        "Federica Battisti",
        "Alessandro Neri"
      ],
      "year": "2021",
      "venue": "2020 28th European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "4",
      "title": "Application of fuzzy c-means clustering algorithm to spectral features for emotion classification from speech",
      "authors": [
        "Semiye Demircan",
        "Humar Kahramanli"
      ],
      "year": "2018",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "5",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "6",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "7",
      "title": "Fastai: A layered api for deep learning",
      "authors": [
        "Jeremy Howard",
        "Sylvain Gugger"
      ],
      "year": "2020",
      "venue": "Information"
    },
    {
      "citation_id": "8",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "Philip Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "9",
      "title": "Deep learning for robust feature generation in audiovisual emotion recognition",
      "authors": [
        "Yelin Kim",
        "Honglak Lee",
        "Emily Provost"
      ],
      "year": "2013",
      "venue": "2013 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "10",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "11",
      "title": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio"
      ],
      "year": "1995",
      "venue": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks",
      "authors": [
        "Wootaek Lim",
        "Daeyoung Jang",
        "Taejin Lee"
      ],
      "year": "2016",
      "venue": "2016 Asia-Pacific signal and information processing association annual summit and conference (APSIPA)"
    },
    {
      "citation_id": "13",
      "title": "Funding Information Natural Sciences and Engineering Research Council of Canada: 2012-341583 Hear the world research chair in music and emotional speech from Phonak",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "Funding Information Natural Sciences and Engineering Research Council of Canada: 2012-341583 Hear the world research chair in music and emotional speech from Phonak",
      "doi": "10.5281/zenodo.1188976"
    },
    {
      "citation_id": "14",
      "title": "Eric Battenberg, and Oriol Nieto. 2015. librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Mcvicar"
      ],
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "15",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "Daniel Park",
        "William Chan",
        "Yu Zhang",
        "Chung-Cheng Chiu",
        "Barret Zoph",
        "Ekin Cubuk",
        "V Quoc",
        "Le"
      ],
      "year": "2019",
      "venue": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "doi": "10.21437/interspeech.2019-2680"
    },
    {
      "citation_id": "16",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Pytorch: An imperative style, high-performance deep learning library",
      "arxiv": "arXiv:1912.01703"
    },
    {
      "citation_id": "17",
      "title": "Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in python",
      "authors": [
        "Fabian Pedregosa",
        "Gaël Varoquaux",
        "Alexandre Gramfort",
        "Vincent Michel",
        "Bertrand Thirion",
        "Olivier Grisel",
        "Mathieu Blondel",
        "Peter Prettenhofer",
        "Ron Weiss"
      ],
      "venue": "Journal of machine Learning research"
    },
    {
      "citation_id": "18",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang",
        "Andrej Karpathy",
        "Aditya Khosla",
        "Michael Bernstein"
      ],
      "year": "2015",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "19",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "20",
      "title": "Hidden markov model-based speech emotion recognition",
      "authors": [
        "Björn Schuller",
        "Gerhard Rigoll",
        "Manfred Lang"
      ],
      "year": "2003",
      "venue": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Privacy preserving inference of personalized content for out of matrix users",
      "authors": [
        "Michael Sun",
        "Tai Vu",
        "Andrew Wang"
      ],
      "year": "2025",
      "venue": "Privacy preserving inference of personalized content for out of matrix users"
    },
    {
      "citation_id": "22",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need",
      "arxiv": "arXiv:1706.03762"
    },
    {
      "citation_id": "24",
      "title": "How not to give a flop: Combining regularization and pruning for efficient inference",
      "authors": [
        "Tai Vu",
        "Emily Wen",
        "Roy Nehoran"
      ],
      "year": "2020",
      "venue": "How not to give a flop: Combining regularization and pruning for efficient inference"
    },
    {
      "citation_id": "25",
      "title": "2025a. Bert-vqa: Visual question answering on plots",
      "authors": [
        "Tai Vu",
        "Robert Yang"
      ],
      "venue": "2025a. Bert-vqa: Visual question answering on plots"
    },
    {
      "citation_id": "26",
      "title": "2025b. Ganime: Generating anime and manga character drawings from sketches with deep learning",
      "authors": [
        "Tai Vu",
        "Robert Yang"
      ],
      "venue": "2025b. Ganime: Generating anime and manga character drawings from sketches with deep learning"
    },
    {
      "citation_id": "27",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "Hongyi Zhang",
        "Moustapha Cisse",
        "Yann Dauphin",
        "David Lopez-Paz"
      ],
      "year": "2018",
      "venue": "mixup: Beyond empirical risk minimization"
    }
  ]
}