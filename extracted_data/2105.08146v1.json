{
  "paper_id": "2105.08146v1",
  "title": "Muser: Multimodal Stress Detection Using Emotion Recognition As An Auxiliary Task",
  "published": "2021-05-17T20:24:46Z",
  "authors": [
    "Yiqun Yao",
    "Michalis Papakostas",
    "Mihai Burzo",
    "Mohamed Abouelenien",
    "Rada Mihalcea"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The capability to automatically detect human stress can benefit artificial intelligent agents involved in affective computing and humancomputer interaction. Stress and emotion are both human affective states, and stress has proven to have important implications on the regulation and expression of emotion. Although a series of methods have been established for multimodal stress detection, limited steps have been taken to explore the underlying inter-dependence between stress and emotion. In this work, we investigate the value of emotion recognition as an auxiliary task to improve stress detection. We propose MUSER -a transformer-based model architecture and a novel multi-task learning algorithm with speed-based dynamic sampling strategy. Evaluations on the Multimodal Stressed Emotion (MuSE) dataset show that our model is effective for stress detection with both internal and external auxiliary tasks, and achieves state-ofthe-art results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Stress is a feeling of emotional or physical tension, as a response to the environment when people have difficulty dealing with the conditions  (Dobson and Smith, 2000; Muthukumar and Nachiappan, 2013) . Stress detection is a classification task that predicts whether a certain target is under stress. The task has drawn research attention for two reasons: first, stress detection plays an important role in applications related to psychological well-being  (Cohen et al., 1991) , cognitive behavior therapies  (Tull et al., 2007) , and safe driving  (Gao et al., 2014; Chen et al., 2017) ; second, stress is a known regulator of human emotion mechanisms  (Tull et al., 2007) , and thus research on stress detection can potentially benefit the development of emotionally intelligent agents.\n\nThe impact of stress on human behavior can be observed through various modalities. Previ-ous work has considered both unimodal and multimodal stress detection using acoustic, video and physiological sensor signals  (Lane et al., 2015; Jaques et al., 2016; Aigrain et al., 2016; Alberdi et al., 2016; Bara et al., 2020) . However, text-based stress detection remains vastly underexplored, with some studies  (Lin et al., 2014)  showing the potential for further research. In recent years, the surge of advanced natural language understanding models and structures provides a great opportunity for stress detection systems, especially using the textual modality. In this work, we focus on the textual and acoustic modalities. For the model architecture, we use Transformers  (Vaswani et al., 2017)  as a textual encoder and Multi-Layer Perceptrons (MLP) as an acoustic encoder.\n\nThe majority of existing stress detection methods are based on single-task learning with the binary stress/non-stress labels. However, stress is not an isolated affective state, but closely related to the expression and regulation of human emotions. Physiological studies  (Wang and Saudino, 2011)  have demonstrated that emotion and stress share some neural structures, including prefrontal cortex  (Taylor and Stanton, 2007) , anterior cingulate cortex  (Pruessner et al., 2008) , and amygdala  (Adolphs, 2003) . Acoustic studies  (Paulmann et al., 2016)  have shown that the pitch and amplitude of human emotional prosody is different under stress and non-stressed status. Inspired by these studies, our work aims to exploit the inter-dependence between emotion and stress. Specifically, we investigate the value of emotion recognition as an auxiliary task for stress detection.\n\nMulti-task learning  (Pasunuru and Bansal, 2017; Gottumukkala et al., 2020; Guo et al., 2018a; Gong et al., 2019)  has proven to be effective for transferring knowledge between different tasks. Dynamic sampling strategies, which aim at adaptively adjusting the ratio of samples from different tasks, are widely used to balance the training schedule.\n\nHowever, strategies based on gradients  (Chen et al., 2018b) , uncertainty  (Kendall et al., 2018)  or loss  (Liu et al., 2019)  cannot leverage the validation performances, while some performance-based strategies  (Gottumukkala et al., 2020)  are impractical if the metrics for different tasks are not directly comparable (i.e., with different scale ranges). To this end, we propose a novel speed-based strategy that is both effective and efficient in the multi-task learning for stress and emotion.\n\nOur method is evaluated on the Multimodal Stressed Emotion (MuSE) dataset  (Jaiswal et al., 2019 (Jaiswal et al., , 2020)) , which includes both stress and emotion labels, making it the ideal benchmark for an in-depth analysis of their inter-dependence. To test the generalization ability of our method, we also use an external emotion dataset for the auxiliary task. Multimodal emotion recognition is a wellstudied field with many existing datasets  (Busso et al., 2008 (Busso et al., , 2016;; Chen et al., 2018a; Barros et al., 2018; Zadeh et al., 2018) . We choose the OMG-Emotion dataset  (Barros et al., 2018)  as the external auxiliary task because it is representative and challenging, with numerical emotion scores instead of categorical labels.\n\nOur paper makes four main contributions. First, we show the inter-dependence between stress and emotion via quantitative analyses on linguistic and acoustic features, and propose to use emotion recognition as an auxiliary task for stress detection. Second, we establish a stress detection model with a transformer structure, as well as a novel speed-based dynamic sampling strategy for multi-task learning. We name our framework the MUltimodal Stress Detector with Emotion Recognition (MUSER). Third, we achieve stateof-the-art results on the MuSE dataset via multitask training with stress and emotion labels. We also achieve competitive results when we use the OMG-Emotion  (Barros et al., 2018)  dataset as an external auxiliary task. Finally, experimental results show that our speed-based dynamic sampling significantly outperforms other widely-used methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Unimodal Stress Detection",
      "text": "Stress detection based on textual modality has been studied by  (Lin et al., 2014)  and  (Jaiswal et al., 2020) , using the Linguistic Inquiry and Word Count (LIWC) lexicon  (Pennebaker et al., 2001)  to extract features that are indicative of human emotion. Acoustic features  (Lane et al., 2015; Paulmann et al., 2016; Horvath, 1982; Lech and He, 2014)  have also been used for unimodal stress detection in both physiological and computational studies.\n\nA drawback of the unimodal approaches is that they only have access to partial information about the expression of stress, while multiple modalities can potentially be informative at the same time  (Aigrain et al., 2016) . As demonstrated by previous work on human sentiment and emotion prediction  (Zadeh et al., 2016 (Zadeh et al., , 2018;; Yao et al., 2020) , multimodal features usually results in better performances.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Stress Detection",
      "text": "Commonly-used modalities for stress detection include video, audio, text and physiological signals such as thermal maps from sensors  (Aigrain et al., 2016; Alberdi et al., 2016; Lane et al., 2015; Jaques et al., 2016) .  Jaiswal et al. (2020)  proposed the Multimodal Stressed Emotion (MuSE) dataset, which includes records from all the commonly-used modalities. Each video clip is annotated for both stress detection and emotion recognition. Unimodal and multimodal baselines are provided for each task.  Bara et al. (2020)  developed a multimodal deep learning method that learns modality-independent representations in an unsupervised approach. However, none of these models leverage the intrinsic connections between stress and emotion.\n\nOur experiments are conducted on the MuSE dataset using only the textual and acoustic modalities, to be compatible with most external emotion recognition tasks. However, our proposed multitask learning method is model-agnostic and can be generalized to any structure and any modality combinations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Widely-used multimodal emotion recognition datasets include SEMAINE  (McKeown et al., 2011) , IEMOCAP  (Busso et al., 2008) , MOSEI  (Zadeh et al., 2018)  and OMG-Emotion  (Barros et al., 2018) . Emotion can be annotated either with pre-defined emotion categories or through two-dimensional scores of activation (arousal) and valence, according to the self-assessment manikin proposed by  (Bradley and Lang, 1994) . MuSE, in particular, has emotion annotations expressed by activation and valence scores (1∼9), which is more fine-grained than categorical definitions (happy, angry, etc.). The OMG-Emotion dataset we use as external auxiliary task is annotated in the same way with a score range of 0∼1.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Task Learning",
      "text": "Because of the different task natures, balancing the training procedure with all the tasks is a critical problem for multi-task learning. Loss-balancing strategies  (Chen et al., 2018b; Kendall et al., 2018; Liu et al., 2019; Gong et al., 2019; Guo et al., 2018a; Lample et al., 2017; Yao et al., 2019)  are suitable for situations in which there are multiple training objectives that can be combined via weighted summation for each data point. In contrast, for multi-task learning across different datasets, a sampling strategy should be applied to decide the mixing ratio (how many batches to sample from each task) in each epoch. To this end,  Pasunuru et al. (2017)  used a fixed sampling ratio;  Guo et al. (2018b)  proposed a dynamic sampling strategy based on reinforcement learning, which depends on the estimation of Q-values; Gottumukkala et al. (  2020 ) used a dynamic sampling procedure based on the gap between multi-task and singletask results -a performance-based method that requires all the tasks to use the same set of evaluation metrics. For comparison, our proposed strategy is also based on how fast the model is learning each task, but does not require the metrics to be directly comparable.\n\n3 Expressions of Stress in Data",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "The MuSE dataset  (Jaiswal et al., 2020)  is collected from the multimodal video recordings of 28 student participants, 9 female and 19 male. Each partici- pant is invited to a video-recording session before and after the final exam period; sessions before exams are labeled as stressed, and the remainng ones are labeled as non-stressed. We use only the records from the monologue sub-sessions where both acoustic and textual modalities are available.\n\nIn these sub-sessions, the participants view five emotion-eliciting questions on the screen in a sequence, and answer them with monologues. After each monologue, the participants provide selfassessment scores for activation (calm vs. excited) and valence (negative vs. positive). The scores range from 1∼9. The monologues are segmented into sentences for pre-processing; each sentence is annotated with the same stress label and emotion scores as the whole monologue. We use a train, validation, and test split of 1,853, 200, and 273 sentences, respectively. Textual features come from the automatic transcripts for the audio clips of each sentence. Although the sentences come with visual and thermal features as well, we focus mainly on the textual and acoustic modalities because this allows us to use almost any external emotion recognition dataset as our auxiliary task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Characteristics Of Stress In Language",
      "text": "In order to analyze the connections between linguistic features that are most indicative of stress, activation, and valence, we first extract a feature vector based on the LIWC lexicon  (Pennebaker et al., 2001) . Each dimension of the vector corresponds to a certain word category and has a value equal Features that appear in top 20 for all three tasks are shown in Table  1 . The features are ranked by the absolute value of their linear coefficients. As shown, the \"positive-emotion\" and \"perceptual\" word classes are critical for both emotion and stress tasks, which is intuitive because they are a pair of inter-dependent human affect status. Bio, health, and body words are also on the list, suggesting that both stress and emotion are closely related to physiological status and feelings, which is potentially because they share some neural structures in brain  (Wang and Saudino, 2011) . The intersection of all the three top-indicator sets has nine elements, reflecting a reasonable overlap.\n\nTable  2  shows the word classes appearing uniquely in the top 20 indicator list for each task. It is worth noticing that the non-fluent words (er, hmm, etc.) are the strongest unique indicator of stress, which reflects the differences in the audio speeches under stressed/non-stressed conditions. We could also observe that activation is more connected to entities and events, while valence is more related to personal feelings.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Characteristics Of Stress In Speech",
      "text": "For stress indicators in the acoustic modality, we extract 88-dimensional features using OpenSmile  (Eyben et al., 2010)  with the eGeMaps  (Eyben et al., 2015)  configuration. We follow  (Jaiswal et al., 2020)  to do speaker-level z-normalization on each feature, and fit a linear classification/regression model as we did for the textual features.\n\nTable  3  shows the most indicative acoustic feature classes for all the tasks, as well as the ones that are unique for each task. Amplitude/loudness is the strongest indicator class for all tasks, followed by spectral flux, which is a measure of how quickly the power spectrum of a signal is changing. It also suggests that stress has a closer relationship with spectral features such as slope, describing how quickly the spectrum of an audio sound tails off towards the high frequencies.\n\nThe intersection of all three indicator sets has 11 elements, suggesting that they share many acoustic patterns. For more detailed explanations and examples of the eGeMaps features please refer to  (Eyben et al., 2015)  and  (Botelho et al., 2019) .\n\nRegarding the differences in the task nature, as seen in Table  4 , the number of unique indicators for each each and for each modality show that the activation task is less independent of the stress task than the valence task. In other words, the activation task has more indicators in common with the stress task.  stress detection. Since MuSE has both stress and emotion labels, their activation and valence scores can be used as an internal auxiliary task.\n\nTo test the generalization capability of our multitask learning method, we choose OMG-Emotion  (Barros et al., 2018)  as an external emotion recognition dataset for the auxiliary task, which is annotated in the same manner as MuSE (activation/valence). We download the videos from the training and validation sets and filter out all the samples where the video link is broken or the length of automatic transcription is less than 5, resulting in 1,484 videos. The contents and scenarios in the OMG-Emotion dataset are completely different from MuSE. We hold out 300 videos as a validation set to enable dynamic sampling.\n\nNote that stress detection is a binary classification task, while the two auxiliary emotion tasks have a regressive nature.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Pre-Processing",
      "text": "Each utterance in the MuSE dataset is automatically segmented into sentences, transcribed, and tokenized by a pre-trained BERT tokenizer  (Devlin et al., 2019) . For the acoustic modality, we use OpenSmile  (Eyben et al., 2010)  with the eGeMAPS configuration  (Eyben et al., 2015)  to extract 88 utterance-level statistical features. Following  (Jaiswal et al., 2020) , we perform speaker-level z-normalization on all acoustic features. For videos in the OMG-Emotion dataset, we first extract the audio and automatic transcripts, and then do the same pre-processing as on MuSE.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Muser: Architecture",
      "text": "We propose MUSER: MUltimodal Stress Detector using Emotion Recognition. The model structure is based on neural networks. Specifically, we use a Transformer  (Vaswani et al., 2017)  textual encoder pre-trained with BERT  (Devlin et al., 2019) , and an MLP-based acoustic encoder to generate representations on each modality, and fuse them before classification or regression. Our model architecture is depicted in Figure  1 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Textual Encoder",
      "text": "For the textual encoder, we use a Transformer neural network pre-trained with BERT on BookCorpus and English Wikipedia  (Devlin et al., 2019) . Our Transformer model has 12 layers, 12 attention heads, and 768-dimensional hidden states. The averaged hidden states on the top level are projected to 256-dimensional representations by a fully-connected layer.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Acoustic Encoder",
      "text": "Our acoustic encoder is a Multi-layer Perceptron network with four hidden layers and ReLU activation. The input of the acoustic encoder is the OpenSmile features extracted from the audio speech of each sentence, and the output of each hidden layer is 256-dimensional.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Multimodal Fusion",
      "text": "We fuse the multimodal features by concatenating the top-level 256-dimensional textual and acoustic representations. For the emotion recognition tasks, the concatenated representation is fully connected to a single output unit by a task-specific linear layer with a 0.1 dropout rate. For the stress detection task, two output units are used to predict the logits for stress and non-stress labels. A softmax layer is used to compute probabilities and training loss. Note that in related work  (Jaiswal et al., 2020; Bara et al., 2020) , the \"late fusion\" stands for an ensemble method, while MUSER solves the task with a single model.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Muser: Multi-Task Learning",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Weight Sharing",
      "text": "We directly share all the trainable parameters (hard sharing) except the task-specific output layers. For each batch of training samples, only one task is assigned, and one step of back-propagation is performed according to the task objective function with the task-specific output layer plugged in.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Sampling Strategy",
      "text": "In each epoch of multi-task training, different amounts of training data are sampled from both the auxiliary task of activation/valence regression and the main task of stress classification. We explore both uniform sampling and dynamic sampling strategy to adaptively decide the mixing ratio of the multiple tasks in each epoch.\n\nUniform Sampling. In our conditions, the number of training samples in the main task and the auxiliary tasks are approximately on the same scale. Therefore, an intuitive method is to switch between the tasks with uniform sampling: for each batch, we first decide which task to train with an equal chance, and then randomly select 32 (the batch size) samples; the batch is trained with the corresponding supervision signals (either emotion scores or stress labels) from the selected task.\n\nDynamic Sampling. Having an equal number of samples for each task in each epoch is not the most efficient way for multi-task training because it is not searching for the most informative task during each epoch. It is more intuitive that when one task reaches a bottleneck, more samples from the other tasks should be selected instead.\n\nMotivated by this idea, we propose to dynamically select the task for each batch according to the model's speed of learning each task. After each training epoch, the sampling distribution is updated based on the model's current and historical performance on each task on the validation set. Specifically, for activation and valence tasks, we compute the ratio of the average rooted mean square error (RMSE) score on the past n epochs to the RMSE score in the current epoch. The ratios are noted as r a and r v , respectively. For the stress task, we compute the ratio of the accuracy in the current epoch to the average of the past n epochs, noted as r s . The history length n is picked by hand. The sampling distribution for the next epoch is then computed as: where ρ is the temperature coefficient set to 0.1. We use the \"ratios to history\" instead of the validation scores themselves to compute the distribution because this makes different metrics comparable to each other, and it is a good estimation of which task is the model currently learning the fastest. We name this strategy a \"speed-based\" dynamic sampling. The sampling procedure is shown in Figure  2 , and a comparison to look-alike multitask learning methods is included in Table  5 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Settings",
      "text": "We use an AdamW  (Loshchilov and Hutter, 2018)  optimizer with an initial learning rate of 3e-4 for all our multimodal and multi-task experiments. In each epoch, we repeatedly sample data with a batch-size of 32 from the main task or the auxiliary tasks, and apply one-step back-propagation for each batch, until the total selected number reaches the size of the MuSE training set. Gradients are clipped to have a maximum norm of 1.0. The history length n in speed-based dynamic sampling is chosen from {1, 5, 10} according to the performance on the validation set. We warm up the dynamic sampling by applying uniform sampling for the first n epochs. The maximum epoch number is typically set to 1000, while the training process is controlled by early stopping. For the Transformer textual encoder, we limit the maximum sequence length to be 128. The evaluation metrics include overall accuracy, as well as the precision, recall, and f-score for the \"stressed\" class.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Unimodal Results",
      "text": "For unimodal experiments, we use the textual encoder or the acoustic encoder independently to compute representations before regression or classification. For the Transformer textual encoder, we use a learning rate of 2e-5; for the MLP acoustic model, we use a learning rate of 5e-4. These learning rates are separately fine-tuned on each unimodal task. Other hyperparameters of the models are kept the same as the multimodal structure.\n\nTable  6  shows the stress detection results with single modalities. Our Transformer encoder outperforms the baseline textual model because of its capability to discover syntactic-level long distance relationships in natural language and the external linguistic knowledge from the advanced BERT pretraining; our acoustic model also improves beyond the baseline results, potentially because we used a more up-to-date version of eGeMaps configuration and a fine-tuned learning rate.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Multimodal Results",
      "text": "To jointly train with both the textual and acoustic features, we use the multimodal fusion model introduced in Section 4.3 as a basic architecture.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Pre-Training",
      "text": "Our MUSER model is trained from scratch to set up a single-task baseline for multimodal stress detection. Besides, a potential alternative to multitask learning is pre-training on the auxiliary tasks and fine-tuning on the main task. For a complete comparison, we set up several strategies for pretraining. All the pre-training methods use the internal auxiliary task of MuSE. The compared methods are as follows: Activation-100: pre-train for 100 epochs with the activation annotations, then switch to the main task of stress detection. Valence-100: pre-train for 100 epochs with the valence annotations, then switch to the main task of stress detection. Activation-valence-stress: pre-train for 100 epochs on the activation task, then 100 epochs on the valence task, and switch to stress detection. Valence-activation-stress: pre-train for 100 epochs on the valence task, then 100 epochs on the activation task, and switch to stress detection.\n\nThe results are presented in Table  7 . Among the pre-training and fine-tuning results, Activation-100 shows the most significant improvement. The second-best score is the valence-activation-stress order. Thus, we can conclude that activation is the better auxiliary task under this paradigm. Additionally, using only one auxiliary task is always",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Models",
      "text": "Accuracy Precision Recall F-score MLP+LIWC  (Jaiswal et al., 2020)  0.60 0.74 0.61 0.67 MLP+Opensmile  (Jaiswal et al., 2020)  0   (Hayes et al., 2020) . Pre-training on the emotion recognition tasks using either activation or valence improves stress detection because the model is equipped with the capabilities to encode the features and predict emotions before the training of stress detection task starts.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Multi-Task Learning On Muse",
      "text": "For multi-task learning, we compare two sampling strategies: uniform sampling and our proposed speed-based dynamic sampling. We also implement and modify the loss-based weight balancing method proposed by  (Liu et al., 2019)  to adjust the mixing ratios in dynamic sampling instead, and compare it with our methods. The results using the internal MuSE emotion recognition as an auxiliary task are shown in Table  8 .\n\nComparing the uniform sampling results with Table 7, we conclude that using any auxiliary task is better than training from scratch. However, multitask training with the activation and valence tasks together is better than using them separately. This is different from the observations in Table  7  and can be explained by the differences in the training procedure: in multi-task learning, the model looks back-and-forth into each task in each epoch, making it able to memorize the shared knowledge from all the tasks. Additionally, when the model is optimized for the two emotion tasks at the same time, the lower-level representation becomes more general and informative because it is frequently plugged with different task-specific layers.\n\nComparing the results of using a single auxiliary task of activation vs. valence, activation leads to better results as compared to valence, which is in agreement with Table  7 . This is further supported by the analyses in Tables  2  and 4 : given the lower unique indicator count of the activation task, as well as the fact that the pre-training and multi-task learning results are all compatible, we can conclude that for stress detection, the nature of the activation dimension of emotion is closer and more helpful than the valence dimension. This potentially suggests that stress has a major effect on whether people feel excited (activation), but a minor effect on their opinion toward events and objects (valence).\n\nWe test our speed-based dynamic sampling algorithm using activation and valence together as auxiliary tasks and it yields promising results with history set to 5 and 10. It significantly outperforms both the uniform sampling and our implementation of the loss-based strategy  (Liu et al., 2019 ) (ttest, p < 0.05), achieving state-of-the-art scores on MuSE stress detection task with one single model and only two modalities.\n\nOur model works the best with a history length between 5 and 10. If the history is too short, the model takes longer to converge and has unstable performance, while if the history is too long, it fails to capture the dynamics.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Generalization",
      "text": "In real-world applications, stress detection data does not necessarily have emotion labels. However, because of the intrinsic inter-dependence between emotion and stress, any existing dataset with emotion labels can potentially serve as an external auxiliary task for stress detection. However, this requires our model and multi-task training algorithm to generalize beyond the internal MuSE emotion tasks. We test our model on OMG-Emotions as an example of external emotion datasets.\n\nTable  9  shows results on MuSE stress detection using OMG-Emotion as an auxiliary task. Comparing to Table  7 , although the source and content of OMG-Emotions are different from MuSE, multitask learning still outperforms single-task learning and pre-training (t-test, p < 0.05). This reveals that the connection between stress and emotion widely exists, and our multi-task learning method works in general cases.\n\nAdditionally, Table  9  suggests that while using an external emotion dataset, our speed-based sampling method still outperforms uniform sampling, as well as our implementation of loss-based dynamic sampling  (Liu et al., 2019) . This supports the robustness and effectiveness of our speed-based strategy.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we uncovered the connections and differences between stress detection and emotion recognition using textual and acoustic features, and proposed to use emotion recognition as an auxiliary task for stress detection. We proposed MUSER: a Transformer-based model structure, together with a novel speed-based dynamic sampling strategy for multi-task learning. Experimental results support the inter-dependence of stress and emotion (activation/valence), and proves the effectiveness and robustness of our methods. MUSER achieved state-of-the-art results on the MuSE stress detection task both when internal (MuSE) and when external (OMG-Emotions) emotion data and annotations were used.\n\nOur code is publicly available at https://lit.eecs.umich.edu/ downloads.html#MUSER",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Multimodal fusion architecture for MUSER.",
      "page": 5
    },
    {
      "caption": "Figure 2: Dynamic sampling procedure for MUSER multi-task training.",
      "page": 6
    },
    {
      "caption": "Figure 2: , and a comparison to look-alike multi-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: LIWC features that have the top 20 highest Table2:LIWCfeaturesthatareamongthetop20high-",
      "data": [
        {
          "Feature": "nonﬂ\naffect\nsocial\nfamily",
          "Examples": "er, hmm, umm\nhappy, cried, abandon\nmate, talk, child\ndaughter, husband, aunt"
        },
        {
          "Feature": "past\nmoney\ntentat",
          "Examples": "went, ran, had\naudit, cash, owe\nmaybe, perhaps, guess"
        },
        {
          "Feature": "feel\nsad\nnegate\nanger\nachieve\nquant",
          "Examples": "feels, touch\ncrying, grief, sad\nno, not, never\nhate, kill, annoyed\nearn, hero, win\nfew, many, much"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: The features are ranked withspectralfeaturessuchasslope,describinghow",
      "data": [
        {
          "Task": "All",
          "Feature": "Amplitude\nLoudness\nSpectral Flux",
          "Category": "Energy\nEnergy\nSpectral",
          "Examples": "F1/F2/F3 mean amplitude\nmean loudness\nmean ﬂux in voiced regions"
        },
        {
          "Task": "Only in stress",
          "Feature": "Hammarberg Index\nAlpha Ratio\nSlope",
          "Category": "Spectral\nSpectral\nSpectral",
          "Examples": "hammarberg index in unvoiced regions\nmean ratio for unvoiced regions\n500-1500Hz in unvoiced regions"
        },
        {
          "Task": "Only in activation",
          "Feature": "HNR\nVoice Length",
          "Category": "Energy\nTemporal",
          "Examples": "mean HNR (Harmonics-to-Noise Ratio)\nmean voiced segment length"
        },
        {
          "Task": "Only in valence",
          "Feature": "Pitch\nFormant",
          "Category": "Frequency\nFrequency",
          "Examples": "F0 semitone\nfrequency of formant 3"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 7: This is further sup-",
      "data": [
        {
          "Models": "MLP+LIWC (Jaiswal et al., 2020)\nMLP+Opensmile (Jaiswal et al., 2020)",
          "Accuracy": "0.60\n0.67",
          "Precision": "0.74\n0.70",
          "Recall": "0.61\n0.69",
          "F-score": "0.67\n0.69"
        },
        {
          "Models": "MUSER Textual Encoder\nMUSER Acoustic Encoder",
          "Accuracy": "0.69\n0.79",
          "Precision": "0.77\n0.80",
          "Recall": "0.74\n0.79",
          "F-score": "0.75\n0.80"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 7: This is further sup-",
      "data": [
        {
          "Method": "LIWC+Audio (Jaiswal et al., 2020)\nA-modal GRU (Bara et al., 2020)",
          "Accuracy": "0.60\n0.573",
          "Precision": "0.74\n0.582",
          "Recall": "0.61\n0.557",
          "F-score": "0.67\n0.569"
        },
        {
          "Method": "MUSER from scratch\nActivation-100\nValence-100",
          "Accuracy": "0.821\n0.842\n0.823",
          "Precision": "0.834\n0.866\n0.830",
          "Recall": "0.828\n0.832\n0.839",
          "F-score": "0.831\n0.849\n0.834"
        },
        {
          "Method": "Activation-valence-stress\nValence-activation-stress",
          "Accuracy": "0.819\n0.828",
          "Precision": "0.841\n0.854",
          "Recall": "0.813\n0.817",
          "F-score": "0.827\n0.835"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Strategy": "Uniform sampling with activation\nUniform sampling with valence\nUniform sampling with activation & valence",
          "Accuracy": "0.832\n0.823\n0.846",
          "Precision": "0.833\n0.837\n0.862",
          "Recall": "0.857\n0.828\n0.846",
          "F-score": "0.845\n0.832\n0.854"
        },
        {
          "Strategy": "(Liu et al., 2019) - modiﬁed",
          "Accuracy": "0.842",
          "Precision": "0.856",
          "Recall": "0.846",
          "F-score": "0.851"
        },
        {
          "Strategy": "Speed-based sampling (history=5)\nSpeed-based sampling (history=10)",
          "Accuracy": "0.854\n0.856",
          "Precision": "0.861\n0.867",
          "Recall": "0.864\n0.861",
          "F-score": "0.863\n0.864"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Is the human amygdala specialized for processing social information?",
      "authors": [
        "Ralph Adolphs"
      ],
      "year": "2003",
      "venue": "Annals of the New York Academy of Sciences"
    },
    {
      "citation_id": "2",
      "title": "Multimodal stress detection from multiple assessments",
      "authors": [
        "Jonathan Aigrain",
        "Michel Spodenkiewicz",
        "Séverine Dubuiss",
        "Marcin Detyniecki",
        "David Cohen",
        "Mohamed Chetouani"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Towards an automatic early stress recognition system for office environments based on multimodal measurements: A review",
      "authors": [
        "Ane Alberdi",
        "Asier Aztiria",
        "Adrian Basarab"
      ],
      "year": "2016",
      "venue": "Journal of biomedical informatics"
    },
    {
      "citation_id": "4",
      "title": "A deep learning approach towards multimodal stress detection",
      "authors": [
        "Cristian Paul Bara",
        "Michalis Papakostas",
        "Rada Mihalcea"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI-20 Workshop on Affective Content Analysis"
    },
    {
      "citation_id": "5",
      "title": "The omg-emotion behavior dataset",
      "authors": [
        "Pablo Barros",
        "Nikhil Churamani",
        "Egor Lakomkin",
        "Henrique Siqueira",
        "Alexander Sutherland",
        "Stefan Wermter"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "6",
      "title": "Speech as a biomarker for obstructive sleep apnea detection",
      "authors": [
        "Catarina Botelho",
        "Isabel Trancoso",
        "Alberto Abad",
        "Teresa Paiva"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "7",
      "title": "Measuring emotion: the self-assessment manikin and the semantic differential",
      "authors": [
        "M Margaret",
        "Peter Bradley",
        "Lang"
      ],
      "year": "1994",
      "venue": "Journal of behavior therapy and experimental psychiatry"
    },
    {
      "citation_id": "8",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "9",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Detecting driving stress in physiological signals based on multimodal feature analysis and kernel classifiers",
      "authors": [
        "Lan-Lan Chen",
        "Yu Zhao",
        "Jian Peng-Fei Ye",
        "Jun-Zhong Zhang",
        "Zou"
      ],
      "year": "2017",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "11",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "Sheng-Yeh Chen",
        "Chao-Chun Hsu",
        "Chuan-Chun Kuo",
        "Lun-Wei Ku"
      ],
      "year": "2018",
      "venue": "Emotionlines: An emotion corpus of multi-party conversations",
      "arxiv": "arXiv:1802.08379"
    },
    {
      "citation_id": "12",
      "title": "Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks",
      "authors": [
        "Vijay Zhao Chen",
        "Chen-Yu Badrinarayanan",
        "Andrew Lee",
        "Rabinovich"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "13",
      "title": "Psychological stress and susceptibility to the common cold",
      "authors": [
        "Sheldon Cohen",
        "David Aj Tyrrell",
        "Andrew Smith"
      ],
      "year": "1991",
      "venue": "New England journal of medicine"
    },
    {
      "citation_id": "14",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "15",
      "title": "What is stress, and how does it affect reproduction? Animal reproduction science",
      "authors": [
        "Hilary Dobson",
        "Smith"
      ],
      "year": "2000",
      "venue": "What is stress, and how does it affect reproduction? Animal reproduction science"
    },
    {
      "citation_id": "16",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Björn Schuller",
        "Johan Sundberg",
        "Elisabeth André",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "17",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Detecting emotional stress from facial expressions for driving safety",
      "authors": [
        "Hua Gao",
        "Anil Yüce",
        "Jean-Philippe Thiran"
      ],
      "year": "2014",
      "venue": "IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "19",
      "title": "A comparison of loss weighting strategies for multi task learning in deep neural networks",
      "authors": [
        "Ting Gong",
        "Tyler Lee",
        "Cory Stephenson",
        "Venkata Renduchintala",
        "Suchismita Padhy",
        "Anthony Ndirango",
        "Gokce Keskin",
        "H Oguz",
        "Elibol"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Dynamic sampling strategies for multi-task reading comprehension",
      "authors": [
        "Ananth Gottumukkala",
        "Dheeru Dua",
        "Sameer Singh",
        "Matt Gardner"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Dynamic multi-level multi-task learning for sentence simplification",
      "authors": [
        "Han Guo",
        "Ramakanth Pasunuru",
        "Mohit Bansal"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "22",
      "title": "Dynamic task prioritization for multitask learning",
      "authors": [
        "Michelle Guo",
        "Albert Haque",
        "De-An",
        "Serena Huang",
        "Li Yeung",
        "Fei-Fei"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "23",
      "title": "Remind your neural network to prevent catastrophic forgetting",
      "authors": [
        "Kushal Tyler L Hayes",
        "Robik Kafle",
        "Manoj Shrestha",
        "Christopher Acharya",
        "Kanan"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "24",
      "title": "Detecting deception: the promise and the reality of voice stress analysis",
      "authors": [
        "Frank Horvath"
      ],
      "year": "1982",
      "venue": "Journal of Forensic Science"
    },
    {
      "citation_id": "25",
      "title": "Muse-ing on the impact of utterance ordering on crowdsourced emotion annotations",
      "authors": [
        "Mimansa Jaiswal",
        "Zakaria Aldeneh",
        "Cristian-Paul Bara",
        "Yuanhang Luo",
        "Mihai Burzo",
        "Rada Mihalcea",
        "Emily Provost"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "26",
      "title": "Muse: a multimodal dataset of stressed emotion",
      "authors": [
        "Mimansa Jaiswal",
        "Cristian-Paul Bara",
        "Yuanhang Luo",
        "Mihai Burzo",
        "Rada Mihalcea",
        "Emily Provost"
      ],
      "year": "2020",
      "venue": "Proceedings of The 12th Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "27",
      "title": "Multi-task learning for predicting health, stress, and happiness",
      "authors": [
        "Natasha Jaques",
        "Sara Taylor",
        "Ehimwenma Nosakhare",
        "Akane Sano",
        "Rosalind Picard"
      ],
      "year": "2016",
      "venue": "NIPS Workshop on Machine Learning for Healthcare"
    },
    {
      "citation_id": "28",
      "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
      "authors": [
        "Alex Kendall",
        "Yarin Gal",
        "Roberto Cipolla"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "29",
      "title": "Unsupervised machine translation using monolingual corpora only",
      "authors": [
        "Guillaume Lample",
        "Alexis Conneau",
        "Ludovic Denoyer",
        "Marc'aurelio Ranzato"
      ],
      "year": "2017",
      "venue": "Unsupervised machine translation using monolingual corpora only",
      "arxiv": "arXiv:1711.00043"
    },
    {
      "citation_id": "30",
      "title": "Deepear: robust smartphone audio sensing in unconstrained acoustic environments using deep learning",
      "authors": [
        "Petko Nicholas D Lane",
        "Lorena Georgiev",
        "Qendro"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing"
    },
    {
      "citation_id": "31",
      "title": "Stress and emotion recognition using acoustic speech analysis",
      "authors": [
        "Margaret Lech",
        "Ling He"
      ],
      "year": "2014",
      "venue": "Mental Health Informatics"
    },
    {
      "citation_id": "32",
      "title": "Userlevel psychological stress detection from social media using deep neural network",
      "authors": [
        "Huijie Lin",
        "Jia Jia",
        "Quan Guo",
        "Yuanyuan Xue",
        "Qi Li",
        "Jie Huang",
        "Lianhong Cai",
        "Ling Feng"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "End-to-end multi-task learning with attention",
      "authors": [
        "Shikun Liu",
        "Edward Johns",
        "Andrew Davison"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "34",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "35",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "Gary Mckeown",
        "Michel Valstar",
        "Roddy Cowie",
        "Maja Pantic",
        "Marc Schroder"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "36",
      "title": "Phosphatidylethanolamine from phosphatidylserine decarboxylase2 is essential for autophagy under cadmium stress in saccharomyces cerevisiae",
      "authors": [
        "Kannan Muthukumar",
        "Vasanthi Nachiappan"
      ],
      "year": "2013",
      "venue": "Cell biochemistry and biophysics"
    },
    {
      "citation_id": "37",
      "title": "Multitask video captioning with video and entailment generation",
      "authors": [
        "Ramakanth Pasunuru",
        "Mohit Bansal"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "38",
      "title": "How psychological stress affects emotional prosody",
      "authors": [
        "Silke Paulmann",
        "Desire Furnes",
        "Ming Bøkenes",
        "Philip Cozzolino"
      ],
      "year": "2016",
      "venue": "Plos one"
    },
    {
      "citation_id": "39",
      "title": "Linguistic inquiry and word count: Liwc",
      "authors": [
        "Martha James W Pennebaker",
        "Roger Francis",
        "Booth"
      ],
      "year": "2001",
      "venue": "Linguistic inquiry and word count: Liwc"
    },
    {
      "citation_id": "40",
      "title": "Deactivation of the limbic system during acute psychosocial stress: evidence from positron emission tomography and functional magnetic resonance imaging studies",
      "authors": [
        "Jens Pruessner",
        "Katarina Dedovic",
        "Najmeh Khalili-Mahani",
        "Veronika Engert",
        "Marita Pruessner",
        "Claudia Buss",
        "Robert Renwick",
        "Alain Dagher",
        "Michael Meaney",
        "Sonia Lupien"
      ],
      "year": "2008",
      "venue": "Biological psychiatry"
    },
    {
      "citation_id": "41",
      "title": "Coping resources, coping processes, and mental health",
      "authors": [
        "E Shelley",
        "Annette Taylor",
        "Stanton"
      ],
      "year": "2007",
      "venue": "Annu. Rev. Clin. Psychol"
    },
    {
      "citation_id": "42",
      "title": "A preliminary investigation of the relationship between emotion regulation difficulties and posttraumatic stress symptoms",
      "authors": [
        "Heidi Matthew T Tull",
        "Elaine Barrett",
        "Lizabeth Mcmillan",
        "Roemer"
      ],
      "year": "2007",
      "venue": "Behavior Therapy"
    },
    {
      "citation_id": "43",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "44",
      "title": "Emotion regulation and stress",
      "authors": [
        "Manjie Wang",
        "Kimberly Saudino"
      ],
      "year": "2011",
      "venue": "Journal of Adult Development"
    },
    {
      "citation_id": "45",
      "title": "Morse: Multimodal sentiment analysis for real-life settings",
      "authors": [
        "Yiqun Yao",
        "Verónica Pérez-Rosas",
        "Mohamed Abouelenien",
        "Mihai Burzo"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "46",
      "title": "The world in my mind: Visual dialog with adversarial multimodal feature encoding",
      "authors": [
        "Yiqun Yao",
        "Jiaming Xu",
        "Bo Xu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "47",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "48",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    }
  ]
}