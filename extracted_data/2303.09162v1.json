{
  "paper_id": "2303.09162v1",
  "title": "Emotieffnet Facial Features In Uni-Task Emotion Recognition In Video At Abaw-5 Competition",
  "published": "2023-03-16T08:57:33Z",
  "authors": [
    "Andrey V. Savchenko"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this article, the results of our team for the fifth Affective Behavior Analysis in-the-wild (ABAW) competition are presented. The usage of the pre-trained convolutional networks from the EmotiEffNet family for frame-level feature extraction is studied. In particular, we propose an ensemble of a multi-layered perceptron and the LightAutoMLbased classifier. The post-processing by smoothing the results for sequential frames is implemented. Experimental results for the large-scale Aff-Wild2 database demonstrate that our model achieves a much greater macro-averaged F1-score for facial expression recognition and action unit detection and concordance correlation coefficients for valence/arousal estimation when compared to baseline.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The affective behavior analysis in-the-wild (ABAW) problem is an essential part of many intelligent systems with human-computer interaction  [8, 9] . It can be used in online learning to recognize student satisfaction and engagement  [24] , understand users' reactions to advertisements, analyze online event participants' emotions, video surveillance  [25] , etc. Despite significant progress of deep learning in image understanding, video-based prediction of human emotions is still a challenging task due to the absence of large emotional datasets without dirty/uncertain labels.\n\nTo speed up progress in this area, a sequence of ABAW workshops and challenges has been launched  [5, 7, 14] . They introduced several tasks of human emotion understanding based on large-scale AffWild  [11, 28]  and Af-fWild2  [12, 13]  datasets. The recent ABAW-5 competition  [10]  contains an extended version of the Aff-Wild2 database for three uni-task challenges, namely, (1) prediction of two continuous affect dimensions, namely, valence and arousal (VA); (2) facial expression recognition (FER); and (3) detection of action units (AU), i.e., atomic facial muscle actions. It is strictly required to refine the model by using only annotations for a given task, i.e., the multi-task learning on the VA, FER, and AU labels of the AffWild2 dataset is not allowed. As emotions can rapidly change over time, frame-level predictions are required.\n\nThe above-mentioned tasks have been studied in the third ABAW challenge  [6]  held in conjunction with CVPR 2022, hence, there exist several promising solutions for its participants. The baseline for VA prediction is a ResNet-50 pre-trained on ImageNet with a (linear) output layer that gives final estimates for valence and arousal  [10] . Much better results on validation and test sets were achieved by EfficientNet-B0  [22]  pre-trained on AffectNet  [16]  from HSEmotion library  [21] . An ensemble approach with the Gated Recurrent Unit (GRU) and Transformer  [4]  combined using Regular Networks (RegNet)  [17]  took the third place for this task. The runner-up was a cross-modal co-attention model for continuous emotion recognition using visualaudio-linguistic information based on ResNet-50 for spatial encoding and a temporal convolutional network (TCN) for temporal encoding  [29] . Finally, the winning solution utilized two types of encoders to capture the temporal context information in the video (Transformer and LSTM)  [15] .\n\nThe baseline for the FER task is a VGG16 network with fixed convolutional weights, pre-trained on the VGGFACE dataset  [10] . The second place was taken by an ensemble of multi-head cross-attention networks (Distract your Attention Network, DAN)  [2] . A unified transformer-based multimodal framework for AU detection and FER that uses InceptionResNet visual features and DLN-based audio features  [30]  took first place in this competition.\n\nSimilarly to FER, the baseline for AU detection is a VGGFACE network  [10] . A visual spatial-temporal transformer-based model and a convolution-based audio model to extract action unit-specific features were proposed in  [27] . An above-mentioned ensemble approach of GRU and Transformer with RegNets  [17]  took third place. Slightly better results were achieved by the IRes-net100 network that utilized feature pyramid networks and single-stage headless  [3] .\n\nThe winner is again the InceptionResNet-based audiovisual ensemble of the Netease Fuxi Virtual Human team  [30]  In this paper, we propose a novel pipeline suitable for all three tasks of ABAW in the video. The unified representation of a facial emotion state is extracted by a pre-trained lightweight EmotiEffNet model  [19] . These convolutional neural networks (CNN) are tuned on external AffectNet dataset  [16] , so the facial embeddings extracted by this neural network do not learn any features that are specific to the Aff-Wild2 dataset  [12, 13] . Several blending ensembles are studied based on combining embeddings and logits at the output of these models for each video frame  [22, 23] . In addition to MLP (multi-layer perceptron), we examine classifiers from the LightAutoML (LAMA) framework  [26] .\n\nThe remaining part of the paper is organized as follows. The proposed workflow is presented in Section 2. Its experimental study for three tasks from the fifth ABAW challenge is provided in Section 3. Finally, Section 4 contains the conclusion and discussion of future studies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Approach",
      "text": "In this Section, the novel workflow for emotion recognition in video is introduced (Fig.  1 ). At first, the faces are detected with an arbitrary technique, and the representations of affective behavior are extracted from each face by using EfficientNet CNN from HSEmotion library  [21] , such as EmotiEffNet-B0  [22]  or the winner of one of the tasks from ABAW-4, namely, MT-EmotiEffNet-B0  [23] . These models were trained for face identification on the VGGFace2 dataset. Next, they were finetuned to recognize facial expression and, in case of multi-task MT-EmotiEffNetmodel, predict valence/arousal from a static photo by using the Af-fectNet dataset  [16] .\n\nFor simplicity, let us assume that every t-th frame of the video contains a single facial image X(t), where t ∈ {1, 2, ..., T } and T is the total number of frames  [1] . These images are resized and fed into the EmotiEffNet PyTorch models to obtain D-dimensional embeddings x(t), eightdimensional logits for 8 facial expressions l(t) from Affect-Net (Anger, Contempt, Disgust, Fear, Happiness, Neutral, Sadness, Surprise) and valence V (t) ∈ [-1; 1] (how positive/negative a person is) and arousal A(t) ∈ [-1; 1] (how active/passive a person is)  [10] .\n\nNext, these facial representations are used to solve an arbitrary downstream task. In this paper, we examine three problems from the ABAW-5 competition, namely (1) VA prediction (multi-output regression); (  2",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": ") Fer (Multi-Class Classification); And (3) Au Detection (Multi-Class Multilabel Classification).",
      "text": "The supervised learning case is assumed where a training set of N > 1 pairs (X n , y n ), n = 1, 2, ...N is available. Here a facial image X n from the video frame and associated with corresponding labels y n . Here are the details about each task: At first, every training example X n is fed into the same CNN to obtain embeddings x n , FER logits l n and valence/arousal V n , A n . In this paper, the following classifiers are trained: MLP and ensemble models trained via the LAMA library  [26] . The latter tries to find the best pre-processing, classifiers and their ensembles, and postprocessing for an arbitrary classification or regression task. Due to computational complexity and poor metrics obtained after 10 minutes of AutoML search, we do not process embeddings x(t) here. The input of LAMA is a concatenation of logits l(t), valence V (t), and arousal A(t) at the output of the last layer of EmotiEffNets.\n\nThe MLP is trained with the TensorFlow 2 framework similarly to  [22] . VA is better predicted using only logits and valence/arousal by an MLP without a hidden layer and two outputs with tanh activation functions trained to maximize the mean estimate of the Concordance Correlation Coefficient (CCC) for valence CCC V and arousal CCC A .\n\nFER and AU detection are solved similarly by feeding embeddings or logits into the MLP with one hidden layer. In the former case, eight outputs with softmax activations were added, and the weighted sparse categorical cross-entropy is used to fit the classifier. In addition, we examined the possibility to finetune the whole EmotiEffNet CNN on the training set of this challenge using PyTorch source code from the HSEmotion library.\n\nThe output layer for the AU detection task contains 12 units with sigmoid activation functions, and the weighted binary cross-entropy loss was optimized. The final prediction is made by matching the outputs with predefined thresholds. It is possible to either set a fixed threshold (0.5 for each unit) or choose the best threshold for each action unit to maximize F1-score on a validation set. The classifier predicts the class label that corresponds to the maximal output of the softmax layer.\n\nIn all tasks, it is possible to build a simple blending decision rule to combine several different classifiers (Ligh-tAutoML, MLP, finetuned model) and input features (embeddings or logits from pre-trained model). Moreover, pre-trained models were used to make predictions for VA and FER tasks. In the former case, the valence V (t) and arousal A(t) predicted by MT-EmotiEffNet  [23]  were directly used to make a final decision (hereinafter \"pre-trained VA only\"). In the second case due to the difference in classes, namely, absence of contempt emotion and the presence of the state \"Other\" in the AffWild2 dataset, we preliminary apply the MLP classifier to make a binary decision (Other/non-Other). If the predicted class label is not equal to Other, predictions of the pre-trained model from other 7 basic facial expressions are used, i.e., the label that corresponds to the maximal logit (hereinafter \"pre-trained logits\").\n\nThe final decision in the pipeline (Fig.  1 ) is made by smoothing predictions (class probabilities for classification tasks and predicted valence/arousal for regression problem)  [1]  for individual frames by using the box filter with kernel size 2k + 1. Here k is a hyperparameter chosen to maximize performance metrics on the validation set. In fact,",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "!!!\"#$%&'()*+",
      "text": ")*+,-)../0, 123)*+,-)../0, 123)*+,-)../0,45-67,89,+15 we compute the average predictions for the current frame, previous k frames, and next k frames  [22] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Results",
      "text": "Let us discuss the results of our workflow (Fig.  1 ) for three tasks from the fifth ABAW challenge  [10] . The training source code to reproduce the experiments for the presented approach is publicly available 1  .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Valence-Arousal Prediction",
      "text": "A comparison of our workflow based on EmotiEffNet-B0 and MT-EmotiEffNet-B0  [23]  with previous results on the Valence-Arousal estimation challenge is presented in Table  1 . We use official performance metrics from the organizers: CCC for valence, arousal, and their average value\n\nHere, we significantly improved the results of the baseline ResNet-50  [10]  for valence and arousal, respectively. Moreover, our best ensemble model is characterized by 0.05 greater CCC V and 0.07 greater CCC A when compared to the best previous usage of EmotiEffNet models  [22] . The LightAu-toML classifier is worse than simple MLP, but their blending achieves one of the top results. One of the most valuable hyperparameters in our pipeline is the kernel size k of the median filter. The dependence of validation CCCs on k for valence and arousal is shown in Fig.  2  and Fig.  2 , respectively. As one can notice, the highest performance is reached by rather high values of k (25...50), i.e., 51...101 predictions should be averaged for each frame.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Facial Expression Recognition",
      "text": "The macro-averaged F1-scores P EXP R and classification accuracy estimated on the official validation set for various FER techniques are shown in Table  2 . The value of P EXP R depending on the kernel size k for our several best classifiers is presented in Fig.  4 .\n\nIn addition to cropped faces provided by the organizers of this challenge, we used here small (112x112) cropped aligned photos. As one can notice, the quality of FER on the former is much better, so we do not use aligned faces in other experiments. The proposed approach makes it possible to increase F1-score on 20% and 3% when compared to the baseline VGGFACE  [10]  and the previous usage of EfficientNet models  [22] . Again, a large kernel size (50...200) of the mean filter is required to provide the bestpossible F1-score (Fig.  4 ) Surprisingly, MT-EmotiEffNet  [23]  is up to 5% worse than EmotiEffNet, though the former was significantly  more accurate on the multi-task learning challenge from ABAW-4 competition  [5] . It is important to emphasize that the F1-score of our best ensemble (43.3%) is approximately equal to the best single model (43.2%), though the difference in accuracy is significant (55.7% vs 54.6%). Nevertheless, we achieved the greatest validation F1-score, which is 4% higher than the F1-score of the ABAW-3 winner team (Netease Fuxi Virtual Human)  [30] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Action Unit Detection",
      "text": "In the last Subsection, macro-averaged F1-score P AU is estimated for the multi-label classification of action units. The estimates of performance metrics on the validation set are shown in Table  3   experiments, the kernel size k should be much lower (3...5) to achieve the best performance. Indeed, at least one action unit is rapidly changed in typical scenarios.\n\nThe LightAutoML ensemble is again slightly worse than a simple MLP, but their blending leads to excellent results. Our best model is 16% better than the baseline, though we  increase the F1-score of EmotiEffNet compared to its previous usage  [22]  on 1%. However, only the second-place winner team (SituTech) has a higher F1 score on the validation set. Finally, the choice of thresholds can definitely improve the AU detection quality, though it is possible that the current choice of a threshold for each action unit using the validation set is not optimal.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion And Future Works",
      "text": "We proposed the video-based emotion recognition pipeline (Fig.  1 ) suitable for a wide range of affective behavior analysis downstream tasks. It exploits the pretrained EmotiEffNet models to extract representative emotional features from each facial frame. Experiments on datasets from the fifth ABAW challenge showed the benefits of our workflow when compared to the baseline CNNs  [10]  and previous application of EfficientNet models  [22] . For example, our best models achieved average CCC for VA estimation P V A = 55% and macro-averaged F1 scores for FER P EXP R = 43.3% and AU detection P AU = 55.3%.\n\nOur workflow (Fig.  1 ) uses only facial modality and frame-level features. Hence, it is possible to significantly improve the quality metrics by combining it with audio processing  [18, 30]  and/or temporal models  [15, 29] . Another direction for future research is an increase in decisionmaking speed by using sequential inference  [20]  and processing of video frames.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed workﬂow for the video-based facial emotion",
      "page": 2
    },
    {
      "caption": "Figure 1: ). At ﬁrst, the faces are",
      "page": 2
    },
    {
      "caption": "Figure 1: ) is made by",
      "page": 3
    },
    {
      "caption": "Figure 2: Dependence of CCC for valence prediction on the",
      "page": 3
    },
    {
      "caption": "Figure 3: Dependence of CCC for arousal prediction on the",
      "page": 3
    },
    {
      "caption": "Figure 2: and Fig. 2, respectively. As one can notice,",
      "page": 4
    },
    {
      "caption": "Figure 4: In addition to cropped faces provided by the orga-",
      "page": 4
    },
    {
      "caption": "Figure 4: Dependence of F1-score for FER on the smoothing ker-",
      "page": 4
    },
    {
      "caption": "Figure 5: In contrast to previous",
      "page": 4
    },
    {
      "caption": "Figure 5: Dependence of F1-score for AU detection on the",
      "page": 6
    },
    {
      "caption": "Figure 1: ) suitable for a wide range of affective",
      "page": 6
    },
    {
      "caption": "Figure 1: ) uses only facial modality and",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: We use official performance metrics from the or-",
      "data": [
        {
          "!!!\"#$%&’()’*\n!\"&\n!\"#*\n!\"#)\n!\"#(\n!\"#’\n!\"#&\n!\"##\n!\"#%\n!\"#$\n!": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: We use official performance metrics from the or-",
      "data": [
        {
          "!!!\"#$%&’()*+\n!\"$’\n!\"$\n!\"&%\n!\"&$\n!\"&#\n!\"&’\n!\"&\n!\"#%\n!\"#$\n!": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "!\"#$%&’()*+,-\n!\"&$\n!\"&#\n!\"&’\n!\"&\n!\"#%\n!\"#$\n!\"##\n!": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Statistical testing of segment homogeneity in classification of piecewiseregular objects",
      "authors": [
        "S Natalya",
        "Andrey Belova",
        "Savchenko"
      ],
      "year": "2015",
      "venue": "International Journal of Applied Mathematics and Computer Science"
    },
    {
      "citation_id": "2",
      "title": "Classification of facial expression in-the-wild based on ensemble of multi-head cross attention networks",
      "authors": [
        "Jae-Yeop Jeong",
        "Yeong-Gi Hong",
        "Daun Kim",
        "Jin-Woo Jeong",
        "Yuchul Jung",
        "Sang-Ho Kim"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "3",
      "title": "Model level ensemble for facial action unit recognition at the 3rd ABAW challenge",
      "authors": [
        "Wenqiang Jiang",
        "Yannan Wu",
        "Fengsheng Qiao",
        "Liyu Meng",
        "Yuanyuan Deng",
        "Chuanhe Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "4",
      "title": "Exploring efficiency of vision transformers for self-supervised monocular depth estimation",
      "authors": [
        "Aleksei Karpov",
        "Ilya Makarov"
      ],
      "year": "2022",
      "venue": "Proceedings of International Symposium on Mixed and Augmented Reality (ISMAR)"
    },
    {
      "citation_id": "5",
      "title": "ABAW: Learning from synthetic data & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "ABAW: Learning from synthetic data & multi-task learning challenges",
      "arxiv": "arXiv:2207.01138"
    },
    {
      "citation_id": "6",
      "title": "ABAW: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "7",
      "title": "Analysing affective behavior in the first ABAW 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "year": "2020",
      "venue": "Proceedings of 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "8",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "9",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "10",
      "title": "ABAW: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2006",
      "venue": "ABAW: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "arxiv": "arXiv:2303.01498"
    },
    {
      "citation_id": "11",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Expression, affect, action unit recognition: Aff-Wild2, multi-task learning and ArcFace",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-Wild2, multi-task learning and ArcFace",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "13",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "14",
      "title": "Analysing affective behavior in the second ABAW2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Visio (ICCV)"
    },
    {
      "citation_id": "15",
      "title": "Valence and arousal estimation based on multimodal temporal-aware features for videos in the wild",
      "authors": [
        "Liyu Meng",
        "Yuchen Liu",
        "Xiaolong Liu",
        "Zhaopei Huang",
        "Wenqiang Jiang",
        "Tenggan Zhang",
        "Chuanhe Liu",
        "Qin Jin"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "16",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "An ensemble approach for facial behavior analysis inthe-wild video",
      "authors": [
        "Hong-Hai Nguyen",
        "Van-Thong Huynh",
        "Soo-Hyung Kim"
      ],
      "year": "2005",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "18",
      "title": "Phonetic words decoding software in the problem of Russian speech recognition",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2013",
      "venue": "Automation and Remote Control"
    },
    {
      "citation_id": "19",
      "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2021",
      "venue": "Proceedings of International Symposium on Intelligent Systems and Informatics (SISY)"
    },
    {
      "citation_id": "20",
      "title": "Fast inference in convolutional neural networks based on sequential three-way decisions",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2021",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "21",
      "title": "HSEmotion: High-speed emotion recognition library",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2022",
      "venue": "Software Impacts"
    },
    {
      "citation_id": "22",
      "title": "Video-based frame-level facial analysis of affective behavior on mobile devices using Efficient-Nets",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "23",
      "title": "MT-EmotiEffNet for multi-task human affective behavior analysis and learning from synthetic data",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2023",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV) Workshops"
    },
    {
      "citation_id": "24",
      "title": "Classifying emotions and engagement in online learning based on a single facial expression recognition neural network",
      "authors": [
        "Andrey Savchenko",
        "Lyudmila Savchenko",
        "Ilya Makarov"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Organizing multimedia data in video surveillance systems based on face verification with convolutional neural networks",
      "authors": [
        "Angelina Anastasiia D Sokolova",
        "Andrey Kharchevnikova",
        "Savchenko"
      ],
      "year": "2018",
      "venue": "Proceedings of Analysis of Images, Social Networks and Texts (AIST)"
    },
    {
      "citation_id": "26",
      "title": "AutoML solution for a large financial services ecosystem",
      "authors": [
        "Anton Vakhrushev",
        "Alexander Ryzhkov",
        "Maxim Savchenko",
        "Dmitry Simakov",
        "Rinchin Damdinov",
        "Alexander Tuzhilin",
        "Lightautoml"
      ],
      "year": "2021",
      "venue": "AutoML solution for a large financial services ecosystem",
      "arxiv": "arXiv:2109.01528"
    },
    {
      "citation_id": "27",
      "title": "Action unit detection by exploiting spatial-temporal and label-wise attention with transformer",
      "authors": [
        "Lingfeng Wang",
        "Jin Qi",
        "Jian Cheng",
        "Kenji Suzuki"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "28",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "29",
      "title": "Continuous emotion recognition using visual-audio-linguistic information: A technical report for ABAW3",
      "authors": [
        "Su Zhang",
        "Ruyi An",
        "Yi Ding",
        "Cuntai Guan"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "30",
      "title": "Transformerbased multimodal information fusion for facial expression analysis",
      "authors": [
        "Wei Zhang",
        "Feng Qiu",
        "Suzhen Wang",
        "Hao Zeng",
        "Zhimeng Zhang",
        "Rudong An",
        "Bowen Ma",
        "Yu Ding"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    }
  ]
}