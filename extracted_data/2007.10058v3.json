{
  "paper_id": "2007.10058v3",
  "title": "It'S Levasa Not Leviosa! Latent Encodings For Valence-Arousal Structure Alignment",
  "published": "2020-07-20T12:52:26Z",
  "authors": [
    "Surabhi S. Nath",
    "Vishaal Udandarao",
    "Jainendra Shukla"
  ],
  "keywords": [
    "Affective computing",
    "representation learning",
    "variational autoencoder",
    "valence",
    "arousal",
    "circumplex model"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In recent years, great strides have been made in the field of affective computing. Several models have been developed to represent and quantify emotions. Two popular ones include (i) categorical models which represent emotions as discrete labels, and (ii) dimensional models which represent emotions in a Valence-Arousal (VA) circumplex domain. However, there is no standard for annotation mapping between the two labelling methods. We build a novel algorithm for mapping categorical and dimensional model labels using annotation transfer across affective facial image datasets. Further, we utilize the transferred annotations to learn rich and interpretable data representations using a variational autoencoder (VAE). We present \"LeVAsa\", a VAE model that learns implicit structure by aligning the latent space with the VA space. We evaluate the efficacy of LeVAsa by comparing performance with the Vanilla VAE using quantitative and qualitative analysis on two benchmark affective image datasets. Our results reveal that LeVAsa achieves high latent-circumplex alignment which leads to improved downstream categorical emotion prediction. The work also demonstrates the trade-off between degree of alignment and quality of reconstructions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are intrinsic characteristics of mammals, particularly overt in human behaviour  [5, 17, 29] . Intelligent systems must employ means to incorporate emotions for a more natural interaction  [31] . This surge for \"emotional intelligence\" has evolved into the field of affective computing, which by definition encompasses the creation of and interaction with machines that can sense, recognize, respond to, and influence emotions  [32] . Several models of emotion have been developed over the years, which are considered as the backbone of affective computing  [11, 12, 25, 40] . Among these models, a popular choice is the Categorical Model which describes six basic discrete emotions, namely, happiness, anger, disgust, sadness, fear, and surprise  [8] . However, this model failed to capture relations between the discrete emotions. Moreover, there is a lack of consistency in the choice of these fundamental emotions  [7] . As a result, Russell & Mehrabian  [35]  developed the Dimensional Model which suggests that each emotional state can be defined in terms of Valence (pleasure of an emotion), Arousal (energy of an emotion) and Dominance (controlling nature of an emotion). The Dominance dimension is commonly ignored since the valencearousal (VA) dimensional model was shown to possess adequate reliability, convergent validity, and discriminant validity  [36] . This led to the conceptualization of the Circumplex Model to represent affective states as a circle in a 2D bipolar VA space  [34] . The VA variables are typically considered independent  [10] . Motivation: The existence of different models of emotions result in a range of possible annotation strategies for affective data  [9, 21, 24, 27, 28] . This poses two challenges: (i) building deep models on affective data, and (ii) drawing collective insights from multiple datasets having potentially different formats of annotations  [6] .\n\nIn this paper, we present a novel algorithm for mapping annotations of the Categorical Model to those of the Dimensional Model through annotation transfer across affective facial image datasets. The subsequent step following annotation mapping is to obtain meaningful representations. With the increased use of deep neural networks and generative models, there have been significant advances in emotion modelling and affective computing  [13, 19, 33] . Variational Autoencoders (VAEs)  [20]  are known to yield disentangled latent representations and generate new data samples  [15, 16, 39] . They have been used extensively in affective computing to represent text, audio, image and electroencephalography (EEG) data  [22, 41] . Applying VAEs on affective facial images to obtain disentangled image representations can (i) provide high quality feature representations for downstream tasks  [2, 30] , and (ii) serve applications like facial editing and data augmentation  [23] . In our study, we obtain interpretable features by aligning the latent space of a VAE with the VA space. We show that this improves performance in downstream tasks of affect classification and regression, as demonstrated on two benchmark affective image datasets. Our major contributions are (i) an annotation transfer algorithm for label transfer between Categorical and Dimensional models of emotion and (ii) a regularised VAE model \"LeVAsa\" (Latent Encodings for Valence-Arousal Structure Alignment) that yields an interpretable latent space with an implicit structure aligned with the VA space.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "Here we present our annotation transfer algorithm and VAE model architectures. Our code and models are publically available 1  .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Annotation Transfer Algorithm",
      "text": "For the task of annotation transfer between Categorical and Dimensional emotion models, we use an external reference dataset (ğ· ğ‘Ÿ ) containing both discrete categorical emotion labels (ğ‘™ ğ‘– âˆˆ {ğ‘’ 1 , ğ‘’ 2 , ..., ğ‘’ ğ‘› }, where ğ‘’ 1 , ğ‘’ 2 , ..., ğ‘’ ğ‘› are the n discrete emotional labels) and valence, arousal values (ğ‘£ ğ‘– âˆˆ [ğ‘™ğ‘™ğ‘–ğ‘š ğ‘£ , ğ‘¢ğ‘™ğ‘–ğ‘š ğ‘£ ], ğ‘ ğ‘– âˆˆ [ğ‘™ğ‘™ğ‘–ğ‘š ğ‘ , ğ‘¢ğ‘™ğ‘–ğ‘š ğ‘ ], where ğ‘™ğ‘™ğ‘–ğ‘š ğ‘£ , ğ‘¢ğ‘™ğ‘–ğ‘š ğ‘£ , ğ‘™ğ‘™ğ‘–ğ‘š ğ‘ , ğ‘¢ğ‘™ğ‘–ğ‘š ğ‘ are the lower and upper limits for valence and arousal values respectively). Each data sample ğ‘¥ ğ‘– âˆˆ ğ· ğ‘Ÿ thus has an emotion label ğ‘™ ğ‘– , a valence value ğ‘£ ğ‘– and an arousal value ğ‘ ğ‘– . ğ· ğ‘Ÿ serves as the standard based on which continuous or discrete VA values can be sampled for data points in a working dataset (ğ·) with only emotion labels (Algorithm 1, Line 4), or conversely, the most likely emotion labels can be obtained for data points in dataset ğ· â€² with only VA tuples (Algorithm 1, Line 6). The ellipse sizes (on average 3% of total area) ensure that the sampled VA values allow both sufficient variability and consistency within emotion classes.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Algorithm 1: Annotation Transfer Algorithm",
      "text": "Input :\n\nworking dataset ğ· with discrete emotion labels, working dataset ğ· â€² with VA tuples Output : VA values for the working dataset ğ·, discrete emotion labels for the working dataset ğ· â€²",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Vae Model Architectures",
      "text": "We train a generative model with an interpretable latent space with an implicit structure given a raw distribution of affective face images. We employ variational autoencoder based models because of their simple training protocols and structured inductive priors. We compare two VAE models, Vanilla VAE and LeVAsa. This comparison is justified because: (1) No baseline VA structured latent    2 ) the Vanilla VAE features being unbiased (owing to no explicit supervision) may perform better on the downstream tasks. The latent space for both models was constructed to comprise of three chunks. Figure  1  depicts our model architectures.\n\nFor the Vanilla VAE, no explicit alignment was imposed on the latent space, whereas for LeVAsa, we take inspiration from recent work  [4, 18]  and model the latent space as follows:\n\nâ€¢ ğ‘ ğ‘£ -subspace consisting of valence attributes ğ‘§ ğ‘£ that learn to encode the valence features of image samples â€¢ ğ‘ ğ‘ -subspace consisting of arousal attributes ğ‘§ ğ‘ that learn to encode the arousal features of image samples â€¢ ğ‘ ğ‘§ -subspace consisting of other miscellaneous generative attributes ğ‘§ ğ‘§ that are required for high-fidelity reconstruction of the input data distribution.\n\nGiven a dataset of ğ‘ affective images ğ‘‹ = {ğ‘¥ 1 , ğ‘¥ 2 , ..., ğ‘¥ ğ‘ }, our VAE backbone consists of encoder ğ‘“ ğœƒ and decoder ğ‘” ğœ™ given by:\n\nWe train the Vanilla VAE with a simple reconstruction loss along with a modified Kullback-Leibler (KL) loss (Eq. 1). We induce a N (0, ğ‘° ) prior on all three attributes ğ‘§ ğ‘£ , ğ‘§ ğ‘ and ğ‘§ ğ‘§ .\n\nWe employ the same backbone Vanilla VAE architecture for the LeVAsa model with two major modifications:\n\n(1) Projection Heads: We use two non-linear projection heads â„ ğ‘£ and â„ ğ‘ which map the encoded valence and arousal representations ğ‘§ ğ‘£ and ğ‘§ ğ‘ to the valence and arousal label space (giving label representations ğ‘Ÿ ğ‘£ and ğ‘Ÿ ğ‘ ). The projections obtained are represented as follows:\n\n(2) VA-regularization loss: To impose an explicit alignment of the ğ‘§ ğ‘£ and ğ‘§ ğ‘ attributes with the VA ground truth factors, we introduce a VA-regularization loss as follows:\n\nwhere L takes the form of MSE for continuous and BCE for discrete annotation types. MSE/BCE are design choices and can be replaced by suitable likelihood-based loss functions. The overall optimization objective for the LeVAsa model is:\n\nwhere ğœ† ğ¾ğ¿ and ğœ† ğ¶ are hyperparameters.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "We use the following datasets in our experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Annotation Transfer: Affectnet",
      "text": "â€¢ AffectNet  [26]  is the largest facial expression dataset, with over 420,000 annotated images and contains both continuous VA annotations and discrete emotional labels. The dataset also incorporates a wide diversity in gender, age and ethnicity, hence is an ideal choice for the reference dataset in the annotation transfer algorithm (Algorithm 1). The generated ellipses are shown in Figure  2 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Latent-Circumplex Alignment",
      "text": "We measure the alignment of LeVAsa's ğ‘ ğ‘£ âˆª ğ‘ ğ‘ latent space with the VA ground truths using normalized Euclidean and Manhattan distance metrics for continuous annotations, and Cross Entropy measure for discrete annotations. This helps quantify the degree of latent-circumplex alignment. For the Vanilla VAE, we determine the ğ‘§ ğ‘£ and ğ‘§ ğ‘ chunks heuristically by considering the two latent chucks which aligned best with the corresponding valence and arousal ground truths. Further, we reduce the dimensionality of the ğ‘§ ğ‘£ and ğ‘§ ğ‘ latent chunks and plot them alongside the ground truth to replicate the circumplex representation.\n\nIt is found that LeVAsa outperformed Vanilla VAE for both continuous and discrete annotations (Table  1 ). This clearly exhibits the superior latent-circumplex alignment achieved by LeVAsa. For discrete annotations, in case of AFEW-VA, the difference between the cross entropy measures of the Vanilla VAE and LeVAsa is greater than in case of IMFDB. This could be attributed to the different image types in both datasets. The circumplex plots (Figure  3 ) for LeVAsa reveal reduced variance and increased alignment with true labels. This validates the quantitative results in Table  1 .  To gain further insights, we assess the regressive power of ğ‘§ ğ‘£ and ğ‘§ ğ‘ by their ability to predict the corresponding VA ground truths. We use Multi Layer Perceptron (MLP) Regression. This analysis applies to continuous annotations hence was conducted only on the models trained on IMFDB dataset with continuous VA values.     2 ). Furthermore, the goodness of fit metrics (explained variance and ğ‘… 2 ) showed better performance in the case of LeVAsa. These results further strengthen our hypothesis.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Categorical Emotion Predictive Power",
      "text": "We predict the discrete emotion labels using different combinations of latent representations obtained from Vanilla VAE and LeVAsa (Table  3 ). Due to lack of discrete emotion labels in the AFEW-VA dataset, it was excluded from this analysis. We randomized the data splits across Continuous and Discrete experiments to ensure an unbiased setup. Model performance is evaluated using classification accuracy. We utilize a simple one-layered MLP to ensure that the accuracy is a direct measure of representation quality and not influenced by the complexity of the classifier.\n\nIt is seen that LeVAsa has significantly better predictive power as compared to the Vanilla VAE. Moreover, for LeVAsa, the VA chunks alone are more informative in emotion prediction as compared to ğ‘§ ğ‘£ âŠ• ğ‘§ ğ‘ âŠ• ğ‘§ ğ‘§ chunks altogether. Also, the improvement in classification accuracy by employing LeVAsa in place of Vanilla VAE can be compared under the continuous and discrete settings. This reveals that LeVAsa representations from the model trained with discrete annotations and BCE loss (Eq. 2) proves to be better at classifying emotion labels. This is due to the discrete nature of emotion labels which correlate well with the model representations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Reconstruction Quality",
      "text": "VAE models are prone to posterior collapse, producing unreliable reconstructions  [14, 37] . Thus, along with analyses of the latent representations, we study the quality of reconstructions (Figure  4 ).\n\nIt is observed that the quality of the reconstructed faces is slightly compromised in the case of LeVAsa as compared to Vanilla VAE. This can be attributed to the slightly higher variance of the learnt LeVAsa decoding distribution  [1, 15] . By Shannon's rate-distortion theory  [3] , there is a trade-off between the distortion (reconstruction quality) and rate (representation quality). Since we are imposing an explicit compression bottleneck on the latent space, it is expected that the reconstruction quality is slightly compromised in order to achieve better interpretability of latent representations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have developed an annotation-transfer algorithm for mapping between Categorical and Dimensional emotion model annotations. Using them, we generated interpretable image features with a VA-regularized VAE model called LeVAsa. We conducted a series of evaluation tasks to verify and validate our experiments and compare performance based on three factors: (i) architecture (Vanilla VAE vs LeVAsa), (ii) dataset (IMFDB vs AFEW-VA), and (iii) nature of annotations (Continuous VA vs Discrete VA). The results showed that the LeVAsa model obtains robust and interpretable representations enabling improved downstream affective task performance. In the future, we hope to extend the annotation-transfer algorithm to action-unit annotations, and test the expressiveness of the representations by performing latent traversals for data augmentation and facial editing.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Model Architecture",
      "page": 2
    },
    {
      "caption": "Figure 1: depicts our model architectures.",
      "page": 2
    },
    {
      "caption": "Figure 2: Model Training: IMFDB, AFEW-VA",
      "page": 3
    },
    {
      "caption": "Figure 2: Ellipses from AffectNet for annotation transfer",
      "page": 3
    },
    {
      "caption": "Figure 3: Circumplex Representation",
      "page": 3
    },
    {
      "caption": "Figure 4: VAE Reconstruction from the five models",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "NathandUdandarao,etal.\nğ\nv z v h v B M C S E E v\nğ¼\nv\nğ\na\nEnc z a Dec\nğ¼\na\nx xÌ‚\nğ\nz z z h a B M C S E E a\nğ¼\nz": "Loss = MSE(x, xÌ‚) + KL(z) + KL(z) + KL(z) + BCE/MSE(h(z), v) + BCE/MSE(h(z), a)\nv a z v v a a",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": ""
        },
        {
          "NathandUdandarao,etal.\nğ\nv z v h v B M C S E E v\nğ¼\nv\nğ\na\nEnc z a Dec\nğ¼\na\nx xÌ‚\nğ\nz z z h a B M C S E E a\nğ¼\nz": "Vanilla VAE LeVAsa",
          "Column_2": "",
          "Column_3": "Vanilla VAE",
          "Column_4": "",
          "Column_5": "LeVAsa",
          "Column_6": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "h\nv": "",
          "BCE": "MSE",
          "v": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "x"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "h\na": "",
          "BCE": "MSE",
          "a": ""
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Fixing a broken ELBO",
      "authors": [
        "Alexander Alemi",
        "Ben Poole",
        "Ian Fischer",
        "Joshua Dillon",
        "Rif Saurous",
        "Kevin Murphy"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "2",
      "title": "Representation learning: A review and new perspectives",
      "authors": [
        "Yoshua Bengio",
        "Aaron Courville",
        "Pascal Vincent"
      ],
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "3",
      "title": "Rate-distortion theory",
      "authors": [
        "Toby Berger"
      ],
      "year": "2003",
      "venue": "Wiley Encyclopedia of Telecommunications"
    },
    {
      "citation_id": "4",
      "title": "DisCont: Self-Supervised Visual Attribute Disentanglement using Context Vectors",
      "authors": [
        "Sarthak Bhagat",
        "Vishaal Udandarao",
        "Shagun Uppal"
      ],
      "year": "2020",
      "venue": "DisCont: Self-Supervised Visual Attribute Disentanglement using Context Vectors",
      "arxiv": "arXiv:2006.05895"
    },
    {
      "citation_id": "5",
      "title": "The expression of the emotions in man and animals",
      "authors": [
        "Charles Darwin",
        "Phillip Prodger"
      ],
      "year": "1998",
      "venue": "The expression of the emotions in man and animals"
    },
    {
      "citation_id": "6",
      "title": "Joint Emotion Label Space Modelling for Affect Lexica",
      "authors": [
        "Luna De Bruyne",
        "Pepa Atanasova",
        "Isabelle Augenstein"
      ],
      "year": "2019",
      "venue": "Joint Emotion Label Space Modelling for Affect Lexica",
      "arxiv": "arXiv:1911.08782"
    },
    {
      "citation_id": "7",
      "title": "What is meant by calling emotions basic",
      "authors": [
        "Paul Ekman",
        "Daniel Cordaro"
      ],
      "year": "2011",
      "venue": "Emotion review"
    },
    {
      "citation_id": "8",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "9",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "Fabian Benitez-Quiroz",
        "Ramprakash Srinivasan",
        "Aleix Martinez"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "10",
      "title": "Independence and bipolarity in the structure of current affect",
      "authors": [
        "Lisa Feldman",
        "James Russell"
      ],
      "year": "1998",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "11",
      "title": "Assessing the validity of appraisal-based models of emotion",
      "authors": [
        "Jonathan Gratch",
        "Stacy Marsella",
        "Ning Wang",
        "Brooke Stankovic"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "12",
      "title": "Mapping discrete and dimensional emotions onto the brain: controversies and consensus",
      "authors": [
        "Stephan Hamann"
      ],
      "year": "2012",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "13",
      "title": "Adversarial training in affective computing and sentiment analysis: Recent advances and perspectives",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "Bjorn Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "14",
      "title": "Lagging inference networks and posterior collapse in variational autoencoders",
      "authors": [
        "Junxian He",
        "Daniel Spokoyny",
        "Graham Neubig",
        "Taylor Berg-Kirkpatrick"
      ],
      "year": "2019",
      "venue": "Lagging inference networks and posterior collapse in variational autoencoders",
      "arxiv": "arXiv:1901.05534"
    },
    {
      "citation_id": "15",
      "title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
      "authors": [
        "Irina Higgins",
        "LoÃ¯c Matthey",
        "Arka Pal",
        "Christopher Burgess",
        "Xavier Glorot",
        "Matthew Botvinick",
        "Shakir Mohamed",
        "Alexander Lerchner"
      ],
      "year": "2017",
      "venue": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework"
    },
    {
      "citation_id": "16",
      "title": "Disentangling factors of variation by mixing them",
      "authors": [
        "Qiyang Hu",
        "Attila SzabÃ³",
        "Tiziano Portenier",
        "Paolo Favaro",
        "Matthias Zwicker"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "Human emotions",
      "authors": [
        "Carroll E Izard"
      ],
      "year": "2013",
      "venue": "Human emotions"
    },
    {
      "citation_id": "18",
      "title": "Disentangling factors of variation with cycle-consistent variational autoencoders",
      "authors": [
        "Ananya Harsh",
        "Saket Anand",
        "Maneesh Singh",
        "Veeravasarapu"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "19",
      "title": "Universal EEG Encoder for Learning Diverse Intelligent Tasks",
      "authors": [
        "Baani Leen",
        "Kaur Jolly",
        "Palash Aggrawal",
        "S Surabhi",
        "Viresh Nath",
        "Manraj Gupta",
        "Rajiv Ratn Singh Grover",
        "Shah"
      ],
      "year": "2019",
      "venue": "2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM)"
    },
    {
      "citation_id": "20",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "P Diederik",
        "Max Kingma",
        "Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "21",
      "title": "AFEW-VA database for valence and arousal estimation in-the-wild",
      "authors": [
        "Jean Kossaifi",
        "Georgios Tzimiropoulos",
        "Sinisa Todorovic",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "22",
      "title": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Junaid Qadir",
        "Julien Epps"
      ],
      "year": "2017",
      "venue": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "arxiv": "arXiv:1712.08708"
    },
    {
      "citation_id": "23",
      "title": "Facial expression editing with continuous emotion labels",
      "authors": [
        "Alexandra Lindt",
        "Pablo Barros",
        "Henrique Siqueira",
        "Stefan Wermter"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "24",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognition-workshops"
    },
    {
      "citation_id": "25",
      "title": "Computational models of emotion. A Blueprint for Affective Computing-A sourcebook and manual",
      "authors": [
        "Stacy Marsella",
        "Jonathan Gratch",
        "Paolo Petta"
      ],
      "year": "2010",
      "venue": "Computational models of emotion. A Blueprint for Affective Computing-A sourcebook and manual"
    },
    {
      "citation_id": "26",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Audio-visual classification and fusion of spontaneous affective data in likelihood space",
      "authors": [
        "A Mihalis",
        "Hatice Nicolaou",
        "Maja Gunes",
        "Pantic"
      ],
      "year": "2010",
      "venue": "2010 20th International Conference on Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space",
      "authors": [
        "A Mihalis",
        "Hatice Nicolaou",
        "Maja Gunes",
        "Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Affective neuroscience: The foundations of human and animal emotions",
      "authors": [
        "Jaak Panksepp"
      ],
      "year": "2004",
      "venue": "Affective neuroscience: The foundations of human and animal emotions"
    },
    {
      "citation_id": "30",
      "title": "Elements of causal inference",
      "authors": [
        "Jonas Peters",
        "Dominik Janzing",
        "Bernhard SchÃ¶lkopf"
      ],
      "year": "2017",
      "venue": "Elements of causal inference"
    },
    {
      "citation_id": "31",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "32",
      "title": "Computers that recognise and respond to user emotion: theoretical and practical implications",
      "authors": [
        "Rosalind Picard",
        "Jonathan Klein"
      ],
      "year": "2002",
      "venue": "Interacting with computers"
    },
    {
      "citation_id": "33",
      "title": "Deep learning for human affect recognition: insights and new developments",
      "authors": [
        "Marc Philipp V Rouast",
        "Raymond Adam",
        "Chiong"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "35",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "A James",
        "Albert Russell",
        "Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "36",
      "title": "Affect grid: a single-item scale of pleasure and arousal",
      "authors": [
        "Anna James A Russell",
        "Gerald Weiss",
        "Mendelsohn"
      ],
      "year": "1989",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "37",
      "title": "Simple and Effective VAE Training with Calibrated Decoders",
      "authors": [
        "Kostas Oleh Rybkin",
        "Sergey Daniilidis",
        "Levine"
      ],
      "year": "2020",
      "venue": "Simple and Effective VAE Training with Calibrated Decoders",
      "arxiv": "arXiv:2006.13202"
    },
    {
      "citation_id": "38",
      "title": "Indian movie face database: a benchmark for face recognition under wide variations",
      "authors": [
        "Moula Shankar Setty",
        "Parisa Husain",
        "Jyothi Beham",
        "Menaka Gudavalli",
        "Radhesyam Kandasamy",
        "Vidyagouri Vaddi",
        "J Hemadri",
        "Raja Karure",
        "Raju",
        "Rajan"
      ],
      "year": "2013",
      "venue": "2013 fourth national conference on computer vision, pattern recognition, image processing and graphics (NCVPRIPG)"
    },
    {
      "citation_id": "39",
      "title": "Product of Orthogonal Spheres Parameterization for Disentangled Representation Learning",
      "authors": [
        "Ankita Shukla",
        "Sarthak Bhagat",
        "Shagun Uppal",
        "Saket Anand",
        "Pavan Turaga"
      ],
      "year": "2019",
      "venue": "BMVC"
    },
    {
      "citation_id": "40",
      "title": "Four models of basic emotions: a review of Ekman and Cordaro, Izard, Levenson, and Panksepp and Watt",
      "authors": [
        "L Jessica",
        "Daniel Tracy",
        "Randles"
      ],
      "year": "2011",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "41",
      "title": "Semi-supervised dimensional sentiment analysis with variational autoencoder",
      "authors": [
        "Chuhan Wu",
        "Fangzhao Wu",
        "Sixing Wu",
        "Zhigang Yuan",
        "Junxin Liu",
        "Yongfeng Huang"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems"
    }
  ]
}