{
  "paper_id": "2002.03898v2",
  "title": "Self-Supervised Ecg Representation Learning For Emotion Recognition",
  "published": "2020-02-04T17:15:37Z",
  "authors": [
    "Pritam Sarkar",
    "Ali Etemad"
  ],
  "keywords": [
    "Self-supervised Learning",
    "ECG",
    "Emotion Recognition",
    "Multi-task Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We exploit a self-supervised deep multi-task learning framework for electrocardiogram (ECG) -based emotion recognition. The proposed solution consists of two stages of learning a) learning ECG representations and b) learning to classify emotions. ECG representations are learned by a signal transformation recognition network. The network learns high-level abstract representations from unlabeled ECG data. Six different signal transformations are applied to the ECG signals, and transformation recognition is performed as pretext tasks. Training the model on pretext tasks helps the network learn spatiotemporal representations that generalize well across different datasets and different emotion categories. We transfer the weights of the self-supervised network to an emotion recognition network, where the convolutional layers are kept frozen and the dense layers are trained with labelled ECG data. We show that the proposed solution considerably improves the performance compared to a network trained using fully-supervised learning. New state-of-the-art results are set in classification of arousal, valence, affective states, and stress for the four utilized datasets. Extensive experiments are performed, providing interesting insights into the impact of using a multi-task self-supervised structure instead of a single-task model, as well as the optimum level of difficulty required for the pretext self-supervised tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective computing is a field of study that deals with understanding human emotions, intelligent human-machine interaction, and computer-assisted learning among others  [1] ,  [2] . The goal of affective computing is to equip machines with the ability to model and interpret the emotional states of humans. Emotion is considered a physiological and psychological expression associated with moods and personalities of individuals. As a result, sensing technologies integrated into wearable devices coupled with machine learning and deep learning techniques have recently been used to analyze physiological signals in order to classify or quantify human emotions  [3] -  [5] . Recent applications of affective computing include human monitoring systems  [6] ,  [7] , stress or anxiety level detection  [8] ,  [9] , emotion and personality recognition  [4] ,  [10] , and others.\n\nA wide variety of data sources have been utilized for affective computing purposes. These data sources include facial expressions  [11] , motion and gait  [12] , speech  [13] , electrocardiogram (ECG)  [14] , electroencephalogram (EEG)  [5] , electrooculogram (EOG)  [15] , and galvanic skin response (GSR)  [16]  among others. In particular, ECG, which is the focus of our study, has been explored in the past with works such as  [14]  showing a strong correlation between emotional attributes and the ECG waveform, and investigating the feasibility and limitations of using ECG signals for emotion detection. Further, recent advancements in machine learning and deep learning have proven ECG to be a 2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works Fig.  1 : An overview of the proposed framework for self-supervised emotion recognition is presented. reliable source of information for emotion recognition systems  [3] ,  [17] .\n\nThe majority of machine learning or deep learning solutions for ECG-based emotion recognition utilize fully-supervised learning methods. Several limitations can be associated with this approach. First and foremost, in a typical setup of fully-supervised learning, the model needs to be trained from scratch for every classification or regression task  [18] , which requires considerable computational resources and time. Additionally, the learned representations from the trained fully supervised models are often very task-specific, which are not expected to generalize well for other tasks. Lastly, fully supervised learning generally requires training using large human-annotated datasets, since small datasets typically result in poor performance with deeper networks.\n\nTo tackle these problems in the context of ECG-based affective computing, inspired by the success of multi-task selfsupervised learning in other domains notably  [19] , we use a deep learning approach based on self-supervised representation learning  [20]  (please see Figure  1 ). In self-supervised learning, models are trained using automatically generated labels instead of human-annotated ones. There are a number of advantages to self-supervised learning. First, the representations learned using this approach are often invariant to inter-and intra-instance variations  [21]  due to learning rather generalized features as opposed to taskspecific ones. As a result, these models can be reused for different tasks within the same domain. This property often improves the performance of the networks and also saves computation time compared to training a model from scratch for each task. Lastly, as self-supervised learning doesn't require human-annotated labels, larger datasets can be acquired and utilized, which in turn results in the ability to train deeper and more sophisticated networks. We use four publicly available datasets, AMIGOS  [4] , DREAMER  [22] , WESAD  [23] , and SWELL  [9]  to perform emotion recognition with ECG. First, a number of transformations are applied to the ECG signals. The automatically generated labels associated with the transformations are then utilized to carry out self-supervised learning of ECG representations through a deep multitask Convolutional Neural Network (CNN). Next, the convolutional layers responsible for learning the ECG representations are frozen and transferred to a second network. The dense layers of this new network are then trained with the labeled ECG data to perform emotion classification.\n\nOur contributions can be summarized as follows:\n\n• We exploit a self-supervised framework for emotion recognition based on multi-task ECG representation learning. While self-supervised techniques have been used for time-series representation learning in other domains (see  [19] ,  [24] ,  [25] ), we propose its use in ECG-based affective computing for the first time. The results show that the self-supervised network outperforms the same network for emotion recognition when trained in a fully-supervised fashion. • We perform very thorough analyses on the parameters associated with the self-supervised learning tasks, providing interesting insights into the impact of different self-supervised tasks and their contribution towards learning of affective ECG representations. Additionally, we show that self-supervised multi-task learning results in better learned representations compared to single-task self-supervised learning. Our analysis also provides interesting insights into the relationship between the difficulty of self-supervised tasks and the original emotion recognition task, where there seems to be an optimum level of difficulty in which the network can learn strong representations in a self-supervised manner. The analysis shows that simpler tasks do not result in proper learning of ECG representations, whereas extremely hard tasks also prevent the network from learning generalized representations. • We set new state-of-the-art results for all the undertaken emotion classification tasks namely arousal, valence, affective states, and stress recognition in the four utilized datasets AMI-GOS, DREAMER, WESAD, and SWELL. We demonstrate that the ECG representations learned by the self-supervised model generalize very well across all four ECG datasets, consistently resulting in accurate emotion recognition.\n\nThis paper is an extension of our work  [26] , compared to which this paper additionally includes the following: a) Two additional publicly available datasets DREAMER and WESAD are used in this paper to further investigate the benefits of the self-supervised learning framework compared to fully-supervised methods; b) The impact of different transformation tasks on the intended emotion recognition tasks are investigated in depth and presented in this paper; c) The relationship between the difficulty of self-supervised tasks and emotion recognition is investigated; d) The benefits of the multi-task architecture for the self-supervised transformation recognition network are compared to a single-task approach, and the results are presented; e) We analyze and discuss the impact of different parameters associated with each of the signal transformations with respect to the network performance; f) An analysis of the relationship between the learned representations and the depth of the proposed network is presented; g) Lastly, the effect of utilizing an aggregation of multiple datasets for self-supervised training as opposed to individual datasets is investigated and presented.\n\nThe rest of the paper is organized as follows. Section 2 discusses the background and related works on emotion recognition with ECG. We describe the methodology in Section 3. The datasets used in this paper and the experiment setup are described in Section 4. Section 5 presents the performance and results of the proposed architecture. A thorough analysis and discussion on the limitations of the framework is presented in Section 6. Section 7 presents the summary and concluding remarks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background And Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Electrocardiography",
      "text": "ECG signals contain information regarding cardiac electrical activity over a period of time, where an ECG beat is composed of a number of prominent waves: the P wave, the QRS complex, the T wave  [14] , and often the U wave  [27] . Each waveform contains significant information that can be used to understand the cardiac state of individuals. To acquire ECG signals, electrodes are placed on the surface of the skin. The most common sensor configurations include 12-lead, 5-lead, 3-lead, or single-channel systems. ECG signals have been widely used to develop deep learning solutions, for example arrhythmia detection  [28] , emotion or stress detection  [14] ,  [29] , biometric identification  [30] , and others. Additionally, research has shown strong correlations between cardiovascular activity and emotions, as emotions can influence the autonomic nervous system which controls the rhythm of heart beats  [14] . To extract useful information from heart beats, different feature extraction techniques have been utilized  [8] ,  [31] . Most common approaches include either utilizing raw ECG signals to directly learn high-level representations  [28] , or extracting time-domain and frequency domain features successive to detection of fiducial points (P, Q, R, S, and T components) from the ECG waveform  [14] ,  [18] . Successive to learning of ECG representations, classifiers such as Multi-Layer Perceptrons are typically used learn the relationships between the high-level representations and the main task  [3] ,  [32] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Ecg-Based Affective Computing",
      "text": "Systems capable of performing affective computing generally aim to classify or quantify the emotional state of individuals based on collected data and information. Recent advancements in affective computing have shown ECG to be a reliable source of information to classify the emotional states of humans  [3] ,  [10] ,  [17] ,  [33] . The type of affective states often studied include stress, happiness, sorrow, and excitement among others.\n\nIn an early work on ECG-based affective computing, stress detection was performed in  [8] , as participants went through a driving task. ECG signals were collected during the study, followed by extraction of time and frequency domain features which were then used to perform classification. In particular, Heart Rate Variability (HRV) features were calculated from power spectrum of ECG signals. The extracted features were used to perform classification using a linear discriminant classifier.\n\nIn a large study, in  [31] , emotion recognition was performed, where ECG signals from 391 subjects were captured, while individuals were shown a movie clip belonging to either the category of joy or sadness. Out of all the participants, 150 subjects were selected who had shown effective elicitation during this study. Next, continuous wavelet transform was performed to accurately detect P-QRS-T wave locations. A total of 79 features were extracted. To perform better and effective classification, feature selection was performed by means of a hybrid particle swarm optimization technique. Their study shows that the selected features can correctly classify emotions with a Fisher classifier, reporting an accuracy of 88.43%.\n\nThe relationship between personality and affect was studied in  [10] . The study was conducted on 58 participants. While subjects were shown emotional movie clips, ECG data were recorded. Similar to  [8] , HRV features were calculated, along with heart rate and inter-beat intervals. These features were utilized to perform classification of arousal and valence scores. Classification was performed by a Support Vector Machine (SVM) and Naive Bayes classifier. The naive Bayes classifier outperformed the SVM, reporting an accuracy of 59% and 60% for arousal and valence respectively.\n\nIn a recent work, deep learning was used to classify cognitive load and expertise of learners in a medical simulation using ECG  [3] . ECG data were recorded from 9 medical practitioners in 2 classes, expert and novice. First, baseline ECG was collected as participants were in rest. Next, ECG signals were collected during simulation where the subjects had to tend to an injured manikin. R-peaks were then detected, followed by the extraction of 11 time-domain features. Additionally, 9 frequency domain features were extracted successive to calculating the power of different frequency bands. Finally, the extracted features were normalized by corresponding baseline features. A deep neural network with 7 layers was employed to perform classification of expertise and cognitive load. The model achieved an accuracy of 89.4% and 96.6% for cognitive load and expertise respectively. The research shows an interesting relationships between level of expertise and cognitive load, indicating that under stressful conditions, experts tend show lower cognitive load compared to novice subjects.\n\nA number of prior works such as  [4] ,  [17] ,  [22] ,  [23] ,  [32] , and  [9]  have used one or more of the four datasets utilized in this paper, AMIGOS, DREAMER, WESAD, and SWELL, for emotion recognition. In  [4] , a Gaussian Naive Bayes (GNB) classifier was used on AMIGOS and reported F1 scores of 54.5% and 55.1% for classifying arousal and valence respectively. To perform a similar study, a CNN was used in  [32] , reporting accuracies of 81% and 71% for arousal and valence classification. An initial study on DREAMER was performed in  [22]  where arousal and valence classification was performed using an SVM classifier and an accuracy of 62.37% was reported. A novel technique was employed in  [17]  to perform emotion recognition on both DREAMER and AMIGOS. Two different sets of features were fused to perform the classification. First HRV features were extracted by calculating RR intervals from the input ECG signals. Next, spectrogram images were generated from time-series ECG signals. A pre-trained VGG-16 was used to extract feature embeddings of size 4096, followed by dimensionality reduction to 30 dimensions. Finally, both sets of features were concatenated and used to perform classification by means of an Extreme Learning Machine (ELM) classifier. Three-level affect state detection was performed in  [23] , where a Linear Discriminant Analysis (LDA) classifier was used on WESAD and reported an accuracy of 66.29%. Later,  [34]  proposed a Mutlimodal-Multisensory Sequential Fusion (MMSF) model, which showed that the proposed model can be trained on different signals with different sampling rates in the same training batch. When utilizing only ECG, an accuracy of 83% was reported for detection of affect state on WESAD. In  [35] , stress detection was performed in SWELL. Feature analysis was performed on extracted time domain and frequency domain features. Time domain feature NN50 showed the best performance compared to other features and reported an accuracy of 86.36% by utilizing an SVM classifier.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Self-Supervised Representation Learning",
      "text": "Recent research in the area of machine learning and deep learning has shown the potential of self-supervised models in learning generalized and robust representations. This concept has been utilized in computer vision  [21] ,  [36] -  [39] , speech  [25] , natural language processing  [40] , and others. Self-supervised learning is a machine learning paradigm in which a network is trained using automatically generated labels instead of human-annotated labels. Different techniques have been employed to generate automatic labels. For example, a self-supervised network was proposed in  [41] , where high-level spatiotemporal features were learned from unlabelled videos. The network was trained using different rotated video clips to predict rotations. Next, the trained model was finetuned on the action recognition datasets. Their research showed that activity recognition accuracy was improved by more than 15% on both utilized datasets, compared to a network trained in a fullysupervised manner. In another work in  [36] , 3D pose estimation was tackled using self-supervised learning. To overcome the need for large amounts of 3D ground truth data generally required for 3D pose estimation, epipolar geometry was employed to calculate 3D poses from the available 2D poses. Subsequently, a convolution neural network was trained using the available 3D poses and reported state-of-the-art results. Self-supervised learning has also been used in learning wearable data. For example, human activity recognition from Inertial Measurement Units (IMU) was performed in  [19] . A set of different signal transformations were performed to train a convolutional neural network to predict transformations. Further, features were extracted from the trained network, and fully-connected layers were utilized to learn different activities such as walking, sitting, and jogging among others using the learned representations.\n\nThe works presented above, among others, demonstrate the advantages and promising results obtained using self-supervised learning in other domains. Nonetheless, such a framework has not been proposed for ECG representation learning, which we undertake in this paper in the context of emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Self-Supervised Solution",
      "text": "Our goal in this paper is to learn ECG representations R ECG capable of distinguishing between different classes of emotion. The proposed framework consists of two stages of learning: first, learning ECG representation, and second, using the network capable of extracting ECG representation to learn another network capable of classifying emotions. Accordingly, we define two sets TABLE 1: The architecture of the signal transformation recognition network is presented.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Module",
      "text": "Layer Details Feature Shape\n\nof tasks, pretext and downstream, referred to as T p and T d respectively, where T p is the set of signal transformation recognition tasks through which ECG representations are learned, while T d learns to classify emotions. The goal of T p is to learn robust generalized features from unlabelled ECG data through a selfsupervised process, i.e. recognition of signal transformations. In the second stage in which T d is learned, the original ECG data and the human-annotated emotion labels y i are used. In this stage, the framework uses the frozen convolution layers of the first network to learn emotional classes upon supervised learning of the fullyconnected layers at the end of the network. Figure  2  shows the proposed self-supervised solution. In the following subsections, we further describe the architecture and details of each learning stage. The optimum architecture and hyperparameters for these two networks described below are selected either empirically or based on searching among a large number of combinations for optimum performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Self-Supervised Learning Of Ecg Representations",
      "text": "We propose the use of a self-supervised deep multi-task CNN to learn R ECG , through recognition of different transformations applied to input ECG signals (without utilizing any of humanannotated labels provided with the data). A similar approach was used in  [19]  for human activity recognition. Let's denote an example tuple of inputs and pseudo-labels for T p as (X j , P j ) where X j is j th transformed signal, P j is the automatically generated label according to the j th transformation, and j ∈ [0, N ] where N is the total number of signal transformations. To learn T p , a signal transformation recognition network, γ(X j , P j , θ) is trained, where θ is the set of trainable parameters. The model learns θ by minimizing the total loss, L total , composed of the weighted average of the individual losses of the signal transformation network L j . The total network loss is defined by:\n\nwhere ψ j and α j are the predicted probability and loss coefficient of the j th transformation task respectively. Six different signal transformation recognition tasks are performed as pretext tasks  [19] ,  [42] . The details of the parameters associated with these tasks (for instance signal-to-noise ratio, scaling factor, stretching factor, and others) are tuned based on a large number of experiments and are explored in depth in Section 6.1 and 6.2. These transformations are described below and a sample of transformed signals with automatically generated labels is illustrated in Figure  3 .\n\nNoise addition: In this transformation, random noise from a Gaussian distribution, N (t), is added to the original ECG signal S(t). N (t) is obtained from a random noise generator where the mean of the distribution is set to 0, and the standard deviation to E Navg . Here, the average power of N (t), E Navg is calculated as 10 (E Savg -α)/10 , where E Savg is the average power of S(t) and α is the desired Signal to Noise Ratio (SNR). Finally the noise-added signal is generated as S(t) + N (t).\n\nScaling: The original magnitude of the ECG S(t) is transformed as β × S(t), where β > 0 and β is the manually assigned scaling factor.\n\nNegation: The original amplitude of the ECG S(t) is multiplied by -1, causing a spatial inversion of the time-series. The transformed signal can be mathematically expressed as -S(t).\n\nTemporal Inversion: Given the original ECG signal S(t), where t = 1, 2, ..., N and N is the length of the time-series, the temporally inverted version of the signal is expressed as S (t), where t = N, N -1, ..., 1.\n\nPermutation: In this transformation, the original ECG, S(t), is divided into m segments and shuffled, randomly perturbing the temporal location of each segment. Let's assume S(t) is expressed as S(t) = [s i (t)|i = 1, 2, ..., m], a sequence of segments with segment numbers i = 1, 2, ..., m. Accordingly, the permuted signal S p (t) can be obtained as S p (t) = [s i (t)], where the sequence of i = 1, 2, ..., m is randomly shuffled.\n\nTime-warping: In this transformation, randomly selected segments of the original ECG signals S(t) are stretched or squeezed along the x (time) axis. Let's assume Φ(S(t), k) is an interpolation-based time-warping function where k is the stretch factor (with the corresponding squeeze factor represented as 1/k). If the signal, S(t), is expressed as S(t) = [s i (t)|i = 1, 2, ..., m], a sequence of segments with segment numbers i = 1, 2, ..., m, the time-warped (transformed) signal T (t) is obtained by applying Φ(s i (t), k) to half of the segments selected randomly, and Φ(s j (t), 1/k) to the other half of the segments where i = j. Where T (t) becomes longer or shorter than S(t) due to the even or odd nature of m, T (t) is clipped or zero-padded accordingly to maintain the original input length.\n\nAll of the above-mentioned transformations are applied to the original signals, the outcome of which are stacked to create the input matrix X, while the corresponding labels of the transformations 0, 1, ..., 6 are stacked to create the corresponding output matrix P , with 0 denoting the original matrix and integers 1 to 6 indicating the 6 transformations. The input-output matrices are then shuffled to re-order the transformations and their corresponding outputs.\n\nThe architecture of the proposed network for learning ECG representations using the created transformation dataset described above consists of 3 convolutional blocks (conv-block) as shared layers, and 7 branches, each with 2 dense layers, as task-specific layers. Each conv-block is made of 2 × 1D convolution layers with ReLu activation functions followed by a max-pooling layer of size 8. The number of filters are increased from 32 to 64 and 128, whereas, the kernel size is decreased from 32 to 16 and 8 respectively. Global max-pooling operation is performed after the final conv-block. Next, successive to flattening the embedding, the output is fed to 2 fully connected layers with 128 hidden nodes and ReLu activation functions. To overcome possible overfitting, 60% dropout is introduced. Finally, the model output is generated Fig.  2 : The self-supervised architecture is presented. First, a multi-task CNN is trained using automatically generated labels to learn ECG representations. Then, the weights are transferred to the emotion recognition network, where the fully connected layers are trained to classify emotions. through a sigmoid layer. The summary of the network architecture is presented in Table  1 . Through learning the set of synthesized input-outputs described above, the network can learn strong ECG representations, which can be obtained by flattening the feature embeddings extracted after the shared convolutional blocks.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Emotion Recognition",
      "text": "In the second stage (T d ) of learning, the model is trained with the true emotion labels y i for emotion classification. R ECG obtained from T p contain useful information regarding the original signals X 0 , which are used to perform T d . To utilize R ECG towards performing T d , a simple network ρ = w(R ECG , y i , θ ) is employed, with ρ as the probability vector of T d classes, and θ as the set of trainable parameters. Finally, we calculate the optimum value of θ by minimizing the cross entropy loss:\n\nwhere M is total number of emotional classes.\n\nThe emotion recognition network contains convolution layers similar to those used in learning ECG representation, followed by fully connected layers with 512 hidden nodes. The weights of the shared layers of the signal transformation recognition network are frozen and transferred to the emotion recognition network. However, the fully connected layers of the emotion recognition network are not carried over from the transformation recognition network, and are instead trained using the labelled dataset (fully supervised) to perform emotion recognition. It should be noted that as we use different datasets in this study, the fully-connected layers are fine-tuned for each dataset. Specifically, 2 dense layers are utilized for SWELL and WESAD datasets, while 3 dense layers with L2 regularization (0.0001) and dropout of 40% and 20% are used for AMIGOS and DREAMER datasets respectively. The final output is taken from a sigmoid (binary classification) or softmax (multi-class classification) layer. We intentionally kept the fully-connected layers simple (single-task as opposed to multitask) and somewhat shallow, in order to be able to evaluate the ability of the self-supervised approach to learn robust generalized ECG representations. The overview of the self-supervised solution is presented in Figure  2 , where successive to a self-supervised learning ECG representations, transfer learning  [43]  is used for emotion recognition with the fully-supervised model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "We used four publicly available datasets to evaluate the proposed solution in depth on a large number of different subjects, in different circumstances and under different data collection protocols, and using different hardware. The important characteristics of the datasets are summarized in Table 2 and a brief description of each dataset is provided bellow.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Amigos",
      "text": "The AMIGOS dataset  [4]  was collected from 40 participants to study personality, mood, and affective responses, while engaging with multimedia content in two different contexts: a) alone, and b) in a group of four people. To perform this study, participants were shown short and long emotional video clips to elicit affective states. The short video clips had a length of 250 seconds, whereas the long video clips were longer than 14 minutes. ECG data were recorded using Shimmer sensors  [44]  at a sampling frequency of 256 Hz. A total of 16 video clips were shown to each participant, and self-assessed arousal and valence scores were recorded after each trial on a scale of 1 to 9.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dreamer",
      "text": "The DREAMER dataset  [22]  was collected from 23 participants while they were presented with audio and video stimuli in the form of movie clips in order to elicit emotional reactions. A total of 18 video clips were shown, inducing 9 different emotions namely amusement, excitement, happiness, calmness, anger, disgust, fear, sadness, and surprise. Each clip was 60 seconds long. In addition, neutral clips were shown before each session to help subjects return to a neutral emotional state. ECG signals were collected using shimmer sensors  [44]  at a sampling rate of 256 Hz. Subjective experience of arousal and valence scores were collected using self-assessment manikins  [45] . At the end of each session, both arousal and valence scores were recorded for the entire session on a scale of 1 to 5, ranging from uninterested/bored to excited/alert for arousal and unpleasant/stressed to happy/elated for valence.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Wesad",
      "text": "The dataset for WEarable Stress and Affect Detection (WESAD) contains ECG data from 17 participants. A RespiBAN Professional  [46]  sensors were used to collect ECG at a sampling rate of 700 Hz. The goal was to study four different affective states namely neutral, stressed, amused, and meditated. To perform this study, four different test scenarios were created. First, 20 minutes of neutral data were collected, during which participants were asked to do normal activities such as reading a magazine and sitting/standing at a table. During the amusement scenario, participants watched 11 funny video clips for a total length of 392 seconds. Next, participants went through public speaking and arithmetic tasks for a total of 10 minutes as part of the stress scenario. Finally, participants went through a guided meditation session of 7 minutes in duration. Upon completion of each trial, the ground truth labels for the affect states were collected using the Positive and Negative Affect Schedule (PANAS) scheme  [47] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Swell",
      "text": "This dataset was created with the goal of understanding employees' mental stress or emotional attributes in a typical office environment under different conditions  [9] . Three types of scenarios namely, normal, time pressure, and interruptions were designed. Under the normal condition, participants were allowed to work on different tasks, for example preparing reports, making presentations, and so on, which were carried out over a maximum duration of 45 minutes. During the time pressure condition, the allowed time was reduced to 30 minutes to induce pressure. In the interruption session, participants were interrupted by sending emails and messages, with the participants being asked to respond to these messages. ECG signals were collected from 25 participants using a TMSI MOBI device  [48]  with self-adhesive electrodes at a sampling rate of 2048 Hz. Finally, at the end of each scenario, the participants' self-reported affect scores were collected on a scale of 1 to 9.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Pre-Processing",
      "text": "Since the above-mentioned datasets have been collected using different hardware, the signals have different spatiotemporal properties such as spatial range and sampling rate. To minimize the effects of such inter-dataset variations and discrepancies, three simple pre-processing steps were taken. First, SWELL and WE-SAD ECG signals are downsampled to 256 Hz to be consistent with AMIGOS and DREAMER. Next, we remove baseline wander from all the four datasets by applying a high-pass IIR filter with a pass-band frequency of 0.8 Hz. Lastly, we perform user-specific z-score normalization. While a number of other pre-processing operations such as feature extraction could have been done, we intentionally kept the pre-processing minimal and simple in order to better understand the impact of the proposed model on learning important ECG representations based on almost raw input. Finally, the filtered ECG signals are segmented into a fixed window size of 10 seconds and stacked into an array. No overlap is designated between segments to avoid any potential data leakage between training and testing data. It should be noted that the selection of the window size was empirical. Prior works utilizing these datasets have selected a wide range of different window sizes. For example,  [4]  and  [32]  selected a window length of 20 seconds for AMIGOS, whereas  [22]  and  [35]  selected a window of 60 seconds for both DREAMER and SWELL. As other examples,  [23]  has selected 5-second windows for WESAD, while  [34]  has selected 1-second windows for the same dataset.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Implementation And Training",
      "text": "In order to train the model, successive to pre-processing, each segment is used to generate the 6 transformations described earlier.\n\nFinally, the original ECG signals along with the transformed signals are used to train the signal transformation recognition network. We implement the proposed architecture using TensorFlow.\n\nWe share the implementation of the self-supervised network  1  . Similar to other works in this area  [4] ,  [22] ,  [23] ,  [32] ,  [34] ,  [35] , we use 10-fold cross-validation to evaluate the performance of the model successive to shuffling of the pre-processed dataset. We randomly select 90% of the data for training and the remaining 10% is used for testing (this process is repeated 10 times without repeating the shuffling step). To train both the signal transformation recognition network and the emotion recognition network, Adam optimizer  [49]  is used with a learning rate of 0.001 and batch size of 128. The signal transformation recognition network is trained for 100 epochs, while the emotion recognition network is trained for 250 epochs. The number of training epochs for each network is selected to enable the training reach a steady state. Figure  4   the transformation recognition and emotion recognition networks respectively during training. Figure  4 (A) shows that the loss for temporal inversion, negation, permutation, and time-warping reach steady states earlier than the other transformations. Therefore, the network is trained until all the individual losses reach their respective steady states. Figure  4 (B) shows that the different datasets reach their training steady states at different epochs. For example, the loss of WESAD and SWELL become stable well before 100 epochs, while the steady states of AMIGOS and DREAMER are achieved after 200 epochs. In summary, Figure  4  shows that both pretext tasks and emotion recognition tasks train well and reach steady states using the proposed solution.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Signal Transformation Recognition",
      "text": "Table  3  and 4 presents the F1 scores and accuracies of our proposed network for transformation recognition. Results shown in Table  3  are calculated on all the datasets combined. The results show very high F1 scores and accuracies for all the transformations as well as the original signal. F1 scores 0.927 and 0.932 are achieved when detecting the original signal and the scaling transformations. Next, relatively higher F1 score of 0.979 is achieved for recognition of noisy signals. We notice few tasks consistently report very high scores (≈ 0.99), namely temporal inversion, negation, permutation, and time-warping. For accuracy, very high values (greater than 0.98) are achieved for every transformation. Beside high F1 scores and accuracies, we also achieve very low standard deviations for all the transformation recognition tasks, which indicates a consistent performance by the network in all the training folds. Finally, an average F1 score of 0.972 and an average accuracy of 0.992 are reported for all the tasks combined. Next, we further investigate the consistency and generalization of the performance of the transformation recognition network across different datasets. Table  4  presents the results on each of the four datasets, showing very high F1 scores and accuracies. This is a strong indicator of the generalizability of our approach as the model performs consistently on all the datasets with F1 scores of 0.959 to 0.987 and accuracies of 0.989 to 0.996, with low standard deviations in every case.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Results obtained from the emotion recognition network are presented in Table  5 . Arousal and valence detection are performed with three datasets AMIGOS, DREAMER, and SWELL, stress detection is performed with SWELL, and classification of different affective states are performed in WESAD, as per the availability of output labels. As presented in Table  2 , the four datasets contain different number of classes for each target output. Therefore, the classification tasks are performed accordingly, the accuracies and F1 scores of which are presented in Table  5 . It should be noted that to the best of our knowledge, no previous work has utilized AMIGOS, DREAMER and SWELL datasets for multiclass classification. this is the first time, multi-class classification of arousal and valence score is attempted on. With AMIGOS, we achieve accuracies of 79.6% and 78.3% for arousal and valence respectively, where 9-class classification is performed. In DREAMER, 5-class classification is performed, achieving accuracies of 77.1% and 74.9% for arousal and valence respectively. Next, we perform detection of 4 affective states using WESAD, reporting an accuracy of 95.0%. Lastly, arousal, valence, and stress detection is performed on SWELL, achieving accuracies of 92.6%, 93.8%, and 90.2% respectively, where arousal and valence scores have 9 classes while stress has 3 classes. Comparison: To further evaluate the proposed framework, we compare our model's performance to past work as shown in Table  6 . It should be noted that most of the prior works using the four studied datasets have performed two-class or three-class classifications  [4] ,  [9] ,  [22] ,  [23]  instead of utilizing all the multiple classes available for each dataset. Therefore, in order to perform a fair comparison, in addition to the multi-class classifications presented in Table  5 , in Table  6  we present the results of the model applied to two/three-class classifications in accordance to the prior works and state-of-the-art methods presented in this table. In Table  6 , AMIGOS, DREAMER, and SWELL have all been classified in two-class format by setting a threshold at the mean output value dividing the outputs to high and low, while WESAD has been classified in three classes by ignoring the meditated output class. It should be noted that the works mentioned in Table  6  have also adopted a 10-fold cross-validation scheme similar to the validation scheme used in this paper. Moreover, for each dataset, we have also carried out fully-supervised classification by skipping the self-supervised step and directly training the emotion recognition network using the input ECG and emotion labels. In  [32] , a CNN was implemented to perform emotion classification on AMIGOS. It reported accuracies of 81% and 71% for classification of arousal and valence respectively. The results in Table  6 (A) show that the",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Noise Addition",
      "text": "0.987 ± 0.002 0.954 ± 0.008 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 0.999 ± 0.000 0.999 ± 0.000 Scaling 0.975 ± 0.003 0.904 ± 0.020 0.986 ± 0.005 0.955 ± 0.024 0.984 ± 0.008 0.952 ± 0.012 0.987 ± 0.001 0.948 ± 0.006 Temporal Inversion 0.997 ± 0.001 0.988 ± 0.004 1.000 ± 0.000 1.000 ± 0.000 0.999 ± 0.001 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 Negation 0.996 ± 0.001 0.987 ± 0.005 1.000 ± 0.000 1.000 ± 0.000 0.999 ± 0.002 0.995 ± 0.007 1.000 ± 0.000 0.998 ± 0.004 Permutation 0.996 ± 0.001 0.984 ± 0.005 0.999 ± 0.000 0.999 ± 0.003 0.999 ± 0.001 0.994 ± 0.007 0.998 ± 0.001 0.991 ± 0.009 Time-warping 0.998 ± 0.001 0.993 ± 0.007 0.998 ± 0.003 0.999 ± 0.003 0.993 ± 0.009 0.991 ± 0.006 0.996 ± 0.004 0.990 ± 0.005 Average 0.989 ± 0.002 0.959 ± 0.008 0.996 ± 0.002 0.987 ± 0.004 0.994 ± 0.004 0.983 ± 0.007 0.995 ± 0.001 0.981 ± 0.005 self-supervised CNN achieves accuracies of 88.9% and 87.5% for classification of arousal and valence respectively, outperforming the past works as well as the fully-supervised baseline CNN. In  [22] , classification of arousal and valence was performed with DREAMER. An SVM classifier was implemented, reporting an accuracy of 62.4% for both arousal and valence. Table  6 (B) shows that the proposed solution achieves considerable improvement over the state-of-the-art and the baseline CNN with accuracies of 85.9% and 85% for arousal and valence respectively. In a study on WESAD,  [34]  proposed a multimodal CNN that was able to achieve state-of-the-art with an accuracy of 83% while using only ECG. As shown in the Table  6 (C), our proposed model is able to outperform the CNN in  [34]  as well as the baseline CNN with accuracy of 96.9%. Lastly, a study on SWELL performed stress detection  [35] , in which two classifiers, kNN and SVM, were utilized. The results in Table  6 (D) show that the baseline CNN slightly outperforms the SVM, whereas the self-supervised model considerably outperforms both with an accuracy of 93.3%. Additionally, we perform two-class classification of arousal and valence on SWELL for the first time, achieving very accurate results.\n\nIt should be noted that in order to perform a fair comparison, Table  6  excludes prior works such as: i)  [50]  and  [17]  which have used different validation schemes; ii)  [51] -  [53]  which have used multiple modalities (ECG with EEG or GSR) available in some of the datasets; and iii)  [54] , which has formulated the problem as regression instead of classification.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Analysis And Discussion",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Single-Task Self-Supervision",
      "text": "We study the impact of individual transformations on the emotion recognition performance. For this study, we create a single-task CNN, instead of a multi-task CNN. For this study, we select a range of possible values for the parameters that control the significance of each individual transformation. For noise addition, the noise amplitude parameter is changed to result in signal-to-noise ratios (SNR) of 2 to 45. For the scaling transformation parameter, scaling factors of 0.1 to 10 are selected. For permutation, the number of segments is varied between 2 to 40. For time-warping, the stretch factor is varied between 1.05 to 4, while the number of segments varies from 2 to 40. Finally, temporal inversion and negation, which do not have any controllable parameters are also studied. We should point out that these ranges of parameters are selected to create a wide range of transformed signals, almost identical to the original signal in one end and considerably different in the other. Figure  5  shows the emotion recognition performance for each dataset vs. the controllable parameters for each individual transformation task. To simplify the search, a single 90-10 split for training-testing was performed instead of 10-fold. This analysis provides in-depth insight into the effect of the parameters associated with each self-supervised recognition task (transformation) on the emotion recognition outcome. Furthermore, this analysis helps us narrow down the set of suitable transformation parameters in order to achieve the best performance. From Figure  5 (A), we notice that for SNR values of 15 and 20 the model shows highest performance for emotion recognition compared to SNR values greater than 30 or less than 10. Moreover, from Figure  5 (B), we notice that for the scaling factor of 1, where the scaled signal and the original signals are identical, the model performance on emotion recognition is poor. However, for slightly greater or smaller scaling values, the model shows significantly better performance. Interestingly, the performance drops when the scaling factor is greater than 1.4 or less than 0.6. Figure  5 (C) illustrates that when utilizing permutations, poor performance is achieved when the input is divided into only 2 segments or more than 20 segments. Otherwise, when the number of segments varies in the range of 3 to 20, a relatively steady performance is observed. Time-warping analysis is presented in a 3D plot in Figure  5 (F) where dark red regions show higher accuracies. It can be seen that number of segments in the range of 6 to 9 and a stretch factor in the range of 1.05 and 1.35 consistently result in better performance compared to other combinations. In summary, the analysis above shows that for all the different transformation tasks, when the transformed signal is highly distorted compared to the original input, for example with high amplitude noise (SNR < 10), large number of segments (> 20), very high or very low scaling factor (< 1.5 or > 0.6), or large stretch factor (> 1.4), emotion recognition accuracy drops. This is due to the fact that distinguishing between the original signal and the highly distorted versions of the signal becomes simple. This results in the network not learning useful ECG representations. Similarly, when the transformed signal remains almost similar to the original signal, for instance when the SNR > 35, or a scaling factor in the range of 0.95 to 1.05, or the number of permutated segments equal to 2, the performance also drops. This is due to the fact that the recognition tasks in such cases become too difficult for the network to properly learn the required ECG representations  from. Hence, we conclude that there is a range of values for the parameters associated with the self-supervised tasks, for which these tasks are located in an suitable difficulty range, resulting in optimum learning.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Multi-Task Self-Supervision",
      "text": "This section discusses the emotion recognition performance when multiple signal transformations (multi-task) are performed to learn ECG representations. The reason we believe this analysis is necessary despite already analyzing individual transformations in the previous subsection is that the use of a multi-task network may have an impact on the overall performance which might be different than the aggregation of several individual self-supervised tasks. To perform this study, we use the results from the previous section (Subsection 6.1) to narrow down the search given the large degree of freedom in the multi-task parameter space. Also, to further simplify the search, similar to Figure  5 , a single 90-10 split for training-testing was performed instead of a complete 10-fold.\n\nThe results obtained from different combination of signal transformation parameters are presented in a six-dimensional plot as shown in Figure  6 . Our analysis shows that (15, 0.9, 20, 9, 1.05) is the optimal vector of parameters for SNR, scaling factor, number of permutation segments, number of time-warping segments, and time-warping stretch factor respectively. Our analysis further confirms that the results obtained from single-task self-supervision are in compliance with results obtained from multi-task selfsupervision. For example, Figure  5 (A) shows that an SNR = 15 results in better emotion recognition compared to SNR = 25. Very similar results are also observed in Figure  6 , as SNR = 15 results in the best accuracy in 6 out of 8 instances. As another example, in both single-task and multi-task settings, a stretch factor of 1.05 shows superior performance compared to 1.35.\n\nFigure  7  compares the emotion recognition results when ECG representations are learned using self-supervised single-tasks to Fig.  6 : Emotion recognition results vs. 5D vector of parameters controlling all 6 tasks simultaneously are presented. The resulting 6 dimensional plots are represented as follows: Y axis presents the emotion recognition accuracy, the two plots separated by the vertical line correspond to SNR, the time-warping stretch factor is represented by the X axis, the scaling factor, permutation segments, and time-warping segments are represented by marker size, color, and shape respectively. Fig.  7 : Comparison between single-task and multi-task selfsupervision is presented. MT denotes multi-task, while T1 through T6 denote noise addition, scaling, permutation, time-warping, temporal inversion, and negation respectively. when multi-task representation learning is used. These results are reported for the optimum parameters for each case. The figure shows that different transformation recognition tasks help the selfsupervised model learn better high-level representations of ECG. As the multi-task network consistently outperforms single-task setups (more prominently in AMIGOS and DREAMER), it can be concluded that different transformations help the model in learning different aspects of ECG representations. For instance while noise addition, scaling, and negation may be contributing more towards learning spatial representations of ECG, others such as temporal inversion, permutation, and time-warping may be contributing more to learning the temporal relations. Moreover, with respect to the relationship between different transformation recognition and emotion recognition tasks, specific commonalities are observed. In particular, we see that scaling (T2) has greater impact on emotion recognition compared to noise addition (T1) and negation (T6). On the other hand, time-warping (T4) has greater impact compared to permutation (T3) and temporal inversion (T5).\n\nIn the multi-task network, the total loss is calculated as a Fig.  8 : The relationship between emotion recognition accuracy and transformation recognition is presented. Time-warping (Stretch Factor) denotes that the stretch factor is varied while the number of segments is set to the optimum value. Similarly, Time-warping (No. of Segments) indicates that the number of segments is varied while the stretch factor is set to the optimum value.\n\nweighted average of individual losses (see Eq. 2). We realize that these weights (loss coefficients) play an important role in learning the ECG representations since during training of the selfsupervised network, different losses reach their steady states at different epochs. Specifically, the losses of the temporal inversion and spatial negation saturate very quickly and report almost perfect F 1 scores just after 5 to 7 epochs. The possible reason for this phenomena is the clear difference between inverted and negated signals with respect to the original signal, making it very easy for the model to correctly classify these transformations. As a result, we utilize smaller weights for temporal inversion and negation losses compared to the other transformations. All the weights were set empirically with the goal of maximizing performance. The loss coefficients of 0.0125 are used for temporal inversion and negation, while for the rest of the 5 losses, the weights are set to 0.195.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Relationship Between Downstream And Pre-Text Tasks",
      "text": "To further understand the relationship between the accuracy of classification of the pre-text tasks and downstream emotion recognition, we present this relationship in Figure  8 . In this figure, the transformation recognition accuracies are obtained based on varying degrees of task difficulty as discussed in Section 6.1.\n\nCorresponding emotion recognition accuracies are then calculated for this analysis. Since similar patterns are noticed across different emotions and datasets, the average accuracies are utilized in the Fig.  9 : The feasibility of using embeddings obtained through the network at different depths for emotion recognition is investigated. In (A) the schematic of the experiment is presented, while (B) presents the results. Fig.  10 : The impact of using multiple datasets with the selfsupervised architecture is investigated and presented.\n\nfigure. For time-warping, there are two transformation parameters: number of segments and stretch factor. Therefore, first we keep the number of segments at optimum value according to the result in Section 6.1, and vary the other transformation parameter (stretch and vice versa. Temporal inversion and negation transformation are not included here as there is no controllable parameters to the transformation recognition difficulty.\n\nThe figure that in the case of noise addition, scaling, and permutation, when the transformation recognition accuracy is poor, the emotion classification performance is also poor. This behaviour occurs when the transformation recognition tasks are too difficult. On the other hand, when the transformed signals are highly distorted compared to the original signal, the model reports high accuracy in transformation recognition, while poor performance is acquired for classifying emotions. This observation further confirms that for an optimum amount of difficulty in the pretext tasks, the self-supervised model shows the best performance for downstream tasks.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Network Embeddings",
      "text": "Here, we evaluate the quality of the learned representations vs. the depth of the self-supervised model. The representations learned with each conv-block are separately extracted and utilized to perform emotion recognition as shown in Figure  9 (A). This analysis helps us understand whether representations at different depths of the network are informative and valuable for the downstream tasks. First, the learned representations from conv-block 1, convblock 2, and conv-block 3 are separately extracted and utilized to perform emotion recognition. As shown in Figure  9 (B), significant improvement in performance is noticed when features are extracted from conv-block 3 compared to conv-block 1 and convblock 2. In addition to using the individual embeddings, we also stacked the three embeddings to create a new embedding, which we then used for the downstream tasks. However, the embedding obtained from conv-block 3 still showed the best performance. This analysis indicates that learning representations obtained from the last convolutional layer is better and more generalizable for the downstream emotion recognition.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Impact Of Multiple Of Datasets",
      "text": "One of the major advantages of the self-supervised solution is that it allows for the self-supervised network to be trained using an aggregation of the existing datasets by removing the main barrier of having different types of output labels for each dataset. To further analyze this aspect of the proposed solution, we investigate the impact of using all four datasets for training the network compared to when individual datasets are used. To perform this study, we use the four datasets separately to train the self-supervised signal transformation recognition network, which we then use for emotion recognition. Figure  10  shows that using all 4 datasets combined, the model learns more generalized and robust features as evident by the better performance in classifying emotions compared to learning ECG representations from individual datasets. In the case of AMIGOS, DREAMER, and WESAD datasets, the performance improves significantly while marginal improvement is noticed for SWELL dataset.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Limitations And Future Work",
      "text": "In this paper we performed an extensive analysis of the proposed framework and highlight the benefits of the self-supervised approach over a fully-supervised technique. However, the limitations of the self-supervised solution should also be noted. We notice that the proposed solution performs poorly in subject-independent emotion recognition. We hypothesize the main possible reasons for this: 1) subject-invariant features may require specific handcrafted features or a combination of hand-crafted and deep learning features such as those used in  [3]  and  [17]  respectively, which might not be learnable solely with convolutions applied to the raw ECG; 2) Moreover, in order to properly normalize userspecific ECG features linked to factors such as genetics, physiology, health, and others, user-specific calibrations with respect to baseline sessions were carried out in  [3] , which enabled subjectindependent validation to perform well. We avoided such calibration steps in the pipeline since: a) a calibration-free framework is generally more convenient and hence preferred, and b) some of the datasets did not include the information required to perform such calibrations.\n\nFor future work, multi-modal emotion recognition will be explored, exploiting other modalities such as EEG available in some datasets. Furthermore, we will employ the proposed solution for other ECG-related domains such as arrhythmia detection, ECGbased activity recognition, and others. Lastly, further research towards utilization of the proposed architecture for cross-subject and cross-corpus schemes will be carried out.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "In this work we present an ECG-based emotion recognition solution using self-supervised deep multi-task learning. To the best of our knowledge, this is the first time self-supervised learning is utilized to perform emotion recognition using ECG. Four public datasets AMIGOS, DREAMER, WESAD and SWELL are used in this study to perform emotion recognition. We set new stateof-the-art results for the four datasets in classification of arousal, valence, affective states, and stress. We show that the proposed approach significantly improves classification performance compared to a fully-supervised solution. We explore different selfsupervised transformation recognition tasks for learning the ECG representations and analyze their impacts. Our analysis shows that for an optimum amount of difficulty in the pretext tasks, the network learns better ECG representations which can used for emotion classification. Our experiments also show that for learning of ECG representations, a multi-task CNN improves the performance compared to a single-task network. Lastly, our analysis on the use multiple datasets for training the self-supervised network shows that utilizing multiple datasets benefits the final downstream classification.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An overview of the proposed framework for self-supervised",
      "page": 1
    },
    {
      "caption": "Figure 1: ). In self-supervised learning,",
      "page": 1
    },
    {
      "caption": "Figure 2: shows the",
      "page": 4
    },
    {
      "caption": "Figure 3: Noise addition: In this transformation, random noise from a",
      "page": 4
    },
    {
      "caption": "Figure 2: The self-supervised architecture is presented. First, a multi-task CNN is trained using automatically generated labels to learn",
      "page": 5
    },
    {
      "caption": "Figure 3: A sample of an original ECG signal with the six trans-",
      "page": 5
    },
    {
      "caption": "Figure 2: , where successive to a self-supervised",
      "page": 5
    },
    {
      "caption": "Figure 4: (A) and (B) show the loss vs. training epoch for",
      "page": 6
    },
    {
      "caption": "Figure 4: Training losses vs. training epochs are presented for",
      "page": 7
    },
    {
      "caption": "Figure 4: (A) shows that the loss for",
      "page": 7
    },
    {
      "caption": "Figure 4: (B) shows that the different",
      "page": 7
    },
    {
      "caption": "Figure 4: shows that both pretext tasks and emotion recognition tasks train",
      "page": 7
    },
    {
      "caption": "Figure 5: shows the emotion recognition performance",
      "page": 8
    },
    {
      "caption": "Figure 5: (C) illustrates that when",
      "page": 8
    },
    {
      "caption": "Figure 5: (F) where dark red",
      "page": 8
    },
    {
      "caption": "Figure 5: The impact of transformation parameters on emotion recognition accuracy is illustrated. Parts (A), (B), and (C) are 2D graphs as",
      "page": 9
    },
    {
      "caption": "Figure 5: , a single 90-10 split",
      "page": 9
    },
    {
      "caption": "Figure 6: Our analysis shows that (15, 0.9, 20, 9, 1.05)",
      "page": 9
    },
    {
      "caption": "Figure 5: (A) shows that an SNR = 15",
      "page": 9
    },
    {
      "caption": "Figure 6: , as SNR = 15 results",
      "page": 9
    },
    {
      "caption": "Figure 7: compares the emotion recognition results when ECG",
      "page": 9
    },
    {
      "caption": "Figure 6: Emotion recognition results vs. 5D vector of parameters controlling all 6 tasks simultaneously are presented. The resulting 6",
      "page": 10
    },
    {
      "caption": "Figure 7: Comparison between single-task and multi-task self-",
      "page": 10
    },
    {
      "caption": "Figure 8: The relationship between emotion recognition accuracy and",
      "page": 10
    },
    {
      "caption": "Figure 8: In this ﬁgure,",
      "page": 10
    },
    {
      "caption": "Figure 9: The feasibility of using embeddings obtained through the network at different depths for emotion recognition is investigated. In",
      "page": 11
    },
    {
      "caption": "Figure 10: The impact of using multiple datasets with the self-",
      "page": 11
    },
    {
      "caption": "Figure 9: (A). This analysis",
      "page": 11
    },
    {
      "caption": "Figure 9: (B), signif-",
      "page": 11
    },
    {
      "caption": "Figure 10: shows that using",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The architecture of the signal transformation recogni-",
      "data": [
        {
          "Input": "Shared Layers",
          "−": "[conv, 1 × 32, 32] × 2\n[maxpool, 1 × 8, stride = 2]\n[conv, 1 × 16, 64] × 2\n[maxpool, 1 × 8, stride = 2]\n[conv, 1 × 8, 128] × 2",
          "2560 × 1": "2560 × 32\n1277 × 32\n1277 × 64\n635 × 64\n635 × 128"
        },
        {
          "Input": "",
          "−": "global max pooling",
          "2560 × 1": "1 × 128"
        },
        {
          "Input": "Task-Speciﬁc\nLayers",
          "−": "[dense] × 2\ntasks\n× 7 parallel",
          "2560 × 1": "128"
        },
        {
          "Input": "Output",
          "−": "−",
          "2560 × 1": "2"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: presents the results on each of",
      "data": [
        {
          "Original\nNoise Addition\nScaling\nTemporal Inversion\nNegation\nPermutation\nTime-warping": "Average",
          "0.980 ± 0.003\n0.995 ± 0.000\n0.982 ± 0.003\n0.998 ± 0.000\n0.998 ± 0.000\n0.998 ± 0.000\n0.997 ± 0.003": "0.992 ± 0.001",
          "0.927 ± 0.007\n0.979 ± 0.003\n0.932 ± 0.010\n0.992 ± 0.004\n0.990 ± 0.000\n0.989 ± 0.003\n0.992 ± 0.006": "0.972 ± 0.005"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: The emotion categories, number of classes for each scaling factors of 0.1 to 10 are selected. For permutation, the",
      "data": [
        {
          "Original\nNoise Addition\nScaling\nTemporal Inversion\nNegation\nPermutation\nTime-warping": "Average",
          "0.973 ± 0.003\n0.987 ± 0.002\n0.975 ± 0.003\n0.997 ± 0.001\n0.996 ± 0.001\n0.996 ± 0.001\n0.998 ± 0.001": "0.989 ± 0.002",
          "0.901 ± 0.010\n0.954 ± 0.008\n0.904 ± 0.020\n0.988 ± 0.004\n0.987 ± 0.005\n0.984 ± 0.005\n0.993 ± 0.007": "0.959 ± 0.008",
          "0.986 ± 0.005\n1.000 ± 0.000\n0.986 ± 0.005\n1.000 ± 0.000\n1.000 ± 0.000\n0.999 ± 0.000\n0.998 ± 0.003": "0.996 ± 0.002",
          "0.953 ± 0.020\n1.000 ± 0.000\n0.955 ± 0.024\n1.000 ± 0.000\n1.000 ± 0.000\n0.999 ± 0.003\n0.999 ± 0.003": "0.987 ± 0.004",
          "0.981 ± 0.008\n1.000 ± 0.000\n0.984 ± 0.008\n0.999 ± 0.001\n0.999 ± 0.002\n0.999 ± 0.001\n0.993 ± 0.009": "0.994 ± 0.004",
          "0.950 ± 0.014\n1.000 ± 0.000\n0.952 ± 0.012\n1.000 ± 0.000\n0.995 ± 0.007\n0.994 ± 0.007\n0.991 ± 0.006": "0.983 ± 0.007",
          "0.984 ± 0.003\n0.999 ± 0.000\n0.987 ± 0.001\n1.000 ± 0.000\n1.000 ± 0.000\n0.998 ± 0.001\n0.996 ± 0.004": "0.995 ± 0.001",
          "0.941 ± 0.009\n0.999 ± 0.000\n0.948 ± 0.006\n1.000 ± 0.000\n0.998 ± 0.004\n0.991 ± 0.009\n0.990 ± 0.005": "0.981 ± 0.005"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: The emotion categories, number of classes for each scaling factors of 0.1 to 10 are selected. For permutation, the",
      "data": [
        {
          "AMIGOS": "",
          "Arousal": "Valence",
          "9": "9",
          "0.796": "0.783",
          "0.777": "0.765"
        },
        {
          "AMIGOS": "DREAMER",
          "Arousal": "Arousal",
          "9": "5",
          "0.796": "0.771",
          "0.777": "0.740"
        },
        {
          "AMIGOS": "",
          "Arousal": "Valence",
          "9": "5",
          "0.796": "0.749",
          "0.777": "0.747"
        },
        {
          "AMIGOS": "WESAD",
          "Arousal": "Affect State",
          "9": "4",
          "0.796": "0.950",
          "0.777": "0.940"
        },
        {
          "AMIGOS": "SWELL",
          "Arousal": "Arousal",
          "9": "9",
          "0.796": "0.926",
          "0.777": "0.930"
        },
        {
          "AMIGOS": "",
          "Arousal": "Valence",
          "9": "9",
          "0.796": "0.938",
          "0.777": "0.943"
        },
        {
          "AMIGOS": "",
          "Arousal": "Stress",
          "9": "3",
          "0.796": "0.902",
          "0.777": "0.900"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 6: The results of the self-supervised model on all the datasets are presented and compared with prior works including the",
      "data": [
        {
          "[23]": "",
          "kNN": "DT",
          "0.548": "0.578",
          "0.478": "0.517"
        },
        {
          "[23]": "",
          "kNN": "RF",
          "0.548": "0.604",
          "0.478": "0.522"
        },
        {
          "[23]": "",
          "kNN": "AB",
          "0.548": "0.617",
          "0.478": "0.525"
        },
        {
          "[23]": "",
          "kNN": "LDA",
          "0.548": "0.663",
          "0.478": "0.560"
        },
        {
          "[23]": "[34]",
          "kNN": "CNN",
          "0.548": "0.83",
          "0.478": "0.81"
        },
        {
          "[23]": "Ours",
          "kNN": "Fully-Supervised CNN",
          "0.548": "0.932",
          "0.478": "0.912"
        },
        {
          "[23]": "",
          "kNN": "Self-Supervised CNN",
          "0.548": "0.969",
          "0.478": "0.963"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Toward machine emotional intelligence: Analysis of affective physiological state",
      "authors": [
        "Rosalind Picard",
        "Elias Vyzas",
        "Jennifer Healey"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Classification of cognitive load and expertise for adaptive simulation using deep multitask learning",
      "authors": [
        "Pritam Sarkar",
        "Kyle Ross",
        "Aaron Ruberto",
        "Dirk Rodenbura",
        "Paul Hungler",
        "Ali Etemad"
      ],
      "year": "2019",
      "venue": "8th IEEE International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "4",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Miranda Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Personalized multitask learning for predicting tomorrow's mood, stress, and health",
      "authors": [
        "S Taylor",
        "N Jaques",
        "E Nosakhare",
        "A Sano",
        "R Picard"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "A deep learning approach to monitoring and detecting atrial fibrillation using wearable technology",
      "authors": [
        "Prajwal Supreeth",
        "Shashikumar",
        "J Amit",
        "Qiao Shah",
        "Li",
        "D Gari",
        "Shamim Clifford",
        "Nemati"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Biomedical & Health Informatics"
    },
    {
      "citation_id": "8",
      "title": "Detecting stress during realworld driving tasks using physiological sensors",
      "authors": [
        "Jennifer Healey",
        "Rosalind Picard"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "9",
      "title": "The swell knowledge work dataset for stress and user modeling research",
      "authors": [
        "Saskia Koldijk",
        "Maya Sappelli",
        "Suzan Verberne",
        "Mark Neerincx",
        "Wessel Kraaij"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "10",
      "title": "Ascertain: Emotion and personality recognition using commercial sensors",
      "authors": [
        "Ramanathan Subramanian",
        "Julia Wache",
        "Mojtaba Khomami Abadi",
        "L Radu",
        "Stefan Vieriu",
        "Nicu Winkler",
        "Sebe"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "A deep framework for facial emotion recognition using light field images",
      "authors": [
        "Alireza Sepas-Moghaddam",
        "Ali Etemad",
        "Paulo Correia",
        "Fernando Pereira"
      ],
      "year": "2019",
      "venue": "th IEEE International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "12",
      "title": "Classification and translation of style and affect in human motion using rbf neural networks",
      "authors": [
        "Ali Etemad",
        "Ali Arya"
      ],
      "year": "2014",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition by speech signals",
      "authors": [
        "Oh-Wook Kwon",
        "Kwokleung Chan",
        "Jiucang Hao",
        "Te-Won Lee"
      ],
      "year": "2003",
      "venue": "8th European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "14",
      "title": "Ecg pattern analysis for emotion detection",
      "authors": [
        "Foteini Agrafioti",
        "Dimitris Hatzinakos",
        "Adam Anderson"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Capsule attention for multimodal eeg and eog spatiotemporal representation learning with application to driver vigilance estimation",
      "authors": [
        "Ali Guangyi",
        "Etemad"
      ],
      "year": "2019",
      "venue": "Capsule attention for multimodal eeg and eog spatiotemporal representation learning with application to driver vigilance estimation",
      "arxiv": "arXiv:1912.07812"
    },
    {
      "citation_id": "16",
      "title": "Stress detection in computer users based on digital signal processing of noninvasive physiological variables",
      "authors": [
        "Jing Zhai",
        "Armando Barreto"
      ],
      "year": "2006",
      "venue": "IEEE International Conference of Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "17",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "S Siddharth",
        "T Jung",
        "T Sejnowski"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Recognizing emotions induced by affective sounds through heart rate variability",
      "authors": [
        "Mimma Nardelli",
        "Gaetano Valenza",
        "Alberto Greco",
        "Antonio Lanata",
        "Enzo Scilingo"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Multi-task selfsupervised learning for human activity detection",
      "authors": [
        "Aaqib Saeed",
        "Tanir Ozcelebi",
        "Johan Lukkien"
      ],
      "year": "2019",
      "venue": "Proceedings of the Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "20",
      "title": "Self-taught learning: transfer learning from unlabeled data",
      "authors": [
        "Rajat Raina",
        "Alexis Battle",
        "Honglak Lee",
        "Benjamin Packer",
        "Andrew Ng"
      ],
      "year": "2007",
      "venue": "Proceedings of the 24th International Conference on Machine Learning"
    },
    {
      "citation_id": "21",
      "title": "Transitive invariance for self-supervised visual representation learning",
      "authors": [
        "Xiaolong Wang",
        "Kaiming He",
        "Abhinav Gupta"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "22",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless lowcost off-the-shelf devices",
      "authors": [
        "Stamos Katsigiannis",
        "Naeem Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "23",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection",
      "authors": [
        "Philip Schmidt",
        "Attila Reiss",
        "Robert Duerichen",
        "Claus Marberger",
        "Kristof Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "24",
      "title": "Multi-task self-supervised learning for robust speech recognition",
      "authors": [
        "M Ravanelli",
        "J Zhong",
        "S Pascual",
        "P Swietojanski",
        "J Monteiro",
        "J Trmal",
        "Y Bengio"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Félix de Chaumont Quitry, and Dominik Roblek. Self-supervised audio representation learning for mobile devices",
      "authors": [
        "Marco Tagliasacchi",
        "Beat Gfeller"
      ],
      "year": "2019",
      "venue": "Félix de Chaumont Quitry, and Dominik Roblek. Self-supervised audio representation learning for mobile devices",
      "arxiv": "arXiv:1905.11796"
    },
    {
      "citation_id": "26",
      "title": "Self-supervised learning for ecg-based emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Basic ECG Waves",
      "authors": [
        "L Ary",
        "Goldberge"
      ],
      "year": "2006",
      "venue": "Basic ECG Waves"
    },
    {
      "citation_id": "28",
      "title": "Cardiologistlevel arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network",
      "authors": [
        "Pranav Awni Y Hannun",
        "Masoumeh Rajpurkar",
        "Geoffrey Haghpanahi",
        "Codie Tison",
        "Bourn",
        "P Mintu",
        "Andrew Turakhia",
        "Ng"
      ],
      "year": "2019",
      "venue": "Nature Medicine"
    },
    {
      "citation_id": "29",
      "title": "Automatic ecg-based emotion recognition in music listening",
      "authors": [
        "Y Hsu",
        "J Wang",
        "W Chiang",
        "C Hung"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Utilizing deep neural nets for an embedded ecg-based biometric authentication system",
      "authors": [
        "Adam Page",
        "Amey Kulkarni",
        "Tinoosh Mohsenin"
      ],
      "year": "2015",
      "venue": "IEEE Biomedical Circuits and Systems Conference"
    },
    {
      "citation_id": "31",
      "title": "A method of emotion recognition based on ecg signal",
      "authors": [
        "Ya Xu",
        "Guang-Yuan Liu"
      ],
      "year": "2009",
      "venue": "IEEE International Conference on Computational Intelligence and Natural Computing"
    },
    {
      "citation_id": "32",
      "title": "Using deep convolutional neural network for emotion detection on a physiological signals dataset (amigos)",
      "authors": [
        "Luz Santamaria-Granados",
        "Mario Munoz-Organero",
        "Gustavo Ramirez-Gonzalez",
        "Enas Abdulhay",
        "Arunkumar"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "33",
      "title": "Toward dynamically adaptive simulation: Multimodal classification of user expertise using wearable devices",
      "authors": [
        "Kyle Ross",
        "Pritam Sarkar",
        "Dirk Rodenburg",
        "Aaron Ruberto",
        "Paul Hungler",
        "Adam Szulewski",
        "Daniel Howes",
        "Ali Etemad"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "34",
      "title": "An explainable deep fusion network for affect recognition using physiological signals",
      "authors": [
        "Jionghao Lin",
        "Shirui Pan",
        "Cheng Siong Lee",
        "Sharon Oviatt"
      ],
      "year": "2019",
      "venue": "Proceedings of the 28th International Conference on Information and Knowledge Management"
    },
    {
      "citation_id": "35",
      "title": "Stress detection in working people",
      "authors": [
        "Sriramprakash",
        "Vadana D Prasanna",
        "Ov Ramana",
        "Murthy"
      ],
      "year": "2017",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "36",
      "title": "Self-supervised learning of 3d human pose using multi-view geometry",
      "authors": [
        "Muhammed Kocabas",
        "Salih Karagoz",
        "Emre Akbas"
      ],
      "year": "2019",
      "venue": "Self-supervised learning of 3d human pose using multi-view geometry",
      "arxiv": "arXiv:1903.02330"
    },
    {
      "citation_id": "37",
      "title": "3d human pose machines with self-supervised learning",
      "authors": [
        "K Wang",
        "L Lin",
        "C Jiang",
        "C Qian",
        "P Wei"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Detection of heart abnormalities via artificial neural network: An application of self learning algorithms",
      "authors": [
        "M Rastgar-Jazi",
        "X Fernando"
      ],
      "year": "2017",
      "venue": "IEEE Canada International Humanitarian Technology Conference"
    },
    {
      "citation_id": "39",
      "title": "Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics",
      "authors": [
        "Jiangliu Wang",
        "Jianbo Jiao",
        "Linchao Bao",
        "Shengfeng He",
        "Yunhui Liu",
        "Wei Liu"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "Open information extraction using wikipedia",
      "authors": [
        "Fei Wu",
        "Daniel Weld"
      ],
      "year": "2010",
      "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "41",
      "title": "Selfsupervised spatiotemporal feature learning via video rotation prediction",
      "authors": [
        "Longlong Jing",
        "Xiaodong Yang",
        "Jingen Liu",
        "Yingli Tian"
      ],
      "year": "2018",
      "venue": "Selfsupervised spatiotemporal feature learning via video rotation prediction",
      "arxiv": "arXiv:1811.11387"
    },
    {
      "citation_id": "42",
      "title": "Data augmentation of wearable sensor data for parkinson's disease monitoring using convolutional neural networks",
      "authors": [
        "Terry Taewoong",
        "Franz Michael",
        "Josef Pfister",
        "Daniel Pichler",
        "Satoshi Endo",
        "Muriel Lang",
        "Sandra Hirche",
        "Urban Fietzek",
        "Dana Kulić"
      ],
      "year": "2017",
      "venue": "Data augmentation of wearable sensor data for parkinson's disease monitoring using convolutional neural networks",
      "arxiv": "arXiv:1706.00527"
    },
    {
      "citation_id": "43",
      "title": "Deep convolutional neural networks for computeraided detection: Cnn architectures, dataset characteristics and transfer learning",
      "authors": [
        "H Shin",
        "H Roth",
        "M Gao",
        "L Lu",
        "Z Xu",
        "I Nogues",
        "J Yao",
        "D Mollura",
        "R Summers"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Medical Imaging"
    },
    {
      "citation_id": "44",
      "title": "Shimmer ecg",
      "year": "2019",
      "venue": "Shimmer ecg"
    },
    {
      "citation_id": "45",
      "title": "Measuring emotion: the selfassessment manikin and the semantic differential",
      "authors": [
        "M Margaret",
        "Peter Bradley",
        "Lang"
      ],
      "year": "1994",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry"
    },
    {
      "citation_id": "46",
      "title": "Respiban professional",
      "year": "2019",
      "venue": "Respiban professional"
    },
    {
      "citation_id": "47",
      "title": "Development and validation of brief measures of positive and negative affect: the panas scales",
      "authors": [
        "David Watson",
        "Anna Clark",
        "Auke Tellegen"
      ],
      "year": "1988",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "48",
      "title": "",
      "authors": [
        "Tmsi-Mobi"
      ],
      "year": "2019",
      "venue": ""
    },
    {
      "citation_id": "49",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "50",
      "title": "A bayesian deep learning framework for end-to-end prediction of emotion from heartbeat",
      "authors": [
        "Ross Harper",
        "Joshua Southern"
      ],
      "year": "2019",
      "venue": "A bayesian deep learning framework for end-to-end prediction of emotion from heartbeat",
      "arxiv": "arXiv:1902.03043"
    },
    {
      "citation_id": "51",
      "title": "Multimodal emotion recognition using deep canonical correlation analysis",
      "authors": [
        "Wei Liu",
        "Jie-Lin Qiu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2019",
      "venue": "Multimodal emotion recognition using deep canonical correlation analysis",
      "arxiv": "arXiv:1908.05349"
    },
    {
      "citation_id": "52",
      "title": "Detecting work stress in offices by combining unobtrusive sensors",
      "authors": [
        "Saskia Koldijk",
        "Mark Neerincx",
        "Wessel Kraaij"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "53",
      "title": "A comprehensive framework for student stress monitoring in fog-cloud iot environment: m-health perspective",
      "authors": [
        "Prabal Verma",
        "K Sandeep",
        "Sood"
      ],
      "year": "2019",
      "venue": "Medical & Biological Engineering & Computing"
    },
    {
      "citation_id": "54",
      "title": "Using smart offices to predict occupational stress",
      "authors": [
        "Ane Alberdi",
        "Asier Aztiria",
        "Adrian Basarab",
        "Diane Cook"
      ],
      "year": "2018",
      "venue": "International Journal of Industrial Ergonomics"
    }
  ]
}