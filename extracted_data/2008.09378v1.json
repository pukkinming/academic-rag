{
  "paper_id": "2008.09378v1",
  "title": "Emograph: Capturing Emotion Correlations Using Graph Networks",
  "published": "2020-08-21T08:59:29Z",
  "authors": [
    "Peng Xu",
    "Zihan Liu",
    "Genta Indra Winata",
    "Zhaojiang Lin",
    "Pascale Fung"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Most emotion recognition methods tackle the emotion understanding task by considering individual emotion independently while ignoring their fuzziness nature and the interconnections among them. In this paper, we explore how emotion correlations can be captured and help different classification tasks. We propose EmoGraph that captures the dependencies among different emotions through graph networks. These graphs are constructed by leveraging the co-occurrence statistics among different emotion categories. Empirical results on two multi-label classification datasets demonstrate that EmoGraph outperforms strong baselines, especially for macro-F1. An additional experiment illustrates the captured emotion correlations can also benefit a single-label classification task. * * Equal contributions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Understanding human emotions is considered as the key to building engaging dialogue systems  (Zhou et al., 2018) . However, most works on emotion understanding tasks treat individual emotions independently while ignoring the fuzziness nature and the interconnections among them. A psychoevolutionary theory proposed by  Plutchik (1984)  shows that different emotions are actually correlated, and all emotions follow a circular structure. For example, \"optimism\" is close to \"joy\" and \"anticipation\" instead of \"disgust\" and \"sadness\". Without considering the fundamental intercorrelation between them, the understanding of emotions can be unilateral, leading to sub-optimal performance. These understanding can be particularly important for low resource emotions, such as \"surprise\" and \"trust\" whose training samples are hard to get. Therefore, the research question we ask is, how can we obtain and incorporate the emotion correlation to improve emotion understanding tasks, such as classification?\n\nTo obtain emotion correlations, a possible way is to take advantage of a multi-label emotion dataset. Intuitively, emotions with high correlations will be labeled together, and therefore, emotion correlations can be extracted from the label cooccurrences. Recently, a multi-label emotion classification competition  (Mohammad et al., 2018)  with 11 emotions has been introduced to promote research into emotional understanding. To tackle this challenge, the best team  (Baziotis et al., 2018)  first pre-trains on a large amount of external emotionrelated datasets and then performs transfer learning on this multi-label task. However, they still neglect the correlations between different emotions.\n\nIn this paper, we propose EmoGraph which leverages graph neural networks to model the dependencies between different emotions. We take each emotion as a node and first construct an emotion graph based on the co-occurrence statistics between every two emotion classes. Graph neural networks are then applied to extract the features from the neighbours of each emotion node. We conduct experiments on two multi-label emotion classification datasets. Empirical results show that our model outperforms strong baselines, especially for macro-F1 score. The analysis shows that low resource emotions, such as \"trust\", can particularly benefit from the emotion correlations. An additional experiment illustrates that the captured emotion correlations can also help the single-label emotion classification task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "For emotion classifications,  Tang et al. (2016)  proposed sentiment embeddings that incorporate sentiment information into word vectors.  Felbo et al. (2017)  trained a huge LSTM-based emotion representation by predicting emojis. Various methods have also been developed for automatic constructions of sentiment lexicons using both a supervised and unsupervised method  (Wang and Xia, 2017) .  Duppada et al. (2018)   Multi-label classification is an important yet challenging task in natural language processing. Binary relevance  (Boutell et al., 2004)  transformed the multi-label problem into several independent classifiers. Other methods to model the dependencies have since been proposed by creating new labels  (Tsoumakas and Katakis, 2007) , using classifier chains  (Read et al., 2011) , graphs  (Li et al., 2015) , and RNN  (Chen et al., 2017; Yang et al., 2018) . These models are either non-scalable or modeling the labels as a sequence.\n\nGraph networks have been applied to model relations across different tasks such as image recognition  (Chen et al., 2019; Satorras and Estrach, 2018) , and text classification  (Ghosal et al., 2019; Yao et al., 2019)  with different graph networks  (Kipf and Welling, 2017; Velikovi et al., 2018) .\n\nDespite the growing interests in low-resource studies in machine translation  (Artetxe et al., 2017; Lample et al., 2017) , dialogue systems  (Bapna et al., 2017; Liu et al., 2019 Liu et al., , 2020)) , speech recognition  (Miao et al., 2013; Thomas et al., 2013; Winata et al., 2020) , emotion recognition  (Haider et al., 2020) , and etc, emotion detection for low-resource emotions has been less studied.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we first introduce the emotion graphs and then our emotion classification models. We denote the input sentence as x and the emotion classes as e = {e 1 , e 2 , • • • , e n }. The label for x is y, where y ∈ {0, 1} n and y j denotes the label for e j . The embedding matrix is E. The co-occurrence matrix of these emotions is M .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Graphs",
      "text": "We take each emotion class as a node in the emotion graph. To create the connections between emotion nodes, we use the co-occurrence statistics between emotions. The intuition is that if two emotions co-occurs frequently, they will have a high correlation. Directly using this co-occurrence matrix as our graph may be problematic because the cooccurrence matrix is symmetric while the emotion relation is not symmetric. For example, in our corpus, \"anticipation\" co-occurs with \"optimism\" 197 times, while \"anticipation\" appears 425 times and \"optimism\" appears 1143 times. Thus, knowing \"anticipation\" and \"optimism\" co-occur is notably more important for \"anticipation\" than \"optimism\". Thus, we calculate the co-occurrence matrix M from a given emotion corpus and then normalize M i,j with M i,i so that the graph encodes the asymmetric relation between different emotions.\n\nDue to the fuzziness nature of emotions, the graph matrix G 1 may contain some noise. Thus, we adopted the approach in  Chen et al. (2019)  to binarize G 1 with a threshold µ to reduce noise and tune another hyper-parameter w to mitigate the over-smoothing problem  (Li et al., 2018) :\n\n(2)\n\n(3)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Classification Models",
      "text": "Our emotion classification model consists of an encoder and graph-based emotion classifiers following the framework of  Chen et al. (2019) . For the encoder, we choose the Transformer (TRS)  (Vaswani et al., 2017)  and the pre-trained model BERT  (Devlin et al., 2019)  for their strong representation capacities on many natural language tasks. We denote the encoded representation of x as s. For graphbased emotion classifiers, we experiment with two types of graph networks: GCN (Kipf and Welling, 2017) and GAT  (Velikovi et al., 2018) . The GCN takes the features of each emotion node as inputs and applies a convolution over neighboring nodes to generate the classifier for e i :\n\nwhere G is the normalized matrix of G following Kipf and Welling (  2017 ). E e is the embeddings for all emotions e, and W 1 is the trainable parameters.\n\nOur emotion classifiers are\n\nwhere C i is the classifier for e i . Alternatively, GAT takes the features of each node as input and learns a multi-head self-attention over the emotion nodes to generate classifiers C.\n\nWe then simply take the inner product between s and C i to compute ŷi , the logits of the emotion class e i for classification:\n\n(5)\n\nAs our task is a multi-label prediction problem, we add a sigmoid activation to ŷi and use a crossentropy loss function.\n\n4 Experimental Setup",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset And Evaluation Metrics",
      "text": "We choose two datasets for our multi-label classification training and evaluation. SemEval-2018  (Mohammad et al., 2018)     2018 ), we use Jaccard accuracy, micro-average F1-score (micro-F1), and macro-average F1-score (macro-F1) as our evaluation metrics.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emograph And Baselines",
      "text": "Our EmoGraph has four variants,TRS-GCN,TRS-GAT,BERT-GCN, and BERT-GAT, depending on the choice of sentence encoder (TRS/BERT) and graph networks (GCN/GAT). We compare our model to several strong baselines. NTUA-SLP  (Baziotis et al., 2018)  is the top-1 system of the SemEval-2018 competition, which pre-trains the model on large amounts of external emotionrelated datasets. DATN  (Yu et al., 2018)  is the system that transfers sentiment information with dual attention transfer network. SGM  (Yang et al., 2018)   the labels using beam search. TRS is the system that adds a linear layer on top of the Transformer  (Vaswani et al., 2017) . BERT is the system that adds a linear layer on top of the BERT  (Devlin et al., 2019) .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results On Two Emotion Classification Datasets",
      "text": "The results on SemEval-2018 dataset are shown in Table  1 , which illustrates several points. Firstly, BERT-GCN/TRS-GCN consistently improves over the baseline of BERT/TRS, in terms of accuracy (+0.5%/+0.8%), micro-F1 (+0.6%/+0.4%) and macro-F1 (+2.5%/+2.5%). It shows the effectiveness of the emotion graph by giving the performance boost on the strong baseline BERT, especially on the macro-F1 score. Secondly, our BERT-GCN achieves a 58.9% accuracy score, 70.7% micro-F1 score, and 56.3% macro-F1 score, which beats the best system NTUA-SLP in terms of all metrics, without using any affect features or pretraining on emotional datasets. Thirdly, EmoGraph is particularly effective in macro-F1. For example, BERT-GAT is 2.5% better than DATN and 3.1% better than BERT. We notice that for both accuracy and micro-F1, the improvements of graph networks are very marginal on the SemEval-2018 dataset. This is because the small emotion label space (only 11 emotions) limits the effectiveness of emotion graphs. Thus, we train our models on another Twitter dataset with 64 emoji labels. Table  2  shows both TRS-GCN and TRS-GAT consistently improves TRS with a large margin for all metrics, which shows that graph networks are better at dealing with rich emotion information. Connection to the wheel of emotions Following the positioning of the wheel of emotion  (Plutchik, 1984) , we visualize the graph structure G in Figure  1 . It shows that our emotion graph can be divided into three sub-graphs, 1) the nodes connected with \"disgust\", 2) \"fear\" 3) the nodes connected with \"joy\". Most nodes are locally connected, which means the positioning in the wheel of emotion does contain the correlation information.\n\nOne exception is that \"anticipation\" is also close to 1 Data statistics, training details, and ablation studies for threshold µ (Eq. 2) and weight w (Eq. 3) are in the appendix.\n\n\"anger\" in the wheel of emotions, which is not true in our case. Another exception we found is that \"surprise\" is close to \"joy\" in our emotion graph while they are quite far away from each other in the wheel of emotions. We believe it reflects the bias when people post tweets online, and the distance between neighboring emotions are not quantitatively the same as the wheel of emotions.  4  shows that the significant improvements in terms of the macro-F1 on the SemEval-2018 dataset mainly come from the low resource emotions, such as \"surprise\" and \"trust\", where only 5% of the labels are positive. From Figure  1 , we observe that both \"surprise\" and \"trust\" are connected to \"joy\", which has 39.3% samples labeled as positive. We conjecture that low resource emotion classifiers learn effective inductive bias through connections with high-resource emotion classes, such as \"joy\" and \"optimism\", and achieve better performances.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Analysis On Low Resource Emotions Table",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Generalization To Single Emotion Classification Task",
      "text": "To verify the generalization ability of Emo-Graph, we conduct an extra experiment on IEMO-CAP dataset  (Busso et al., 2008) , which is a single label multi-class classification dataset. We only use the six emotions that overlap with the SemEval-2018 task, which are \"anger\", \"sadness\", \"happiness\", \"disgust\", \"fear\" and \"surprise\" with 2931 samples (the graph matrix G is obtained from the SemEval-2018 dataset). We then train our Emo-Graph using either a one-layer LSTM with attention or BERT as an encoder and GCN as the graph structure. To adapt the change from 11 emotions to 6 emotions, we only train the classifiers C i corresponding to those six emotions. The results with 10-fold cross-validation are reported in Table 3. We didn't report \"disgust\" as there are less than five positive examples. The results confirm that with captured emotion correlations, classification performances can be improved by 2.8%/8.5% accuracy/average F1 score using LSTM encoder and 1.3%/3.4% accuracy/average F1 score using BERT encoder. Surprisingly, for both encoders with GCN improves more than 10% on \"fear\" (a low-resource emotion in the IEMOCAP dataset), although it doesn't have connections with other emotions in the graph. We conjecture that the improvements come from the shared representation of W 1 in Eq. 4 among all emotion categories, which helps our model optimize to a better local minimal.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed EmoGraph which leverages graph neural networks to model the dependencies among different emotions. We consider each emotion as a node and construct an emotion graph based on the co-occurrence statistics. Our model with EmoGraph outperforms the existing strong multi-label classification baselines. Our analysis shows EmoGraph is especially helpful for low resource emotions and large emotion space. An additional experiment shows that it can also help a single-label emotion classification task.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A Appendices A.1 Preprocessing",
      "text": "To cope with the noise in the twitter data, we lowercase the tweets, remove the \"#\" punctuation and replace \"URL link\" with a special token <url>, \"user mentions\" with <user> and \"number\" with <num>. For example, one original tweet \"@Jessi-caZ00 @ZRlondon ditto!! Such an amazing atmosphere! We have 10 people here. #LondonEvents #cheer\" will be cleaned as \"<user> <user> ditto! such an amazing atmosphere! we have <num> people here. londonevents cheer\". By doing so, we can reduce the vocabulary size and make the model easier to learn.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A.2 Training Details",
      "text": "To train the SemEval-2018, we calculate the cooccurrence matrix G 1 based on the training and development sets. We set the threshold µ as 0.4, and the weight w as 0.35 for GCN. We set the threshold µ as 0.5 for GAT. For both BERT-GCN and BERT-GAT, the emotion label embeddings are initialized with 300-dim GloVe vectors. For the BERT structure, we choose 12 layers BERT-base model, and the hidden size of the graph is set to 768. An Adam optimizer with the learning rate 1e-4 is used to train the graph networks. Another Adam optimizer is used to train the BERT model with the learning set as 2e-5 and dropout rate as 0.3. For TRS-GCN and TRS-GAT, the hidden size of the graph network is set to 200. The heads of the Transformer are 4, and the depth is 44. An Adam optimizer is used to train the full model with a 0.001 learning rate. For the Twitter dataset, we calculate the co-occurrence matrix G 1 based on the training set. We set the threshold µ as 0.1, and the weight w is set as 0.35 for both. We use the TRS as the sentence encoder. The hidden size of both graph networks and TRS are 200. The heads of TRS are 6, and the depth is 120. An Adam optimizer is used to train our model with a 0.001 learning rate.\n\nTo train IEMOCAP dataset, the parameters of both GCN and BERT follow the same setting as in SemEval 2018 task. For the LSTM based method, we set the hidden size of LSTM encoder as 200 and initialize the word embedding with GloVe. An Adam optimizer with a learning rate of 0.001 is used to optimize all parameters.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A.3 Data Statistics",
      "text": "The data statistics for the SemEval-2018, Twitter and IEMOCAP are shown in Table  5 , Table  6 , and Table  7 , respectively.\n\nFrom Table  5 , we can see that in the SemEval-2018 dataset, only 5.2% and 5.0% samples are labeled as positive for \"surprise\" and \"trust\", respectively. And from Table  7 , we can see that in the IEMOCAP dataset, only 1.4% samples are labeled as positive for \"fear\".",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A.4 Effects Of Threshold Μ And Weight W",
      "text": "As illustrated in Figure  2  and Figure  3 , we plot the diagram of accuracy, micro-F1 and macro-F1 on the test set by varying µ and w, respectively, for BERT-GCN. We also include another baseline BERT-GCN model which is trained by setting G = G 1 (w/o binarization), to see the effects of binarization (Eq. 2) and weighting (Eq. 3). Figure  2  shows that removing moderate amount of noisy connections by changing µ can improve the performance. For macro-F1, it clearly shows that small µ achieves much better results than large µ. If µ is large, useful emotion connections might be cut off, therefore leading to worse results.\n\nFigure  3  shows that for accuracy and micro-F1, changing w can achieve better performance than the baseline without binarization and weighting.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The graph visualization of G. The arrow de-",
      "page": 4
    },
    {
      "caption": "Figure 1: It shows that our emotion graph",
      "page": 4
    },
    {
      "caption": "Figure 1: , we observe that both “sur-",
      "page": 4
    },
    {
      "caption": "Figure 2: and Figure 3, we plot",
      "page": 8
    },
    {
      "caption": "Figure 2: Evaluation Scores for different threshold µ",
      "page": 9
    },
    {
      "caption": "Figure 3: Evaluation Scores for different weight w on",
      "page": 9
    },
    {
      "caption": "Figure 2: shows that removing moderate amount",
      "page": 9
    },
    {
      "caption": "Figure 3: shows that for accuracy and micro-F1,",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "micro-\njaccar",
          "Column_8": "F1 (w/o\nd acc. (",
          "Column_9": "binariza\nw/o bina",
          "Column_10": "tion)\nrization",
          "Column_11": ")"
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "macro\nmicro-",
          "Column_8": "-F1 (w/\nF1",
          "Column_9": "o binariz",
          "Column_10": "ation)",
          "Column_11": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "jaccar\nmacro",
          "Column_8": "d acc.\n-F1",
          "Column_9": "",
          "Column_10": "",
          "Column_11": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "m\nja",
          "Column_8": "icro-F1\nccard a",
          "Column_9": "(w/o b\ncc. (w/",
          "Column_10": "inariza\no bina",
          "Column_11": "tion)\nrization",
          "Column_12": ")"
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "m\nm",
          "Column_8": "acro-F\nicro-F1",
          "Column_9": "1 (w/o",
          "Column_10": "binariz",
          "Column_11": "ation)",
          "Column_12": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "ja\nm",
          "Column_8": "ccard a\nacro-F",
          "Column_9": "cc.\n1",
          "Column_10": "",
          "Column_11": "",
          "Column_12": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Unsupervised neural machine translation",
      "authors": [
        "Mikel Artetxe",
        "Gorka Labaka",
        "Eneko Agirre",
        "Kyunghyun Cho"
      ],
      "year": "2017",
      "venue": "Unsupervised neural machine translation",
      "arxiv": "arXiv:1710.11041"
    },
    {
      "citation_id": "2",
      "title": "Towards zero-shot frame semantic parsing for domain scaling",
      "authors": [
        "Ankur Bapna",
        "Gokhan Tür",
        "Dilek Hakkani-Tür",
        "Larry Heck"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "3",
      "title": "Ntua-slp at semeval-2018 task 1: Predicting affective content in tweets with deep attentive rnns and transfer learning",
      "authors": [
        "Christos Baziotis",
        "Athanasiou Nikolaos",
        "Alexandra Chronopoulou",
        "Athanasia Kolovou"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "4",
      "title": "Learning multilabel scene classification",
      "authors": [
        "Jiebo Matthew R Boutell",
        "Xipeng Luo",
        "Christopher Shen",
        "Brown"
      ],
      "year": "2004",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "5",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "6",
      "title": "Ensemble application of convolutional and recurrent neural networks for multi-label text categorization",
      "authors": [
        "Guibin Chen",
        "Deheng Ye",
        "Zhenchang Xing",
        "Jieshan Chen",
        "Erik Cambria"
      ],
      "year": "2017",
      "venue": "2017 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "7",
      "title": "Multi-label image recognition with graph convolutional networks",
      "authors": [
        "Xiu-Shen Zhao-Min Chen",
        "Peng Wei",
        "Yanwen Wang",
        "Guo"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "9",
      "title": "Seernet at semeval-2018 task 1: Domain adaptation for affect in tweets",
      "authors": [
        "Venkatesh Duppada",
        "Royal Jain",
        "Sushant Hiray"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "10",
      "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
      "authors": [
        "Bjarke Felbo",
        "Alan Mislove",
        "Anders Søgaard",
        "Iyad Rahwan",
        "Sune Lehmann"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Empathetic dialog systems",
      "authors": [
        "Pascale Fung",
        "Dario Bertero",
        "Peng Xu",
        "Ji Park",
        "Chien-Sheng Wu",
        "Andrea Madotto"
      ],
      "venue": "Empathetic dialog systems"
    },
    {
      "citation_id": "12",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition in lowresource settings: An evaluation of automatic feature selection methods",
      "authors": [
        "Fasih Haider",
        "Senja Pollak",
        "Pierre Albert",
        "Saturnino Luz"
      ],
      "year": "2020",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "14",
      "title": "Semisupervised classification with graph convolutional networks",
      "authors": [
        "N Thomas",
        "Max Kipf",
        "Welling"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "15",
      "title": "Unsupervised machine translation using monolingual corpora only",
      "authors": [
        "Guillaume Lample",
        "Alexis Conneau",
        "Ludovic Denoyer",
        "Marc'aurelio Ranzato"
      ],
      "year": "2017",
      "venue": "Unsupervised machine translation using monolingual corpora only",
      "arxiv": "arXiv:1711.00043"
    },
    {
      "citation_id": "16",
      "title": "Deeper insights into graph convolutional networks for semi-supervised learning",
      "authors": [
        "Qimai Li",
        "Zhichao Han",
        "Xiao-Ming Wu"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Sentence-level emotion classification with label and context dependence",
      "authors": [
        "Shoushan Li",
        "Lei Huang",
        "Rong Wang",
        "Guodong Zhou"
      ],
      "year": "2015",
      "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Moel: Mixture of empathetic listeners",
      "authors": [
        "Zhaojiang Lin",
        "Andrea Madotto",
        "Jamin Shin",
        "Peng Xu",
        "Pascale Fung"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "19",
      "title": "Zero-shot cross-lingual dialogue systems with transferable latent variables",
      "authors": [
        "Zihan Liu",
        "Jamin Shin",
        "Yan Xu",
        "Genta Indra Winata",
        "Peng Xu",
        "Andrea Madotto",
        "Pascale Fung"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "20",
      "title": "Coach: A coarse-to-fine approach for cross-domain slot filling",
      "authors": [
        "Zihan Liu",
        "Genta Indra Winata",
        "Peng Xu",
        "Pascale Fung"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Deep maxout networks for low-resource speech recognition",
      "authors": [
        "Yajie Miao",
        "Florian Metze",
        "Shourabh Rawat"
      ],
      "year": "2013",
      "venue": "2013 IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "22",
      "title": "Semeval-2018 task 1: Affect in tweets",
      "authors": [
        "Saif Mohammad",
        "Felipe Bravo-Marquez",
        "Mohammad Salameh",
        "Svetlana Kiritchenko"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "23",
      "title": "Plusemo2vec at semeval-2018 task 1: Exploiting emotion knowledge from emoji and# hashtags",
      "authors": [
        "Ji Ho",
        "Peng Xu",
        "Pascale Fung"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "24",
      "title": "Emotions: A general psychoevolutionary theory. Approaches to emotion",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1984",
      "venue": "Emotions: A general psychoevolutionary theory. Approaches to emotion"
    },
    {
      "citation_id": "25",
      "title": "Classifier chains for multi-label classification",
      "authors": [
        "Jesse Read",
        "Bernhard Pfahringer",
        "Geoff Holmes",
        "Eibe Frank"
      ],
      "year": "2011",
      "venue": "Machine learning"
    },
    {
      "citation_id": "26",
      "title": "Few-shot learning with graph neural networks",
      "authors": [
        "Garcia Victor",
        "Joan Satorras",
        "Estrach Bruna"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "27",
      "title": "Happybot: Generating empathetic dialogue responses by improving user experience lookahead",
      "authors": [
        "Jamin Shin",
        "Peng Xu",
        "Andrea Madotto",
        "Pascale Fung"
      ],
      "year": "2019",
      "venue": "Happybot: Generating empathetic dialogue responses by improving user experience lookahead",
      "arxiv": "arXiv:1906.08487"
    },
    {
      "citation_id": "28",
      "title": "Sentiment embeddings with applications to sentiment analysis",
      "authors": [
        "Duyu Tang",
        "Furu Wei",
        "Bing Qin",
        "Nan Yang",
        "Ting Liu",
        "Ming Zhou"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "29",
      "title": "Deep neural network features and semi-supervised training for low resource speech recognition",
      "authors": [
        "Samuel Thomas",
        "L Michael",
        "Kenneth Seltzer",
        "Hynek Church",
        "Hermansky"
      ],
      "year": "2013",
      "venue": "2013 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "30",
      "title": "Multi-label classification: An overview",
      "authors": [
        "Grigorios Tsoumakas",
        "Ioannis Katakis"
      ],
      "year": "2007",
      "venue": "International Journal of Data Warehousing and Mining (IJDWM)"
    },
    {
      "citation_id": "31",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "32",
      "title": "Graph attention networks",
      "authors": [
        "Petar Velikovi",
        "Guillem Cucurull",
        "Arantxa Casanova",
        "Adriana Romero",
        "Pietro Li",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "33",
      "title": "Sentiment lexicon construction with representation learning based on hierarchical sentiment supervision",
      "authors": [
        "Leyi Wang",
        "Rui Xia"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "34",
      "title": "Learning fast adaptation on cross-accented speech recognition",
      "authors": [
        "Genta Indra Winata",
        "Samuel Cahyawijaya",
        "Zihan Liu",
        "Zhaojiang Lin",
        "Andrea Madotto",
        "Peng Xu",
        "Pascale Fung"
      ],
      "year": "2020",
      "venue": "Learning fast adaptation on cross-accented speech recognition",
      "arxiv": "arXiv:2003.01901"
    },
    {
      "citation_id": "35",
      "title": "Emo2vec: Learning generalized emotion representation by multitask training",
      "authors": [
        "Peng Xu",
        "Andrea Madotto",
        "Chien-Sheng Wu",
        "Ji Park",
        "Pascale Fung"
      ],
      "year": "2018",
      "venue": "Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "36",
      "title": "Clickbait? sensational headline generation with auto-tuned reinforcement learning",
      "authors": [
        "Peng Xu",
        "Chien-Sheng Wu",
        "Andrea Madotto",
        "Pascale Fung"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "37",
      "title": "Sgm: Sequence generation model for multi-label classification",
      "authors": [
        "Pengcheng Yang",
        "Xu Sun",
        "Wei Li",
        "Shuming Ma",
        "Wei Wu",
        "Houfeng Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "38",
      "title": "Graph convolutional networks for text classification",
      "authors": [
        "Liang Yao",
        "Chengsheng Mao",
        "Yuan Luo"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Improving multilabel emotion classification via sentiment classification with dual attention transfer network",
      "authors": [
        "Jianfei Yu",
        "Luís Marujo",
        "Jing Jiang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1137"
    },
    {
      "citation_id": "40",
      "title": "The design and implementation of xiaoice, an empathetic social chatbot",
      "authors": [
        "Li Zhou",
        "Jianfeng Gao",
        "Di Li",
        "Heung-Yeung Shum"
      ],
      "year": "2018",
      "venue": "The design and implementation of xiaoice, an empathetic social chatbot",
      "arxiv": "arXiv:1812.08989"
    }
  ]
}