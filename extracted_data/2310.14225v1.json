{
  "paper_id": "2310.14225v1",
  "title": "Customising General Large Language Models For Specialised Emotion Recognition Tasks",
  "published": "2023-10-22T08:09:13Z",
  "authors": [
    "Liyizhe Peng",
    "Zixing Zhang",
    "Tao Pang",
    "Jing Han",
    "Huan Zhao",
    "Hao Chen",
    "Björn W. Schuller"
  ],
  "keywords": [
    "Emotion Recognition",
    "Large Language Model",
    "Prompt Tuning",
    "Low-Rank Adaptation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The advent of large language models (LLMs) has gained tremendous attention over the past year. Previous studies have shown the astonishing performance of LLMs not only in other tasks but also in emotion recognition in terms of accuracy, universality, explanation, robustness, few/zero-shot learning, and others. Leveraging the capability of LLMs inevitably becomes an essential solution for emotion recognition. To this end, we further comprehensively investigate how LLMs perform in linguistic emotion recognition if we concentrate on this specific task. Specifically, we exemplify a publicly available and widely used LLM -Chat General Language Model, and customise it for our target by using two different modal adaptation techniques, i. e., deep prompt tuning and low-rank adaptation. The experimental results obtained on six widely used datasets present that the adapted LLM can easily outperform other state-ofthe-art but specialised deep models. This indicates the strong transferability and feasibility of LLMs in the field of emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition, a highly interdisciplinary research field spanning psychology, cognitive, and computer science, plays an increasingly important role in research related to human-computer interaction  [1] . Over the past decades, the domain of emotion recognition has undergone a profound transformation, thanks to the growing wealth of emotion datasets, enhanced computational capabilities, and continuous advancements in deep learning algorithms.\n\nRecently, the emergence of large language models (LLMs), exemplified by ChatGPT and Claude, has ushered in a new era in the domain of emotion recognition. LLMs are typically pretrained on vast text corpora, showcasing their robust capabilities in various domains, including text generation and natural language understanding (NLU). Prior research has illuminated the remarkable capability of LLMs in the realm of emotion recognition, attaining commendable benchmarks in accuracy, universality, explanation, robustness, and few/zero-shot learning, among others  [2] . However, it is imperative to address certain challenges. While few-shot learning can enhance model performance by providing limited demonstration examples within prompts, extending prompt length results in a quadratic escalation in inference computational costs. Furthermore, overly lengthy prompts risk truncation, as they may exceed maximum input limits, leading to diminished LLM output quality.\n\nAs such, researchers and engineers are tasked with the endeavour to devise efficient methodologies for fine-tuning LLMs on domainspecific datasets. The modal adaptation technique represents a training strategy meticulously crafted to further refine a pretrained model, with the principal objective of aligning the model's capabilities with specific tasks or domains. This methodology can facilitate the tailoring of pretrained LLMs to cater to particular downstream tasks, all while preserving their formidable language comprehension prowess.\n\nTo this end, we aim to shed some light on how LLMs perform if they are customised to the emotion recognition domain, and to find out whether they are competitive or better than a conventional deep model specifically designed for emotion recognition. For this purpose, we select a specific open-source LLM, i. e., the Chat General Language Model, and employ two distinct model adaptation techniques: deep prompt tuning (P-Tuning v2) and low-rank adaptation (LoRA). Subsequently, we conduct a comprehensive comparative analysis, evaluating the performance of LLMs both pre-and post-adaptation on six emotional datasets. Furthermore, to provide a holistic perspective on the effectiveness and advancements achieved through these model adaptation approaches, we conducted comparative assessments with other state-of-the-art (SOTA) non-LLM-based studies. These comparative evaluations enable us to gauge the relative merits and contributions of different model adaptation strategies in the context of emotion recognition. It is hoped that this work will bring more discussions in the field of emotion recognition, as the new era of general large models is coming.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "A substantial body of research has been dedicated to the domain of model adaptation for pre-trained LLMs. Within the spectrum of contemporary approaches, a widely-used and the most basic technique is known as full fine-tuning (FFT), necessitating the retraining of all model parameters. While FFT has proven effective in enhancing LLM performance, it demands substantial computational resources during training, incurring significant costs and rendering it increasingly impractical. In response to the need for reducing the computational burden, the research community has introduced numerous parameter-efficient fine-tuning (PEFT) methods  [3] . These innovative methodologies entail the selective training of a limited subset of model parameters, either by modifying existing parameters or introducing novel ones into the model architecture.\n\nThe strategies for training partial model parameters involve adapting the characteristics of layer types or internal architecture within a network  [3] . Methods involving the introduction of additional parameters can be broadly categorised into two groups: Adapter-like methods and Soft Prompts methods. Adapters in-  troduce small fully-connected networks after Transformer sublayers  [4] . Soft prompts can be trained for the input layer exclusively or for all layers within the model. Furthermore, reparametrisationbased PEFT methods leverage low-rank representations to minimise the number of trainable parameters.\n\nIn this paper, we choose two widely recognised model adaptation methods, namely P-Tuning v2 and LoRA. These two methods involve a limited number of trainable parameters, which ensures their computational resource demands remain within reasonable limits. Consequently, the adaptation training process can be feasibly conducted on consumer-grade graphics processing units (GPUs), rendering both P-Tuning v2 and LoRA highly practical choices.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Adaptation Of Large Language Models",
      "text": "In this section, we present an introduction to the selected LLM -Chat General Language Model, alongside an explanation of the core principles underlying the two model adaptation techniques, namely P-Tuning and LoRA.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "General Language Model",
      "text": "General Language Model (GLM) is a general pre-training framework based on a novel autoregressive blank infilling objective and can be adapted to various NLU and natural language generation tasks  [5] . GLM formulates NLU tasks as 'cloze' questions that contain task descriptions, which can be answered by autoregressive generation. Remarkably, when operating with equivalent parameter counts and computational resources, GLM consistently outperforms BERT on the SuperGLUE benchmark, and excels beyond RoBERTa and BART when pretrained on corpora of comparable sizes  [5] . Furthermore, GLM demonstrates superior performance to T5 in NLU and generation tasks, while utilising fewer parameters and data  [5] .\n\nChat General Language Model (ChatGLM), launched in March 2023 by the Tsinghua University KEG Laboratory and the Zhipu AI Company, is a GLM-based AI Chatbot. ChatGLM draws inspiration from ChatGPT to integrate code pre-training into the trillionparameter base model GLM-130B  [6] , achieving human intent alignment through techniques like supervised fine-tuning. It is worth mentioning that ChatGLM was pre-trained on both Chinese and English corpora, thus, it possesses bilingual capabilities. ChatGLM2, a second-generation model, is open-sourced in June 2023. It boasts enhanced performance, extended context capabilities, improved inference efficiency, and a more permissive open-source license. In our study, we opted to utilise the more lightweight model ChatGLM2-6B, which employs the same technology as ChatGLM2 but with a reduced parameter count of 6.2 billion. It requires a minimum of 13GB of GPU memory for inference when using FP16 precision, enabling the deployment of ChatGLM-6B on consumer-grade graphics cards.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "P-Tuning",
      "text": "P-Tuning (aka Prompt Tuning) is a cost-effective model adaptation methodology, which freezes all model parameters and introduces additional parameters  [7] . It can be viewed as an optimised and tailored implementation of deep prompt tuning, specifically designed for generation and knowledge probing tasks. Deep prompt tuning expands the capacity of continuous prompts, bridging the adaptation gap across various settings, with particular efficacy for smaller models and challenging tasks. Nevertheless, the P-Tuning method is subject to certain constraints due to its exclusive utilisation of continuous prompts within the input embedding sequence. This results in a limited number of trainable parameters, and the input embeddings have a relatively indirect impact on model predictions.\n\nTo overcome these challenges, the P-Tuning v2 technique incorporates continuous prompts into every layer of the model (shown in Fig.  1 (a) ), rather than solely in the input embedding sequence  [7] . Such adjustment in P-tuning v2 introduces a greater number of tunable task-specific parameters (from 0.01% to 0.1%-3%) to enhance task-specific capacity while maintaining parameter efficiency. Additionally, prompts added to deeper layers have a more direct impact on model predictions. Currently, P-Tuning v2 consistently achieves comparable performance to fine-tuning across a broad spectrum of model scales, ranging from 300M to 10B parameters  [7] . It was frequently demonstrated to perform particularly well on challenging sequence tagging tasks, including extractive question answering and named entity recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Low-Rank Adaptation",
      "text": "Low-Rank Adaptation (LoRA) is one of the reparametrisation-based model adaptation methods  [8]  and is illustrated in Fig.  1 (b) . It freezes the pre-trained model weights and injects trainable low-rank decomposition matrices into each layer of the Transformer architecture, considerably reducing the number of trainable parameters for downstream tasks. Its inspiration comes from a statement that pretrained language models have a lower \"intrinsic dimension\" and can still effectively learn despite being randomly projected into smaller subspaces  [8] . Therefore, researchers hypothesised the updates to the weights also have a low \"intrinsic rank\" during adaptation and proposed LoRA methods. Compared to GPT-3's 175B parameters fine-tuned with Adam, LoRA can achieve a 10,000-fold reduction in the number of trainable parameters and a 3-fold decrease in GPU memory requirements  [8] . In terms of effectiveness, LoRA matches or surpasses full fine-tuning for RoBERTa, DeBERTa, GPT-2, and GPT-3, even though it has fewer trainable parameters and a higher training throughput  [8] . More importantly, it does not introduce any additional inference latency.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Selected Datasets",
      "text": "In this part, we present the six datasets utilised in our research. More detailed information is placed in Table  1 , ranging from English to Chinese languages, from binary/ternary sentiment anlaysis to multiclass emotion classification. Since these datasets are publicly accessible, we can use them to verify the effectiveness of different model adaptation methods.\n\nSST: The Stanford Sentiment Treebank is an English corpus containing fine-grained sentiment annotations for 11,855 individual sentences sourced from movie review data  [9] . For the fine-grained task, each sentence is categorised into one of five sentiment classes. For the binary task, each sentence is simply classified as either positive or negative, with the neutral category excluded.\n\nFriends: Friends is an English corpus based on the TV show Friends, containing 1,000 dialogues from seasons one to nine  [10] . The 14,503 utterances from the 1,000 dialogues are categorised into seven classes. The annotators considered the context of the dialogue when labelling sentiments.\n\nMastodon: The Mastodon dataset  [11]  consists of English posts from the Mastodon social media platform. While the dataset was initially designed for both sentiment recognition and dialogue act recognition, we only focus on the former.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mosi:",
      "text": "The Multimodal Opinion-level Sentiment Intensity (MOSI) dataset  [12]  is a multimodal sentiment analysis dataset, including 2,199 opinion segments extracted from 93 videos. Each opinion segment received annotations on a sentiment spectrum ranging from highly negative to highly positive within the interval  [-3, 3] .\n\nCH-SIMS: CH-SIMS is a Chinese single-and multi-modal sentiment analysis dataset  [13] . It collected 2,281 video segments from movies, TV series, and a variety of shows. The sentiment annotation is divided into five categories.\n\nM 3 ED: Multi-modal Multi-scene Multi-label Emotional Dialogue (M 3 ED) is the first multimodal emotional dialogue dataset in Chinese  [14] . The dataset contains 990 dyadic emotional dialogues from 56 different TV series, including 9,082 turns and 24,449 utterances.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation Details",
      "text": "We conducted emotion recognition tasks on six selected datasets to evaluate the effectiveness of two model adaptation methods on LLMs, i. e., P-Tuning v2 and LoRA. For the SST dataset, we conducted both binary and five-class classification tasks. Three-class sentiment classification tasks, distinguishing among positive, neutral, and negative sentiments, were performed on the MOSI and the Mastodon dataset. The CH-SIMS dataset and the MOSI were used for a binary classification task: positive and negative. Finally, we implemented a seven-class emotion classification task on the Friends and M 3 ED datasets. Moreover, for the purpose of performance comparison with specialised emotion recognition models, we have chosen recently published SOTA works that exhibit competitive perfor-mance on each selected datasets, separately, under strictly comparable conditions.\n\nFor each individual dataset, we designed three sets of comparative experiments: ChatGLM2 without adaptation, ChatGLM2 adapted with P-Tuning v2, and ChatGLM2 adapted with LoRA. During the inference with ChatGLM2, we utilise a \"prompt\" to acquire a response from it. The prompt should encompass both task-guiding sentences that necessitate emotion recognition. Our prompt is structured as follows: Classify the sentiment of the sentence to Emotion 1, Emotion 2, ... or Emotion k: <provide only one sentence from a test set>. The value of k here is determined by the number of sentiment/emotion categories specific to the dataset. For example, the prompt is \"Classify the sentiment of the sentence to Positive, Negative or Neutral\" for MOSI as k = 3. When adapting the model, we add a task-guiding sentence before each training sample to construct a complete prompt, and then present it into the model for learning. Note that, although the Mastodon, Friends, and M 3 ED datasets are context-based, we treat them like other datasets, regardless of the context.\n\nOur experiments were conducted on an NVIDIA GeForce RTX 3090 with 24GB of RAM, and the adaptation training and inference tasks were performed only on one single GPU. For the adaptation training, we set the training batch size to 16 due to the constraints of GPU memory. Additionally, we set the prompt length to 32 for P-Tuning v2, and configured the rank of 8 for LoRA. We employed the accuracy and macro F1 score as the primary metrics for performance evaluation. For the M 3 ED dataset, we employed the weighted average F1 score to provide equitable comparisons with other research.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "To evaluate the effectiveness and the transferability of generalised LLMs in emotion recognition, we conducted extensive experiments on six publicly available datasets (see Section 4.1). Table  2  to Table 5 present the results obtained from the ChatGLM2 with or without adaptation technologies on these six datasets, respectively. Besides, for each selected dataset, we offer SOTA performance from specialised models in the latest studies for comparison.\n\nFirst of all, we can see that ChatGLM2 performs competitively with these specialised models in many datasets, such as MOSI, CH-SIMS, and Mastodon. This finding is consistent with the one shown in our previous work but evaluated with other LLMs  [2] . For the datasets of Friends and M 3 ED, there is an obvious performance gap, which might be attributed to the lack of context information provided for ChatGLM2 for inference. Then, when comparing the performance of ChatGLM2 with or without adaptation, we can generally observe that the adapted large models, either by P-Tuning v2 or by LoRA, considerably outperform the non-adapted ones, both in binary and multi-class classification tasks. For instance, on the SST-5 dataset (cf. Table  3 ), the P-Tuning v2 method performs the most substantial improvement. The accuracy and macro F1 scores increase from 30.09 % and 25.82 % to 57.59 % and 56.45 %, nearly doubling the performance before adaptation. This suggests that both P-Tuning v2 and LoRA algorithms work efficiently for the adaptation of LLMs in emotion recognition.\n\nMoreover, it can be seen that the adapted ChatGLM2s outperform other SOTA-specialised model in most cases, but vary depending on the complexity of the classification tasks. In simpler tasks like binary or three-class classification, adapted models often outperform SOTA-specialised models. Conversely, for tasks involving five or more categories (e. g., Friends and M3ED), models with adaptation remain a substantial performance gap compared to SOTA works. This is largely due to the missing of context information for training and inference as aforementioned. Surprisingly, for the context-rich Mastodon dataset, even without considering context during adaptation, the adapted model exhibits superior performance compared to the SOTA works. This could be attributed to the relative simplicity of the three-class classification task or the dataset may have a less pronounced dependency on contextual information. Generally speaking, these observations indicate that the pretrained and generalised ChatGLM2 can efficiently transfer their knowledge to a specific domain without much training data and computation resources.\n\nFinally, in comparison with the two selected model adaptation methods, i. e., P-Tuning v2 and LoRA, the latter outperforms in binary tasks, while the former demonstrates superior performance in ternary and multi-class tasks. Consequently, there is no consistent observation to definitively favour one method over the other, as the optimal adaptation approach varies across different datasets.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we focused on the capability of different model adaptation methods for Large Language Models (LLMs) in the field of emotion recognition. We investigate this by assessing the performance of the Chat General Language Model on six datasets using two adaptation techniques, i. e., deep prompt tuning and lowrank adaptation. The experimental result shows that both adaptation methods perform exceptionally well in emotion recognition tasks, particularly for simple classification tasks that are without context. Compared to traditional specialised models, utilising the adapted LLMs for emotion recognition considerably reduces the modelling efforts for researchers, and the computational resources required for adaptation are also accessible. This opens up brand-new possibilities for future emotion recognition systems.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Schematic representation of two model adaptation methods:",
      "page": 2
    },
    {
      "caption": "Figure 1: (a)), rather than solely in the input embedding sequence [7].",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "As such,\nresearchers and engineers are tasked with the endeavour"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "to devise efficient methodologies for fine-tuning LLMs on domain-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "specific datasets. The modal adaptation technique represents a train-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "ing strategy meticulously crafted to further refine a pretrained model,"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "with the principal objective of aligning the model’s capabilities with"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "specific tasks or domains. This methodology can facilitate the tailor-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "ing of pretrained LLMs to cater to particular downstream tasks, all"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "while preserving their formidable language comprehension prowess."
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "To this end, we aim to shed some light on how LLMs perform if"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "they are customised to the emotion recognition domain, and to find"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "out whether they are competitive or better than a conventional deep"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "model specifically designed for emotion recognition. For this pur-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "pose, we select a specific open-source LLM,\ni. e.,\nthe Chat General"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "Language Model, and employ two distinct model adaptation tech-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "niques:\ndeep prompt\ntuning (P-Tuning v2) and low-rank adapta-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "tion (LoRA). Subsequently, we conduct a comprehensive compar-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "ative analysis, evaluating the performance of LLMs both pre- and"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "post-adaptation on six emotional datasets. Furthermore, to provide a"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "holistic perspective on the effectiveness and advancements achieved"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "through these model adaptation approaches, we conducted compara-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "tive assessments with other state-of-the-art (SOTA) non-LLM-based"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "studies. These comparative evaluations enable us to gauge the rela-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "tive merits and contributions of different model adaptation strategies"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "in the context of emotion recognition. It is hoped that this work will"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "bring more discussions in the field of emotion recognition, as the"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "new era of general large models is coming."
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "2. RELATED WORK"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "A substantial body of research has been dedicated to the domain of"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "model adaptation for pre-trained LLMs. Within the spectrum of con-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "temporary approaches, a widely-used and the most basic technique"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "is known as full fine-tuning (FFT), necessitating the retraining of"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "all model parameters. While FFT has proven effective in enhancing"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "LLM performance,\nit demands substantial computational resources"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "during training,\nincurring significant costs and rendering it\nincreas-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "ingly impractical.\nIn response to the need for reducing the compu-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "tational burden,\nthe research community has introduced numerous"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "parameter-efficient fine-tuning (PEFT) methods [3]. These innova-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "tive methodologies entail the selective training of a limited subset of"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "model parameters, either by modifying existing parameters or intro-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "ducing novel ones into the model architecture."
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "The\nstrategies\nfor\ntraining\npartial model\nparameters\ninvolve"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "adapting the characteristics of\nlayer\ntypes or\ninternal architecture"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": ""
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "within a network [3]. Methods\ninvolving the introduction of ad-"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "ditional parameters\ncan be broadly categorised into two groups:"
        },
        {
          "3 GLAM, Department of Computing, Imperial College London, UK": "Adapter-like methods\nand Soft Prompts methods.\nAdapters\nin-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: , ranging from English to",
      "data": [
        {
          "h": ""
        },
        {
          "h": "Back Propagation"
        },
        {
          "h": "Prompt Encoder"
        },
        {
          "h": "x"
        },
        {
          "h": ""
        },
        {
          "h": "Pretrained\nInput"
        },
        {
          "h": "Weights\nEmbedding"
        },
        {
          "h": ""
        },
        {
          "h": "Transformers"
        },
        {
          "h": ". . .\n. . ."
        },
        {
          "h": "x"
        },
        {
          "h": ""
        },
        {
          "h": "(a) P-Tuning v2\n(b) Low-Rank Adaptation"
        },
        {
          "h": ""
        },
        {
          "h": "Fig. 1: Schematic representation of two model adaptation methods:"
        },
        {
          "h": "P-Tuning v1 (a) and Low-Rank Adaption (b). Red blocks refer to the"
        },
        {
          "h": "additional\ntrainable parameters, while the yellow blocks represent"
        },
        {
          "h": "the frozen parameters of the pre-trained model."
        },
        {
          "h": ""
        },
        {
          "h": ""
        },
        {
          "h": "troduce\nsmall\nfully-connected\nnetworks\nafter Transformer\nsub-"
        },
        {
          "h": ""
        },
        {
          "h": "layers [4]. Soft prompts can be trained for the input layer exclusively"
        },
        {
          "h": ""
        },
        {
          "h": "or for all\nlayers within the model. Furthermore, reparametrisation-"
        },
        {
          "h": ""
        },
        {
          "h": "based PEFT methods leverage low-rank representations to minimise"
        },
        {
          "h": ""
        },
        {
          "h": "the number of trainable parameters."
        },
        {
          "h": ""
        },
        {
          "h": "In this paper, we choose two widely recognised model adapta-"
        },
        {
          "h": ""
        },
        {
          "h": "tion methods, namely P-Tuning v2 and LoRA. These two methods"
        },
        {
          "h": ""
        },
        {
          "h": "involve a limited number of\ntrainable parameters, which ensures"
        },
        {
          "h": ""
        },
        {
          "h": "their\ncomputational\nresource\ndemands\nremain within\nreasonable"
        },
        {
          "h": ""
        },
        {
          "h": "limits. Consequently, the adaptation training process can be feasibly"
        },
        {
          "h": ""
        },
        {
          "h": "conducted on consumer-grade graphics processing units\n(GPUs),"
        },
        {
          "h": ""
        },
        {
          "h": "rendering both P-Tuning v2 and LoRA highly practical choices."
        },
        {
          "h": ""
        },
        {
          "h": ""
        },
        {
          "h": "3. ADAPTATION OF LARGE LANGUAGE MODELS"
        },
        {
          "h": ""
        },
        {
          "h": "In this section, we present an introduction to the selected LLM –"
        },
        {
          "h": "Chat General Language Model, alongside an explanation of the core"
        },
        {
          "h": "principles underlying the two model adaptation techniques, namely"
        },
        {
          "h": "P-Tuning and LoRA."
        },
        {
          "h": ""
        },
        {
          "h": "3.1. General Language Model"
        },
        {
          "h": ""
        },
        {
          "h": ""
        },
        {
          "h": "General Language Model\n(GLM)\nis a general pre-training frame-"
        },
        {
          "h": ""
        },
        {
          "h": "work based on a novel autoregressive blank infilling objective and"
        },
        {
          "h": ""
        },
        {
          "h": "can be adapted to various NLU and natural\nlanguage generation"
        },
        {
          "h": ""
        },
        {
          "h": "tasks\n[5].\nGLM formulates NLU tasks as\n‘cloze’ questions\nthat"
        },
        {
          "h": ""
        },
        {
          "h": "contain task descriptions, which can be answered by autoregressive"
        },
        {
          "h": ""
        },
        {
          "h": "generation. Remarkably, when operating with equivalent parameter"
        },
        {
          "h": ""
        },
        {
          "h": "counts and computational resources, GLM consistently outperforms"
        },
        {
          "h": ""
        },
        {
          "h": "BERT on the SuperGLUE benchmark, and excels beyond RoBERTa"
        },
        {
          "h": ""
        },
        {
          "h": "and BART when pretrained on corpora of comparable sizes [5]. Fur-"
        },
        {
          "h": ""
        },
        {
          "h": "thermore, GLM demonstrates superior performance to T5 in NLU"
        },
        {
          "h": ""
        },
        {
          "h": "and generation tasks, while utilising fewer parameters and data [5]."
        },
        {
          "h": ""
        },
        {
          "h": "Chat General Language Model (ChatGLM), launched in March"
        },
        {
          "h": ""
        },
        {
          "h": "2023 by the Tsinghua University KEG Laboratory and the Zhipu"
        },
        {
          "h": "AI Company,\nis a GLM-based AI Chatbot. ChatGLM draws inspi-"
        },
        {
          "h": "ration from ChatGPT to integrate code pre-training into the trillion-"
        },
        {
          "h": "parameter base model GLM-130B [6], achieving human intent align-"
        },
        {
          "h": ""
        },
        {
          "h": "ment\nthrough techniques\nlike supervised fine-tuning.\nIt\nis worth"
        },
        {
          "h": "mentioning that ChatGLM was pre-trained on both Chinese and En-"
        },
        {
          "h": "glish corpora,\nthus,\nit possesses bilingual capabilities. ChatGLM2,"
        },
        {
          "h": "a second-generation model,\nis open-sourced in June 2023.\nIt boasts"
        },
        {
          "h": "enhanced performance, extended context capabilities,\nimproved in-"
        },
        {
          "h": "ference efficiency, and a more permissive open-source license. In our"
        },
        {
          "h": "study, we opted to utilise the more lightweight model ChatGLM2-"
        },
        {
          "h": "6B, which employs the same technology as ChatGLM2 but with a"
        },
        {
          "h": "reduced parameter count of 6.2 billion.\nIt\nrequires a minimum of"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": "speakers, dialogues, utterances of the whole dataset and its test subset, words per utterance, and emotional classes."
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": "dataset"
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": ""
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": "SST"
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": ""
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": ""
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": "Friends"
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": ""
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": ""
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": "Mastodon"
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": ""
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": "MOSI"
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": "CH-SIMS"
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": ""
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": ""
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": "M3ED"
        },
        {
          "Table 1: Detailed information of the selected six datasets. #sp., #dia., #total (test) utt., #words/utt., #classes denotes the number of distinct": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "shows": "M3ED\nMandarin\na, v, t\nyes\nTV series\n626",
          "0.6}{0.8, 1.0}": "990\n24 449 (4 201)\n7.4\n7 (happy, surp., sad,"
        },
        {
          "shows": "",
          "0.6}{0.8, 1.0}": "disgust, anger, fear, neut.)"
        },
        {
          "shows": "sentences sourced from movie review data [9]. For the fine-grained",
          "0.6}{0.8, 1.0}": "mance on each selected datasets, separately, under strictly compara-"
        },
        {
          "shows": "task, each sentence is categorised into one of five sentiment classes.",
          "0.6}{0.8, 1.0}": "ble conditions."
        },
        {
          "shows": "For the binary task, each sentence is simply classified as either posi-",
          "0.6}{0.8, 1.0}": "For\neach individual dataset, we designed three\nsets of\ncom-"
        },
        {
          "shows": "tive or negative, with the neutral category excluded.",
          "0.6}{0.8, 1.0}": "parative experiments: ChatGLM2 without adaptation, ChatGLM2"
        },
        {
          "shows": "Friends: Friends is an English corpus based on the TV show",
          "0.6}{0.8, 1.0}": "adapted with P-Tuning v2,\nand ChatGLM2 adapted with LoRA."
        },
        {
          "shows": "Friends, containing 1,000 dialogues from seasons one to nine [10].",
          "0.6}{0.8, 1.0}": "During the\ninference with ChatGLM2, we utilise\na\n“prompt”\nto"
        },
        {
          "shows": "The 14,503 utterances from the 1,000 dialogues are categorised into",
          "0.6}{0.8, 1.0}": "acquire a response from it.\nThe prompt\nshould encompass both"
        },
        {
          "shows": "seven classes. The annotators considered the context of the dialogue",
          "0.6}{0.8, 1.0}": "task-guiding sentences\nthat necessitate emotion recognition.\nOur"
        },
        {
          "shows": "when labelling sentiments.",
          "0.6}{0.8, 1.0}": "prompt\nis structured as follows: Classify the sentiment of the sen-"
        },
        {
          "shows": "Mastodon: The Mastodon dataset [11] consists of English posts",
          "0.6}{0.8, 1.0}": "tence to Emotion 1, Emotion 2,\n...\nor Emotion k: <provide only"
        },
        {
          "shows": "from the Mastodon social media platform. While the dataset was",
          "0.6}{0.8, 1.0}": "one sentence from a test set>. The value of k here is determined by"
        },
        {
          "shows": "initially designed for both sentiment\nrecognition and dialogue act",
          "0.6}{0.8, 1.0}": "the number of sentiment/emotion categories specific to the dataset."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Performance comparison between adapted models and Table 4: Performance comparison between adapted models and",
      "data": [
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "SOTA works on the MOSI datasets measured by accuracy (Acc)",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": "SOTA works on the CH-SIMS and Mastodon datasets measured"
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "and macro-F1 (F1).",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": "by accuracy (Acc) and macro-F1 (F1)."
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "MOSI-2\nMOSI-3",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": "CH-SIMS\nMastodon"
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "Model [%]\nAcc\nF1\nAcc\nF1",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": "Model [%]\nAcc\nF1\nAcc\nF1"
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "TFR-Net (2021) [15]\n83.49\n-\n-\n-",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": "MLF-DNN (2020) [13]\n80.26\n-\n-\n-"
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "CHFN (2022) [16]\n85.20\n-\n-\n-",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": "DARER (2022) [23]\n-\n-\n-\n59.59"
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "SeqSeq2Sent (2018) [17]\n-\n-\n77.00\n-",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": ""
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": "ChatGLM2\n77.58\n75.95\n55.43\n55.45"
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "CTFN (2021) [18]\n-\n-\n80.79\n-",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": ""
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": "67.25\n67.23\nChatGLM2 (P-Tuning)\n82.47\n81.12"
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "ChatGLM2\n84.12\n84.12\n77.26\n58.19",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": "82.73\n81.25\nChatGLM2 (LoRA)\n67.08\n66.81"
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "61.03\nChatGLM2 (P-Tuning)\n84.60\n84.04\n81.78",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": ""
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "87.02\n86.56\n83.82\nChatGLM2 (LoRA)\n57.04",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": ""
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": "Table 5: Performance comparison on Friends (first half) and M3ED"
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": "(second half) in terms of accuracy (Acc), F1, and unweighted accu-"
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "Table 3:\nPerformance\ncomparison between adapted models\nand",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": "racy (UA). Note that, F1 indicates macro-F1 and weighted average"
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "SOTA works on the SST datasets measured by accuracy (Acc) and",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": "F1 for Friends and M3ED, respectively, for a fair performance com-"
        },
        {
          "Table 2:\nPerformance\ncomparison between adapted models\nand": "macro-F1 (F1).",
          "Table 4:\nPerformance\ncomparison between adapted models\nand": "parison."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Performance comparison between adapted models and Table 4: Performance comparison between adapted models and",
      "data": [
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "racy (UA). Note that, F1 indicates macro-F1 and weighted average"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "F1 for Friends and M3ED, respectively, for a fair performance com-"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "parison."
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "Friends"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "Model [%]\nAcc\nF1\nUA"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "BERT+SRL-GNN-8 (2020) [24]\n72.10\n-\n53.71"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "XLNet+SRL-GNN-8 (2020) [24]\n72.82\n-\n53.41"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "81.30\n65.90\nPRE-CODE (2020) [25]\n-"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": ""
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "ChatGLM2\n63.79\n29.48\n26.03"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "55.06\nChatGPT (P-Tuning)\n54.92\n51.92"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "ChatGPT (LoRA)\n72.83\n52.97\n51.93"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": ""
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "M3ED"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "Model [%]\nAcc\nF1\nUA"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "DialogueGCN (2019) [26]\n-\n46.09\n-"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "DialogueRNN (2019) [27]\n-\n48.80\n-"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "49.42\nMDI (2022) [14]\n-\n-"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": ""
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "ChatGLM2\n45.68\n30.52\n16.82"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": ""
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "45.75\n28.64\nChatGLM2 (P-Tuning)\n37.31"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": ""
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": "ChatGLM2 (LoRA)\n42.54\n33.31\n23.59"
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": ""
        },
        {
          "(second half) in terms of accuracy (Acc), F1, and unweighted accu-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "ture reconstruction network for\nrobust multimodal sentiment"
        },
        {
          "6. REFERENCES": "J. Han, Z. Zhang, and B. W. Schuller, “Adversarial\ntraining in",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "the 29th ACM International Conference on\nanalysis,” in Proc."
        },
        {
          "6. REFERENCES": "affective computing and sentiment analysis: Recent advances",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "Multimedia (MM), 2021, pp. 4400–4407."
        },
        {
          "6. REFERENCES": "IEEE Computational\nIntelligence Maga-\nand perspectives,”",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "[16]\nJ. Guo, J. Tang, W. Dai, Y. Ding, and W. Kong, “Dynamically"
        },
        {
          "6. REFERENCES": "zine, vol. 14, no. 2, pp. 68–81, Sep. 2019.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "adjust word representations using unaligned multimodal infor-"
        },
        {
          "6. REFERENCES": "J. Han, H. Zhao,\nand B. W.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "the 30th ACM International Conference on\nmation,” in Proc."
        },
        {
          "6. REFERENCES": "Schuller, “Refashioning emotion recognition modelling: The",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "Multimedia (MM), 2022, pp. 3394–3402."
        },
        {
          "6. REFERENCES": "arXiv preprint arXiv:\nadvent of generalised large models,”",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "[17] H.\nPham,\nT. Manzini,\nP.\nP.\nLiang,\nand\nB.\nPocz´os,"
        },
        {
          "6. REFERENCES": "2308.11578, Aug. 2023.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "“Seq2Seq2Sentiment: Multimodal sequence to sequence mod-"
        },
        {
          "6. REFERENCES": "[3] V. Lialin, V. Deshpande, and A. Rumshisky, “Scaling down",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "els\nfor\nsentiment\nanalysis,”\nin Proc. Grand Challenge and"
        },
        {
          "6. REFERENCES": "to scale up: A guide to parameter-efficient fine-tuning,” arXiv",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "Workshop on Human Multimodal Language (Challenge-HML),"
        },
        {
          "6. REFERENCES": "preprint arXiv: 2303.15647, Apr. 2023.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "2018, pp. 53–63."
        },
        {
          "6. REFERENCES": "J. Pfeiffer, A. R¨uckl´e, C. Poth,\nand\net\nal.,\n“Adapterhub:",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "[18]\nJ. Tang, K. Li, X. Jin, A. Cichocki, Q. Zhao, and W. Kong,"
        },
        {
          "6. REFERENCES": "A framework\nfor\nadapting\ntransformers,”\nin Proc. Confer-",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "“CTFN: hierarchical learning for multimodal sentiment analy-"
        },
        {
          "6. REFERENCES": "ence on Empirical Methods in Natural Language Processing",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "sis using coupled-translation fusion network,” in Proc. the 59th"
        },
        {
          "6. REFERENCES": "(EMNLP), 2020, pp. 46–54.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "Annual Meeting of the Association for Computational Linguis-"
        },
        {
          "6. REFERENCES": "[5] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang,",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "tics (ACL), 2021, pp. 5301–5311."
        },
        {
          "6. REFERENCES": "“GLM: general\nlanguage model pretraining with autoregres-",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "[19]\nJ. Lee,\nJ. Kim, and P. Kang, “Back-translated task adaptive"
        },
        {
          "6. REFERENCES": "the 60th Annual Meeting of\nthe\nsive blank infilling,” in Proc.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "pretraining: Improving accuracy and robustness on text classi-"
        },
        {
          "6. REFERENCES": "Association for Computational Linguistics\n(ACL), 2022, pp.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "fication,” arXiv preprint arXiv:2107.10474, Aug. 2021."
        },
        {
          "6. REFERENCES": "320–335.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "[20] B. Wang, B. Liang,\nJ. Du, M. Yang,\nand R. Xu,\n“SEM-"
        },
        {
          "6. REFERENCES": "[6] A. Zeng, X. Liu, Z. Du, and etc., “GLM-130B: an open bilin-",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "Graph: Incorporating sentiment knowledge and eye movement"
        },
        {
          "6. REFERENCES": "gual pre-trained model,” in Proc. International Conference on",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "into graph model\nfor\nsentiment\nanalysis,”\nin Proc. Confer-"
        },
        {
          "6. REFERENCES": "Learning Representations (ICLR), 2023.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "ence on Empirical Methods in Natural Language Processing"
        },
        {
          "6. REFERENCES": "[7] X. Liu, K. Ji, Y. Fu, Z. Du, Z. Yang, and J. Tang, “P-tuning",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "(EMNLP), 2022, pp. 7521–7531."
        },
        {
          "6. REFERENCES": "v2: Prompt tuning can be comparable to fine-tuning universally",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "across\nscales and tasks,” arXiv preprint arXiv:\n2110.07602,",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "[21]\nP. Ke, H.\nJi, S. Liu, X. Zhu,\nand M. Huang,\n“SentiLARE:"
        },
        {
          "6. REFERENCES": "Sep. 2021.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "Sentiment-aware\nlanguage\nrepresentation learning with\nlin-"
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "guistic knowledge,” in Proc. Conference on Empirical Methods"
        },
        {
          "6. REFERENCES": "[8] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang,",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "in Natural Language Processing (EMNLP), 2020, pp. 6975–"
        },
        {
          "6. REFERENCES": "L. Wang, and W. Chen, “LoRA: Low-rank adaptation of large",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "6988."
        },
        {
          "6. REFERENCES": "language models,” in Proc. The Tenth International Conference",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "on Learning Representations (ICLR), 2022.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "[22]\nS. Fan, C. Lin, H. Li, Z. Lin, J. Su, H. Zhang, Y. Gong, J. Guo,"
        },
        {
          "6. REFERENCES": "J. Wu,\nJ. Chuang, C. D. Manning,",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "and N. Duan, “Sentiment-aware word and sentence level pre-"
        },
        {
          "6. REFERENCES": "A. Y. Ng, and C. Potts, “Recursive deep models for semantic",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "training for sentiment analysis,” in Proc. Conference on Em-"
        },
        {
          "6. REFERENCES": "compositionality over a sentiment\ntreebank,” in Proc. Confer-",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "pirical Methods\nin Natural Language Processing (EMNLP),"
        },
        {
          "6. REFERENCES": "ence on Empirical Methods in Natural Language Processing",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "2022, pp. 4984–4994."
        },
        {
          "6. REFERENCES": "(EMNLP), 2013, pp. 1631–1642.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "[23] B. Xing and I. W. Tsang,\n“DARER: dual-task temporal\nre-"
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "lational\nrecurrent\nreasoning network for\njoint dialog senti-"
        },
        {
          "6. REFERENCES": "“EmotionLines: An emotion corpus of multi-party conversa-",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "the 60th An-\nment classification and act recognition,” in Proc."
        },
        {
          "6. REFERENCES": "tions,” in Proc. the 11th International Conference on Language",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "nual Meeting of the Association for Computational Linguistics"
        },
        {
          "6. REFERENCES": "Resources and Evaluation (LREC), 2018, pp. 1597–1601.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "(ACL), 2022, pp. 3611–3621."
        },
        {
          "6. REFERENCES": "Cerisara,\nS.\nJafaritazehjani,\nA. Oluokun,\nand H.\nT.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "[24] C. T. Heaton and D. M. Schwartz, “Language models as emo-"
        },
        {
          "6. REFERENCES": "Le,\n“Multi-task\ndialog\nact\nand\nsentiment\nrecognition\non",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "the 28th\ntional classifiers\nfor\ntextual conversation,” in Proc."
        },
        {
          "6. REFERENCES": "the\n27th\nInternational Conference\non\nmastodon,”\nin Proc.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "ACM International Conference on Multimedia (MM), 2020,"
        },
        {
          "6. REFERENCES": "Computational Linguistics (COLING), 2018, pp. 745–754.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "pp. 2918–2926."
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "[25] W. Jiao, M. R. Lyu, and I. King, “Exploiting unsupervised data"
        },
        {
          "6. REFERENCES": "modal sentiment\nintensity analysis in videos: Facial gestures",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "for emotion recognition in conversations,” in Proc. the 58th An-"
        },
        {
          "6. REFERENCES": "and verbal messages,” IEEE Intelligent Systems, vol. 31, no. 6,",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "nual Meeting of the Association for Computational Linguistics"
        },
        {
          "6. REFERENCES": "pp. 82–88, Nov. 2016.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "(ACL), 2020, pp. 4839–4846."
        },
        {
          "6. REFERENCES": "J. Wu,\nJ. Zou, and",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "[26] D. Ghosal, N. Majumder, S. Poria, N. Chhaya, and A. F. Gel-"
        },
        {
          "6. REFERENCES": "K. Yang, “CH-SIMS: A Chinese multimodal sentiment analy-",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "bukh,\n“DialogueGCN: A Graph Convolutional Neural Net-"
        },
        {
          "6. REFERENCES": "sis dataset with fine-grained annotation of modality,” in Proc.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "work for emotion recognition in conversation,” in Proc. Con-"
        },
        {
          "6. REFERENCES": "the 58th Annual Meeting of the Association for Computational",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "ference on Empirical Methods in Natural Language Processing"
        },
        {
          "6. REFERENCES": "Linguistics (ACL), 2020, pp. 3718–3727.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "(EMNLP), 2019, pp. 154–164."
        },
        {
          "6. REFERENCES": "J. Zhao, T. Zhang, J. Hu, Y. Liu, Q. Jin, X. Wang, and H. Li,",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "[27] N. Majumder, S. Poria, D. Hazarika, R. Mihalcea, A. F. Gel-"
        },
        {
          "6. REFERENCES": "“M3ED: multi-modal multi-scene multi-label emotional dia-",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "bukh, and E. Cambria, “DialogueRNN: An attentive RNN for"
        },
        {
          "6. REFERENCES": "logue database,” in Proc. the 60th Annual Meeting of the Asso-",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "emotion detection in conversations,” in Proc. AAAI Conference"
        },
        {
          "6. REFERENCES": "ciation for Computational Linguistics (ACL), 2022, pp. 5699–",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": "on Artificial Intelligence, 2019, pp. 6818–6825."
        },
        {
          "6. REFERENCES": "5710.",
          "[15] Z. Yuan, W. Li, H. Xu, and W. Yu, “Transformer-based fea-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Adversarial training in affective computing and sentiment analysis: Recent advances and perspectives",
      "authors": [
        "J Han",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "3",
      "title": "Refashioning emotion recognition modelling: The advent of generalised large models",
      "authors": [
        "Z Zhang",
        "L Peng",
        "T Pang",
        "J Han",
        "H Zhao",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Refashioning emotion recognition modelling: The advent of generalised large models",
      "arxiv": "arXiv:2308.11578"
    },
    {
      "citation_id": "4",
      "title": "Scaling down to scale up: A guide to parameter-efficient fine-tuning",
      "authors": [
        "V Lialin",
        "V Deshpande",
        "A Rumshisky"
      ],
      "year": "2023",
      "venue": "Scaling down to scale up: A guide to parameter-efficient fine-tuning",
      "arxiv": "arXiv:2303.15647"
    },
    {
      "citation_id": "5",
      "title": "Adapterhub: A framework for adapting transformers",
      "authors": [
        "J Pfeiffer",
        "A Rücklé",
        "C Poth"
      ],
      "year": "2020",
      "venue": "Proc. Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "6",
      "title": "GLM: general language model pretraining with autoregressive blank infilling",
      "authors": [
        "Z Du",
        "Y Qian",
        "X Liu",
        "M Ding",
        "J Qiu",
        "Z Yang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "Proc. the 60th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "7",
      "title": "GLM-130B: an open bilingual pre-trained model",
      "authors": [
        "A Zeng",
        "X Liu",
        "Z Du"
      ],
      "venue": "Proc. International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "8",
      "title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
      "authors": [
        "X Liu",
        "K Ji",
        "Y Fu",
        "Z Du",
        "Z Yang",
        "J Tang"
      ],
      "year": "2021",
      "venue": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
      "arxiv": "arXiv:2110.07602"
    },
    {
      "citation_id": "9",
      "title": "LoRA: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "venue": "Proc. The Tenth International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "10",
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
      "authors": [
        "R Socher",
        "A Perelygin",
        "J Wu",
        "J Chuang",
        "C Manning",
        "A Ng",
        "C Potts"
      ],
      "year": "2013",
      "venue": "Proc. Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "EmotionLines: An emotion corpus of multi-party conversations",
      "authors": [
        "C.-C Hsu",
        "S.-Y Chen",
        "C.-C Kuo",
        "T.-H Huang",
        "L.-W Ku"
      ],
      "year": "2018",
      "venue": "Proc. the 11th International Conference on Language Resources and Evaluation (LREC)"
    },
    {
      "citation_id": "12",
      "title": "Multi-task dialog act and sentiment recognition on mastodon",
      "authors": [
        "C Cerisara",
        "S Jafaritazehjani",
        "A Oluokun",
        "H Le"
      ],
      "year": "2018",
      "venue": "Proc. the 27th International Conference on Computational Linguistics (COLING)"
    },
    {
      "citation_id": "13",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "14",
      "title": "CH-SIMS: A Chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "W Yu",
        "H Xu",
        "F Meng",
        "Y Zhu",
        "Y Ma",
        "J Wu",
        "J Zou",
        "K Yang"
      ],
      "year": "2020",
      "venue": "Proc. the 58th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "15",
      "title": "M3ED: multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "J Zhao",
        "T Zhang",
        "J Hu",
        "Y Liu",
        "Q Jin",
        "X Wang",
        "H Li"
      ],
      "year": "2022",
      "venue": "Proc. the 60th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "16",
      "title": "Transformer-based feature reconstruction network for robust multimodal sentiment analysis",
      "authors": [
        "Z Yuan",
        "W Li",
        "H Xu",
        "W Yu"
      ],
      "year": "2021",
      "venue": "Proc. the 29th ACM International Conference on Multimedia (MM)"
    },
    {
      "citation_id": "17",
      "title": "Dynamically adjust word representations using unaligned multimodal information",
      "authors": [
        "J Guo",
        "J Tang",
        "W Dai",
        "Y Ding",
        "W Kong"
      ],
      "year": "2022",
      "venue": "Proc. the 30th ACM International Conference on Multimedia (MM)"
    },
    {
      "citation_id": "18",
      "title": "Seq2Seq2Sentiment: Multimodal sequence to sequence models for sentiment analysis",
      "authors": [
        "H Pham",
        "T Manzini",
        "P Liang",
        "B Poczós"
      ],
      "year": "2018",
      "venue": "Proc. Grand Challenge and Workshop on Human Multimodal Language"
    },
    {
      "citation_id": "19",
      "title": "CTFN: hierarchical learning for multimodal sentiment analysis using coupled-translation fusion network",
      "authors": [
        "J Tang",
        "K Li",
        "X Jin",
        "A Cichocki",
        "Q Zhao",
        "W Kong"
      ],
      "year": "2021",
      "venue": "Proc. the 59th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "20",
      "title": "Back-translated task adaptive pretraining: Improving accuracy and robustness on text classification",
      "authors": [
        "J Lee",
        "J Kim",
        "P Kang"
      ],
      "year": "2021",
      "venue": "Back-translated task adaptive pretraining: Improving accuracy and robustness on text classification",
      "arxiv": "arXiv:2107.10474"
    },
    {
      "citation_id": "21",
      "title": "SEM-Graph: Incorporating sentiment knowledge and eye movement into graph model for sentiment analysis",
      "authors": [
        "B Wang",
        "B Liang",
        "J Du",
        "M Yang",
        "R Xu"
      ],
      "year": "2022",
      "venue": "Proc. Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "22",
      "title": "SentiLARE: Sentiment-aware language representation learning with linguistic knowledge",
      "authors": [
        "P Ke",
        "H Ji",
        "S Liu",
        "X Zhu",
        "M Huang"
      ],
      "year": "2020",
      "venue": "Proc. Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "23",
      "title": "Sentiment-aware word and sentence level pretraining for sentiment analysis",
      "authors": [
        "S Fan",
        "C Lin",
        "H Li",
        "Z Lin",
        "J Su",
        "H Zhang",
        "Y Gong",
        "J Guo",
        "N Duan"
      ],
      "year": "2022",
      "venue": "Proc. Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "24",
      "title": "DARER: dual-task temporal relational recurrent reasoning network for joint dialog sentiment classification and act recognition",
      "authors": [
        "B Xing",
        "I Tsang"
      ],
      "year": "2022",
      "venue": "Proc. the 60th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "25",
      "title": "Language models as emotional classifiers for textual conversation",
      "authors": [
        "C Heaton",
        "D Schwartz"
      ],
      "year": "2020",
      "venue": "Proc. the 28th ACM International Conference on Multimedia (MM)"
    },
    {
      "citation_id": "26",
      "title": "Exploiting unsupervised data for emotion recognition in conversations",
      "authors": [
        "W Jiao",
        "M Lyu",
        "I King"
      ],
      "year": "2020",
      "venue": "Proc. the 58th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "27",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya"
      ],
      "year": "2019",
      "venue": "Proc. Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "28",
      "title": "DialogueRNN: An attentive RNN for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proc. AAAI Conference on Artificial Intelligence"
    }
  ]
}