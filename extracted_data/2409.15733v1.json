{
  "paper_id": "2409.15733v1",
  "title": "Evofa: Evolvable Fast Adaptation For Eeg Emotion Recognition",
  "published": "2024-09-24T04:35:10Z",
  "authors": [
    "Ming Jin",
    "Danni Zhang",
    "Gangming Zhao",
    "Changde Du",
    "Jinpeng Li"
  ],
  "keywords": [
    "Domain adaptation",
    "Electroencephalography",
    "Emotion recognition",
    "Meta learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Electroencephalography (EEG)-based emotion recognition has gained significant traction due to its accuracy and objectivity. However, the non-stationary nature of EEG signals leads to distribution drift over time, causing severe performance degradation when the model is reused. While numerous domain adaptation (DA) approaches have been proposed in recent years to address this issue, their reliance on large amounts of target data for calibration restricts them to offline scenarios, rendering them unsuitable for real-time applications. To address this challenge, this paper proposes Evolvable Fast Adaptation (EvoFA), an online adaptive framework tailored for EEG data. EvoFA organically integrates the rapid adaptation of Few-Shot Learning (FSL) and the distribution matching of Domain Adaptation (DA) through a two-stage generalization process. During the training phase, a robust base metalearning model is constructed for strong generalization. In the testing phase, a designed evolvable meta-adaptation module iteratively aligns the marginal distribution of target (testing) data with the evolving source (training) data within a model-agnostic meta-learning framework, enabling the model to learn the evolving trends of testing data relative to training data and improving online testing performance. Experimental results demonstrate that EvoFA achieves significant improvements compared to the basic FSL method and previous online methods. The introduction of EvoFA paves the way for broader adoption of EEG-based emotion recognition in real-world applications. Our code will be released upon publication.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model",
      "text": "emotions  [3] ,  [4] . However, despite significant advancements in improving the efficiency and accuracy of recognition, the decline in model performance when reused, especially in online settings, remains a prominent issue limiting the practical application of EEG-based emotion recognition  [5] .\n\nA major obstacle to the reusability of EEG-based emotion recognition is the problem of \"domain drift.\" This issue manifests as changes in the statistical properties of EEG signals over time, significantly impacting the accuracy and reliability of emotion recognition models. Factors contributing to domain drift include changes in the physiological state of subjects (such as fatigue or hunger), environmental interference (such as noise or light), and errors in device usage  [6] ,  [7] .\n\nDomain drift poses a significant challenge to model performance . Existing methods to address this issue can be broadly categorized into two approaches.\n\n• Supervised methods: As depicted in Figure  1 (a), this approach relies on a large amount of labeled data for direct model training or fine-tuning  [8] ,  [9] . While it grants generalization capability initially, the model's performance progressively degrades as domain drift in new data becomes persistent. Maintaining high performance necessitates frequent retraining with substantial amounts of newly labeled data, hindering real-world practicality.\n\n• Offline adaptation methods: Illustrated in Figure  1 (b), this approach utilizes transfer learning to bridge the distributional gap between source and target domains  [3] ,  [10] . By leveraging a pre-trained model and fine-tuning it with target data, the models gain discriminative ability on new data. However, existing transfer learning methods still require a significant amount of unlabeled target data for distribution matching, which can be a limitation in resource-constrained scenarios.\n\nBoth approaches share a common dependency on substantial calibration data for model adjustments. This reliance on large datasets hinders their applicability in real-world situations where data acquisition might be expensive or limited. Humans exhibit remarkable ability to recognize new categories with only a few examples. Inspired by this capability, FSL has emerged as a promising approach to significantly reduce the calibration data required for models on new data. Applying FSL to EEG-based emotion recognition holds the potential to address the challenge of time-consuming calibration  [11] -  [13] . However, the FSL framework primarily focuses on rapid adaptation to entirely new categories, where the training and test sets have non-overlapping in label space. Applying the basic FSL model rigidly to EEG emotion recognition overlooks two critical issues: for EEG data, there may be only limited modality shift between source and target data, and the label space for training and test data are completely identical.\n\nConsidering that DA is mainly designed to address data drift from the training set to the test set within the same label space, and FSL enables rapid calibration in an online learning mode, an ideal approach would be to organically combine the online rapid recognition capability of FSL with the distribution matching ability of DA to address data drift.\n\nTo address the issue of data drift faced by models during online use, this paper proposes EvoFA, a novel online adaptation framework for EEG-based emotion recognition. As illustrated in Figure  1  (c), EvoFA requires only a small amount of calibration data to quickly calibrate the model for effective testing. EvoFA combines the advantages of FSL rapid generalization and DA distribution matching through a twostep generalization process. During the training phase, FSL is utilized to initialize a general emotion recognition model with strong generalization capabilities. In the meta-testing phase, domain shifts induced by data drift are simulated, and an evolvable meta-adaptation module is introduced to align the target domain with the gradually changing source domain, thereby mitigating the impact of data drift on model performance. Experimental results demonstrate that EvoFA achieves significant improvements compared to the basic FSL method and previous online methods.\n\nOur work makes the following significant contributions:\n\n• We propose EvoFA, a novel framework tailored for online adaptation of EEG data, which significantly reduces calibration data and lowers the barrier for deploying EEGbased emotion recognition in real-world scenarios. • Within this framework, we introduce a flexible and lightweight rapid test-time transfer method that mitigates the impact of domain drift caused by continuous changes in EEG data modalities. This method requires no retraining and is compatible with various FSL models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "• Experimental Results On Two Datasets Demonstrate That",
      "text": "EvoFA not only achieves superior performance in online recognition tasks but also significantly reduces calibration data. The subsequent sections are organized as follows: Section II reviews the progress in EEG-based emotion recognition, DA and meta-learning. Section III primarily introduces our proposed EvoFA framework, which cleverly combines the advantages of FSL rapid generalization and DA distribution matching, making it suitable for EEG emotion recognition tasks. Section IV demonstrates the improvements achieved by EvoFA on intra-subject and inter-subject tasks. Section V summarizes this work and provides future perspectives.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Eeg Emotion Recognition",
      "text": "EEG-based emotion recognition can be broadly categorized into three types based on the level of access to test/target data.\n\nSupervised Learning: In this approach, test data is entirely unknown. The primary reliance is on supervised learning, where models are trained on training data to develop strong generalization capabilities that can fit well on test data. Since deeper networks typically have stronger fitting abilities, constructing deep models to learn robust representations is a mainstream method  [9] ,  [14] . In recent years, Graph Convolutional Networks (GCNs) have also been proven effective for handling EEG data due to their ability to capture critical information from brain topologies, allowing better discrimination of emotional patterns at both the feature and brain topology levels  [8] ,  [15] -  [17] . However, the performance of these trained models often significantly declines when applied to completely unknown test data.\n\nOffline Learning: This approach allows access to part or all of the unlabeled test data. To address the inevitable issue of modal drift, many studies have turned to offline learning modes based on transfer learning  [3] ,  [4] ,  [10] . She et al.  [4]  proposed a new emotion recognition method based on a multisource associate domain adaptation (DA) network, considering both domain invariant and domain-specific features. SSDA was proposed to align the joint distributions of subjects, assuming that fine-grained structures must be aligned to perform a greater knowledge transfer  [3] . However, since offline learning requires access to a large amount of test data, it often loses its applicability in most real-world scenarios.\n\nOnline Learning: In this scenario, only a minimal amount of test set calibration data is accessible. To better meet practical application needs, some studies have attempted to achieve rapid calibration and online recognition of EEG data. Pan et al.  [18]  proposed a novel Online Multimodal Hypergraph Learning (OMHGL) method, which integrates multimodal information based on time-series physiological signals for emotion recognition. Blanco et al.  [19]  developed a machine learning model using principal component analysis, power spectral density, random forest, and Extra-Trees that can recognize emotions in real time, providing estimates of valence, arousal, and dominance (VAD) every five seconds. However, to maintain compatibility with real-time requirements, these methods largely rely on traditional machine learning techniques for training.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Unsupervised Domain Adaptation",
      "text": "Unsupervised domain adaptation (UDA) has emerged as a solution to address the insufficient generalization of models across different data distributions. UDA primarily tackles the challenges of costly data collection and annotation by pretraining on existing source data and fine-tuning on unlabeled target data to enhance model performance in the target domain.\n\nUDA methods can be categorized into three main approaches. One approach effectively aligns the distributions of source and target domain data by minimizing the discrepancy in the embedding space, thereby improving model performance on target data, where the domain discrepancy is measured by Maximum Mean Discrepancy (MMD)  [20]  and Joint MMD  [21] . Another common UDA method introduces domain discriminators to train the model to make it difficult for the discriminators to distinguish between source and target domain features, achieving feature alignment  [22] ,  [23] . Self-trainingbased methods generate pseudo-labels for target domain data and utilize these labels to supervise further model training iteratively, progressively enhancing model performance on target data  [24] ,  [25] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Meta Learning",
      "text": "Meta-learning, also known as 'learning to learn', stands in contrast to traditional artificial intelligence methods that solve tasks from scratch using fixed learning algorithms. Instead, meta-learning aims to improve the learning algorithm itself based on the experience gained from multiple learning episodes. This approach has gained significant interest in recent years, particularly for tasks involving FSL, which aligns well with the challenges of limited calibration data in EEGbased emotion recognition. The two primary approaches are metric-based and optimization-based approaches.\n\nMetric-based methods focus on learning a meaningful distance or similarity measure, allowing the model to embed data into a space where similar patterns are close together. Common distance metrics include cosine similarity  [26] , Euclidean distance  [27] , and CNN-based relational modules  [28] . Relation networks  [28]  and Prototypical Networks  [27]  are examples of such approaches in FSL, enabling models to quickly adapt to new tasks with only a few samples.\n\nOptimization-based meta-learning focuses on 'learning to fine-tune' through good parameter initialization, enabling rapid adaptation to new tasks with minimal gradient updates. MAML prepares a model for fast adaptation to new tasks through a two-step training process, maintaining compatibility with any model that learns via gradient descent  [29] . Reptile simplifies MAML by steering the initialization towards weights that perform well across multiple tasks, effectively rendering it a simpler variant of MAML  [30] . The optimization framework of MAML and its variants provides ideas for the rapid adaptation of EvoFA in the second stage  [29] -  [31] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methods",
      "text": "EvoFA achieves rapid calibration of EEG through twostep generalization, mitigating the impact of data modality drift. The main architecture is detailed in Figure  2 . FSL-based primary generalization: We employ G2G paired with 2D CNN as the backbone network to extract deep representations from EEG signals and utilize FSL to enhance the model's generalization ability during online calibration. Rapid adaptation secondary generalization: In the meta-testing phase, we cache the data patterns of the source data as they change over time and construct meta-adaptation of the target data relative to the source domain based on time changes, enabling the model to adapt to in new data. The backbone network parameters are denoted by θ, representing the function g θ . Extracted features are then processed through a novel adaptation optimization layer h ϕ parameterized by ϕ, followed by a classification layer c W to yield the corresponding emotional output. In the following description, to avoid misunderstandings, we will selectively use the terms source domain data / target domain data and training data / testing data in different contexts.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Backbone Network",
      "text": "G2G offers a novel approach to processing EEG data by capturing the dense information interactions between electrode nodes  [9] . This allows the processed EEG data to maintain a larger shape and become more suitable for deep learning models, particularly for tasks like emotion recognition where the relationships between electrodes are crucial. Inspired by this concept, we process the EEG feature, denoted as F ∈ R n×d , into a 2D feature interaction matrix F ′ ∈ R c×n×n using the G2G transformation:\n\nwhere n represents the number of electrodes, d denotes the number of features per electrode, and c indicate channels.\n\nAfter processing EEG into 2D interaction matrices, we further employ a ConvNet to learn deep representations from these matrices. The ConvNet includes four distinct Conv-BN-ReLU structures. Both the G2G and the subsequent ConvNet are jointly denoted by the representation function g θ . Next, the data is input into the meta adaptation optimization layer denoted by h ϕ . This layer comprises two fully-connected layers and aims to capture the evolving emotional representations during meta-testing in section III-D, enabling the model to adapt to domain shift. Ultimately, the classification layer c W outputs the emotion recognition results. It's worth noting that in the context of RelationNet  [28] , the classification layer serves a similar function as the relation module.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Fsl-Based Preliminary Generalization",
      "text": "Traditional deep learning trains models on large datasets, mapping different categories of data to their corresponding labels. FSL, on the other hand, learns distinguishing features from a small amount of data, enabling the model to differentiate between different categories.  In the outer loop, the model selected an appropriate direction for updating the adapter's parameters. By narrowing the gap with the evolving source, the impact of domain drift on test results can be mitigated.\n\nMost EEG emotion recognition methods rely on offline learning with large datasets, rendering them impractical for online use. In contrast, FSL quickly achieves classification ability with a few samples, making it an inherently suitable framework for online emotion recognition.\n\nFSL empowers the model to recognize target data categories in query set Q using few-shot support set S calibration samples through episode training. During the training phase, we randomly sample N classes from the training data. For each class, we randomly select K + Q labeled samples, where K are used for the support set and the remaining Q for the query set (forming an N -way K-shot setting). The corresponding support and query sets are:\n\nThe model is updated by calculating its loss on the query set. In this paper, to demonstrate EvoFA's broad adaptability, we trained three models based on three commonly used FSL frameworks: MatchingNet (MN)  [26] , RelationNet (RN)  [28] , and ProtoNet (PN)  [27] . The corresponding loss functions are:\n\nC. From Conventional FSL to EvoFA\n\nIn the basic FSL framework, during the testing phase, the category of unlabeled samples in the query set is determined based on a small labeled support set. The label spaces of the training and testing sets are mutually exclusive, and each episode's support set and query set correspond one-to-one in terms of categories.\n\nUnlike typical FSL tasks, in EEG emotion recognition, due to the limited emotion categories, the labels of the training and test sets are usually identical. Additionally, differences in the distribution of test set data relative to training set data generally stem from modal drift in the subjects' EEG data.\n\nBy reducing the modal drift of test data relative to training data, it is expected to improve the model's performance in few-shot testing scenarios. A common approach to reducing modal drift is DA. However, traditional domain adaptation typically requires access to all or a significant portion of the test set data and introduces additional training processes during the training phase. Implementing such domain adaptation contradicts existing online calibration patterns.\n\nEvoFA innovatively introduces test-time adaptation to address the performance degradation caused by modal drift. Before each episode's testing, DA is pre-introduced to adapt the test data to the evolving source data described in Section III-D, enhancing the model's adaptability to the test data. EvoFA's introduction offers the following advantages: update g θ , h ϕ and c W with source data S:\n\n3: end for Step 2: Rapid Adaptation Secondary Generalization Sample evolving S ′ = {X s1 , X s2 , . . . , X sn } 3:\n\nfor i=0 to n do 5:\n\nCompute adapted parameters with gradient descent: 6:\n\nend for 8: updata h ϕ with: 9:",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Rapid Adaptation Secondary Generalization",
      "text": "Unlike traditional FSL-based methods, which directly measure the similarity between the query set and a small labeled support set to determine the category of samples during testing, EvoFA introduces an additional evolvable fast adaptation module before classification. This module aims to capture potential patterns of modal drift in the source data and apply them for rapid calibration of the target data.\n\nHowever, while there indeed exists a distribution drift of test data relative to training data caused by modal drift, this change is unknown. To better capture the trend of data changes, we sampled a series of subsets of the source data S ′ = {X s1 , X s2 , . . . , X sn } on the source domain S, and T ′ = {X t n+k } from the target domain T . Each source domain subset represents a snapshot of modal drift to target domain.\n\nMAML learns transferable features by minimizing errors on the support set in the inner loop and errors on the query set in the outer loop. Inspired by this, by gradually narrowing the distance between target data and different subsets of source data in the inner loop, we aim to capture potential patterns of modal drift from source to target. Subsequently, updating the optimizer's parameters in the outer loop is expected to mitigate the damage of modal drift and improve the model's accuracy on the test set.\n\nWe constructed different learning patterns for intra and inter-subject tasks, with a more detailed description of both tasks in Section IV-A. For intra-subject experiments, the modal drift of test data relative to training data gradually intensifies over time. To reduce the impact of modal drift caused by temporal changes, we uniformly sample the training set data over time to form subsets of the source data. For inter-subject experiments, the modal drift of test data relative to training data mainly stems from modal differences between different subjects. To mitigate modal drift caused by different subjects, we sample a subset of data for each subject in the training set.\n\nFast Adapt to Snapshot In the inner loop, we quantify the drift of each source domain snapshot relative to the target domain and sequentially calculate the MMD loss between the corresponding subset data and the test data. After computing the MMD loss for each subset, we update the parameters of the adaptation layer h ϕ . During this stage, the parameters of g θ and c W are fixed. For i = 0, 1, . . . , n -1,\n\nReducing Drift Re-generalization To ensure the effectiveness of the representations learned in the inner loop, we aggregate the losses incurred during each sub-domain adaptation and update h ϕ through gradient backpropagation. This enables the model to learn the trend of changes from the source domain to the target data, thereby enhancing the model's adaptability to the test data.\n\nHere, d H∆H denotes the H∆H distance.\n\nAfter obtaining the updated adaptation layer, we perform FSL-based testing.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets And Protocols",
      "text": "We evaluated the proposed EvoFA on two open-source datasets, SEED  [32]  and SEED-V  [33] , and conducted both intra-subject and inter-subject experiments on each dataset.\n\nFirst, we trained the model under three common FSL frameworks and used the standard FSL-based test results as the baseline  [26] -  [28] . To demonstrate the effectiveness of EvoFA, we directly used the model trained on the standard FSL framework and introduced the fast adaptation meta-testing module shown in Figure  2 (b) during testing.\n\nIn this section, all results are derived from the author's replication of the original study, except for the comparison results in Figure  4 , which are extracted from the corresponding citations. Due to the differences in tasks and the strict division of the training, validation, and test sets in this study, there are significant discrepancies between the reported results and those in the original papers.\n\n1) Datasets: SEED dataset: The SEED dataset collects EEG-based emotion information from 15 subjects, with each subject providing data in three different sessions on separate dates. Each session contains 15 trials, with each trial corresponding to one of three different emotional states: positive, negative, and neutral. In this study, we use preprocessed and feature-extracted EEG signals, each EEG feature corresponds to one second of data and a 3-second sliding window.\n\nSEED5 dataset: The SEED-V dataset collects multimodal emotional information from 16 subjects. Each subject provided data in three sessions on different dates, with each session comprising 15 trials. Each trial corresponds to one of five emotional states: happy, disgust, neutral, fear, and sad.\n\n2) Protocols: Intra-subject experiment To simulate intrasubject modal drift, we designed an Intra-subject experiment. For a selected subject, the data from first session was used as the training set, and the data from the subsequent two sessions were respectively used as the validation and test sets. The time intervals between the validation and test sets, relative to the training data, were designed to mimic the effects of modal drift the subject's EEG data over time.\n\nInter-subject experiment Inter-subject domain drift, a common phenomenon, is extensively present. To simulate this, we designed an Inter-subject experiment. Each subject was sequentially used as the test data, with 12 randomly selected subjects serving as the training data, and the remaining subjects used as validation data. Since each subject's data encompasses three sessions, and there is variability in the data across different sessions for the same subject, we conducted intersubject emotion recognition experiments separately for each session. The results of these experiments are independently presented in Table  III  and Table  IV .\n\nIn both intra-subject and inter-subject experiments, each test was conducted ensuring that the support and query sets were sampled from the same subject's selected session, to maintain a high degree of similarity between the support and query sets.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Intra-Subject Emotion Recognition",
      "text": "Intra-subject emotion recognition aims to demonstrate: (1) the advantages of online calibration over supervised learning;\n\n(2) the further improvement of EvoFA over standard FSL.\n\nFor supervised learning methods, we strictly followed the train-validation-test paradigm. During the training of each model, the model with the best performance on the validation set was saved and tested centrally after all training was completed. The learning rate of the model was set to 0.003 and the batch size was set to 32.\n\nFor online calibration methods, we conducted 3-way 1-shot and 3-way 5-shot experiments on the 3-class SEED dataset, where the number of samples per class in the query set was 10. We conducted 5-way 1-shot and 5-way 5-shot experiments on the 5-class SEED-V dataset, where the number of samples per class in the query set was set to five. This was done to ensure that the batch size was close to that of supervised learning.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "1) Fsl-Based Online Calibration:",
      "text": "To demonstrate the superiority of online calibration over supervised learning, we reimplemented four supervised learning-based methods and three FSL-based online calibration methods.  Table  I  demonstrates that online calibration methods outperform supervised learning methods by over 10% on the SEED dataset. This substantial difference intuitively reflects the modal drift of a subject's EEG data over time due to data collection from different sessions at different times. Moreover, supervised learning models cannot directly address this modal drift by deepening the network. In contrast, online calibration methods achieve significant improvements over supervised learning models by utilizing a minimal amount of calibration data from the test set.\n\nTable  II  reveals an even more substantial performance difference between supervised learning and online calibration methods compared to Table  I . Additionally, the model achieves a recognition accuracy of over 86% in the 5-class task after introducing only one calibration sample for each class. We speculate that this significant performance improvement stems not only from the advantages of the online calibration model but also from the high similarity between data within the same trial in the SEED-V dataset.\n\nTo more intuitively demonstrate the differences in emotion recognition between supervised models and online calibration models, we present the t-SNE visualizations of emotion recognition outputs in Figure  3 . These visualizations compare models trained on GCN and PN+EvoFA frameworks for two different subjects. Benefiting from the guidance of calibration data, the online models exhibit better data classification capabilities. 2) EvoFA's testing performance boost: Observing all the results in Tables  I  and II , it is evident that all the online calibration networks performed quite well across every setting on each dataset. We speculate two main reasons for this. First, meta-learning based methods forgo learning to fit the data, instead learning an appropriate metric, which gives them better generalization performance relative to traditional methods. Another reason is that the test data has high similarity within the same trial (corresponding to the same class), which makes it easy for the network to distinguish the category to which the query data belongs.\n\nAmong the three FSL frameworks, RelationNet demonstrated a significant disadvantage relative to the other two, with its accuracy on the smaller SEED-V dataset being about 10% lower than the other networks. We hypothesize that the additional relation module in RelationNet might not be adequately trained on small-sample EEG-based emotion recognition datasets, thus failing to learn an effective metric between the support and query sets. On the SEED dataset, increasing the number of samples per category in the support set from 1shot to 5-shot led to an approximate 4% improvement in model performance. Similarly, on the SEED-V dataset, increasing the volume of support set data also resulted in about a 2% improvement, indicating the positive impact of augmenting calibration data quantity on model effectiveness.\n\nEvoFA enhances the adaptability of the test data by making it conform to the temporal variations of the training data and by narrowing the distance to the overall distribution of the test data during the testing process. This approach results in improved model compatibility with the test data. In the testing phase, the introduction of EvoFA led to performance gains across all three different FSL frameworks. On the SEED dataset, there was an average accuracy increase of 0.47%; on the SEED-V dataset, the average improvement was 0.44%. Furthermore, comparing the 1-shot and 5-shot settings, a larger support set contributes more effectively to the optimization of meta-adaptation, thereby resulting in more significant improvements.\n\nAlthough these enhancements are relatively modest compared to the original FSL frameworks, they are significant given that the model does not require retraining. The incorporation of the rapid adaptation module at the time of use, without introducing additional training losses, is a key advantage. Furthermore, the module's effectiveness across all three FSL frameworks demonstrates its general applicability.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Inter-Subject Emotion Recognition",
      "text": "Tables III and IV respectively display the experimental results of inter-subject emotion recognition on the SEED and SEED-V datasets. The volume of data for the support and query sets was set identically to the intra-subject experiments described in the previous section.\n\nThe performance of the three FSL frameworks in the intersubject tasks showed a trend similar to that in the intrasubject tasks, where RelationNet continued to underperform, with its effectiveness significantly lower than the other two frameworks. In 1-shot tasks, MatchingNet exhibited the best performance; however, as the amount of calibration data increased, ProtoNet surpassed the others, achieving significantly better results than the other two frameworks.\n\nCompared to the intra-experiment, the accuracy in the interexperiment dropped precipitously. Since the query and support sets for testing were both sampled from the same session of a single subject, this eliminates the possibility of significant differences between them. We speculate that the drastic decline in model performance can be attributed primarily to the substantial disparity between the training and testing data. By incorporating the EvoFA module during testing, the model's test accuracy on the SEED dataset improved by an average of 0.41%, and on the SEED-V dataset, there was an average increase of 0.26% in test accuracy. Compared to the intra-subject experiments, the effectiveness of EvoFA in inter-subject tasks decreased, which may also stem from the substantial differences between the training and test data, rendering the model's fine-tuning during meta-testing less effective.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D. Comparison With Existing Online Methods",
      "text": "To validate the superior advantages of the FSL framework in rapid calibration for EEG-based emotion recognition, we compared ProtoNet with several existing online EEG emotion recognition methods. The results are displayed in Figure  4 . MS-S-STM refers to a multi-source online cross-subject emotion recognition method, S-STM represents its version using unified source data, and IC denotes training independent classifiers on calibration data and testing on unlabeled data  [36] . To ensure fair comparison, we replicated the experimental setup of MS-S-STM. In our experiments, each of the 15 subjects was alternately used as the test set. The first 3 trials of a specified session were used to sample the support set, the following 12 trials for the query set, and the data from the remaining 14 subjects were used as the training set.\n\nIt is observable that although the FSL model's test accuracy is only 78% with just one calibration data per category, increasing the number of calibration data per category to 3 already achieves a recognition accuracy comparable to MS-S-STM using 10 calibration data. When the FSL has five calibration data per category, it surpasses the performance of comparative methods using more calibration data. Moreover, the introduction of the EvoFA module during testing further enhances the model's performance on ProtoNet. These experimental results demonstrate the outstanding advantage of FSL methods over traditional models directly trained with data in the online recognition tasks.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "E. Discussion",
      "text": "Comparing the experimental results of the first four sections, we found that:\n\n1) Modal shift is widely prevalent. On the SEED dataset, the performance of the online calibration mode declined by approximately 25% in the inter-subject tasks compared to the intra-subject tasks; on the SEED-V dataset, this performance drop increased to 40%. This indicates a more severe data modal difference across inter-subject tasks. In EEG emotion recognition tasks, facing this modal shift is unavoidable.\n\n2) Supervised learning encounters bottlenecks. Faced with such significant modal drift in EEG data, supervised learning has struggled to address this issue through increasing network depth or generalization ability. Although there has been a plethora of work in recent years focusing on improving the accuracy of models on validation sets in EEG emotion recognition tasks, this improvement faces significant shrinkage when additional test sets are introduced. 3) FSL still faces limitations. Current FSL-based online calibration methods still require labeled data. However, it is still costly to calibrate the data category during calibration, so it is very meaningful to update it to an unlabeled online calibration model. 4) EvoFA repairs the calibration mode. EvoFA attempts to address the issue of distribution differences caused by data drift during testing (calibration) by introducing domain adaptation between training and testing data, and preliminary results in experimental results show promise. By introducing testing-time adaptation, models are expected to achieve unlabeled online calibration, which is also our future directions.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "V. Conclusion",
      "text": "This paper addresses the adaptability issue in EEG emotion recognition caused by domain drift and proposes a framework, EvoFA, suitable for rapid calibration in online EEG emotion recognition. To tackle the data modal shift in EEG data and the subsequent need for rapid calibration, EvoFA integrates the rapid adaptation of FSL with the distribution alignment of DA in an organic manner. By adding EvoFA as a plug-in to various FSL frameworks based on online calibration modes, it reduces the domain gap between the source domain data and target domain data during testing, thereby enhancing the model's recognition ability. EvoFA, with its high compatibility and no need for retraining, brings about an additional improvement of around 0.4% in testing structures across three FSL frameworks. Moving forward, we will continue to focus on rapid modal adaptation in EEG, advancing the practical application of deep models in the EEG domain.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Approaches for adapting EEG emotion recognition models. (a)",
      "page": 1
    },
    {
      "caption": "Figure 1: (c), EvoFA requires only a small",
      "page": 2
    },
    {
      "caption": "Figure 2: FSL-based",
      "page": 3
    },
    {
      "caption": "Figure 2: A schematic representation of the two-step generalization in EvoFA. (a) First generalization (meta-training): initializing an emotion",
      "page": 4
    },
    {
      "caption": "Figure 2: (b) during testing.",
      "page": 5
    },
    {
      "caption": "Figure 4: , which are extracted from the corresponding",
      "page": 5
    },
    {
      "caption": "Figure 3: These visualizations compare",
      "page": 6
    },
    {
      "caption": "Figure 3: t-SNE visualization of emotion recognition output using (a)",
      "page": 7
    },
    {
      "caption": "Figure 4: MS-S-STM refers to a multi-source online cross-subject",
      "page": 8
    },
    {
      "caption": "Figure 4: Experimental results of five online calibration models with",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "test\ntrain"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "tion recognition has gained significant\ntraction due to its",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "accuracy and objectivity. However,\nthe non-stationary na-",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "train\ntrain\n(b)\n(c)"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "ture of EEG signals leads to distribution drift over\ntime,",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "causing severe performance degradation when the model",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "Source\nSource\nModel\nModel"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "is\nreused. While numerous domain adaptation (DA)\nap-",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "proaches have been proposed in recent years to address",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "test"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "this issue,\ntheir\nreliance on large amounts of\ntarget data",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "Adaptation\ntest"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "Adaptation"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "for calibration restricts them to offline scenarios, rendering",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "Target\nTarget"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "them unsuitable for real-time applications. To address this",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "Test \nTest \nCalibration"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "challenge,\nthis paper proposes Evolvable Fast Adaptation",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "data\ndata\ndata"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "(EvoFA), an online adaptive framework tailored for EEG",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "data. EvoFA organically integrates the rapid adaptation of",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "few samples"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "Few-Shot Learning (FSL) and the distribution matching of",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "Domain Adaptation (DA) through a two-stage generalization",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "Seen during training\nUnseen during training"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "process. During the training phase, a robust base meta-",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "learning model\nis constructed for strong generalization.\nIn",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "Fig. 1.\nApproaches for adapting EEG emotion recognition models. (a)"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "the testing phase, a designed evolvable meta-adaptation",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "Calibration-based (supervised) methods employ a significant quantity"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "module iteratively aligns the marginal distribution of target",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "of\nlabeled calibration data in calibration experiments to retrain or fine-"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "(testing) data with the evolving source (training) data within",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "tune the model.\n(b) Offline adaptation methods utilize all\ntarget data,"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "a model-agnostic meta-learning framework, enabling the",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "encompassing both calibration and test data,\nto conduct\ntransductive"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "model\nto learn the evolving trends of\ntesting data relative",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "adaptation.\n(c) Online adaptation approaches prioritize rapid and effi-"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "cient model adjustments using a smaller amount of calibration data for"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "to training data and improving online testing performance.",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "inductive adaptation,\ntesting on unseen data."
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "Experimental results demonstrate that EvoFA achieves sig-",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "nificant\nimprovements compared to the basic FSL method",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "and previous online methods. The introduction of EvoFA",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "paves the way for broader adoption of EEG-based emotion",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "emotions [3],\n[4]. However, despite significant advancements"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "recognition in real-world applications. Our\ncode will be",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "in improving the efficiency and accuracy of\nrecognition,\nthe"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "released upon publication.",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "decline\nin model\nperformance when\nreused,\nespecially\nin"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "Index Terms— Domain adaptation, Electroencephalogra-",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "online settings, remains a prominent issue limiting the practical"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "phy, Emotion recognition, Meta learning",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "application of EEG-based emotion recognition [5]."
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "A major obstacle to the reusability of EEG-based emotion"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "recognition is the problem of ”domain drift.” This issue man-"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "I.\nINTRODUCTION",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "ifests as changes\nin the statistical properties of EEG signals"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "for vari-",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "E MOTION recognition holds immense potential",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "over\ntime, significantly impacting the accuracy and reliability"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "including education and psychological",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "of emotion recognition models. Factors contributing to domain"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "diagnosis and treatment\n[1],\n[2]. EEG-based recognition has",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "drift\ninclude\nchanges\nin\nthe\nphysiological\nstate\nof\nsubjects"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "attracted research interest\nin recent years due to its low cost,",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "(such as fatigue or hunger), environmental\ninterference (such"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "high accuracy, and the difficulty for subjects to conceal\ntheir",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "as noise or\nlight), and errors in device usage [6],\n[7]."
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "Domain drift poses a significant challenge to model perfor-"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "Manuscript\nsubmitted 31 May 2024. This work was\nsupported in",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "part by National Natural Science Foundation of China under Grant",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "mance . Existing methods to address this issue can be broadly"
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "62106248,\nin part by the Ningbo Clinical Research Center\nfor Med-",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "categorized into two approaches."
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "ical\nImaging\nunder Grant\n2021L003,\nand\nin\npart\nby\nthe Guang-",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": ""
        },
        {
          "Abstract— Electroencephalography\n(EEG)-based\nemo-": "",
          "(a)\nCalibration Experiment\nTest Experiment\nModel": "•\nSupervised methods: As\ndepicted\nin Figure\n1(a),\nthis"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": "Domain drift poses a significant challenge to model perfor-"
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": ""
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": "mance . Existing methods to address this issue can be broadly"
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": "categorized into two approaches."
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": ""
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": "•\nSupervised methods: As\ndepicted\nin Figure\n1(a),\nthis"
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": ""
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": "approach relies on a\nlarge\namount of\nlabeled data\nfor"
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": ""
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": "direct model\ntraining\nor\nfine-tuning[8],\n[9]. While\nit"
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": ""
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": "grants generalization capability initially,\nthe model’s per-"
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": ""
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": "formance progressively degrades as domain drift\nin new"
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": ""
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": "data becomes persistent. Maintaining high performance"
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": ""
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": "necessitates frequent\nretraining with substantial amounts"
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": ""
        },
        {
          "as noise or\nlight), and errors in device usage [6],\n[7].": "of newly labeled data, hindering real-world practicality."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "on new data. However, existing transfer learning methods": "still require a significant amount of unlabeled target data",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "The\nsubsequent\nsections\nare\norganized\nas\nfollows:\nSec-"
        },
        {
          "on new data. However, existing transfer learning methods": "for distribution matching, which can be\na\nlimitation in",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "tion II reviews the progress in EEG-based emotion recognition,"
        },
        {
          "on new data. However, existing transfer learning methods": "resource-constrained scenarios.",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "DA and meta-learning. Section III primarily introduces our"
        },
        {
          "on new data. However, existing transfer learning methods": "Both approaches share a common dependency on substantial",
          "data.": "proposed EvoFA framework, which\ncleverly\ncombines\nthe"
        },
        {
          "on new data. However, existing transfer learning methods": "calibration data for model adjustments. This reliance on large",
          "data.": "advantages of FSL rapid generalization and DA distribution"
        },
        {
          "on new data. However, existing transfer learning methods": "datasets\nhinders\ntheir\napplicability\nin\nreal-world\nsituations",
          "data.": "matching, making\nit\nsuitable\nfor EEG emotion\nrecognition"
        },
        {
          "on new data. However, existing transfer learning methods": "where data acquisition might be expensive or\nlimited.",
          "data.": "tasks. Section\nIV demonstrates\nthe\nimprovements\nachieved"
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "by EvoFA on intra-subject and inter-subject\ntasks. Section V"
        },
        {
          "on new data. However, existing transfer learning methods": "Humans exhibit\nremarkable ability to recognize new cate-",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "summarizes this work and provides future perspectives."
        },
        {
          "on new data. However, existing transfer learning methods": "gories with only a few examples.\nInspired by this capability,",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "FSL has\nemerged\nas\na\npromising\napproach to\nsignificantly",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "reduce the calibration data required for models on new data.",
          "data.": "II. RELATED WORK"
        },
        {
          "on new data. However, existing transfer learning methods": "Applying FSL to EEG-based emotion recognition holds\nthe",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "A. EEG Emotion Recognition"
        },
        {
          "on new data. However, existing transfer learning methods": "potential\nto address the challenge of\ntime-consuming calibra-",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "EEG-based emotion recognition can be broadly categorized"
        },
        {
          "on new data. However, existing transfer learning methods": "tion [11]–[13]. However, the FSL framework primarily focuses",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "into three types based on the level of access to test/target data."
        },
        {
          "on new data. However, existing transfer learning methods": "on rapid adaptation to entirely new categories, where the train-",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "Supervised Learning:\nIn this approach,\ntest data is entirely"
        },
        {
          "on new data. However, existing transfer learning methods": "ing and test sets have non-overlapping in label space. Applying",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "unknown. The\nprimary\nreliance\nis\non\nsupervised\nlearning,"
        },
        {
          "on new data. However, existing transfer learning methods": "the\nbasic\nFSL model\nrigidly\nto EEG emotion\nrecognition",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "where models are trained on training data to develop strong"
        },
        {
          "on new data. However, existing transfer learning methods": "overlooks two critical\nissues: for EEG data,\nthere may be only",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "generalization capabilities that can fit well on test data. Since"
        },
        {
          "on new data. However, existing transfer learning methods": "limited modality shift between source and target data, and the",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "deeper networks typically have stronger fitting abilities, con-"
        },
        {
          "on new data. However, existing transfer learning methods": "label space for training and test data are completely identical.",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "structing deep models to learn robust representations is a main-"
        },
        {
          "on new data. However, existing transfer learning methods": "Considering that DA is mainly designed to address data",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "stream method [9], [14]. In recent years, Graph Convolutional"
        },
        {
          "on new data. However, existing transfer learning methods": "drift from the training set\nto the test set within the same label",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "Networks (GCNs) have also been proven effective for handling"
        },
        {
          "on new data. However, existing transfer learning methods": "space, and FSL enables rapid calibration in an online learning",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "EEG data due to their ability to capture critical\ninformation"
        },
        {
          "on new data. However, existing transfer learning methods": "mode, an ideal approach would be to organically combine the",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "from brain topologies, allowing better discrimination of emo-"
        },
        {
          "on new data. However, existing transfer learning methods": "online rapid recognition capability of FSL with the distribution",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "tional patterns at both the feature and brain topology levels"
        },
        {
          "on new data. However, existing transfer learning methods": "matching ability of DA to address data drift.",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "[8],\n[15]–[17]. However,\nthe\nperformance\nof\nthese\ntrained"
        },
        {
          "on new data. However, existing transfer learning methods": "To\naddress\nthe\nissue\nof\ndata\ndrift\nfaced\nby models\ndur-",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "models often significantly declines when applied to completely"
        },
        {
          "on new data. However, existing transfer learning methods": "ing online use,\nthis paper proposes EvoFA,\na novel online",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "unknown test data."
        },
        {
          "on new data. However, existing transfer learning methods": "adaptation\nframework\nfor EEG-based\nemotion\nrecognition.",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "Offline Learning: This approach allows access to part or all"
        },
        {
          "on new data. However, existing transfer learning methods": "As\nillustrated in Figure 1 (c), EvoFA requires only a small",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "of\nthe unlabeled test data. To address\nthe inevitable issue of"
        },
        {
          "on new data. However, existing transfer learning methods": "amount of calibration data to quickly calibrate the model\nfor",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "modal drift, many studies have turned to offline learning modes"
        },
        {
          "on new data. However, existing transfer learning methods": "effective testing. EvoFA combines the advantages of FSL rapid",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "based on transfer learning [3], [4], [10]. She et al. [4] proposed"
        },
        {
          "on new data. However, existing transfer learning methods": "generalization and DA distribution matching through a two-",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "a new emotion recognition method based on a multisource"
        },
        {
          "on new data. However, existing transfer learning methods": "step generalization process. During the\ntraining phase, FSL",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "associate domain adaptation (DA) network, considering both"
        },
        {
          "on new data. However, existing transfer learning methods": "is utilized to initialize a general emotion recognition model",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "domain\ninvariant\nand\ndomain-specific\nfeatures.\nSSDA was"
        },
        {
          "on new data. However, existing transfer learning methods": "with\nstrong\ngeneralization\ncapabilities.\nIn\nthe meta-testing",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "proposed to align the joint distributions of subjects, assuming"
        },
        {
          "on new data. However, existing transfer learning methods": "phase,\ndomain\nshifts\ninduced\nby\ndata\ndrift\nare\nsimulated,",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "that\nfine-grained\nstructures must\nbe\naligned\nto\nperform a"
        },
        {
          "on new data. However, existing transfer learning methods": "and\nan\nevolvable meta-adaptation module\nis\nintroduced\nto",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "greater knowledge transfer [3]. However, since offline learning"
        },
        {
          "on new data. However, existing transfer learning methods": "align the\ntarget domain with the gradually changing source",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "requires access\nto a large amount of\ntest data,\nit often loses"
        },
        {
          "on new data. However, existing transfer learning methods": "domain,\nthereby mitigating the impact of data drift on model",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "its applicability in most\nreal-world scenarios."
        },
        {
          "on new data. However, existing transfer learning methods": "performance. Experimental\nresults\ndemonstrate\nthat EvoFA",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "Online Learning: In this scenario, only a minimal amount of"
        },
        {
          "on new data. However, existing transfer learning methods": "achieves significant\nimprovements compared to the basic FSL",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "test set calibration data is accessible. To better meet practical"
        },
        {
          "on new data. However, existing transfer learning methods": "method and previous online methods.",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "application\nneeds,\nsome\nstudies\nhave\nattempted\nto\nachieve"
        },
        {
          "on new data. However, existing transfer learning methods": "Our work makes the following significant contributions:",
          "data.": ""
        },
        {
          "on new data. However, existing transfer learning methods": "",
          "data.": "rapid\ncalibration\nand\nonline\nrecognition\nof EEG data. Pan"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "• Offline\nadaptation methods:\nIllustrated\nin Figure\n1(b),",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "in EEG data modalities. This method requires no retrain-"
        },
        {
          "2": "this\napproach\nutilizes\ntransfer\nlearning\nto\nbridge\nthe",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "ing and is compatible with various FSL models."
        },
        {
          "2": "distributional gap between source and target domains[3],",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "• Experimental\nresults\non\ntwo\ndatasets\ndemonstrate\nthat"
        },
        {
          "2": "[10]. By leveraging a pre-trained model and fine-tuning",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "EvoFA not only achieves superior performance in online"
        },
        {
          "2": "it with target data,\nthe models gain discriminative ability",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "recognition tasks but also significantly reduces calibration"
        },
        {
          "2": "on new data. However, existing transfer learning methods",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "data."
        },
        {
          "2": "still require a significant amount of unlabeled target data",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "The\nsubsequent\nsections\nare\norganized\nas\nfollows:\nSec-"
        },
        {
          "2": "for distribution matching, which can be\na\nlimitation in",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "tion II reviews the progress in EEG-based emotion recognition,"
        },
        {
          "2": "resource-constrained scenarios.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "DA and meta-learning. Section III primarily introduces our"
        },
        {
          "2": "Both approaches share a common dependency on substantial",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "proposed EvoFA framework, which\ncleverly\ncombines\nthe"
        },
        {
          "2": "calibration data for model adjustments. This reliance on large",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "advantages of FSL rapid generalization and DA distribution"
        },
        {
          "2": "datasets\nhinders\ntheir\napplicability\nin\nreal-world\nsituations",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "matching, making\nit\nsuitable\nfor EEG emotion\nrecognition"
        },
        {
          "2": "where data acquisition might be expensive or\nlimited.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "tasks. Section\nIV demonstrates\nthe\nimprovements\nachieved"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "by EvoFA on intra-subject and inter-subject\ntasks. Section V"
        },
        {
          "2": "Humans exhibit\nremarkable ability to recognize new cate-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "summarizes this work and provides future perspectives."
        },
        {
          "2": "gories with only a few examples.\nInspired by this capability,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "FSL has\nemerged\nas\na\npromising\napproach to\nsignificantly",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "reduce the calibration data required for models on new data.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "II. RELATED WORK"
        },
        {
          "2": "Applying FSL to EEG-based emotion recognition holds\nthe",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "A. EEG Emotion Recognition"
        },
        {
          "2": "potential\nto address the challenge of\ntime-consuming calibra-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "EEG-based emotion recognition can be broadly categorized"
        },
        {
          "2": "tion [11]–[13]. However, the FSL framework primarily focuses",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "into three types based on the level of access to test/target data."
        },
        {
          "2": "on rapid adaptation to entirely new categories, where the train-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Supervised Learning:\nIn this approach,\ntest data is entirely"
        },
        {
          "2": "ing and test sets have non-overlapping in label space. Applying",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "unknown. The\nprimary\nreliance\nis\non\nsupervised\nlearning,"
        },
        {
          "2": "the\nbasic\nFSL model\nrigidly\nto EEG emotion\nrecognition",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "where models are trained on training data to develop strong"
        },
        {
          "2": "overlooks two critical\nissues: for EEG data,\nthere may be only",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "generalization capabilities that can fit well on test data. Since"
        },
        {
          "2": "limited modality shift between source and target data, and the",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "deeper networks typically have stronger fitting abilities, con-"
        },
        {
          "2": "label space for training and test data are completely identical.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "structing deep models to learn robust representations is a main-"
        },
        {
          "2": "Considering that DA is mainly designed to address data",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "stream method [9], [14]. In recent years, Graph Convolutional"
        },
        {
          "2": "drift from the training set\nto the test set within the same label",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Networks (GCNs) have also been proven effective for handling"
        },
        {
          "2": "space, and FSL enables rapid calibration in an online learning",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "EEG data due to their ability to capture critical\ninformation"
        },
        {
          "2": "mode, an ideal approach would be to organically combine the",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "from brain topologies, allowing better discrimination of emo-"
        },
        {
          "2": "online rapid recognition capability of FSL with the distribution",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "tional patterns at both the feature and brain topology levels"
        },
        {
          "2": "matching ability of DA to address data drift.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[8],\n[15]–[17]. However,\nthe\nperformance\nof\nthese\ntrained"
        },
        {
          "2": "To\naddress\nthe\nissue\nof\ndata\ndrift\nfaced\nby models\ndur-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "models often significantly declines when applied to completely"
        },
        {
          "2": "ing online use,\nthis paper proposes EvoFA,\na novel online",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "unknown test data."
        },
        {
          "2": "adaptation\nframework\nfor EEG-based\nemotion\nrecognition.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Offline Learning: This approach allows access to part or all"
        },
        {
          "2": "As\nillustrated in Figure 1 (c), EvoFA requires only a small",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "of\nthe unlabeled test data. To address\nthe inevitable issue of"
        },
        {
          "2": "amount of calibration data to quickly calibrate the model\nfor",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "modal drift, many studies have turned to offline learning modes"
        },
        {
          "2": "effective testing. EvoFA combines the advantages of FSL rapid",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "based on transfer learning [3], [4], [10]. She et al. [4] proposed"
        },
        {
          "2": "generalization and DA distribution matching through a two-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "a new emotion recognition method based on a multisource"
        },
        {
          "2": "step generalization process. During the\ntraining phase, FSL",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "associate domain adaptation (DA) network, considering both"
        },
        {
          "2": "is utilized to initialize a general emotion recognition model",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "domain\ninvariant\nand\ndomain-specific\nfeatures.\nSSDA was"
        },
        {
          "2": "with\nstrong\ngeneralization\ncapabilities.\nIn\nthe meta-testing",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "proposed to align the joint distributions of subjects, assuming"
        },
        {
          "2": "phase,\ndomain\nshifts\ninduced\nby\ndata\ndrift\nare\nsimulated,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "that\nfine-grained\nstructures must\nbe\naligned\nto\nperform a"
        },
        {
          "2": "and\nan\nevolvable meta-adaptation module\nis\nintroduced\nto",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "greater knowledge transfer [3]. However, since offline learning"
        },
        {
          "2": "align the\ntarget domain with the gradually changing source",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "requires access\nto a large amount of\ntest data,\nit often loses"
        },
        {
          "2": "domain,\nthereby mitigating the impact of data drift on model",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "its applicability in most\nreal-world scenarios."
        },
        {
          "2": "performance. Experimental\nresults\ndemonstrate\nthat EvoFA",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Online Learning: In this scenario, only a minimal amount of"
        },
        {
          "2": "achieves significant\nimprovements compared to the basic FSL",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "test set calibration data is accessible. To better meet practical"
        },
        {
          "2": "method and previous online methods.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "application\nneeds,\nsome\nstudies\nhave\nattempted\nto\nachieve"
        },
        {
          "2": "Our work makes the following significant contributions:",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "rapid\ncalibration\nand\nonline\nrecognition\nof EEG data. Pan"
        },
        {
          "2": "• We propose EvoFA, a novel\nframework tailored for on-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "et al.\n[18] proposed a novel Online Multimodal Hypergraph"
        },
        {
          "2": "line adaptation of EEG data, which significantly reduces",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Learning\n(OMHGL) method, which\nintegrates multimodal"
        },
        {
          "2": "calibration data and lowers the barrier for deploying EEG-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "information\nbased\non\ntime-series\nphysiological\nsignals\nfor"
        },
        {
          "2": "based emotion recognition in real-world scenarios.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "emotion recognition. Blanco et al.\n[19] developed a machine"
        },
        {
          "2": "• Within\nthis\nframework, we\nintroduce\na\nflexible\nand",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "learning model\nusing\nprincipal\ncomponent\nanalysis,\npower"
        },
        {
          "2": "lightweight rapid test-time transfer method that mitigates",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "spectral density,\nrandom forest, and Extra-Trees that can rec-"
        },
        {
          "2": "the impact of domain drift caused by continuous changes",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "ognize emotions in real\ntime, providing estimates of valence,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR et al.: TITLE": "arousal, and dominance (VAD) every five seconds. However,",
          "3": "III. METHODS"
        },
        {
          "AUTHOR et al.: TITLE": "to maintain compatibility with real-time\nrequirements,\nthese",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "EvoFA achieves\nrapid\ncalibration\nof EEG through\ntwo-"
        },
        {
          "AUTHOR et al.: TITLE": "methods\nlargely\nrely\non\ntraditional machine\nlearning\ntech-",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "step\ngeneralization, mitigating\nthe\nimpact\nof\ndata modality"
        },
        {
          "AUTHOR et al.: TITLE": "niques for\ntraining.",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "drift. The main architecture is detailed in Figure 2. FSL-based"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "primary generalization: We employ G2G paired with 2D CNN"
        },
        {
          "AUTHOR et al.: TITLE": "B. Unsupervised Domain Adaptation",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "as the backbone network to extract deep representations from"
        },
        {
          "AUTHOR et al.: TITLE": "Unsupervised domain adaptation (UDA) has emerged as a",
          "3": "EEG signals and utilize FSL to enhance the model’s gener-"
        },
        {
          "AUTHOR et al.: TITLE": "solution to address\nthe\ninsufficient generalization of models",
          "3": "alization\nability\nduring\nonline\ncalibration. Rapid\nadaptation"
        },
        {
          "AUTHOR et al.: TITLE": "across different data distributions. UDA primarily tackles the",
          "3": "secondary generalization: In the meta-testing phase, we cache"
        },
        {
          "AUTHOR et al.: TITLE": "challenges of\ncostly data\ncollection and annotation by pre-",
          "3": "the data patterns of\nthe source data as they change over\ntime"
        },
        {
          "AUTHOR et al.: TITLE": "training on existing source data and fine-tuning on unlabeled",
          "3": "and construct meta-adaptation of the target data relative to the"
        },
        {
          "AUTHOR et al.: TITLE": "target data to enhance model performance in the target domain.",
          "3": "source domain based on time changes, enabling the model\nto"
        },
        {
          "AUTHOR et al.: TITLE": "UDA methods\ncan\nbe\ncategorized\ninto\nthree main\nap-",
          "3": "adapt\nto in new data. The backbone network parameters are"
        },
        {
          "AUTHOR et al.: TITLE": "proaches. One approach effectively aligns the distributions of",
          "3": "denoted by θ, representing the function gθ. Extracted features"
        },
        {
          "AUTHOR et al.: TITLE": "source and target domain data by minimizing the discrepancy",
          "3": "are\nthen processed through a novel\nadaptation optimization"
        },
        {
          "AUTHOR et al.: TITLE": "in\nthe\nembedding\nspace,\nthereby\nimproving model\nperfor-",
          "3": "parameterized\nby ϕ,\nfollowed\nby\na\nclassification\nlayer hϕ"
        },
        {
          "AUTHOR et al.: TITLE": "mance on target data, where the domain discrepancy is mea-",
          "3": "In the\nlayer cW to yield the corresponding emotional output."
        },
        {
          "AUTHOR et al.: TITLE": "sured by Maximum Mean Discrepancy (MMD) [20] and Joint",
          "3": "following\ndescription,\nto\navoid misunderstandings, we will"
        },
        {
          "AUTHOR et al.: TITLE": "MMD [21]. Another common UDA method introduces domain",
          "3": "selectively use the terms source domain data /\ntarget domain"
        },
        {
          "AUTHOR et al.: TITLE": "discriminators\nto train the model\nto make it difficult\nfor\nthe",
          "3": "data and training data /\ntesting data in different contexts."
        },
        {
          "AUTHOR et al.: TITLE": "discriminators to distinguish between source and target domain",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "features, achieving feature alignment [22], [23]. Self-training-",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "A. Backbone Network"
        },
        {
          "AUTHOR et al.: TITLE": "based methods generate pseudo-labels for\ntarget domain data",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "G2G offers a novel approach to processing EEG data by"
        },
        {
          "AUTHOR et al.: TITLE": "and utilize\nthese\nlabels\nto supervise\nfurther model\ntraining",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "capturing the dense information interactions between electrode"
        },
        {
          "AUTHOR et al.: TITLE": "iteratively,\nprogressively\nenhancing model\nperformance\non",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "nodes\n[9]. This\nallows\nthe processed EEG data\nto maintain"
        },
        {
          "AUTHOR et al.: TITLE": "target data [24],\n[25].",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "a\nlarger\nshape\nand become more\nsuitable\nfor deep learning"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "models, particularly for\ntasks like emotion recognition where"
        },
        {
          "AUTHOR et al.: TITLE": "C. Meta Learning",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "the relationships between electrodes are crucial.\nInspired by"
        },
        {
          "AUTHOR et al.: TITLE": "Meta-learning,\nalso\nknown\nas\n’learning\nto\nlearn’,\nstands",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "this\nconcept, we process\nthe EEG feature, denoted as F ∈"
        },
        {
          "AUTHOR et al.: TITLE": "in contrast\nto traditional\nartificial\nintelligence methods\nthat",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "Rn×d, into a 2D feature interaction matrix F′ ∈ Rc×n×n using"
        },
        {
          "AUTHOR et al.: TITLE": "solve\ntasks\nfrom scratch\nusing\nfixed\nlearning\nalgorithms.",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "the G2G transformation:"
        },
        {
          "AUTHOR et al.: TITLE": "Instead, meta-learning aims to improve the learning algorithm",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "itself based on the experience gained from multiple learning",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "F′2D = G2G(F1D),\n(1)"
        },
        {
          "AUTHOR et al.: TITLE": "episodes. This\napproach\nhas\ngained\nsignificant\ninterest\nin",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "recent years, particularly for tasks involving FSL, which aligns",
          "3": "where n represents\nthe number of electrodes, d denotes\nthe"
        },
        {
          "AUTHOR et al.: TITLE": "well with the challenges of\nlimited calibration data in EEG-",
          "3": "number of\nfeatures per electrode, and c indicate channels."
        },
        {
          "AUTHOR et al.: TITLE": "based emotion recognition. The two primary approaches are",
          "3": "After\nprocessing EEG into\n2D interaction matrices, we"
        },
        {
          "AUTHOR et al.: TITLE": "metric-based and optimization-based approaches.",
          "3": "further employ a ConvNet\nto learn deep representations from"
        },
        {
          "AUTHOR et al.: TITLE": "Metric-based methods focus on learning a meaningful dis-",
          "3": "these matrices. The ConvNet\nincludes four distinct Conv-BN-"
        },
        {
          "AUTHOR et al.: TITLE": "tance or similarity measure, allowing the model\nto embed data",
          "3": "ReLU structures. Both the G2G and the subsequent ConvNet"
        },
        {
          "AUTHOR et al.: TITLE": "into a space where similar patterns are close together. Common",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "are jointly denoted by the representation function gθ. Next, the"
        },
        {
          "AUTHOR et al.: TITLE": "distance metrics include cosine similarity [26], Euclidean dis-",
          "3": "data is\ninput\ninto the meta adaptation optimization layer de-"
        },
        {
          "AUTHOR et al.: TITLE": "tance [27], and CNN-based relational modules [28]. Relation",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "noted by hϕ. This layer comprises two fully-connected layers"
        },
        {
          "AUTHOR et al.: TITLE": "networks\n[28] and Prototypical Networks\n[27] are examples",
          "3": "and aims\nto capture\nthe\nevolving emotional\nrepresentations"
        },
        {
          "AUTHOR et al.: TITLE": "of such approaches in FSL, enabling models to quickly adapt",
          "3": "during meta-testing in section III-D,\nenabling the model\nto"
        },
        {
          "AUTHOR et al.: TITLE": "to new tasks with only a few samples.",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "adapt\nto domain shift. Ultimately,\nthe classification layer cW"
        },
        {
          "AUTHOR et al.: TITLE": "Optimization-based meta-learning focuses on ’learning to",
          "3": "outputs the emotion recognition results.\nIt’s worth noting that"
        },
        {
          "AUTHOR et al.: TITLE": "fine-tune’\nthrough\ngood\nparameter\ninitialization,\nenabling",
          "3": "in\nthe\ncontext\nof RelationNet\n[28],\nthe\nclassification\nlayer"
        },
        {
          "AUTHOR et al.: TITLE": "rapid adaptation to new tasks with minimal gradient updates.",
          "3": "serves a similar\nfunction as the relation module."
        },
        {
          "AUTHOR et al.: TITLE": "MAML prepares\na model\nfor\nfast\nadaptation\nto\nnew tasks",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "through a two-step training process, maintaining compatibility",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "3": "B. FSL-based Preliminary Generalization"
        },
        {
          "AUTHOR et al.: TITLE": "with any model\nthat\nlearns via gradient descent\n[29]. Rep-",
          "3": ""
        },
        {
          "AUTHOR et al.: TITLE": "tile\nsimplifies MAML by steering the\ninitialization towards",
          "3": "Traditional deep learning trains models on large datasets,"
        },
        {
          "AUTHOR et al.: TITLE": "weights\nthat perform well\nacross multiple\ntasks,\neffectively",
          "3": "mapping different\ncategories of data\nto their\ncorresponding"
        },
        {
          "AUTHOR et al.: TITLE": "rendering it a simpler variant of MAML [30]. The optimization",
          "3": "labels. FSL, on the other hand,\nlearns distinguishing features"
        },
        {
          "AUTHOR et al.: TITLE": "framework of MAML and its variants provides\nideas\nfor\nthe",
          "3": "from a small amount of data, enabling the model\nto differen-"
        },
        {
          "AUTHOR et al.: TITLE": "rapid adaptation of EvoFA in the second stage [29]–[31].",
          "3": "tiate between different categories."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "FSL endows models"
        },
        {
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "with generalization"
        },
        {
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "capability to target"
        },
        {
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "data."
        },
        {
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "target\nsource"
        },
        {
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "seEvolving Source Data"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR et al.: TITLE": "Algorithm 1 Two-step generalisation in EvoFA",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "the damage of modal drift and improve the model’s accuracy"
        },
        {
          "AUTHOR et al.: TITLE": "Require: The source data S and the target data T .",
          "5": "on the test set."
        },
        {
          "AUTHOR et al.: TITLE": "Parameter: backbone gθ, adapter hϕ and classifier cW .",
          "5": "We\nconstructed\ndifferent\nlearning\npatterns\nfor\nintra\nand"
        },
        {
          "AUTHOR et al.: TITLE": "Step 1: FSL-based Preliminary Generalization",
          "5": "inter-subject\ntasks, with a more detailed description of both"
        },
        {
          "AUTHOR et al.: TITLE": "1:\nfor t=0 to MaxEpoch do",
          "5": "tasks in Section IV-A. For intra-subject experiments, the modal"
        },
        {
          "AUTHOR et al.: TITLE": "2:\nupdate gθ, hϕ and cW with source data S:",
          "5": "drift of\ntest data relative to training data gradually intensifies"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "over\ntime. To\nreduce\nthe\nimpact\nof modal\ndrift\ncaused\nby"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "temporal changes, we uniformly sample the training set data"
        },
        {
          "AUTHOR et al.: TITLE": "1 Q\nQ(cid:88) i\nN(cid:88) n\nyin log(pin(θ, ϕ, W ))\nLMatchingNet(θ, ϕ, W ) = −",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "=1\n=1",
          "5": "over time to form subsets of the source data. For inter-subject"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "experiments,\nthe modal drift of\ntest data relative to training"
        },
        {
          "AUTHOR et al.: TITLE": "1 Q\nQ(cid:88) i\n(ri(θ, ϕ, W ) − yi)2\nLRelationNet(θ, ϕ, W ) =",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "data mainly stems\nfrom modal differences between different"
        },
        {
          "AUTHOR et al.: TITLE": "=1",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "subjects. To mitigate modal drift caused by different subjects,"
        },
        {
          "AUTHOR et al.: TITLE": "exp(−d(fθ,ϕ,W (xi), nyi ))",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "1 Q\nQ(cid:88) i\nlog\nLProtoNet(θ, ϕ, W ) = −",
          "5": "we sample a subset of data for each subject\nin the training set."
        },
        {
          "AUTHOR et al.: TITLE": "(cid:80)N",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "n=1 exp(−d(fθ,ϕ,W (xi), nn))",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "=1",
          "5": "Fast Adapt\nto Snapshot\nIn the\ninner\nloop, we quantify"
        },
        {
          "AUTHOR et al.: TITLE": "3:\nend for",
          "5": "the drift of each source domain snapshot relative to the target"
        },
        {
          "AUTHOR et al.: TITLE": "Step 2: Rapid Adaptation Secondary Generalization",
          "5": "domain and sequentially calculate the MMD loss between the"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "corresponding subset data and the test data. After computing"
        },
        {
          "AUTHOR et al.: TITLE": "1:\nfor t=0 to MaxIter do",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "the MMD loss\nfor each subset, we update the parameters of"
        },
        {
          "AUTHOR et al.: TITLE": "2:\nSample evolving S′ = {Xs1, Xs2 , . . . , Xsn }",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "the parameters of\nthe adaptation layer hϕ. During this stage,"
        },
        {
          "AUTHOR et al.: TITLE": "3:\nSample T ′ = {Xtn+k }",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "i = 0, 1, . . . , n − 1,\ngθ and cW are fixed. For"
        },
        {
          "AUTHOR et al.: TITLE": "4:\nfor i=0 to n do",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "5:\nCompute adapted parameters with gradient descent:",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "6:",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "(3)\nϕi+1 ← ϕi − ηin ∇ϕ\n(cid:2)d (cid:0)fθ,ϕi,W (Xsi ) , fθ,ϕi,W (Xt)(cid:1)(cid:3) ."
        },
        {
          "AUTHOR et al.: TITLE": "(cid:104)\n(cid:16)\n(cid:16)\n(cid:17)\n(cid:16)\n(cid:17)(cid:17)(cid:105)",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "d\nXspt\nXspt",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "t\nfθ,ϕi,W\n, fθ,ϕi,W\nsi",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "Reducing Drift Re-generalization To\nensure\nthe\neffec-"
        },
        {
          "AUTHOR et al.: TITLE": "7:\nend for",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "tiveness of\nthe representations\nlearned in the inner\nloop, we"
        },
        {
          "AUTHOR et al.: TITLE": "8:\nupdata hϕ with:",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "aggregate the losses\nincurred during each sub-domain adap-"
        },
        {
          "AUTHOR et al.: TITLE": "9:",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "(cid:17)(cid:17)\n(cid:16)\n(cid:16)\n(cid:17)\n(cid:16)",
          "5": "tation and update\nthrough gradient backpropagation.\nthe hϕ"
        },
        {
          "AUTHOR et al.: TITLE": "1 n\nn(cid:88) i\nd\nXqry\nXspt",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "L(ϕ) =\nt\nfθ,ϕn,W\n, fθ,ϕn,W\nsi",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "=1",
          "5": "This\nenables\nthe model\nto learn the\ntrend of\nchanges\nfrom"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "the source domain to the target data,\nthereby enhancing the"
        },
        {
          "AUTHOR et al.: TITLE": "10:\nend for",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "model’s adaptability to the test data."
        },
        {
          "AUTHOR et al.: TITLE": "derlying patterns of modal drift\nin the data, better addressing",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "1 n\nn(cid:88) i\n(4)\nL(ϕ) =\nd (fθ,ϕn,W (Xsi) , fθ,ϕn,W (Xt)) ."
        },
        {
          "AUTHOR et al.: TITLE": "unknown test data. (3) It exhibits broad adaptability, enabling",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "=1"
        },
        {
          "AUTHOR et al.: TITLE": "its use with various FSL models.",
          "5": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "Here, dH∆H denotes the H∆H distance."
        },
        {
          "AUTHOR et al.: TITLE": "D. Rapid Adaptation Secondary Generalization",
          "5": "After obtaining the updated adaptation layer, we perform"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "5": "FSL-based testing."
        },
        {
          "AUTHOR et al.: TITLE": "Unlike traditional FSL-based methods, which directly mea-",
          "5": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "training data, were designed to mimic": "drift on the subject’s EEG data over",
          "the\neffects of modal": "time.",
          "GCN [35]": "DGCNN [8]",
          "48.56 / 10.41": "48.12 / 11.84"
        },
        {
          "training data, were designed to mimic": "",
          "the\neffects of modal": "",
          "GCN [35]": "G2G [9]",
          "48.56 / 10.41": "48.67 / 14.82"
        },
        {
          "training data, were designed to mimic": "Inter-subject experiment",
          "the\neffects of modal": "Inter-subject domain drift, a com-",
          "GCN [35]": "",
          "48.56 / 10.41": ""
        },
        {
          "training data, were designed to mimic": "",
          "the\neffects of modal": "",
          "GCN [35]": "Online Method",
          "48.56 / 10.41": "5-shot"
        },
        {
          "training data, were designed to mimic": "phenomenon,\nis",
          "the\neffects of modal": "extensively\npresent. To\nsimulate\nthis,",
          "GCN [35]": "",
          "48.56 / 10.41": ""
        },
        {
          "training data, were designed to mimic": "we designed an Inter-subject",
          "the\neffects of modal": "experiment. Each subject was",
          "GCN [35]": "MN [26]",
          "48.56 / 10.41": "96.88 ± 1.73"
        },
        {
          "training data, were designed to mimic": "",
          "the\neffects of modal": "",
          "GCN [35]": "MN + EvoFA",
          "48.56 / 10.41": "97.72 ± 1.52"
        },
        {
          "training data, were designed to mimic": "sequentially used as the test data, with 12 randomly selected",
          "the\neffects of modal": "",
          "GCN [35]": "",
          "48.56 / 10.41": ""
        },
        {
          "training data, were designed to mimic": "",
          "the\neffects of modal": "",
          "GCN [35]": "RN [28]",
          "48.56 / 10.41": "88.02 ± 4.04"
        },
        {
          "training data, were designed to mimic": "serving as",
          "the\neffects of modal": "the training data, and the remaining sub-",
          "GCN [35]": "",
          "48.56 / 10.41": ""
        },
        {
          "training data, were designed to mimic": "",
          "the\neffects of modal": "",
          "GCN [35]": "RN + EvoFA",
          "48.56 / 10.41": "89.13 ± 10.18"
        },
        {
          "training data, were designed to mimic": "jects used as validation data. Since each subject’s data encom-",
          "the\neffects of modal": "",
          "GCN [35]": "",
          "48.56 / 10.41": ""
        },
        {
          "training data, were designed to mimic": "",
          "the\neffects of modal": "",
          "GCN [35]": "PN [27]",
          "48.56 / 10.41": "97.75 ± 1.32"
        },
        {
          "training data, were designed to mimic": "passes three sessions, and there is variability in the data across",
          "the\neffects of modal": "",
          "GCN [35]": "",
          "48.56 / 10.41": ""
        },
        {
          "training data, were designed to mimic": "",
          "the\neffects of modal": "",
          "GCN [35]": "PN + EvoFA",
          "48.56 / 10.41": "97.89 ± 1.64"
        },
        {
          "training data, were designed to mimic": "sessions\nfor",
          "the\neffects of modal": "the\nsame\nsubject, we\nconducted inter-",
          "GCN [35]": "",
          "48.56 / 10.41": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "Table\nI demonstrates\nthat online\ncalibration methods out-"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "perform supervised\nlearning methods\nby\nover\n10% on\nthe"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "SEED dataset. This\nsubstantial difference intuitively reflects"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "the modal drift of a subject’s EEG data over time due to data"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "collection from different sessions at different times. Moreover,"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "supervised learning models cannot directly address this modal"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "drift by deepening the network. In contrast, online calibration"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "methods\nachieve\nsignificant\nimprovements\nover\nsupervised"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "learning models by utilizing a minimal amount of calibration"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "data from the test set."
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "Table\nII\nreveals\nan\neven more\nsubstantial\nperformance"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "difference between supervised learning and online calibration"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "methods compared to Table I. Additionally, the model achieves"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "a recognition accuracy of over 86% in the 5-class\ntask after"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "introducing only one\ncalibration sample\nfor\neach class. We"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "speculate that\nthis significant performance improvement stems"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "not only from the advantages of\nthe online calibration model"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "but also from the high similarity between data within the same"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "trial\nin the SEED-V dataset."
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": ""
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "To more intuitively demonstrate the differences in emotion"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "recognition\nbetween\nsupervised models\nand\nonline\ncalibra-"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "tion models, we present\nthe t-SNE visualizations of emotion"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "recognition outputs in Figure 3. These visualizations compare"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "models trained on GCN and PN+EvoFA frameworks for\ntwo"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "different subjects. Benefiting from the guidance of calibration"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "data,\nthe online models exhibit better data classification capa-"
        },
        {
          "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64": "bilities."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": "1) Datasets:\nSEED dataset: The\nSEED dataset\ncollects",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "TABLE I"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "INTRA-SUBJECT EMOTION RECOGNITION ON THE SEED DATASET"
        },
        {
          "6": "EEG-based emotion information from 15 subjects, with each",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "subject providing data in three different sessions on separate",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Supervised Method\nAcc / Std"
        },
        {
          "6": "dates. Each session contains 15 trials, with each trial corre-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "DBN [34]\n58.51 / 12.20"
        },
        {
          "6": "sponding to one of\nthree different emotional\nstates: positive,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "GCN [35]\n61.97 / 9.89"
        },
        {
          "6": "negative, and neutral.\nIn this study, we use preprocessed and",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "DGCNN [8]\n63.85 / 11.04"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "G2G [9]\n66.35 / 10.53"
        },
        {
          "6": "feature-extracted EEG signals, each EEG feature corresponds",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Online Method\n1-shot\n5-shot"
        },
        {
          "6": "to one second of data and a 3-second sliding window.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "SEED5 dataset: The SEED-V dataset collects multimodal",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "MN [26]\n80.35 ± 6.72\n84.37 ± 5.36"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "MN + EvoFA\n81.10 ± 6.22\n84.92 ± 5.12"
        },
        {
          "6": "emotional information from 16 subjects. Each subject provided",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "RN [28]\n79.11 ± 10.18\n83.97 ± 5.35"
        },
        {
          "6": "data\nin three\nsessions on different dates, with each session",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "RN + EvoFA\n79.26 ± 10.17\n84.89 ± 5.66"
        },
        {
          "6": "comprising 15 trials. Each trial\ncorresponds\nto\none of five",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "PN [27]\n84.07 ± 7.60\n87.88 ± 6.20"
        },
        {
          "6": "emotional states: happy, disgust, neutral,\nfear, and sad.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "PN + EvoFA\n84.27 ± 7.82\n88.15 ± 5.91"
        },
        {
          "6": "2) Protocols:\nIntra-subject\nexperiment To\nsimulate\nintra-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "subject modal drift, we designed an Intra-subject experiment.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "For a selected subject,\nthe data from first session was used as",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "TABLE II"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "INTRA-SUBJECT EMOTION RECOGNITION ON THE SEED-V DATASET"
        },
        {
          "6": "the training set, and the data from the subsequent\ntwo sessions",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "were respectively used as the validation and test sets. The time",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Supervised Method\nAcc / Std"
        },
        {
          "6": "intervals between the validation and test\nsets,\nrelative to the",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "DBN [34]\n46.30 / 14.88"
        },
        {
          "6": "training data, were designed to mimic\nthe\neffects of modal",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "GCN [35]\n48.56 / 10.41"
        },
        {
          "6": "drift on the subject’s EEG data over\ntime.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "DGCNN [8]\n48.12 / 11.84"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "G2G [9]\n48.67 / 14.82"
        },
        {
          "6": "Inter-subject experiment\nInter-subject domain drift, a com-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Online Method\n1-shot\n5-shot"
        },
        {
          "6": "mon\nphenomenon,\nis\nextensively\npresent. To\nsimulate\nthis,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "we designed an Inter-subject\nexperiment. Each subject was",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "MN [26]\n94.90 ± 2.11\n96.88 ± 1.73"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "MN + EvoFA\n95.10 ± 2.63\n97.72 ± 1.52"
        },
        {
          "6": "sequentially used as the test data, with 12 randomly selected",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "RN [28]\n86.35 ± 5.84\n88.02 ± 4.04"
        },
        {
          "6": "subjects\nserving as\nthe training data, and the remaining sub-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "RN + EvoFA\n87.23 ± 5.48\n89.13 ± 10.18"
        },
        {
          "6": "jects used as validation data. Since each subject’s data encom-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "PN [27]\n95.69 ± 2.52\n97.75 ± 1.32"
        },
        {
          "6": "passes three sessions, and there is variability in the data across",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "PN + EvoFA\n95.18 ± 2.95\n97.89 ± 1.64"
        },
        {
          "6": "different\nsessions\nfor\nthe\nsame\nsubject, we\nconducted inter-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "subject emotion recognition experiments\nseparately for each",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "session. The\nresults of\nthese\nexperiments\nare\nindependently",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Table\nI demonstrates\nthat online\ncalibration methods out-"
        },
        {
          "6": "presented in Table III and Table IV.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "perform supervised\nlearning methods\nby\nover\n10% on\nthe"
        },
        {
          "6": "In both intra-subject and inter-subject experiments, each test",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "SEED dataset. This\nsubstantial difference intuitively reflects"
        },
        {
          "6": "was conducted ensuring that\nthe support and query sets were",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "the modal drift of a subject’s EEG data over time due to data"
        },
        {
          "6": "sampled from the same subject’s selected session,\nto maintain",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "collection from different sessions at different times. Moreover,"
        },
        {
          "6": "a high degree of similarity between the support and query sets.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "supervised learning models cannot directly address this modal"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "drift by deepening the network. In contrast, online calibration"
        },
        {
          "6": "B.\nIntra-subject Emotion Recognition",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "methods\nachieve\nsignificant\nimprovements\nover\nsupervised"
        },
        {
          "6": "Intra-subject emotion recognition aims to demonstrate:\n(1)",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "learning models by utilizing a minimal amount of calibration"
        },
        {
          "6": "the advantages of online calibration over supervised learning;",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "data from the test set."
        },
        {
          "6": "(2)\nthe further\nimprovement of EvoFA over standard FSL.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Table\nII\nreveals\nan\neven more\nsubstantial\nperformance"
        },
        {
          "6": "For\nsupervised learning methods, we strictly followed the",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "difference between supervised learning and online calibration"
        },
        {
          "6": "train-validation-test\nparadigm. During\nthe\ntraining\nof\neach",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "methods compared to Table I. Additionally, the model achieves"
        },
        {
          "6": "model,\nthe model with the best performance on the validation",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "a recognition accuracy of over 86% in the 5-class\ntask after"
        },
        {
          "6": "set was\nsaved\nand\ntested\ncentrally\nafter\nall\ntraining was",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "introducing only one\ncalibration sample\nfor\neach class. We"
        },
        {
          "6": "completed. The learning rate of\nthe model was\nset\nto 0.003",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "speculate that\nthis significant performance improvement stems"
        },
        {
          "6": "and the batch size was set\nto 32.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "not only from the advantages of\nthe online calibration model"
        },
        {
          "6": "For online calibration methods, we conducted 3-way 1-shot",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "but also from the high similarity between data within the same"
        },
        {
          "6": "and 3-way 5-shot experiments on the 3-class SEED dataset,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "trial\nin the SEED-V dataset."
        },
        {
          "6": "where the number of samples per class in the query set was 10.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "We conducted 5-way 1-shot and 5-way 5-shot experiments on",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "To more intuitively demonstrate the differences in emotion"
        },
        {
          "6": "the 5-class SEED-V dataset, where the number of samples per",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "recognition\nbetween\nsupervised models\nand\nonline\ncalibra-"
        },
        {
          "6": "class in the query set was set\nto five. This was done to ensure",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "tion models, we present\nthe t-SNE visualizations of emotion"
        },
        {
          "6": "that\nthe batch size was close to that of supervised learning.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "recognition outputs in Figure 3. These visualizations compare"
        },
        {
          "6": "1) FSL-based online calibration:\nTo\ndemonstrate\nthe\nsu-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "models trained on GCN and PN+EvoFA frameworks for\ntwo"
        },
        {
          "6": "periority of online\ncalibration over\nsupervised learning, we",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "different subjects. Benefiting from the guidance of calibration"
        },
        {
          "6": "reimplemented\nfour\nsupervised\nlearning-based methods\nand",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "data,\nthe online models exhibit better data classification capa-"
        },
        {
          "6": "three FSL-based online calibration methods.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "bilities."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED DATASET": "SESSION 1"
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED DATASET": ""
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED DATASET": "5-shot"
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED DATASET": "69.42 ± 5.92"
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED DATASET": "69.82 ± 6.23"
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED DATASET": "60.19 ± 12.15"
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED DATASET": "60.40 ± 11.37"
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED DATASET": "73.54 ± 7.16"
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED DATASET": "74.32 ± 6.37"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "0\n2",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": "2"
        },
        {
          "1.0": "",
          "2": "2\n1\n1",
          "2\n2\n2": ""
        },
        {
          "1.0": "0.8",
          "2": "",
          "2\n2\n2": "2"
        },
        {
          "1.0": "",
          "2": "2\n1",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "1\n0",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": "2"
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "1\n1\n1",
          "2\n2\n2": ""
        },
        {
          "1.0": "0.6",
          "2": "",
          "2\n2\n2": "2"
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "0",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "0",
          "2\n2\n2": "2"
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "1",
          "2\n2\n2": ""
        },
        {
          "1.0": "0.4\n0",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": "2"
        },
        {
          "1.0": "0",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "0",
          "2\n2\n2": "2"
        },
        {
          "1.0": "",
          "2": "1",
          "2\n2\n2": "2"
        },
        {
          "1.0": "0.2",
          "2": "0",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "0",
          "2\n2\n2": "2"
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "0",
          "2\n2\n2": ""
        },
        {
          "1.0": "0.0",
          "2": "0",
          "2\n2\n2": ""
        },
        {
          "1.0": "0.0",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "1.0",
          "2": "1",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": "0"
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "2",
          "2\n2\n2": ""
        },
        {
          "1.0": "0.8",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "2",
          "2\n2\n2": "0\n0"
        },
        {
          "1.0": "",
          "2": "2",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "2",
          "2\n2\n2": "0"
        },
        {
          "1.0": "",
          "2": "2",
          "2\n2\n2": "00"
        },
        {
          "1.0": "0.6",
          "2": "2",
          "2\n2\n2": "0\n0"
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": "0"
        },
        {
          "1.0": "",
          "2": "2\n2\n0",
          "2\n2\n2": "0"
        },
        {
          "1.0": "",
          "2": "2",
          "2\n2\n2": "0"
        },
        {
          "1.0": "0",
          "2": "1",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "2",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "1",
          "2\n2\n2": ""
        },
        {
          "1.0": "0.4",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "1",
          "2\n2\n2": "0"
        },
        {
          "1.0": "",
          "2": "1",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "0.2",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": "1"
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": "1"
        },
        {
          "1.0": "",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "0.0",
          "2": "",
          "2\n2\n2": ""
        },
        {
          "1.0": "0.0",
          "2": "",
          "2\n2\n2": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED-V DATASET": ""
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED-V DATASET": ""
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED-V DATASET": "5-shot"
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED-V DATASET": "61.72 ± 5.82"
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED-V DATASET": "62.26 ± 5.63"
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED-V DATASET": "58.25 ± 8.24"
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED-V DATASET": "58.48 ± 8.33"
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED-V DATASET": "75.37 ± 8.37"
        },
        {
          "INTER-SUBJECT EMOTION RECOGNITION ON THE SEED-V DATASET": "76.16 ± 8.58"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "PN + EvoFA\n53.17 ± 6.63\n76.16 ± 8.58",
          "55.62 ± 7.43": "55.38 ± 8.20",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "77.26 ± 8.42\n54.82 ± 7.42\n77.82 ± 6.18"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "95",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "By\nincorporating\nthe EvoFA module\nduring\ntesting,\nthe",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "model’s\ntest accuracy on the SEED dataset\nimproved by an",
          "55.62 ± 7.43": "90",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "average\nof\n0.41%,\nand\non\nthe SEED-V dataset,\nthere was",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "85",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "an\naverage\nincrease\nof\n0.26% in\ntest\naccuracy. Compared",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "to the intra-subject experiments,\nthe effectiveness of EvoFA",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "80",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "in inter-subject\ntasks decreased, which may also stem from",
          "55.62 ± 7.43": "Accuracy(%)",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "the substantial differences between the training and test data,",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "ProtoNet"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "75",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "rendering\nthe model’s\nfine-tuning\nduring meta-testing\nless",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "ProtoNet+EvoFA"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "70",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "effective.",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "MS-S-STM"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "S-STM"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "65",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "D. Comparison with Existing Online Methods",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "IC"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "To validate the superior advantages of\nthe FSL framework",
          "55.62 ± 7.43": "60",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "1\n3\n5\n10\n20\n30\n40\n80"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "in rapid calibration for EEG-based emotion recognition, we",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "Calibration data amount\n(# - shots)"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "compared ProtoNet with several existing online EEG emotion",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "recognition methods.\nThe\nresults\nare\ndisplayed\nin\nFigure",
          "55.62 ± 7.43": "Fig. 4.",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "Experimental\nresults of\nfive online calibration models with"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "different amounts of calibration data.",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "4. MS-S-STM refers\nto a multi-source online\ncross-subject",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "emotion\nrecognition method, S-STM represents\nits\nversion",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "using unified source data, and IC denotes training independent",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "this performance drop increased to 40%. This indicates"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "classifiers on calibration data\nand testing on unlabeled data",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "a more severe data modal difference across inter-subject"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "[36]. To ensure fair comparison, we replicated the experimen-",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "tasks.\nIn EEG emotion\nrecognition\ntasks,\nfacing\nthis"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "tal\nsetup of MS-S-STM.\nIn our experiments, each of\nthe 15",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "modal shift\nis unavoidable."
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "subjects was alternately used as the test set. The first 3 trials",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "2)",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "Supervised learning encounters bottlenecks. Faced with"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "of a specified session were used to sample the support set,\nthe",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "such\nsignificant modal\ndrift\nin EEG data,\nsupervised"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "following 12 trials\nfor\nthe query set, and the data from the",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "learning\nhas\nstruggled\nto\naddress\nthis\nissue\nthrough"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "remaining 14 subjects were used as the training set.",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "increasing network depth or generalization ability. Al-"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "It\nis observable that although the FSL model’s test accuracy",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "though\nthere\nhas\nbeen\na\nplethora\nof work\nin\nrecent"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "is\nonly\n78% with\njust\none\ncalibration\ndata\nper\ncategory,",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "years focusing on improving the accuracy of models on"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "increasing the number of\ncalibration data per\ncategory to 3",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "validation sets\nin EEG emotion recognition tasks,\nthis"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "already achieves a recognition accuracy comparable to MS-",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "improvement faces significant shrinkage when additional"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "S-STM using\n10\ncalibration\ndata. When\nthe FSL has five",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "test sets are introduced."
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "calibration data per category,\nit surpasses the performance of",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "3) FSL still",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "faces\nlimitations. Current FSL-based\nonline"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "comparative methods using more calibration data. Moreover,",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "calibration methods still\nrequire labeled data. However,"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "the introduction of\nthe EvoFA module during testing further",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "it\nis\nstill\ncostly\nto\ncalibrate\nthe\ndata\ncategory\nduring"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "enhances the model’s performance on ProtoNet. These exper-",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "calibration,\nso it\nis very meaningful\nto update it\nto an"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "imental results demonstrate the outstanding advantage of FSL",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "unlabeled online calibration model."
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "methods over\ntraditional models directly trained with data in",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "4) EvoFA repairs",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "the\ncalibration mode. EvoFA attempts"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "the online recognition tasks.",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "to address\nthe\nissue of distribution differences\ncaused"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "by data drift during testing (calibration) by introducing"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "E. Discussion",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "domain\nadaptation\nbetween\ntraining\nand\ntesting\ndata,"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "Comparing the experimental results of the first four sections,",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "and\npreliminary\nresults\nin\nexperimental\nresults\nshow"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "we found that:",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "promise. By introducing testing-time adaptation, models"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "shift\n1) Modal\nis widely prevalent. On the SEED dataset,",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "are\nexpected\nto\nachieve\nunlabeled\nonline\ncalibration,"
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "the performance of the online calibration mode declined",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": "which is also our\nfuture directions."
        },
        {
          "PN [27]\n52.87 ± 6.87\n75.37 ± 8.37": "by approximately 25% in the\ninter-subject\ntasks\ncom-",
          "55.62 ± 7.43": "",
          "76.60 ± 8.87\n54.72 ± 7.11\n77.49 ± 6.37": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR et al.: TITLE": "V. CONCLUSION",
          "9": "M. Jafari, A. Shoeibi, M. Khodatars, et al., “Emotion recognition in"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "in\neeg signals using deep learning methods: A review,” Computers"
        },
        {
          "AUTHOR et al.: TITLE": "This paper addresses the adaptability issue in EEG emotion",
          "9": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "Biology and Medicine, p. 107 450, 2023."
        },
        {
          "AUTHOR et al.: TITLE": "recognition caused by domain drift and proposes a framework,",
          "9": "G. Du,\nJ. Su, L. Zhang, et al., “A multi-dimensional graph convo-"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "lution network for eeg emotion recognition,” IEEE Transactions on"
        },
        {
          "AUTHOR et al.: TITLE": "EvoFA, suitable for\nrapid calibration in online EEG emotion",
          "9": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "Instrumentation and Measurement, vol. 71, pp. 1–11, 2022."
        },
        {
          "AUTHOR et al.: TITLE": "recognition. To tackle the data modal\nshift\nin EEG data and",
          "9": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition"
        },
        {
          "AUTHOR et al.: TITLE": "the\nsubsequent need for\nrapid calibration, EvoFA integrates",
          "9": "IEEE Transactions\non\nusing\nregularized\ngraph\nneural\nnetworks,”"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "Affective Computing, vol. 13, no. 3, pp. 1290–1301, 2020."
        },
        {
          "AUTHOR et al.: TITLE": "the rapid adaptation of FSL with the distribution alignment of",
          "9": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "et\nH.\nZeng, Q. Wu, Y.\nJin,\nal.,\n“Siam-gcan: A siamese\ngraph"
        },
        {
          "AUTHOR et al.: TITLE": "DA in an organic manner. By adding EvoFA as a plug-in to",
          "9": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "convolutional attention network for eeg emotion recognition,” IEEE"
        },
        {
          "AUTHOR et al.: TITLE": "various FSL frameworks based on online calibration modes,",
          "9": "Transactions on Instrumentation and Measurement, vol. 71, pp. 1–9,"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "2022."
        },
        {
          "AUTHOR et al.: TITLE": "it\nreduces\nthe domain gap between the\nsource domain data",
          "9": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "T. Pan, Y. Ye, H. Cai, S. Huang, Y. Yang, and G. Wang, “Multimodal"
        },
        {
          "AUTHOR et al.: TITLE": "and target domain data during testing,\nthereby enhancing the",
          "9": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "physiological signals fusion for online emotion recognition,” in Pro-"
        },
        {
          "AUTHOR et al.: TITLE": "model’s recognition ability. EvoFA, with its high compatibility",
          "9": "ceedings of\nthe 31st ACM International Conference on Multimedia,"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "2023, pp. 5879–5888."
        },
        {
          "AUTHOR et al.: TITLE": "and\nno\nneed\nfor\nretraining,\nbrings\nabout\nan\nadditional\nim-",
          "9": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "M. A. Blanco-R´ıos, M. O. Candela-Leal, C. Orozco-Romo,\net al.,"
        },
        {
          "AUTHOR et al.: TITLE": "provement of around 0.4% in testing structures across\nthree",
          "9": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "“Real-time eeg-based emotion recognition for neurohumanities: Per-"
        },
        {
          "AUTHOR et al.: TITLE": "FSL frameworks. Moving forward, we will continue to focus",
          "9": "spectives\nfrom principal\ncomponent\nanalysis\nand\ntree-based\nalgo-"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "rithms,” Frontiers in Human Neuroscience, vol. 18, 2024."
        },
        {
          "AUTHOR et al.: TITLE": "on rapid modal\nadaptation in EEG,\nadvancing the practical",
          "9": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "M. Long, Y. Cao,\nJ. Wang,\nand M.\nJordan,\n“Learning transferable"
        },
        {
          "AUTHOR et al.: TITLE": "application of deep models in the EEG domain.",
          "9": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "features with deep adaptation networks,” in International conference"
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "on machine learning, PMLR, 2015, pp. 97–105."
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "M.\nLong, H.\nZhu,\nJ. Wang,\nand M.\nI.\nJordan,\n“Deep\ntransfer"
        },
        {
          "AUTHOR et al.: TITLE": "REFERENCES",
          "9": ""
        },
        {
          "AUTHOR et al.: TITLE": "",
          "9": "learning with joint adaptation networks,” in International conference"
        },
        {
          "AUTHOR et al.: TITLE": "[1]\nW. Wang, K. Xu, H. Niu,\nand X. Miao,\n“Emotion recognition of",
          "9": "on machine learning, PMLR, 2017, pp. 2208–2217."
        },
        {
          "AUTHOR et al.: TITLE": "students based on facial expressions in online education based on the",
          "9": "H. Xia, H. Zhao,\nand Z. Ding,\n“Adaptive\nadversarial network for"
        },
        {
          "AUTHOR et al.: TITLE": "perspective of computer simulation,” Complexity, vol. 2020, pp. 1–9,",
          "9": "of\nthe\nIEEE/CVF\nsource-free\ndomain\nadaptation,”\nin Proceedings"
        },
        {
          "AUTHOR et al.: TITLE": "2020.",
          "9": "international conference on computer vision, 2021, pp. 9010–9019."
        },
        {
          "AUTHOR et al.: TITLE": "[2]\nN. Banskota, A. Alsadoon, P. Prasad, A. Dawoud, T. A. Rashid, and",
          "9": "Y\n. Ganin, E. Ustinova, H. Ajakan, et al., “Domain-adversarial training"
        },
        {
          "AUTHOR et al.: TITLE": "O. H. Alsadoon, “A novel enhanced convolution neural network with",
          "9": "of neural networks,” Journal of machine learning research, vol. 17,"
        },
        {
          "AUTHOR et al.: TITLE": "extreme learning machine: Facial emotional\nrecognition in psychol-",
          "9": "no. 59, pp. 1–35, 2016."
        },
        {
          "AUTHOR et al.: TITLE": "ogy practices,” Multimedia Tools and Applications, vol. 82, no. 5,",
          "9": "H. Liu,\nJ. Wang,\nand M. Long,\n“Cycle\nself-training\nfor\ndomain"
        },
        {
          "AUTHOR et al.: TITLE": "pp. 6479–6503, 2023.",
          "9": "in Neural\nInformation Processing\nadaptation,” Advances\nSystems,"
        },
        {
          "AUTHOR et al.: TITLE": "[3]\nM.\nJim´enez-Guarneros\nand G. Fuentes-Pineda,\n“Cross-subject\neeg-",
          "9": "vol. 34, pp. 22 968–22 981, 2021."
        },
        {
          "AUTHOR et al.: TITLE": "based\nemotion\nrecognition\nvia\nsemi-supervised multi-source\njoint",
          "9": "Y\n. Zou, Z. Yu, B. Kumar, and J. Wang, “Unsupervised domain adap-"
        },
        {
          "AUTHOR et al.: TITLE": "distribution adaptation,” IEEE Transactions on Instrumentation and",
          "9": "tation for semantic segmentation via class-balanced self-training,” in"
        },
        {
          "AUTHOR et al.: TITLE": "Measurement, 2023.",
          "9": "Proceedings of\nthe European conference on computer vision (ECCV),"
        },
        {
          "AUTHOR et al.: TITLE": "[4]\nQ. She, C. Zhang, F. Fang, Y. Ma,\nand Y. Zhang,\n“Multisource",
          "9": "2018, pp. 289–305."
        },
        {
          "AUTHOR et al.: TITLE": "associate domain adaptation for cross-subject and cross-session eeg",
          "9": "O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al., “Matching"
        },
        {
          "AUTHOR et al.: TITLE": "IEEE Transactions\non\nInstrumentation\nand\nemotion\nrecognition,”",
          "9": "in\nneural\ninformation\nnetworks\nfor\none\nshot\nlearning,” Advances"
        },
        {
          "AUTHOR et al.: TITLE": "Measurement, 2023.",
          "9": "processing systems, vol. 29, 2016."
        },
        {
          "AUTHOR et al.: TITLE": "et\n[5]\nC. Li, P. Li, Y. Zhang,\nal.,\n“Effective\nemotion\nrecognition\nby",
          "9": "J. Snell, K. Swersky, and R. Zemel, “Prototypical networks for\nfew-"
        },
        {
          "AUTHOR et al.: TITLE": "learning discriminative graph topologies in eeg brain networks,” IEEE",
          "9": "in neural\nshot\nlearning,” Advances\ninformation processing systems,"
        },
        {
          "AUTHOR et al.: TITLE": "Transactions on Neural Networks and Learning Systems, 2023.",
          "9": "vol. 30, 2017."
        },
        {
          "AUTHOR et al.: TITLE": "[6]\nL. Zhang, D. Xiao, X. Guo, F. Li, W. Liang, and B. Zhou, “Cross-",
          "9": "F.\nSung, Y. Yang, L. Zhang, T. Xiang,\nP. H. Torr,\nand T. M."
        },
        {
          "AUTHOR et al.: TITLE": "subject\nemotion eeg signal\nrecognition based on source microstate",
          "9": "Hospedales,\n“Learning to compare: Relation network for\nfew-shot"
        },
        {
          "AUTHOR et al.: TITLE": "analysis,” Frontiers in Neuroscience, vol. 17, p. 1 288 580, 2023.",
          "9": "the IEEE conference on computer vision\nlearning,” in Proceedings of"
        },
        {
          "AUTHOR et al.: TITLE": "[7]\nX. Gong, C. P. Chen, B. Hu, and T. Zhang, “Ciabl: Completeness-",
          "9": "and pattern recognition, 2018, pp. 1199–1208."
        },
        {
          "AUTHOR et al.: TITLE": "induced adaptative broad learning for\ncross-subject\nemotion recog-",
          "9": "C. Finn, P. Abbeel,\nand S. Levine,\n“Model-agnostic meta-learning"
        },
        {
          "AUTHOR et al.: TITLE": "nition with eeg and eye movement\nsignals,” IEEE Transactions on",
          "9": "for fast adaptation of deep networks,” in International conference on"
        },
        {
          "AUTHOR et al.: TITLE": "Affective Computing, 2024.",
          "9": "machine learning, PMLR, 2017, pp. 1126–1135."
        },
        {
          "AUTHOR et al.: TITLE": "[8]\nT. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition",
          "9": "A. Nichol and J. Schulman, “Reptile: A scalable metalearning algo-"
        },
        {
          "AUTHOR et al.: TITLE": "using dynamical graph convolutional neural networks,” IEEE Trans-",
          "9": "rithm,” arXiv preprint arXiv:1803.02999, vol. 2, no. 3, p. 4, 2018."
        },
        {
          "AUTHOR et al.: TITLE": "actions on Affective Computing, vol. 11, no. 3, pp. 532–541, 2018.",
          "9": "C. Finn, K. Xu, and S. Levine, “Probabilistic model-agnostic meta-"
        },
        {
          "AUTHOR et al.: TITLE": "[9]\nM.\nJin and J. Li, “Graph to grid: Learning deep representations\nfor",
          "9": "learning,” Advances in neural information processing systems, vol. 31,"
        },
        {
          "AUTHOR et al.: TITLE": "the 31st ACM\nmultimodal\nemotion recognition,”\nin Proceedings of",
          "9": "2018."
        },
        {
          "AUTHOR et al.: TITLE": "International Conference on Multimedia, 2023, pp. 5985–5993.",
          "9": "W.-L. Zheng and B.-L. Lu,\n“Investigating critical\nfrequency bands"
        },
        {
          "AUTHOR et al.: TITLE": "et\n[10]\nZ. Li, E. Zhu, M.\nJin,\nal.,\n“Dynamic\ndomain\nadaptation\nfor",
          "9": "and\nchannels\nfor\neeg-based\nemotion\nrecognition with\ndeep\nneural"
        },
        {
          "AUTHOR et al.: TITLE": "class-aware cross-subject and cross-session eeg emotion recognition,”",
          "9": "networks,”\nIEEE Transactions on autonomous mental development,"
        },
        {
          "AUTHOR et al.: TITLE": "IEEE Journal of Biomedical and Health Informatics, vol. 26, no. 12,",
          "9": "vol. 7, no. 3, pp. 162–175, 2015."
        },
        {
          "AUTHOR et al.: TITLE": "pp. 5964–5973, 2022.",
          "9": "W. Liu, J.-L. Qiu, W.-L. Zheng, and B.-L. Lu, “Comparing recognition"
        },
        {
          "AUTHOR et al.: TITLE": "[11]\nS. An,\nS. Kim,\nP. Chikontwe,\nand\nS. H.\nPark,\n“Dual\nattention",
          "9": "performance and robustness of multimodal deep learning models for"
        },
        {
          "AUTHOR et al.: TITLE": "relation\nnetwork with fine-tuning\nfor\nfew-shot\neeg motor\nimagery",
          "9": "IEEE Transactions on Cognitive\nmultimodal\nemotion recognition,”"
        },
        {
          "AUTHOR et al.: TITLE": "classification,” IEEE Transactions on Neural Networks and Learning",
          "9": "and Developmental Systems, vol. 14, no. 2, pp. 715–729, 2021."
        },
        {
          "AUTHOR et al.: TITLE": "Systems, 2023.",
          "9": "G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality"
        },
        {
          "AUTHOR et al.: TITLE": "[12]\nR. Ning, C. P. Chen, and T. Zhang, “Cross-subject eeg emotion recog-",
          "9": "of data with neural networks,” science, vol. 313, no. 5786, pp. 504–"
        },
        {
          "AUTHOR et al.: TITLE": "nition using domain adaptive few-shot\nlearning networks,” in 2021",
          "9": "507, 2006."
        },
        {
          "AUTHOR et al.: TITLE": "IEEE International Conference on Bioinformatics and Biomedicine",
          "9": "T. N. Kipf and M. Welling, “Semi-supervised classification with graph"
        },
        {
          "AUTHOR et al.: TITLE": "(BIBM),\nIEEE, 2021, pp. 1468–1472.",
          "9": "convolutional networks,” arXiv preprint arXiv:1609.02907, 2016."
        },
        {
          "AUTHOR et al.: TITLE": "[13]\nL. Zhu, Y. Liu, R. Liu, et al., “Decoding multi-brain motor\nimagery",
          "9": "J. Li,\nS. Qiu, Y.-Y.\nShen, C.-L. Liu,\nand H. He,\n“Multisource"
        },
        {
          "AUTHOR et al.: TITLE": "from eeg using coupling feature\nextraction and few-shot\nlearning,”",
          "9": "IEEE\ntransfer\nlearning for\ncross-subject\neeg emotion recognition,”"
        },
        {
          "AUTHOR et al.: TITLE": "IEEE Transactions on Neural Systems and Rehabilitation Engineer-",
          "9": "transactions on cybernetics, vol. 50, no. 7, pp. 3281–3293, 2019."
        },
        {
          "AUTHOR et al.: TITLE": "ing, vol. 31, pp. 4683–4692, 2023.",
          "9": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition of students based on facial expressions in online education based on the perspective of computer simulation",
      "authors": [
        "W Wang",
        "K Xu",
        "H Niu",
        "X Miao"
      ],
      "year": "2020",
      "venue": "Complexity"
    },
    {
      "citation_id": "2",
      "title": "A novel enhanced convolution neural network with extreme learning machine: Facial emotional recognition in psychology practices",
      "authors": [
        "N Banskota",
        "A Alsadoon",
        "P Prasad",
        "A Dawoud",
        "T Rashid",
        "O Alsadoon"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "3",
      "title": "Cross-subject eegbased emotion recognition via semi-supervised multi-source joint distribution adaptation",
      "authors": [
        "M Jiménez-Guarneros",
        "G Fuentes-Pineda"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "4",
      "title": "Multisource associate domain adaptation for cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "Q She",
        "C Zhang",
        "F Fang",
        "Y Ma",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "5",
      "title": "Effective emotion recognition by learning discriminative graph topologies in eeg brain networks",
      "authors": [
        "C Li",
        "P Li",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "6",
      "title": "Crosssubject emotion eeg signal recognition based on source microstate analysis",
      "authors": [
        "L Zhang",
        "D Xiao",
        "X Guo",
        "F Li",
        "W Liang",
        "B Zhou"
      ],
      "year": "2023",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "7",
      "title": "Ciabl: Completenessinduced adaptative broad learning for cross-subject emotion recognition with eeg and eye movement signals",
      "authors": [
        "X Gong",
        "C Chen",
        "B Hu",
        "T Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Graph to grid: Learning deep representations for multimodal emotion recognition",
      "authors": [
        "M Jin",
        "J Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "Dynamic domain adaptation for class-aware cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "Z Li",
        "E Zhu",
        "M Jin"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "11",
      "title": "Dual attention relation network with fine-tuning for few-shot eeg motor imagery classification",
      "authors": [
        "S An",
        "S Kim",
        "P Chikontwe",
        "S Park"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "12",
      "title": "Cross-subject eeg emotion recognition using domain adaptive few-shot learning networks",
      "authors": [
        "R Ning",
        "C Chen",
        "T Zhang"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "13",
      "title": "Decoding multi-brain motor imagery from eeg using coupling feature extraction and few-shot learning",
      "authors": [
        "L Zhu",
        "Y Liu",
        "R Liu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition in eeg signals using deep learning methods: A review",
      "authors": [
        "M Jafari",
        "A Shoeibi",
        "M Khodatars"
      ],
      "year": "2023",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "15",
      "title": "A multi-dimensional graph convolution network for eeg emotion recognition",
      "authors": [
        "G Du",
        "J Su",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "16",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Siam-gcan: A siamese graph convolutional attention network for eeg emotion recognition",
      "authors": [
        "H Zeng",
        "Q Wu",
        "Y Jin"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "18",
      "title": "Multimodal physiological signals fusion for online emotion recognition",
      "authors": [
        "T Pan",
        "Y Ye",
        "H Cai",
        "S Huang",
        "Y Yang",
        "G Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Real-time eeg-based emotion recognition for neurohumanities: Perspectives from principal component analysis and tree-based algorithms",
      "authors": [
        "M Blanco-Ríos",
        "M Candela-Leal",
        "C Orozco-Romo"
      ],
      "year": "2024",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "20",
      "title": "Learning transferable features with deep adaptation networks",
      "authors": [
        "M Long",
        "Y Cao",
        "J Wang",
        "M Jordan"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "21",
      "title": "Deep transfer learning with joint adaptation networks",
      "authors": [
        "M Long",
        "H Zhu",
        "J Wang",
        "M Jordan"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "22",
      "title": "Adaptive adversarial network for source-free domain adaptation",
      "authors": [
        "H Xia",
        "H Zhao",
        "Z Ding"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "23",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan"
      ],
      "year": "2016",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "24",
      "title": "Cycle self-training for domain adaptation",
      "authors": [
        "H Liu",
        "J Wang",
        "M Long"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "25",
      "title": "Unsupervised domain adaptation for semantic segmentation via class-balanced self-training",
      "authors": [
        "Y Zou",
        "Z Yu",
        "B Kumar",
        "J Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "26",
      "title": "Matching networks for one shot learning",
      "authors": [
        "O Vinyals",
        "C Blundell",
        "T Lillicrap",
        "D Wierstra"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "27",
      "title": "Prototypical networks for fewshot learning",
      "authors": [
        "J Snell",
        "K Swersky",
        "R Zemel"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "28",
      "title": "Learning to compare: Relation network for few-shot learning",
      "authors": [
        "F Sung",
        "Y Yang",
        "L Zhang",
        "T Xiang",
        "P Torr",
        "T Hospedales"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "29",
      "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "authors": [
        "C Finn",
        "P Abbeel",
        "S Levine"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "30",
      "title": "Reptile: A scalable metalearning algorithm",
      "authors": [
        "A Nichol",
        "J Schulman"
      ],
      "year": "2018",
      "venue": "Reptile: A scalable metalearning algorithm",
      "arxiv": "arXiv:1803.02999"
    },
    {
      "citation_id": "31",
      "title": "Probabilistic model-agnostic metalearning",
      "authors": [
        "C Finn",
        "K Xu",
        "S Levine"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "32",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "33",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "34",
      "title": "Reducing the dimensionality of data with neural networks",
      "authors": [
        "G Hinton",
        "R Salakhutdinov"
      ],
      "year": "2006",
      "venue": "science"
    },
    {
      "citation_id": "35",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2016",
      "venue": "Semi-supervised classification with graph convolutional networks",
      "arxiv": "arXiv:1609.02907"
    },
    {
      "citation_id": "36",
      "title": "Multisource transfer learning for cross-subject eeg emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    }
  ]
}