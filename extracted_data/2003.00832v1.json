{
  "paper_id": "2003.00832v1",
  "title": "An End-To-End Visual-Audio Attention Network For Emotion Recognition In User-Generated Videos",
  "published": "2020-02-12T15:33:59Z",
  "authors": [
    "Sicheng Zhao",
    "Yunsheng Ma",
    "Yang Gu",
    "Jufeng Yang",
    "Tengfei Xing",
    "Pengfei Xu",
    "Runbo Hu",
    "Hua Chai",
    "Kurt Keutzer"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in user-generated videos plays an important role in human-centered computing. Existing methods mainly employ traditional two-stage shallow pipeline, i.e. extracting visual and/or audio features and training classifiers. In this paper, we propose to recognize video emotions in an end-to-end manner based on convolutional neural networks (CNNs). Specifically, we develop a deep Visual-Audio Attention Network (VAANet), a novel architecture that integrates spatial, channel-wise, and temporal attentions into a visual 3D CNN and temporal attentions into an audio 2D CNN. Further, we design a special classification loss, i.e. polarity-consistent cross-entropy loss, based on the polarity-emotion hierarchy constraint to guide the attention generation. Extensive experiments conducted on the challenging VideoEmotion-8 and Ekman-6 datasets demonstrate that the proposed VAANet outperforms the state-of-the-art approaches for video emotion recognition. Our source code is released at: https://github.com/maysonma/VAANet.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The convenience of mobile devices and social networks has enabled users to generate videos and upload to Internet in daily life to share their experiences and express personal opinions. As a result, an explosive growing volume of videos are being created, which results in urgent demand for the analysis and management of these videos. Besides the objective content recognition, such as objects and actions  (Zhu et al. 2018; Choutas et al. 2018) , understanding the emotional impact of the videos plays an important role in humancentered computing. On the one hand, the videos can, to a large extent, reflect the psychological states of the video generators. We can predict the generators' possible extreme behaviors, such as depression and suicide, and take corresponding preventive actions. On the other hand, the videos that evoke strong emotions can easily resonate with viewers and bring them immersive watching experiences. Appropriate emotional resonation is crucial in intelligent advertising and video recommendation. Further, emotion recognition in Figure  1 : Illustration of the keyframes and discriminative regions for emotion recognition in user-generated videos. Although the story in a video may contain different stages, the emotion is mainly evoked by some keyframes (as shown by the temporal attentions in the color bar) and corresponding discriminative regions (as illustrated by the spatial attentions in the heat map). user-generated videos (UGVs) can help companies analyze how customers evaluate their products and assist governments to manage the Internet.\n\nAlthough with the advent of deep learning, remarkable progress has been made on text sentiment classification  (Zhang, Wang, and Liu 2018) , image emotion analysis  (Zhao et al. 2018a; 2018b; Yang et al. 2018a) , and video semantic understanding  (Zhu et al. 2018; Choutas et al. 2018) . Emotion recognition in UGVs still remains an unsolved problem, due to the following challenges. (1) Large intra-class variation. Videos captured in quite different scenarios may evoke similar emotions. For example, visiting an amusement park, taking part in sport competition, and playing video games may all make viewers feel \"excited\". This results in obvious \"affective gap\" between low-level features and high-level emotions. (2) Low structured consistency. Unlike professional and commercial videos, such as movies  (Wang and Cheong 2006)  and GIFs  (Jou, Bhattacharya, and Chang 2014; Yang, Zhang, and Luo 2019) , UGVs are usually taken with diverse structures, e.g. various resolutions and image blurring noises. (3) Sparse keyframe expression. Only limited keyframes directly convey and determine emotions, as shown in Figure  1 , while the rest are used to introduce the background and context.\n\nMost existing approaches on emotion recognition in UGVs focus on the first challenge, i.e. employing advanced image representations to bridge the affective gap, such as (1) mid-level attribute features  (Jiang, Xu, and Xue 2014; Tu et al. 2019 ) like ObjectBank  (Li et al. 2010 ) and Sen-tiBank  (Borth et al. 2013) , (2) high-level semantic features  (Chen, Wu, and Jiang 2016 ) like detected events  (Jiang et al. 2017; Caba Heilbron et al. 2015) , objects  (Deng et al. 2009) , and scenes  (Zhou et al. 2014) , and (3) deep convolutional neural network (CNN) features  (Xu et al. 2018; Zhang and Xu 2018) .  Zhang and Xu (2018)  transformed frame-level spatial features to another kernelized feature space via discrete Fourier transform, which partially addresses the second challenge. For the third challenge, the videos are either downsampled averagely to a fixed number of frames  (Zhang and Xu 2018) , or represented by continuous frames from only one segment  (Tu et al. 2019) .\n\nThe above methods have contributed to the development of emotion recognition in UGVs, but they still have some problems. (1) They mainly employ a two-stage shallow pipeline, i.e. extracting visual and/or audio features and training classifiers. (2) The visual CNN features of each frame are separately extracted, which ignore the temporal correlation of adjacent frames. (3) The fact that emotions may be determined by keyframes from several discrete segments is neglected. (4) Some methods require auxiliary data, which is not always available in real applications. For example, the extracted event, object, and scene features in  (Chen, Wu, and Jiang 2016)  are trained on FCVID  (Jiang et al. 2017)  and ActivityNet  (Caba Heilbron et al. 2015) , Ima-geNet  (Deng et al. 2009), and Places205 (Zhou et al. 2014 ) datasets, respectively. (5) They do not consider the correlations of different emotions, such as the polarity-emotion hierarchy constraint, i.e. the relation of two different emotions belonging to the same polarity is closer than those from opposite polarities.\n\nIn this paper, we propose an end-to-end Visual-Audio Attention Network, termed VAANet, to address the above problems for recognizing the emotions in UGVs, without requiring any auxiliary data except the data for pre-training. First, we spit each video into an equal number of segments. Second, for each segment, we randomly select some successive frames and feed them into a 3D CNN  (Hara, Kataoka, and Satoh 2018)  with both spatial and channel-wise attentions to extract visual features. Meanwhile, we transform the corresponding audio waves into spectrograms and feed them into a 2D CNN  (He et al. 2016)  to extract audio features. Finally, the visual and audio features of different segments are weighted by temporal attentions to obtain the whole video's feature representation, which is followed by a fully connected layer to obtain emotion predictions. Considering the polarity-emotion hierarchy constraint, we design a novel classification loss, i.e. polarity-consistent crossentropy (PCCE) loss, to guide the attention generation.\n\nIn summary, the contributions of this paper are threefold:\n\n1. We are the first to study the emotion recognition task in user-generated videos in an end-to-end manner.\n\n2. We develop a novel network architecture, i.e. VAANet, that integrates spatial, channel-wise, and temporal attentions into a visual 3D CNN and temporal attentions into an audio 2D CNN for video emotion recognition. We propose a novel PCCE loss, which enables VAANet to generate polarity preserved attention map.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Video Emotion Recognition: Psychologists usually employ two kinds of models to represent emotions: categorical emotion states (CES) and dimensional emotions space (DES). CES classify emotions into several basic categories, such as Ekman's 6 basic categories  (Ekman 1992 ) and Plutchik's wheel of emotions  (Plutchik and Kellerman 1980) . DES usually employ a Cartesian space to represent emotions, such as valence-arousal-dominance  (Schlosberg 1954) . Since CES are easy for users to understand and label, here we adopt CES to represent emotions in videos.\n\nEarly research on video emotion recognition mainly focused on movies, which are well structured.  Kang (2003)  employed a Hidden Markov Model to detect affective event based on low-level features, including color, motion, and shot cut rate. Joint combination of visual and audio features with support vector machine  (Wang and Cheong 2006)  and conditional random fields  (Xu et al. 2013 ) achieves promising result. Some recent methods work on Animated GIFs  (Jou, Bhattacharya, and Chang 2014; Chen and Picard 2016; Yang, Zhang, and Luo 2019) .  Jou, Bhattacharya, and Chang (2014)  firstly proposed to recognize GIF emotions by using features of different types.  Chen and Picard (2016)  improved the performance by adopting 3D ConvNets to extract spatiotemporal features. Human-centered GIF emotion recognition is conducted by considering human related information and visual attention  (Yang, Zhang, and Luo 2019) .\n\nBecause of the content diversity and low quality, UGVs are more challenging to recognize emotions.  Jiang, Xu, and Xue (2014)  investigated a large set of low-level visual-audio features and mid-level attributes, e.g. ObjectBank  (Li et al. 2010)  and SentiBank  (Borth et al. 2013) .  Chen, Wu, and Jiang (2016)  extracted various high-level semantic features based on existing detectors. Compared with hand-crafted features, deep features are more widely used  (Xu et al. 2018; Zhang and Xu 2018) . By combining low-level visual-audiotextual features,  Pang, Zhu, and Ngo (2015)  showed that learned joint representations are complementary to handcrafted features. Different from these methods, which employ a two-stage shallow pipeline, we propose the first endto-end method to recognize emotions in UGVs by extracting attended visual and audio CNN features.\n\nPlease note that emotion recognition has also been widely studied in other modalities, such as text  (Zhang, Wang, and Liu 2018) , images  (Zhao et al. 2017; Yang et al. 2018b; Zhao et al. 2018a; Yang et al. 2018a; Zhao et al. 2018c; 2019c; 2019b; Yao et al. 2019; Zhan et al. 2019) , speech (El Ayadi, Kamel, and Karray 2011), physiological signals  (Alarcao and Fonseca 2017; Zhao et al. 2019a) , and multi-modal data  (Soleymani et al. 2017; Zhao et al. 2019d ).\n\nAttention-Based Models: Since attention can be considered as a dynamic feature extraction mechanism that combines contextual fixations over time  (Mnih et al. 2014; Chen et al. 2017) , it has been seamlessly incorporated into deep learning architectures and achieved outstanding performances in many vision-related tasks, such as image classification  (Woo et al. 2018) , image captioning  (You et al. 2016; Chen et al. 2017; 2018) , and action recognition  (Song et al. 2017 ). These attention methods can be roughly divided into four categories: spatial attention  (Song et al. 2017; Woo et al. 2018) , semantic attention  (You et al. 2016) , channel-wise attention  (Chen et al. 2017; Woo et al. 2018) , and temporal attention  (Song et al. 2017) .\n\nThere are also several methods that employ attention for emotion recognition in images  (You, Jin, and Luo 2017; Yang et al. 2018a; Zhao et al. 2019b ) and speech (Mirsamadi, Barsoum, and Zhang 2017). The former methods mainly consider spatial attention except PDANet  (Zhao et al. 2019b ) which also employs channel-wise attention, while the latter one only uses temporal attention. To the best of our knowledge, attention has not been studied on emotion recognition in user-generated videos. In this paper, we systematically investigate the influence of different attentions in video emotion recognition, including the importance of local spatial context by spatial attention, the interdependency between different channels by channel-wise attention, and the importance of different segments by temporal attention.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Visual-Audio Attention Network",
      "text": "We propose a novel CNN architecture with spatial, channelwise, and temporal attention mechanisms for emotion recognition in user generated videos. Figure  2  shows the overall framework of the proposed VAANet. Specifically, VAANet has two streams to respectively exploit the visual and audio information. The visual stream consists of three attention modules and the audio stream contains a temporal attention module. The spatial attention and the channel-wise attention sub-networks in the visual stream are designed to automatically focus on the regions and channels that carry discriminative information within each feature map. The temporal attention sub-networks in both the visual and audio streams are designed to assign weights to different segments of a video. The training of VAANet is performed by minimizing the newly designed polarity-consistent cross-entropy loss in an end-to-end manner.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Visual Representation Extraction",
      "text": "To extract visual representations from a long-term video, following  (Wang et al. 2016) , the visual stream of our model works on short snippets sparsely sampled from the entire video. Specifically, we divide each video into t segments with equal duration, and then randomly sample a short snippet of k successive frames from each segment. We use 3D ResNet-101  (Hara, Kataoka, and Satoh 2018)  as backbone of the visual stream. It takes the t snippets (each has k successive frames) as input and independently processes them up to the last spatiotemporal convolutional layer conv5 into a super-frame. Suppose we are given N training samples\n\n, where x V l is the visual information of video l, and y l is the corresponding emotion label. For sample x V l , suppose the feature map of the conv5 in 3D ResNet-101 is F V l ∈ R t×h×w×n (we omit l for simplicity in the following), where h and w are the spatial size (height and width) of the feature map, n is the number of channels, and t is the number of snippets. We reshape F V as\n\nby flattening the height and width of the original F V , where\n\nHere we can consider f V ij as the visual feature of the j-th location in the i-th super-frame. In the following, we omit the superscript V for simplicity.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Visual Spatial Attention Estimation",
      "text": "We employ a spatial attention module to automatically explore the different contributions of the regions in superframes to predict the emotions. Following  (Chen et al. 2017) , we employ a two-layer neural network, i.e. a 1 × 1 convolutional layer followed by a fully-connected layer with a softmax function to generate the spatial attention distributions over all the super-frame regions. That is, for each\n\nwhere W S1 ∈ R m×m and W S2 ∈ R 1×n are two learnable parameter matrices, is the transpose of a matrix, and\n\nAnd then we can obtain a weighted feature map based on spatial attention as follows\n\n) where ⊗ is the multiplication of a matrix and a vector, which is performed by multiplying each value in the vector to each column of the matrix.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Visual Channel-Wise Attention Estimation",
      "text": "Assuming that each channel of a feature map in a CNN is a response activation of the corresponding convolutional layer, channel-wise attention can be viewed as a process of selecting semantic attributes  (Chen et al. 2017) . To generate the channel-wise attention, we first transpose\n\nwhere g ij ∈ R m represents the j-th channel in the i-th super-frame of the feature map G. The channel-wise attention for\n\n) is defined as\n\nwhere W C1 ∈ R n×n and W C2 ∈ R 1×m are two learnable parameter matrices, and A C i ∈ R n×1 . And then a weighted feature map based on channel-wise attention is computed as follows\n\nwhere ⊗ is the multiplication of a matrix and a vector.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Visual Temporal Attention Estimation",
      "text": "For a video, the discriminability of each frame to recognize emotions is obviously different. Only some keyframes contain discriminative information, while the others only provide the background and context information  (Song et al. 2017) . Based on such observations, we design a temporal attention sub-network to automatically focus on the important segments that contain keyframes. To generate the temporal attention, we first apply spatial average pooling to G C and reshape it to\n\nwhere\n\nHere we can consider p j as the visual feature of the j-th super-frame. The temporal attention is then defined as\n\nwhere W T1 ∈ R t×t and W T2 ∈ R 1×n are two learnable parameter matrices, and A T ∈ R t×1 . Following  (Song et al. 2017) , we use ReLU (Rectified Linear Units) as the activation function here for its better convergence performance.\n\nThe final visual embedding is the weighted sum of all the super-frames",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Audio Representation Extraction",
      "text": "Audio features are complementary to visual features, because they contain information of another modality. In our problem, we choose to use the most well-known audio representation: the mel-frequency cepstral coefficients (MFCC). Suppose we are given N audio training samples {(x A l , y l )} N l=1 , where x A l is a descriptor from the entire soundtrack of the video V l and y l is the corresponding emotion label. We center-crop x A l to a fixed length of q to get x A l , and pad itself when it is necessary. Similar to the method we take in extracting visual representation, we divide each descriptor into t segments and use 2D ResNet-18  (He et al. 2016)  as backbone of the audio stream of our model which processes descriptor segments independently. For descriptor x A l , suppose the feature map of the conv5 in 2D ResNet-18 is F A l ∈ R t×h ×w ×n (we omit l for simplicity in the following), where h and w are the height and width of the feature map, n is the number of channels, and t is the number of segments. We apply spatial average pooling to F A and obtain F A ∈ R t×n .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Audio Temporal Attention Estimation",
      "text": "With similar motivation to integrate temporal attention subnetwork into the visual stream, we introduce a temporal attention sub-network to explore the influence of audio information in different segments for recognizing emotions as\n\nwhere W A1 ∈ R t×t and W A2 ∈ R 1×n are two learnable parameter matrices, and A A ∈ R t×1 . The final audio embedding is the weighted sum of all the segments",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Polarity-Consistent Cross-Entropy Loss",
      "text": "We concatenate E V and E A to obtain an aggregated semantic vector E = [E V , E A ] , which can be viewed as the final representation of a video and is fed into a fully connected layer to predict the emotion labels. The traditional crossentropy loss is defined as\n\nwhere C is the number of emotion classes (C = 8 for VideoEmotion-8 and C = 6 for Ekman-6 in this paper), 1 [c=yi] is a binary indicator, and p i,c is the predicted probability that video i belongs to class c. Directly optimizing the cross-entropy loss in Eq. (  12 ) can lead some videos to be incorrectly classified into categories that have opposite polarity. In this paper, we design a novel polarity-consistent cross-entorpy (PCCE) loss to guide the attention generation. That is, the penalty of the predictions that have opposite polarity to the ground truth is increased. The PCCE loss is defined as\n\nwhere λ is a penalty coefficient that controls the penalty extent. Similar to the indicator function, g(.,.) represents whether to add the penalty or not and is defined as\n\nwhere polarity(.) is a function that maps an emotion category to its polarity (positive or negative). Since the derivatives with respect to all parameters can be computed, we can train the proposed VAANet effectively in an end-to-end manner using off-the-shelf optimizer to minimize the loss function in Eq. (  13 ).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we evaluate the proposed VAANet model on emotion recognition in user-generated videos. We first introduce the employed benchmarks, compared baselines, and implementation details. And then we report and analyze the major results together with some empirical analysis.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Settings",
      "text": "Benchmarks We evaluate the performances of the proposed method on two publicly available datasets that contain emotion labels in user-generated videos: VideoEmotion-8  (Jiang, Xu, and Xue 2014)  and  Ekman-6 (Xu et al. 2018) .\n\nVideoEmotion-8 (Jiang, Xu, and Xue 2014) (VE-8) consists of 1,101 videos collected from Youtube and Flickr with average duration 107 seconds. The videos are labeled into one of the Plutchik's eight basic categories  (Plutchik and Kellerman 1980) : negative anger, disgust, fear, sadness and positive anticipation, joy, surprise, trust. In each category, there are at least 100 videos.  Ekman-6 (Xu et al. 2018 ) (E-6) contains 1,637 videos also collected from Youtube and Flickr. The average duration is 112 seconds. The videos are labeled with Ekman's six emotion categories  (Ekman 1992) , i.e. negative anger, disgust, fear, sadness and positive joy, surprise.\n\nBaselines To compare VAANet with the state-of-the-art approaches for video emotion recognition, we select the following methods as baselines: (1) SentiBank  (Borth et al. 2013 ), (2) Enhanced Multimodal Deep Bolzmann Machine (E-MDBM)  (Pang, Zhu, and Ngo 2015) , (3) Image Transfer Encoding (ITE)  (Xu et al. 2018 ), (4) Vi-sual+Audio+Attribute (V.+Au.+At.) (Jiang, Xu, and Xue 2014), (  5 ) Context Fusion Net (CFN) (Chen, Wu, and Jiang 2016), (6) V.+Au.+At.+E-MDBM  (Pang, Zhu, and Ngo 2015) , (  7 ) Kernelized and Kernelized+SentiBank  (Zhang and Xu 2018) .\n\nImplementation Details Following  (Jiang, Xu, and Xue 2014; Zhang and Xu 2018) , the experiments on VE-8 are conducted 10 runs. In each run, we randomly select 2/3 of the data from each category for training and the rest for testing. We report the average classification accuracy of the 10 runs. For E-6, we employ the split provided by the dataset, i.e. 819 videos for training and 818 for testing. The classification accuracy on the test set is evaluated. Our model is based on two state-of-the-art CNN architectures: 2D ResNet-18  (He et al. 2016 ) and 3D ResNet-101  (Hara, Kataoka, and Satoh 2018) , which are initialized with the weights pre-trained on ImageNet  (Deng et al. 2009 ) and Kinetics  (Carreira and Zisserman 2017) , respectively. In addition, for the visual stream, we divide the input video into 10 segments and sample 16 successive frames from each of them. We resize each frame of the visual sample and make the short side length of the sample equal to 112 pixels, and then apply random horizontal flips and crop a random 112 x 112 patch as data augmentation to reduce overfitting. In our training, Adam (Kingma and Ba 2014) is adopted to automatically adjust the learning rate during optimization, with the initial learning rate set to 0.0002 and the model is trained with batch-size 32 for 150 epochs. Our model is implemented using PyTorch.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With The State-Of-The-Art",
      "text": "The extracted features, training strategies, and average performance comparisons between the proposed VAANet and the state-of-the-art approaches are shown in Tables  1  and 2  Table  1 : Comparison between the proposed VAANet and several state-of-the-art methods on the VE-8 dataset, where 'Visual', 'Audio', and 'Attribute' indicate whether corresponding features are used, 'Auxiliary' means whether no auxiliary data is used except the commonly used ImageNet  (Deng et al. 2009)  and Kinetics  (Kay et al. 2017 ) for pre-training, and 'End-to-end' indicates whether the corresponding algorithm is trained in an end-to-end manner. The best method is emphasized in bold. Our method achieves the best results, outperforming the state-of-the-art approaches.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Method",
      "text": "Visual Audio Attribute Auxiliary End-to-end Accuracy SentiBank  (Borth et al. 2013)  35.5 E-MDBM  (Pang, Zhu, and Ngo 2015)  40.4 ITE  (Xu et al. 2018)  44.7 V.+Au.+At.  (Jiang, Xu, and Xue 2014)  46.1 CFN  (Chen, Wu, and Jiang 2016)  50.4 V.+Au.+At.+E-MDBM  (Pang, Zhu, and Ngo 2015)  51.1 Kernelized  (Zhang and Xu 2018)  49.7 Kernelized+SentiBank  (Zhang and Xu 2018)  52.5 VAANet (Ours) 54.5\n\nTable  2 : Comparison between the proposed VAANet and several state-of-the-art methods on the E-6 dataset. The best method is emphasized in bold. Our method performs better than the state-of-theart approaches.\n\nMethod Accuracy ITE  (Xu et al. 2018)  51.2 CFN  (Chen, Wu, and Jiang 2016)  51.8 Kernelized  (Zhang and Xu 2018)  54.4 VAANet (Ours)\n\n55.3\n\non VE-8 and E-6 datsets, respectively. From the results, we have the following observations:\n\n(1) All these methods consider visual features, which is reasonable since the visual content in videos is the most direct way to evoke emotions. Further, existing methods all employ the traditional shallow learning pipeline, which indicates that the corresponding algorithms are trained step by step instead of end-to-end.\n\n(2) Most previous methods extract attribute features. It is demonstrated that attributes indeed contribute to the emotion recognition task  (Chen, Wu, and Jiang 2016) . However, this requires some auxiliary data to train attribute classifiers. For example, though highly related to emotions, the adjective noun pairs obtained by SentiBank are trained on the VSO dataset  (Borth et al. 2013) . Besides high computation cost, the auxiliary data to train such attribute classifiers are often not available in real applications.\n\n(3) Without extracting attribute features or requiring auxiliary data, the proposed VAANet is the only end-to-end model and achieves the best emotion recognition accuracy. Compared with the reported state-of-the-art results, i.e. Ker-nelized+SentiBank  (Zhang and Xu 2018)  on VE-8 and Kernelized  (Zhang and Xu 2018 ) on E-6, VAANet can respectively obtain 2% and 0.9% performance gains. The performance improvements benefit from the advantages of VAANet. First, the various attentions enable the network to focus on discriminative key segments, spatial context, and channel interdependency. Second, the novel PCCE loss considers the polarity-emotion hierarchy constraint, i.e. the emotion correlations, which can guide the detailed learning process. Third, the visual features extracted by 3D ResNet-101 can model the temporal correlation of the adjacent frames in a given segment.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Study",
      "text": "The proposed VAANet model contains two major components: a novel attention mechanism and a novel crossentropy loss. We conduct ablation study to further verify their effectiveness by changing one component and fixing the other. First, using polarity-consistent cross-entropy loss, we investigate the influence of different attentions, including visual spatial (VS), visual channel-wise (VCW), visual temporal (VT), and audio temporal (AT) ones. The emotion recognition accuracy of each emotion category and the average accuracy on VE-8 and E-6 datasets are shown in Table  3  and 4 , respectively. From the results, we can observe that: (1) visual attentions even only using spatial attention significantly outperform audio attentions (on average more than 10% improvement), which is understandable because in many videos the audio does not change much;\n\n(2) adding each one of them introduces performance gains, which demonstrates that all these attentions contribute to the video emotion recognition task; (3) though not performing well alone, combining audio features with visual features can boost the performance with about 1% accuracy gains.\n\nSecond, we evaluate the effectiveness of the proposed polarity-consistent cross-entropy loss (PCCE) by comparing with traditional cross-entropy loss (CE). Table  5  shows the results when visual attentions (VS+VCW+VT) and vi-sual+audio attentions (VS+VCW+VT+AT) are considered. From the results, it is clear that for both settings, PCCE performs better. The performance improvements of PCCE over CE for visual attentions and visual+audio attentions are 1.5%, 0.6% and 2.5%, 0.7% on the VE-8 and E-6 datasets, respectively. This demonstrates the effectiveness of emotion hierarchy as prior knowledge. This novel loss can also be easily extended to other machine learning tasks if some prior knowledge is available.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Visualization",
      "text": "In order to show the interpretability of our model, we use the heat map generated by the Gram-Cam algorithm  (Selvaraju et al. 2017)  to visualize the visual spatial attention obtained by the proposed VAANet. The visual temporal attention generated by our model is also illustrated through the color bar. As illustrated in Figure  3 , the well-trained VAANet can successfully pay more attention not only to the discriminative frames, but also to different salient regions in correspond-ing frames. For example, in the top left test case, the key object that makes people feel 'disgust' is a caterpillar, and a man is touching it with his finger. The model assigns the highest temporal attention when the finger is removed, and the caterpillar is completely exposed to the camera. In the bottom left case, our model can focus on the person and the dog during the whole video. Further, when the dog rushes out from the bottom right corner and makes the audience feel 'anticipated', the temporal attention becomes larger. In the middle bottom case, our model pays more attention when the 'surprise' comes up.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have proposed an effective emotion recognition method in user-generated videos based on visual and audio attentions. The developed novel VAANet model consists of a novel attention mechanism and a novel crossentropy loss, with less auxiliary data used. By considering various attentions, VAANet can better focus on the discriminative key segments and their key regions. The polarity-consistent cross-entropy loss can guide the attention generation. The extensive experiments conducted on VideoEmotion-8 and Ekman-6 benchmarks demonstrate that VAANet achieves 2.0% and 0.9% performance improvements as compared to the best state-of-the-art video emotion recognition approach. In future studies, we plan to extend the VAANet model to both fine-tuned emotion classification and emotion regression tasks. We also aim to investigate attentions that can better concentrate on the key frames in each video segment.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the keyframes and discriminative regions",
      "page": 1
    },
    {
      "caption": "Figure 1: , while the rest are",
      "page": 1
    },
    {
      "caption": "Figure 2: The framework of the proposed Visual and Audio Attention Network (VAANet). First, the MFCC descriptor from the soundtrack",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the overall",
      "page": 3
    },
    {
      "caption": "Figure 3: Visualization of the learned visual spatial attention and visual temporal attention. In both learned color bar and attention maps, red",
      "page": 7
    },
    {
      "caption": "Figure 3: , the well-trained VAANet can suc-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "approaches for video emotion recognition. Our source code": "is released at: https://github.com/maysonma/VAANet."
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "Introduction"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "The convenience of mobile devices and social networks has"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "enabled users to generate videos and upload to Internet\nin"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "daily life to share their experiences and express personal"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "opinions. As a result, an explosive growing volume of videos"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "are being created, which results in urgent demand for\nthe"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "analysis and management of\nthese videos. Besides the ob-"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "jective content recognition, such as objects and actions (Zhu"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "et al. 2018; Choutas et al. 2018), understanding the emo-"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "tional impact of the videos plays an important role in human-"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "centered computing. On the one hand,\nthe videos can,\nto"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "a large extent,\nreﬂect\nthe psychological states of\nthe video"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "generators. We can predict the generators’ possible extreme"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "behaviors, such as depression and suicide, and take corre-"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "sponding preventive actions. On the other hand,\nthe videos"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "that evoke strong emotions can easily resonate with viewers"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "and bring them immersive watching experiences. Appropri-"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "ate emotional resonation is crucial\nin intelligent advertising"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "and video recommendation. Further, emotion recognition in"
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": ""
        },
        {
          "approaches for video emotion recognition. Our source code": "∗Corresponding Author. # Equal Contribution."
        },
        {
          "approaches for video emotion recognition. Our source code": "Copyright\n(cid:13) 2020, Association for the Advancement of Artiﬁcial"
        },
        {
          "approaches for video emotion recognition. Our source code": "Intelligence (www.aaai.org). All rights reserved."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "Abstract"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "Emotion recognition in user-generated videos plays an im-"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "portant\nrole in human-centered computing. Existing meth-"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "ods mainly employ traditional\ntwo-stage\nshallow pipeline,"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "i.e. extracting visual and/or audio features and training clas-"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "siﬁers.\nIn this paper, we propose to recognize video emo-"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "tions in an end-to-end manner based on convolutional neu-"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "ral networks (CNNs). Speciﬁcally, we develop a deep Visual-"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "Audio Attention Network (VAANet),\na novel\narchitecture"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "that\nintegrates spatial, channel-wise, and temporal attentions"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "into a visual 3D CNN and temporal attentions\ninto an au-"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "dio\n2D CNN. Further, we\ndesign\na\nspecial\nclassiﬁcation"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "loss,\ni.e. polarity-consistent cross-entropy loss, based on the"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "polarity-emotion hierarchy constraint\nto guide the attention"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "generation. Extensive\nexperiments\nconducted on the\nchal-"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "lenging VideoEmotion-8 and Ekman-6 datasets demonstrate"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "that\nthe proposed VAANet outperforms\nthe state-of-the-art"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "approaches for video emotion recognition. Our source code"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "is released at: https://github.com/maysonma/VAANet."
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "Introduction"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "The convenience of mobile devices and social networks has"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "enabled users to generate videos and upload to Internet\nin"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "daily life to share their experiences and express personal"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "opinions. As a result, an explosive growing volume of videos"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "are being created, which results in urgent demand for\nthe"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "analysis and management of\nthese videos. Besides the ob-"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "jective content recognition, such as objects and actions (Zhu"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "et al. 2018; Choutas et al. 2018), understanding the emo-"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "tional impact of the videos plays an important role in human-"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "centered computing. On the one hand,\nthe videos can,\nto"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "a large extent,\nreﬂect\nthe psychological states of\nthe video"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "generators. We can predict the generators’ possible extreme"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "behaviors, such as depression and suicide, and take corre-"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "sponding preventive actions. On the other hand,\nthe videos"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "that evoke strong emotions can easily resonate with viewers"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "and bring them immersive watching experiences. Appropri-"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "ate emotional resonation is crucial\nin intelligent advertising"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "and video recommendation. Further, emotion recognition in"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": ""
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "∗Corresponding Author. # Equal Contribution."
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "Copyright\n(cid:13) 2020, Association for the Advancement of Artiﬁcial"
        },
        {
          "{guyangdavid,xingtengfei,xupengfeipf,hurunbo,chaihua}@didiglobal.com, keutzer@berkeley.edu": "Intelligence (www.aaai.org). All rights reserved."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "UGVs focus on the ﬁrst challenge, i.e. employing advanced",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "tions into a visual 3D CNN and temporal attentions into"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "image representations to bridge the affective gap, such as",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "an audio 2D CNN for video emotion recognition. We pro-"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "(1) mid-level attribute features (Jiang, Xu, and Xue 2014;",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "pose a novel PCCE loss, which enables VAANet\nto gen-"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "Tu et al. 2019)\nlike ObjectBank (Li et al. 2010) and Sen-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "erate polarity preserved attention map."
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "tiBank\n(Borth\net\nal.\n2013),\n(2)\nhigh-level\nsemantic\nfea-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": ""
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "3. We conduct extensive experiments on the VideoEmotion-"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "tures (Chen, Wu, and Jiang 2016) like detected events (Jiang",
          "that\nintegrates spatial, channel-wise, and temporal atten-": ""
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "8 (Jiang, Xu, and Xue 2014) and Ekman-6 (Xu et al. 2018)"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "et al. 2017; Caba Heilbron et al. 2015), objects\n(Deng et",
          "that\nintegrates spatial, channel-wise, and temporal atten-": ""
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "datasets, and the results demonstrate the superiority of the"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "al. 2009), and scenes (Zhou et al. 2014), and (3) deep con-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": ""
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "proposed VAANet method, as compared to the state-of-"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "volutional neural network (CNN)\nfeatures (Xu et al. 2018;",
          "that\nintegrates spatial, channel-wise, and temporal atten-": ""
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "the-art approaches."
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "Zhang and Xu 2018). Zhang and Xu (2018)\ntransformed",
          "that\nintegrates spatial, channel-wise, and temporal atten-": ""
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "frame-level\nspatial\nfeatures\nto another kernelized feature",
          "that\nintegrates spatial, channel-wise, and temporal atten-": ""
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "space via discrete Fourier\ntransform, which partially ad-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "Related Work"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "dresses\nthe second challenge. For\nthe third challenge,\nthe",
          "that\nintegrates spatial, channel-wise, and temporal atten-": ""
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "Video Emotion Recognition: Psychologists usually employ"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "videos are either downsampled averagely to a ﬁxed number",
          "that\nintegrates spatial, channel-wise, and temporal atten-": ""
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "two kinds of models to represent emotions: categorical emo-"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "of frames (Zhang and Xu 2018), or represented by continu-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": ""
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "tion states (CES) and dimensional emotions space (DES)."
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "ous frames from only one segment (Tu et al. 2019).",
          "that\nintegrates spatial, channel-wise, and temporal atten-": ""
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "CES classify emotions\ninto several basic categories,\nsuch"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "The above methods have contributed to the development",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "as Ekman’s 6 basic categories (Ekman 1992) and Plutchik’s"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "of emotion recognition in UGVs, but\nthey still have some",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "wheel of emotions (Plutchik and Kellerman 1980). DES usu-"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "problems.\n(1) They mainly\nemploy\na\ntwo-stage\nshallow",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "ally employ a Cartesian space to represent emotions, such as"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "i.e.\npipeline,\nextracting visual\nand/or\naudio features\nand",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "valence-arousal-dominance (Schlosberg 1954). Since CES"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "training classiﬁers.\n(2) The visual CNN features of\neach",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "are easy for users to understand and label, here we adopt"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "frame are separately extracted, which ignore the temporal",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "CES to represent emotions in videos."
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "correlation of adjacent\nframes.\n(3) The fact\nthat emotions",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "Early research on video emotion recognition mainly fo-"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "may be determined by keyframes from several discrete seg-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "cused on movies, which are well structured. Kang (2003)"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "ments is neglected. (4) Some methods require auxiliary data,",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "employed a Hidden Markov Model to detect affective event"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "which is not always available in real applications. For exam-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "based on low-level\nfeatures,\nincluding color, motion, and"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "ple, the extracted event, object, and scene features in (Chen,",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "shot cut\nrate.\nJoint combination of visual and audio fea-"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "Wu, and Jiang 2016) are trained on FCVID (Jiang et al.",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "tures with support vector machine (Wang and Cheong 2006)"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "2017) and ActivityNet\n(Caba Heilbron et al. 2015),\nIma-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "and conditional\nrandom ﬁelds\n(Xu et\nal. 2013)\nachieves"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "geNet (Deng et al. 2009), and Places205 (Zhou et al. 2014)",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "promising result. Some recent methods work on Animated"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "datasets, respectively. (5) They do not consider the correla-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "GIFs (Jou, Bhattacharya, and Chang 2014; Chen and Picard"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "tions of different emotions, such as the polarity-emotion hi-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "2016; Yang, Zhang, and Luo 2019). Jou, Bhattacharya, and"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "erarchy constraint, i.e. the relation of two different emotions",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "Chang (2014) ﬁrstly proposed to recognize GIF emotions"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "belonging to the same polarity is closer than those from op-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "by using features of different types. Chen and Picard (2016)"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "posite polarities.",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "improved the performance by adopting 3D ConvNets to ex-"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "In this paper, we propose\nan end-to-end Visual-Audio",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "tract spatiotemporal features. Human-centered GIF emotion"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "Attention Network,\ntermed VAANet,\nto address the above",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "recognition is conducted by considering human related in-"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "problems\nfor\nrecognizing the emotions\nin UGVs, without",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "formation and visual attention (Yang, Zhang, and Luo 2019)."
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "requiring any auxiliary data except the data for pre-training.",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "Because of\nthe content diversity and low quality, UGVs"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "First, we spit each video into an equal number of segments.",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "are more challenging to recognize emotions. Jiang, Xu, and"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "Second, for each segment, we randomly select some succes-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "Xue (2014) investigated a large set of low-level visual-audio"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "sive frames and feed them into a 3D CNN (Hara, Kataoka,",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "features and mid-level attributes, e.g. ObjectBank (Li et al."
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "and Satoh 2018) with both spatial and channel-wise atten-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "2010) and SentiBank (Borth et al. 2013). Chen, Wu, and"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "tions\nto extract visual\nfeatures. Meanwhile, we transform",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "Jiang (2016) extracted various high-level semantic features"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "the corresponding audio waves into spectrograms and feed",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "based on existing detectors. Compared with hand-crafted"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "them into a 2D CNN (He et al. 2016) to extract audio fea-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "features, deep features are more widely used (Xu et al. 2018;"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "tures. Finally, the visual and audio features of different seg-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "Zhang and Xu 2018). By combining low-level visual-audio-"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "ments\nare weighted by temporal\nattentions\nto obtain the",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "textual\nfeatures, Pang, Zhu, and Ngo (2015)\nshowed that"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "whole video’s feature representation, which is followed by",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "learned joint\nrepresentations are complementary to hand-"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "a fully connected layer to obtain emotion predictions. Con-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "crafted features. Different\nfrom these methods, which em-"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "sidering the polarity-emotion hierarchy constraint, we de-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "ploy a two-stage shallow pipeline, we propose the ﬁrst end-"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "sign a novel classiﬁcation loss, i.e. polarity-consistent cross-",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "to-end method to recognize emotions in UGVs by extracting"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "entropy (PCCE) loss, to guide the attention generation.",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "attended visual and audio CNN features."
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "In summary, the contributions of this paper are threefold:",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "Please note that emotion recognition has also been widely"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "studied in other modalities, such as text (Zhang, Wang, and"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "1. We are the ﬁrst\nto study the emotion recognition task in",
          "that\nintegrates spatial, channel-wise, and temporal atten-": ""
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "Liu 2018), images (Zhao et al. 2017; Yang et al. 2018b; Zhao"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "user-generated videos in an end-to-end manner.",
          "that\nintegrates spatial, channel-wise, and temporal atten-": ""
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "et al. 2018a; Yang et al. 2018a; Zhao et al. 2018c; 2019c;"
        },
        {
          "Most\nexisting\napproaches\non\nemotion\nrecognition\nin": "2. We develop a novel network architecture,\ni.e. VAANet,",
          "that\nintegrates spatial, channel-wise, and temporal atten-": "2019b; Yao et al. 2019; Zhan et al. 2019), speech (El Ayadi,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "Figure 2: The framework of the proposed Visual and Audio Attention Network (VAANet). First,\nthe MFCC descriptor from the soundtrack"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "and the visual\ninformation are both divided into segments and fed into 2D ResNet-18 and 3D ResNet-101 respectively to extract audio and"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "visual representation. The response feature maps of the visual stream are then fed into the stacked spatial attention, channel-wise attention,"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "and temporal attention sub-networks, and the response feature map of the audio stream are fed into a temporal attention module. Finally, the"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "attended semantic vectors that carry visual and audio information are concatenated. Meanwhile, a novel polarity-consistent cross-entropy loss"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "is optimized to guide the attention generation for video emotion recognition."
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "Kamel, and Karray 2011), physiological\nsignals\n(Alarcao\nVisual-Audio Attention Network"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "and Fonseca 2017; Zhao et\nal. 2019a),\nand multi-modal"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "We propose a novel CNN architecture with spatial, channel-"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "data (Soleymani et al. 2017; Zhao et al. 2019d)."
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "wise, and temporal attention mechanisms for emotion recog-"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "nition in user generated videos. Figure 2 shows the overall\nAttention-Based Models: Since attention can be consid-"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "framework of the proposed VAANet. Speciﬁcally, VAANet\nered as a dynamic feature extraction mechanism that com-"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "has two streams to respectively exploit\nthe visual and au-\nbines\ncontextual ﬁxations\nover\ntime\n(Mnih\net\nal.\n2014;"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "dio information. The visual stream consists of three attention\nChen et al. 2017),\nit has been seamlessly incorporated into"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "modules and the audio stream contains a temporal attention\ndeep learning architectures and achieved outstanding perfor-"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "module. The spatial attention and the channel-wise attention\nmances in many vision-related tasks, such as image classiﬁ-"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "sub-networks in the visual stream are designed to automat-\ncation (Woo et al. 2018), image captioning (You et al. 2016;"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "ically focus on the regions and channels that carry discrim-\nChen et al. 2017; 2018), and action recognition (Song et"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "inative information within each feature map. The temporal\nal. 2017). These attention methods can be roughly divided"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "attention sub-networks in both the visual and audio streams\ninto four\ncategories:\nspatial\nattention (Song et\nal. 2017;"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "are designed to assign weights\nto different\nsegments of a\nWoo\net\nal.\n2018),\nsemantic\nattention\n(You\net\nal.\n2016),"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "video. The training of VAANet is performed by minimizing\nchannel-wise attention (Chen et al. 2017; Woo et al. 2018),"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "the newly designed polarity-consistent cross-entropy loss in\nand temporal attention (Song et al. 2017)."
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "an end-to-end manner."
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "There are also several methods that employ attention for"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "emotion recognition in images\n(You,\nJin,\nand Luo 2017;"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "Visual Representation Extraction"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "Yang et\nal. 2018a; Zhao et\nal. 2019b)\nand speech (Mir-"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "samadi, Barsoum, and Zhang 2017). The former methods\nTo extract visual\nrepresentations\nfrom a long-term video,"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "mainly consider spatial attention except PDANet\n(Zhao et\nfollowing (Wang et al. 2016), the visual stream of our model"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "al. 2019b) which also employs channel-wise attention, while\nworks on short\nsnippets\nsparsely sampled from the entire"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "the latter one only uses temporal attention. To the best of\nvideo. Speciﬁcally, we divide each video into t\nsegments"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "our knowledge, attention has not been studied on emotion\nwith equal duration, and then randomly sample a short snip-"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "recognition in user-generated videos. In this paper, we sys-\npet of k successive frames from each segment. We use 3D"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "tematically investigate the inﬂuence of different attentions in\nResNet-101 (Hara, Kataoka, and Satoh 2018) as backbone"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "video emotion recognition,\nincluding the importance of lo-\nof the visual stream. It\ntakes the t snippets (each has k suc-"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "cal spatial context by spatial attention,\nthe interdependency\ncessive frames) as input and independently processes them"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "between different channels by channel-wise attention, and\nup to the last spatiotemporal convolutional layer conv5 into"
        },
        {
          "FC\nSoftmax\nFC\nSoftmax\nFC\nattention\nattention\nattention": "the importance of different segments by temporal attention.\na super-frame. Suppose we are given N training samples"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{(xV\nis the visual\ninformation of video": "l\nl=1, where xV",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "is the corresponding emotion label. For sample xV\n,",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "parameter matrices, and AC\n∈ Rn×1."
        },
        {
          "{(xV\nis the visual\ninformation of video": "l, and yl\nl",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "i"
        },
        {
          "{(xV\nis the visual\ninformation of video": "suppose the feature map of the conv5 in 3D ResNet-101 is",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "And then a weighted feature map based on channel-wise"
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "attention is computed as follows"
        },
        {
          "{(xV\nis the visual\ninformation of video": "l\nFV\n∈ Rt×h×w×n (we omit\nfor simplicity in the follow-\nl",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "ing), where h and w are the spatial size (height and width)",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "GC\n(6)\ni = AC\ni ⊗ Gi,"
        },
        {
          "{(xV\nis the visual\ninformation of video": "of the feature map, n is the number of channels, and t is the",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "number of snippets. We reshape FV as",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "where ⊗ is the multiplication of a matrix and a vector."
        },
        {
          "{(xV\nis the visual\ninformation of video": "· · ·\nfV\nfV\nfV\n11\n12\n1m",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "Visual Temporal Attention Estimation"
        },
        {
          "{(xV\nis the visual\ninformation of video": " \n \n· · ·\nfV\nfV\nfV\n21\n22\n2m",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "∈ Rt×m×n,\nFV =\n(1)",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "For a video, the discriminability of each frame to recognize"
        },
        {
          "{(xV\nis the visual\ninformation of video": ".\n.\n.",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": ".\n.\n.\n. . .\n.\n.\n.",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "emotions is obviously different. Only some keyframes con-"
        },
        {
          "{(xV\nis the visual\ninformation of video": "· · ·\nfV\nfV\nfV",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "tain discriminative information, while the others only pro-"
        },
        {
          "{(xV\nis the visual\ninformation of video": "t1\nt2\ntm",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "vide the background and context\ninformation (Song et al."
        },
        {
          "{(xV\nis the visual\ninformation of video": "by ﬂattening the height and width of the original FV , where",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "2017). Based on such observations, we design a temporal at-"
        },
        {
          "{(xV\nis the visual\ninformation of video": "fV\nij ∈ Rn and m = h × w. Here we can consider fV\nij as the",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "tention sub-network to automatically focus on the important"
        },
        {
          "{(xV\nis the visual\ninformation of video": "visual feature of the j-th location in the i-th super-frame. In",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "segments that contain keyframes. To generate the temporal"
        },
        {
          "{(xV\nis the visual\ninformation of video": "the following, we omit the superscript V for simplicity.",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "attention, we ﬁrst apply spatial average pooling to GC and"
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "reshape it to P"
        },
        {
          "{(xV\nis the visual\ninformation of video": "Visual Spatial Attention Estimation",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "We employ a spatial attention module to automatically ex-",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "] ∈ Rt×n,\n(7)\nP = [ p1, · · · , pt"
        },
        {
          "{(xV\nis the visual\ninformation of video": "plore\nthe different\ncontributions of\nthe\nregions\nin super-",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "where pj ∈ Rn(j = 1, 2, · · ·\n, t). Here we can consider pj"
        },
        {
          "{(xV\nis the visual\ninformation of video": "frames to predict the emotions. Following (Chen et al. 2017),",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "as the visual feature of the j-th super-frame. The temporal"
        },
        {
          "{(xV\nis the visual\ninformation of video": "we employ a two-layer neural network,\ni.e. a 1 × 1 con-",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "attention is then deﬁned as"
        },
        {
          "{(xV\nis the visual\ninformation of video": "volutional\nlayer\nfollowed by a fully-connected layer with",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "a softmax function to generate the spatial attention distri-",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "HT = WT1(WT2P(cid:62))(cid:62),"
        },
        {
          "{(xV\nis the visual\ninformation of video": "butions over all\nthe super-frame regions. That\nis,\nfor each",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "(8)"
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "AT = ReLU(HT ),"
        },
        {
          "{(xV\nis the visual\ninformation of video": ", t)\nFi ∈ Rm×n(i = 1, 2, · · ·",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": ")(cid:62),\nHS\ni = WS1 (WS2F(cid:62)",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "where WT1 ∈ Rt×t and WT2 ∈ R1×n are two learnable"
        },
        {
          "{(xV\nis the visual\ninformation of video": "(2)",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "parameter matrices, and AT ∈ Rt×1. Following\n(Song et"
        },
        {
          "{(xV\nis the visual\ninformation of video": "AS\ni = Softmax(HS\ni ),",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "al. 2017), we use ReLU (Rectiﬁed Linear Units) as the acti-"
        },
        {
          "{(xV\nis the visual\ninformation of video": "where WS1 ∈ Rm×m and WS2 ∈ R1×n are two learnable",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "vation function here for its better convergence performance."
        },
        {
          "{(xV\nis the visual\ninformation of video": "parameter matrices, (cid:62) is the transpose of a matrix, and AS\ni ∈",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "The ﬁnal visual embedding is the weighted sum of all\nthe"
        },
        {
          "{(xV\nis the visual\ninformation of video": "Rm×1.",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "super-frames"
        },
        {
          "{(xV\nis the visual\ninformation of video": "And then we can obtain a weighted feature map based on",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "spatial attention as follows",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "t(cid:88) j\nEV =\n(9)\nj ∈ Rn.\npj · AT"
        },
        {
          "{(xV\nis the visual\ninformation of video": "FS\n(3)\ni ⊗ Fi,\ni = AS",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "=1"
        },
        {
          "{(xV\nis the visual\ninformation of video": "where ⊗ is the multiplication of a matrix and a vector, which",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "is performed by multiplying each value in the vector to each",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "Audio Representation Extraction"
        },
        {
          "{(xV\nis the visual\ninformation of video": "column of the matrix.",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "Audio features are complementary to visual\nfeatures, be-"
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "cause\nthey\ncontain\ninformation\nof\nanother modality.\nIn"
        },
        {
          "{(xV\nis the visual\ninformation of video": "Visual Channel-Wise Attention Estimation",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "our problem, we choose to use the most well-known au-"
        },
        {
          "{(xV\nis the visual\ninformation of video": "Assuming that each channel of a feature map in a CNN",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "dio representation:\nthe mel-frequency cepstral coefﬁcients"
        },
        {
          "{(xV\nis the visual\ninformation of video": "is a response activation of the corresponding convolutional",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "(MFCC). Suppose we are given N audio training samples"
        },
        {
          "{(xV\nis the visual\ninformation of video": "layer, channel-wise attention can be viewed as a process of",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "{(xA\nxA"
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "is\na\ndescriptor\nfrom the\nentire\n, yl)}N\nl\nl=1, where\nl"
        },
        {
          "{(xV\nis the visual\ninformation of video": "selecting semantic attributes (Chen et al. 2017). To generate",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": ""
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "is the corresponding emo-\nsoundtrack of the video Vl and yl"
        },
        {
          "{(xV\nis the visual\ninformation of video": "the channel-wise attention, we ﬁrst transpose FV to G",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "(cid:48)"
        },
        {
          "{(xV\nis the visual\ninformation of video": "",
          "where WC1 ∈ Rn×n and WC2 ∈ R1×m are two learnable": "tion label. We center-crop xA\nto a ﬁxed length of q to get xA\n,"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio Temporal Attention Estimation": "With similar motivation to integrate temporal attention sub-",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "Benchmarks\nWe evaluate the performances of\nthe pro-"
        },
        {
          "Audio Temporal Attention Estimation": "network into the visual stream, we introduce a temporal at-",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "posed method on two publicly available datasets that contain"
        },
        {
          "Audio Temporal Attention Estimation": "tention sub-network to explore the inﬂuence of audio infor-",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "emotion\nlabels\nin\nuser-generated\nvideos: VideoEmotion-"
        },
        {
          "Audio Temporal Attention Estimation": "mation in different segments for recognizing emotions as",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "8 (Jiang, Xu, and Xue 2014) and Ekman-6 (Xu et al. 2018)."
        },
        {
          "Audio Temporal Attention Estimation": "(cid:62)",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "VideoEmotion-8 (Jiang, Xu, and Xue 2014) (VE-8) con-"
        },
        {
          "Audio Temporal Attention Estimation": ")\n)(cid:62),\nHA = WA1(WA2(FA(cid:48)",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "(10)",
          "Experimental Settings": "sists of 1,101 videos collected from Youtube and Flickr with"
        },
        {
          "Audio Temporal Attention Estimation": "AA = ReLU(HA),",
          "Experimental Settings": "average duration 107 seconds. The videos are labeled into"
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "one of\nthe Plutchik’s eight basic categories\n(Plutchik and"
        },
        {
          "Audio Temporal Attention Estimation": "where WA1 ∈ Rt×t and WA2 ∈ R1×n(cid:48)\nare two learnable",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "Kellerman 1980): negative anger, disgust, fear, sadness and"
        },
        {
          "Audio Temporal Attention Estimation": "parameter matrices, and AA ∈ Rt×1. The ﬁnal audio em-",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "positive anticipation,\njoy, surprise,\ntrust.\nIn each category,"
        },
        {
          "Audio Temporal Attention Estimation": "bedding is the weighted sum of all the segments",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "there are at\nleast 100 videos. Ekman-6 (Xu et al. 2018) (E-"
        },
        {
          "Audio Temporal Attention Estimation": "(cid:48)",
          "Experimental Settings": "6) contains 1,637 videos also collected from Youtube and"
        },
        {
          "Audio Temporal Attention Estimation": "t(cid:88) j\n.\nEA =\nFA\n· AA\n(11)\nj\nj ∈ Rn(cid:48)",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "Flickr. The average duration is 112 seconds. The videos are"
        },
        {
          "Audio Temporal Attention Estimation": "=1",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "labeled with Ekman’s six emotion categories (Ekman 1992),"
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "i.e. negative anger, disgust,\nfear, sadness and positive joy,"
        },
        {
          "Audio Temporal Attention Estimation": "Polarity-Consistent Cross-Entropy Loss",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "surprise."
        },
        {
          "Audio Temporal Attention Estimation": "We concatenate EV\nand EA to obtain an aggregated seman-",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "tic vector E = [EV , EA]\n, which can be viewed as the ﬁnal",
          "Experimental Settings": "Baselines\nTo compare VAANet with the state-of-the-art"
        },
        {
          "Audio Temporal Attention Estimation": "representation of a video and is fed into a fully connected",
          "Experimental Settings": "approaches\nfor video emotion recognition, we\nselect\nthe"
        },
        {
          "Audio Temporal Attention Estimation": "layer\nto predict\nthe emotion labels. The traditional cross-",
          "Experimental Settings": "following methods\nas baselines:\n(1) SentiBank (Borth et"
        },
        {
          "Audio Temporal Attention Estimation": "entropy loss is deﬁned as",
          "Experimental Settings": "al. 2013),\n(2) Enhanced Multimodal Deep Bolzmann Ma-"
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "chine\n(E-MDBM)\n(Pang, Zhu,\nand Ngo\n2015),\n(3)\nIm-"
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "age Transfer Encoding\n(ITE)\n(Xu\net\nal.\n2018),\n(4) Vi-"
        },
        {
          "Audio Temporal Attention Estimation": "1 N\nN(cid:88) i\nC(cid:88) c\n(12)\nLCE = −\n1[c=yi] log pi,c,",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "=1\n=1",
          "Experimental Settings": "sual+Audio+Attribute\n(V.+Au.+At.)\n(Jiang, Xu,\nand Xue"
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "2014), (5) Context Fusion Net (CFN) (Chen, Wu, and Jiang"
        },
        {
          "Audio Temporal Attention Estimation": "where C is\nthe number of\nemotion classes\n(C = 8 for",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "2016),\n(6) V.+Au.+At.+E-MDBM (Pang, Zhu,\nand Ngo"
        },
        {
          "Audio Temporal Attention Estimation": "VideoEmotion-8 and C = 6 for Ekman-6 in this paper),",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "2015),\n(7) Kernelized and Kernelized+SentiBank (Zhang"
        },
        {
          "Audio Temporal Attention Estimation": "is a binary indicator, and pi,c is the predicted proba-\n1[c=yi]",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "and Xu 2018)."
        },
        {
          "Audio Temporal Attention Estimation": "bility that video i belongs to class c.",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "Directly optimizing the cross-entropy loss in Eq. (12) can",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "Implementation Details\nFollowing (Jiang, Xu, and Xue"
        },
        {
          "Audio Temporal Attention Estimation": "lead some videos to be incorrectly classiﬁed into categories",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "2014; Zhang and Xu 2018),\nthe experiments on VE-8 are"
        },
        {
          "Audio Temporal Attention Estimation": "that have opposite polarity. In this paper, we design a novel",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "conducted 10 runs.\nIn each run, we\nrandomly select 2/3"
        },
        {
          "Audio Temporal Attention Estimation": "polarity-consistent cross-entorpy (PCCE)\nloss to guide the",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "of\nthe data\nfrom each category for\ntraining and the\nrest"
        },
        {
          "Audio Temporal Attention Estimation": "attention generation. That\nis,\nthe penalty of the predictions",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "for\ntesting. We report\nthe average classiﬁcation accuracy"
        },
        {
          "Audio Temporal Attention Estimation": "that have opposite polarity to the ground truth is increased.",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "of\nthe 10 runs. For E-6, we employ the split provided by"
        },
        {
          "Audio Temporal Attention Estimation": "The PCCE loss is deﬁned as",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "the dataset,\ni.e. 819 videos\nfor\ntraining and 818 for\ntest-"
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "ing. The classiﬁcation accuracy on the test set\nis evaluated."
        },
        {
          "Audio Temporal Attention Estimation": "1 N\nN(cid:88) i\nC(cid:88) c\nLP CCE = −\n(1 + λ(g(ˆyi, yi)))\n1[c=yi] log pi,c,",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "Our model\nis based on two state-of-the-art CNN architec-"
        },
        {
          "Audio Temporal Attention Estimation": "=1\n=1",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "tures: 2D ResNet-18\n(He et al. 2016) and 3D ResNet-101"
        },
        {
          "Audio Temporal Attention Estimation": "(13)",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "(Hara, Kataoka, and Satoh 2018), which are initialized with"
        },
        {
          "Audio Temporal Attention Estimation": "where λ is a penalty coefﬁcient\nthat controls\nthe penalty",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "the weights pre-trained on ImageNet (Deng et al. 2009) and"
        },
        {
          "Audio Temporal Attention Estimation": "extent. Similar\nto the\nindicator\nfunction, g(.,.)\nrepresents",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "Kinetics (Carreira and Zisserman 2017), respectively. In ad-"
        },
        {
          "Audio Temporal Attention Estimation": "whether to add the penalty or not and is deﬁned as",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "dition, for the visual stream, we divide the input video into"
        },
        {
          "Audio Temporal Attention Estimation": "(cid:26)1,\nif polarity(ˆy) (cid:54)= polarity(y),",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "10 segments and sample 16 successive frames from each of"
        },
        {
          "Audio Temporal Attention Estimation": "g(ˆy, y) =\n(14)",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "0,\notherwise,",
          "Experimental Settings": "them. We resize each frame of the visual sample and make"
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "the short side length of the sample equal\nto 112 pixels, and"
        },
        {
          "Audio Temporal Attention Estimation": "where polarity(.) is a function that maps an emotion cate-",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "then apply random horizontal ﬂips and crop a random 112"
        },
        {
          "Audio Temporal Attention Estimation": "gory to its polarity (positive or negative). Since the deriva-",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "x 112 patch as data augmentation to reduce overﬁtting.\nIn"
        },
        {
          "Audio Temporal Attention Estimation": "tives with respect\nto all parameters can be computed, we",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "our\ntraining, Adam (Kingma and Ba 2014)\nis adopted to"
        },
        {
          "Audio Temporal Attention Estimation": "can train the proposed VAANet effectively in an end-to-end",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "automatically adjust\nthe learning rate during optimization,"
        },
        {
          "Audio Temporal Attention Estimation": "manner using off-the-shelf optimizer\nto minimize the loss",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "with the initial\nlearning rate set\nto 0.0002 and the model\nis"
        },
        {
          "Audio Temporal Attention Estimation": "function in Eq. (13).",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "trained with batch-size 32 for 150 epochs. Our model is im-"
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "plemented using PyTorch."
        },
        {
          "Audio Temporal Attention Estimation": "Experiments",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "In this section, we evaluate the proposed VAANet model on",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "",
          "Experimental Settings": "Comparison with the State-of-the-art"
        },
        {
          "Audio Temporal Attention Estimation": "emotion recognition in user-generated videos. We ﬁrst\nin-",
          "Experimental Settings": ""
        },
        {
          "Audio Temporal Attention Estimation": "troduce the employed benchmarks, compared baselines, and",
          "Experimental Settings": "The extracted features,\ntraining strategies, and average per-"
        },
        {
          "Audio Temporal Attention Estimation": "implementation details. And then we report and analyze the",
          "Experimental Settings": "formance comparisons between the proposed VAANet and"
        },
        {
          "Audio Temporal Attention Estimation": "major results together with some empirical analysis.",
          "Experimental Settings": "the state-of-the-art approaches are shown in Tables 1 and 2"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: Comparison between the proposed VAANet and several state-of-the-art methods on the VE-8 dataset, where ‘Visual’, ‘Audio’,",
      "data": [
        {
          "Table 1: Comparison between the proposed VAANet and several state-of-the-art methods on the VE-8 dataset, where ‘Visual’,\n‘Audio’,": "and ‘Attribute’ indicate whether corresponding features are used, ‘Auxiliary’ means whether no auxiliary data is used except\nthe commonly"
        },
        {
          "Table 1: Comparison between the proposed VAANet and several state-of-the-art methods on the VE-8 dataset, where ‘Visual’,\n‘Audio’,": "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)\nfor pre-training, and ‘End-to-end’\nindicates whether\nthe corresponding"
        },
        {
          "Table 1: Comparison between the proposed VAANet and several state-of-the-art methods on the VE-8 dataset, where ‘Visual’,\n‘Audio’,": "algorithm is trained in an end-to-end manner. The best method is emphasized in bold. Our method achieves the best results, outperforming"
        },
        {
          "Table 1: Comparison between the proposed VAANet and several state-of-the-art methods on the VE-8 dataset, where ‘Visual’,\n‘Audio’,": "the state-of-the-art approaches."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Comparison between the proposed VAANet and several state-of-the-art methods on the VE-8 dataset, where ‘Visual’, ‘Audio’,",
      "data": [
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "algorithm is trained in an end-to-end manner. The best method is emphasized in bold. Our method achieves the best results, outperforming",
          "for pre-training, and ‘End-to-end’": "",
          "indicates whether": "",
          "the corresponding": ""
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "the state-of-the-art approaches.",
          "for pre-training, and ‘End-to-end’": "",
          "indicates whether": "",
          "the corresponding": ""
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "Method",
          "for pre-training, and ‘End-to-end’": "Audio",
          "indicates whether": "End-to-end",
          "the corresponding": "Accuracy"
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "",
          "for pre-training, and ‘End-to-end’": "",
          "indicates whether": "",
          "the corresponding": ""
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "SentiBank (Borth et al. 2013)",
          "for pre-training, and ‘End-to-end’": "",
          "indicates whether": "",
          "the corresponding": "35.5"
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "",
          "for pre-training, and ‘End-to-end’": "(cid:88)",
          "indicates whether": "",
          "the corresponding": ""
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "E-MDBM (Pang, Zhu, and Ngo 2015)",
          "for pre-training, and ‘End-to-end’": "",
          "indicates whether": "",
          "the corresponding": "40.4"
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "",
          "for pre-training, and ‘End-to-end’": "(cid:88)",
          "indicates whether": "",
          "the corresponding": ""
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "ITE (Xu et al. 2018)",
          "for pre-training, and ‘End-to-end’": "",
          "indicates whether": "",
          "the corresponding": "44.7"
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "",
          "for pre-training, and ‘End-to-end’": "(cid:88)",
          "indicates whether": "",
          "the corresponding": ""
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "V\n.+Au.+At. (Jiang, Xu, and Xue 2014)",
          "for pre-training, and ‘End-to-end’": "",
          "indicates whether": "",
          "the corresponding": "46.1"
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "",
          "for pre-training, and ‘End-to-end’": "(cid:88)",
          "indicates whether": "",
          "the corresponding": ""
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "CFN (Chen, Wu, and Jiang 2016)",
          "for pre-training, and ‘End-to-end’": "",
          "indicates whether": "",
          "the corresponding": "50.4"
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "",
          "for pre-training, and ‘End-to-end’": "(cid:88)",
          "indicates whether": "",
          "the corresponding": ""
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "V\n.+Au.+At.+E-MDBM (Pang, Zhu, and Ngo 2015)",
          "for pre-training, and ‘End-to-end’": "",
          "indicates whether": "",
          "the corresponding": "51.1"
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "",
          "for pre-training, and ‘End-to-end’": "",
          "indicates whether": "",
          "the corresponding": ""
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "Kernelized (Zhang and Xu 2018)",
          "for pre-training, and ‘End-to-end’": "",
          "indicates whether": "",
          "the corresponding": "49.7"
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "",
          "for pre-training, and ‘End-to-end’": "",
          "indicates whether": "",
          "the corresponding": ""
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "Kernelized+SentiBank (Zhang and Xu 2018)",
          "for pre-training, and ‘End-to-end’": "",
          "indicates whether": "",
          "the corresponding": "52.5"
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "",
          "for pre-training, and ‘End-to-end’": "(cid:88)",
          "indicates whether": "(cid:88)",
          "the corresponding": ""
        },
        {
          "used ImageNet\n(Deng et al. 2009) and Kinetics (Kay et al. 2017)": "VAANet (Ours)",
          "for pre-training, and ‘End-to-end’": "",
          "indicates whether": "",
          "the corresponding": "54.5"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Comparison between the proposed VAANet and several state-of-the-art methods on the VE-8 dataset, where ‘Visual’, ‘Audio’,",
      "data": [
        {
          "(cid:88)": "52.5"
        },
        {
          "(cid:88)": "(cid:88)\n(cid:88)\n(cid:88)"
        },
        {
          "(cid:88)": "54.5"
        },
        {
          "(cid:88)": ""
        },
        {
          "(cid:88)": "emotion correlations, which can guide the detailed learning"
        },
        {
          "(cid:88)": ""
        },
        {
          "(cid:88)": "process. Third,\nthe visual features extracted by 3D ResNet-"
        },
        {
          "(cid:88)": ""
        },
        {
          "(cid:88)": "101\ncan model\nthe\ntemporal\ncorrelation\nof\nthe\nadjacent"
        },
        {
          "(cid:88)": ""
        },
        {
          "(cid:88)": "frames in a given segment."
        },
        {
          "(cid:88)": ""
        },
        {
          "(cid:88)": ""
        },
        {
          "(cid:88)": ""
        },
        {
          "(cid:88)": "Ablation Study"
        },
        {
          "(cid:88)": ""
        },
        {
          "(cid:88)": "The\nproposed VAANet model\ncontains\ntwo major\ncom-"
        },
        {
          "(cid:88)": ""
        },
        {
          "(cid:88)": "ponents: a novel attention mechanism and a novel cross-"
        },
        {
          "(cid:88)": "entropy loss. We conduct ablation study to further verify"
        },
        {
          "(cid:88)": "their effectiveness by changing one component and ﬁxing"
        },
        {
          "(cid:88)": ""
        },
        {
          "(cid:88)": "the other. First, using polarity-consistent cross-entropy loss,"
        },
        {
          "(cid:88)": ""
        },
        {
          "(cid:88)": "we investigate the inﬂuence of different attentions,\ninclud-"
        },
        {
          "(cid:88)": "ing visual spatial (VS), visual channel-wise (VCW), visual"
        },
        {
          "(cid:88)": "temporal\n(VT), and audio temporal\n(AT) ones. The emo-"
        },
        {
          "(cid:88)": "tion recognition accuracy of each emotion category and the"
        },
        {
          "(cid:88)": "average accuracy on VE-8 and E-6 datasets are shown in"
        },
        {
          "(cid:88)": "Table 3 and 4,\nrespectively. From the results, we can ob-"
        },
        {
          "(cid:88)": "serve that:\n(1) visual attentions even only using spatial at-"
        },
        {
          "(cid:88)": "tention signiﬁcantly outperform audio attentions (on aver-"
        },
        {
          "(cid:88)": "age more than 10% improvement), which is understandable"
        },
        {
          "(cid:88)": "because in many videos the audio does not change much;"
        },
        {
          "(cid:88)": "(2) adding each one of them introduces performance gains,"
        },
        {
          "(cid:88)": "which demonstrates that all these attentions contribute to the"
        },
        {
          "(cid:88)": "video emotion recognition task; (3) though not performing"
        },
        {
          "(cid:88)": "well alone, combining audio features with visual\nfeatures"
        },
        {
          "(cid:88)": "can boost the performance with about 1% accuracy gains."
        },
        {
          "(cid:88)": "Second, we\nevaluate\nthe\neffectiveness of\nthe proposed"
        },
        {
          "(cid:88)": "polarity-consistent cross-entropy loss\n(PCCE) by compar-"
        },
        {
          "(cid:88)": "ing with traditional cross-entropy loss (CE). Table 5 shows"
        },
        {
          "(cid:88)": "the results when visual attentions (VS+VCW+VT) and vi-"
        },
        {
          "(cid:88)": "sual+audio attentions (VS+VCW+VT+AT) are considered."
        },
        {
          "(cid:88)": "From the results,\nit\nis clear\nthat\nfor both settings, PCCE"
        },
        {
          "(cid:88)": "performs better. The performance improvements of PCCE"
        },
        {
          "(cid:88)": "over CE for visual attentions and visual+audio attentions are"
        },
        {
          "(cid:88)": "1.5%, 0.6% and 2.5%, 0.7% on the VE-8 and E-6 datasets,"
        },
        {
          "(cid:88)": "respectively. This demonstrates the effectiveness of emotion"
        },
        {
          "(cid:88)": "hierarchy as prior knowledge. This novel\nloss can also be"
        },
        {
          "(cid:88)": "easily extended to other machine learning tasks if some prior"
        },
        {
          "(cid:88)": "knowledge is available."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: Ablation study of different attentions in the proposed VAANet for video emotion recognition on the VE-8 dataset, where ‘VS’,",
      "data": [
        {
          "Table 3: Ablation study of different attentions in the proposed VAANet": "‘VCW’, ‘VT’, and ‘AT’ are short for visual spatial, visual channel-wise, visual temporal, and audio temporal attentions, respectively. All the",
          "for video emotion recognition on the VE-8 dataset, where ‘VS’,": ""
        },
        {
          "Table 3: Ablation study of different attentions in the proposed VAANet": "attentions contribute to the emotion regression task.",
          "for video emotion recognition on the VE-8 dataset, where ‘VS’,": ""
        },
        {
          "Table 3: Ablation study of different attentions in the proposed VAANet": "Attentions",
          "for video emotion recognition on the VE-8 dataset, where ‘VS’,": "Joy"
        },
        {
          "Table 3: Ablation study of different attentions in the proposed VAANet": "AT",
          "for video emotion recognition on the VE-8 dataset, where ‘VS’,": "46.1"
        },
        {
          "Table 3: Ablation study of different attentions in the proposed VAANet": "VS",
          "for video emotion recognition on the VE-8 dataset, where ‘VS’,": "49.1"
        },
        {
          "Table 3: Ablation study of different attentions in the proposed VAANet": "VS+VCW",
          "for video emotion recognition on the VE-8 dataset, where ‘VS’,": "50.9"
        },
        {
          "Table 3: Ablation study of different attentions in the proposed VAANet": "VS+VCW+VT",
          "for video emotion recognition on the VE-8 dataset, where ‘VS’,": "57.7"
        },
        {
          "Table 3: Ablation study of different attentions in the proposed VAANet": "VS+VCW+VT+AT",
          "for video emotion recognition on the VE-8 dataset, where ‘VS’,": "55.9"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: Ablation study of different attentions in the proposed VAANet for video emotion recognition on the VE-8 dataset, where ‘VS’,",
      "data": [
        {
          "VS+VCW+VT+AT\n48.2": "",
          "24.1": "Table 4: Ablation study of different attentions in the proposed VAANet for video emotion recognition on the E-6 dataset.",
          "33.3": "",
          "55.9": "",
          "52.5": "",
          "77.1": "",
          "35.6": ""
        },
        {
          "VS+VCW+VT+AT\n48.2": "Attentions",
          "24.1": "Anger",
          "33.3": "Disgust",
          "55.9": "Joy",
          "52.5": "Sadness",
          "77.1": "Surprise",
          "35.6": "Average"
        },
        {
          "VS+VCW+VT+AT\n48.2": "AT",
          "24.1": "32.4",
          "33.3": "14.1",
          "55.9": "28.1",
          "52.5": "63.9",
          "77.1": "46.5",
          "35.6": "37.2"
        },
        {
          "VS+VCW+VT+AT\n48.2": "VS",
          "24.1": "59.9",
          "33.3": "44.8",
          "55.9": "46.2",
          "52.5": "35.3",
          "77.1": "62.5",
          "35.6": "50.8"
        },
        {
          "VS+VCW+VT+AT\n48.2": "VS+VCW",
          "24.1": "58.4",
          "33.3": "52.6",
          "55.9": "46.3",
          "52.5": "44.0",
          "77.1": "65.1",
          "35.6": "53.4"
        },
        {
          "VS+VCW+VT+AT\n48.2": "VS+VCW+VT",
          "24.1": "57.1",
          "33.3": "53.1",
          "55.9": "57.0",
          "52.5": "38.7",
          "77.1": "65.0",
          "35.6": "54.5"
        },
        {
          "VS+VCW+VT+AT\n48.2": "VS+VCW+VT+AT",
          "24.1": "55.1",
          "33.3": "50.6",
          "55.9": "53.7",
          "52.5": "50.3",
          "77.1": "68.9",
          "35.6": "55.3"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: Ablation study of different attentions in the proposed VAANet for video emotion recognition on the VE-8 dataset, where ‘VS’,",
      "data": [
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": ""
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "entropy loss\n(CE) and our polarity-consistent cross-entropy loss"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": ""
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "(PCCE) measured by average accuracy."
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": ""
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": ""
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "Attentions\nLoss\nVE-8\nE-6"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": ""
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "CE\n51.9\n52.0"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": ""
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "VS+VCW+VT"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "PCCE\n53.6\n54.5"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": ""
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "CE\n53.9\n54.6"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "VS+VCW+VT+AT"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "PCCE\n54.5\n55.3"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": ""
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": ""
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "Visualization"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": ""
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "In order to show the interpretability of our model, we use the"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "heat map generated by the Gram-Cam algorithm (Selvaraju"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "et al. 2017) to visualize the visual spatial attention obtained"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "by the proposed VAANet. The visual temporal attention gen-"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "erated by our model is also illustrated through the color bar."
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "As illustrated in Figure 3, the well-trained VAANet can suc-"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "cessfully pay more attention not only to the discriminative"
        },
        {
          "Table\n5:\nPerformance\ncomparison\nbetween\ntraditional\ncross-": "frames, but also to different salient\nregions in correspond-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "tention generation. The\nextensive\nexperiments\nconducted",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "vey on speech emotion recognition: Features, classiﬁcation"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "on VideoEmotion-8 and Ekman-6 benchmarks demonstrate",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "schemes, and databases. PR 44(3):572–587."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "that VAANet\nachieves\n2.0% and\n0.9% performance\nim-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Hara, K.; Kataoka, H.; and Satoh, Y. 2018. Can spatiotem-"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "provements as compared to the best\nstate-of-the-art video",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "poral 3d cnns retrace the history of 2d cnns and imagenet?"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "emotion recognition approach. In future studies, we plan to",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "In CVPR, 6546–6555."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "extend the VAANet model\nto both ﬁne-tuned emotion clas-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "siﬁcation and emotion regression tasks. We also aim to in-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "learning for image recognition.\nIn CVPR, 770–778."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "vestigate attentions that can better concentrate on the key",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Jiang, Y.-G.; Wu, Z.; Wang, J.; Xue, X.; and Chang, S.-F."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "frames in each video segment.",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "2017.\nExploiting feature and class\nrelationships\nin video"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "IEEE\ncategorization with regularized deep neural networks."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "Acknowledgments",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "TPAMI 40(2):352–364."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "This work is\nsupported by Berkeley DeepDrive,\nthe Na-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Jiang, Y.-G.; Xu, B.; and Xue, X. 2014. Predicting emotions"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "tional Natural Science Foundation of China (Nos. 61701273,",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "in user-generated videos.\nIn AAAI, 73–79."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "61876094, U1933114),\nthe Major Project for New Genera-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Jou, B.; Bhattacharya, S.; and Chang, S.-F. 2014. Predicting"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "tion of AI Grant\n(No. 2018AAA010040003), Natural Sci-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "viewer perceived emotions in animated gifs.\nIn ACM MM,"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "ence Foundation of Tianjin, China (Nos.18JCYBJC15400,",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "213–216."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "18ZXZNGX00110), and the Open Project Program of\nthe",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Kang, H.-B. 2003. Affective content detection using hmms."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "National Laboratory of Pattern Recognition (NLPR).",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "In ACM MM, 259–262."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Kay, W.; Carreira, J.; Simonyan, K.; Zhang, B.; Hillier, C.;"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "References",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Vijayanarasimhan, S.; Viola, F.; Green, T.; Back, T.; Natsev,"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "P.; et al.\n2017.\nThe kinetics human action video dataset."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "Alarcao, S. M., and Fonseca, M. J. 2017. Emotions recog-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "arXiv:1705.06950."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "nition using eeg signals: A survey.\nIEEE TAFFC.",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Kingma, D. P., and Ba,\nJ.\n2014.\nAdam: A method for"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "Borth, D.; Chen, T.; Ji, R.; and Chang, S.-F.\n2013.\nSen-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "stochastic optimization. arXiv:1412.6980."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "tibank: large-scale ontology and classiﬁers for detecting sen-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "timent and emotions in visual content.\nIn ACM MM, 459–",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Li, L.-J.; Su, H.; Fei-Fei, L.; and Xing, E. P.\n2010. Object"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "460.",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "bank: A high-level image representation for scene classiﬁca-"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "tion & semantic feature sparsiﬁcation. In NIPS, 1378–1386."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "Caba Heilbron, F.; Escorcia, V.; Ghanem, B.;\nand Car-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Mirsamadi, S.; Barsoum, E.; and Zhang, C. 2017. Automatic"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "los Niebles, J. 2015. Activitynet: A large-scale video bench-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "speech emotion recognition using recurrent neural networks"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "mark for human activity understanding.\nIn CVPR, 961–970.",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "with local attention.\nIn ICASSP, 2227–2231."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "Carreira, J., and Zisserman, A.\n2017.\nQuo vadis, action",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Mnih, V.; Heess, N.; Graves, A.; et al.\n2014.\nRecurrent"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "recognition? a new model and the kinetics dataset. In CVPR,",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "models of visual attention.\nIn NIPS, 2204–2212."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "6299–6308.",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Pang, L.; Zhu, S.; and Ngo, C.-W. 2015. Deep multimodal"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "Chen, W., and Picard, R. W.\n2016.\nPredicting perceived",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "IEEE TMM\nlearning for affective analysis and retrieval."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "emotions in animated gifs with 3d conolutional neural net-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "17(11):2008–2020."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "works.\nIn ISM, 367–368.",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Plutchik, R., and Kellerman, H. 1980. Emotion, theory, re-"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "Chen, L.; Zhang, H.; Xiao, J.; Nie, L.; Shao, J.; Liu, W.;",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "search, and experience. Academic press."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "and Chua, T.-S.\n2017.\nSca-cnn: Spatial and channel-wise",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Schlosberg, H.\n1954. Three dimensions of emotion. Psy-"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "attention in convolutional networks for image captioning. In",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "chological Review 61(2):81."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "CVPR, 5659–5667.",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Selvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "Chen, H.; Ding, G.; Lin, Z.; Zhao, S.; and Han, J.\n2018.",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Parikh, D.; and Batra, D.\n2017. Grad-cam: Visual expla-"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "Show, observe and tell: Attribute-driven attention model for",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "nations from deep networks via gradient-based localization."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "image captioning.\nIn IJCAI, 606–612.",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "In ICCV, 618–626."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "Chen, C.; Wu, Z.; and Jiang, Y.-G.\n2016. Emotion in con-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Soleymani, M.; Garcia, D.; Jou, B.; Schuller, B.; Chang, S.-"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "text: Deep semantic feature fusion for video emotion recog-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "F.; and Pantic, M. 2017. A survey of multimodal sentiment"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "nition.\nIn ACM MM, 127–131.",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "analysis.\nIVC 65:3–14."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "Choutas, V.; Weinzaepfel, P.; Revaud,\nJ.; and Schmid, C.",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Song, S.; Lan, C.; Xing,\nJ.; Zeng, W.; and Liu,\nJ.\n2017."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "2018. Potion: Pose motion representation for action recog-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "An end-to-end spatio-temporal attention model\nfor human"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "nition.\nIn CVPR, 7024–7033.",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": ""
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "action recognition from skeleton data.\nIn AAAI, 4263–4270."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Tu, G.; Fu, Y.; Li, B.; Gao,\nJ.; Jiang, Y.-G.; and Xue, X."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "Fei, L.\n2009.\nImagenet: A large-scale hierarchical\nimage",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "2019. A multi-task neural approach for emotion attribution,"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "database.\nIn CVPR, 248–255.",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "classiﬁcation and summarization.\nIEEE TMM."
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "Ekman, P. 1992. An argument for basic emotions. Cognition",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "Wang, H. L., and Cheong, L.-F. 2006. Affective understand-"
        },
        {
          "polarity-consistent\ncross-entropy\nloss\ncan\nguide\nthe\nat-": "& Emotion 6(3-4):169–200.",
          "El Ayadi, M.; Kamel, M. S.; and Karray, F.\n2011.\nSur-": "ing in ﬁlm.\nIEEE TCSVT 16(6):689–704."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "and Van Gool, L.\n2016. Temporal segment networks: To-",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "2019b.\nPdanet: Polarity-consistent deep attention network"
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "wards good practices for deep action recognition.\nIn ECCV,",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "for ﬁne-grained visual emotion regression.\nIn ACM MM,"
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "20–36.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "192–201."
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Woo, S.; Park, J.; Lee, J.-Y.; and So Kweon, I. 2018. Cbam:",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "Zhao, S.; Lin, C.; Xu, P.; Zhao, S.; Guo, Y.; Krishna, R.;"
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Convolutional block attention module.\nIn ECCV, 3–19.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "Ding, G.; and Keutzer, K. 2019c. Cycleemotiongan: Emo-"
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "tional semantic consistency preserved cyclegan for adapting"
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Xu, M.; Xu, C.; He, X.;\nJin,\nJ. S.; Luo, S.; and Rui, Y.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "image emotions.\nIn AAAI, 2620–2627."
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "2013. Hierarchical affective content analysis in arousal and",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "valence dimensions. SIGPRO 93(8):2140–2150.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "Zhao, S.; Wang, S.; Soleymani, M.;\nJoshi, D.; and Ji, Q."
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "2019d. Affective computing for large-scale heterogeneous"
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Xu, B.; Fu, Y.; Jiang, Y.-G.; Li, B.; and Sigal, L. 2018. Het-",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "multimedia data: A survey. ACM ToMM."
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "erogeneous knowledge transfer\nin video emotion recogni-",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "tion, attribution and summarization. IEEE TAFFC 9(2):255–",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "Zhou, B.; Lapedriza, A.; Xiao, J.; Torralba, A.; and Oliva,"
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "270.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "A. 2014. Learning deep features for scene recognition using"
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "places database.\nIn NIPS, 487–495."
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Yang, J.; She, D.; Lai, Y.-K.; Rosin, P. L.; and Yang, M.-",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "H.\n2018a. Weakly supervised coupled networks for visual",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "Zhu, X.; Dai, J.; Yuan, L.; and Wei, Y. 2018. Towards high"
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "sentiment analysis.\nIn CVPR, 7584–7592.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": "performance video object detection.\nIn CVPR, 7210–7218."
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Yang, J.; She, D.; Lai, Y.-K.; and Yang, M.-H. 2018b. Re-",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "trieving and classifying affective\nimages via deep metric",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "learning.\nIn AAAI.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Yang, Z.; Zhang, Y.; and Luo, J.\n2019.\nHuman-centered",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "emotion recognition in animated gifs.\nIn ICME.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Yao, X.; She, D.; Zhao, S.; Liang, J.; Lai, Y.-K.; and Yang,",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "J.\n2019. Attention-aware polarity sensitive embedding for",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "affective image retrieval.\nIn ICCV, 1140–1150.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "You, Q.;\nJin, H.; Wang, Z.; Fang, C.; and Luo,\nJ.\n2016.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Image captioning with semantic attention.\nIn CVPR, 4651–",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "4659.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "You, Q.; Jin, H.; and Luo, J. 2017. Visual sentiment analysis",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "by attending on local image regions.\nIn AAAI, 231–237.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Zhan, C.; She, D.; Zhao, S.; Cheng, M.-M.; and Yang,\nJ.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "2019. Zero-shot emotion recognition via affective structural",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "embedding.\nIn ICCV, 1151–1160.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Zhang, H., and Xu, M.\n2018. Recognition of emotions in",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "IEEE TMM\nuser-generated videos with kernelized features.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "20(10):2824–2835.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Zhang, L.; Wang, S.; and Liu, B.\n2018. Deep learning for",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "sentiment analysis: A survey. DMKD 8(4):e1253.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Zhao, S.; Yao, H.; Gao, Y.; Ji, R.; and Ding, G. 2017. Con-",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "tinuous probability distribution prediction of\nimage emo-",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "IEEE TMM\ntions via multi-task shared sparse regression.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "19(3):632–645.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Zhao, S.; Ding, G.; Huang, Q.; Chua, T.-S.; Schuller, B. W.;",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "and Keutzer, K. 2018a. Affective image content analysis: A",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "comprehensive survey.\nIn IJCAI, 5534–5541.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Zhao, S.; Yao, H.; Gao, Y.; Ding, G.; and Chua, T.-S. 2018b.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Predicting personalized image emotion perceptions in social",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "networks.\nIEEE TAFFC 9(4):526–540.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Zhao, S.; Zhao, X.; Ding, G.; and Keutzer, K. 2018c. Emo-",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "tiongan: unsupervised domain adaptation for\nlearning dis-",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "crete probability distributions of image emotions.\nIn ACM",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "MM, 1319–1327.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Zhao, S.; Gholaminejad, A.; Ding, G.; Gao, Y.; Han, J.; and",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "Keutzer, K.\n2019a.\nPersonalized emotion recognition by",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "personality-aware high-order learning of physiological sig-",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        },
        {
          "Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;": "nals. ACM ToMM 15(1s):14.",
          "Zhao, S.; Jia, Z.; Chen, H.; Li, L.; Ding, G.; and Keutzer, K.": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE TAFFC"
    },
    {
      "citation_id": "2",
      "title": "Sentibank: large-scale ontology and classifiers for detecting sentiment and emotions in visual content",
      "authors": [
        "D Borth",
        "T Chen",
        "R Ji",
        "S.-F Chang",
        "F Caba Heilbron",
        "V Escorcia",
        "B Ghanem",
        "Carlos Niebles"
      ],
      "year": "2013",
      "venue": "ACM MM"
    },
    {
      "citation_id": "3",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "4",
      "title": "Predicting perceived emotions in animated gifs with 3d conolutional neural networks",
      "authors": [
        "W Chen",
        "R Picard"
      ],
      "year": "2016",
      "venue": "ISM"
    },
    {
      "citation_id": "5",
      "title": "Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning",
      "authors": [
        "L Chen",
        "H Zhang",
        "J Xiao",
        "L Nie",
        "J Shao",
        "W Liu",
        "T.-S Chua"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "6",
      "title": "Show, observe and tell: Attribute-driven attention model for image captioning",
      "authors": [
        "H Chen",
        "G Ding",
        "Z Lin",
        "S Zhao",
        "J Han"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "7",
      "title": "Emotion in context: Deep semantic feature fusion for video emotion recognition",
      "authors": [
        "C Chen",
        "Z Wu",
        "Y.-G Jiang"
      ],
      "year": "2016",
      "venue": "ACM MM"
    },
    {
      "citation_id": "8",
      "title": "Potion: Pose motion representation for action recognition",
      "authors": [
        "V Choutas",
        "P Weinzaepfel",
        "J Revaud",
        "C Schmid"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "9",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "CVPR"
    },
    {
      "citation_id": "10",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "11",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "PR"
    },
    {
      "citation_id": "12",
      "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet",
      "authors": [
        "K Hara",
        "H Kataoka",
        "Y Satoh"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "13",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "14",
      "title": "Exploiting feature and class relationships in video categorization with regularized deep neural networks",
      "authors": [
        "Y.-G Jiang",
        "Z Wu",
        "J Wang",
        "X Xue",
        "S.-F Chang"
      ],
      "year": "2017",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "15",
      "title": "Predicting emotions in user-generated videos",
      "authors": [
        "Y.-G Jiang",
        "B Xu",
        "X Xue"
      ],
      "year": "2014",
      "venue": "AAAI"
    },
    {
      "citation_id": "16",
      "title": "Predicting viewer perceived emotions in animated gifs",
      "authors": [
        "B Jou",
        "S Bhattacharya",
        "S.-F Chang"
      ],
      "year": "2014",
      "venue": "ACM MM"
    },
    {
      "citation_id": "17",
      "title": "Affective content detection using hmms",
      "authors": [
        "H.-B Kang"
      ],
      "year": "2003",
      "venue": "ACM MM"
    },
    {
      "citation_id": "18",
      "title": "The kinetics human action video dataset",
      "authors": [
        "W Kay",
        "J Carreira",
        "K Simonyan",
        "B Zhang",
        "C Hillier",
        "S Vijayanarasimhan",
        "F Viola",
        "T Green",
        "T Back",
        "P Natsev"
      ],
      "year": "2017",
      "venue": "The kinetics human action video dataset",
      "arxiv": "arXiv:1705.06950"
    },
    {
      "citation_id": "19",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "20",
      "title": "Object bank: A high-level image representation for scene classification & semantic feature sparsification",
      "authors": [
        "L.-J Li",
        "H Su",
        "L Fei-Fei",
        "E Xing"
      ],
      "year": "2010",
      "venue": "NIPS"
    },
    {
      "citation_id": "21",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "Automatic speech emotion recognition using recurrent neural networks with local attention"
    },
    {
      "citation_id": "22",
      "title": "Recurrent models of visual attention",
      "authors": [
        "V Mnih",
        "N Heess",
        "A Graves"
      ],
      "year": "2014",
      "venue": "NIPS"
    },
    {
      "citation_id": "23",
      "title": "Deep multimodal learning for affective analysis and retrieval",
      "authors": [
        "L Pang",
        "S Zhu",
        "C.-W Ngo"
      ],
      "year": "2015",
      "venue": "IEEE TMM"
    },
    {
      "citation_id": "24",
      "title": "Emotion, theory, research, and experience",
      "authors": [
        "R Plutchik",
        "H Kellerman"
      ],
      "year": "1980",
      "venue": "Emotion, theory, research, and experience"
    },
    {
      "citation_id": "25",
      "title": "Three dimensions of emotion",
      "authors": [
        "H Schlosberg"
      ],
      "year": "1954",
      "venue": "Psychological Review"
    },
    {
      "citation_id": "26",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "27",
      "title": "A survey of multimodal sentiment analysis",
      "authors": [
        "M Soleymani",
        "D Garcia",
        "B Jou",
        "B Schuller",
        "S.-F Chang",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "IVC"
    },
    {
      "citation_id": "28",
      "title": "An end-to-end spatio-temporal attention model for human action recognition from skeleton data",
      "authors": [
        "S Song",
        "C Lan",
        "J Xing",
        "W Zeng",
        "J Liu"
      ],
      "year": "2017",
      "venue": "AAAI"
    },
    {
      "citation_id": "29",
      "title": "A multi-task neural approach for emotion attribution, classification and summarization",
      "authors": [
        "G Tu",
        "Y Fu",
        "B Li",
        "J Gao",
        "Y.-G Jiang",
        "X Xue"
      ],
      "year": "2019",
      "venue": "A multi-task neural approach for emotion attribution, classification and summarization"
    },
    {
      "citation_id": "30",
      "title": "Affective understanding in film",
      "authors": [
        "H Wang",
        "L.-F Cheong"
      ],
      "year": "2006",
      "venue": "IEEE TCSVT"
    },
    {
      "citation_id": "31",
      "title": "Temporal segment networks: Towards good practices for deep action recognition",
      "authors": [
        "L Wang",
        "Y Xiong",
        "Z Wang",
        "Y Qiao",
        "D Lin",
        "X Tang",
        "L Van Gool"
      ],
      "year": "2016",
      "venue": "ECCV"
    },
    {
      "citation_id": "32",
      "title": "Cbam: Convolutional block attention module",
      "authors": [
        "S Woo",
        "J Park",
        "J.-Y Lee",
        "So Kweon"
      ],
      "year": "2018",
      "venue": "ECCV"
    },
    {
      "citation_id": "33",
      "title": "Hierarchical affective content analysis in arousal and valence dimensions",
      "authors": [
        "M Xu",
        "C Xu",
        "X He",
        "J Jin",
        "S Luo",
        "Y Rui"
      ],
      "year": "2013",
      "venue": "SIGPRO"
    },
    {
      "citation_id": "34",
      "title": "Heterogeneous knowledge transfer in video emotion recognition, attribution and summarization",
      "authors": [
        "B Xu",
        "Y Fu",
        "Y.-G Jiang",
        "B Li",
        "L Sigal",
        "J Yang",
        "D She",
        "Y.-K Lai",
        "P Rosin",
        "M.-H Yang"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "35",
      "title": "Retrieving and classifying affective images via deep metric learning",
      "authors": [
        "J Yang",
        "D She",
        "Y.-K Lai",
        "M.-H Yang"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "36",
      "title": "Human-centered emotion recognition in animated gifs",
      "authors": [
        "Z Yang",
        "Y Zhang",
        "J Luo"
      ],
      "year": "2019",
      "venue": "ICME"
    },
    {
      "citation_id": "37",
      "title": "Attention-aware polarity sensitive embedding for affective retrieval",
      "authors": [
        "X Yao",
        "D She",
        "S Zhao",
        "J Liang",
        "Y.-K Lai",
        "J Yang"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "38",
      "title": "Image captioning with semantic attention",
      "authors": [
        "Q You",
        "H Jin",
        "Z Wang",
        "C Fang",
        "J Luo"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "39",
      "title": "Visual sentiment analysis by attending on local image regions",
      "authors": [
        "Q You",
        "H Jin",
        "J Luo"
      ],
      "year": "2017",
      "venue": "AAAI"
    },
    {
      "citation_id": "40",
      "title": "Zero-shot emotion recognition via affective structural embedding",
      "authors": [
        "C Zhan",
        "D She",
        "S Zhao",
        "M.-M Cheng",
        "J Yang"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "41",
      "title": "Recognition of emotions in user-generated videos with kernelized features",
      "authors": [
        "H Zhang",
        "M Xu"
      ],
      "year": "2018",
      "venue": "IEEE TMM"
    },
    {
      "citation_id": "42",
      "title": "Deep learning for sentiment analysis: A survey",
      "authors": [
        "L Zhang",
        "S Wang",
        "B Liu"
      ],
      "year": "2018",
      "venue": "DMKD"
    },
    {
      "citation_id": "43",
      "title": "Continuous probability distribution prediction of image emotions via multi-task shared sparse regression",
      "authors": [
        "S Zhao",
        "H Yao",
        "Y Gao",
        "R Ji",
        "G Ding"
      ],
      "year": "2017",
      "venue": "IEEE TMM"
    },
    {
      "citation_id": "44",
      "title": "Affective image content analysis: A comprehensive survey",
      "authors": [
        "S Zhao",
        "G Ding",
        "Q Huang",
        "T.-S Chua",
        "B Schuller",
        "K Keutzer"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "45",
      "title": "Predicting personalized image emotion perceptions in social networks",
      "authors": [
        "S Zhao",
        "H Yao",
        "Y Gao",
        "G Ding",
        "T.-S Chua"
      ],
      "year": "2018",
      "venue": "IEEE TAFFC"
    },
    {
      "citation_id": "46",
      "title": "Emotiongan: unsupervised domain adaptation for learning discrete probability distributions of image emotions",
      "authors": [
        "S Zhao",
        "X Zhao",
        "G Ding",
        "K Keutzer"
      ],
      "year": "2018",
      "venue": "ACM MM"
    },
    {
      "citation_id": "47",
      "title": "Personalized emotion recognition by personality-aware high-order learning of physiological signals",
      "authors": [
        "S Zhao",
        "A Gholaminejad",
        "G Ding",
        "Y Gao",
        "J Han",
        "K Keutzer"
      ],
      "year": "2019",
      "venue": "ACM ToMM"
    },
    {
      "citation_id": "48",
      "title": "Pdanet: Polarity-consistent deep attention network for fine-grained visual emotion regression",
      "authors": [
        "S Zhao",
        "Z Jia",
        "H Chen",
        "L Li",
        "G Ding",
        "K Keutzer"
      ],
      "year": "2019",
      "venue": "ACM MM"
    },
    {
      "citation_id": "49",
      "title": "Cycleemotiongan: Emotional semantic consistency preserved cyclegan for adapting image emotions",
      "authors": [
        "S Zhao",
        "C Lin",
        "P Xu",
        "S Zhao",
        "Y Guo",
        "R Krishna",
        "G Ding",
        "K Keutzer"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "50",
      "title": "Affective computing for large-scale heterogeneous multimedia data: A survey",
      "authors": [
        "S Zhao",
        "S Wang",
        "M Soleymani",
        "D Joshi",
        "Q Ji"
      ],
      "year": "2019",
      "venue": "Affective computing for large-scale heterogeneous multimedia data: A survey"
    },
    {
      "citation_id": "51",
      "title": "Learning deep features for scene recognition using places database",
      "authors": [
        "B Zhou",
        "A Lapedriza",
        "J Xiao",
        "A Torralba",
        "A Oliva"
      ],
      "year": "2014",
      "venue": "NIPS"
    },
    {
      "citation_id": "52",
      "title": "Towards high performance video object detection",
      "authors": [
        "X Zhu",
        "J Dai",
        "L Yuan",
        "Y Wei"
      ],
      "year": "2018",
      "venue": "CVPR"
    }
  ]
}