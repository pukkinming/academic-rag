{
  "paper_id": "2410.04633v1",
  "title": "A Cross-Lingual Meta-Learning Method Based On Domain Adaptation For Speech Emotion Recognition",
  "published": "2024-10-06T21:33:51Z",
  "authors": [
    "David-Gabriel Ion",
    "Răzvan-Alexandru Smădu",
    "Dumitru-Clementin Cercel",
    "Florin Pop",
    "Mihaela-Claudia Cercel"
  ],
  "keywords": [
    "Meta-Learning",
    "Domain Adaptation",
    "Speech Emotion Recognition",
    "Cross-Lingual",
    "Gated Linear Unit",
    "Lateral Inhibition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Best-performing speech models are trained on large amounts of data in the language they are meant to work for. However, most languages have sparse data, making training models challenging. This shortage of data is even more prevalent in speech emotion recognition. Our work explores the model's performance in limited data, specifically for speech emotion recognition. Meta-learning specializes in improving the few-shot learning. As a result, we employ meta-learning techniques on speech emotion recognition tasks, accent recognition, and person identification. To this end, we propose a series of improvements over the multistage meta-learning method. Unlike other works focusing on smaller models due to the high computational cost of meta-learning algorithms, we take a more practical approach. We incorporate a large pre-trained backbone and a prototypical network, making our methods more feasible and applicable. Our most notable contribution is an improved finetuning technique during meta-testing that significantly boosts the performance on out-of-distribution datasets. This result, together with incremental improvements from several other works, helped us achieve accuracy scores of 83.78% and 56.30% for Greek and Romanian speech emotion recognition datasets not included in the training or validation splits in the context of 4-way 5-shot learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition presents significant challenges regarding available data, which is crucial in obtaining a high-performance model. Data availability is a general issue even for speech. Common Voice  [1] , one of the largest publicly available multilingual datasets, only includes 43 languages with more than 100 hours of data, 11 of which have more than 1,000 hours.\n\nGiven this, we focus on the low-data scenario, where techniques such as metalearning  [15]  are suitable. Meta-learning  [19]  encompasses a set of algorithms and techniques that focus on learning how to learn, essentially training models that can generalize on unknown but related data distributions. It is particularly useful for one-shot and few-shot learning.\n\nThis paper focuses on the practical aspect, similar to Hu et al.  [20] ; hence, our work starts from those findings. The authors defined three distinct steps to achieve a high-performance pipeline: pre-training, meta-learning, and finetuning, namely P>M>F. Pre-training usually refers to a self-supervised training step that yields a large pre-trained model. Meta-learning turns our pre-trained backbone into a meta-learner. Finally, in fine-tuning, we train our meta-learner for a few steps on the newly seen data to squeeze better results out of our model at the expense of computational costs.\n\nOur contribution consists of improving each step of this multistage process in at least some regard. We employ a larger backbone structure for the pre-training stage featuring the Wav2Vec2 XLS-R 300M transformer model  [3, 4, 39]  instead of other works  [7, 13]  that employ smaller models. The meta-learning stage is the focus point of our research. We employ a prototypical network  [37]  on top of our model, chosen for its simplicity, performance, and versatility  [20] . We use a feature extractor network as an intermediary between our transformer model and the prototypical network. This network aims to reduce the variable-length matrix outputted by the transformer into a fixed-length vector, which acts as the sample prototype in our meta-learning algorithm. Finally, in the fine-tuning stage, we test the P>M>F method  [20]  and then introduce a novel approach that is twice as fast and obtains a higher accuracy in the context of few-shot learning with minimal data augmentation.\n\nTo summarize, the main contributions of this work are as follows:\n\n-Introduction of a novel meta-testing fine-tuning technique that is both faster and yields better performance; -Analysis of various feature extractors used to produce embeddings employed in the prototypical network; -A study into the performance achieved in speech emotion recognition while training on person identification and accent recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Feng et al.  [13]  introduced a setting employing a siamese network  [22]  as their meta-learning algorithm. Their method used a more traditional approach, with extensive data preprocessing and a smaller model. In contrast, our method leverages the power of a large pre-trained model, namely Wav2Vec2 XLS-R 300M. Chopra et al.  [10]  made a significant stride by employing an optimizationbased model-agnostic meta-learning algorithm, MAML  [15] . Their work involved training on the entire test dataset instead of only 4 to 20 samples, which substantially improved per-sample efficiency. However, our prototypical network would not be suitable for their use case.\n\nCai et al.  [7]  explored a similar approach by adding a two-stage training process. In the first stage, a model was trained on the valence-activation-dominance multi-label classification problem. During the next stage, another model was initialized with the weights resulting from the previous step before starting the training process.\n\nLiu et al.  [25]  employed domain adaptation  [16]  to solve the speech emotion recognition problem. It featured a two-step training process, similar to  [7] , with the difference that the model was initially trained on the source domain instead of a different multi-label classification task on the target domain. With this, the second stage, the actual meta-learning, can begin with part of the model having the weights already partially trained.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Architecture Overview",
      "text": "The overall architecture is shown in Figure  1 , employing several components. The backbone consists of the Wav2Vec2 XLS-R 300M model  [3, 4] , which maps an audio input to a latent representation. Next, the feature extractor generates the embeddings used by the prototypical network. To modify the pre-trained backbone as little as possible, we do not use a parameter-less feature extractor to create the prototypical network embeddings. As Kumar et al.  [24]  showed in their work, it is beneficial to only train the randomly initialized portion of a model, i.e., the feature extractor, before unfreezing the backbone and training the model as a whole. This approach minimizes propagating random noise into the backbone. Optionally, we attach a dataset discriminator to our model to implement domain adversarial training  [16] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Extractors",
      "text": "We compare three feature extractor architectures to determine which approach maximizes the performance. Since the Wav2Vec2 XLS-R 300M model is not trained to receive a CLS token as input, we apply techniques that collapse the resulting variable-length embedding matrix depending on the input length into a fixed-size vector.\n\nMean-FC is one of the most straightforward available feature extractors. It averages the per-channel features into a 1024-dimensional vector, thus eliminating the dynamic component. It is followed by a fully connected layer, which further reduces the size of the embeddings and can be trained using linear probing.\n\nLateral Inhibition  [28, 31]  involves selectively disabling certain values, similar to attention. This layer follows Eq. 1, where Diag(•) takes as input a vector and outputs a diagonal matrix with the main diagonal equal to the input vector, ZeroDiag(•) zeros the elements on the main diagonal of a matrix, Θ represents the Heaviside step function (i.e., it returns 0 for negative inputs, and 1 for positive inputs), and W and b represent trainable parameters. This lateral inhibition layer is followed by a fully connected layer, which reduces the embedding vector's dimensionality. Finally, a mean over the sequence length obtains a fixed-length embedding vector.\n\nGated Linear Unit (GLU)  [12, 36]  has been successfully used to reduce a variable-length matrix into a fixed-length vector  [35] . This layer is also reminiscent of attention; however, unlike lateral inhibition, it does not require the replacement of any gradient function. The implementation of this layer follows Eq. 2. The original implementation used fully connected layers instead of the convolutional layers Conv(•). However, unlike the 1D convolutional layers we use in this case, those do not work for variable lengths. Therefore, the output of this layer is passed through a per-channel max function to reduce the variable sequence length, leaving us with a fixed-length vector.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Glu (X) = Conv",
      "text": "(2)",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Prototypical Network",
      "text": "A prototypical network  [37]  is a parameter-less module that takes an initial set of embeddings in a latent feature space for support (i.e., labeled data) and a query (i.e., unlabeled data) set obtained through a feature extractor. It then computes a class prototype for each class in the support set. The class prototypes are calculated by averaging the embeddings for all K examples of the same class. There are N -class prototypes in the context of N -way K-shot learning.\n\nFinally, examples in the query set are compared with each class prototype using a distance metric, like cosine similarity in our case, to determine the closest match, which determines the assigned label. The embedding function can be transferlearned from another already-trained model. Fine-tuning consists of identifying the classes in the dataset, how close they are to each other using the distance metric, and maximizing the distance between different classes. In this way, we move the embeddings for different class examples far from one another and closer if they are from the same class.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training Procedure",
      "text": "The training procedure follows the P>M>F stages  [20] : pre-training, metalearning, and fine-tuning. We employ the pre-training checkpoint provided by the Hugging Face transformers library  [41]  for the pre-training stage.\n\nMeta-Learning. The general meta-learning procedure consists of supplying labeled examples, called the support set, and unlabeled examples, called the query set, on which we wish to perform inference. We use a prototypical network  [37]  as our meta-learning algorithm. We follow the indications in  [24]  for the training by performing linear probing before training. As a result, we initially trained only our feature extractor for a few epochs before unfreezing the backbone (i.e., the Wav2Vec2 XLS-R 300M) and trained the entire model as a whole.\n\nFine-Tuning. Similar to Hu et al.  [20] , we perform fine-tuning during metatesting to adapt our model to unknown data distributions better. They showed that employing data augmentation in the context of the domain shift between training and testing plays a critical role in achieving good performance. In the speech emotion recognition setting, we use SpecAugment  [32]  for the data augmentation strategy. Our approach is simpler than  [20] , which involved randomly selecting image transformations such as random brightness, contrast, translation, and horizontal flips. This is because SpecAugment masks time and frequency segments from the input audio spectrograms. We experiment with using the support set both as support and query during fine-tuning, as suggested by  [20] . While this improves the performance, it does so at a high computational cost since we evaluate each support example twice because it has different SpecAugment masks each time. Our novel solution utilizes a subset of examples per class as support and the rest as a query. The disadvantage of this approach is it does not work for 1-shot learning. However, it runs twice as fast and achieves even better performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Domain Adversarial Training.",
      "text": "To further improve generalization, we incorporate the domain adversarial network (DANN) approach  [16, 17]  through a dataset discriminator located after the Wav2Vec2 XLS-R embeddings (see Figure  1 ). This discriminator predicts the dataset from which each sample came, as in a multi-class classification problem. Domain generalization is achieved by minimizing the classification loss for this problem for the parameters of the dataset discriminator while maximizing it for the parameters of the backbone. This adversarial objective is implemented by using the gradient reversal layer  [16]  before the discriminator, which reverses the gradient in the backpropagation step and scales it by a constant to avoid the divergence of the backbone's parameters.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Datasets",
      "text": "We train our model on person identification, accent recognition, and speech emotion recognition. Table  1  illustrates the differences between all the datasets described further.\n\nPerson Identification is done on the VoxCeleb2 dataset  [11] . The task is to correctly identify the person speaking in the test audio file, given 1 or 5 reference audio files. We chose this dataset because it features a larger speaker diversity of around 6,000 people from 145 nationalities.\n\nAccent Recognition is done on the Common Voice dataset. To train on this task, we create the classification problem of determining the language-accent pair for an audio file. As shown in Table  2 , we select 15 languages (i.e., English, Romanian, French, Italian, Spanish, Portuguese, Catalan, German, Swedish, Russian, Polish, Galician, Dutch, Ukrainian, and Bulgarian) from the Common Voice dataset, mainly spoken around Europe, and include the languages in the emotion recognition datasets. For this task, we freely sample an accent, such that it is possible that at one meta-training episode, all tasks contain different languages; thus, the problem degenerates into a language recognition problem. We have yet to explore the within-dataset sampling strategy to avoid this issue because most languages do not have an accent associated with them. We select only the language-accent pairs that contain at least 100 audio files and end up with over 200 tasks, the breakdown of which can be viewed in Table  2 .\n\nSpeech Emotion Recognition (SER) is done on a collection of datasets, illustrated in Table  3 , where we use a total of 14 different datasets, among which 10 are used primarily for training, 2 for validation, and 2 for testing. The validation datasets are only used to tune the fine-tuning procedure performed during meta-testing. There is data in seven different languages, split as follows: English, Russian, Persian, Bengali, and German are used during training, German and Greek during validation, and Greek and Romanian during testing. The reasoning is to compare the performance in languages both included or not in the training set. Selecting which datasets to put in the validation and test sets is influenced by the amount of data each dataset contains since most data should be placed in the training set. The smallest datasets in our collection are EmoDB, EmoRO, AESI, and AESDD. EmoDB is a German dataset and putting it in the validation set allows us to test the performance of our approach on a different distribution while sharing a language present in the training set. AESI and AESDD are the only Greek datasets in our collection. We put one in the validation set and the other in the test set in a language not used during training. Lastly, EmoRO is the only dataset in Romanian we have, and placing it in the test set allows us to test on a language absent in the training and validation sets.\n\nFinally, we also use a reduced subset of those datasets when benchmarking the dataset sampling techniques to speed up the training process. This is further referred to as the Reduced SER dataset (see Table  1 ), which includes the RAVDESS  [26] , DUSHA  [23] , SUBESCO  [38] , EmoDB  [5] , and EmoRO  [27]  datasets. The complete collection of datasets is referred to as the All SER dataset.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Data Preprocessing",
      "text": "Due to computing limitations, we filter out all audio files in the SER datasets that are over 9.375s, or the equivalent of 150,000 waveform samples, knowing that Wav2Vec2 XLS-R 300M uses a sampling rate of 16kHz. This removes around 3% of the total number of audio files (i.e., 1,800 files) or about 8.5% of the total audio length (i.e., 3 hours). We do not truncate the data to avoid losing relevant information, such as intonation, at the end of the audio, which would lead to misclassification.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Dataset Sampling",
      "text": "We explore two dataset sampling techniques  [18] : free dataset sampling and within-dataset sampling. The former requires sampling any task from any dataset, whereas the latter implies that only tasks from one dataset are present in a batch at any time. The implications are that with free dataset sampling, people speaking different languages will be sampled in the same batch, which can degenerate into a language classification problem instead of using emotional data for the prediction. In contrast, within-dataset sampling forces the network to use the emotional data instead of the language or other audio characteristics, such as the accent or background noise, since all audio in the same batch comes from the same dataset.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Hyperparameters",
      "text": "We use a prototypical network embedding size of 256, which is the output size of the feature extractors. For the GLU feature extractor, we use two 1D convolutional layers with a stride of 1, a kernel size of 32, and a 1D dropout probability of 0.1. Unless otherwise specified, we use a dropout of 0.5 for all fully connected layers in the feature extractor. In the domain adversarial training setting, we use a GLU feature extractor with the same parameters but an output size equal to the number of training datasets. The domain adaptation parameter λ is set to 0.01.\n\nThe training uses gradient checkpointing with a query size of 12 samples per class and 5 in the support set. We train using the Adam optimizer  [21]  with a constant learning rate of 10 -4 and gradient accumulation to perform a backpropagation step every 20 forward steps. We do linear probing for 5 epochs before unfreezing the backbone and training until overfitting, with a minimum validation loss often reached in 2-4 epochs. Each epoch consists of 1,000 randomly sampled batches, equivalent to 50 gradient descent steps.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation",
      "text": "We test 4-way classification in either 1-shot or 5-shot scenarios. It is noteworthy that random chance accuracy is 25%. Table  3  shows that the training, validation, and test sets contain data from various datasets. For each dataset that includes data in more than one set, we split the list of samples into disjoint sets so there is no data leakage between the splits. This is further helped by the fact that some datasets are only in some splits, such as AESI  [9]  or EmoRO  [27]  being used only in the test split.\n\nComparing meta-learning for speech emotion recognition against other approaches  [7, 10, 13]  is difficult since no standard training or testing methodology exists. Emotion datasets, not necessarily speech-based, generally contain several classes that may or may not match another dataset's emotions. There is a way to convert the multi-class emotion classification problem to binary classification by mapping each emotion by arousal and valence  [14] . However, this eliminates the granularity of the multi-class classification task.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Person Identification And Accent Recognition",
      "text": "The results in Table  4  indicate that the accent recognition task on Common Voice is more effective than on VoxCeleb2. Despite that, even the 5-shot setting results only approach the performance obtained using the emotion datasets on the 1-shot setting, as seen in Table  5 . This indicates the challenge of training a model on a generic or related task and having it generalize well on other tasks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Dataset Sampling",
      "text": "Table  5  shows that the dataset sampling technique affects performance. We notice that within-dataset sampling offers better performance than free dataset sampling. This may be because the sampling technique in testing is always within the dataset, and training in the same setting prepares the model better for this evaluation phase. This result contradicts the findings in  [18] , indicating that the sampling technique depends on the task. The performance proves that training on SER data outperforms person identification and accent recognition; therefore, we use this training objective for the remainder of this work.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Feature Extractors",
      "text": "The overall performance remains relatively the same when testing various feature extractors. However, there is a significant increase in performance for challenging datasets, such as AESI. Detailed results for a training run when using the GLU feature extractor are shown in Table  6 . The results of the ablation study for the feature extractor used are shown in Table  7 . We observe that employing DANN decreases the overall performance slightly. This may be because it requires more careful hyperparameter tuning or harms the information stored in the pre-trained backbone. This finding aligns with Hu et al.  [20] . In either case, more research is required in this direction. We notice high differences between the performances on datasets sharing the same language, ranging from 36.24% on the MELD dataset to 92.76% on the RAVDESS dataset, both in English. This can be explained by the intrinsic difficulty of each task, which is influenced by audio quality, the voiced text, speakers' accents or the ability to convey certain emotions.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Fine-Tuning During Meta-Testing",
      "text": "We initially perform a hyperparameter search over the desired number of optimizer steps and learning rates. For this round, we keep the inner support and query sets equal to the initial support set, with the only difference being the augmentations, as stated in the original work  [20] . The results are in Table  8 .\n\nWe reach the same conclusion as Hu et al.  [20] , where no unique set of hyperparameters yields the best results. For the optimal learning rates, we observe that the performance increases with the number of steps applied, which indicates that our model is undertrained. Due to the high computational cost of training a 300M parameter transformer, we set a maximum number of steps of 25. Regarding the learning rate, since we know the EmoRO dataset is a more difficult one with performances similar to AESI, we choose to continue with the best learning rate for AESI, which is 10 -5 . For our next round of hyperparameter tuning, we deviate from the original algorithm and split the support set into two disjoint sets: the inner support set and the remaining samples representing the query set. At each step, new inner support and query sets are sampled. The results can be seen in Table  9 . A support size of 2 yields the best results on the AESI dataset, which we keep as the optimal configuration. We then evaluate the model's performance on the test set using these settings, yielding a performance shown in Table  10 .\n\nThis process improves the performance on the EmoRO dataset by 4%. We observe the same ascending trend on the AESI dataset, where the more fine-tuning steps we do, the better the performance. Again, the issue with computational resources arises since we need to perform gradient descent during meta-testing. Despite that, the only factor left is determining what trade-off between the number of steps and inference time is acceptable.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Conclusions",
      "text": "We explored various approaches to increasing performance in a low-data regime for speech emotion recognition, using either this task in the multilingual setting or surrogate tasks, namely person identification and accent recognition. We showed that the former approach performs best, especially when using indataset sampling. We have seen that multiple incremental improvements can make a significant difference when applied together. The choice of architecture for the feature extractor, coupled with fine-tuning during meta-testing brought our performance on the EmoRO dataset.\n\nIn future work, we propose to train a model to translate between emotional data. That is, to train a model to generate audio data with different emotions from a base emotion, possibly using CycleGAN  [42] . This would allow us to use the Common Voice dataset, which features data in many other languages and changes the emotion from neutral to any emotion we want. This way, we can train a model on more data in any possible language, not just the datasets already available.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , employing several components.",
      "page": 3
    },
    {
      "caption": "Figure 1: The model architecture consists of the Wav2Vec2 XLS-R 300M backbone, the",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: illustrates the differences between all the datasets",
      "data": [
        {
          "Dataset Name": "",
          "Total": "",
          "Train": "",
          "Test": ""
        },
        {
          "Dataset Name": "VoxCeleb2",
          "Total": "1,092,009",
          "Train": "982,750",
          "Test": "109,259"
        },
        {
          "Dataset Name": "Common Voice 3,685,697",
          "Total": "",
          "Train": "3,678,882",
          "Test": "6,815"
        },
        {
          "Dataset Name": "Reduced SER",
          "Total": "23,882",
          "Train": "22,825",
          "Test": "1,057"
        },
        {
          "Dataset Name": "All SER",
          "Total": "58,074",
          "Train": "35,972",
          "Test": "22,102"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Breakdown of the Common Voice dataset adapted for accent recognition.",
      "data": [
        {
          "Language # Accents # Files": "7"
        },
        {
          "Language # Accents # Files": "24"
        },
        {
          "Language # Accents # Files": "61"
        },
        {
          "Language # Accents # Files": "6"
        },
        {
          "Language # Accents # Files": "4"
        },
        {
          "Language # Accents # Files": "1"
        },
        {
          "Language # Accents # Files": "2"
        },
        {
          "Language # Accents # Files": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: Breakdown of the Common Voice dataset adapted for accent recognition.",
      "data": [
        {
          "Language # Accents # Files": "8"
        },
        {
          "Language # Accents # Files": "6"
        },
        {
          "Language # Accents # Files": "33"
        },
        {
          "Language # Accents # Files": "3"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: Listofalldatasetsusedforspeechemotionrecognition,alongwiththedata",
      "data": [
        {
          "Dataset": "",
          "Language Hours of audio # Files": "",
          "Used in": ""
        },
        {
          "Dataset": "CREMA-D [8]",
          "Language Hours of audio # Files": "5.26",
          "Used in": "✓"
        },
        {
          "Dataset": "IEMOCAP [6]",
          "Language Hours of audio # Files": "5.00",
          "Used in": "✓"
        },
        {
          "Dataset": "MELD [34]",
          "Language Hours of audio # Files": "11.22",
          "Used in": "✓"
        },
        {
          "Dataset": "RAVDESS [26]",
          "Language Hours of audio # Files": "1.28",
          "Used in": "✓"
        },
        {
          "Dataset": "TESS [33]",
          "Language Hours of audio # Files": "1.60",
          "Used in": "✓"
        },
        {
          "Dataset": "DUSHA [23]",
          "Language Hours of audio # Files": "15.44",
          "Used in": "✓"
        },
        {
          "Dataset": "RES-D [2]",
          "Language Hours of audio # Files": "1.48",
          "Used in": "✓"
        },
        {
          "Dataset": "SHEMO [29]",
          "Language Hours of audio # Files": "2.91",
          "Used in": "✓"
        },
        {
          "Dataset": "SUBESCO [38]",
          "Language Hours of audio # Files": "7.83",
          "Used in": "✓"
        },
        {
          "Dataset": "THORSTEN [30]",
          "Language Hours of audio # Files": "1.64",
          "Used in": "✓"
        },
        {
          "Dataset": "AESI [9]",
          "Language Hours of audio # Files": "0.46",
          "Used in": "-"
        },
        {
          "Dataset": "EmoDB [5]",
          "Language Hours of audio # Files": "0.41",
          "Used in": "-"
        },
        {
          "Dataset": "AESDD [40]",
          "Language Hours of audio # Files": "0.69",
          "Used in": "-"
        },
        {
          "Dataset": "EmoRO [27]",
          "Language Hours of audio # Files": "0.18",
          "Used in": "-"
        },
        {
          "Dataset": "Training Set",
          "Language Hours of audio # Files": "33.78",
          "Used in": "✓"
        },
        {
          "Dataset": "Validation Set",
          "Language Hours of audio # Files": "9.58",
          "Used in": "-"
        },
        {
          "Dataset": "Test Set",
          "Language Hours of audio # Files": "12.05",
          "Used in": "-"
        },
        {
          "Dataset": "All aggregated",
          "Language Hours of audio # Files": "55.40",
          "Used in": "✓"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 5: Results on the test sets using the free dataset sampling and within-dataset",
      "data": [
        {
          "4-way 1-shot": "Person",
          "4-way 5-shot": "Person\nIdentification Recognition"
        },
        {
          "4-way 1-shot": "34.25",
          "4-way 5-shot": "43.40"
        },
        {
          "4-way 1-shot": "32.05",
          "4-way 5-shot": "35.30"
        },
        {
          "4-way 1-shot": "43.42",
          "4-way 5-shot": "57.53"
        },
        {
          "4-way 1-shot": "29.67",
          "4-way 5-shot": "33.50"
        },
        {
          "4-way 1-shot": "34.85",
          "4-way 5-shot": "42.43"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 5: Results on the test sets using the free dataset sampling and within-dataset",
      "data": [
        {
          "4-way 1-shot": "Language Free Dataset Within-Dataset Free Dataset Within-Dataset\nSampling",
          "4-way 5-shot": "Sampling"
        },
        {
          "4-way 1-shot": "62.29",
          "4-way 5-shot": "76.60"
        },
        {
          "4-way 1-shot": "35.81",
          "4-way 5-shot": "43.55"
        },
        {
          "4-way 1-shot": "49.05",
          "4-way 5-shot": "60.08"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 6: Accuracy (%) for training on the full SER collection of datasets.",
      "data": [
        {
          "Dataset": "CREMA-D [8]",
          "Language Validation Accuracy Test Accuracy": "79.03"
        },
        {
          "Dataset": "IEMOCAP [6]",
          "Language Validation Accuracy Test Accuracy": "61.60"
        },
        {
          "Dataset": "MELD [34]",
          "Language Validation Accuracy Test Accuracy": "36.24"
        },
        {
          "Dataset": "RAVDESS [26]",
          "Language Validation Accuracy Test Accuracy": "92.76"
        },
        {
          "Dataset": "TESS [33]",
          "Language Validation Accuracy Test Accuracy": "99.97"
        },
        {
          "Dataset": "DUSHA [23]",
          "Language Validation Accuracy Test Accuracy": "65.95"
        },
        {
          "Dataset": "RES-D [2]",
          "Language Validation Accuracy Test Accuracy": "69.25"
        },
        {
          "Dataset": "SHEMO [29]",
          "Language Validation Accuracy Test Accuracy": "89.27"
        },
        {
          "Dataset": "SUBESCO [38]",
          "Language Validation Accuracy Test Accuracy": "93.61"
        },
        {
          "Dataset": "THORSTEN [30]",
          "Language Validation Accuracy Test Accuracy": "98.43"
        },
        {
          "Dataset": "AESI [9]",
          "Language Validation Accuracy Test Accuracy": "49.24"
        },
        {
          "Dataset": "EmoDB [5]",
          "Language Validation Accuracy Test Accuracy": "91.24"
        },
        {
          "Dataset": "AESDD [40]",
          "Language Validation Accuracy Test Accuracy": "-"
        },
        {
          "Dataset": "EmoRO [27]",
          "Language Validation Accuracy Test Accuracy": "-"
        },
        {
          "Dataset": "Overall",
          "Language Validation Accuracy Test Accuracy": "76.35"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 6: Accuracy (%) for training on the full SER collection of datasets.",
      "data": [
        {
          "Feature Extractor Mean Accuracy EmoDB Accuracy AESI Accuracy": "77.74"
        },
        {
          "Feature Extractor Mean Accuracy EmoDB Accuracy AESI Accuracy": "77.64"
        },
        {
          "Feature Extractor Mean Accuracy EmoDB Accuracy AESI Accuracy": "78.15"
        },
        {
          "Feature Extractor Mean Accuracy EmoDB Accuracy AESI Accuracy": "78.13"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 8: Accuracy (%) for the hyperparameter search on the two validation datasets",
      "data": [
        {
          "#\nSteps": "0",
          "Learning Rate\n10−3\n10−4\n10−5\n10−6": "91.24"
        },
        {
          "#\nSteps": "1",
          "Learning Rate\n10−3\n10−4\n10−5\n10−6": "31.60"
        },
        {
          "#\nSteps": "3",
          "Learning Rate\n10−3\n10−4\n10−5\n10−6": "26.03"
        },
        {
          "#\nSteps": "5",
          "Learning Rate\n10−3\n10−4\n10−5\n10−6": "25.80"
        },
        {
          "#\nSteps": "10",
          "Learning Rate\n10−3\n10−4\n10−5\n10−6": "25.88"
        },
        {
          "#\nSteps": "15",
          "Learning Rate\n10−3\n10−4\n10−5\n10−6": "26.12"
        },
        {
          "#\nSteps": "20",
          "Learning Rate\n10−3\n10−4\n10−5\n10−6": "25.62"
        },
        {
          "#\nSteps": "25",
          "Learning Rate\n10−3\n10−4\n10−5\n10−6": "26.35"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 8: Accuracy (%) for the hyperparameter search on the two validation datasets",
      "data": [
        {
          "#\nSteps": "",
          "Learning Rate": "10−3\n10−4\n10−5\n10−6"
        },
        {
          "#\nSteps": "0",
          "Learning Rate": "49.24"
        },
        {
          "#\nSteps": "1",
          "Learning Rate": "29.45"
        },
        {
          "#\nSteps": "3",
          "Learning Rate": "25.15"
        },
        {
          "#\nSteps": "5",
          "Learning Rate": "25.08"
        },
        {
          "#\nSteps": "10",
          "Learning Rate": "25.18"
        },
        {
          "#\nSteps": "15",
          "Learning Rate": "25.18"
        },
        {
          "#\nSteps": "20",
          "Learning Rate": "25.27"
        },
        {
          "#\nSteps": "25",
          "Learning Rate": "25.12"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 8: Accuracy (%) for the hyperparameter search on the two validation datasets",
      "data": [
        {
          "#\nSteps": "",
          "Support Size": "1"
        },
        {
          "#\nSteps": "1",
          "Support Size": "49.20"
        },
        {
          "#\nSteps": "3",
          "Support Size": "51.13"
        },
        {
          "#\nSteps": "5",
          "Support Size": "52.87"
        },
        {
          "#\nSteps": "10",
          "Support Size": "53.27"
        },
        {
          "#\nSteps": "15",
          "Support Size": "54.57"
        },
        {
          "#\nSteps": "20",
          "Support Size": "55.17"
        },
        {
          "#\nSteps": "25",
          "Support Size": "56.17"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 8: Accuracy (%) for the hyperparameter search on the two validation datasets",
      "data": [
        {
          "#\nSteps": "",
          "Support Size": "1"
        },
        {
          "#\nSteps": "1",
          "Support Size": "91.43"
        },
        {
          "#\nSteps": "3",
          "Support Size": "91.87"
        },
        {
          "#\nSteps": "5",
          "Support Size": "91.47"
        },
        {
          "#\nSteps": "10",
          "Support Size": "91.83"
        },
        {
          "#\nSteps": "15",
          "Support Size": "92.63"
        },
        {
          "#\nSteps": "20",
          "Support Size": "92.87"
        },
        {
          "#\nSteps": "25",
          "Support Size": "93.23"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 10: Test sets accuracy (%) using the optimal fine-tuning hyperparameters.",
      "data": [
        {
          "# Steps EmoDB": "0",
          "AESI": "49.41",
          "AESDD EmoRO": "52.53",
          "Avg.": "67.05"
        },
        {
          "# Steps EmoDB": "1",
          "AESI": "51.16",
          "AESDD EmoRO": "53.86",
          "Avg.": "68.85"
        },
        {
          "# Steps EmoDB": "3",
          "AESI": "53.48",
          "AESDD EmoRO": "54.16",
          "Avg.": "70.42"
        },
        {
          "# Steps EmoDB": "5",
          "AESI": "55.02",
          "AESDD EmoRO": "55.12",
          "Avg.": "71.36"
        },
        {
          "# Steps EmoDB": "10",
          "AESI": "56.52",
          "AESDD EmoRO": "55.70",
          "Avg.": "72.34"
        },
        {
          "# Steps EmoDB": "15",
          "AESI": "56.90",
          "AESDD EmoRO": "55.88",
          "Avg.": "72.54"
        },
        {
          "# Steps EmoDB": "20",
          "AESI": "57.38",
          "AESDD EmoRO": "56.06",
          "Avg.": "72.68"
        },
        {
          "# Steps EmoDB": "25",
          "AESI": "58.30",
          "AESDD EmoRO": "56.30",
          "Avg.": "72.96"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "R Ardila",
        "M Branson",
        "K Davis",
        "M Kohler",
        "J Meyer",
        "M Henretty",
        "R Morais",
        "L Saunders",
        "F Tyers",
        "G Weber"
      ],
      "year": "2020",
      "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "2",
      "title": "Open artificial intelligence library for analyzing and identifying emotional shades of human speech",
      "authors": [
        "Artem Amentes",
        "Ilya Lubenets"
      ],
      "year": "2022",
      "venue": "Open artificial intelligence library for analyzing and identifying emotional shades of human speech"
    },
    {
      "citation_id": "3",
      "title": "XLS-R: selfsupervised cross-lingual speech representation learning at scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal",
        "K Singh",
        "P Von Platen",
        "Y Saraf",
        "J Pino",
        "A Baevski",
        "A Conneau",
        "M Auli"
      ],
      "year": "2022",
      "venue": "23rd Annual Conference of the International Speech Communication Association, Interspeech 2022",
      "doi": "10.21437/Interspeech.2022-143"
    },
    {
      "citation_id": "4",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "5",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "6",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "Meta multi-task learning for speech emotion recognition",
      "authors": [
        "R Cai",
        "K Guo",
        "B Xu",
        "X Yang",
        "Z Zhang"
      ],
      "year": "2020",
      "venue": "Meta multi-task learning for speech emotion recognition"
    },
    {
      "citation_id": "8",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "9",
      "title": "The development of the athens emotional states inventory (aesi): collection, validation and automatic processing of emotionally loaded sentences",
      "authors": [
        "T Chaspari",
        "C Soldatos",
        "P Maragos"
      ],
      "year": "2015",
      "venue": "The World Journal of Biological Psychiatry"
    },
    {
      "citation_id": "10",
      "title": "Meta-learning for low-resource speech emotion recognition",
      "authors": [
        "S Chopra",
        "P Mathur",
        "R Sawhney",
        "R Shah"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "J Chung",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Voxceleb2: Deep speaker recognition"
    },
    {
      "citation_id": "12",
      "title": "Language modeling with gated convolutional networks",
      "authors": [
        "Y Dauphin",
        "A Fan",
        "M Auli",
        "D Grangier"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "13",
      "title": "Few-shot learning in emotion recognition of spontaneous speech using a siamese neural network with adaptive sample pair formation",
      "authors": [
        "K Feng",
        "T Chaspari"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Cross-language acoustic emotion recognition: An overview and some tendencies",
      "authors": [
        "S Feraru",
        "D Schuller"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "15",
      "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "authors": [
        "C Finn",
        "P Abbeel",
        "S Levine"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "16",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Y Ganin",
        "V Lempitsky"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "17",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "18",
      "title": "Metaaudio: A few-shot audio classification benchmark",
      "authors": [
        "C Heggan",
        "S Budgett",
        "T Hospedales",
        "M Yaghoobi"
      ],
      "year": "2022",
      "venue": "International Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "19",
      "title": "Meta-learning in neural networks: A survey",
      "authors": [
        "T Hospedales",
        "A Antoniou",
        "P Micaelli",
        "A Storkey"
      ],
      "year": "2021",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "20",
      "title": "Pushing the limits of simple pipelines for few-shot learning: External data and fine-tuning make a difference",
      "authors": [
        "S Hu",
        "D Li",
        "J Stühmer",
        "M Kim",
        "T Hospedales"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "21",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "22",
      "title": "Siamese neural networks for oneshot image recognition",
      "authors": [
        "G Koch",
        "R Zemel",
        "R Salakhutdinov"
      ],
      "year": "2015",
      "venue": "ICML deep learning workshop"
    },
    {
      "citation_id": "23",
      "title": "Large raw emotional dataset with aggregation mechanism",
      "authors": [
        "V Kondratenko",
        "A Sokolov",
        "N Karpov",
        "O Kutuzov",
        "N Savushkin",
        "F Minkin"
      ],
      "year": "2022",
      "venue": "Large raw emotional dataset with aggregation mechanism",
      "arxiv": "arXiv:2212.12266"
    },
    {
      "citation_id": "24",
      "title": "Fine-tuning can distort pretrained features and underperform out-of-distribution",
      "authors": [
        "A Kumar",
        "A Raghunathan",
        "R Jones",
        "T Ma",
        "P Liang"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "25",
      "title": "Speech emotion recognition based on meta-transfer learning with domain adaption",
      "authors": [
        "Z Liu",
        "B Wu",
        "M Han",
        "W Cao",
        "M Wu"
      ],
      "year": "2023",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "26",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition for emergency services",
      "authors": [
        "B Marghescu",
        "Ş Toma",
        "L Morogan",
        "I Bica"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)"
    },
    {
      "citation_id": "28",
      "title": "Improving romanian bioner using a biologically inspired system",
      "authors": [
        "M Mitrofan",
        "V Păis"
      ],
      "year": "2022",
      "venue": "Proceedings of the 21st Workshop on Biomedical Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Shemo: a large-scale validated database for persian speech emotion detection",
      "authors": [
        "Mohamad Nezami",
        "O Jamshid Lou",
        "P Karami"
      ],
      "year": "2019",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "30",
      "title": "Thorsten-voice -\"thorsten-21.06-emotional",
      "authors": [
        "D Müller Thorsten"
      ],
      "year": "2021",
      "venue": "Thorsten-voice -\"thorsten-21.06-emotional",
      "doi": "10.5281/zenodo.5525023."
    },
    {
      "citation_id": "31",
      "title": "Racai at semeval-2022 task 11: Complex named entity recognition using a lateral inhibition mechanism",
      "authors": [
        "V Păis"
      ],
      "year": "2022",
      "venue": "Proceedings of the 16th international workshop on semantic evaluation (SemEval-2022)"
    },
    {
      "citation_id": "32",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Specaugment: A simple data augmentation method for automatic speech recognition"
    },
    {
      "citation_id": "33",
      "title": "Toronto emotional speech set (TESS",
      "authors": [
        "M Pichora-Fuller",
        "K Dupuis"
      ],
      "year": "2020",
      "venue": "Toronto emotional speech set (TESS",
      "doi": "10.5683/SP2/E8H2MF"
    },
    {
      "citation_id": "34",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "35",
      "title": "Classifying sequences of extreme length with constant memory applied to malware detection",
      "authors": [
        "E Raff",
        "W Fleshman",
        "R Zak",
        "H Anderson",
        "B Filar",
        "M Mclean"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "36",
      "title": "Glu variants improve transformer",
      "authors": [
        "N Shazeer"
      ],
      "year": "2020",
      "venue": "Glu variants improve transformer"
    },
    {
      "citation_id": "37",
      "title": "Prototypical networks for few-shot learning. Advances in neural information processing systems",
      "authors": [
        "J Snell",
        "K Swersky",
        "R Zemel"
      ],
      "year": "2017",
      "venue": "Prototypical networks for few-shot learning. Advances in neural information processing systems"
    },
    {
      "citation_id": "38",
      "title": "Sust bangla emotional speech corpus (subesco): An audio-only emotional speech corpus for bangla",
      "authors": [
        "S Sultana",
        "M Rahman",
        "M Selim",
        "M Iqbal"
      ],
      "year": "2021",
      "venue": "Plos one"
    },
    {
      "citation_id": "39",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "40",
      "title": "Speech emotion recognition for performance interaction",
      "authors": [
        "N Vryzas",
        "R Kotsakis",
        "A Liatsou",
        "C Dimoulas",
        "G Kalliris"
      ],
      "year": "2018",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "41",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations"
    },
    {
      "citation_id": "42",
      "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
      "authors": [
        "J Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    }
  ]
}