{
  "paper_id": "2308.01919v1",
  "title": "Emotion Recognition Based On Multi-Modal Electrophysiology Multi-Head Attention Contrastive Learning",
  "published": "2023-07-12T05:55:40Z",
  "authors": [
    "Yunfei Guo",
    "Tao Zhang",
    "Wu Huang"
  ],
  "keywords": [
    "Emotion recognition",
    "ME",
    "Self-supervised contrast learning",
    "multi-head attention mechanism",
    "Meiosis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is an important research direction in artificial intelligence, helping machines understand and adapt to human emotional states. Multimodal electrophysiological(ME) signals, such as EEG, GSR, respiration(Resp), and temperature(Temp), are effective biomarkers for reflecting changes in human emotions. However, using electrophysiological signals for emotion recognition faces challenges such as data scarcity, inconsistent labeling, and difficulty in cross-individual generalization. To address these issues, we propose ME-MHACL, a self-supervised contrastive learning-based multimodal emotion recognition method that can learn meaningful feature representations from unlabeled electrophysiological signals and use multi-head attention mechanisms for feature fusion to improve recognition performance. Our method includes two stages: first, we use the Meiosis method to group sample and augment unlabeled electrophysiological signals and design a self-supervised contrastive learning task; second, we apply the trained feature extractor to labeled electrophysiological signals and use multihead attention mechanisms for feature fusion. We conducted experiments on two public datasets, DEAP and MAHNOB-HCI, and our method outperformed existing benchmark methods in emotion recognition tasks and had good cross-individual generalization ability.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion recognition  [1]  refers to the use of computational techniques to identify and analyze human emotional states. This technology has applications in a variety of fields, including psychology, medicine, education, and social networking  [2] . In the medical field, emotion recognition can assist physicians in better understanding their patients' emotional states, thereby improving treatment outcomes. Emotion recognition in education can assist teachers in assessing the emotional states of their students, enabling more impactful instruction. In social networking, emotion recognition can help platforms better understand their users' emotional states, leading to improved service provision.\n\nElectrophysiological signals, such as electroencephalogram (EEG), galvanic skin response (GSR)  [3] , respiration rate (Respiration), and body temperature (Temperature), have certain advantages as input data for emotion recognition  [4] . These signals directly reflect physiological states and are closely related to emotional changes  [5] . For instance, when an individual experiences tension or anxiety, their skin resistance decreases, their respiration rate increases, and their body temperature changes  [6] . By measuring these signals, accurate inferences can be made about an individual's emotional state.\n\nHowever, there are also challenges associated with using electrophysiological signals for emotion recognition. Firstly, measuring these signals requires specialized equipment and expertise  [7] , which may increase costs and affect portability. Secondly, electrophysiological signals may be subject to interference or noise from external sources, necessitating preprocessing and filtering to improve signal quality  [8] . Furthermore, the accuracy of emotion recognition may be influenced by physiological variations among individuals.\n\nIn summary, while the use of electrophysiological signals for emotion recognition has certain advantages, it also presents challenges. Future research must explore ways to overcome these challenges in order to improve the accuracy and reliability of emotion recognition.\n\nSelf-supervised contrastive learning  [9]  is an unsupervised learning method that learns the intrinsic structure of data by comparing the similarities and differences between different data samples. This approach can effectively leverage large amounts of unlabeled data to train deep neural networks, thereby improving the generalization and accuracy of the model. In emotion recognition, self-supervised contrastive learning can be used to learn emotion-related feature representations  [10] , providing strong support for subsequent emotion classification  [1] .\n\nMulti-head attention  [11]  is a mechanism for capturing long-range dependencies within sequential data. It computes attention weights between different positions in parallel using multiple \"heads,\" effectively capturing complex patterns in sequential data  [12] . In emotion recognition, multi-head attention arXiv:2308.01919v1 [cs.MM] 12 Jul 2023 can be used to process time-series data such as speech signals, text data, or physiological signals to extract emotion-related features  [13] .\n\nIn summary, self-supervised contrastive learning and multihead attention have great potential for application in emotion recognition. Future research can further explore the use of these techniques in emotion recognition and combine them with other methods to improve the accuracy and reliability of emotion recognition.\n\nThe primary contributions and novelties of this paper encompass: 1) the application of self-supervised contrastive learning to cross-domain experiments with multimodal data fusion, exploring a novel approach to improve the generalization performance of cross-domain learning; 2) the effective extraction of task-based features from multimodal data through multi-head attention mechanisms, providing strong support for applications such as emotion recognition; and 3) the maintenance of good robustness on cross-data and cross-modal data with certain domain differences, illustrates the dependability and feasibility of the suggested approach in real-world scenarios. In summary, by applying self-supervised contrastive learning and multi-head attention mechanisms to multimodal data fusion and feature extraction, this paper provides new insights and methods for research in fields such as emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In previous research, emotion recognition using electrophysiology has been primarily divided into two categories: traditional feature engineering methods and deep learning approaches. Conventional feature engineering methods depend on manually designed features to represent electrophysiological signals. These features often comprise statistical measures (such as mean  [2] , variance  [14] , and skewness  [7] ), frequency domain features  [15]  (e.g., power spectral density), and timefrequency features (e.g., wavelet transform  [16] [17] ). These methods often require domain knowledge and expertise to select appropriate features and may not fully exploit complex patterns in the data. Deep learning approaches automatically extract data representations through multi-layer neural networks  [18] . These methods do not require hand-crafted features and instead train models on large amounts of data to automatically discover relevant patterns. Deep learning methods have achieved significant success in emotion recognition, demonstrating excellent accuracy and robustness  [19] . In summary, emotion recognition methods based on electrophysiological signals include traditional feature engineering approaches and deep learning methods. Each approach has its strengths and weaknesses, and the choice of method depends on the specific application scenario and data characteristics.\n\nIn related work on electrophysiological data, researchers have often achieved performance improvements through the use of self-supervised contrastive learning algorithms, data augmentation, and targeted loss functions. Data augmentation is a technique used to expand the dataset by artificially generating new data samples through methods such as rotation, flipping, cropping, or adding noise  [20] . In self-supervised contrastive learning, data augmentation can be used to generate positive and negative samples to help the model learn the intrinsic structure of the data. Meiosis  [21]  is a geneticsinspired data augmentation method for the proposed Selfsupervised Group Meiosis Contrastive learning  [1] (SGMC) framework for emotion recognition. It leverages the alignment of stimuli between a set of EEG samples to generate augmented groups through pairing, crossover exchange and separation. The role of meiosis in self-supervised learning is to increase the meaningful difficulty for the model to decode EEG signal samples and mix signals from different subjects while preserving original stimulus-related features for SGMC extraction. Meiosis facilitates diversity exploitation of group composition through random pairing for crossover and separation. Overall, meiosis plays a crucial role in improving the performance of SGMC emotion recognition models, especially in label-scarce scenarios.\n\nAn important approach is to design loss functions specific to a given task, which measure the distance between predicted and actual values based on particular data augmentation methods. Common loss functions include mean squared error  [22] , cross-entropy  [23] , and contrastive loss  [24] . In self-supervised contrastive learning, the loss function is used to guide model optimization to minimize prediction errors  [25] .\n\nIn summary, in the analysis of temporal electrophysiological data, self-supervised contrastive learning provides new ideas and methods for applications such as emotion recognition by combining techniques such as meiosis data augmentation and loss functions.\n\nSince ME signals are inherently long sequential data, it is theoretically feasible to extract cross-modal effective featur es from ME data using multi-head attention mechanisms  [26] . It computes attention weights between different positions in parallel through multiple \"heads\", effectively capturing complex patterns in sequence data. In ME data processing  [27] , multi-head attention can be used to capture local dependencies between different modalities to extract task-relevant features.\n\nIn addition, multi-head attention has the advantages of enhanced expressiveness and improved computational efficiency. Since each \"head\" can learn different attention weights, multihead attention can better express complex patterns in the data. At the same time, since multiple \"heads\" can be computed in parallel  [28] , multi-head attention can also improve computational efficiency.\n\nIn summary, in multimodal data processing, multi-head attention provides new ideas and methods for applications such as emotion recognition by capturing local dependencies, enhancing expressiveness and improving computational efficiency.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Method A. Overall Framework",
      "text": "This paper implements emotion recognition using a multihead attention self-supervised group meiosis contrastive learning framework for ME data based on domain differences. As shown in Fig.  1 , the proposed framework consists of a contrastive learning pre-training stage and a model fine-tuning stage. The pre-training stage includes: a ME group sampler, meiosis data augmentation, a base encoder, a multi-head attention group projector, and a contrastive loss function. First, the ME group sampler generates mini-batch data by sampling from the ME data of the samples; secondly, meiosis is used to split and splice the ME signals of each group to generate two groups of ME signals to construct positive and negative ME signal pairs; thirdly, the base encoder extracts sample-level stimulus-related representations from each ME signal; then, the multi-head attention group projector aggregates the multimodal representations of each group to extract group-level video stimulus cross-modal related representations and maps them to the latent space; finally, the representations mapped to the latent space are optimized through the contrastive loss function for the parameters of the base encoder and group projector to achieve the purpose of minimizing contrastive loss. In the model fine-tuning stage, emotion recognition inference is performed using a pre-trained base encoder and an initialized classifier.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Meiosis Data Augmentation",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Base Encoder",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Me Group Sampler",
      "text": "Extracting features related to video stimuli from ME data and using contrastive learning algorithms for experimentation is challenging in terms of achieving data alignment. Therefore, this paper proposes the use of a ME data group sampler to obtain small batch data  [2] , providing a good foundation for subsequent data representation learning.\n\nFor the processed data, the video sequence number and subject are used as two tensor dimensions of the ME data, where each ME sample is defined as M E s v ∈ R M * C , corresponding to the t-second ME signal recorded when subject s watches a t-second video clip v, where M represents the number of samples and C represents the number of channels used for ME data. To obtain a small batch of data, as shown in Fig. ??, the ME data sampler first randomly samples P video segments v 1 , v 2 ,..., i.e. v P that has not been sampled in the current epoch. In order to extract two equal sample groups and construct positive pairs for each clip stimulus, the sampler then randomly selects 2Q subjects s 1 , s 2 , ..., s 2Q for grouping. Further, the sampler extracts the ME data corresponding to the selected subjects and video segments, 2PQ samples D = {M E sk vi |i = 1, 2, ...P ; K = 1, 2, ..., 2Q}, recorded respectively by 2Q subjects watching P video segments. In addition, we note that a group of samples\n\nvi } corresponds to video clip vi. In G i , each individual sample shares similar related features. Thus, the sampler will provide P groups of samples G 1 , G 2 , ..., G P corresponding to P different pre-training stimuli.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Meiosis Data Augmentation",
      "text": "Meiosis aims to build positive and negative sample pairs by utilizing the alignment of stimuli in ME groups, expanding a set of samples into two groups to maintain the same stimulusrelated characteristics  [2] .\n\nIn order to increase the difficulty of the model in decoding the meaning of ME signal samples, we hope to mix signals from different subjects. In addition, in order to retain the original stimulus-related features extracted by ME-MHACL, we choose to split and splice signals corresponding to the same stimulus. Therefore, we designed the crossover transformation\n\nIn addition, to take full advantage of the diversity of group combinations, we can randomly pair for crossover and separation. As shown in Fig.  3 , the overall design of meiosis data enhancement is as follows:\n\n• Individual pairing: For one original ME signals group\n\nvi } for crossover.\n\n.., 2Q} can be obtained that sharing the similar group-level stimuli-related features. For the data expansion of ME data grouping samples, we use the following function expression: When meiosis is established, for a minibatch of P group sample ς, 2P group sample ςcan be obtained as follows:\n\nGA i could from a positive pair with GB i , from negative pairs with any other 2(P-1) group samples.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Base Encoder",
      "text": "In order to extract group-level stimulus-related features for contrastive learning, a basic encoder was first designed to extract individual-level stimulus-related features from each individual ME sample. This paper introduces the basic encoder f : R M ×C → R D , which maps individual ME samples X to representations h in a 512-dimensional feature space. Based on the existing model ResNet18-1D [?], the basic encoder is designed as follows:\n\nAs shown in Fig.  4 , it mainly consists of 17 Conv layers with 1D kernels. The first Conv layer has a kernel parallel to the time axis of the ME signal tensor, with a length of 9. Each residual block contains two Conv layers with the same number of kernels and length. In each residual block, the first layer's kernel is parallel to the input ME tensor's time axis, while the second layer's kernel is parallel to the channel axis. For the 8 residual blocks, the kernel lengths decrease from large to small in the order of  15, 15, 11, 11, 7, 3, 3, and 5 . The positions of the max pooling (Maxpool) with a 1D kernel, average pooling (Avgpool) with a 1D kernel, batch normalization (BN), and rectified linear unit (RELU) layers are shown in the figure . \nThrough the basic encoder, for the augmented group sample Gt i , the set of individual-level stimulus-related representations {h 1 , h 2 , . . . , h Q } can be obtained as follows: This collection is used to further extract group-level features. Individual representation can also be used to extract emotional features and classify emotions.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "E. Multi-Head Attention Group Projector",
      "text": "The multi-head attention group projector aims to accurately project stimulus-related representations from ME signals into a latent space to compute the similarity of video clip stimuli. To alleviate the obstacles (fatigue, attention distraction, etc.) when extracting stimulus-related features from individual samples, a group projector was designed to extract group-level features from multiple samples.\n\nA single ME sample set is a disordered matrix set, which lacks a specific extraction method. Most models focus on regular input representation, such as multi-channel images with a fixed order between different channels, and videos with a fixed order between different frames. In the unordered point cloud classification problem, Charles R. Qi  [29]  proposed PointNet, which uses a symmetric function to construct the network, achieving feature extraction of unordered point clouds.\n\nTo reduce the loss of individual features, extraction can be performed by increasing the dimension of individual representations. This paper proposes a basic projector l : R D → R H ,",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Valence-Group Representation",
      "text": "Base encoder",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multi-Head Attention",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multi-Head Attention",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Concat",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Max Pooling",
      "text": "Arousal-Group representation The multi-head attention mechanism is a technique that can capture the local dependencies and global semantic information of input data. It divides the input data into multiple subspaces, calculates attention weights in each subspace, and then concatenates and linearly transforms the outputs of different subspaces to obtain the final output  [30] . The formula for the multi-head attention mechanism is as follows:\n\nwhere each head is computed as:\n\nand Attention is a scaled dot-product attention function:\n\nWhere Q, K, and V represent the query, key, and value matrices, respectively, d head represents the dimension of each subspace, head represents the number of subspaces,\n\ni M E , and W V i M E represent the multimodal learnable representations obtained by transforming the group-level features H t i M E acquired through the basic encoder.\n\nWe employ an 8-head multi-head attention layer for feature fusion to obtain a comprehensive feature representation for emotion prediction. The multi-head attention mechanism takes the ME data as queries, keys, and values, and concatenates the output. In the binary classification tasks based on Arousal and Valence, as well as the quad-classification task introduced in the experiment, the attention of each modality is distributed differently due to task differences. Therefore, the model needs to be trained separately based on different labels to obtain the attention weights of different channels for each modality.\n\nTo ensure a constant output to represent a group sample with any input permutation, one-dimensional maximum pooling (MaxPool1D) is used to aggregate information from each dimension's upgrade representation. As shown in Fig.  5 , Max-Pool1D's 1D kernel is perpendicular to the dimensionally upgraded representation vector. The scan direction of the kernel is parallel to the upgraded representation vector, the step size is 1, and the fill is 0. The MaxPool can extract the maximum value of 4096 feature dimensions from Q-dimensional upgrade representation to obtain group-level feature representation in latent space.\n\nWe denote the group projection as R Q×D → R H . The group representation extracted in the latent space can be obtained through g:\n\nInspired by the above idea, we designed a model suitable for feature extraction of group ME signals using a symmetric function. As shown in Fig.  5 , we designed a multi-head attention group projector composed of a basic projector, a multihead attention module, and a symmetric function MaxPool1D.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "F. Classifier",
      "text": "In the fine-tuning task of emotion classification based on arousal and valence, we use a classifier to extract emotional features from the representations extracted by the basic encoder and predict emotional labels. As shown in Table  1 , the classifier mainly consists of three fully connected layers, with hidden units decreasing from high to low in the order of 512, 256, and 128. The corresponding positions in the figure are ReLU and Dropout set to 0.5.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "G. Contrastive Loss",
      "text": "To measure the similarity of group-level stimulus-related features between two groups of samples, we can calculate the cosine similarity of their group representation vectors. Given the input group samples { Gt i |i = 1, 2, . . . , P ; t ∈ {A, B}},\n\nThe contrastive loss aims to maximize the similarity of group-level representations of two groups sharing the same stimulus label in a positive pair.\n\nwhere γ is an indicator function equaling to 1 if j ̸ = i. Similar to the SimCLR framework  [31] , we use normalized temperature-scaled cross-entropy to define the loss function as follows:\n\nτ is the temperature parameter of softmax. The smaller the loss function is, the larger similarity between me A i and me B j , and the similarity between me A i and othor group representations come from the same minibatch.\n\nFinally, the total loss for an iteration is the average of all contrastive losses for backpropagation as follows:\n\nH. Pre-training Process ME-MHACL pre-training can be performed based on the constructed ME group sampler, meiosis data enhancement, basic encoder, multi-head attention group projector and contrast loss function.\n\nIn pre-training, we first set a number of epochs T 1 , and then iterate over the epochs. In each iteration, we continue to sample P video clips until all video clips are enumerated. In each iteration, the sampler extracts 2P Q ME samples D = {M E s k vi |i = 1, 2, . . . , P ; k = 1, 2, . . . , 2Q} and combines them into groups ς = {G i |i = 1, 2, . . . , P }.\n\nThen, in the Meiosis data augmentation stage, to avoid deceiving the model by recognizing the splitting position, a fixed splitting position c is randomly generated and sent to each Meiosis data of this iteration (1 < c < M -1). The 2Q augmented group samples ς = { Gt i |i = 1, 2, . . . , P ; t ∈ {A, B}} can be obtained through equation  (3) . Further, we extract group-level features by fusing multimodal representations through the multi-head attention mechanism and projecting them into the latent space, obtaining group representations through equations (  4 )-  (8) . Further, we calculate the loss L through equations (  9 )-  (12) . Finally, we minimize the loss L through back propagation to compute gradients for updating the parameters of f and g using an optimizer. The specific steps are summarized in Algorithm 1.\n\nRandomly generate a split position c.  Obtain me = {me t i |i = 1, 2, ..., P ; t ∈ {A, B}} from ς through f and g by (  4 )-(  8 ).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "9:",
      "text": "Calculate loss L by (  9 )-  (12) . 10:\n\nAbate loss L through optimizer updating parameters of f and g.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "11:",
      "text": "until all video clips are enumerated. 12: end for Ensure: base encoder f , throw away group projector g.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "I. Fine-Tuning Process",
      "text": "To achieve excellent emotion classification performance, based on the learned feature representations, we further finetune the model using labeled samples. As shown in Fig.  1 , we perform supervised training for emotion classification on a model composed of an initialized classifier and a basic encoder pre-trained with ME-NHACL.\n\nWe represent the training data as M E and their labels as y. We represent the classifier as c(•). The label y is a categorical variable. For example, if there are 4 emotion categories, y can take 4 values: 0, 1, 2, or 3. We need to predict the emotion category y for each sample X ∈ R M ×C . The pre-trained basic encoder f extracts representations from the raw ME signals X, which are used by the classifier c(•) to extract prediction features to obtain the predicted category y pre = c(f (X)). We apply the cross-entropy function to define the loss function for the emotion classification task and apply an optimizer to minimize the loss function to optimize the model parameters.\n\nFinally, when the loss function converges, we obtain a model for predicting emotion recognition based on ME signals.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experimental",
      "text": "In this section, we introduce the implementation details and experimental evaluation on the DEAP and HCI datasets. In the experiments, we compared ME-MHACL with other existing emotion recognition methods and evaluated its performance under limited labeled sample learning. By visualizing the feature representations learned by ME-MHACL, we explored the reasons for its effectiveness. By evaluating different combinations of hyperparameters, we explored meaningful patterns of the framework. In addition, we verified the rationality of the architecture design through control and ablation experiments.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Implementation Detail",
      "text": "In this section, we elaborate on the implementation details of the data set used in the experiment, data processing, and basic hyperparameters.\n\n1) Dataset: The widely used DEAP dataset  [2]  includes ME signals recorded from 32 subjects while watching 40 oneminute music videos. The ME data of this dataset contains 32-channel EEG signals, 2-channel EOG signals, 2-channel EMG signals, 1-channel GSR signal, 1-channel respiration rate signal, 1-channel respiration belt pneumotachograph signal and 1-channel body temperature change signal, totaling 40 channels of valid data. Each trial data was recorded under rest for 3 seconds and stimulation for 60 seconds. The provider down-sampled the recorded 40-channel ME data to a sampling rate of 128hz and processed it with a band-pass filter in the frequency range of 4-45hz. After watching each video, the subjects were asked to rate each video on a scale of 1 to 9 for emotional arousal, valence, liking and dominance. We used arousal and valence scores for emotion recognition. We set the threshold values for arousal and valence ratings to 5. When the rating value is greater than 5.0, the corresponding ME signal is marked as high arousal or high valence. Otherwise, it is marked as low arousal or low valence. Each ME signal corresponds to two labels of valence and arousal, which can be used to construct 2 or 4 classification tasks. Table  2  shows the age and gender distribution of the samples in this dataset.\n\nThe MAHNOB-HCI dataset  [7]  is an emotional dataset generated by 30 subjects watching 28 movie clips and 28 pictures. This dataset simultaneously collects relevant data with 6 cameras, a head-mounted microphone, an eye tracker and ME sensors. The ME data includes 32-channel EEG data, 3-channel ECG data, 1-channel GSR data on the finger, 1channel skin temperature data (Temp), 1-channel respiration belt pneumotachograph signal (Resp) and 1 channel for marking the state. Each subject rated the Arousal and Valence values of each movie clip on a scale of 1-9. We converted the ratings into continuous values and used them as emotional labels. The ME signals contain valid data from 39 channels and down-sample the data to a sampling rate of 256Hz. We obtained labels in the two dimensions of arousal and valence by binarizing the evaluation values of arousal and valence, and 2) Data Process: On the DEAP dataset, we used a 1-second sliding window to separate the 63s signal of each trial into 63 non-overlapping ME signal segments. To improve accuracy, based on existing work  [2] , we reduced the 3s resting state signal from the 60s emotional stimulation ME signal. In each trial, we averaged the 3s baseline ME signal segment to obtain a 1s average baseline ME signal segment  [1] . The remaining 60 segments minus the average baseline segment become input samples. All samples correspond to a total of 2400 (40 60second videos) repeated 1-second video clips. From the 2400 video clips, in a ratio of 70:15:15, 1680, 320 (actually should be 360) and 320 (actually should be 360) 1-second video clips were randomly divided into three groups. The three groups of video clips watched by the 32 subjects correspond to 53760, 11520 and 11520 (70:15:15) ME data segments, respectively, as training set, test set and validation set.\n\nOn the MAHNOB-HCI dataset, we first scaled and removed baseline drift for each channel of the ME signals  [7] . Similar to the DEAP dataset, we divided the movie videos into 1second windows. Since the lengths of the test videos are different, we split adjacent windows along the time axis from front to back, removing the first three seconds of resting state signals from the 30-second emotional stimulation ME signals.\n\nIn each trial, we averaged the 3s baseline ME signal segment to obtain a 1s average baseline ME signal segment. The remaining 24 segments minus the average baseline segment become input samples. From the 30 samples provided in the dataset, 5 samples without labels were removed. In a ratio of 70:15:15, each sample's 480 data segments with mean baseline removed were divided into 336, 72 and 72 (70:15:15) ME data segments, respectively as training set, test set and validation set.\n\n3) Basic Configuration: In order to accurately evaluate the performance of the pre-training framework for emotion recognition, we used two steps to evaluate the results. First, save the pre-trained model with different epochs. Next, select the model with the highest average accuracy of emotion recognition after 5 fine-tunings. This average accuracy is used as the result for evaluation.\n\nIn order to speed up the sampling, during the pre-training process, we set the five dimensions of the dataset tensor to correspond to video clips, subjects, 1, channels, and sliding window width, respectively. In the fine-tuning process, the first two axes of the dataset, video clips and subjects, are reshaped into a sample axis. The reshaped dataset's axes correspond to samples, 1, channels, and sampling points in turn. In the pre-training task, each epoch traverses each video clip of the dataset. A good pre-training task usually requires training for more than 2000 epochs. In order to reduce workload, we use the validation dataset to adjust the hyperparameters of the ME-MHACL framework and use the test dataset to evaluate the model. Listed in Table  3 , SHAP E tr , SHAP E te , SHAP E val represent the tensor sizes of training test and validation datasets used for pre-training or fine-tuning. Epoch represents an appropriate number of pre-training or fine-tuning epochs to achieve good emotion recognition performance. Batchsize represents the number of samples in a small batch.\n\nThis paper implements experiments using PyTorch  [32]  on an NVIDIA RTX2080ti GPU. The Adam optimizer  [33]  is used to minimize the loss function during the pre-training and fine-tuning processes. We denote the learning rate of the optimizer as lr. During the pre-training and fine-tuning processes, different values are applied for the number of iterations, batch size, temperature parameter τ , learning rate lr, number of video clips per iteration P , number of samples per group Q, and tensor size of the dataset. As shown in Table  II , we list all the hyperparameters used in the two processes on the DEAP and MAHNOB-HCI datasets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Emotion Classification Performance",
      "text": "1) Performance on DEAP: As shown in Table  4 , on the DEAP dataset, due to the pioneering use of ME data, we can only compare with algorithms that use DEAP EEG data alone and algorithms that use EEG and EOG bimodal data. First, ME-MHACL is compared with two state-of-the-art methods on the two emotional dimensions of valence and arousal: MMResLSTM  [34] , which uses multimodal data with residual long short-term memory networks, and ACRNN  [12] , a hybrid network combining recurrent networks with channel attention mechanisms. From Table  II , it can be seen that the proposed ME-MHACL is 2.67% higher than the second highest in the valence dimension and 2.3% higher than the second highest in the arousal dimension. The experimental results demonstrate the effectiveness of ME-MHACL in emotion recognition.\n\nIn order to verify the effectiveness of the proposed framework in the multimodal aspect, we first compared ME-MHACL with MindLink-Eumpy  [15] , which is based on EEG and subject facial images collected from videos. ME-MHACL is 26.14% higher than MindLink-Eumpy in the valence dimension and 34.44% higher than MindLink-Eumpy in the arousal dimension. Secondly, we compared ME-MHACL with DCCA  [35] , which is based on EEG and EOG. ME-MHACL is 12.06% higher than DCCA in the valence dimension and 7.8% higher than DCCA in the arousal dimension. In addition to experiments with binary labels based on Valence and binary labels based on Arousal, we further compared them on a fourcategory classification problem: distinguishing four emotional labels: high valence and high arousal, high valence and low arousal, low valence and high arousal, low valence and high arousal, low valence and high arousal. In the four-category classification experiment, ME-MHACL is 0.84% higher than DCCA. Once again, we compared ME-MHACL with GA-MLP  [18]  based on EEG. ME-MHACL is 5.29% higher than GA-MLP in the valence dimension, 2.42% higher than GA-MLP in the arousal dimension, and 5.83% higher than GA-MLP in the four-category classification experiment. In the dimensions of valence, arousal, and four categories, the average accuracy of ME-MHACL's fine-tuning scheme exceeds that of the self-supervised baseline scheme by 0.67%, 1.33%, and 0.06%, respectively.\n\nComparing the above data, we can find that whether it is the experiment of a single dimension of Arousal or Valence, or the four-category classification experiment, ME-MHACL has better performance than DCCA, proving that ME-MHACL can more effectively extract emotional features that adapt to sample differences in ME data compared to other modal information.\n\nComparing the confusion matrices of the four-category classification experiments of DEAP shown in Fig.  6  for ME-MHACL's fine-tuning structure and self-supervised baseline, ME-MHACL's fine-tuning structure performs better in the case of high arousal and high valence and in the case of low arousal and high valence, while ME-MHACL's self-supervised scheme performs better in the case of high arousal and high valence and in the case of high arousal and low valence. In the case of changes in data channels, ME-MHACL's fine-tuning scheme can still maintain stable inference performance, proving that ME-MHACL has generalization in cross-dataset experiments.\n\n2) Performance on MAHNOB-HCI: As shown in Table  5 , on the MAHNOB-HCI dataset, our proposed ME-MHACL is first compared with TSception  [36] , which is based on single-modal EEG data: ME-MHACL is 33.62% higher than",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Table Iv: Performance Comparison On Deap",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Method",
      "text": "Valence a Arousal b Four c MindLink-Eumpy (2021)  [15]  70.25% 59.00% -MMResLSTM (2019)  [34]  92.87% 92.30% -ARCNN (2019)  [12]  93.72% 93.38% -DCCA (2019)  [35]  84.33% 85.62% 88.51% GA-MLP (2021)  [18]  91 TSception in the valence dimension and 35.28% higher than DCCA in the arousal dimension. Then, ME-MHACL is compared with MINDLINK EUMPY  [15] , which is based on EEG and facial video screenshots: in the valence dimension, ME-MHACL is 16.33% higher than MINDLINK-EUMPY; in the arousal dimension, ME-MHACL is 18.89% higher than MINDLINK-EUMPY. Finally, ME-MHACL is compared with HCNNS-MFB  [19] , which is also based on single-modal EEG data: ME-MHACL is 4.72% higher than HCNNS-MFB in the valence dimension and 5.94% higher than HCNNS-MFB in the arousal dimension. By observing Table  IV , we can see that ME-MHACL's model fine-tuning scheme and self-supervised learning algorithm have a significant advantage over other multimodal algorithms and single-modal algorithms, and ME-MHACL can also achieve an average accuracy of over 93.5% in the four-category classification experiment. In the dimensions of valence, arousal, and four categories, the average accuracy of ME-MHACL's fine-tuning scheme exceeds that of the self-supervised baseline scheme by 0.06%, 0.22%, and 0.11%, respectively.\n\nThus, it can be seen that ME-MHACL can obtain effective representations based on the valence and arousal dimensions in the multimodal data of MAHNOB-HCI. The four-category classification experiment further proves the robustness of ME-MHACL in the challenging task of MAHNOB-HCI.\n\nIn this experiment, the data modalities used in MAHNOB-HCI and DEAP are the same: EEG, GSR, Resp, and Temp. However, the data of MAHNOB-HCI has one less channel of GSR data than that of DEAP. Based on the above crossdataset differences, we conducted a four-category classification experiment based on the dual labels of Valence and Arousal.\n\nComparing the confusion matrices of the four-category classification experiments of MAHNOB-HCI shown in Fig.  7  for ME-MHACL's fine-tuning structure and self-supervised base-",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Table V: Performance Comparison On Mahnob-Hci",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Method",
      "text": "Valence a Arousal b Four c TSception (2021)  [36]  61.27% 60.61% -MindLink-Eumpy (2021)  [15]  78.56% 77.22% -HCNNS-MFB (2022)  [19]  90.17%  line, ME-MHACL's fine-tuning structure performs better in the case of high arousal and high valence and in the case of low arousal and low valence, while ME-MHACL's self-supervised scheme performs better in the case of high arousal and low valence and in the case of low arousal and low valence. In the case of changes in data channels, ME-MHACL's finetuning scheme can still maintain stable inference performance, proving that ME-MHACL has generalization in cross-dataset experiments.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C. The Experiments Based On Data Modes",
      "text": "In this section, we conduct ablation experiments by selecting data modalities. Firstly, we investigate the impact of the number of channels in ME data on the accuracy of emotion recognition. Secondly, we study the effect of the combination of ME data on the accuracy of emotion recognition. Finally, we use attention heatmaps to visualize the attention of each channel data for each modality.   in an average test accuracy of 94.37% and an average loss of 20.77%, and the above results were obtained by averaging over five experiments.\n\nAlthough the overall trend shows that as the number of data modalities increases, the accuracy of emotion recognition remains on an overall upward trend. However, due to the domain specificity of specific tasks, the effectiveness of data is implicitly related to the task, and it is not positively correlated with the number of channels of ME data.\n\n2) Dual mode combination comparison: In this experiment, we tested the binary classification of Valence based on different combinations of ME data from the MAHNOB HCI dataset, including combinations of EEG and temperature, EEG and skin resistance, and EEG and respiration rate.As shown in Fig.  9 , we first compared the accuracy of emotion recognition for fine-tuning and self-supervised learning of three bimodal data combinations in the first row. In the fine-tuning stage, the accuracy of the EEG and temperature combination was 97.06%; the accuracy of the EEG and skin resistance combination was 97.56%; and the accuracy of the EEG and respiration rate combination was 94.5%. In self-supervised learning, the accuracy of the EEG and temperature combination was 97.89%; the accuracy of the EEG and skin resistance combination was 97.11%; and the accuracy of the EEG and respiration rate combination was 95.56%. Then, in the second row, we compared the loss rate of emotion recognition for fine-tuning and self-supervised learning of three bimodal data combinations. In the fine-tuning stage, the loss rate of the EEG and temperature combination was 10.02%; the loss rate of the EEG and skin resistance combination was 9.93%; and the loss rate of the EEG and respiration rate combination was 19.48%. In self-supervised learning, the loss rate of the EEG and temperature combination was 6.87%; the loss rate of the EEG and skin resistance combination was 9.16%; and the loss rate of the EEG and respiration rate combination was 14.73%.\n\nIn the binary classification experiment based on arousal using bimodal data, it also includes combinations of EEG and temperature, EEG and skin resistance, and EEG and respiration rate. As shown in Fig.  10 , firstly, we compared the accuracy of emotion recognition for fine-tuning and selfsupervised learning of three bimodal data combinations in the first row. In the fine-tuning stage, the accuracy of the EEG and temperature combination was 96.39%; the accuracy of the EEG and skin resistance combination was 97.62%; and the accuracy of the EEG and respiration rate combination was  94.39%. In self-supervised learning, the accuracy of the EEG and temperature combination was 97.33%; the accuracy of the EEG and skin resistance combination was 96.94%; and the accuracy of the EEG and respiration rate combination was 95.56%. Then, in the second row, we compared the loss rate of emotion recognition for fine-tuning and self-supervised learning of three bimodal data combinations. In the fine-tuning stage, the loss rate of the EEG and temperature combination was 13.48%; the loss rate of the EEG and skin resistance combination was 9.23%; and the loss rate of the EEG and respiration rate combination was 19.71%. In self-supervised learning, the loss rate of the EEG and temperature combination was 6.87%; the loss rate of the EEG and skin resistance combination was 9.48%; and the loss rate of the EEG and respiration rate combination was 14.7%.\n\nThrough the above experiments, we found that in emotion recognition tasks based on different labels, different combinations of bimodal data have different effects on the emotion recognition of Arousal and valence. Lang et al.  [37]  found that the average value of GSR is related to the level of arousal, and slow breathing is related to relaxation, while irregular rhythm, rapid breathing, and cessation of breathing are related to stronger emotions such as anger or fear. [DEAP] When the subject is in a state of anger or fear, their skin temperature may rise and their breathing rate may increase; when the subject's emotions are relaxed, their breathing may slow down  [2] . The above phenomenon is the origin of the difference in effective representation between different modal electrophysiological data and emotion recognition tasks.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "D. Ablation Experiments Based On Mha",
      "text": "In this section, we will compare the representation performance of tensors using multi-head attention mechanism in group projectors in binary classification and four-class classification tasks.\n\n1) Four-classification T-SNE visualization based on MHA: As shown in Fig.  11 , based on the MAHNOB-HCI dataset, according to the self-supervised four-class task mentioned earlier, the data of four modalities is mapped to a twodimensional plane through the 1800-dimensional features extracted by the multimodal basic encoder using t-SNE  [38] . After 10 samples of self-supervised learning, we selected three sampling results to obtain the three images in the first row. After the 1800-dimensional feature representation processed by the multimodal multi-head attention mechanism, we also sampled three times to obtain the three images in the second row.\n\n2) Two-classification T-SNE visualization based on MHA: As shown in Fig.  12 , based on the MAHNOB-HCI dataset, according to the binary classification task of the Valence label, the data of four modalities is mapped to a two-dimensional plane through the 1800-dimensional features extracted by the multimodal basic encoder using t-SNE. After 10 samples of self-supervised learning, we selected three sampling results to obtain the three images in the first row. After the 1800dimensional feature representation processed by the multimodal multi-head attention mechanism, we also sampled three times to obtain the three images in the second row.\n\nWhether it is a binary classification task based on Arousal or Valence, or a four-class classification task based on twodimensional labels, the multi-head attention module in the multimodal group projector of ME-MHACL not only learns stimulus-related feature representations, but also enables the model to distinguish whether different stimuli come from continuous videos. Without using the multi-head attention module, there are more indistinguishable representations where different emotion labels are mixed together. When using the multi-head attention module, there are fewer feature representations where different emotion labels are mixed together, showing better emotion discrimination ability. This reflects that the multi-head attention module in the multimodal group projector enables ME-MHACL to learn video-level stimulusrelated representations, thereby improving emotion recognition performance.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "V. Conclusion And Prospect",
      "text": "This paper proposes a multimodal emotion recognition method based on multi-head attention mechanism, which can effectively use ME signals for emotion recognition and fully mine the complementary information between electroencephalogram (EEG), skin resistance (GSR), respiration rate (Respiration), and temperature (Temperature). Experiments were conducted on two publicly available multimodal emotion datasets, and the results show that the proposed method outperforms existing benchmark methods in terms of accuracy whether it is fine-tuning or self-supervised, the effect of using the EEG and temperature combination is basically equal to the effect of using the EEG and skin resistance combination. However, the EEG and respiration rate combination has a lower recognition rate based on the valence label and a higher loss rate.\n\nand stability of emotion prediction, and can better distinguish different emotional states.\n\nThere are several aspects of this work that can be further improved and expanded: whether it is fine-tuning or self-supervised, the effect of using the EEG and temperature combination is basically equal to the effect of using the EEG and skin resistance combination. However, the EEG and respiration rate combination has a lower recognition rate based on the valence label and a higher loss rate.\n\nand valence, and in the future, more emotional dimensions, such as dominance, value, expectation, etc., can be considered to more comprehensively describe emotional states. • This paper only uses static emotion labels, and in the future, dynamic emotion labels, such as continuous emotion curves and changing emotion intensity, can be considered to more realistically reflect the process of emotion change.",
      "page_start": 12,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The framework of ME MHAC algorithm. Fig. 1 shows in the model pre-training stage, ME signals corresponding to a video clip stimulus were",
      "page": 3
    },
    {
      "caption": "Figure 1: , the proposed framework consists of a",
      "page": 3
    },
    {
      "caption": "Figure 2: Illustration of small batch sampling. The sampler first samples P video",
      "page": 4
    },
    {
      "caption": "Figure 3: , the overall design of meiosis",
      "page": 4
    },
    {
      "caption": "Figure 3: Illustration of meiotic data enhancement. A group of ME samples",
      "page": 4
    },
    {
      "caption": "Figure 4: , it mainly consists of 17 Conv layers",
      "page": 4
    },
    {
      "caption": "Figure 4: Architectural details of the basic encoder. Conv represents the con-",
      "page": 5
    },
    {
      "caption": "Figure 5: Architecture details of the ME projection unit are as follows. FC layer",
      "page": 5
    },
    {
      "caption": "Figure 5: , we designed a multi-head atten-",
      "page": 6
    },
    {
      "caption": "Figure 6: Four categories of performance comparison using DEAP. (a) is the ME-MHACL fine-tuning scheme quad-classification confusion matrix; (b) is a",
      "page": 10
    },
    {
      "caption": "Figure 8: , fine-tuning",
      "page": 10
    },
    {
      "caption": "Figure 7: Four-class performance comparison using MAHNOB-HCI. (a) is the ME-MHACL fine-tuning scheme quad-classification confusion matrix; (b) is a",
      "page": 10
    },
    {
      "caption": "Figure 8: Four-class performance comparison using MAHNOB-HCI. (a) is the ME-MHACL fine-tuning scheme quad-classification confusion matrix; (b) is a",
      "page": 11
    },
    {
      "caption": "Figure 9: , we first compared the accuracy of emotion recog-",
      "page": 11
    },
    {
      "caption": "Figure 10: , firstly, we compared",
      "page": 11
    },
    {
      "caption": "Figure 9: In the fine-tuning and self-supervised comparison experiment based",
      "page": 11
    },
    {
      "caption": "Figure 10: In the fine-tuning and self-supervised comparison experiment based",
      "page": 12
    },
    {
      "caption": "Figure 11: , based on the MAHNOB-HCI dataset,",
      "page": 12
    },
    {
      "caption": "Figure 12: , based on the MAHNOB-HCI dataset,",
      "page": 12
    },
    {
      "caption": "Figure 11: In the fine-tuning and self-supervised comparison experiment based on the valence label, in the binary classification task based on the valence label,",
      "page": 13
    },
    {
      "caption": "Figure 12: In the fine-tuning and self-supervised comparison experiment based on the valence label, in the binary classification task based on the valence label,",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , the constructedMEgroupsampler,meiosisdataenhancement,ba-",
      "data": [
        {
          "Model\nstructure": "",
          "Parameters of\nlayers": "Layers"
        },
        {
          "Model\nstructure": "Layer 1",
          "Parameters of\nlayers": "fc-classifier1"
        },
        {
          "Model\nstructure": "",
          "Parameters of\nlayers": "bn-fc1"
        },
        {
          "Model\nstructure": "Layer 2",
          "Parameters of\nlayers": "fc-classifier2"
        },
        {
          "Model\nstructure": "",
          "Parameters of\nlayers": "bn-fc2"
        },
        {
          "Model\nstructure": "Layer 3",
          "Parameters of\nlayers": "fc-classifier3"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "MindLink-Eumpy (2021)\n[15]",
          "Valencea": "70.25%",
          "Arousalb": "59.00%",
          "Fourc": "-"
        },
        {
          "Method": "MMResLSTM (2019)\n[34]",
          "Valencea": "92.87%",
          "Arousalb": "92.30%",
          "Fourc": "-"
        },
        {
          "Method": "ARCNN (2019)\n[12]",
          "Valencea": "93.72%",
          "Arousalb": "93.38%",
          "Fourc": "-"
        },
        {
          "Method": "DCCA (2019)\n[35]",
          "Valencea": "84.33%",
          "Arousalb": "85.62%",
          "Fourc": "88.51%"
        },
        {
          "Method": "GA-MLP (2021)\n[18]",
          "Valencea": "91.10%",
          "Arousalb": "91.02%",
          "Fourc": "83.52%"
        },
        {
          "Method": "ME-MHACL (Fully-supervised)",
          "Valencea": "95.72%",
          "Arousalb": "92.11%",
          "Fourc": "89.29%"
        },
        {
          "Method": "ME-MHACL (Fine-tuning)",
          "Valencea": "96.39%",
          "Arousalb": "93.44%",
          "Fourc": "89.35%"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "TSception (2021)\n[36]",
          "Valencea": "61.27%",
          "Arousalb": "60.61%",
          "Fourc": "-"
        },
        {
          "Method": "MindLink-Eumpy (2021)\n[15]",
          "Valencea": "78.56%",
          "Arousalb": "77.22%",
          "Fourc": "-"
        },
        {
          "Method": "HCNNS-MFB (2022)\n[19]",
          "Valencea": "90.17%",
          "Arousalb": "90.17%",
          "Fourc": "-"
        },
        {
          "Method": "ME-MHACL (Fully-supervised)",
          "Valencea": "94.83%",
          "Arousalb": "95.89%",
          "Fourc": "93.61%"
        },
        {
          "Method": "ME-MHACL (Fine-tuning)",
          "Valencea": "94.89%",
          "Arousalb": "96.11%",
          "Fourc": "93.72%"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Self-supervised group meiosis contrastive learning for eeg-based emotion recognition",
      "authors": [
        "H Kan",
        "J Yu",
        "J Huang",
        "Z Liu",
        "H Zhou"
      ],
      "year": "2022",
      "venue": "Self-supervised group meiosis contrastive learning for eeg-based emotion recognition",
      "arxiv": "arXiv:2208.00877"
    },
    {
      "citation_id": "2",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "3",
      "title": "Wearable and automotive systems for affect recognition from physiology",
      "authors": [
        "J Healey"
      ],
      "year": "2000",
      "venue": "Wearable and automotive systems for affect recognition from physiology"
    },
    {
      "citation_id": "4",
      "title": "Multimodal continuous emotion recognition with data augmentation using recurrent neural networks",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian",
        "M Niu",
        "M Yang"
      ],
      "year": "2018",
      "venue": "Multimodal continuous emotion recognition with data augmentation using recurrent neural networks"
    },
    {
      "citation_id": "5",
      "title": "Detecting stress during real-world driving tasks using physiological sensors",
      "authors": [
        "J Healey",
        "R Picard"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on intelligent transportation systems"
    },
    {
      "citation_id": "6",
      "title": "A survey of affect recognition methods: audio, visual and spontaneous expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman",
        "T Huang"
      ],
      "year": "2007",
      "venue": "A survey of affect recognition methods: audio, visual and spontaneous expressions"
    },
    {
      "citation_id": "7",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "8",
      "title": "Combining brains: a survey of methods for statistical pooling of information",
      "authors": [
        "N Lazar",
        "B Luna",
        "J Sweeney",
        "W Eddy"
      ],
      "year": "2002",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "9",
      "title": "A survey on contrastive self-supervised learning",
      "authors": [
        "A Jaiswal",
        "A Babu",
        "M Zadeh",
        "D Banerjee",
        "F Makedon"
      ],
      "year": "2020",
      "venue": "A survey on contrastive self-supervised learning"
    },
    {
      "citation_id": "10",
      "title": "Contig: Selfsupervised multimodal contrastive learning for medical imaging with genetics",
      "authors": [
        "A Taleb",
        "M Kirchler",
        "R Monti",
        "C Lippert"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "12",
      "title": "Eegbased emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Sentiment analysis of student feedback using multi-head attention fusion model of word and context embedding for lstm",
      "authors": [
        "K Sangeetha",
        "D Prabha"
      ],
      "year": "2021",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "14",
      "title": "Affective understanding in film",
      "authors": [
        "H Wang",
        "L.-F Cheong"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on circuits and systems for video technology"
    },
    {
      "citation_id": "15",
      "title": "Mindlink-eumpy: an open-source python toolbox for multimodal emotion recognition",
      "authors": [
        "R Li",
        "Y Liang",
        "X Liu",
        "B Wang",
        "W Huang",
        "Z Cai",
        "Y Ye",
        "L Qiu",
        "J Pan"
      ],
      "year": "2021",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "16",
      "title": "Classification of eeg signals using the wavelet transform",
      "authors": [
        "N Hazarika",
        "J Chen",
        "A Tsoi",
        "A Sergejew"
      ],
      "year": "1997",
      "venue": "Signal processing"
    },
    {
      "citation_id": "17",
      "title": "Identification of clear days from solar irradiance observations using a new method based on the wavelet transform",
      "authors": [
        "D Djafer",
        "A Irbah",
        "M Zaiani"
      ],
      "year": "2017",
      "venue": "Renewable Energy"
    },
    {
      "citation_id": "18",
      "title": "Eeg-based emotion recognition using genetic algorithm optimized multi-layer perceptron",
      "authors": [
        "S Marjit",
        "U Talukdar",
        "S Hazarika"
      ],
      "year": "2021",
      "venue": "2021 International Symposium of Asian Control Association on Intelligent Robotics and Industrial Automation (IRIA)"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition using heterogeneous convolutional neural networks combined with multimodal factorized bilinear pooling",
      "authors": [
        "Y Zhang",
        "C Cheng",
        "S Wang",
        "T Xia"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "20",
      "title": "Self-supervised learning in medicine and healthcare",
      "authors": [
        "R Krishnan",
        "P Rajpurkar",
        "E Topol"
      ],
      "year": "2022",
      "venue": "Nature Biomedical Engineering"
    },
    {
      "citation_id": "21",
      "title": "Meiosis: how could it work?",
      "authors": [
        "N Kleckner"
      ],
      "year": "1996",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "22",
      "title": "Planning to explore via self-supervised world models",
      "authors": [
        "R Sekar",
        "O Rybkin",
        "K Daniilidis",
        "P Abbeel",
        "D Hafner",
        "D Pathak"
      ],
      "year": "2020",
      "venue": "Planning to explore via self-supervised world models"
    },
    {
      "citation_id": "23",
      "title": "Unleashing the power of contrastive self-supervised visual models via contrast-regularized finetuning",
      "authors": [
        "Y Zhang",
        "B Hooi",
        "D Hu",
        "J Liang",
        "J Feng"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "Provable guarantees for self-supervised deep learning with spectral contrastive loss",
      "authors": [
        "J Haochen",
        "C Wei",
        "A Gaidon",
        "T Ma"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "25",
      "title": "Self-supervised learning of pretextinvariant representations",
      "authors": [
        "I Misra",
        "L Maaten"
      ],
      "year": "2020",
      "venue": "Self-supervised learning of pretextinvariant representations"
    },
    {
      "citation_id": "26",
      "title": "Attend and diagnose: Clinical time series analysis using attention models",
      "authors": [
        "H Song",
        "D Rajan",
        "J Thiagarajan",
        "A Spanias"
      ],
      "year": "2018",
      "venue": "Attend and diagnose: Clinical time series analysis using attention models"
    },
    {
      "citation_id": "27",
      "title": "Video sentiment analysis with bimodal information-augmented multihead attention",
      "authors": [
        "T Wu",
        "J Peng",
        "W Zhang",
        "H Zhang",
        "S Tan",
        "F Yi",
        "C Ma",
        "Y Huang"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "28",
      "title": "Multi-head attention based probabilistic vehicle trajectory prediction",
      "authors": [
        "H Kim",
        "D Kim",
        "G Kim",
        "J Cho",
        "K Huh"
      ],
      "year": "2020",
      "venue": "Multi-head attention based probabilistic vehicle trajectory prediction"
    },
    {
      "citation_id": "29",
      "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
      "authors": [
        "C Qi",
        "H Su",
        "K Mo",
        "L Guibas"
      ],
      "year": "2017",
      "venue": "Pointnet: Deep learning on point sets for 3d classification and segmentation"
    },
    {
      "citation_id": "30",
      "title": "Multimodal approach of speech emotion recognition using multi-level multi-head fusion attention-based recurrent neural network",
      "authors": [
        "N.-H Ho",
        "H.-J Yang",
        "S.-H Kim",
        "G Lee"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "31",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "A simple framework for contrastive learning of visual representations"
    },
    {
      "citation_id": "32",
      "title": "Pytorch: An imperative style, highperformance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga",
        "A Desmaison",
        "A Kopf",
        "E Yang",
        "Z Devito",
        "M Raison",
        "A Tejani",
        "S Chilamkurthy",
        "B Steiner",
        "L Fang",
        "J Bai",
        "S Chintala"
      ],
      "year": "2019",
      "venue": "Pytorch: An imperative style, highperformance deep learning library"
    },
    {
      "citation_id": "33",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2017",
      "venue": "Adam: A method for stochastic optimization"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition using multimodal residual lstm network",
      "authors": [
        "J Ma",
        "H Tang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Emotion recognition using multimodal residual lstm network"
    },
    {
      "citation_id": "35",
      "title": "Multimodal emotion recognition using deep canonical correlation analysis",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Multimodal emotion recognition using deep canonical correlation analysis",
      "arxiv": "arXiv:1908.05349"
    },
    {
      "citation_id": "36",
      "title": "Tsception: Capturing temporal dynamics and spatial asymmetry from eeg for emotion recognition",
      "authors": [
        "Y Ding",
        "N Robinson",
        "S Zhang",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Looking at pictures: Affective, facial, visceral, and behavioral reactions",
      "authors": [
        "P Lang",
        "M Greenwald",
        "M Bradley",
        "A Hamm"
      ],
      "year": "1993",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "38",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}