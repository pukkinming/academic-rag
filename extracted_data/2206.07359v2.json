{
  "paper_id": "2206.07359v2",
  "title": "The Emotion Is Not One-Hot Encoding: Learning With Grayscale Label For Emotion Recognition In Conversation",
  "published": "2022-06-15T08:14:42Z",
  "authors": [
    "Joosung Lee"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In emotion recognition in conversation (ERC), the emotion of the current utterance is predicted by considering the previous context, which can be utilized in many natural language processing tasks. Although multiple emotions can coexist in a given sentence, most previous approaches take the perspective of a classification task to predict only a given label. However, it is expensive and difficult to label the emotion of a sentence with confidence or multi-label. In this paper, we automatically construct a grayscale label considering the correlation between emotions and use it for learning. That is, instead of using a given label as a one-hot encoding, we construct a grayscale label by measuring scores for different emotions. We introduce several methods for constructing grayscale labels and confirm that each method improves the emotion recognition performance. Our method is simple, effective, and universally applicable to previous systems. The experiments show a significant improvement in the performance of baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As interest in interactive applications (e.g. chatbots) increases, emotion recognition in conversation becomes more important. Emotions are additional information that can better understand the speaker's state of conversation, and this is used to design an empathic dialogue system  [1, 2, 3] . Emotion also helps provide personalized results, such as social media opinion mining  [4]  and recommendation systems  [5] .\n\nIn Emotion Recognition in Sentence (ERS) studies, a grayscale label is constructed because one utterance can have multiple emotions.  [6]  introduces a grayscale label using emotion lexicons such as NRC  [7]  and Emoticnet  [8] .  [9]  introduces grayscale labels with pre-trained word embeddings. However, since these methods do not consider utterance, there is a limitation that one-hot encoding is mapped to one grayscale label.  [10]  trains the model with grayscale labels through the basic predictor. However, it depends on the basic predictor with poor performance and cannot handle the noise.\n\nIn emotion recognition in conversation (ERC), an utterance has a more sensitive distribution of emotions than in ERS because emotions must be recognized in consideration of both context and utterance. However, most ERC datasets are labeled for only one emotion, and the latest models are trained with labels of one-hot encoding. That is, when the utterance \"yeah, i do\" contains both joy and neutral emotions depending on the context, learning with only one emotion can cause an error. Our paper focuses on solving the limitation that a model is trained with only one emotion, but it is very expensive for humans to relabel multiple emotions in every utterance. Therefore, we construct grayscale labels in several automatic ways. We propose the following methods for constructing grayscale labels: 1) Category 2) Word-Embedding 3) Self 4) Self-Adjust 5) Future-Self. \"Cateogry\" and \"Word-embedding\" are mapping one-hot encodings to grayscale label (i.e. soft-label encoding), which means that training samples with the same emotion are always mapped to the same grayscale label. \"Self\"-type methods utilize a pre-trained self-teacher-model to construct grayscale according to utterances, which are mapped to different grayscale labels regardless of ground-truth emotion. To the best of our knowledge, our work is the first attempt to create multiple grayscale labels in ERC.\n\nWe construct grayscale labels to four ERC datasets: IEMO-CAP, dailydialog, MELD, and EmoryNLP. Experimental results show that RoBERTa-large  [11]  achieves competitive performance with only grayscale labels, which is simple but effective. We also show that grayscale labels can be used in the latest approaches, Psychological  [12] , CoG-BART  [13] , DAG-ERC  [14]  and CoMPM  [15] .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Approach",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Overview",
      "text": "Figure  1  is an overview of our approach to recognizing the emotion of the k-th utterance u k . First, we construct grayscale labels on our training samples in various ways. Then, the model is trained on both the one-hot encoding and the grayscale label. The input is concatenated with the previous context and the current utterance, and special tokens indicate the speaker by prepending each utterance. Also, we prepend the [CLS] token to the input in RoBERTa. The model is trained to predict emotions with the [CLS] token.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Construction Of Grayscale Label",
      "text": "We propose 5 ways to automatically construct grayscale labels. The \"category\" method constructs a grayscale label heuristically. The \"word-embedding\" method constructs a grayscale label using word-embedding. Other methods (i.e. self-methods) construct grayscale using logit of the self-model (i.e. teachermodel).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Category",
      "text": "[6] generates grayscale labels as a strategy based on linguistic resources, which uses words from each sentence and groundtruth together. Inspired by this, using only the ground-truth, we simply divide emotions into sentiment categories and score emotions differently for each category. Categories are shown in Table  2 . The scores (si) of emotions (ei) are as follows:\n\narXiv:2206.07359v2 [cs.CL] 16 Jun 2022 where egt is the the ground-truth emotion. In the category method, emotions included in the same sentiment give positive scores to each other. We calculate the grayscale label g = {g1, g2, ..., g k } (k = the number of class) by normalizing s as follows:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Word-Embedding",
      "text": "Inspired by  [9] , we calculate the similarity between emotions using FastText  [16] , a publicly released word-embedding  1  . Word-embedding refers to the latent representation of a word. Therefore, the similarity between words with similar meanings is high, and the similarity between words with different meanings is low. The score for each emotion is calculated using cosine similarity as follows:\n\nwhere wgt is the embedding vector of the ground-truth emotion. We construct the grayscale label g in two steps. First, if the similarity score is negative, it is changed to 0. Because a negative cosine similarity value indicates a less relevant emotion, the si of the unrelated emotion is changed to 0 linear normalizing. Then, the grayscale label is calculated through normalizing as in Equation  2 . The spelling for each emotion is explained in Table  2 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Self",
      "text": "The \"category\" and \"word-embedding\" methods have a limitation in that grayscale labels are calculated only in the relation to emotions without considering utterances. Also, the wordembedding method has a disadvantage in that the embedding can be similar even if the meaning of the word is different (e.g night and day). So we propose the self-method, which is similar to  [10]  in ERS. The self-grayscale label is constructed from the emotion of the utterance predicted by the self-model (fine-tuned model), which is a kind of distillation  [17] . Since distillation is not our goal, we use a self-model instead of exploring a new teacher-model.\n\nFirst, the teacher-model is trained as a classification task using one-hot encoding. The logit of the pre-trained teachermodel becomes the score, and then the grayscale label is calculated through the softmax as follows:\n\nWhen the student-model is trained, the parameters of the teacher-model are fixed, where the student-model is the final model trained with ERC data  (RoBERTa) . Each training sample has a different grayscale label according to the utterance regardless of the ground-truth emotion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Self-Adjust",
      "text": "The self-method depends on the performance of the self-model. So, for example, if the one-hot encoding of the ground-truth is (1, 0, 0, 0) and the self-grayscale label is (0.3, 0.4, 0.2, 0.1), there is a risk that the model is confused about the best emotion. To alleviate this problem, we propose a self-adjust-method to adjust the self-grayscale label. Figure  2  is an example of the self-adjust-method.\n\nIf ground-truth emotion is different from the maximum probability of the self-grayscale label, we adjust as follows: 1) The self-adjust-grayscale value corresponding to the gold emotion is adjusted to 0.5. 2) Other emotions have a value divided by 0.5 as much as the distribution of self-grayscale labels. The adjusting function is calculated as follows:\n\nwhere gt is the index corresponding to the ground-truth emotion, and g i is the adjusted grayscale label.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Future-Self",
      "text": "We improve the performance with an additional strategy when a self-model is trained. In ERC research, it is common to predict the emotion of an utterance by considering only the past context. If the model captures information in a future context, it cannot predict emotions in real-time during inference. A nonreal-time emotion recognition system has the disadvantage of being difficult to be utilized in other natural language processing tasks. However, it can be helpful in the training phase because the future context has information about the current emotion that affects the listener's response. Therefore, we propose a method to improve the performance of the self-model by using the future context as input. The input is concatenated with the utterances of two future turns, and this model is called the future-self-model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Loss",
      "text": "We train the model on both a one-hot encoding and a grayscale label. The one-hot encoding is used to predict the top one of emotions, and the grayscale label is used to predict the probability distribution reflecting the distance between emotions. Both tasks use cross-entropy loss:\n\nwhere N is the number of training data, k is the number of emotion classes, oi is a one-hot encoding, gi is a grayscale label, and pi is the emotion probability predicted by the model. α is described in Section 3.4.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "In the \"category\" and \"word-embedding\" methods, the model is jointly trained with a one-hot encoding and a grayscale label as Equation  8 . In the self-methods, first, the teacher-model is trained as a classification task using one-hot encoding. Then, the parameters of the teacher-model are frozen and the studentmodel is jointly trained using one-hot encoding and grayscale labels as Equation  8 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training Setup",
      "text": "We use the pre-trained model from the huggingface library 2  .\n\nThe optimizer is AdamW and the learning rate is 1e-6 as an initial value. The learning rate scheduler used for training is get linear schedule with warmup, and the maximum value of 10 is used for the gradient clipping. We select the model with the best performance on the validation set. All experiments are conducted on one A100 GPU.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset And Evaluation",
      "text": "We experiment on four datasets: IEMOCAP  [18] , DailyDialog  [19] , MELD  [20] , and EmoryNLP  [21] . Table  1  shows the statistics of the data. DailyDialog uses 7 classes for training, but we measure Macro-F1 for only 6 classes excluding neutral.\n\nOther datasets are evaluated with weighted average F1.\n\nIn IMEOCAP, the emotional inventory (  6 ) is given as \"happy, sad, angry, excited, frustrated and neutral\". In Daily-Dialog, the emotional inventory (  7 ) is given as \"anger, disgust, fear, joy, surprise, sadness and neutral\". In MELD, the emotional inventory  (7)  is given as \"anger, disgust, sadness, joy, surprise, fear and neutrality\". In EmoryNLP, the emotional inventory  (7)  is given as \"joyful, peaceful, powerful, scared, mad, sad and neutral\". Table  2  shows the sentiment category of each emotion, and the corresponding word-embedding is used.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "Table  3  shows our experimental results. First, our proposed RoBERTa has slightly improved performance by using special tokens compared to the RoBERTa proposed in previous studies. RoBERTa trained with grayscale labels generally improves performance. However, the performance doesn't improve according to the combination of data and grayscale type (i.e. EmoryNLP+(W or S)). In particular, +S negatively affects RoBERTa in EmoryNLP. We infer that +S contains a lot of noise due to the insufficient performance of the self-model in EmoryNLP. +SA proposed to alleviate the noise problem is effective in improving the performance. In RoBERTa, we observe that self-methods are generally superior to +C and +W. +C and +W are very simple and effective, but there is a limit that does not consider utterance. However, the self-methods considering dialogue further improve performance because they are more fine-grained grayscale labels. Because future-self-RoBERTa is inferior to original RoBERTa in IEMOCAP, RoBERTa+FSA was not tested. In other datasets, +FSA outperforms +SA because the future-self-model constructs fine-grained grayscale labels more than the self-model.\n\nWe apply +S and +SA, which were effective in RoBERTa, to other models. In other models, unlike RoBERTa, +FSA is not used because future inputs cannot simply be combined with the models. +S and +SA give a positive signal to the performance of the other models. In addition, unlike RoBERTa, +S also improves model performance on EmoryNLP. Analysis of this should be explored, but we assume that (teacher-) comparative systems learned about EmeryNLP do not have false biases, unlike RoBERTa. Therefore, the proposed grayscale labels can improve the average performance regardless of the model structure.\n\nInstead of focusing on modeling, our research focuses on constructing a grayscale label with the distribution of emotions contained in the utterance. We achieve high performance competitive with the state-of-the-art models just by combining grayscale with RoBERTa. We also show performance improvements by combining grayscale with the original state-of-the-art models. In other words, grayscale labels are easy to effectively combine with other models. However, since grayscale labels have different effects depending on the combination of model and data, it is difficult to select the optimal grayscale label and α for the model through several experiments. Exploring α is shown in the next section.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Performance According To Α",
      "text": "This section demonstrates that the optimal α is determined by a combination of model structure, data, and grayscale type. When RoBERTAa is trained with MELD, the performance according to α for the most effective +FSA and least effective +C is measured and the results are shown in Figure  3 . Since +FSA and +C have the largest difference in effect, it is considered intuitive to compare these two methods.\n\nAs α increases, RoBERTa is trained with more emphasis on the distribution of grayscale labels. α up to the threshold improves the performance, but when it becomes larger than the threshold, the performance decreases. The threshold depends on the type of data and grayscale. We focus on the effect of grayscale labels, not to find the optimal α for each combination. Therefore, α is used as a fixed value (=1) in our experiments. We confirm that α shows similar effects if it is not too small, which is verified through multiple runs. Our future work is to find the optimal α according to the learning environment.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We introduce a novel approach in ERC, which automatically constructs different types of grayscale labels taking into account correlations between emotions. The \"category\" and \"wordembedding\" methods construct a grayscale label based on the ground-truth emotion, while the other methods construct a grayscale label based on the dialogue. However, the self-model reflecting the dialogue may have incorrect information with ground-truth because it leverages the pre-trained model to generate grayscale labels. Therefore we introduce enhanced methods such as SA and FSA. We show competitive performance by learning RoBERTa as a grayscale label. It can also improve the performance of state-of-the-art models.\n\nThe different types of grayscale labels we proposed have different effects depending on the data and model structure. Also, finding the optimal α in the loss Equation 8 should be determined through several experiments. We will explore further in this related future study.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: is an overview of our approach to recognizing the emo-",
      "page": 1
    },
    {
      "caption": "Figure 1: An overview of our framework based on RoBERTa. For each input, a grayscale label is constructed, and the model is trained",
      "page": 2
    },
    {
      "caption": "Figure 2: is an example of the",
      "page": 2
    },
    {
      "caption": "Figure 2: The example of constructing a self-adjust-grayscale",
      "page": 3
    },
    {
      "caption": "Figure 3: Since +FSA and +C",
      "page": 4
    },
    {
      "caption": "Figure 3: Performance of RoBERTa+SA and RoBERTa+C ac-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 3: shows our experimental results. First, our proposed",
      "data": [
        {
          "Dataset": "",
          "dialogues": "train\ndev\ntest",
          "utterance": "train\ndev\ntest"
        },
        {
          "Dataset": "IEMOCAP",
          "dialogues": "108\n12\n31",
          "utterance": "5163\n647\n1623"
        },
        {
          "Dataset": "DailyDialog",
          "dialogues": "11118\n1000\n1000",
          "utterance": "87170\n8069\n7740"
        },
        {
          "Dataset": "MELD",
          "dialogues": "1038\n114\n280",
          "utterance": "9989\n1109\n2610"
        },
        {
          "Dataset": "EmoryNLP",
          "dialogues": "713\n99\n85",
          "utterance": "9934\n1344\n1328"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Models IEMOCAP DailyDialog MELD EmoryNLP W-Avg F1 Macro F1 Micro F1 W-Avg F",
      "venue": "Models IEMOCAP DailyDialog MELD EmoryNLP W-Avg F1 Macro F1 Micro F1 W-Avg F"
    },
    {
      "citation_id": "4",
      "title": "Our results are the average of three runs. The performance of the comparison system is the result of our re-experiment with the published code",
      "venue": "Our results are the average of three runs. The performance of the comparison system is the result of our re-experiment with the published code"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "6",
      "title": "Shallow-to-deep training for neural machine translation",
      "authors": [
        "B Li",
        "Z Wang",
        "H Liu",
        "Y Jiang",
        "Q Du",
        "T Xiao",
        "H Wang",
        "J Zhu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "7",
      "title": "Caire: An end-to-end empathetic chatbot",
      "authors": [
        "Z Lin",
        "P Xu",
        "G Winata",
        "F Siddique",
        "Z Liu",
        "J Shin",
        "P Fung"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "8",
      "title": "SemEval-2019 task 3: EmoContext contextual emotion detection in text",
      "authors": [
        "A Chatterjee",
        "K Narahari",
        "M Joshi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 13th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "9",
      "title": "Emotion-based recommender system for overcoming the problem of information overload",
      "authors": [
        "H Costa",
        "L Macedo"
      ],
      "year": "2013",
      "venue": "Emotion-based recommender system for overcoming the problem of information overload"
    },
    {
      "citation_id": "10",
      "title": "Text emotion distribution learning via multi-task convolutional neural network",
      "authors": [
        "Y Zhang",
        "J Fu",
        "D She",
        "Y Zhang",
        "S Wang",
        "J Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18",
      "doi": "10.24963/ijcai.2018/639"
    },
    {
      "citation_id": "11",
      "title": "Nrc emotion lexicon",
      "authors": [
        "S Mohammad",
        "P Turney"
      ],
      "year": "2013",
      "venue": "National Research Council"
    },
    {
      "citation_id": "12",
      "title": "Emosenticspace: A novel framework for affective commonsense reasoning",
      "authors": [
        "S Poria",
        "A Gelbukh",
        "E Cambria",
        "A Hussain",
        "G.-B Huang"
      ],
      "year": "2014",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "13",
      "title": "Word-level emotion distribution with two schemas for short text emotion classification",
      "authors": [
        "Z Li",
        "H Xie",
        "G Cheng",
        "Q Li"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "14",
      "title": "Label confusion learning to enhance text classification models",
      "authors": [
        "B Guo",
        "S Han",
        "X Han",
        "H Huang",
        "T Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "1907",
      "venue": "CoRR"
    },
    {
      "citation_id": "16",
      "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "J Li",
        "Z Lin",
        "P Fu",
        "W Wang"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "17",
      "title": "Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "S Li",
        "H Yan",
        "X Qiu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "authors": [
        "J Lee",
        "W Lee"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "20",
      "title": "Advances in pre-training distributed word representations",
      "authors": [
        "T Mikolov",
        "E Grave",
        "P Bojanowski",
        "C Puhrsch",
        "A Joulin"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)"
    },
    {
      "citation_id": "21",
      "title": "Knowledge distillation: A survey",
      "authors": [
        "J Gou",
        "B Yu",
        "S Maybank",
        "D Tao"
      ],
      "year": "2021",
      "venue": "International Journal of Computer Vision",
      "doi": "10.1007/s11263-021-01453-z"
    },
    {
      "citation_id": "22",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "23",
      "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "24",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "25",
      "title": "Emotion detection on TV show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "The Workshops of the The Thirty-Second AAAI Conference on Artificial Intelligence"
    }
  ]
}