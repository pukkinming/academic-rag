{
  "paper_id": "2302.02845v1",
  "title": "Audio Representation Learning By Distilling Video As Privileged Information",
  "published": "2023-02-06T15:09:34Z",
  "authors": [
    "Amirhossein Hajavi",
    "Ali Etemad"
  ],
  "keywords": [
    "Deep Learning",
    "Learning Using Privileged Information",
    "Knowledge Distillation",
    "Multi-modal Data",
    "Audio-visual Representation Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deep audio representation learning using multimodal audio-visual data often leads to a better performance compared to uni-modal approaches. However, in real-world scenarios both modalities are not always available at the time of inference, leading to performance degradation by models trained for multi-modal inference. In this work, we propose a novel approach for deep audio representation learning using audiovisual data when the video modality is absent at inference. For this purpose, we adopt teacher-student knowledge distillation under the framework of learning using privileged information (LUPI). While the previous methods proposed for LUPI use softlabels generated by the teacher, in our proposed method we use embeddings learned by the teacher to train the student network. We integrate our method in two different settings: sequential data where the features are divided into multiple segments throughout time, and non-sequential data where the entire features are treated as one whole segment. In the non-sequential setting both the teacher and student networks are comprised of an encoder component and a task header. We use the embeddings produced by the encoder component of the teacher to train the encoder of the student, while the task header of the student is trained using ground-truth labels. In the sequential setting, the networks have an additional aggregation component that is placed between the encoder and task header. We use two sets of embeddings produced by the encoder and aggregation component of the teacher to train the student. Similar to the non-sequential setting, the task header of the student network is trained using groundtruth labels. We test our framework on two different audiovisual tasks, namely speaker recognition and speech emotion recognition. Through these experiments we show that by treating the video modality as privileged information for the main goal of audio representation learning, our method results in considerable improvements over sole audio-based recognition as well as prior works that use LUPI.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "D EEP audio representation learning has recently attracted significant interest, specially in applications such as speaker recognition (SR)  [9] ,  [10] ,  [43] ,  [31] ,  [11]  and speech emotion recognition (SER)  [1] ,  [18] ,  [25] . The goal in deep audio representation learning is to learn embeddings from audio or visual signals, which could be used in retrieving\n\nThe authors would like to thank IMRSV Data Labs for their support of this work. The authors would also like to acknowledge the Natural Sciences and Engineering Research Council of Canada (NSERC) for supporting this research (grant number: CRDPJ 533919-18).\n\nThe Authors of this work are in Electrical and Computer Engineering Department of the Queen's University at Kingston, Canada. (e-mail: {a.hajavi, ali.etemad}@queensu.ca). information such as identity or the emotional state of the speaker. This goal is generally best achieved when multimodal audio-visual inputs are used  [31] ,  [19] ,  [2]  as opposed to when only a single modality of audio or video is used  [9] ,  [10] ,  [43] ,  [23] ,  [24] ,  [11] . Nonetheless, in many realworld scenarios, both modalities may not be simultaneously available at inference, resulting in the inability of the model to perform effectively. To tackle this, we pose the question: \"how can training with both modalities be performed effectively to benefit inference with a single modality?\"\n\nThe study done by Vapnik et al.  [42]  defined information only available during training (and not at inference) as \"privileged information\". They introduced a new learning paradigm called 'learning using privileged information' (LUPI) in which a secondary SVM model trained on the privileged information helped the main SVM to perform better on its task by reducing the complexity of the problem by optimizing the slack variables. This paradigm was later adapted into deep neural networks in  [28] , showing that LUPI can be performed using knowledge distillation techniques proposed by Hinton et al. in  [15] . In their work, a teacher model was trained using the privileged information using a textual modality. The teacher along with the ground-truth labels were then used to train the student model to perform image classification.\n\nIn this study, to perform uni-modal audio representation learning while training with multi-modal audio-visual streams, we propose a novel solution by adopting privileged information and considering video as such. An overview of our method is shown in Figure  1 . Our model is built based on teacherstudent knowledge distillation to allow for the video stream to be learned alongside the main audio modality during training. First, we train a teacher network using the video stream as the privileged information. Next, our student network is trained using audio as input and the ground-truth output labels as well as the video embeddings obtained from the teacher, simultaneously as outputs. By doing so, our student model can perform solely on the audio signals during inference, while having been trained with and benefited from both audio and video modalities during training. We perform extensive experiments using multiple widely used audio-visual datasets to demonstrate the effectiveness of the proposed framework in exploiting the privileged information for audio representation learning.\n\nIn summary, we make the following contributions:\n\n• We propose a new solution for deep audio representation learning for SR and SER that utilizes video as privileged information during the training of the network to improve its performance. • We perform SR and SER experiments on our model and observe considerable improvements versus uni-modal baselines. We also compare our approach to other studies based on privileged information and show that our method performs better for audio representation learning. • We provide an analysis on the impact of the privilege information by adjusting its influence on training the networks.\n\nThe rest of the paper is organized as follows. Section II presents the previous work on privileged information and knowledge distillation. In section III, we present a detailed description of our proposed solution. Afterwards, we describe the performed experiments in detail and report the results in section IV. Finally we conclude the work with a summary and discussion on potential future directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Different approaches have been taken in the literature for learning with the help of privileged information  [1] ,  [5] ,  [6] ,  [7] ,  [33] ,  [35] ,  [41] ,  [29] ,  [40] ,  [30] ,  [32] ,  [37] ,  [26] . We can categorize these studies into two main groups:  (1)  Those that rely on knowledge distillation techniques via teacher-student models for deep representation learning; (2) Those that utilize privileged information without the use of knowledge distillation. In the following, we first review the general concept of knowledge distillation given its relevance in LUPI as well as our method. This is followed by a review of LUPI with and without adopting knowledge distillation. While our work lies in the field of audio representation learning, given the low number of works in the area of using privileged information for audio, we expand our discussion to other modalities to provide a more comprehensive review.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Knowledge Distillation",
      "text": "Knowledge distillation was proposed by Hinton et al.  [15]  to enable smaller machine learning models (referred to as 'student') to learn from larger machine learning models (referred to as 'teacher'). A large number of studies have since explored the use of knowledge distillation for deep representation learning. Seminal works in this area include  [36] ,  [17] ,  [20] ,  [47] ,  [13] ,  [44] ,  [8] ,  [14] . These studies demonstrated that the use of knowledge distillation enables student models to achieve competitive performances to the teachers, while reducing the number of learnable parameters, hence computational load and required memory.\n\nKnowledge distillation in general is performed via two main approaches. The first approach is to use the 'soft labels' obtained from the teacher to train the student  [45] ,  [46] , while the second approach instead relies on the 'embeddings' learned by the teacher to train the student  [36] ,  [17] ,  [20] ,  [47] ,  [13] ,  [44] ,  [8] . In addition to these two approaches, a few other solutions have also been proposed in the literature. For instance, the activation boundaries of the neurons in the teacher were transferred to the student in  [14]  to reduce the training time. In  [22] ,  [34] , knowledge distillation was performed by transferring the attention maps generated by the teacher to the student. This technique helped the student to find the salient areas of the input with the help of the transferred attention maps, which in turn boosted the performance of the student without the need for additional training. In  [4] , instead of relying only on a single embedding, the knowledge from multiple layers of the teacher was used to train the student, boosting the generalizability of the student during inference.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Lupi With Knowledge Distillation",
      "text": "As one of the earliest solutions for LUPI with deep neural networks, the use of teacher-student knowledge distillation was proposed in  [28] . In their work, it was proposed that the teacher can receive its input from the privileged information and its output can be used alongside the ground-truth labels to train the student which receives its input from the main data. A number of recent literature have used similar techniques in their studies  [1] ,  [5] ,  [6] ,  [7] ,  [33] ,  [35] ,  [41] ,  [29] ,  [40] ,  [30] ,  [32] . The main idea behind these methods has been to use a secondary modality for the input of the teacher model as the source of privileged information.\n\nA very limited number of prior works have targeted audio representation learning while considering 'video' as the secondary modality  [1] ,  [30] ,  [32] . In their solutions, the teacher model takes video frames as input and generates soft-labels which are then used as the only source for training the student. The main aim of these studies is to alleviate the need for labeled training data in the main modality (audio) by training the student models using only the soft-labels generated from a secondary modality (video). While these studies successfully achieve their goal of training audio using video as a [self-] supervisory signal, they do not explore the impact of using the secondary modality as privileged information for helping the networks in learning the main modality. However, in this work we aim to take advantage of the secondary modality to boost the performance of the networks alongside the labeled data from the main modality instead of avoiding the use of output labels altogether.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Lupi Without Knowledge Distillation",
      "text": "A number of studies have also taken approaches other than teacher-student frameworks toward utilizing privileged information for training their networks. In the study performed by  [37] , multi-task learning was used for training the model. Their proposed network performs action recognition while also aiming to reconstruct the privileged information from a different modality. The main modality used in their study was the video frames of individuals performing specific actions while for the privileged information, the positions of skeletal joints of individuals in the videos were used. The use of privileged information in this work boosted the performance compared to several baselines. While their proposed method proved beneficial for secondary modalities with limited dimensionality, its integration with secondary modalities with high dimensionality, for instance video frames, has not been explored. In  [26] , dropout masks derived from privileged information were used in order to generalize a DNN. The study used privileged information in the form of image segments obtained from the input to generate heuristic dropout masks. These masks were then applied over the learnt representations of the DNN in order to help the model with generalization. While the method was shown to boost the generalizability of models, the dropout masks are generated using a segment of the original input. Hence this method requires the privileged information to be sourced from the same modality as the original input.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Method",
      "text": "Our objective in this study is to train a deep neural network to learn audio representations using audio-visual data under the condition that the video modality is not available during inference/testing. Our approach, adopts the paradigm of teacher-student knowledge distillation and LUPI to train a network that can handle this condition. In our approach, the student model operates on the audio modality and is trained using two outputs, one from the ground-truth labels and the other from the embeddings obtained by the teacher model which operates on the secondary modality. In this section we describe the different components of our proposed method.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Preliminaries",
      "text": "The training data used for supervised learning often comes in the form of tuples (x i , y i ) i∈{0,..,n} where x i is a vector representation of the training sample and y i is the output label. The aim of the model is to find an optimal network F, that predicts the labels of the test data y i = F(x i ) with the least amount of error. In this type of training the format of data stays the same during training, testing, and deployment of the model. Occasionally, during the training phase we may have access to additional information other than what is available in the test set. Vapnik et al. referred to this kind of information as privileged information and proposed a technique called \"LUPI\" to take advantage of this information for better training machine learning models  [42] . The training of the models through such a paradigm is done using the tuple (x i , x * i , y i ) where x * i represents the privileged information. However, during inference with the trained models, the data would still maintain its previous format and the additional information is not available.\n\nThe LUPI paradigm was first introduced in the context of support vector machine (SVM)  [42] . The model, namely SVM+, was shown to perform better than the classical SVM, i.e. SVM trained without privileged information. The LUPI paradigm can also be implemented using teacher-student knowledge distillation. In this view of the paradigm, the original knowledge distillation method  [15]  is expanded by using the privileged information as the input for the teacher model. Given the tuple (x i , x * i , y i ), the teacher model is trained using the tuples of privileged information and label (x * i , y i ) at first. In the next step, soft-labels s i are obtained for each training sample from the teacher as the predicted output probability, using the privileged information. Finally, any layer L of the student model is trained using the tuples (x i , y i ) and (x i , s i ) concurrently with the final gradient ∇ s calculated as follows:\n\nHere the parameter α is the imitation parameter that determines how much the student model should follow the teacher.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Our Solution",
      "text": "As described earlier, our goal is to develop a framework capable of distilling video data into an audio learner, so that training is improved given the availability of both modalities, while audio alone is used at inference. For this purpose we use teacher-student knowledge distillation in which the teacher network operates on video data, while the embeddings extracted from its intermediate layers are used to help train the student network which operates on audio data. An overview of our method was depicted earlier in Figure  1 . Through the following subsection the details of each component in our approach is presented. It should be noted that similar to most works on audio representation learning, we use spectrograms extracted from the audio signals in our model. However, these spectrograms can be considered in two different formats: nonsequential and sequential. The non-sequential representation of the audio considers the entire spectrogram as one single image, while in the sequential version, the spectrograms are divided into smaller segments across time. Accordingly, we design our model for both approaches (non-sequential and sequential) and present our teacher-student method for both versions in the following subsections.\n\n1) Video Learning: Here we describe the process of video representation learning in our model, which is carried out by the teacher network. The process of training the teacher network is done separately and happens prior to the training of the student network. During the training of the student, the teacher network is frozen and its weights are unchanged.\n\nLet us assume the training samples are represented as a set of tuples (x i , x * i , y i ), where x i is the i th training sample from the audio modality and x * i is the corresponding sample from the video modality. In the non-sequential approach, the teacher network is comprised of an encoder, F Tenc , accompanied by the task header which includes a Fully Connected (FC) layer followed by softmax activation, denoted by F T head . Here F Tenc generates embeddings for each frame of the video from\n\nwhere j is the video frame index. The accompanying F T head then generates the predicted labels of the model. The final output of the teacher model O T is accordingly calculated as\n\nThe final output O T ij comes in the form of a vector with a dimensionality equal to the number of classes considered in by the model. Each index of O T ij contains the calculated probability of x * ij belonging to the class represented by the index.\n\nIn general, when learning audio-video representations, several video frames often correspond to a spectrogram spanning over a period of time. To address this, prior works have proposed the selection of a single frame known as peak frame  [1] ,  [30] ,  [32]  for each given input spectrogram. Inspired by this approach, we select a peak frame embedding in our solution as per the following:\n\nwhere the frame with the highest calculated probability for its class is selected. The class is determined by performing an Argmax on the O T ij by the softmax function. On the other hand, in the sequential approach, we use the entire video as the input for the teacher network. In this version, an aggregation sub-network, F T agg , is added to the teacher network and is placed between the encoder and the task header. Here F Tagg generates an embedding from the output of the encoder using:\n\nwhere E T ij are the encoder embeddings obtained by Eq.2 and N i is the number of frames in each video.\n\nWhen taking the sequential approach, we also divide the input x i of the audio into fixed sized segments x i k where k is the index of the audio segment. As mentioned earlier, spectrograms that span across a period of time often cover multiple frames of video. Therefore, each segment of the audio x i k will be matched with multiple embeddings E T ij . In the sequential version of the teacher network, each frame does not have a probability score for its class and therefore selection of a peak frame is not possible. To address this issue we define the embedding E T i k as the average, Avg, of all of the embeddings in the segment:\n\nwhere r is the number of frames per audio segment. The calculated embedding E T i k is then matched with the audio segment x i k . 2) Audio Learning: In the non-sequential approach the student network takes in the entirety of the audio signal in a single spectrogram. Figure  2  shows the general scheme for the non-sequential version of the proposed method. In this version, the student model is comprised of an encoder, F Senc , and a task header which consists of an FC layer with a softmax activation, denoted by F S head . The encoder extracts embeddings E S i from the input x i using\n\nThe embedding E S i is then given to F S head to predict the output y i . We then define a loss function L E (E S i , E T i peak ), which calculates the distance between the embeddings of the encoder component of the student network and the embedding calculated using Eq.2. We define a second loss function L Y (y i , y i ), which calculates the distance between y i and the ground-truth labels y i . The output layer of the student model is trained using only L Y , whereas the encoder is trained by the gradient ∇ s calculated using\n\nwhere α acts as the imitation parameter. The value of α defines the weight of the gradients while training the encoder component of the student and ranges from 0 to 1.\n\nIn the sequential approach, the audio input of the student model is divided into fixed-sized segments across time. Here, an aggregator sub-network F Sagg is added to the student network between the encoder and the task header. The encoder\n\nThe final embedding E Tagg i is then calculated using\n\nwhere M i is number of audio segments in x i . Lastly, the output of the student model y i is generated using\n\nFigure  3  shows the two possible approaches for the sequential version of the proposed method for training the student networks: (1) distilling video information at the encoder-level Fig.  3 . The proposed framework for learning privileged information through teacher-student distillation. In the sequential setting of our method, the audio signal is divided into multiple same-sized sections throughout time. Each section is then given to the encoder component of the student network. The embeddings generated by the encoder component are then collected and passed onto an aggregator which extracts time dependencies from these embeddings. The teacher network on the other hand generates embedding for each video frame as well as an overall embedding of the entire video. In the encoder-level implementation of our method (left), part of the gradients used for training the encoder of the student network are obtained from L E , which compares the embeddings generated by the encoder of the student network and embeddings generated by the encoder of the teacher network. In aggregator-level implementation of the proposed method (right), both encoder and aggregator components of the student network receive gradients from privileged information through L AG . This loss compares the embeddings generated by the aggregator of the student network with the embeddings generated by the aggregator of the teacher network. In both implementations, the other part of the gradients which are also used for the remainder of the student network are generated using ground truth labels. shown in Figure  3  (left); (2) distilling video information at the aggregator-level shown in Figure  3  (right). In the encoderlevel distillation we use the loss function L E (E S i k , E T i k ) which calculates the loss between the embedding of each audio segment E S i k and the teacher embedding E T i k corresponding to that segment. The encoder component of the student model F Senc is then trained by the gradients calculated using\n\nwhile the aggregator and the output layer of the student network are trained only using gradients calculated by the loss function L Y (y i , y i ).\n\nIn the aggregator-level distillation we use the loss function\n\n), which calculates the distance between the embeddings generated by the aggregator component of the student model and the embedding E Tagg i calculated using Eq.5. The output layer of the student network is train using",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "C. Implementation Details",
      "text": "The networks in the non-sequential version of our solution consist of an encoder and an output layer. We use 3 architectures based on VGG  [38] , ResNet  [12] , and Squeezeand-Excitation (SE) Networks  [16]  to implement both the teacher and student networks. For the first benchmark we use a standard VGG16 network for the teacher and a VGG-based network customized for audio in the student. Table  I  presents the details of the VGG-based student network. In this network each convolutional layer is coupled with a batch-normalization layer. We use a stride length of 1 in the convolutional and batch-normalization layers throughout the network. For the the maxpooling layers we use a filter size of (2, 2) and a stride length of 2. We add 2 FC layers with 512 neurons after the last maxpooling layer. The output of the last FC layer is used as the student embedding E s , described earlier in Section III. We use Rectified Linear Unit (ReLU) as the activation function for the convolutional layers and the last FC layer. Lastly, for the task header we use an FC layer with the number of neurons equal to the number of classes in the experiments. We then use a softmax function for the this layer.\n\nFor the second benchmark we utilize a standard ResNet34 for the teacher network and a ResNet-based model customized for audio, in the student network. Details of the ResNet-based student model are presented in Table  II . The first convolutional layer of the student model uses a filter size of (7, 7) with a stride length of 1. This layer is followed by a maxpooling layer with a filter size of (2, 2) and a stride length of 2. Afterwards, we use residual blocks in the student model. Each block contains 3 sets of coupled convolutional and batchnormalization layers and a shortcut connection that links the input of the block to its output. The first and last convolutional layers in each block have a filter size of (1, 1) and the second convolutional layer has a filter size of (3, 3) with a stride of 1. Each block is then repeated multiple times as shown in Table  II . We add a maxpooling layer with a filter size of (2, 2) and a stride of 2 after each block. The last maxpooling layer is then followed by 2 FC layers, each with 512 neurons and an FC layer in the task header with neurons equal to the number of classes. Similar to the first benchmark, we use ReLU as the activation functions for the convolutional layers and the last FC layer in the encoder, while softmax is used for the FC layer in the task header.\n\nIn the third benchmark we construct the networks using SE blocks. These blocks use the same layer format of the residual blocks with the difference that an SE module is added to each block. The SE module consists of a global pooling layer, which extracts channel information, 2 FC layers with a ReLU activation function in between, and a sigmoid activation function following the FC layers. We implement the teacher network by replacing the residual blocks from a standard ResNet34 network with SE blocks. The student network is implemented by replacing the residual blocks in the network from the second benchmark with SE blocks.\n\nIn the sequential version of our solution, the networks comprise of an encoder component and an aggregator component. Similar to the non-sequential format, we perform our experiments using 3 benchmark networks. For the encoder component of our teacher and student networks we use the teacher and student networks used in the non-sequential version, respectively. However, the task headers of the student networks are removed and the encoder component generates an embedding vector with a size of 512 for each segment of the input. The aggregator component of the networks include 2 BiLSTM layers accompanied by an attention module. Table  III  shows the details of the student networks for the experiments in the sequential version of the proposed method.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets And Data Preparation",
      "text": "The aim of the experiments is to evaluate the change in performance between learning audio representations alone (for audio representation learning) versus using video as privileged information. The experiments are done on two different tasks of SR and SER. We use 3 publicly available datasets, namely VoxCeleb  [31] , Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [27] , and IEMOCAP  [3] .\n\nWe use the audio-video version of the VoxCeleb dataset  [31]  for SR. In this task we aim to identify the speaker of a given utterance among a set of known speakers. This version of the VoxCeleb dataset is comprised of 21,819 audiovisual recordings from 1,211 individuals. We use 70% of the recordings from all the 1,211 individuals for training, 10% for validation, and 20% for testing. We use the spectrogram representations of audio as inputs to the student model. The frequency features are extracted from the audio using Shortterm Fourier Transform (STFT) with a window of 25 ms. The process is repeated across the entire utterance with a window overlap of 10 ms. The duration of the utterances is not the same for all of the recordings. In order to rule out the complications caused by the variable length of the inputs all the recordings are cropped at 5-second durations when training the models, resulting in spectrograms of size 257×500. It should be noted that the original length of the recordings are used for inference. The shorter utterances are padded using repetition to match the desired length. The videos are recorded at a frame-rate of 25 frames per second. Each frame is annotated using automated face detection models, giving the location and boundaries of the face of the speaking person. The size of the boundaries are not equal across the dataset, therefore we crop the images and resize them to a fixed dimension of 224 × 224. For evaluation of the sequential implementation of the proposed method, the recordings are divided into 1-second segments. This results in a sequence of 5 smaller spectrograms with a dimensionality of 257 × 100 for each utterance and 5 sets of frames for each video, with 25 frames in each set.\n\nFor SER we use two datasets, RAVDESS  [27]  and IEMO-CAP  [3] . In this task, we aim to identify the emotional state of the speaker of an utterance and classify that state into different discrete emotion categories namely Sad, Happy, Fearful, Disgusted, Surprised, Angry, and Neutral. The RAVDESS dataset is comprised of recordings from 24 participants. We use cross-validation through leave-one-subject-out for training and validating our method and report the mean accuracy. Participants are asked to deliver a sentence with 7 different emotions in two forms of normal speech and song. Each actor performs a sentence 60 times in normal speech and 44 times in singing voice with one actor exempted from singing. The total number of video recordings used for our experiments is 2,452. The video clips are recorded under controlled conditions without any environmental noise. The length of recordings are fixed to 4 seconds, therefore no additional padding or cutting is performed on the input. The frame rate of the recorded videos is set to 25 frames per second. In most of the frames, the face of the speaker is located in the center of the frame and covers at most 50% of the frame. Therefore, we crop the frames in the center and resize the resulting image to the fixed dimensions of 224 × 224.\n\nLastly, we also use IEMOCAP for evaluating our method on SER. This dataset contains a total of 6 thousand audio-visual recordings performed by 10 individuals. We use 5-fold crossvalidation for evaluation of the proposed method and report the mean accuracy. The recordings contain single improvised or scripted sentences uttered by each actor. Each utterance is annotated by 3 different people and categorized into four emotional categories of Sad, Angry, and Neutral. The length of the recordings are not standard throughout the dataset. Therefor we fix the length of the recordings to 4 seconds by cutting the longer utterances and padding the shorter utterances by repetition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Training Details",
      "text": "The teacher and student networks are trained for 50 epochs on the same dataset. For the optimizer we use Adam optimizer  [21]  with β1 = 0.9 and β2 = 0.99. We use cyclical learning rates  [39]  to train the networks with the initial learning rate of 10 -4 . We choose cyclical learning rates in order to decrease the probability of getting trapped in local minima. All networks are trained on a single Nvidia Titan RTX (24 GB vRAM) GPU with batch size of 32.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Baselines",
      "text": "We compare our method with two re-implemented baselines: (1) \"Soft-label distillation\", which uses the soft-labels generated by the teacher network from video modality to train the student network. This approach has been used in knowledge distillation studies such as  [1] ,  [30] ,  [32] ; In this baseline the parameter α (see Equation  1 ) determines how much the student model should follow the teacher. (  2 ) \"Multitask Learning\", which we described in Section II-C. This approach has been previously used in  [37]  for LUPI to perform action recognition from videos. In this case, the student model is trained using the ground-truth labels and gradients returning from a secondary decoder component which is tasked with generating a representation of the training sample in the feature space of the secondary modality. Here the parameter α represents the weight that is put on the gradients coming from the decoder component.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Performance",
      "text": "Speaker Recognition. We use identification accuracy and equal error rate to evaluate the performance of the student model when the privileged information has been integrated into the framework.\n\nWe exclude other works such as  [26]  for the reason that their proposed method requires the privileged information and original training data to be from the same modality. Figure  4  shows the results of our experiments on the performance student models after using the proposed method and compares it with the baseline methods for different values of α (imitation parameter) and different model architectures. The figure shows that our proposed method has a positive impact on the performance of the student network for all values of α. Moreover, we observe that the baseline methods show a negative impact when α is increased. This shows that while the baseline models exhibit successful performances when video is used to provide supervision to the audio in the absence of ground-truth labels  [1] ,  [30] ,  [32] , they do not provide any benefits for the scenario where supervised training is performed with both modalities but inference is performed only on audio. It can also be observed that our method has the highest impact when α is either 0.5 or 0.6. This indicates that the best performance gain is achieved when the weight of the privileged information distillation on training of the networks is almost equal to that of the ground-truth labels.\n\nTable  IV  shows the result of our experiment for integration of our method in different architectures for speaker identification on the video version of VoxCeleb dataset. We present the accuracy of the teacher Acc T for person identification task using the video modality. This will allow us to investigate the impact of our proposed method on the student for different teachers with varying performances. We also show the accuracy of the student network without distillation of privileged information so that we can better observe the performance gain using the proposed method and compare it to other methods. Lastly, we show the accuracy of the student network after distillation of privileged information and compare it with the student model without distillation (∆Acc S ). As shown by the results, we observe a substantial performance increase in student networks while using the proposed method. The highest performance gain is obtained when using the sequential networks with VGG-based encoders and distillation of privileged information on the aggregator. This is achieved while the accuracy of the teacher compared to the accuracy of the student is at its highest value, i.e., 9.09%. We also observe that the highest performance gain, in comparison with the difference in the performance of teacher and student networks, occurs with the sequential network with a ResNet-based encoder and privileged information distillation on the aggregator.\n\nTable  V  presents the results of our experiments for the benchmark networks for speaker verification on the VoxCeleb1  [31]  standard test set which includes recordings from 40 speakers outside of the training set. We present the error rate of the student model by EER S , before and after distillation, and compare the performance of the student model at these two points by calculating the difference and normalizing it by the EER w/o distillation (∆EER S %). We observe that the highest decrease in the error rate has been obtained by the ResNet-based encoders when the non-sequential implementation is used, indicating that these encoders often benefit more from being trained by the teacher models compared to other networks. Emotion Recognition. We use unweighted accuracy as the metric to evaluate the performance of the networks trained using our method for SER on RAVDESS. We compare our method to the re-implemented baselines described earlier for all the values of α. Figure  5  shows the results of this experiment. We observe that while our method exhibits a positive impact on the performance of the student networks, the baseline methods have a negative impact. This further shows that while the baseline models that utilize the video  as the only source of supervision for learning audio are successful when the ground-truth labels are not present, they do not improve the performance of deep neural networks when training is done using both modalities but only audio is available at inference.\n\nWe also extend out experiments by comparing the performance of the proposed method integrated into different architectures. Table  VI  shows the result of this evaluation. We include the accuracy of the teacher network along with the accuracy of the student before and after distillation. It can be observed that using our method, the performance of the student models improve and the highest increase in the performance is achieved when the non-sequential student network using a SEResNet-based architecture is employed. We can also observe a similar behaviour to that of the previous experiment when comparing our method with the baselines.\n\nLastly we evaluate our method on the IEMOCAP dataset. In this experiment we intend to show the effect of our method in cases where the accuracy of the teacher network is lower than the student network. We compare the proposed method to the baseline methods described earlier for all the values of α. Figure  6  shows the results of this experiment. As shown in the figure, when using the proposed method, the performance of the student networks are not negatively effected by weaker teacher networks for low values of α, whereas in the baseline methods, the negative impact is observed from very early values of α. Table  VII  shows the performance of our method integrated into different architectures. We observe that the best performance is achieved using the SEResNet architecture equipped with BiLSTM layers, and despite the teacher having a lower performance than the student, the proposed method does not negatively affect the performance of this network.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "V. Conclusion And Future Work",
      "text": "We use teacher-student knowledge distillation for LUPI in order to take advantage of both audio and video inputs for training deep neural networks, while only using audio at inference. In our framework, embeddings are first extracted from the video input using a teacher model. The embeddings alongside the ground-truth labels are then used to train the  student. We integrate our method in two different settings for non-sequential and sequential data. In the non-sequential setting, both the teacher and student networks are constructed using and encoder and a task header. We use the embeddings generated by the encoder of the teacher to train the encoder of the student, as the task header of the student is trained using the ground-truth labels. In the sequential setting, an additional aggregation component is introduced to the teacher and student networks, which is placed between the encoder and task header. We use two sets of embeddings produced by the encoder and aggregation component of the teacher to train the encoder and aggregation component of the student respectively. Similar to the non-sequential setting, the task header of the student is trained using ground-truth labels.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Model",
      "text": "By performing experiments on two tasks of SR and SER we show that our proposed framework leads to considerable performance gains in the student compared to previous studies. While the benchmark models rely on different aspects of the input for SR and SER, and thus different architectures exhibit different performances for each task, our method consistently improves the performance of the benchmarks. In summary, our approach opens a new path towards integration of LUPI by means of knowledge distillation into deep audio representation learning using audio-visual data, when only audio is available at inference. Our work also introduces a new set of challenges for future work. An immediate step for future work would be to study the use of generative models such as Generative Adversarial Networks and Variational Autoencoders for creating the embeddings from the teacher model with better domain adaptation and generalization. More recent and upcoming approaches such as normalizing flows can also be explored in this context.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the proposed method. Embeddings extracted from the",
      "page": 1
    },
    {
      "caption": "Figure 1: Our model is built based on teacher-",
      "page": 2
    },
    {
      "caption": "Figure 1: Through the",
      "page": 3
    },
    {
      "caption": "Figure 2: The proposed framework for learning privileged information through",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the general scheme for",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the two possible approaches for the sequen-",
      "page": 4
    },
    {
      "caption": "Figure 3: The proposed framework for learning privileged information through teacher-student distillation. In the sequential setting of our method, the audio signal",
      "page": 5
    },
    {
      "caption": "Figure 3: (left); (2) distilling video information at",
      "page": 5
    },
    {
      "caption": "Figure 3: (right). In the encoder-",
      "page": 5
    },
    {
      "caption": "Figure 4: A comparison between the effect of using our proposed method for LUPI versus using Soft-label and multitask training for speaker identiﬁcation in",
      "page": 7
    },
    {
      "caption": "Figure 4: shows the results of our experiments on the performance",
      "page": 7
    },
    {
      "caption": "Figure 5: shows the results of this",
      "page": 8
    },
    {
      "caption": "Figure 5: A comparison between the effect of using our proposed method for LUPI versus using Soft-label and multitask training for SER on RAVDESS in",
      "page": 9
    },
    {
      "caption": "Figure 6: shows the results of this experiment. As shown in",
      "page": 9
    },
    {
      "caption": "Figure 6: A comparison between the effect of using our proposed method for LUPI versus using Soft-label and multitask training for SER on IEMOCAP in",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Input": "Conv2D + BN\nM axpool",
          "–": "ReLU\n–",
          "257 × 500 × 1": "257 × 250 × 64\n128 × 250 × 64"
        },
        {
          "Input": "Conv2D + BN\nM axpool",
          "–": "ReLU\n–",
          "257 × 500 × 1": "128 × 250 × 128\n64 × 125 × 128"
        },
        {
          "Input": "Conv2D + BN\nM axpool",
          "–": "ReLU\n–",
          "257 × 500 × 1": "64 × 125 × 128\n32 × 62 × 128"
        },
        {
          "Input": "Conv2D + BN\nConv2D\nM axpool",
          "–": "–\nReLU\n–",
          "257 × 500 × 1": "32 × 62 × 256\n32 × 62 × 256\n16 × 31 × 256"
        },
        {
          "Input": "Conv2D + BN\nM axpool",
          "–": "ReLU\n–",
          "257 × 500 × 1": "16 × 31 × 512\n8 × 15 × 512"
        },
        {
          "Input": "Conv2D + BN\nM axpool",
          "–": "ReLU\n–",
          "257 × 500 × 1": "8 × 15 × 512\n4 × 15 × 512"
        },
        {
          "Input": "F C\nF C\nOutput",
          "–": "–\nReLU\nSoftmax",
          "257 × 500 × 1": "512\n512\n# of Classes"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Input": "Conv2D\nM axpool",
          "–": "ReLU\n–",
          "257 × 500 × 1": "257 × 500 × 64\n128 × 250 × 64"
        },
        {
          "Input": " \nConv2D + BN\nConv2D + BN\n× 2\nConv2D + BN",
          "–": "ReLU\nReLU\nReLU",
          "257 × 500 × 1": "128 × 250 × 96"
        },
        {
          "Input": "M axpool",
          "–": "–",
          "257 × 500 × 1": "64 × 125 × 96"
        },
        {
          "Input": " \nConv2D + BN\nConv2D + BN\n× 3\nConv2D + BN",
          "–": "ReLU\nReLU\nReLU",
          "257 × 500 × 1": "64 × 125 × 128"
        },
        {
          "Input": "M axpool",
          "–": "–",
          "257 × 500 × 1": "32 × 62 × 128"
        },
        {
          "Input": " \nConv2D + BN\nConv2D + BN\n× 3\nConv2D + BN",
          "–": "ReLU\nReLU\nReLU",
          "257 × 500 × 1": "32 × 62 × 256"
        },
        {
          "Input": "M axpool",
          "–": "–",
          "257 × 500 × 1": "16 × 31 × 256"
        },
        {
          "Input": " \nConv2D + BN\nConv2D + BN\n× 3\nConv2D + BN",
          "–": "ReLU\nReLU\nReLU",
          "257 × 500 × 1": "16 × 31 × 512"
        },
        {
          "Input": "M axpool",
          "–": "–",
          "257 × 500 × 1": "8 × 15 × 512"
        },
        {
          "Input": "F C\nF C\nOutput",
          "–": "–\nReLU\nSoftmax",
          "257 × 500 × 1": "512\n512\n# of Classes"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Input": "Encoder",
          "–": "–",
          "N × 257 × 100 × 1": "N × 512"
        },
        {
          "Input": "BiLST M\nBiLST M\nSelf − Attention",
          "–": "ReLU\nReLU\nAttention",
          "N × 257 × 100 × 1": "N × 512\nN × 512\n512"
        },
        {
          "Input": "Output",
          "–": "Softmax",
          "N × 257 × 100 × 1": "# of Classes"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "S Albanie",
        "A Nagrani",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "The ACM International Conference on Multimedia"
    },
    {
      "citation_id": "2",
      "title": "Multitask learning and multistage fusion for dimensional audiovisual emotion recognition",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "4",
      "title": "Cross-layer distillation with semantic calibration",
      "authors": [
        "D Chen",
        "J.-P Mei",
        "Y Zhang",
        "C Wang",
        "Z Wang",
        "Y Feng",
        "C Chen"
      ],
      "year": "2021",
      "venue": "The AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "5",
      "title": "Compact trilinear interaction for visual question answering",
      "authors": [
        "T Do",
        "T.-T Do",
        "H Tran",
        "E Tjiputra",
        "Q Tran"
      ],
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "6",
      "title": "Residual error based knowledge distillation",
      "authors": [
        "M Gao",
        "Y Wang",
        "L Wan"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "7",
      "title": "Modality distillation with multiple stream networks for action recognition",
      "authors": [
        "N Garcia",
        "P Morerio",
        "V Murino"
      ],
      "year": "2018",
      "venue": "The European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "8",
      "title": "Differentiable feature aggregation search for knowledge distillation",
      "authors": [
        "Y Guan",
        "P Zhao",
        "B Wang",
        "Y Zhang",
        "C Yao",
        "K Bian",
        "J Tang"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "9",
      "title": "A deep neural network for short-segment speaker recognition",
      "authors": [
        "A Hajavi",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "A deep neural network for short-segment speaker recognition"
    },
    {
      "citation_id": "10",
      "title": "Knowing what to listen to: Early attention for deep speech representation learning",
      "authors": [
        "A Hajavi",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "Knowing what to listen to: Early attention for deep speech representation learning",
      "arxiv": "arXiv:2009.01822"
    },
    {
      "citation_id": "11",
      "title": "Siamese capsule network for end-to-end speaker recognition in the wild",
      "authors": [
        "A Hajavi",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "12",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "13",
      "title": "A comprehensive overhaul of feature distillation",
      "authors": [
        "B Heo",
        "J Kim",
        "S Yun",
        "H Park",
        "N Kwak",
        "J Choi"
      ],
      "year": "2019",
      "venue": "The IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "14",
      "title": "Knowledge transfer via distillation of activation boundaries formed by hidden neurons",
      "authors": [
        "B Heo",
        "M Lee",
        "S Yun",
        "J Choi"
      ],
      "year": "2019",
      "venue": "The AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "16",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "17",
      "title": "Like what you like: Knowledge distill via neuron selectivity transfer",
      "authors": [
        "Z Huang",
        "N Wang"
      ],
      "year": "2017",
      "venue": "Like what you like: Knowledge distill via neuron selectivity transfer",
      "arxiv": "arXiv:1707.01219"
    },
    {
      "citation_id": "18",
      "title": "Learning temporal clusters using capsule routing for speech emotion recognition",
      "authors": [
        "M Jalal",
        "E Loweimi",
        "R Moore",
        "T Hain"
      ],
      "year": "2019",
      "venue": "Learning temporal clusters using capsule routing for speech emotion recognition"
    },
    {
      "citation_id": "19",
      "title": "An active learning paradigm for online audio-visual emotion recognition",
      "authors": [
        "I Kansizoglou",
        "L Bampis",
        "A Gasteratos"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Paraphrasing complex network: network compression via factor transfer",
      "authors": [
        "J Kim",
        "S Park",
        "N Kwak"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "21",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "22",
      "title": "Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer",
      "authors": [
        "N Komodakis",
        "S Zagoruyko"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "23",
      "title": "Detecting multiple speech disfluencies using a deep residual network with bidirectional long shortterm memory",
      "authors": [
        "T Kourkounakis",
        "A Hajavi",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "24",
      "title": "Fluentnet: End-to-end detection of stuttered speech disfluencies with deep learning",
      "authors": [
        "T Kourkounakis",
        "A Hajavi",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Towards the explainability of multimodal speech emotion recognition",
      "authors": [
        "P Kumar",
        "V Kaushik",
        "B Raman"
      ],
      "year": "2021",
      "venue": "Towards the explainability of multimodal speech emotion recognition"
    },
    {
      "citation_id": "26",
      "title": "Deep learning under privileged information using heteroscedastic dropout",
      "authors": [
        "J Lambert",
        "O Sener",
        "S Savarese"
      ],
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS One"
    },
    {
      "citation_id": "28",
      "title": "Unifying distillation and privileged information",
      "authors": [
        "D Lopez-Paz",
        "L Bottou",
        "B Schölkopf",
        "V Vapnik"
      ],
      "year": "2016",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "29",
      "title": "Learnable pins: Cross-modal embeddings for person identity",
      "authors": [
        "A Nagrani",
        "S Albanie",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "The European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "30",
      "title": "Disentangled speech embeddings using cross-modal self-supervision",
      "authors": [
        "A Nagrani",
        "J Chung",
        "S Albanie",
        "A Zisserman"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "31",
      "title": "Voxceleb: Largescale speaker verification in the wild",
      "authors": [
        "A Nagrani",
        "J Chung",
        "W Xie",
        "A Zisserman"
      ],
      "year": "2020",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "32",
      "title": "Cross-modal speaker verification and recognition: A multilingual perspective",
      "authors": [
        "S Nawaz",
        "M Saeed",
        "P Morerio",
        "A Mahmood",
        "I Gallo",
        "M Yousaf",
        "A Del Bue"
      ],
      "year": "2021",
      "venue": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "33",
      "title": "Learning deep representations with probabilistic knowledge transfer",
      "authors": [
        "N Passalis",
        "A Tefas"
      ],
      "year": "2018",
      "venue": "The European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "34",
      "title": "Alp-kd: Attentionbased layer projection for knowledge distillation",
      "authors": [
        "P Passban",
        "Y Wu",
        "M Rezagholizadeh",
        "Q Liu"
      ],
      "year": "2021",
      "venue": "The AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "35",
      "title": "Cross-modality distillation: A case for conditional generative adversarial networks",
      "authors": [
        "S Roheda",
        "B Riggan",
        "H Krim",
        "L Dai"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "36",
      "title": "Fitnets: Hints for thin deep nets. International Conference on Learning Representations (ICLR)",
      "authors": [
        "A Romero",
        "N Ballas",
        "S Kahou",
        "A Chassang",
        "C Gatta",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "Fitnets: Hints for thin deep nets. International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "37",
      "title": "Learning and refining of privileged informationbased rnns for action recognition from depth sequences",
      "authors": [
        "Z Shi",
        "T.-K Kim"
      ],
      "year": "2017",
      "venue": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "38",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "39",
      "title": "Cyclical learning rates for training neural networks",
      "authors": [
        "L Smith"
      ],
      "year": "2017",
      "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "40",
      "title": "Cross-modal knowledge distillation for action recognition",
      "authors": [
        "F Thoker",
        "J Gall"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "41",
      "title": "Contrastive representation distillation",
      "authors": [
        "Y Tian",
        "D Krishnan",
        "P Isola"
      ],
      "year": "2019",
      "venue": "Contrastive representation distillation",
      "arxiv": "arXiv:1910.10699"
    },
    {
      "citation_id": "42",
      "title": "A new learning paradigm: Learning using privileged information",
      "authors": [
        "V Vapnik",
        "A Vashist"
      ],
      "year": "2009",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "43",
      "title": "Utterance-level Aggregation For Speaker Recognition In The Wild",
      "authors": [
        "W Xie",
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "44",
      "title": "Feature normalized knowledge distillation for image classification",
      "authors": [
        "K Xu",
        "L Rui",
        "Y Li",
        "L Gu"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "45",
      "title": "Learning from multiple teacher networks",
      "authors": [
        "S You",
        "C Xu",
        "C Xu",
        "D Tao"
      ],
      "year": "2017",
      "venue": "The ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)"
    },
    {
      "citation_id": "46",
      "title": "Better and faster: knowledge transfer from multiple self-supervised learning tasks via graph distillation for video classification",
      "authors": [
        "C Zhang",
        "Y Peng"
      ],
      "year": "2018",
      "venue": "The International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "47",
      "title": "Rocket launching: A universal and efficient framework for training well-performing light net",
      "authors": [
        "G Zhou",
        "Y Fan",
        "R Cui",
        "W Bian",
        "X Zhu",
        "K Gai"
      ],
      "year": "2018",
      "venue": "The AAAI Conference on Artificial Intelligence"
    }
  ]
}