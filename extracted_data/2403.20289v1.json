{
  "paper_id": "2403.20289v1",
  "title": "Emotion-Anchored Contrastive Learning Framework For Emotion Recognition In Conversation",
  "published": "2024-03-29T17:00:55Z",
  "authors": [
    "Fangxu Yu",
    "Junjie Guo",
    "Zhen Wu",
    "Xinyu Dai"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversation (ERC) involves detecting the underlying emotion behind each utterance within a conversation. Effectively generating representations for utterances remains a significant challenge in this task. Recent works propose various models to address this issue, but they still struggle with differentiating similar emotions such as excitement and happiness. To alleviate this problem, We propose an Emotion-Anchored Contrastive Learning (EACL) framework that can generate more distinguishable utterance representations for similar emotions. To achieve this, we utilize label encodings as anchors to guide the learning of utterance representations and design an auxiliary loss to ensure the effective separation of anchors for similar emotions. Moreover, an additional adaptation process is proposed to adapt anchors to serve as effective classifiers to improve classification performance. Across extensive experiments, our proposed EACL achieves state-of-the-art emotion recognition performance and exhibits superior performance on similar emotions. Our code is available at https://github.com/Yu-Fangxu/EACL.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion Recognition in Conversation (ERC) aims to identify the emotions of each utterance in a conversation. It plays an important role in various scenarios, such as chatbots, healthcare applications, and opinion mining on social media. However, the ERC task faces several challenges. Depending on the context, similar statements may exhibit entirely different emotional attributes. Simultaneously, distinguishing conversation texts that contain similar emotional attributes is also extremely difficult  (Ong et al., 2022; Zhang et al., 2023a) . Figure  1  is an example of a chat between a man and a woman. Differentiating between happy and excited can be * Corresponding author.\n\nWell you paint a romantic picture.\n\n[Excited]\n\nWell I didn't want it to be like cheesy, you know.\n\n[Happy]\n\nDid she cry? [Excited]\n\nShe didn't cry, but she was laughing a lot and it was very exciting, so... challenging for machines due to their frequent occurrence in similar contexts. Appendix A exhibits quantitative analysis for emotions. This requires the model to accurately distinguish different emotions based on the context.\n\nTherefore, abundant efforts have been made implicitly to obtain distinguishable utterance representations from two lines, model design and representation learning. As the representative of the former line, DialogueRNN  (Majumder et al., 2019)  designs recurrent modules to track dialogue history for classification. Representation learning methods primarily exploit supervised contrastive learning (SupCon)  (Khosla et al., 2020)  for learning utterance representations. SPCL  (Song et al., 2022)  proposes a prototypical contrastive learning method to alleviate the class imbalance problem and achieve state-of-the-art performance. Our preliminary finegrained experimental results for SPCL, as shown in Figure  2 , use the normalized confusion matrix to evaluate the prediction performance. The findings reveal that similar emotions such as happy and excited are frequently misclassified as each other. This suggests that SPCL still struggles with effectively differentiating similar emotoins.\n\nTo tackle the aforementioned issues, this paper presents a novel Emotion-Anchored Contrastive Learning framework (EACL). EACL utilizes textual emotion labels to generate anchors that are emotionally semantic-rich representations. These representations as anchors explicitly strengthen the distinction between similar emotions in the representation space. Specifically, we introduce a penalty loss that encourages the corresponding emotion anchors to distribute uniformly in the representation space. By doing so, uniformly distributed emotion anchors guide utterance representations with similar emotions to learn larger dissimilarities, leading to enhanced discriminability. After generating separable utterance representations, we aim to compute the optimal positions of emotion anchors to which utterance representations can be assigned for classification purposes. To achieve better assignment, inspired by the two-stage frameworks  (Kang et al., 2019; Menon et al., 2020; Nam et al., 2023) , we propose the second stage to shift the decision boundaries of emotion anchors with fixed utterance representations and achieve better classification performance, which is simple yet effective.\n\nWe conduct experiments on three widely used benchmark datasets, the results demonstrate that EACL achieves a new state-of-the-art performance. Moreover, EACL achieves a significantly higher separability in similar emotions, which validates the effectiveness of our method.\n\nThe main contributions of this work are summa-rized as follows:\n\n• We propose a novel emotion-anchored contrastive learning framework for ERC, that can generate more distinguishable representations for utterances.\n\n• To the best of our knowledge, our method is the first to explicitly alleviate the problem of emotion similarity by introducing label semantic information in modeling for ERC, which can effectively guide representation learning.\n\n• Experimental results show that our proposed EACL achieves a new state-of-the-art performance on benchmark datasets.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition In Conversation",
      "text": "Most of the present works adopt graph-based and sequence-based methods. DialogueGCN  (Ghosal et al., 2019)  builds a graph treating utterances as nodes, and models intra-speaker and inter-speaker relationships by setting different edge types between two nodes. MMGCN  (Hu et al., 2021b)  fuses multi-modal utterance representations into a graph. Differently, DAG-ERC  (Shen et al., 2021)  exploits directed acyclic graphs to naturally capture the spatial and temporal structure of the dialogue. COGMEN  (Joshi et al., 2022)  combines graph neural network and graph transformer to leverage both local and global information respectively. Another group of works exploits transformers and recurrent models to learn the interactions between utterances.  DialogueRNN (Majumder et al., 2019)  combines several RNNs to model dialogue dynamics. DialogueCRN  (Hu et al., 2021a)  introduces a cognitive reasoning module. Commensense Knowledge is explored by KET  (Zhong et al., 2019)  and COSMIC  (Ghosal et al., 2020) . Cog-BART  (Li et al., 2022a)  employs BART  (Lewis et al., 2019)  to simultaneously generate responses and detect emotions with the auxiliary of contrastive learning. EmoCaps  (Li et al., 2022c)  and DialogueEIN  (Liu et al., 2022)  design several modules to explicitly model emotional tendency and inertia, local and global information in dialogue. The power of the language models is utilized by CoMPM  (Lee and Lee, 2021)  which learns and tracks contextual information by the language model itself and SPCL  (Song et al., 2022) , a prototypical supervised contrastive learning method to alleviate the data imbalance problem. SACL  (Hu et al., 2023) introduces adversarial examples to learn robust representations. Our EACL goes along this track. Unlike the above approaches, HCL  (Yang et al., 2022)  comes up with a general curriculum learning paradigm that can be applied to all ERC models. InstructERC  (Lei et al., 2023)  and DialogueLLM  (Zhang et al., 2023c)  construct instructions and fine-tune LLMs for ERC.  (Lee, 2022; Guo et al., 2021)  learn from soft labels.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Supervised Contrastive Learning",
      "text": "Recent works  (Chen et al., 2020; He et al., 2020a)  in unsupervised contrastive learning provide a similarity-based learning framework for representation learning. These methods maximize the similarity between positive samples while minimizing the similarity between negative sample pairs. To make use of supervised information, supervised contrastive learning (SupCon)  (Gunel et al., 2020)  aims to make the data that have the same label closer in the representation space and push away those that have different labels. However, Sup-Con works poorly in data imbalance settings. To mitigate this problem, KCL  (Kang et al., 2021)  explicitly pursues a balanced representation space. TSC  (Li et al., 2022b)  uniformly set targets in the hypersphere and enforce data representations to close to the targets. BCL  (Zhu et al., 2022)  regards classifier weights as prototypes in the representation space and incorporates them in the contrastive loss. LaCon  (Zhang et al., 2022)  incorporates label embedding for better language understanding. Our method is inspired by TSC, differently, we incorporate emotion semantics in the representation space and dynamically adjust the emotion anchors for better classification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Definition",
      "text": "A conversation can be denoted as a sequence of utterances {u 1 , u 2 , u 3 , ..., u n }, each utterance u t is uttered by one of the conversation speakers s j . There are m (m ≥ 2) speakers in the conversation, denoted as {s 1 , s 2 , ..., s m }. Given the set of emotion labels E and conversation context {(u 1 , s u 1 ), (u 2 , s u 2 ), ..., (u t , s ut )}, the ERC task aims to predict emotion e t (e t ∈ E) for current utterance u t . E is a set of emotions. For instance, in the IEMOCAP dataset, E = {excited, frustrated, sad, neutral, angry, happy}.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Overview",
      "text": "The overview of our model is shown in Figure  3 . The encoding strategy of our model adopts the paradigm of prompt learning (Section 3.3). Our training process is composed of two stages.\n\nThe first stage (Section 3.4) is called representation learning, which aims to learn more distinctive representations with emotion anchors. Concretely, we incorporate anchors containing semantic information into the contrastive learning framework and utilize them to guide the learning of utterance representations. Our objectives are (1) to bring utterances with the same emotion closer to their corresponding anchors and push utterances with different emotions farther away, and (2) to achieve a more uniform distribution of anchors in the hyperspace for better classifying different emotions.\n\nThe second stage (Section 3.5) is called emotion anchor adaptation, which aims to further improve classification performance by slightly adjusting anchors. The anchors in the first stage can help the model learn separable representations of utterances. However, separated emotion anchors may not be located in the most representative positions of each category of utterance representation for the following emotion recognition because contrastive learning in the first stage aims not to achieve this goal. Therefore, we design the second stage to slightly adjust the positions of emotion anchors to shift the decision boundaries for better classification performance. In this stage, we freeze the parameters of the language model and only fine-tune the emotion anchors, as shown on the right side of Figure  3 . Lastly, EACL matches the utterance representations with the most similar emotion anchors to make predictions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Prompt Context Encoding",
      "text": "Following previous work  (Song et al., 2022) , we employ pre-trained language models and adopt prompt tuning to transform the classification into masked language modeling. An effective prompt template aligns the downstream task with the large semantic information learned by the language model in the pre-training stage, which boosts the model's performance in downstream tasks.\n\nTo predict the emotion of utterance u t , we take k utterances before timestamp t as the context to predict e t . Formally, the input for the language model is composed as:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Language Model",
      "text": "Monica: Enough! Joey: Lean-leanlean! For utterance: Lean-leanlean, Speaker Joey feels <mask>",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Contrastive Utterance Representation Learning",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Anchor Utterance Repr",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Language Model",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Representation Learning 2. Emotion Anchor Adaptation",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Anchor Learning Language Model",
      "text": "Monica: Enough! Joey: Lean-leanlean! For utterance: Lean-leanlean, Speaker Joey feels <mask> <mask> token where Prompt P = \"For utterance u t , speaker s t feels [mask]\" . We take the last hidden state of [mask] as utterance representation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Before Adaptation After Adaptation",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Trainable",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Stage One: Representation Learning",
      "text": "In this section, we will introduce two main components of EACL in stage one: utterance representation learning and emotion anchor learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Utterance Representation Learning",
      "text": "The objective in this section is to acquire discernible representations for each individual utterance. To accomplish this, we employ label encodings to generate emotion anchors and incorporate them into a contrastive learning framework. By utilizing these anchors, we can proficiently steer the process of representation learning.\n\nGiven a batch of samples X = {x 1 , x 2 , . . . , x b } ∈ R b×ℓ , where b, ℓ are batch size and max length of input respectively. We feed X into the pretrained language model and get the last hidden states Z = Encoder(X ). Then we use the hidden state of [mask] token at the end of the sentence as the representation of utterance u t . Finally, we obtain the representations of utterances with an MLP layer:\n\nwhere\n\nSimilarly, we take textual emotion labels as the input of language models to obtain emotion anchors for all emotions E = {e 1 , e 2 , . . . , e s }:\n\nwhere A ∈ R s×d , each row of which represents a emotion anchor. s represents the number of emotions. To ensure we get a stable anchor representation, Z a is frozen in our training process.\n\nWe propose an emotion-anchored contrastive learning loss to utilize emotion label semantics for better representation learning. More specifically, in each mini-batch, we let V = {v 1 , v 2 , . . . , v b+s } = R ∪ A and V + i represents the set of utterances or anchor representation that have the same label as utterance r i except for itself. Finally, our emotionanchored contrastive loss is as follows:\n\nwhere |V + i | represents number of positive examples. τ is the temperature hyperparameter for the contrastive loss. sim represents a similarity function, we adopt cosine similarity here.\n\nIn equation 4, the interactions between representations can be divided into three components: utterances-utterances, anchors-utterances, and anchors-anchors. Representations with the same label are brought closer to each other, while those with different labels are pushed farther apart. The utterances-utterances interactions are similar to traditional contrastive learning, while the anchorsutterances interactions represent the process of anchor-guided utterance representation learning. The anchors-anchors interaction ensures a better distinction between different emotions.\n\nRecent research  (Gunel et al., 2020)  has indicated that combining cross-entropy loss with contrastive learning facilitates language models with more discriminative ability. Therefore crossentropy loss is added to help improve representation learning. We additionally add a linear mapping for classification:\n\n(5)\n\nwhere Ŷ ∈ R b×s represents the possibility distribution of b utterances over s emotions. y ij represents the element in the i-th row and j-th column of Ŷ. MLP ce is a linear layer for classification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Anchor Learning",
      "text": "Nevertheless, despite the implementation of the interaction between representations, the three types of interactions mentioned in Section 3.4.1 alone are insufficient to explicitly disperse the distance between the most similar emotion anchors. To further tackle the issue of similarity, we propose an anchor angle loss. This loss is designed to incentivize emotion anchors to maximize the angle between themselves and their most similar emotion anchors within the contrastive space:\n\nwhere a i represents i-th emotion anchor representation in A. L Ag aims to minimize the maximal pairwise cosine similarity between all the emotion anchors. It is equivalent to maximizing the minimal pairwise angle. The more dispersed emotion anchors are, the better their capacity to recognize similar emotions.\n\nCombining all the components mentioned in stage one, the overall loss is a weighted average of cross-entropy loss, anchor angle loss, and contrastive loss, as given in equation 8.\n\nwhere λ 1 and λ 2 are hyper-parameters to balance loss terms.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Stage Two: Emotion Anchor Adaptation",
      "text": "In the first stage, we used emotion anchors generated from emotion labels to guide the convergence of utterance representations toward different emotion clusters. These emotion anchors serve as representatives for each emotion, which are suitable to function as effective nearest-neighbor classifiers for utterance representations. However, separated emotion anchors trained from stage one may not be located in the most representative positions of each category of utterance representation, which weakens the classification ability of emotion anchors. To ensure the alignment between utterance representations and emotion anchors, we propose the second stage to adapt the emotion anchors to shift the decision boundaries by training them with a small number of epochs. This approach aims to enhance the ability of emotion anchors for classification purposes.\n\nTo be more specific, we freeze the parameters of the language model and make the emotion anchors inherited from stage one a i (i = 1, ..., s) trainable parameters, which corresponds to the right side in Figure  3 . In order to be consistent with the representation learning, we still use the same similarity measure for adapting emotion anchors.\n\nThe loss function for emotion anchor adaptation:\n\nwhere c ij means adjusted cosine similarity between the i-th utterance representation r i and j-th emotion anchors a j . τ is the same temperature hyperparameter in stage one.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotion Prediction",
      "text": "During the inference stage, we predict emotion labels by matching each utterance representation with the nearest emotion anchor:\n\nWhere r i is the representation of utterance x i and a j is the emotion anchor of class j.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setup",
      "text": "The language model loads the initial parameter with SimCSE-Roberta-Large  (Gao et al., 2021) .\n\nAll experiments are conducted on a single NVIDIA A100 GPU 80GB and we implement models with PyTorch 2.0 framework. More experimental details are provided in Appendix B.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Datasets",
      "text": "In this section, we will introduce three adopted popular benchmark datasets: IEMOCAP  (Busso et al., 2008) , MELD  (Poria et al., 2018)  and EmoryNLP (Zahiri and Choi, 2017).\n\n(1) IEMOCAP: consists of 151 videos of two speakers' dialogues with 7433 utterances. Each utterance is annotated by an emotion label from 6 classes, including excited, frustrated, sad, neutral, angry, and happy.\n\n(2) MELD: is extracted from the TV show Friends. It contains about 13000 utterances from 1433 dialogues. Each utterance is labeled by one of the following 7 emotion labels: surprise, neutral, anger, sadness, disgusting, joy, and fear.\n\n(3) EmoryNLP: contains 97 episodes, 897 scenes, and 12606 utterances from TV show Friends. It differs from MELD in that the emotional tags contained are: joyful, sad, powerful, mad, neutral, scared, and peaceful.\n\nIn our experiments, we only use textual modality. The detailed statistics of the three datasets are shown in Table  1 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Metrics",
      "text": "Following previous works  (Lee and Lee, 2021; Song et al., 2022) , we choose the weighted-average F1 score as the evaluation metric.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Baselines",
      "text": "For a comprehensive evaluation, we compare our method with the following baselines:\n\n(1) Graph-based model: DialogueGCN  (Ghosal et al., 2019)  employs GCNs to gather context features for learning utterance representations, Shen  (Shen et al., 2021)  shows the performance of replacing the feature extractor with Roberta-Large. RGAT  (Ishiwatari et al., 2020)  proposes relational position encodings to model both speaker relationship and sequential information. DAG-ERC  (Shen et al., 2021)  utilizes an acyclic graph neural network to intuitively model a conversation's natural structure without introducing any external information. DAG-ERC+HCL  (Yang et al., 2022)  proposes a curriculum learning paradigm combined with DAG-ERC for learning from easy to hard. SIGAT  (Jia et al., 2023 ) models speaker and sequence information in a unified graph to learn the interactive influence between them.\n\n(2) Sequence-based model: COSMIC  (Ghosal et al., 2020)  incorporates different elements of commonsense and leverages them to learn self-speaker dependency. Cog-BART  (Li et al., 2022a)  applies BART with contrastive learning to take response generation into consideration. DialogueEIN  (Liu et al., 2022)  designs emotion interaction and tendency blocks to explicitly simulate emotion inertia and stimulus. CoMPM (Lee and Lee, 2021) utilizes pretrained models to directly learn contextual information and track dialogue history. Sup-Con  (Gunel et al., 2020)  is the vanilla supervised contrastive learning. SCCL  (Yang et al., 2023)  conducts contrastive learning with 3-dimensional affect representations. DIEU  (Zhao et al., 2023a)  aims to solve the long-range context propagation problem. CKCL  (Tu et al., 2023)  denoises information irrelevant context and knowledge when training. MPLP  (Zhang et al., 2023b ) models the history and experience of speakers and exploits paraphrasing to enlarge the difference between labels. Emocaps  (Li et al., 2022c)   5 Results and Analysis",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Main Results",
      "text": "Table  2  reports the results of our method and the baselines. Our model outperforms other baselines and achieves a new state-of-the-art performance on IEMOCAP, MELD, and EmoryNLP datasets. The results exhibit the effectiveness of our emotionanchored contrastive learning framework.\n\nBased on the results, we can observe that sequence-based methods have overall better performance than graph-based methods. Compared to the graph-based models, EACL improves a large margin over the DAG-ERC  (Shen et al., 2021)     tations. EACL outperforms the state-of-the-art results on the IEMOCAP dataset by 0.92%, the MELD dataset by 0.6%, and the EmoryNLP dataset by 0.59%. Besides, EACL has an overwhelming performance advantage over ChatGPT, one possible reason is that the few-shot prompt setting may not be enough to achieve satisfactory performance. Table  3  reports the fine-grained performance on benchmark datasets. EACL outperforms SPCL+CL which is the most relevant method to us in most emotion categories on all benchmark datasets. Specifically, in the IEMOCAP dataset, We have observed a significant improvement in performance on two pairs of similar emotions, happy and excited with an increase of 7.33% and 4.55%, frustrated and angry with an increase of 3.80% and 2.72% respectively. Detailed performance analysis is provided in Appendix C.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct a series of experiments to confirm the effectiveness of components in our method. The results are shown in Table  4 . Removing any element of EACL makes the overall performance worse.\n\nTo validate the effects of components in the first stage, We remove the L Ag which encourages the angle of different emotion anchors to be uniform. We can find that the lack of L Ag results in a significant decline in the performance of nearly 0.5%, as reported in line 2 in Table  4 , indicating that emotion anchor learning helps for separating utterance representations. Also, the removal of L CE drops the performance by about 0.5% on average, the result demonstrates that supervised learning benefits the fine-tuning of language models.\n\nIn the second stage, We explore whether adapting emotion anchors and emotion semantics are necessary. Similar to classifier re-training  (Kang et al., 2019; Nam et al., 2023) , we randomly initialize emotion anchors that lie far from the data distribution after learning the utterance representations. Training from scratch is a cold start and cannot reach the optimal position. This result in Line 4 verifies the importance of inheriting emotion an- chors and the result shows that the trained emotion anchors express a more powerful ability of recognition. When we remove the anchor adaptation or take the center of training representations for each emotion category as emotion anchors, performance will degrade significantly, indicating the improper positions of emotion anchors weaken the classification performance and verifying the importance of stage two. Lines 5 and 6 in Table  4  confirms our assumption. In summary, the components of our method contribute to the results substantially.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Performance On Different Language Models",
      "text": "To evaluate the versatility of our learning framework, we conducted experiments using different pretrained language models. Specifically, we examined the performance of our framework on two additional popular language models, namely Deberta-Large  (He et al., 2020b)  and Promcse-Roberta-Large  (Jiang et al., 2022) . The results, presented in Table  5 , demonstrate that all the pretrained models deliver competitive performance. This observation serves as evidence for the robustness and effectiveness of our framework across various pre-trained language models. It further emphasizes the generalizability of our approach in conversational emotion recognition tasks. We report fine-grained performance in Appendix D.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Emotion Similarity Comparison",
      "text": "In this section, we conducted a comparison of the similarity between pairs of emotions before and after training with EACL in Figure  4 . To observe the angle change more intuitively, we also include the angle degree. Figure  4  reveals a significant decrease in similarity for emotion anchors that are considered similar. For instance, the cosine similarity between excited and happy drops sharply from 0.77 to 0.08, while for frustrated and angry, it decreases from 0.84 to -0.3. Meanwhile, naturally dissimilar emotions are now positioned further apart.\n\nFor instance, the similarity between neutral and other emotions also experiences a notable decline. These observations suggest that EACL effectively   increases the separation between similar emotions, thereby enhancing the model's ability to distinguish between them. Figure  5  visualizes the positions of anchors before and after training, where similar emotions are separated by EACL.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "This paper introduces a novel framework for conversational emotion recognition called emotionanchored contrastive learning. The proposed EACL leverages emotion representations as anchors to enhance the learning process of distinctive utterance representations. Building upon this foundation, we further adapt the emotion anchors through finetuning, bringing them the optimal positions and more suitable for classification purposes. Through extensive experiments and evaluations on three popular benchmark datasets, our approach achieves a new state-of-the-art performance. Ablation studies and evaluations confirm that the proposed EACL framework significantly benefits dialogue modeling and enhances the learning of utterance representations for more accurate emotion recognition. The proposed EACL distributes the utterances in representation space more uniformly, which is beneficial for multi-class ERC tasks. When considering the context of multi-label classification, EACL can group relevant emotions guided by human knowledge, or adjust the inter-class weights of contrastive losses with label similarity  (Wang et al.,   2022;  Zhao et al., 2022) . Then, EACL can serve to detect multiple emotions in a single utterance, which will be left for future work.\n\nTo better understand our motivation, we exhibit the emotion similarity in Figure  6 . We split the emotions into 3 groups which are composed of positive emotions, negative emotions, and neutral, where positive emotions include excited and happy, negative emotions contain frustrated, sad, angry, and neutral. It is observed that excited and happy have a cosine similarity of 0.77, and for frustrated and angry, they have 0.84 cosine similarity. The similarity of the positive emotions group is higher than that of the negative emotions group. For neutral, it is almost equally similar to other emotions.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "B Experimental Setup",
      "text": "EACL loads the initial parameter by SimCSE-Roberta-Large  (Gao et al., 2021)  which is identical to the setting of SPCL. All the hyperparameters are reported in Table  6 . We exploit grid-search for λ 1 in {0, 0.1, 0.3, 0.5, 0.7, 0.9}, λ 2 in {0, 0.01, 0.1, 1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.  Figure  7 : The normalized confusion matrix of three benchmark datasets, each row is the true classes and column is predictions. The Coordinate i, j means the percentage of emotion i predicted to be emotion j.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "C Detailed Performance Analysis",
      "text": "In Figure  7 , we provide the normalized confusion matrices for our EACL and SPCL+CL models across various datasets. These matrices serve as crucial tools for assessing the models' performance. Notably, when we examine the diagonal elements of these matrices, it becomes evident that EACL consistently outperforms the state-of-the-art method SPCL+CL in terms of true positives for most fine-grained emotion categories. This suggests that EACL excels at learning features that are more distinguishable.\n\nParticularly noteworthy is the performance of EACL in comparison to SPCL+CL when considering specific emotion pairs, such as excited and happy, as well as frustrated and angry on the IEMOCAP dataset. In these cases, EACL demonstrates superior performance. This underscores the effectiveness of the EACL framework in effectively addressing the challenge of misclassification, especially when dealing with emotions that share similar characteristics. When we focus on the MELD and EmoryNLP datasets, we observe that EACL significantly reduces misclassifications be-   43 55.28 44.44 37.59 60.85 65.34 53.99 67.8 PromCSE 23.59 81.0 54.96 43.35 30.53 59.51 65.12 51.15 67.38 SPCL+CL 26.59 77.92 54.40 43.53 30.94 59.26 60.34 50.43     54.42 28.33 14.21 43.35 51.64 23.42 41.30 36.68 40.93 SPCL+CL 53.52 31.61 10.28 44.21 51.40 16.83 39.51 35.34 39.52  Table  7 : Fine-grained performance record on different language models for all emotions on three benchmark datasets, the F1-score is used for each class.\n\ntween neutral emotions and other emotional states. This highlights EACL's capability to effectively mitigate misclassification issues not only for similar emotions but for all emotion categories.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "D Fine-Grained Performance On Different Models",
      "text": "In this section, we report the fine-grained performance when using Deberta-Large  (He et al., 2020b)  and Promcse-Roberta-Large  (Jiang et al., 2022)  in Table  7 . The results indicate that our learning framework is robust to different language models. Similar to the result under Roberta-SimCSE, these models can also effectively separate similar emotions and achieve state-of-the-art performance on the benchmark datasets.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of a conversation in the IEMO-",
      "page": 1
    },
    {
      "caption": "Figure 2: , use the normalized confusion matrix",
      "page": 1
    },
    {
      "caption": "Figure 2: Normalized confusion matrix of SPCL on the",
      "page": 2
    },
    {
      "caption": "Figure 3: The encoding strategy of our model adopts the",
      "page": 3
    },
    {
      "caption": "Figure 3: Lastly, EACL matches the utterance represen-",
      "page": 3
    },
    {
      "caption": "Figure 3: Overview of our proposed framework. Left side introduces representation learning, which is composed of",
      "page": 4
    },
    {
      "caption": "Figure 3: In order to be consistent with the repre-",
      "page": 5
    },
    {
      "caption": "Figure 4: To observe",
      "page": 8
    },
    {
      "caption": "Figure 4: reveals a significant de-",
      "page": 8
    },
    {
      "caption": "Figure 4: The cosine similarity of pair-wise emotions.",
      "page": 9
    },
    {
      "caption": "Figure 5: visualizes the positions of",
      "page": 9
    },
    {
      "caption": "Figure 5: The t-SNE visualization of emotion anchors.",
      "page": 9
    },
    {
      "caption": "Figure 6: We split the emo-",
      "page": 13
    },
    {
      "caption": "Figure 6: Cosine similarity between emotion word repre-",
      "page": 13
    },
    {
      "caption": "Figure 7: The normalized confusion matrix of three",
      "page": 13
    },
    {
      "caption": "Figure 7: , we provide the normalized confu-",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "former line, DialogueRNN (Majumder et al., 2019)"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "Introduction",
          "resentation learning. As the representative of the": "designs recurrent modules to track dialogue history"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "for classification. Representation learning methods"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "Emotion Recognition in Conversation (ERC) aims",
          "resentation learning. As the representative of the": ""
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "primarily exploit supervised contrastive learning"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "to identify the emotions of each utterance in a con-",
          "resentation learning. As the representative of the": ""
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "(SupCon) (Khosla et al., 2020) for learning utter-"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": ""
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "ance representations. SPCL (Song et al., 2022) pro-"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "narios, such as chatbots, healthcare applications,",
          "resentation learning. As the representative of the": ""
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "poses a prototypical contrastive learning method to"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "and opinion mining on social media. However, the",
          "resentation learning. As the representative of the": ""
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "alleviate the class imbalance problem and achieve"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "ERC task faces several challenges. Depending on",
          "resentation learning. As the representative of the": ""
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "state-of-the-art performance. Our preliminary fine-"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "the context, similar statements may exhibit entirely",
          "resentation learning. As the representative of the": ""
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "grained experimental results for SPCL, as shown"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "different emotional attributes. Simultaneously, dis-",
          "resentation learning. As the representative of the": ""
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "in Figure 2, use the normalized confusion matrix"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "tinguishing conversation texts that contain similar",
          "resentation learning. As the representative of the": ""
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "to evaluate the prediction performance. The find-"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "emotional attributes is also extremely difficult (Ong",
          "resentation learning. As the representative of the": ""
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "ings reveal\nthat similar emotions such as happy"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "et al., 2022; Zhang et al., 2023a). Figure 1 is an",
          "resentation learning. As the representative of the": ""
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "and excited are frequently misclassified as each"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "example of a chat between a man and a woman.",
          "resentation learning. As the representative of the": ""
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "other. This suggests that SPCL still struggles with"
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "Differentiating between happy and excited can be",
          "resentation learning. As the representative of the": ""
        },
        {
          "https://github.com/Yu-Fangxu/EACL.": "",
          "resentation learning. As the representative of the": "effectively differentiating similar emotoins."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "",
          "Woman\nMan": "Well you paint a romantic"
        },
        {
          "Abstract": "Emotion Recognition in Conversation (ERC)",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "picture. [Excited]"
        },
        {
          "Abstract": "involves detecting the underlying emotion be-",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "Well I didn't want it to be like"
        },
        {
          "Abstract": "hind each utterance within a conversation. Ef-",
          "Woman\nMan": "cheesy, you know. [Happy]"
        },
        {
          "Abstract": "fectively generating representations for utter-",
          "Woman\nMan": ""
        },
        {
          "Abstract": "ances remains a significant challenge in this",
          "Woman\nMan": "Did she cry? [Excited]"
        },
        {
          "Abstract": "task. Recent works propose various models to",
          "Woman\nMan": "She didn't cry, but she was"
        },
        {
          "Abstract": "",
          "Woman\nMan": "laughing a lot and it was very"
        },
        {
          "Abstract": "address this issue, but\nthey still struggle with",
          "Woman\nMan": "exciting, so... [Happy]"
        },
        {
          "Abstract": "differentiating similar emotions such as excite-",
          "Woman\nMan": "Oh, my god.  How long have"
        },
        {
          "Abstract": "",
          "Woman\nMan": "you been planning on doing"
        },
        {
          "Abstract": "ment and happiness. To alleviate this problem,",
          "Woman\nMan": "this? [Excited]"
        },
        {
          "Abstract": "We propose an Emotion-Anchored Contrastive",
          "Woman\nMan": ""
        },
        {
          "Abstract": "Learning (EACL) framework that can generate",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "Figure 1: An example of a conversation in the IEMO-"
        },
        {
          "Abstract": "more distinguishable utterance representations",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "CAP dataset."
        },
        {
          "Abstract": "for similar emotions. To achieve this, we utilize",
          "Woman\nMan": ""
        },
        {
          "Abstract": "label encodings as anchors to guide the learn-",
          "Woman\nMan": ""
        },
        {
          "Abstract": "ing of utterance representations and design an",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "challenging for machines due to their frequent oc-"
        },
        {
          "Abstract": "auxiliary loss to ensure the effective separation",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "currence in similar contexts. Appendix A exhibits"
        },
        {
          "Abstract": "of anchors for similar emotions. Moreover, an",
          "Woman\nMan": ""
        },
        {
          "Abstract": "additional adaptation process is proposed to",
          "Woman\nMan": "quantitative analysis for emotions. This requires"
        },
        {
          "Abstract": "adapt anchors to serve as effective classifiers",
          "Woman\nMan": "the model to accurately distinguish different emo-"
        },
        {
          "Abstract": "to improve classification performance. Across",
          "Woman\nMan": "tions based on the context."
        },
        {
          "Abstract": "extensive experiments, our proposed EACL",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "Therefore, abundant efforts have been made im-"
        },
        {
          "Abstract": "achieves state-of-the-art emotion recognition",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "plicitly to obtain distinguishable utterance repre-"
        },
        {
          "Abstract": "performance and exhibits superior performance",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "sentations from two lines, model design and rep-"
        },
        {
          "Abstract": "on similar emotions. Our code is available at",
          "Woman\nMan": ""
        },
        {
          "Abstract": "https://github.com/Yu-Fangxu/EACL.",
          "Woman\nMan": "resentation learning. As the representative of the"
        },
        {
          "Abstract": "",
          "Woman\nMan": "former line, DialogueRNN (Majumder et al., 2019)"
        },
        {
          "Abstract": "Introduction",
          "Woman\nMan": "designs recurrent modules to track dialogue history"
        },
        {
          "Abstract": "",
          "Woman\nMan": "for classification. Representation learning methods"
        },
        {
          "Abstract": "",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "primarily exploit supervised contrastive learning"
        },
        {
          "Abstract": "",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "(SupCon) (Khosla et al., 2020) for learning utter-"
        },
        {
          "Abstract": "",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "ance representations. SPCL (Song et al., 2022) pro-"
        },
        {
          "Abstract": "",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "poses a prototypical contrastive learning method to"
        },
        {
          "Abstract": "",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "alleviate the class imbalance problem and achieve"
        },
        {
          "Abstract": "",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "state-of-the-art performance. Our preliminary fine-"
        },
        {
          "Abstract": "",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "grained experimental results for SPCL, as shown"
        },
        {
          "Abstract": "",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "in Figure 2, use the normalized confusion matrix"
        },
        {
          "Abstract": "",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "to evaluate the prediction performance. The find-"
        },
        {
          "Abstract": "",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "ings reveal\nthat similar emotions such as happy"
        },
        {
          "Abstract": "",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "and excited are frequently misclassified as each"
        },
        {
          "Abstract": "",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "other. This suggests that SPCL still struggles with"
        },
        {
          "Abstract": "",
          "Woman\nMan": ""
        },
        {
          "Abstract": "",
          "Woman\nMan": "effectively differentiating similar emotoins."
        },
        {
          "Abstract": "Corresponding author.",
          "Woman\nMan": "To tackle the aforementioned issues, this paper"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "rized as follows:": ""
        },
        {
          "rized as follows:": ""
        },
        {
          "rized as follows:": "• We propose a novel emotion-anchored con-"
        },
        {
          "rized as follows:": ""
        },
        {
          "rized as follows:": "trastive learning framework for ERC, that can"
        },
        {
          "rized as follows:": "generate more distinguishable representations"
        },
        {
          "rized as follows:": ""
        },
        {
          "rized as follows:": "for utterances."
        },
        {
          "rized as follows:": ""
        },
        {
          "rized as follows:": ""
        },
        {
          "rized as follows:": "• To the best of our knowledge, our method is"
        },
        {
          "rized as follows:": "the first to explicitly alleviate the problem of"
        },
        {
          "rized as follows:": ""
        },
        {
          "rized as follows:": "emotion similarity by introducing label seman-"
        },
        {
          "rized as follows:": "tic information in modeling for ERC, which"
        },
        {
          "rized as follows:": ""
        },
        {
          "rized as follows:": "can effectively guide representation learning."
        },
        {
          "rized as follows:": ""
        },
        {
          "rized as follows:": ""
        },
        {
          "rized as follows:": "• Experimental results show that our proposed"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the actual classes and predictions made by the model": "respectively. The cross-point (i, j) means the percentage"
        },
        {
          "the actual classes and predictions made by the model": "of emotion i predicted to be emotion j. Except for the"
        },
        {
          "the actual classes and predictions made by the model": ""
        },
        {
          "the actual classes and predictions made by the model": "diagonal, the bigger values and deeper color mean these"
        },
        {
          "the actual classes and predictions made by the model": "emotions are easily misclassified."
        },
        {
          "the actual classes and predictions made by the model": ""
        },
        {
          "the actual classes and predictions made by the model": ""
        },
        {
          "the actual classes and predictions made by the model": "presents a novel Emotion-Anchored Contrastive"
        },
        {
          "the actual classes and predictions made by the model": ""
        },
        {
          "the actual classes and predictions made by the model": "Learning framework (EACL). EACL utilizes tex-"
        },
        {
          "the actual classes and predictions made by the model": ""
        },
        {
          "the actual classes and predictions made by the model": "tual emotion labels to generate anchors that are"
        },
        {
          "the actual classes and predictions made by the model": ""
        },
        {
          "the actual classes and predictions made by the model": "emotionally semantic-rich representations. These"
        },
        {
          "the actual classes and predictions made by the model": ""
        },
        {
          "the actual classes and predictions made by the model": "representations as anchors explicitly strengthen"
        },
        {
          "the actual classes and predictions made by the model": ""
        },
        {
          "the actual classes and predictions made by the model": "the distinction between similar emotions\nin the"
        },
        {
          "the actual classes and predictions made by the model": ""
        },
        {
          "the actual classes and predictions made by the model": "representation space.\nSpecifically, we introduce"
        },
        {
          "the actual classes and predictions made by the model": ""
        },
        {
          "the actual classes and predictions made by the model": "a penalty loss that encourages the corresponding"
        },
        {
          "the actual classes and predictions made by the model": ""
        },
        {
          "the actual classes and predictions made by the model": "emotion anchors to distribute uniformly in the rep-"
        },
        {
          "the actual classes and predictions made by the model": ""
        },
        {
          "the actual classes and predictions made by the model": "resentation space.\nBy doing so, uniformly dis-"
        },
        {
          "the actual classes and predictions made by the model": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "amples to learn robust representations. Our EACL",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "The overview of our model\nis\nshown in Figure"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "goes along this track. Unlike the above approaches,",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "3. The encoding strategy of our model adopts the"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "HCL (Yang et al., 2022) comes up with a general",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "paradigm of prompt\nlearning (Section 3.3). Our"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "curriculum learning paradigm that can be applied",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "training process is composed of two stages."
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "to all ERC models. InstructERC (Lei et al., 2023)",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "The first stage (Section 3.4) is called representa-"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "and DialogueLLM (Zhang et al., 2023c) construct",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "tion learning, which aims to learn more distinctive"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "instructions and fine-tune LLMs for ERC.\n(Lee,",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "representations with emotion anchors. Concretely,"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "2022; Guo et al., 2021) learn from soft labels.",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "we incorporate anchors containing semantic infor-"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "mation into the contrastive learning framework and"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "2.2\nSupervised Contrastive Learning",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "utilize them to guide the learning of utterance rep-"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "resentations. Our objectives are (1) to bring utter-"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "Recent works (Chen et al., 2020; He et al., 2020a)",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "ances with the same emotion closer to their cor-"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "in unsupervised contrastive\nlearning provide\na",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "responding anchors and push utterances with dif-"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "similarity-based learning framework for represen-",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "ferent emotions farther away, and (2) to achieve a"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "tation learning. These methods maximize the simi-",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "more uniform distribution of anchors in the hyper-"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "larity between positive samples while minimizing",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "space for better classifying different emotions."
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "the similarity between negative sample pairs. To",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "The second stage (Section 3.5) is called emotion"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "make use of supervised information, supervised",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "anchor adaptation, which aims to further improve"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "contrastive learning (SupCon) (Gunel et al., 2020)",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "classification performance by slightly adjusting an-"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "aims to make the data that have the same label",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "chors. The anchors in the first stage can help the"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "closer in the representation space and push away",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "model learn separable representations of utterances."
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "those that have different\nlabels. However, Sup-",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "However, separated emotion anchors may not be"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "Con works poorly in data imbalance settings. To",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "located in the most representative positions of each"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "mitigate this problem, KCL (Kang et al., 2021)",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "category of utterance representation for the follow-"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "explicitly pursues a balanced representation space.",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "ing emotion recognition because contrastive learn-"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "TSC (Li et al., 2022b) uniformly set targets in the",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "ing in the first stage aims not to achieve this goal."
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "hypersphere and enforce data representations to",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "Therefore, we design the second stage to slightly"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "close to the targets. BCL (Zhu et al., 2022) regards",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "adjust the positions of emotion anchors to shift the"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "classifier weights as prototypes in the representa-",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "decision boundaries for better classification perfor-"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "tion space and incorporates them in the contrastive",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "mance.\nIn this stage, we freeze the parameters of"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "loss. LaCon (Zhang et al., 2022) incorporates label",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "the language model and only fine-tune the emo-"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "embedding for better language understanding. Our",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "tion anchors, as shown on the right side of Figure"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "method is inspired by TSC, differently, we incorpo-",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "3. Lastly, EACL matches the utterance represen-"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "rate emotion semantics in the representation space",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "tations with the most similar emotion anchors to"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "and dynamically adjust\nthe emotion anchors for",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "make predictions."
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "better classification.",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "3.3\nPrompt Context Encoding"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "3\nMethodology",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "Following previous work (Song et al., 2022), we"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "employ pre-trained language models and adopt"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "3.1\nProblem Definition",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "prompt tuning to transform the classification into"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "A conversation can be denoted as a sequence of",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "masked language modeling. An effective prompt"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "utterances {u1, u2, u3, ..., un}, each utterance ut",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "template aligns the downstream task with the large"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "is uttered by one of\nthe\nconversation speakers",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "semantic\ninformation\nlearned\nby\nthe\nlanguage"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "(m ≥ 2) speakers in the con-\nsj. There are m",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "model in the pre-training stage, which boosts the"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "versation, denoted as {s1, s2, ..., sm}. Given the",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "model’s performance in downstream tasks."
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "set of emotion labels E and conversation context",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "To predict the emotion of utterance ut, we take"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "the ERC task\n{(u1, su1), (u2, su2), ..., (ut, sut)},",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "k utterances before timestamp t as the context to"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "aims to predict emotion et(et ∈ E) for current ut-",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "Formally,\nthe input\nfor\nthe language\npredict et."
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "terance ut. E is a set of emotions. For instance,",
          "3.2\nModel Overview": ""
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "",
          "3.2\nModel Overview": "model is composed as:"
        },
        {
          "SACL (Hu et al., 2023)introduces adversarial ex-": "in the IEMOCAP dataset, E = {excited, frustrated,",
          "3.2\nModel Overview": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": "Utterance Repr\nEmotion Anchor"
        },
        {
          "<mask> token\nEmotion token": "Figure 3: Overview of our proposed framework. Left side introduces representation learning, which is composed of"
        },
        {
          "<mask> token\nEmotion token": "utterance representation and emotion anchor learning. Right side describes the process of adapting emotion anchors"
        },
        {
          "<mask> token\nEmotion token": "to the optimal positions for classification."
        },
        {
          "<mask> token\nEmotion token": "where Prompt P = \"For utterance ut, speaker st"
        },
        {
          "<mask> token\nEmotion token": "feels [mask]\" . We take the last hidden state of"
        },
        {
          "<mask> token\nEmotion token": "[mask] as utterance representation."
        },
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": "3.4\nStage One: Representation Learning"
        },
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": "In this section, we will introduce two main compo-"
        },
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": "nents of EACL in stage one: utterance representa-"
        },
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": "tion learning and emotion anchor learning."
        },
        {
          "<mask> token\nEmotion token": "3.4.1\nUtterance Representation Learning"
        },
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": "The objective\nin this\nsection is\nto acquire dis-"
        },
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": "cernible representations for each individual utter-"
        },
        {
          "<mask> token\nEmotion token": "ance. To accomplish this, we employ label encod-"
        },
        {
          "<mask> token\nEmotion token": "ings to generate emotion anchors and incorporate"
        },
        {
          "<mask> token\nEmotion token": "them into a contrastive learning framework. By"
        },
        {
          "<mask> token\nEmotion token": "utilizing these anchors, we can proficiently steer"
        },
        {
          "<mask> token\nEmotion token": "the process of representation learning."
        },
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": "Given a batch of samples X = {x1, x2, . . . , xb}"
        },
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": "∈ Rb×ℓ, where b, ℓ are batch size and max length"
        },
        {
          "<mask> token\nEmotion token": "of\ninput\nrespectively. We feed X into the pre-"
        },
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": "trained language model and get\nthe last hidden"
        },
        {
          "<mask> token\nEmotion token": "states Z = Encoder(X ). Then we use the hidden"
        },
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": "state of [mask] token at the end of the sentence as"
        },
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": "the representation of utterance ut. Finally, we ob-"
        },
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": "tain the representations of utterances with an MLP"
        },
        {
          "<mask> token\nEmotion token": ""
        },
        {
          "<mask> token\nEmotion token": "layer:"
        },
        {
          "<mask> token\nEmotion token": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "contrastive loss. sim represents a similarity func-": "tion, we adopt cosine similarity here.",
          "angle. The more dispersed emotion anchors are, the": "better their capacity to recognize similar emotions."
        },
        {
          "contrastive loss. sim represents a similarity func-": "In equation 4,\nthe\ninteractions between rep-",
          "angle. The more dispersed emotion anchors are, the": "Combining all\nthe components mentioned in"
        },
        {
          "contrastive loss. sim represents a similarity func-": "resentations\ncan\nbe\ndivided\ninto\nthree\ncompo-",
          "angle. The more dispersed emotion anchors are, the": "stage one,\nthe overall\nloss is a weighted average"
        },
        {
          "contrastive loss. sim represents a similarity func-": "nents:\nutterances-utterances, anchors-utterances,",
          "angle. The more dispersed emotion anchors are, the": "of cross-entropy loss, anchor angle loss, and con-"
        },
        {
          "contrastive loss. sim represents a similarity func-": "and anchors-anchors.\nRepresentations with the",
          "angle. The more dispersed emotion anchors are, the": "trastive loss, as given in equation 8."
        },
        {
          "contrastive loss. sim represents a similarity func-": "same label are brought closer to each other, while",
          "angle. The more dispersed emotion anchors are, the": ""
        },
        {
          "contrastive loss. sim represents a similarity func-": "those with different labels are pushed farther apart.",
          "angle. The more dispersed emotion anchors are, the": "(8)\nL = λ1(Lsup + λ2LAg) + (1 − λ1)LCE"
        },
        {
          "contrastive loss. sim represents a similarity func-": "The utterances-utterances interactions are similar to",
          "angle. The more dispersed emotion anchors are, the": ""
        },
        {
          "contrastive loss. sim represents a similarity func-": "traditional contrastive learning, while the anchors-",
          "angle. The more dispersed emotion anchors are, the": "where λ1 and λ2 are hyper-parameters to balance"
        },
        {
          "contrastive loss. sim represents a similarity func-": "utterances\ninteractions\nrepresent\nthe process of",
          "angle. The more dispersed emotion anchors are, the": "loss terms."
        },
        {
          "contrastive loss. sim represents a similarity func-": "anchor-guided utterance representation learning.",
          "angle. The more dispersed emotion anchors are, the": ""
        },
        {
          "contrastive loss. sim represents a similarity func-": "",
          "angle. The more dispersed emotion anchors are, the": "3.5\nStage Two: Emotion Anchor Adaptation"
        },
        {
          "contrastive loss. sim represents a similarity func-": "The anchors-anchors interaction ensures a better",
          "angle. The more dispersed emotion anchors are, the": ""
        },
        {
          "contrastive loss. sim represents a similarity func-": "distinction between different emotions.",
          "angle. The more dispersed emotion anchors are, the": ""
        },
        {
          "contrastive loss. sim represents a similarity func-": "",
          "angle. The more dispersed emotion anchors are, the": "In the first stage, we used emotion anchors gener-"
        },
        {
          "contrastive loss. sim represents a similarity func-": "Recent\nresearch (Gunel\net\nal., 2020) has\nin-",
          "angle. The more dispersed emotion anchors are, the": ""
        },
        {
          "contrastive loss. sim represents a similarity func-": "",
          "angle. The more dispersed emotion anchors are, the": "ated from emotion labels to guide the convergence"
        },
        {
          "contrastive loss. sim represents a similarity func-": "dicated that combining cross-entropy loss with",
          "angle. The more dispersed emotion anchors are, the": ""
        },
        {
          "contrastive loss. sim represents a similarity func-": "",
          "angle. The more dispersed emotion anchors are, the": "of utterance representations toward different emo-"
        },
        {
          "contrastive loss. sim represents a similarity func-": "contrastive learning facilitates\nlanguage models",
          "angle. The more dispersed emotion anchors are, the": ""
        },
        {
          "contrastive loss. sim represents a similarity func-": "",
          "angle. The more dispersed emotion anchors are, the": "tion clusters. These emotion anchors serve as rep-"
        },
        {
          "contrastive loss. sim represents a similarity func-": "with more discriminative ability. Therefore cross-",
          "angle. The more dispersed emotion anchors are, the": ""
        },
        {
          "contrastive loss. sim represents a similarity func-": "",
          "angle. The more dispersed emotion anchors are, the": "resentatives for each emotion, which are suitable"
        },
        {
          "contrastive loss. sim represents a similarity func-": "entropy loss is added to help improve representa-",
          "angle. The more dispersed emotion anchors are, the": ""
        },
        {
          "contrastive loss. sim represents a similarity func-": "",
          "angle. The more dispersed emotion anchors are, the": "to function as effective nearest-neighbor classifiers"
        },
        {
          "contrastive loss. sim represents a similarity func-": "tion learning. We additionally add a linear mapping",
          "angle. The more dispersed emotion anchors are, the": ""
        },
        {
          "contrastive loss. sim represents a similarity func-": "",
          "angle. The more dispersed emotion anchors are, the": "for utterance representations. However, separated"
        },
        {
          "contrastive loss. sim represents a similarity func-": "for classification:",
          "angle. The more dispersed emotion anchors are, the": ""
        },
        {
          "contrastive loss. sim represents a similarity func-": "",
          "angle. The more dispersed emotion anchors are, the": "emotion anchors trained from stage one may not"
        },
        {
          "contrastive loss. sim represents a similarity func-": "",
          "angle. The more dispersed emotion anchors are, the": "be located in the most representative positions of"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "IEMOCAP\n100\n20\n31\n4810\n1000\n1623\n6"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "MELD\n1038\n114\n280\n9989\n1109\n2610\n7"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "EmoryNLP\n659\n89\n79\n7551\n954\n984\n7"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "Table 1: Statistics of the three datasets, where CLS is"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "the number of classes."
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "4.4\nBaselines"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "For a comprehensive evaluation, we compare our"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "method with the following baselines:"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "(1) Graph-based model: DialogueGCN (Ghosal"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "et al., 2019)\nemploys GCNs\nto gather\ncontext"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "features\nfor\nlearning\nutterance\nrepresentations,"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "Shen (Shen et al., 2021) shows the performance of"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "replacing the feature extractor with Roberta-Large."
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "RGAT (Ishiwatari et al., 2020) proposes relational"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "position encodings to model both speaker relation-"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "ship and sequential information. DAG-ERC (Shen"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "et al., 2021) utilizes an acyclic graph neural net-"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "work to intuitively model a conversation’s natural"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "structure without introducing any external informa-"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "tion. DAG-ERC+HCL (Yang et al., 2022) pro-"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "poses a curriculum learning paradigm combined"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "with DAG-ERC for\nlearning from easy to hard."
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "SIGAT (Jia et al., 2023) models speaker and se-"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "quence information in a unified graph to learn the"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "interactive influence between them."
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "(2) Sequence-based model: COSMIC (Ghosal"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "et al., 2020) incorporates different elements of com-"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "monsense and leverages them to learn self-speaker"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "dependency. Cog-BART (Li et al., 2022a) applies"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "BART with contrastive learning to take response"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "generation into consideration. DialogueEIN (Liu"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "et al., 2022) designs emotion interaction and ten-"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "dency blocks to explicitly simulate emotion iner-"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "tia and stimulus. CoMPM (Lee and Lee, 2021)"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "utilizes pretrained models to directly learn contex-"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "tual information and track dialogue history. Sup-"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "Con (Gunel et al., 2020) is the vanilla supervised"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "SCCL (Yang et al., 2023)\ncontrastive learning."
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "conducts contrastive learning with 3-dimensional"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "affect representations. DIEU (Zhao et al., 2023a)"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "aims to solve the long-range context propagation"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "problem. CKCL (Tu et al., 2023) denoises in-"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "formation irrelevant context and knowledge when"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "training. MPLP (Zhang et al., 2023b) models"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": ""
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "the history and experience of\nspeakers and ex-"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "ploits paraphrasing to enlarge the difference be-"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "tween labels. Emocaps (Li et al., 2022c) devises"
        },
        {
          "train\ndev\ntest\ntrain\ndev\ntest": "transformer\nto a novel architecture, Emoformer,"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: reports the results of our method and the",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "Graph-based models",
          "MELD": "",
          "EmoryNLP": "",
          "Average": ""
        },
        {
          "Methods": "DialogueGCN (Ghosal et al., 2019)",
          "IEMOCAP": "64.91",
          "MELD": "63.02",
          "EmoryNLP": "38.10",
          "Average": "55.34"
        },
        {
          "Methods": "RGAT (Ishiwatari et al., 2020)",
          "IEMOCAP": "66.36",
          "MELD": "62.80",
          "EmoryNLP": "37.89",
          "Average": "55.68"
        },
        {
          "Methods": "DAG-ERC (Shen et al., 2021)",
          "IEMOCAP": "68.03",
          "MELD": "63.65",
          "EmoryNLP": "39.02",
          "Average": "56.9"
        },
        {
          "Methods": "DAG-ERC+HCL (Yang et al., 2022)",
          "IEMOCAP": "68.73",
          "MELD": "63.89",
          "EmoryNLP": "39.82",
          "Average": "57.48"
        },
        {
          "Methods": "SIGAT (Jia et al., 2023)",
          "IEMOCAP": "70.17",
          "MELD": "66.20",
          "EmoryNLP": "39.95",
          "Average": "58.77"
        },
        {
          "Methods": "",
          "IEMOCAP": "Sequence-based models",
          "MELD": "",
          "EmoryNLP": "",
          "Average": ""
        },
        {
          "Methods": "COSMIC (Ghosal et al., 2020)",
          "IEMOCAP": "65.25",
          "MELD": "65.21",
          "EmoryNLP": "38.11",
          "Average": "56.19"
        },
        {
          "Methods": "+CKCL (Tu et al., 2023)",
          "IEMOCAP": "67.16",
          "MELD": "66.21",
          "EmoryNLP": "40.23",
          "Average": "57.87"
        },
        {
          "Methods": "Cog-BART (Li et al., 2022a)",
          "IEMOCAP": "66.18",
          "MELD": "64.81",
          "EmoryNLP": "39.04",
          "Average": "56.68"
        },
        {
          "Methods": "DialogueEIN (Liu et al., 2022)",
          "IEMOCAP": "68.93",
          "MELD": "65.37",
          "EmoryNLP": "38.92",
          "Average": "57.74"
        },
        {
          "Methods": "CoMPM (Lee and Lee, 2021)",
          "IEMOCAP": "69.46",
          "MELD": "66.52",
          "EmoryNLP": "38.93",
          "Average": "58.3"
        },
        {
          "Methods": "SupCon (Gunel et al., 2020)",
          "IEMOCAP": "68.14",
          "MELD": "65.63",
          "EmoryNLP": "39.28",
          "Average": "57.68"
        },
        {
          "Methods": "Emocaps (Li et al., 2022c)",
          "IEMOCAP": "69.49",
          "MELD": "63.51",
          "EmoryNLP": "-",
          "Average": "-"
        },
        {
          "Methods": "SPCL+CL (Song et al., 2022)",
          "IEMOCAP": "67.19",
          "MELD": "65.74",
          "EmoryNLP": "39.52",
          "Average": "57.48"
        },
        {
          "Methods": "SACL (Hu et al., 2023)",
          "IEMOCAP": "69.22",
          "MELD": "66.45",
          "EmoryNLP": "39.65",
          "Average": "58.44"
        },
        {
          "Methods": "SCCL (Yang et al., 2023)",
          "IEMOCAP": "69.88",
          "MELD": "65.70",
          "EmoryNLP": "38.75",
          "Average": "58.11"
        },
        {
          "Methods": "DIEU (Zhao et al., 2023a)",
          "IEMOCAP": "69.90",
          "MELD": "66.43",
          "EmoryNLP": "40.12",
          "Average": "58.81"
        },
        {
          "Methods": "MPLP (Zhang et al., 2023b)",
          "IEMOCAP": "66.65",
          "MELD": "66.51",
          "EmoryNLP": "-",
          "Average": "-"
        },
        {
          "Methods": "ChatGPT 3-shot (Zhao et al., 2023b)",
          "IEMOCAP": "48.58",
          "MELD": "58.35",
          "EmoryNLP": "35.92",
          "Average": "47.62"
        },
        {
          "Methods": "EACL (ours)",
          "IEMOCAP": "70.41",
          "MELD": "67.12 †",
          "EmoryNLP": "40.24",
          "Average": "59.26†"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: reports the results of our method and the",
      "data": [
        {
          "EACL": "∆",
          "71.27": "+4.55",
          "67.76": "+3.80",
          "81.80": "+1.77",
          "73.32": "+1.03",
          "67.54": "+2.72",
          "51.29": "+7.33",
          "68.81": "+3.51",
          "70.41": "+3.22"
        },
        {
          "EACL": "",
          "71.27": "",
          "67.76": "",
          "81.80": "",
          "73.32": "",
          "67.54": "",
          "51.29": "",
          "68.81": "",
          "70.41": ""
        },
        {
          "EACL": "",
          "71.27": "",
          "67.76": "",
          "81.80": "",
          "73.32": "",
          "67.54": "",
          "51.29": "",
          "68.81": "",
          "70.41": ""
        },
        {
          "EACL": "",
          "71.27": "",
          "67.76": "",
          "81.80": "",
          "73.32": "",
          "67.54": "",
          "51.29": "",
          "68.81": "",
          "70.41": ""
        },
        {
          "EACL": "Methods",
          "71.27": "Fear",
          "67.76": "Neu",
          "81.80": "Ang",
          "73.32": "Dis",
          "67.54": "Surp",
          "51.29": "Joy",
          "68.81": "Avg",
          "70.41": "W-f1"
        },
        {
          "EACL": "SPCL+CL",
          "71.27": "26.59",
          "67.76": "77.92",
          "81.80": "54.40",
          "73.32": "30.94",
          "67.54": "59.26",
          "51.29": "60.34",
          "68.81": "50.43",
          "70.41": "65.74"
        },
        {
          "EACL": "EACL",
          "71.27": "23.54",
          "67.76": "80.44",
          "81.80": "54.01",
          "73.32": "33.86",
          "67.54": "60.48",
          "51.29": "65.22",
          "68.81": "51.42",
          "70.41": "67.12"
        },
        {
          "EACL": "∆",
          "71.27": "-3.05",
          "67.76": "+2.52",
          "81.80": "-0.39",
          "73.32": "+2.92",
          "67.54": "+1.22",
          "51.29": "+4.88",
          "68.81": "+0.99",
          "70.41": "+1.38"
        },
        {
          "EACL": "",
          "71.27": "",
          "67.76": "",
          "81.80": "",
          "73.32": "",
          "67.54": "",
          "51.29": "",
          "68.81": "",
          "70.41": ""
        },
        {
          "EACL": "",
          "71.27": "",
          "67.76": "",
          "81.80": "",
          "73.32": "",
          "67.54": "",
          "51.29": "",
          "68.81": "",
          "70.41": ""
        },
        {
          "EACL": "Methods",
          "71.27": "Joy",
          "67.76": "Sad",
          "81.80": "Pow",
          "73.32": "Neu",
          "67.54": "Pea",
          "51.29": "Sca",
          "68.81": "Avg",
          "70.41": "W-f1"
        },
        {
          "EACL": "SPCL+CL",
          "71.27": "53.52",
          "67.76": "31.61",
          "81.80": "10.28",
          "73.32": "51.40",
          "67.54": "16.83",
          "51.29": "39.51",
          "68.81": "35.34",
          "70.41": "39.52"
        },
        {
          "EACL": "EACL",
          "71.27": "52.73",
          "67.76": "30.77",
          "81.80": "15.27",
          "73.32": "49.76",
          "67.54": "23.48",
          "51.29": "41.18",
          "68.81": "36.45",
          "70.41": "40.24"
        },
        {
          "EACL": "∆",
          "71.27": "-0.79",
          "67.76": "-0.84",
          "81.80": "+4.99",
          "73.32": "-1.64",
          "67.54": "+6.65",
          "51.29": "+1.67",
          "68.81": "+1.11",
          "70.41": "+0.72"
        },
        {
          "EACL": "",
          "71.27": "",
          "67.76": "",
          "81.80": "",
          "73.32": "",
          "67.54": "",
          "51.29": "",
          "68.81": "",
          "70.41": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": ""
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "Original\n70.41\n67.12\n40.24"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": ""
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "w/o Emotion Anchor Learning\n69.78 (0.63 ↓)\n66.63(0.49 ↓)\n39.90(0.34 ↓)"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "w/o Classification Objective\n69.98(0.43 ↓)\n66.24(0.88 ↓)\n39.73(0.51 ↓)"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "w/o Anchor Inheritance\n69.79(0.62 ↓)\n67.03(0.09 ↓)\n38.46 (1.78 ↓)"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": ""
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "w/o Anchor Adaptation\n69.67(0.74 ↓)\n64.43(2.89 ↓)\n39.98 (0.26 ↓)"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": ""
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "Table 4: Ablation results on benchmark datasets."
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": ""
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "tations.\nEACL outperforms\nthe state-of-the-art"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "results on the IEMOCAP dataset by 0.92%,\nthe"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "MELD dataset by 0.6%, and the EmoryNLP dataset"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "by 0.59%. Besides, EACL has an overwhelming"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "performance advantage over ChatGPT, one possi-"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "ble reason is that the few-shot prompt setting may"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "not be enough to achieve satisfactory performance."
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "Table 3 reports the fine-grained performance on"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "benchmark datasets. EACL outperforms SPCL+CL"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "which is the most relevant method to us in most"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "emotion\ncategories\non\nall\nbenchmark\ndatasets."
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": ""
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "Specifically,\nin the IEMOCAP dataset, We have"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": ""
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "observed a significant improvement in performance"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "on two pairs of similar emotions, happy and excited"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "frustrated\nwith an increase of 7.33% and 4.55%,"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "and angry with an increase of 3.80% and 2.72%"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "respectively. Detailed performance analysis is pro-"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "vided in Appendix C."
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": ""
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "5.2\nAblation Study"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": ""
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "We conduct a series of experiments to confirm the"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "effectiveness of components in our method. The re-"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "sults are shown in Table 4. Removing any element"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "of EACL makes the overall performance worse."
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "To validate the effects of components in the first"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "stage, We remove the LAg which encourages the"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "angle of different emotion anchors to be uniform."
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "We can find that the lack of LAg results in a signifi-"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "cant decline in the performance of nearly 0.5%, as"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": ""
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "reported in line 2 in Table 4, indicating that emo-"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "tion anchor learning helps for separating utterance"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "representations. Also,\nthe removal of LCE drops"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "the performance by about 0.5% on average, the re-"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "sult demonstrates that supervised learning benefits"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "the fine-tuning of language models."
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "In the second stage, We explore whether adapt-"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "ing emotion anchors and emotion semantics are"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "necessary. Similar to classifier re-training (Kang"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "et al., 2019; Nam et al., 2023), we randomly initial-"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "ize emotion anchors that lie far from the data distri-"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "bution after learning the utterance representations."
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "Training from scratch is a cold start and cannot"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "reach the optimal position. This result\nin Line 4"
        },
        {
          "Dataset\nIEMOCAP\nMELD\nEmoryNLP": "verifies the importance of inheriting emotion an-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": ""
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "Table 4: Ablation results on benchmark datasets."
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": ""
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "tations.\nEACL outperforms\nthe state-of-the-art"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "results on the IEMOCAP dataset by 0.92%,\nthe"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "MELD dataset by 0.6%, and the EmoryNLP dataset"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "by 0.59%. Besides, EACL has an overwhelming"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "performance advantage over ChatGPT, one possi-"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "ble reason is that the few-shot prompt setting may"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "not be enough to achieve satisfactory performance."
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "Table 3 reports the fine-grained performance on"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "benchmark datasets. EACL outperforms SPCL+CL"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "which is the most relevant method to us in most"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "emotion\ncategories\non\nall\nbenchmark\ndatasets."
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": ""
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "Specifically,\nin the IEMOCAP dataset, We have"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": ""
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "observed a significant improvement in performance"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "on two pairs of similar emotions, happy and excited"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "frustrated\nwith an increase of 7.33% and 4.55%,"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "and angry with an increase of 3.80% and 2.72%"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "respectively. Detailed performance analysis is pro-"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "vided in Appendix C."
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": ""
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "5.2\nAblation Study"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": ""
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "We conduct a series of experiments to confirm the"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "effectiveness of components in our method. The re-"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "sults are shown in Table 4. Removing any element"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "of EACL makes the overall performance worse."
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "To validate the effects of components in the first"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "stage, We remove the LAg which encourages the"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "angle of different emotion anchors to be uniform."
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "We can find that the lack of LAg results in a signifi-"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "cant decline in the performance of nearly 0.5%, as"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": ""
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "reported in line 2 in Table 4, indicating that emo-"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "tion anchor learning helps for separating utterance"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "representations. Also,\nthe removal of LCE drops"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "the performance by about 0.5% on average, the re-"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "sult demonstrates that supervised learning benefits"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "the fine-tuning of language models."
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "In the second stage, We explore whether adapt-"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "ing emotion anchors and emotion semantics are"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "necessary. Similar to classifier re-training (Kang"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "et al., 2019; Nam et al., 2023), we randomly initial-"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "ize emotion anchors that lie far from the data distri-"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "bution after learning the utterance representations."
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "Training from scratch is a cold start and cannot"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "reach the optimal position. This result\nin Line 4"
        },
        {
          "w/ representation center\n69.84(0.57 ↓)\n66.49(0.63 ↓)\n39.84(0.38 ↓)": "verifies the importance of inheriting emotion an-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1.0": ""
        },
        {
          "1.0": "0.8"
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        },
        {
          "1.0": "0.6"
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        },
        {
          "1.0": "0.4"
        },
        {
          "1.0": ""
        },
        {
          "1.0": "0.2"
        },
        {
          "1.0": ""
        },
        {
          "1.0": "0.0"
        },
        {
          "1.0": ""
        },
        {
          "1.0": "0.2"
        },
        {
          "1.0": ""
        },
        {
          "1.0": "0.4"
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        },
        {
          "1.0": "100"
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        },
        {
          "1.0": "80"
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        },
        {
          "1.0": "60"
        },
        {
          "1.0": ""
        },
        {
          "1.0": "40"
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        },
        {
          "1.0": "20"
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        },
        {
          "1.0": "0"
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        },
        {
          "1.0": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dyadic motion capture database. Language resources": "and evaluation, 42:335–359.",
          "Conference on Empirical Methods in Natural Lan-": "guage Processing (EMNLP), pages 7360–7370."
        },
        {
          "dyadic motion capture database. Language resources": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and",
          "Conference on Empirical Methods in Natural Lan-": "Zhaohong Jia, Yunwei Shi, Weifeng Liu, Zhenhua"
        },
        {
          "dyadic motion capture database. Language resources": "Geoffrey Hinton. 2020.\nA simple framework for",
          "Conference on Empirical Methods in Natural Lan-": "Huang, and Xiao Sun. 2023.\nSpeaker-aware inter-"
        },
        {
          "dyadic motion capture database. Language resources": "contrastive learning of visual representations.\nIn In-",
          "Conference on Empirical Methods in Natural Lan-": "active graph attention network for emotion recogni-"
        },
        {
          "dyadic motion capture database. Language resources": "ternational conference on machine learning, pages",
          "Conference on Empirical Methods in Natural Lan-": "ACM Transactions on Asian\ntion in conversation."
        },
        {
          "dyadic motion capture database. Language resources": "1597–1607. PMLR.",
          "Conference on Empirical Methods in Natural Lan-": "and Low-Resource Language Information Process-"
        },
        {
          "dyadic motion capture database. Language resources": "",
          "Conference on Empirical Methods in Natural Lan-": "ing, 22(12):1–18."
        },
        {
          "dyadic motion capture database. Language resources": "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.",
          "Conference on Empirical Methods in Natural Lan-": ""
        },
        {
          "dyadic motion capture database. Language resources": "Simcse: Simple contrastive learning of sentence em-",
          "Conference on Empirical Methods in Natural Lan-": "Yuxin Jiang, Linhan Zhang, and Wei Wang. 2022.\nIm-"
        },
        {
          "dyadic motion capture database. Language resources": "beddings. arXiv preprint arXiv:2104.08821.",
          "Conference on Empirical Methods in Natural Lan-": "proved universal sentence embeddings with prompt-"
        },
        {
          "dyadic motion capture database. Language resources": "",
          "Conference on Empirical Methods in Natural Lan-": "based contrastive learning and energy-based learning."
        },
        {
          "dyadic motion capture database. Language resources": "Deepanway Ghosal, Navonil Majumder, Alexander Gel-",
          "Conference on Empirical Methods in Natural Lan-": "the Association for Computational\nIn Findings of"
        },
        {
          "dyadic motion capture database. Language resources": "bukh, Rada Mihalcea,\nand Soujanya Poria. 2020.",
          "Conference on Empirical Methods in Natural Lan-": "Linguistics: EMNLP 2022, pages 3021–3035."
        },
        {
          "dyadic motion capture database. Language resources": "Cosmic:\nCommonsense\nknowledge\nfor\nemotion",
          "Conference on Empirical Methods in Natural Lan-": ""
        },
        {
          "dyadic motion capture database. Language resources": "arXiv\npreprint\nidentification\nin\nconversations.",
          "Conference on Empirical Methods in Natural Lan-": "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram"
        },
        {
          "dyadic motion capture database. Language resources": "arXiv:2010.02795.",
          "Conference on Empirical Methods in Natural Lan-": "Singh, and Ashutosh Modi. 2022. Cogmen: Contex-"
        },
        {
          "dyadic motion capture database. Language resources": "",
          "Conference on Empirical Methods in Natural Lan-": "tualized gnn based multimodal emotion recognition."
        },
        {
          "dyadic motion capture database. Language resources": "Deepanway Ghosal, Navonil Majumder, Soujanya Poria,",
          "Conference on Empirical Methods in Natural Lan-": "arXiv preprint arXiv:2205.02455."
        },
        {
          "dyadic motion capture database. Language resources": "Niyati Chhaya, and Alexander Gelbukh. 2019. Dia-",
          "Conference on Empirical Methods in Natural Lan-": ""
        },
        {
          "dyadic motion capture database. Language resources": "loguegcn: A graph convolutional neural network for",
          "Conference on Empirical Methods in Natural Lan-": "Bingyi Kang, Yu Li, Sa Xie, Zehuan Yuan, and Jiashi"
        },
        {
          "dyadic motion capture database. Language resources": "emotion recognition in conversation. arXiv preprint",
          "Conference on Empirical Methods in Natural Lan-": "Feng. 2021. Exploring balanced feature spaces for"
        },
        {
          "dyadic motion capture database. Language resources": "arXiv:1908.11540.",
          "Conference on Empirical Methods in Natural Lan-": "representation learning.\nIn International Conference"
        },
        {
          "dyadic motion capture database. Language resources": "",
          "Conference on Empirical Methods in Natural Lan-": "on Learning Representations."
        },
        {
          "dyadic motion capture database. Language resources": "Beliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoy-",
          "Conference on Empirical Methods in Natural Lan-": ""
        },
        {
          "dyadic motion capture database. Language resources": "anov. 2020. Supervised contrastive learning for pre-",
          "Conference on Empirical Methods in Natural Lan-": "Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng"
        },
        {
          "dyadic motion capture database. Language resources": "trained language model fine-tuning. arXiv preprint",
          "Conference on Empirical Methods in Natural Lan-": "Yan, Albert Gordo, Jiashi Feng, and Yannis Kalan-"
        },
        {
          "dyadic motion capture database. Language resources": "arXiv:2011.01403.",
          "Conference on Empirical Methods in Natural Lan-": "tidis. 2019.\nDecoupling representation and clas-"
        },
        {
          "dyadic motion capture database. Language resources": "",
          "Conference on Empirical Methods in Natural Lan-": "arXiv preprint\nsifier\nfor\nlong-tailed recognition."
        },
        {
          "dyadic motion capture database. Language resources": "Biyang Guo, Songqiao Han, Xiao Han, Hailiang Huang,",
          "Conference on Empirical Methods in Natural Lan-": "arXiv:1910.09217."
        },
        {
          "dyadic motion capture database. Language resources": "and Ting Lu. 2021. Label confusion learning to en-",
          "Conference on Empirical Methods in Natural Lan-": ""
        },
        {
          "dyadic motion capture database. Language resources": "hance text classification models.\nIn Proceedings of",
          "Conference on Empirical Methods in Natural Lan-": "Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron"
        },
        {
          "dyadic motion capture database. Language resources": "the AAAI conference on artificial\nintelligence, vol-",
          "Conference on Empirical Methods in Natural Lan-": "Sarna,\nYonglong\nTian,\nPhillip\nIsola,\nAaron"
        },
        {
          "dyadic motion capture database. Language resources": "ume 35, pages 12929–12936.",
          "Conference on Empirical Methods in Natural Lan-": "Maschinot, Ce Liu, and Dilip Krishnan. 2020. Su-"
        },
        {
          "dyadic motion capture database. Language resources": "",
          "Conference on Empirical Methods in Natural Lan-": "Advances in neural\npervised contrastive learning."
        },
        {
          "dyadic motion capture database. Language resources": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and",
          "Conference on Empirical Methods in Natural Lan-": "information processing systems, 33:18661–18673."
        },
        {
          "dyadic motion capture database. Language resources": "Ross Girshick. 2020a. Momentum contrast for unsu-",
          "Conference on Empirical Methods in Natural Lan-": ""
        },
        {
          "dyadic motion capture database. Language resources": "pervised visual representation learning.\nIn Proceed-",
          "Conference on Empirical Methods in Natural Lan-": "Joosung Lee. 2022.\nThe emotion is not one-hot en-"
        },
        {
          "dyadic motion capture database. Language resources": "ings of the IEEE/CVF conference on computer vision",
          "Conference on Empirical Methods in Natural Lan-": "coding:\nLearning with grayscale\nlabel\nfor\nemo-"
        },
        {
          "dyadic motion capture database. Language resources": "and pattern recognition, pages 9729–9738.",
          "Conference on Empirical Methods in Natural Lan-": "arXiv preprint\ntion recognition in conversation."
        },
        {
          "dyadic motion capture database. Language resources": "",
          "Conference on Empirical Methods in Natural Lan-": "arXiv:2206.07359."
        },
        {
          "dyadic motion capture database. Language resources": "Pengcheng He, Xiaodong Liu,\nJianfeng Gao,\nand",
          "Conference on Empirical Methods in Natural Lan-": ""
        },
        {
          "dyadic motion capture database. Language resources": "Weizhu Chen. 2020b. Deberta: Decoding-enhanced",
          "Conference on Empirical Methods in Natural Lan-": "Joosung Lee and Wooin Lee. 2021. Compm: Context"
        },
        {
          "dyadic motion capture database. Language resources": "arXiv preprint\nbert with disentangled attention.",
          "Conference on Empirical Methods in Natural Lan-": "modeling with speaker’s pre-trained memory track-"
        },
        {
          "dyadic motion capture database. Language resources": "arXiv:2006.03654.",
          "Conference on Empirical Methods in Natural Lan-": "ing for emotion recognition in conversation. arXiv"
        },
        {
          "dyadic motion capture database. Language resources": "",
          "Conference on Empirical Methods in Natural Lan-": "preprint arXiv:2108.11626."
        },
        {
          "dyadic motion capture database. Language resources": "Dou Hu, Yinan Bao, Lingwei Wei, Wei Zhou,\nand",
          "Conference on Empirical Methods in Natural Lan-": ""
        },
        {
          "dyadic motion capture database. Language resources": "Songlin Hu. 2023. Supervised adversarial contrastive",
          "Conference on Empirical Methods in Natural Lan-": "Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng"
        },
        {
          "dyadic motion capture database. Language resources": "learning for emotion recognition in conversations.",
          "Conference on Empirical Methods in Natural Lan-": "Wang, and Sirui Wang. 2023.\nInstructerc: Reform-"
        },
        {
          "dyadic motion capture database. Language resources": "arXiv preprint arXiv:2306.01505.",
          "Conference on Empirical Methods in Natural Lan-": "ing emotion recognition in conversation with a re-"
        },
        {
          "dyadic motion capture database. Language resources": "",
          "Conference on Empirical Methods in Natural Lan-": "arXiv preprint\ntrieval multi-task llms framework."
        },
        {
          "dyadic motion capture database. Language resources": "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021a. Di-",
          "Conference on Empirical Methods in Natural Lan-": "arXiv:2309.11911."
        },
        {
          "dyadic motion capture database. Language resources": "aloguecrn: Contextual reasoning networks for emo-",
          "Conference on Empirical Methods in Natural Lan-": ""
        },
        {
          "dyadic motion capture database. Language resources": "arXiv preprint\ntion recognition in conversations.",
          "Conference on Empirical Methods in Natural Lan-": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan"
        },
        {
          "dyadic motion capture database. Language resources": "arXiv:2106.01978.",
          "Conference on Empirical Methods in Natural Lan-": "Ghazvininejad, Abdelrahman Mohamed, Omer Levy,"
        },
        {
          "dyadic motion capture database. Language resources": "",
          "Conference on Empirical Methods in Natural Lan-": "Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-"
        },
        {
          "dyadic motion capture database. Language resources": "Jingwen Hu, Yuchen Liu, Jinming Zhao, and Qin Jin.",
          "Conference on Empirical Methods in Natural Lan-": "noising sequence-to-sequence pre-training for natural"
        },
        {
          "dyadic motion capture database. Language resources": "2021b. Mmgcn: Multimodal fusion via deep graph",
          "Conference on Empirical Methods in Natural Lan-": "language generation, translation, and comprehension."
        },
        {
          "dyadic motion capture database. Language resources": "convolution network for emotion recognition in con-",
          "Conference on Empirical Methods in Natural Lan-": "arXiv preprint arXiv:1910.13461."
        },
        {
          "dyadic motion capture database. Language resources": "versation. arXiv preprint arXiv:2107.06779.",
          "Conference on Empirical Methods in Natural Lan-": ""
        },
        {
          "dyadic motion capture database. Language resources": "",
          "Conference on Empirical Methods in Natural Lan-": "Shimin Li, Hang Yan, and Xipeng Qiu. 2022a. Contrast"
        },
        {
          "dyadic motion capture database. Language resources": "Taichi Ishiwatari, Yuki Yasuda, Taro Miyazaki, and Jun",
          "Conference on Empirical Methods in Natural Lan-": "and generation make bart a good dialogue emotion"
        },
        {
          "dyadic motion capture database. Language resources": "Goto. 2020. Relation-aware graph attention networks",
          "Conference on Empirical Methods in Natural Lan-": "recognizer.\nIn Proceedings of the AAAI Conference"
        },
        {
          "dyadic motion capture database. Language resources": "with relational position encodings for emotion recog-",
          "Conference on Empirical Methods in Natural Lan-": "on Artificial Intelligence, volume 36, pages 11002–"
        },
        {
          "dyadic motion capture database. Language resources": "nition in conversations.\nIn Proceedings of the 2020",
          "Conference on Empirical Methods in Natural Lan-": "11010."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Yang, Rogerio S Feris, Piotr Indyk, and Dina Katabi.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "enhanced nearest neighbor mechanism for multi-"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "2022b.\nTargeted supervised contrastive learning",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "label text classification.\nIn Proceedings of the 60th"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "the\nfor\nlong-tailed recognition.\nIn Proceedings of",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Annual Meeting of the Association for Computational"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "IEEE/CVF Conference on Computer Vision and Pat-",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Linguistics (Volume 2:\nShort Papers), pages 672–"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "tern Recognition, pages 6918–6928.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "679."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Zaijing Li, Fengxiao Tang, Ming Zhao, and Yusen Zhu.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Kailai Yang, Tianlin Zhang, Hassan Alhuzali,\nand"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "2022c. Emocaps: Emotion capsule based model for",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Sophia Ananiadou. 2023. Cluster-level contrastive"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "conversational emotion recognition. arXiv preprint",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "learning for emotion recognition in conversations."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "arXiv:2203.13504.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "IEEE Transactions on Affective Computing."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Lin Yang, Yi Shen, Yue Mao, and Longjun Cai. 2022."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Yuchen Liu, Jinming Zhao, Jingwen Hu, Ruichen Li,",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Hybrid curriculum learning for emotion recognition"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "and Qin Jin. 2022. Dialogueein: Emotion interaction",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "the AAAI Con-\nin conversation.\nIn Proceedings of"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "network for dialogue affective analysis.\nIn Proceed-",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "ference on Artificial Intelligence, volume 36, pages"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "ings of the 29th International Conference on Compu-",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "11595–11603."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "tational Linguistics, pages 684–693.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Sayyed M Zahiri\nand Jinho D Choi. 2017.\nEmo-"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Navonil Majumder, Soujanya Poria, Devamanyu Haz-",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "tion detection on tv show transcripts with sequence-"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "arika, Rada Mihalcea, Alexander Gelbukh, and Erik",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "based convolutional neural networks. arXiv preprint"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Cambria. 2019. Dialoguernn: An attentive rnn for",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "arXiv:1708.04299."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "emotion detection in conversations.\nIn Proceedings",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "of the AAAI conference on artificial intelligence, vol-",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Duzhen Zhang, Feilong Chen, and Xiuyi Chen. 2023a."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "ume 33, pages 6818–6825.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Dualgats: Dual graph attention networks for emotion"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "recognition in conversations.\nIn Proceedings of the"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Aditya\nKrishna\nMenon,\nSadeep\nJayasumana,",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "61st Annual Meeting of the Association for Compu-"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Ankit\nSingh\nRawat,\nHimanshu\nJain,\nAndreas",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "tational Linguistics (Volume 1: Long Papers), pages"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Veit, and Sanjiv Kumar. 2020. Long-tail learning via",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "7395–7408."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "logit adjustment. arXiv preprint arXiv:2007.07314.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Ting Zhang, Zhuang Chen, Ming Zhong, and Tieyun"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Giung Nam,\nSunguk\nJang,\nand\nJuho\nLee.\n2023.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Qian. 2023b. Mimicking the thinking process for"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Decoupled\ntraining\nfor\nlong-tailed\nclassification",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "emotion recognition in conversation with prompts"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "arXiv\npreprint\nwith\nstochastic\nrepresentations.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "and paraphrasing. arXiv preprint arXiv:2306.06601."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "arXiv:2304.09426.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Yazhou Zhang, Mengyao Wang, Prayag Tiwari, Qiuchi"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Donovan Ong, Jian Su, Bin Chen, Anh Tuan Luu, Ashok",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Li, Benyou Wang, and Jing Qin. 2023c. Dialoguellm:"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Narendranath, Yue Li, Shuqi Sun, Yingzhan Lin, and",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Context and emotion knowledge-tuned llama mod-"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Haifeng Wang. 2022.\nIs discourse role important for",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "els for emotion recognition in conversations. arXiv"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "emotion recognition in conversation?\nIn Proceedings",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "preprint arXiv:2310.11374."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "of\nthe AAAI Conference on Artificial\nIntelligence,",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Zhenyu Zhang, Yuming Zhao, Meng Chen, and Xi-"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "volume 36, pages 11121–11129.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "aodong He. 2022. Label anchored contrastive learn-"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "arXiv preprint\ning for\nlanguage understanding."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "arXiv:2205.10227."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "jumder, Gautam Naik, Erik Cambria, and Rada Mi-",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "halcea. 2018.\nMeld:\nA multimodal multi-party",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Fei Zhao, Yuchen Shen, Zhen Wu, and Xinyu Dai. 2022."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "dataset\nfor\nemotion recognition in conversations.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Label-driven denoising framework for multi-label"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "arXiv preprint arXiv:1810.02508.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "few-shot aspect category detection. arXiv preprint"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "arXiv:2210.04220."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Weizhou Shen, Siyue Wu, Yunyi Yang, and Xiaojun",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Quan. 2021.\nDirected acyclic graph network for",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Shu Zhao, Weifeng Liu, Jie Chen, and Xiao Sun. 2023a."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "conversational emotion recognition. arXiv preprint",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Dieu: A dynamic interaction emotion unit for emo-"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "arXiv:2105.12907.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "tion recognition in conversation. ACM Transactions"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "on Asian and Low-Resource Language Information"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Xiaohui Song, Longtao Huang, Hui Xue, and Songlin",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Processing, 22(10):1–18."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Hu. 2022. Supervised prototypical contrastive learn-",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "ing for emotion recognition in conversation. arXiv",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Weixiang Zhao, Yanyan Zhao, Xin Lu, Shilong Wang,"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "preprint arXiv:2210.08713.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Yanpeng Tong,\nand Bing Qin. 2023b.\nIs\nchat-"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "gpt equipped with emotional dialogue capabilities?"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Geng Tu, Bin Liang, Ruibin Mao, Min Yang,\nand",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "arXiv preprint arXiv:2304.09582."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "Ruifeng Xu. 2023.\nContext or knowledge is not",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": ""
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "always necessary: A contrastive learning framework",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Peixiang Zhong, Di Wang, and Chunyan Miao. 2019."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "for emotion recognition in conversations.\nIn Find-",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "Knowledge-enriched transformer\nfor\nemotion de-"
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "ings of the Association for Computational Linguistics:",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "arXiv preprint\ntection in textual\nconversations."
        },
        {
          "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe": "ACL 2023, pages 14054–14067.",
          "Ran Wang, Xinyu Dai, et al. 2022. Contrastive learning-": "arXiv:1909.10681."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Jianggang Zhu,": "",
          "Zheng Wang,\nJingjing Chen, Yi-": "Ping Phoebe Chen, and Yu-Gang Jiang. 2022. Bal-"
        },
        {
          "Jianggang Zhu,": "anced\ncontrastive",
          "Zheng Wang,\nJingjing Chen, Yi-": "learning\nfor\nlong-tailed\nvisual"
        },
        {
          "Jianggang Zhu,": "recognition.",
          "Zheng Wang,\nJingjing Chen, Yi-": "In Proceedings of the IEEE/CVF Con-"
        },
        {
          "Jianggang Zhu,": "",
          "Zheng Wang,\nJingjing Chen, Yi-": "ference on Computer Vision and Pattern Recognition,"
        },
        {
          "Jianggang Zhu,": "pages 6908–6917.",
          "Zheng Wang,\nJingjing Chen, Yi-": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.8": "neutral\nneutral\n0.76\n0.034\n0.096\n0.036\n0.052\n0.018\n0.74\n0.026\n0.14\n0.023\n0.039\n0.029\n0.7"
        },
        {
          "0.8": "0.7"
        },
        {
          "0.8": "excited\nexcited\n0.6\n0.087\n0.68\n0.0033\n0.013\n0.21\n0.0067\n0.097\n0.63\n0\n0.017\n0.25\n0.01"
        },
        {
          "0.8": "0.6"
        },
        {
          "0.8": "frustrated\nfrustrated\n0.5\n0.5\n0.15\n0.016\n0.64\n0.029\n0\n0.16\n0.15\n0.013\n0.62\n0.031\n0\n0.18"
        },
        {
          "0.8": "emotions\nemotions\n0.4"
        },
        {
          "0.8": "0.4"
        },
        {
          "0.8": "sad\nsad\n0.065\n0\n0.09\n0.84\n0.0041\n0.0041\n0.09\n0\n0.098\n0.78\n0.02\n0.0082"
        },
        {
          "0.8": "0.3\n0.3"
        },
        {
          "0.8": "happy\nhappy\n0.084\n0.33\n0.007\n0.07\n0.51\n0\n0.2\n0.13\n0.34\n0.007\n0.098\n0.43\n0\n0.2"
        },
        {
          "0.8": "0.1\n0.1"
        },
        {
          "0.8": "angry\nangry\n0.041\n0\n0.22\n0.029\n0\n0.71\n0.024\n0.0059\n0.18\n0.041\n0\n0.75"
        },
        {
          "0.8": "0.0\n0.0"
        },
        {
          "0.8": "neutral\nexcited frustrated\nsad\nhappy\nangry\nneutral\nexcited frustrated\nsad\nhappy\nangry\nemotions\nemotions"
        },
        {
          "0.8": ""
        },
        {
          "0.8": "(a) IEMOCAP (EACL)\n(b) IEMOCAP (SPCL+CL)"
        },
        {
          "0.8": "0.8\nanger\n0.52\n0.043\n0.023\n0.075\n0.052\n0.11\n0.18\nanger\n0.51\n0.04\n0.018\n0.094\n0.036\n0.04\n0.26"
        },
        {
          "0.8": "0.6"
        },
        {
          "0.8": "0.7"
        },
        {
          "0.8": "disgust\n0.15\n0.31\n0.029\n0.044\n0.074\n0.12\n0.28\ndisgust\n0.12\n0.34\n0.015\n0.059\n0.015\n0.12\n0.34\n0.5\n0.6"
        },
        {
          "0.8": "fear\n0.12\n0.02\n0.26\n0.04\n0.12\n0.16\n0.28\nfear\n0.08\n0.04\n0.22\n0.04\n0.12\n0.12\n0.38"
        },
        {
          "0.8": "0.5\n0.4"
        },
        {
          "0.8": "emotions\nemotions\njoy\n0.055\n0.012\n0\n0.66\n0.032\n0.067\n0.17\njoy\n0.027\n0.015\n0.005\n0.66\n0.012\n0.05\n0.23\n0.4"
        },
        {
          "0.8": "0.3\n0.3\nsadness\n0.12\n0.014\n0.043\n0.048\n0.4\n0.058\n0.32\nsadness\n0.12\n0.0096\n0.034\n0.072\n0.34\n0.053\n0.38"
        },
        {
          "0.8": "0.2"
        },
        {
          "0.8": "0.2\nsurprise\n0.078\n0.021\n0.011\n0.1\n0.028\n0.65\n0.11\nsurprise\n0.085\n0.011\n0.011\n0.13\n0.014\n0.59\n0.16\n0.1"
        },
        {
          "0.8": "0.1"
        },
        {
          "0.8": "neutral\n0.035\n0.008\n0.01\n0.057\n0.036\n0.042\n0.81\nneutral\n0.23\n0.0089 0.0063\n0.043\n0.018\n0.026\n0.67"
        },
        {
          "0.8": "0.0\nanger\ndisgust\nfear\njoy\nsadnesssurprise neutral\nanger\ndisgust\nfear\njoy\nsadnesssurprise neutral"
        },
        {
          "0.8": "emotions\nemotions"
        },
        {
          "0.8": ""
        },
        {
          "0.8": "(c) MELD (EACL)\n(d) MELD (SPCL+CL)"
        },
        {
          "0.8": "0.7\njoyful\n0.62\n0.28\n0.0035\n0.046\n0.035\n0.0035\n0.014\n0.5"
        },
        {
          "0.8": "joyful\n0.53\n0.17\n0.085\n0.053\n0.064\n0.071\n0.028"
        },
        {
          "0.8": "0.6"
        },
        {
          "0.8": "neutral\n0.14\n0.7\n0.011\n0.034\n0.074\n0.023\n0.011\nneutral\n0.1\n0.51\n0.072\n0.046\n0.13\n0.11\n0.029"
        },
        {
          "0.8": "0.4"
        },
        {
          "0.8": "0.5"
        },
        {
          "0.8": "powerful\n0.41\n0.28\n0\n0.1\n0.12\n0.041\n0.048\npowerful\n0.32\n0.12\n0.12\n0.097\n0.15\n0.12\n0.076"
        },
        {
          "0.8": ""
        },
        {
          "0.8": "0.4\n0.3"
        },
        {
          "0.8": "emotions\nemotions\nmad\n0.097\n0.25\n0.035\n0.47\n0.12\n0.018\n0.018\nmad\n0.071\n0.097\n0.16\n0.46\n0.12\n0.044\n0.044"
        },
        {
          "0.8": "0.3"
        },
        {
          "0.8": "scared\n0.12\n0.36\n0.027\n0.082\n0.32\n0.016\n0.077\nscared\n0.071\n0.17\n0.049\n0.077\n0.48\n0.093\n0.06\n0.2"
        },
        {
          "0.8": "0.2"
        },
        {
          "0.8": "peaceful\n0.22\n0.48\n0.019\n0.063\n0.094\n0.082\n0.038\npeaceful\n0.19\n0.24\n0.13\n0.044\n0.13\n0.24\n0.038"
        },
        {
          "0.8": "0.1\n0.1"
        },
        {
          "0.8": "sad\n0.061\n0.29\n0.02\n0.18\n0.12\n0.031\n0.3\nsad\n0.092\n0.15\n0.061\n0.16\n0.18\n0.092\n0.26"
        },
        {
          "0.8": "0.0"
        },
        {
          "0.8": "joyful\nneutral powerful mad\nscared peaceful\nsad\njoyful\nneutral powerful mad\nscared peaceful\nsad"
        },
        {
          "0.8": "emotions\nemotions"
        },
        {
          "0.8": ""
        },
        {
          "0.8": "(e) EmoryNLP (EACL)\n(f) EmoryNLP (SPCL+CL)"
        },
        {
          "0.8": ""
        },
        {
          "0.8": ""
        },
        {
          "0.8": "Figure 7: The normalized confusion matrix of\nthree"
        },
        {
          "0.8": "benchmark datasets, each row is the true classes and"
        },
        {
          "0.8": ""
        },
        {
          "0.8": "column is predictions. The Coordinate i, j means the"
        },
        {
          "0.8": ""
        },
        {
          "0.8": "percentage of emotion i predicted to be emotion j."
        },
        {
          "0.8": ""
        },
        {
          "0.8": ""
        },
        {
          "0.8": ""
        },
        {
          "0.8": "C\nDetailed Performance Analysis"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": ""
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": ""
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": "Hyperparameters"
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": ""
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": "λ1"
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": "λ2"
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": ""
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": "Temperature τ"
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": ""
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": "Epochs"
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": ""
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": "Maximum length"
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": ""
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": "Learning rate"
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": "Dropout"
        },
        {
          "1.0} and τ in { 0.05, 0.07, 0.1, 0.15, 0.2}.": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 7: The results indicate that our learning",
      "data": [
        {
          "SPCL+CL": "",
          "66.72": "",
          "63.96": "",
          "80.03": "",
          "72.29": "",
          "64.82": "",
          "43.96": "",
          "65.30": "",
          "67.19": ""
        },
        {
          "SPCL+CL": "Methods",
          "66.72": "Fear",
          "63.96": "Neu",
          "80.03": "Ang",
          "72.29": "Dis",
          "64.82": "Surp",
          "43.96": "Joy",
          "65.30": "Avg",
          "67.19": "W-f1"
        },
        {
          "SPCL+CL": "Deberta",
          "66.72": "34.0",
          "63.96": "80.43",
          "80.03": "55.28",
          "72.29": "37.59",
          "64.82": "60.85",
          "43.96": "65.34",
          "65.30": "53.99",
          "67.19": "67.8"
        },
        {
          "SPCL+CL": "PromCSE",
          "66.72": "23.59",
          "63.96": "81.0",
          "80.03": "54.96",
          "72.29": "30.53",
          "64.82": "59.51",
          "43.96": "65.12",
          "65.30": "51.15",
          "67.19": "67.38"
        },
        {
          "SPCL+CL": "SPCL+CL",
          "66.72": "26.59",
          "63.96": "77.92",
          "80.03": "54.40",
          "72.29": "30.94",
          "64.82": "59.26",
          "43.96": "60.34",
          "65.30": "50.43",
          "67.19": "65.74"
        },
        {
          "SPCL+CL": "",
          "66.72": "",
          "63.96": "",
          "80.03": "",
          "72.29": "",
          "64.82": "",
          "43.96": "",
          "65.30": "",
          "67.19": ""
        },
        {
          "SPCL+CL": "Methods",
          "66.72": "Joy",
          "63.96": "Sad",
          "80.03": "Pow",
          "72.29": "Neu",
          "64.82": "Pea",
          "43.96": "Sca",
          "65.30": "Avg",
          "67.19": "W-f1"
        },
        {
          "SPCL+CL": "Deberta",
          "66.72": "54.04",
          "63.96": "28.74",
          "80.03": "21.54",
          "72.29": "51.75",
          "64.82": "18.12",
          "43.96": "42.52",
          "65.30": "36.92",
          "67.19": "41.09"
        },
        {
          "SPCL+CL": "PromCSE",
          "66.72": "54.42",
          "63.96": "28.33",
          "80.03": "14.21",
          "72.29": "51.64",
          "64.82": "23.42",
          "43.96": "41.30",
          "65.30": "36.68",
          "67.19": "40.93"
        },
        {
          "SPCL+CL": "SPCL+CL",
          "66.72": "53.52",
          "63.96": "31.61",
          "80.03": "10.28",
          "72.29": "51.40",
          "64.82": "16.83",
          "43.96": "39.51",
          "65.30": "35.34",
          "67.19": "39.52"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "2",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "In International conference on machine learning"
    },
    {
      "citation_id": "3",
      "title": "Simcse: Simple contrastive learning of sentence embeddings",
      "authors": [
        "Tianyu Gao",
        "Xingcheng Yao",
        "Danqi Chen"
      ],
      "year": "2021",
      "venue": "Simcse: Simple contrastive learning of sentence embeddings",
      "arxiv": "arXiv:2104.08821"
    },
    {
      "citation_id": "4",
      "title": "Alexander Gelbukh, Rada Mihalcea, and Soujanya Poria",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Commonsense knowledge for emotion identification in conversations",
      "arxiv": "arXiv:2010.02795"
    },
    {
      "citation_id": "5",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "6",
      "title": "Supervised contrastive learning for pretrained language model fine-tuning",
      "authors": [
        "Beliz Gunel",
        "Jingfei Du"
      ],
      "year": "2020",
      "venue": "Supervised contrastive learning for pretrained language model fine-tuning",
      "arxiv": "arXiv:2011.01403"
    },
    {
      "citation_id": "7",
      "title": "Label confusion learning to enhance text classification models",
      "authors": [
        "Biyang Guo",
        "Songqiao Han",
        "Xiao Han",
        "Hailiang Huang",
        "Ting Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "8",
      "title": "Saining Xie, and Ross Girshick. 2020a. Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "Kaiming He",
        "Haoqi Fan",
        "Yuxin Wu"
      ],
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "Deberta: Decoding-enhanced bert with disentangled attention",
      "authors": [
        "Pengcheng He",
        "Xiaodong Liu",
        "Jianfeng Gao",
        "Weizhu Chen"
      ],
      "year": "2020",
      "venue": "Deberta: Decoding-enhanced bert with disentangled attention",
      "arxiv": "arXiv:2006.03654"
    },
    {
      "citation_id": "10",
      "title": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Yinan Bao",
        "Lingwei Wei",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "year": "2023",
      "venue": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "arxiv": "arXiv:2306.01505"
    },
    {
      "citation_id": "11",
      "title": "2021a. Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "venue": "2021a. Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "arxiv": "arXiv:2106.01978"
    },
    {
      "citation_id": "12",
      "title": "2021b. Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "venue": "2021b. Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "arxiv": "arXiv:2107.06779"
    },
    {
      "citation_id": "13",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Speaker-aware interactive graph attention network for emotion recognition in conversation",
      "authors": [
        "Zhaohong Jia",
        "Yunwei Shi",
        "Weifeng Liu",
        "Zhenhua Huang",
        "Xiao Sun"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing"
    },
    {
      "citation_id": "15",
      "title": "Improved universal sentence embeddings with promptbased contrastive learning and energy-based learning",
      "authors": [
        "Yuxin Jiang",
        "Linhan Zhang",
        "Wei Wang"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022"
    },
    {
      "citation_id": "16",
      "title": "Cogmen: Contextualized gnn based multimodal emotion recognition",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain"
      ],
      "year": "2022",
      "venue": "Cogmen: Contextualized gnn based multimodal emotion recognition",
      "arxiv": "arXiv:2205.02455"
    },
    {
      "citation_id": "17",
      "title": "Exploring balanced feature spaces for representation learning",
      "authors": [
        "Bingyi Kang",
        "Yu Li",
        "Sa Xie",
        "Zehuan Yuan",
        "Jiashi Feng"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "18",
      "title": "Decoupling representation and classifier for long-tailed recognition",
      "authors": [
        "Bingyi Kang",
        "Saining Xie",
        "Marcus Rohrbach",
        "Zhicheng Yan",
        "Albert Gordo",
        "Jiashi Feng",
        "Yannis Kalantidis"
      ],
      "year": "2019",
      "venue": "Decoupling representation and classifier for long-tailed recognition",
      "arxiv": "arXiv:1910.09217"
    },
    {
      "citation_id": "19",
      "title": "Supervised contrastive learning",
      "authors": [
        "Prannay Khosla",
        "Piotr Teterwak",
        "Chen Wang",
        "Aaron Sarna",
        "Yonglong Tian",
        "Phillip Isola",
        "Aaron Maschinot",
        "Ce Liu",
        "Dilip Krishnan"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "20",
      "title": "The emotion is not one-hot encoding: Learning with grayscale label for emotion recognition in conversation",
      "authors": [
        "Joosung Lee"
      ],
      "year": "2022",
      "venue": "The emotion is not one-hot encoding: Learning with grayscale label for emotion recognition in conversation",
      "arxiv": "arXiv:2206.07359"
    },
    {
      "citation_id": "21",
      "title": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2021",
      "venue": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "arxiv": "arXiv:2108.11626"
    },
    {
      "citation_id": "22",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "23",
      "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "Mike Lewis",
        "Yinhan Liu",
        "Naman Goyal",
        "Marjan Ghazvininejad",
        "Abdelrahman Mohamed",
        "Omer Levy",
        "Ves Stoyanov",
        "Luke Zettlemoyer"
      ],
      "year": "2019",
      "venue": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "arxiv": "arXiv:1910.13461"
    },
    {
      "citation_id": "24",
      "title": "2022a. Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "Shimin Li",
        "Hang Yan",
        "Xipeng Qiu"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "2022b. Targeted supervised contrastive learning for long-tailed recognition",
      "authors": [
        "Tianhong Li",
        "Peng Cao",
        "Yuan Yuan",
        "Lijie Fan",
        "Yuzhe Yang",
        "S Rogerio",
        "Piotr Feris",
        "Dina Indyk",
        "Katabi"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "2022c. Emocaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "venue": "2022c. Emocaps: Emotion capsule based model for conversational emotion recognition",
      "arxiv": "arXiv:2203.13504"
    },
    {
      "citation_id": "27",
      "title": "Dialogueein: Emotion interaction network for dialogue affective analysis",
      "authors": [
        "Yuchen Liu",
        "Jinming Zhao",
        "Jingwen Hu",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "28",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "29",
      "title": "Long-tail learning via logit adjustment",
      "authors": [
        "Aditya Krishna Menon",
        "Sadeep Jayasumana",
        "Ankit Singh Rawat",
        "Himanshu Jain",
        "Andreas Veit",
        "Sanjiv Kumar"
      ],
      "year": "2020",
      "venue": "Long-tail learning via logit adjustment",
      "arxiv": "arXiv:2007.07314"
    },
    {
      "citation_id": "30",
      "title": "Decoupled training for long-tailed classification with stochastic representations",
      "authors": [
        "Giung Nam",
        "Sunguk Jang",
        "Juho Lee"
      ],
      "year": "2023",
      "venue": "Decoupled training for long-tailed classification with stochastic representations",
      "arxiv": "arXiv:2304.09426"
    },
    {
      "citation_id": "31",
      "title": "Is discourse role important for emotion recognition in conversation?",
      "authors": [
        "Donovan Ong",
        "Jian Su",
        "Bin Chen",
        "Anh Luu",
        "Ashok Narendranath",
        "Yue Li",
        "Shuqi Sun",
        "Yingzhan Lin",
        "Haifeng Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "33",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Directed acyclic graph network for conversational emotion recognition",
      "arxiv": "arXiv:2105.12907"
    },
    {
      "citation_id": "34",
      "title": "Supervised prototypical contrastive learning for emotion recognition in conversation",
      "authors": [
        "Xiaohui Song",
        "Longtao Huang",
        "Hui Xue",
        "Songlin Hu"
      ],
      "year": "2022",
      "venue": "Supervised prototypical contrastive learning for emotion recognition in conversation",
      "arxiv": "arXiv:2210.08713"
    },
    {
      "citation_id": "35",
      "title": "Context or knowledge is not always necessary: A contrastive learning framework for emotion recognition in conversations",
      "authors": [
        "Geng Tu",
        "Bin Liang",
        "Ruibin Mao",
        "Min Yang",
        "Ruifeng Xu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "36",
      "title": "Contrastive learningenhanced nearest neighbor mechanism for multilabel text classification",
      "authors": [
        "Ran Wang",
        "Xinyu Dai"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "37",
      "title": "Cluster-level contrastive learning for emotion recognition in conversations",
      "authors": [
        "Kailai Yang",
        "Tianlin Zhang",
        "Hassan Alhuzali",
        "Sophia Ananiadou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "Lin Yang",
        "Yi Shen",
        "Yue Mao",
        "Longjun Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Emotion detection on tv show transcripts with sequencebased convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho D Zahiri",
        "Choi"
      ],
      "year": "2017",
      "venue": "Emotion detection on tv show transcripts with sequencebased convolutional neural networks",
      "arxiv": "arXiv:1708.04299"
    },
    {
      "citation_id": "40",
      "title": "2023a. Dualgats: Dual graph attention networks for emotion recognition in conversations",
      "authors": [
        "Duzhen Zhang",
        "Feilong Chen",
        "Xiuyi Chen"
      ],
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "41",
      "title": "2023b. Mimicking the thinking process for emotion recognition in conversation with prompts and paraphrasing",
      "authors": [
        "Ting Zhang",
        "Zhuang Chen",
        "Ming Zhong",
        "Tieyun Qian"
      ],
      "venue": "2023b. Mimicking the thinking process for emotion recognition in conversation with prompts and paraphrasing",
      "arxiv": "arXiv:2306.06601"
    },
    {
      "citation_id": "42",
      "title": "Benyou Wang, and Jing Qin. 2023c. Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations",
      "authors": [
        "Yazhou Zhang",
        "Mengyao Wang",
        "Prayag Tiwari",
        "Qiuchi Li"
      ],
      "venue": "Benyou Wang, and Jing Qin. 2023c. Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations",
      "arxiv": "arXiv:2310.11374"
    },
    {
      "citation_id": "43",
      "title": "Label anchored contrastive learning for language understanding",
      "authors": [
        "Zhenyu Zhang",
        "Yuming Zhao",
        "Meng Chen",
        "Xiaodong He"
      ],
      "year": "2022",
      "venue": "Label anchored contrastive learning for language understanding",
      "arxiv": "arXiv:2205.10227"
    },
    {
      "citation_id": "44",
      "title": "Label-driven denoising framework for multi-label few-shot aspect category detection",
      "authors": [
        "Fei Zhao",
        "Yuchen Shen",
        "Zhen Wu",
        "Xinyu Dai"
      ],
      "year": "2022",
      "venue": "Label-driven denoising framework for multi-label few-shot aspect category detection",
      "arxiv": "arXiv:2210.04220"
    },
    {
      "citation_id": "45",
      "title": "2023a. Dieu: A dynamic interaction emotion unit for emotion recognition in conversation",
      "authors": [
        "Shu Zhao",
        "Weifeng Liu",
        "Jie Chen",
        "Xiao Sun"
      ],
      "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing"
    },
    {
      "citation_id": "46",
      "title": "Yanpeng Tong, and Bing Qin. 2023b. Is chatgpt equipped with emotional dialogue capabilities? arXiv preprint",
      "authors": [
        "Weixiang Zhao",
        "Yanyan Zhao",
        "Xin Lu",
        "Shilong Wang"
      ],
      "venue": "Yanpeng Tong, and Bing Qin. 2023b. Is chatgpt equipped with emotional dialogue capabilities? arXiv preprint",
      "arxiv": "arXiv:2304.09582"
    },
    {
      "citation_id": "47",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "arxiv": "arXiv:1909.10681"
    },
    {
      "citation_id": "48",
      "title": "Balanced contrastive learning for long-tailed visual recognition",
      "authors": [
        "Jianggang Zhu",
        "Zheng Wang",
        "Jingjing Chen",
        "Yi-Ping Phoebe Chen",
        "Yu-Gang Jiang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    }
  ]
}