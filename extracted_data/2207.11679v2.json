{
  "paper_id": "2207.11679v2",
  "title": "Affective Behaviour Analysis Using Pretrained Model With Facial Prior",
  "published": "2022-07-24T07:28:08Z",
  "authors": [
    "Yifan Li",
    "Haomiao Sun",
    "Zhaori Liu",
    "Hu Han"
  ],
  "keywords": [
    "Multi-task affective behaviour analysis",
    "AU recognition",
    "expression recognition",
    "VA regression",
    "facial prior",
    "MAE",
    "ABAW4"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective behavior analysis has aroused researchers' attention due to its broad applications. However, it is labor exhaustive to obtain accurate annotations for massive face images. Thus, we propose to utilize the prior facial information via Masked Auto-Encoder (MAE) pretrained on unlabeled face images. Furthermore, we combine MAE pretrained Vision Transformer (ViT) and AffectNet pretrained CNN to perform multi-task emotion recognition. We notice that expression and action unit (AU) scores are pure and intact features for valence-arousal (VA) regression. As a result, we utilize AffectNet pretrained CNN to extract expression scores concatenating with expression and AU scores from ViT to obtain the final VA features. Moreover, we also propose a co-training framework with two parallel MAE pretrained ViTs for expression recognition tasks. In order to make the two views independent, we randomly mask most patches during the training process. Then, JS divergence is performed to make the predictions of the two views as consistent as possible. The results on ABAW4 show that our methods are effective, and our team reached 2nd place in the multi-task learning (MTL) challenge and 4th place in the learning from synthetic data (LSD) challenge. Code is available 3 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective behaviour analysis such as facial expression recognition (EXPR)  [21] , action unit (AU) recognition  [24]  and valence arousal (VA) regression  [23] , raised ⋆ These authors contribute equally to this work. 3 https://github.com/JackYFL/EMMA_CoTEX_ABAW4 much attention due to its wide application scenarios. With the superior performance of deep learning, the traditional artificial designed emotional representations are gradually replaced by the deep neural networks (DNNs) extracted ones. While DNNs-based methods perform well in affective behaviour analysis tasks, the limited size of the existing emotion benchmark has hindered the recognition performance and generalization ability.\n\nAlthough it costs much to obtain data with accurate emotion annotations, we can acquire unlabeled face images more easily. To capitalize on the massive unlabeled face images, we propose to learn facial prior knowledge using selfsupervised learning methods. In computer vision field, general self-supervised learning methods can be summarized into two major directions, contrastive learning based methods, e.g., MoCo  [6] , SimCLR  [2] , and generative methods, e.g., BeiT  [1] , Masked Auto-Encoder (MAE)  [5] . SSL-based methods have reached great success, achieving even better performance compared with supervised pretrained ones in some downstream tasks. Inspired by MAE, which randomly masks image patches and utilizes vision transformer  [4]  (ViT) to reconstruct pixels and learn the intrinsic relationships and discriminant representations of patches, we propose to learn facial prior using MAE on massive unlabeled face images. We suppose MAE could model the relationship of different face components, which contains the prior about face structure and has a better parameter initialization compared with the ImageNet pretrained model.\n\nBased on MAE pretrained ViT, we propose a simple but effective framework called Emotion Multi-Model Aggregation (EMMA, see Fig.  1 ) method for multi-task affective behaviour analysis, i.e., EXPR, AU recognition and VA regression. According to experiment results, we find that it's easier to overfit for VA regression task when finetuning MAE pretrained ViT for all three tasks. As a result, we propose to use an extra convolutional neural network (CNN) to extract features for VA regression tasks. We found that expression scores could provide pure and intact features for VA regression. Thus we choose to use a CNN (DAN  [32] ) pretrained on an EXPR benchmark AffectNet that contains expression prior knowledge to extract VA features. Furthermore, we also utilize the scores of both EXPR and AU recognition extracted by ViT to aid VA regression. Note that we only finetune the linear layer for VA regression to prevent overfitting.\n\nMoreover, we also propose a masked Co-Training method for EXpression recognition (masked CoTEX, see Fig.  2 ), in which we use MAE pretrained ViT as the backbones of two views. To form two information-complementary views, we randomly mask most patches of the face images. Inspired by MLCT  [33]  and MLCR  [24] , we also use Jenson-Shannon (JS) divergence to constrain the predictions from both views to be consistent to the average distribution of two predictions. Different from MLCT or MLCR, we apply JS divergence to singlelabel classification (e.g., expression) by modifying the entropy, and this method is prepared for supervised learning.\n\nThe contributions of this paper can be summarized into the following aspects.\n\n-We use face images to pretrain MAE, which shows better performance than ImageNet pretrained MAE for affective behaviour analysis related tasks. -We design EMMA, which uses ViT (pretrained on unlabeled face images) and CNN (pretrained on expression benchmark) to extract features for multitask affective behaviour analysis. We also propose to use the concatenation of expression scores extracted by CNN and ViT, and the AU scores extracted by ViT to finetune the linear layer for VA regression. -We propose masked CoTEX for EXPR, which is a co-training framework utilizing JS divergence on two random masked views to make them consistent. We find that a large mask ratio could not only improve training speed and decrease memory usage, but also increase accuracy. Furthermore, we also find that the accuracy could be improved further by increasing the batch size.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "In this section, we first review studies on multi-task affective behaviour analysis and then introduce related work on static image-based EXPR.\n\nMulti-task Affective Behaviour Analysis. The MTL affective behaviour analysis task refers to the simultaneous analysis of EXPR, AU recognition, and VA regression, etc. Unlike the experimental setup in ABAW and ABAW2, where three different tasks were completed independently, ABAW3 presents an integrated metric and evaluates the performance of all three tasks simultaneously. Deng  [3]  employs psychological prior knowledge for multi-task estimation, which uses local features for AU recognition and merges the messages of different regions for EXPR and VA. Jeong et al.  [8]  apply the knowledge distillation technique for a better generalization performance and the domain adaptation techniques to improve accuracy in target domains. Savchenko et al.  [11]  used a lightweight EfficientNet model to develop a real-time framework and improve performance by pre-training based on additional data. Unlike previous methods for MTL, we utilize the facial prior information in both unlabeled face images and expression benchmark to improve the performance.\n\nExpression Recognition. The aim of expression recognition is to recognize basic human expression categories. In the ABAW3 competition, some researchers utilize multi-modal information such as audio and text to improve the model performance and achieve a better ranking  [11, 37] . However, these methods have higher requirements for data collection. Therefore, it is worth exploring how to use static images for a more generalized usage scenario. Jeong et al.  [9]  use an affinity loss approach, which uses affinity loss to train a feature extractor for images. In addition, they propose a multi-head attention network in a coordinated manner to extract diverse attention for EXPR. Xue et al.  [34]  propose a twostage CFC network that separates negative and positive expressions, and then distinguishes between similar ones. Phan et al.  [25]  use a pretrained model Reg-Net  [27]  as a backbone and introduce the Transformer for better modelling the temporal information. Different from the previous methods, our masked CoTEX uses a masked co-training framework which consists of two ViTs pretrained by MAE on unlabeled face images to achieve better performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "In this section, we first formulate the problem, then introduce EMMA for multitask affective behaviour analysis and masked CoTEX for EXPR.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Formulation",
      "text": "Let X = {x i ∈ R C×H×W , i = 1, 2, ..., N } and Y denotes face images and according labels, respectively. For multi-task affective behaviour analysis, the labels Y = {y i M T L , i = 1, 2, ..., N } consists of three sub-task labels, i.e.,\n\nwhere y i V A , y i EXP , y i AU indicate VA labels, EXP labels and AU labels, respectively. y i V A is a two dimension vector representing valence and arousal in the range of [-1, 1]. y i EXP is an integer ranging from 0 to 7 representing one of eight expression categories, i.e., neutral, anger, disgust, fear, happiness, sadness, surprise, and other. y i AU includes 12 AU labels, i.e., AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23, AU24, AU25 and AU26. If a face is invisible due to large pose or occlusion, the values of y i V A , y i EXP and y i AU can be -5, -1 and 0, respectively. For EXPR task, Y = {y i EXP ∈ Z, i = 1, 2, ..., N }. There are only six expression categories in ABAW4  [12]  synthetic expression dataset, i.e., anger, disgust, fear, happiness, sadness, and surprise.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emma",
      "text": "EMMA shown in Fig.  1  is a two-branch architecture, with one (MAE pretrained ViT) for AU recognition and EXPR tasks, and the other (AffectNet pretrained DAN) for VA recognition task. Assume MAE pretrained ViT and AffectNet pretrained DAN can be denoted as f V iT and f CN N , respectively. Given an image x i , we can obtain the AU and EXP prediction score\n\nEXP (x i ) , and another EXP prediction score p 2 EXP (x i ) = f CN N (x i ). Then the VA feature can be obtained by concatenating three scores. Finally, we can obtain the VA score p V A (x i ) by passing the VA feature through a two-layer fully connected layer. Since ViT and CNN are both easy to get overfitting when finetuning the overall network for VA regression task, we stop the gradient of p V A and only finetune the linear layer. The optimization objective L can be expressed as:\n\nwhere L AU , L V A , L EXP indicate losses for AU recognition, VA regression, and EXPR recognition, respectively.\n\nFor the AU recognition task, the training loss L AU is the binary cross-entropy loss, which is given by:\n\nwhere L is the number of AUs, and σ denotes sigmoid function: σ(x) = 1 1+e -x . For EXPR task, the training loss L EXP is the cross-entropy loss:\n\nwhere ρ y i EXP is the softmax probability of the prediction score p EXP (x i ) indexed by the expression label y i EXP . For VA regression task, we regard Concordance Correlation Coefficient (CCC, see Eq. (  9 )) loss as training loss L V A :\n\nwhere CCC A and CCC V are the CCC of arousal and valence, respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Masked Cotex",
      "text": "Masked CoTEX (Fig.  2 ) is designed for EXPR, which is a co-training framework with MAE pretrained ViT in each view. In order to make two views as independent as possible, we randomly mask most patches during training process to form two approximately information-complementary views. Masking random patches in face images means we drop a few patches and put the rest patches into the ViT. Given a face image x i , we can obtain two expression scores p 1 (x i ) and p 2 (x i ) by two parallel ViTs. JS divergence L JS is performed to make the predictions of two views consistent. We use the average predictions from two views during inference phase to further improve the performance. The overall optimization objective is:\n\nλ is the hyper-parameter to balance the influence of L JS . L consists of two components, JS divergence L JS and cross-entropy loss H t , t = {1, 2}. The JS divergence L JS that constrains the predictions of two views can be expressed as:\n\nwhere ρ 1 and ρ 2 indicate the softmax probabilities of prediction scores, m is the average of ρ 1 and ρ 2 , and H is the entropy of the probability. JS divergence can also be denoted by the average KL divergence which constrains two distributions similar to the average distribution.\n\nIn each view, we use cross-entropy loss H to constrain:\n\nwhere y i is the expression label of face image x i .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Benchmarks And Evaluation Metrics",
      "text": "The benchmark we used is provided by ABAW4 challenge  [12] , which includes two sub-challenges, i.e., the multi-task learning (MTL) challenge and the learning from synthetic data (LSD) challenge. All the face images are aligned and cropped as 112×112.\n\nFor MTL challenge, static version of Aff-Wild2  [12] [13] [14] [15] [16] [17] [18] [19] [20] 35]  are utilized, which contains selected-specific frames from Aff-Wild2. There are in total of around 220K images including training set (142K), validation set (27K) and test set (51K). MTL challenge includes three sub-tasks, i.e., VA regression, EXPR (in-cluding 8 categories) and AU recognition (including 12 AUs). The MTL evaluation metrics P M T L consists of three parts:\n\nwhere CCC and F 1 are expressed as:\n\nwhere ρ is the Pearson Coefficient, σ x is the standard deviation of vector x, and µ x is the mean value of the vector x.\n\nFor the LSD challenge, the synthetic face images for the EXPR task (in a total of six basic expressions) are generated from some specific frames in the AffWild2. There are around 2770K synthetic face images for training, and 4.6K and 106K real face images in validation and testing set, respectively. The LSD evaluation metrics P LSD can be expressed as:\n\nwhich is the average of six expression F1 scores.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Training Details",
      "text": "All the data we used are the aligned and cropped version (112×112) provided by ABAW4. We resize the face images to 232×232 (228×228 for LSD task) and perform the random cropping to obtain the image size of 224×224. Furthermore, we also apply random color jitter and horizontal flipping as data augmentation during the training process. We use ViT-base as the backbone for both two methods, which is pretrained by MAE  [5]  on face image benchmark CelebA  [22] . We use AffectNet pretrained DAN  [32]  in EMMA. The overall hyper-parameter settings follow common practice of supervised ViT training, which is shown in Table  3 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Recognition Results",
      "text": "The results for MTL and LSD task on ABAW4 validation set are shown in Table  1  and Table  2 , respectively. For MTL task, we compare EMMA with baseline (VGG-face) provided by ABAW4, Clip  [26]  (RN-50), ResNet-50  [7] , EmotionNet  [3]  (SOTA method in ABAW3 MTL task), InceptionV3  [29] , EfficientNet-b2  [30]  and ViT-base  [4] . We can see from Table  1  that EMMA achieves the best or competitive performance compared with other methods especially for VA task. EMMA utilizes AffectNet pretrained CNN to extract EXP score, and combines the AU and EXP score from MAE pretrained ViT, which could provide pure and intact features for VA regression. Moreover, we only finetune the linear layer for VA regression which is easy to overfit when finetuning the entire network. Furthermore, the MAE pretrained ViT also contributes to the improvement of the final result, since it could provide more facial structure information which is a better initialization weight for optimization.\n\nFor EXPR task, we compare EMMA with baseline (ResNet-50 pretrained on ImageNet), EfficientNet-b2  [30]  (pretrained on AffectNet), Clip  [26]  (ViT-base), ViT-base (MAE pretrained on CelebA) and CoTEX. From Table  2  we can see that masked CoTEX outperforms all the other methods. It is worth noting that a large mask ratio and batch size is beneficial for improving the performance.\n\nTable  4  shows the best test results of each team in ABAW4. Our team reached 2nd place in the MTL challenge. The Situ-RUCAIM3 team  [36]  outperforms our proposed method due to the utilization of temporal information during the testing phase and more pre-trained models. Meanwhile, our method achieved fourth place out of ten teams in the LSD task. HSE-NN  [28]  uses a model pretrained with multi-task labels in an external training set, and IXLAB  [10]  applies a bagging strategy to obtain better performance.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "We also perform an ablation study on ABAW4 validation set to investigate the effectiveness of each component, which is shown in Table  5  for EMMA and Table  6  for masked CoTEX, respectively.\n\nFor EMMA, we can see that face images pretrained MAE has a better performance than ImageNet pretrained one, which indicates that face images may contain facial structure information that is beneficial for affective behaviour analysis. Furthermore, the AffectNet pretrained CNN could provide facial prior knowledge to improve VA regression performance. Moreover, we also explored the influence of face image dataset. We perform experiments on EmotioNet  [31] , which includes around 1000,000 face images. However, the performance drops when using EmotioNet pretraind model compared with the CelebA pretrained one. We think this may be caused by the quality of face images, such as the resolution, the image noise in face images, etc. We also consider adding p 2 EXP for expression task, while the performance has dropped about 3%, which is probably caused by the noise in expression logits. In the end, we also attempt the ensemble technique to further improve the final performance, and the results show that this technique is useful.\n\nFor masked CoTEX, the results show that a large mask ratio contributes to the improvement of the performance. Since a large mask ratio can reduce memory consumption, we can use a larger batch size. We notice that with the increase of the batch size, the performance is improved accordingly. However, since the training set is generated from the validation set, this improvement may not reflect the actual performance on test set. In all our submissions for ABAW4 LSD challenge, CoTEX with 75% mask ratio (λ=0, the 4th submission) achieved the highest F1 score which is 4th place in this challenge. But with 95% mask ratio (the 5th submission), the F1 score is even lower than the one without masking(the 2nd submission). We suppose an excessively large mask ratio may cause overfitting.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose two approaches using pretrained models with facial prior, namely EMMA and masked CoTEX, for the ABAW4 MTL and LSD challenges, respectively. We find that the ViT pretrained by MAE using face images performs better on emotion related tasks compared with the ImageNet pretrained one. Moreover, we notice that the expression score is a pure and intact feature for VA regression, which is prone to get overfitting when finetuning the entire network. Furthermore, we propose a co-training framework, in which two views are generated by randomly masking most patches. According to our experiment results, we find that increasing mask ratio and batch size is beneficial to improve the performance on the LSD validation set. In the future, we can also attempt pretraining MAE on different benchmarks.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) method for",
      "page": 2
    },
    {
      "caption": "Figure 2: ), in which we use MAE pretrained ViT",
      "page": 2
    },
    {
      "caption": "Figure 1: EMMA framework for multi-task affective behaviour analysis.",
      "page": 4
    },
    {
      "caption": "Figure 1: is a two-branch architecture, with one (MAE pre-",
      "page": 4
    },
    {
      "caption": "Figure 2: ) is designed for EXPR, which is a co-training framework",
      "page": 5
    },
    {
      "caption": "Figure 2: Masked CoTEX framework for EXPR.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "Abstract. Affective behavior analysis has aroused researchers’ atten-"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "tion due\nto its broad applications. However,\nit\nis\nlabor\nexhaustive\nto"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "obtain accurate annotations for massive face images. Thus, we propose"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "to utilize the prior facial\ninformation via Masked Auto-Encoder (MAE)"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "pretrained on unlabeled face\nimages. Furthermore, we\ncombine MAE"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "pretrained Vision Transformer (ViT) and AffectNet pretrained CNN to"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "perform multi-task emotion recognition. We notice that expression and"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "action unit (AU) scores are pure and intact features for valence-arousal"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "(VA)\nregression. As a result, we utilize AffectNet pretrained CNN to"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "extract expression scores concatenating with expression and AU scores"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "from ViT to obtain the final VA features. Moreover, we also propose a"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "co-training framework with two parallel MAE pretrained ViTs\nfor ex-"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "pression recognition tasks. In order to make the two views independent,"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "we randomly mask most patches during the training process. Then, JS"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "divergence\nis performed to make\nthe predictions of\nthe\ntwo views as"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "consistent as possible. The results on ABAW4 show that our methods"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "are effective, and our\nteam reached 2nd place in the multi-task learn-"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "ing (MTL) challenge and 4th place in the learning from synthetic data"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "(LSD) challenge. Code is available 3."
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "Keywords: Multi-task affective behaviour analysis, AU recognition, ex-"
        },
        {
          "{hanhu,\nsgshan}@ict.ac.cn": "pression recognition, VA regression,\nfacial prior, MAE, ABAW4."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nLi et al.": "much attention due to its wide application scenarios. With the superior perfor-"
        },
        {
          "2\nLi et al.": "mance of deep learning, the traditional artificial designed emotional representa-"
        },
        {
          "2\nLi et al.": "tions are gradually replaced by the deep neural networks (DNNs) extracted ones."
        },
        {
          "2\nLi et al.": "While DNNs-based methods perform well\nin affective behaviour analysis tasks,"
        },
        {
          "2\nLi et al.": "the limited size of the existing emotion benchmark has hindered the recognition"
        },
        {
          "2\nLi et al.": "performance and generalization ability."
        },
        {
          "2\nLi et al.": "Although it costs much to obtain data with accurate emotion annotations,"
        },
        {
          "2\nLi et al.": "we can acquire unlabeled face images more easily. To capitalize on the massive"
        },
        {
          "2\nLi et al.": "unlabeled face\nimages, we propose\nto learn facial prior knowledge using self-"
        },
        {
          "2\nLi et al.": "supervised learning methods.\nIn computer vision field, general\nself-supervised"
        },
        {
          "2\nLi et al.": "learning methods\ncan be\nsummarized into\ntwo major directions,\ncontrastive"
        },
        {
          "2\nLi et al.": "learning based methods,\ne.g., MoCo\n[6], SimCLR [2],\nand generative meth-"
        },
        {
          "2\nLi et al.": "ods, e.g., BeiT [1], Masked Auto-Encoder (MAE) [5]. SSL-based methods have"
        },
        {
          "2\nLi et al.": "reached great\nsuccess, achieving even better performance\ncompared with su-"
        },
        {
          "2\nLi et al.": "pervised pretrained ones\nin some downstream tasks.\nInspired by MAE, which"
        },
        {
          "2\nLi et al.": "randomly masks image patches and utilizes vision transformer [4] (ViT) to re-"
        },
        {
          "2\nLi et al.": "construct pixels and learn the\nintrinsic\nrelationships and discriminant\nrepre-"
        },
        {
          "2\nLi et al.": "sentations of patches, we propose to learn facial prior using MAE on massive"
        },
        {
          "2\nLi et al.": "unlabeled face images. We suppose MAE could model the relationship of differ-"
        },
        {
          "2\nLi et al.": "ent\nface components, which contains\nthe prior about\nface structure and has a"
        },
        {
          "2\nLi et al.": "better parameter initialization compared with the ImageNet pretrained model."
        },
        {
          "2\nLi et al.": "Based on MAE pretrained ViT, we propose a simple but\neffective\nframe-"
        },
        {
          "2\nLi et al.": "work called Emotion Multi-Model Aggregation (EMMA, see Fig. 1) method for"
        },
        {
          "2\nLi et al.": "multi-task affective behaviour analysis,\ni.e., EXPR, AU recognition and VA re-"
        },
        {
          "2\nLi et al.": "gression. According to experiment results, we find that it’s easier to overfit for"
        },
        {
          "2\nLi et al.": "VA regression task when finetuning MAE pretrained ViT for all\nthree\ntasks."
        },
        {
          "2\nLi et al.": "As a result, we propose to use an extra convolutional neural network (CNN) to"
        },
        {
          "2\nLi et al.": "extract features for VA regression tasks. We found that expression scores could"
        },
        {
          "2\nLi et al.": "provide pure and intact\nfeatures\nfor VA regression. Thus we\nchoose\nto use a"
        },
        {
          "2\nLi et al.": "CNN (DAN [32]) pretrained on an EXPR benchmark AffectNet\nthat contains"
        },
        {
          "2\nLi et al.": "expression prior knowledge to extract VA features. Furthermore, we also utilize"
        },
        {
          "2\nLi et al.": "the scores of both EXPR and AU recognition extracted by ViT to aid VA re-"
        },
        {
          "2\nLi et al.": "gression. Note that we only finetune the linear layer for VA regression to prevent"
        },
        {
          "2\nLi et al.": "overfitting."
        },
        {
          "2\nLi et al.": "Moreover, we also propose a masked Co-Training method for EXpression"
        },
        {
          "2\nLi et al.": "recognition (masked CoTEX, see Fig. 2),\nin which we use MAE pretrained ViT"
        },
        {
          "2\nLi et al.": "as the backbones of two views. To form two information-complementary views,"
        },
        {
          "2\nLi et al.": "we\nrandomly mask most patches of\nthe\nface\nimages.\nInspired by MLCT [33]"
        },
        {
          "2\nLi et al.": "and MLCR [24], we also use Jenson-Shannon (JS) divergence to constrain the"
        },
        {
          "2\nLi et al.": "predictions from both views to be consistent to the average distribution of two"
        },
        {
          "2\nLi et al.": "predictions. Different from MLCT or MLCR, we apply JS divergence to single-"
        },
        {
          "2\nLi et al.": "label classification (e.g., expression) by modifying the entropy, and this method"
        },
        {
          "2\nLi et al.": "is prepared for supervised learning."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n3": "– We use face images to pretrain MAE, which shows better performance than"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n3": "ImageNet pretrained MAE for affective behaviour analysis related tasks."
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n3": "– We design EMMA, which uses ViT (pretrained on unlabeled face\nimages)"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n3": "and CNN (pretrained on expression benchmark) to extract features for multi-"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n3": "task affective behaviour analysis. We also propose to use the concatenation of"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n3": "expression scores extracted by CNN and ViT, and the AU scores extracted"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n3": "by ViT to finetune the linear layer for VA regression."
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n3": "– We propose masked CoTEX for EXPR, which is a co-training framework"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n3": "utilizing JS divergence on two random masked views to make them consistent."
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n3": "We find that a large mask ratio could not only improve training speed and"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n3": "decrease memory usage, but also increase accuracy. Furthermore, we also find"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n3": "that the accuracy could be improved further by increasing the batch size."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3\nProposed Method": "In this section, we first formulate the problem, then introduce EMMA for multi-"
        },
        {
          "3\nProposed Method": "task affective behaviour analysis and masked CoTEX for EXPR."
        },
        {
          "3\nProposed Method": "3.1\nFormulation"
        },
        {
          "3\nProposed Method": "Let X = {xi ∈ RC×H×W , i = 1, 2, ..., N } and Y denotes face images and accord-"
        },
        {
          "3\nProposed Method": "ing labels,\nrespectively. For multi-task affective behaviour analysis,\nthe labels"
        },
        {
          "3\nProposed Method": "Y = {yi"
        },
        {
          "3\nProposed Method": "M T L, i = 1, 2, ..., N } consists of three sub-task labels,"
        },
        {
          "3\nProposed Method": "yi\nM T L = (cid:2)yi\nV A ∈ R2, yi\nEXP ∈ Z, yi\nAU ∈ Z12(cid:3) ,"
        },
        {
          "3\nProposed Method": "respec-\nV A, yi\nEXP , yi\nAU indicate VA labels, EXP labels and AU labels,"
        },
        {
          "3\nProposed Method": "tively. yi"
        },
        {
          "3\nProposed Method": "V A is a two dimension vector"
        },
        {
          "3\nProposed Method": "range of [−1, 1]. yi"
        },
        {
          "3\nProposed Method": "EXP is an integer ranging from 0 to 7 representing one of eight"
        },
        {
          "3\nProposed Method": "expression categories,\ni.e., neutral, anger, disgust,\nfear, happiness, sadness, sur-"
        },
        {
          "3\nProposed Method": "prise, and other. yi"
        },
        {
          "3\nProposed Method": "AU includes 12 AU labels,"
        },
        {
          "3\nProposed Method": "AU10, AU12, AU15, AU23, AU24, AU25 and AU26. If a face is invisible due to"
        },
        {
          "3\nProposed Method": "-1 and 0,\nV A, yi\nEXP\nAU can be -5,"
        },
        {
          "3\nProposed Method": "respectively. For EXPR task, Y = {yi\nEXP ∈ Z, i = 1, 2, ..., N }. There are only"
        },
        {
          "3\nProposed Method": "six expression categories in ABAW4 [12] synthetic expression dataset, i.e., anger,"
        },
        {
          "3\nProposed Method": "disgust,\nfear, happiness, sadness, and surprise."
        },
        {
          "3\nProposed Method": "3.2\nEMMA"
        },
        {
          "3\nProposed Method": "EMMA shown in Fig.\n1\nis\na\ntwo-branch architecture, with one\n(MAE pre-"
        },
        {
          "3\nProposed Method": "trained ViT)\nfor AU recognition and EXPR tasks, and the other\n(AffectNet"
        },
        {
          "3\nProposed Method": "pretrained DAN)\nfor VA recognition task. Assume MAE pretrained ViT and"
        },
        {
          "3\nProposed Method": "AffectNet pretrained DAN can be denoted as\nrespectively.\nfV iT\nand fCN N ,"
        },
        {
          "3\nProposed Method": "Given an image xi, we can obtain the AU and EXP prediction score fV iT (xi) ="
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "(cid:2)pAU (xi), p1\nEXP (xi)(cid:3), and another EXP prediction score p2",
          "5": "EXP (xi) = fCN N (xi)."
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "Then the VA feature can be obtained by concatenating three scores. Finally, we",
          "5": ""
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "can obtain the VA score pV A(xi) by passing the VA feature through a two-layer",
          "5": ""
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "fully connected layer. Since ViT and CNN are both easy to get overfitting when",
          "5": ""
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "finetuning the overall network for VA regression task, we stop the gradient of",
          "5": ""
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "the\nlinear\npV A and only finetune",
          "5": "layer. The optimization objective L can be"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "expressed as:",
          "5": ""
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "L = LAU + LV A + LEXP ,",
          "5": "(1)"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "where LAU , LV A, LEXP indicate losses for AU recognition, VA regression, and",
          "5": ""
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "EXPR recognition, respectively.",
          "5": ""
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "",
          "5": "For the AU recognition task, the training loss LAU is the binary cross-entropy"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "loss, which is given by:",
          "5": ""
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "L−1",
          "5": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2. Masked CoTEX framework for EXPR.": "The JS divergence LJS that constrains the predictions of two views can be"
        },
        {
          "Fig. 2. Masked CoTEX framework for EXPR.": ""
        },
        {
          "Fig. 2. Masked CoTEX framework for EXPR.": "H(ρ1) + H(ρ2)"
        },
        {
          "Fig. 2. Masked CoTEX framework for EXPR.": ""
        },
        {
          "Fig. 2. Masked CoTEX framework for EXPR.": "2"
        },
        {
          "Fig. 2. Masked CoTEX framework for EXPR.": ""
        },
        {
          "Fig. 2. Masked CoTEX framework for EXPR.": "average of ρ1 and ρ2, and H is the entropy of the probability. JS divergence can"
        },
        {
          "Fig. 2. Masked CoTEX framework for EXPR.": "also be denoted by the average KL divergence which constrains two distributions"
        },
        {
          "Fig. 2. Masked CoTEX framework for EXPR.": ""
        },
        {
          "Fig. 2. Masked CoTEX framework for EXPR.": "In each view, we use cross-entropy loss H to constrain:"
        },
        {
          "Fig. 2. Masked CoTEX framework for EXPR.": "Ht(ρt, yi) = −logρyi"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: 4.3 Recognition Results",
      "data": [
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "cluding 8 categories) and AU recognition (including 12 AUs). The MTL evalua-"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "tion metrics PM T L consists of three parts:"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "11"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "1"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "1 2\n1 8\n7(cid:88) i\n(cid:88) i\nF 1i\nF 1i\n(8)\nPM T L =\n(CCCV + CCCA) +\nEXP R +\nAU ,"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "12"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "=0\n=0"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "where CCC and F 1 are expressed as:"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "precision · recall\n2ρσxσy"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "CCC(x, y) =\n,\n(9)"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "precision + recall\nσ2\nx + σ2\ny + (µx − µy)2 , F 1 = 2 ·"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "where ρ is the Pearson Coefficient, σx is the standard deviation of vector x, and"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "µx is the mean value of the vector x."
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "For\nthe LSD challenge,\nthe synthetic face images\nfor\nthe EXPR task (in a"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "total of\nsix basic expressions) are generated from some specific frames\nin the"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "AffWild2. There are around 2770K synthetic face images for training, and 4.6K"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "and 106K real\nface images in validation and testing set, respectively. The LSD"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "evaluation metrics PLSD can be expressed as:"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "1 6\n5(cid:88) i\nF 1i\n(10)\nPLSD =\nLSD,"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "=0"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "which is the average of six expression F1 scores."
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "4.2\nTraining Details"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "All the data we used are the aligned and cropped version (112×112) provided"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "by ABAW4. We resize the face images to 232×232 (228×228 for LSD task) and"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "perform the random cropping to obtain the image size of 224×224. Furthermore,"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "we also apply random color jitter and horizontal flipping as data augmentation"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "during the\ntraining process. We use ViT-base as\nthe backbone\nfor both two"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "methods, which is pretrained by MAE [5] on face image benchmark CelebA [22]."
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "We use AffectNet pretrained DAN [32]\nin EMMA. The overall hyper-parameter"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "settings follow common practice of supervised ViT training, which is shown in"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "Table 3."
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "4.3\nRecognition Results"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "The results for MTL and LSD task on ABAW4 validation set are shown in Table"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "1 and Table 2, respectively."
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "For MTL task, we compare EMMA with baseline (VGG-face) provided by"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "ABAW4, Clip [26]\n(RN-50), ResNet-50 [7], EmotionNet\n[3]\n(SOTA method in"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "ABAW3 MTL task), InceptionV3 [29], EfficientNet-b2 [30] and ViT-base [4]. We"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "can see from Table 1 that EMMA achieves the best or competitive performance"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "compared with other methods especially for VA task. EMMA utilizes AffectNet"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n7": "pretrained CNN to extract EXP score, and combines\nthe AU and EXP score"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: The F1 scores (in %) comparison for multi-task affective behaviour analysis",
      "data": [
        {
          "Table 1. The F1 scores (in %) comparison for multi-task affective behaviour analysis": "on ABAW4 validation set. All the results are recorded for the best final score."
        },
        {
          "Table 1. The F1 scores (in %) comparison for multi-task affective behaviour analysis": "Method"
        },
        {
          "Table 1. The F1 scores (in %) comparison for multi-task affective behaviour analysis": "Baseline (VGGface,\nlinear evaluation)"
        },
        {
          "Table 1. The F1 scores (in %) comparison for multi-task affective behaviour analysis": "Clip (RN-50, finetune)"
        },
        {
          "Table 1. The F1 scores (in %) comparison for multi-task affective behaviour analysis": "ResNet50 (ImageNet pretrained, finetune)"
        },
        {
          "Table 1. The F1 scores (in %) comparison for multi-task affective behaviour analysis": "EmotionNet (ABAW3 SOTA, finetune)"
        },
        {
          "Table 1. The F1 scores (in %) comparison for multi-task affective behaviour analysis": "InceptionV3 (ImageNet pretrained, finetune)"
        },
        {
          "Table 1. The F1 scores (in %) comparison for multi-task affective behaviour analysis": "EfficientNet-b2 (AffectNet pretrained, finetune)"
        },
        {
          "Table 1. The F1 scores (in %) comparison for multi-task affective behaviour analysis": "ViT-base (MAE pretrained on CelebA, finetune)"
        },
        {
          "Table 1. The F1 scores (in %) comparison for multi-task affective behaviour analysis": "EMMA (MAE pretrained on CelebA, finetune)"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: The F1 scores (in %) comparison for multi-task affective behaviour analysis",
      "data": [
        {
          "Table 2. The F1 score (in %) and macro accuracy (acc, in %) for LSD task on ABAW4": "Method"
        },
        {
          "Table 2. The F1 score (in %) and macro accuracy (acc, in %) for LSD task on ABAW4": "Baseline (ResNet-50, ImageNet pretrained)"
        },
        {
          "Table 2. The F1 score (in %) and macro accuracy (acc, in %) for LSD task on ABAW4": "EfficientNet-b2 (ImageNet pretrained)"
        },
        {
          "Table 2. The F1 score (in %) and macro accuracy (acc, in %) for LSD task on ABAW4": "EfficientNet-b2 (AffectNet pretrained)"
        },
        {
          "Table 2. The F1 score (in %) and macro accuracy (acc, in %) for LSD task on ABAW4": "Clip (ViT-base)"
        },
        {
          "Table 2. The F1 score (in %) and macro accuracy (acc, in %) for LSD task on ABAW4": "ViT-base (MAE pretrained on CelebA)"
        },
        {
          "Table 2. The F1 score (in %) and macro accuracy (acc, in %) for LSD task on ABAW4": "CoTEX (mask ratio 0%, batch size 64)"
        },
        {
          "Table 2. The F1 score (in %) and macro accuracy (acc, in %) for LSD task on ABAW4": "Masked CoTEX (mask ratio 75%, batch size 1024)"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: The training settings for EMMA and Masked CoTEX.",
      "data": [
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": ""
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "Config"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "optimizer"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "base learning rate"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "weight decay"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "batch size"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "clip grad"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "layer decay"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "warm up epochs"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "total epochs"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "accumulated iterations"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior": "drop path rate"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: The training settings for EMMA and Masked CoTEX.",
      "data": [
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "4.4\nAblation Study"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "We also perform an ablation study on ABAW4 validation set to investigate the"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "effectiveness of each component, which is shown in Table 5 for EMMA and Table"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "6 for masked CoTEX, respectively."
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "For EMMA, we can see that face images pretrained MAE has a better per-"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "formance than ImageNet pretrained one, which indicates that face images may"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "contain facial\nstructure\ninformation that\nis beneficial\nfor affective behaviour"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "analysis. Furthermore, the AffectNet pretrained CNN could provide facial prior"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "knowledge to improve VA regression performance. Moreover, we also explored"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "the influence of face image dataset. We perform experiments on EmotioNet [31],"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "which includes around 1000,000 face images. However,\nthe performance drops"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "when using EmotioNet pretraind model compared with the CelebA pretrained"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "one. We think this may be caused by the quality of\nface images,\nsuch as\nthe"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "resolution, the image noise in face images, etc. We also consider adding p2"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "EXP"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "for expression task, while the performance has dropped about 3%, which is prob-"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "ably caused by the noise in expression logits.\nIn the end, we also attempt\nthe"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "ensemble technique to further\nimprove the final performance, and the results"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "show that this technique is useful."
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "For masked CoTEX,\nthe results\nshow that a large mask ratio contributes"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "to the\nimprovement of\nthe performance. Since a large mask ratio can reduce"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "memory consumption, we can use a larger batch size. We notice that with the"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "increase of\nthe batch size,\nthe performance is\nimproved accordingly. However,"
        },
        {
          "STAR-2022\n108.55\nHUST-ANT\n34.83": "since\nthe\ntraining set\nis generated from the validation set,\nthis\nimprovement"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 5: The ablation study for EMMA. All the models are pretrained with MAE.",
      "data": [
        {
          "Table 5. The ablation study for EMMA. All the models are pretrained with MAE.": "Method"
        },
        {
          "Table 5. The ablation study for EMMA. All the models are pretrained with MAE.": "ViT (ImageNet)"
        },
        {
          "Table 5. The ablation study for EMMA. All the models are pretrained with MAE.": "ViT (CelebA)"
        },
        {
          "Table 5. The ablation study for EMMA. All the models are pretrained with MAE.": "EMMA (ImageNet)"
        },
        {
          "Table 5. The ablation study for EMMA. All the models are pretrained with MAE.": "EMMA (EmotioNet)"
        },
        {
          "Table 5. The ablation study for EMMA. All the models are pretrained with MAE.": "EMMA (CelebA)"
        },
        {
          "Table 5. The ablation study for EMMA. All the models are pretrained with MAE.": "EMMA (CelebA, LEXP (p1\nEXP + p2\nEXP ))"
        },
        {
          "Table 5. The ablation study for EMMA. All the models are pretrained with MAE.": "EMMA (different epochs ensemble)"
        },
        {
          "Table 5. The ablation study for EMMA. All the models are pretrained with MAE.": "EMMA (different parameters ensemble)"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 5: The ablation study for EMMA. All the models are pretrained with MAE.",
      "data": [
        {
          "Table 6. The ablation study for Masked CoTEX.": "Acc"
        },
        {
          "Table 6. The ablation study for Masked CoTEX.": "71.95"
        },
        {
          "Table 6. The ablation study for Masked CoTEX.": "72.81"
        },
        {
          "Table 6. The ablation study for Masked CoTEX.": "72.53"
        },
        {
          "Table 6. The ablation study for Masked CoTEX.": "74.17"
        },
        {
          "Table 6. The ablation study for Masked CoTEX.": "74.82"
        },
        {
          "Table 6. The ablation study for Masked CoTEX.": "75.77"
        },
        {
          "Table 6. The ablation study for Masked CoTEX.": "78.54"
        },
        {
          "Table 6. The ablation study for Masked CoTEX.": "81.18"
        },
        {
          "Table 6. The ablation study for Masked CoTEX.": "83.85"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "References"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "1. Bao, H., Dong, L., Wei, F.: Beit: Bert pre-training of\nimage transformers. arXiv"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "preprint arXiv:2106.08254 (2021) 2"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "2. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "trastive learning of visual representations.\nIn: Proc.\nICML. pp. 1597–1607 (2020)"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "2"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "3. Deng, D.: Multiple emotion descriptors estimation at the abaw3 challenge (2022)"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "3, 7"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "4. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "T., Dehghani, M., Minderer, M., Heigold, G., Gelly,\nS.,\net\nal.: An\nimage\nis"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "worth 16x16 words: Transformers\nfor\nimage recognition at\nscale. arXiv preprint"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "arXiv:2010.11929 (2020) 2, 7"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "5. He, K., Chen, X., Xie, S., Li, Y., Doll´ar, P., Girshick, R.: Masked autoencoders are"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "scalable vision learners. In: Proc. CVPR. pp. 16000–16009 (2022) 2, 7"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "6. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "visual representation learning. In: Proc. CVPR. pp. 9729–9738 (2020) 2"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual\nlearning for image recognition."
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "In: Proc. CVPR. pp. 770–778 (2016) 7"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "8.\nJeong, E., Oh, G., Lim, S.: Multitask emotion recognition model with knowledge"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "distillation and task discriminator. arXiv preprint arXiv:2203.13072 (2022) 3"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "9.\nJeong, J.Y., Hong, Y.G., Kim, D., Jung, Y., Jeong, J.W.: Facial expression recogni-"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "tion based on multi-head cross attention network. arXiv preprint arXiv:2203.13235"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "(2022) 3"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "10.\nJeong, J.Y., Hong, Y.G., Oh, J., Hong, S., Jeong, J.W., Jung, Y.: Learning from"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "synthetic data: Facial\nexpression classification based on ensemble of multi-task"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "networks. arXiv preprint arXiv:2207.10025 (2022) 8"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "11. Kim, J.H., Kim, N., Won, C.S.: Facial\nexpression recognition with swin trans-"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "former. arXiv preprint arXiv:2203.13472 (2022) 3"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "12. Kollias, D.: Abaw: Valence-arousal estimation, expression recognition, action unit"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "detection & multi-task learning challenges. In: Proc. CVPR. pp. 2328–2336 (2022)"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "4, 6"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "13. Kollias, D., Cheng, S., Pantic, M., Zafeiriou, S.: Photorealistic facial synthesis in"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "the dimensional affect space. In: Proc. ECCVW. pp. 0–0 (2018) 6"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "14. Kollias, D., Cheng, S., Ververas, E., Kotsia, I., Zafeiriou, S.: Deep neural network"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "augmentation: Generating faces for affect analysis. IJCV 128(5), 1455–1484 (2020)"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "6"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "15. Kollias, D., Nicolaou, M.A., Kotsia, I., Zhao, G., Zafeiriou, S.: Recognition of affect"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "in the wild using deep neural networks.\nIn: Proc. CVPRW. pp. 1972–1979.\nIEEE"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "(2017) 6"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "16. Kollias, D., Sharmanska, V., Zafeiriou, S.: Distribution matching for heteroge-"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "neous multi-task learning: a large-scale face study. arXiv preprint arXiv:2105.03790"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "(2021) 6"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "17. Kollias, D., Tzirakis, P., Nicolaou, M.A., Papaioannou, A., Zhao, G., Schuller, B.,"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "Kotsia, I., Zafeiriou, S.: Deep affect prediction in-the-wild: Aff-wild database and"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "challenge, deep architectures, and beyond. IJCV pp. 1–23 (2019) 6"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "18. Kollias, D., Zafeiriou, S.: Expression, affect, action unit\nrecognition: Aff-wild2,"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "multi-task learning and arcface. arXiv preprint arXiv:1910.04855 (2019) 6"
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "19. Kollias, D., Zafeiriou, S.: Va-stargan: Continuous affect generation. In: Int. Conf."
        },
        {
          "Affective Behaviour Analysis Using Pretrained Model with Facial Prior\n11": "Adv. Conc. Intel. Vis. Sys. pp. 227–238. Springer (2020) 6"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12\nLi et al.": "20. Kollias, D., Zafeiriou, S.: Affect analysis in-the-wild: Valence-arousal, expressions,"
        },
        {
          "12\nLi et al.": "action units and a unified framework. arXiv preprint arXiv:2103.15792 (2021) 6"
        },
        {
          "12\nLi et al.": "21. Li, S., Deng, W.: Deep facial expression recognition: A survey. IEEE TAC (2020)"
        },
        {
          "12\nLi et al.": "1"
        },
        {
          "12\nLi et al.": "22. Liu, Z., Luo, P., Wang, X., Tang, X.: Large-scale\ncelebfaces attributes\n(celeba)"
        },
        {
          "12\nLi et al.": "dataset. Retrieved August 15(2018),\n11 (2018) 7"
        },
        {
          "12\nLi et al.": "23. Nicolaou, M.A., Gunes, H., Pantic, M.: Continuous prediction of spontaneous affect"
        },
        {
          "12\nLi et al.": "from multiple cues and modalities in valence-arousal space.\nIEEE TAC 2(2), 92–"
        },
        {
          "12\nLi et al.": "105 (2011) 1"
        },
        {
          "12\nLi et al.": "24. Niu, X., Han, H.,\nShan,\nS., Chen, X.: Multi-label\nco-regularization\nfor\nsemi-"
        },
        {
          "12\nLi et al.": "supervised facial action unit recognition. In: Proc. NeurIPS. pp. 909–919 (2019) 1,"
        },
        {
          "12\nLi et al.": "2"
        },
        {
          "12\nLi et al.": "25. Phan, K.N., Nguyen, H.H., Huynh, V.T., Kim, S.H.: Expression classification us-"
        },
        {
          "12\nLi et al.": "ing concatenation of deep neural network for\nthe 3rd abaw3 competition. arXiv"
        },
        {
          "12\nLi et al.": "preprint arXiv:2203.12899 (2022) 3"
        },
        {
          "12\nLi et al.": "26. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,"
        },
        {
          "12\nLi et al.": "G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models"
        },
        {
          "12\nLi et al.": "from natural\nlanguage supervision. In: Proc. ICML. pp. 8748–8763. PMLR (2021)"
        },
        {
          "12\nLi et al.": "7, 8"
        },
        {
          "12\nLi et al.": "27. Radosavovic, I., Kosaraju, R.P., Girshick, R., He, K., Doll´ar, P.: Designing network"
        },
        {
          "12\nLi et al.": "design spaces. In: Proc. CVPR. pp. 10428–10436 (2020) 3"
        },
        {
          "12\nLi et al.": "28. Savchenko, A.V.: Hse-nn team at\nthe 4th abaw competition: Multi-task emotion"
        },
        {
          "12\nLi et al.": "recognition and learning from synthetic images. arXiv preprint arXiv:2207.09508"
        },
        {
          "12\nLi et al.": "(2022) 8"
        },
        {
          "12\nLi et al.": "29. Szegedy, C., Vanhoucke, V.,\nIoffe, S., Shlens, J., Wojna, Z.: Rethinking the\nin-"
        },
        {
          "12\nLi et al.": "ception architecture for computer vision.\nIn: Proc. CVPR. pp. 2818–2826 (2016)"
        },
        {
          "12\nLi et al.": "7"
        },
        {
          "12\nLi et al.": "30. Tan, M., Le, Q.: Efficientnet: Rethinking model\nscaling for convolutional neural"
        },
        {
          "12\nLi et al.": "networks. In: Proc. ICML. pp. 6105–6114. PMLR (2019) 7, 8"
        },
        {
          "12\nLi et al.": "31. Wang, P., Wang, Z., Ji, Z., Liu, X., Yang, S., Wu, Z.: Tal emotionet challenge 2020"
        },
        {
          "12\nLi et al.": "rethinking the model chosen problem in multi-task learning.\nIn: Proc. CVPRW."
        },
        {
          "12\nLi et al.": "pp. 412–413 (2020) 9"
        },
        {
          "12\nLi et al.": "32. Wen, Z., Lin, W., Wang, T., Xu, G.: Distract your attention: multi-head cross at-"
        },
        {
          "12\nLi et al.": "tention network for facial expression recognition. arXiv preprint arXiv:2109.07270"
        },
        {
          "12\nLi et al.": "(2021) 2, 7"
        },
        {
          "12\nLi et al.": "33. Xing, Y., Yu, G., Domeniconi, C., Wang, J., Zhang, Z.: Multi-label co-training. In:"
        },
        {
          "12\nLi et al.": "Proc. IJCAI. pp. 2882–2888 (2018) 2"
        },
        {
          "12\nLi et al.": "34. Xue, F., Tan, Z., Zhu, Y., Ma, Z., Guo, G.: Coarse-to-fine cascaded networks with"
        },
        {
          "12\nLi et al.": "smooth predicting for video facial\nexpression recognition.\nIn: Proc. CVPR. pp."
        },
        {
          "12\nLi et al.": "2412–2418 (2022) 3"
        },
        {
          "12\nLi et al.": "35. Zafeiriou, S., Kollias, D., Nicolaou, M.A., Papaioannou, A., Zhao, G., Kotsia,\nI.:"
        },
        {
          "12\nLi et al.": "Aff-wild: Valence and arousal\n‘in-the-wild’challenge. In: Proc. CVPRW. pp. 1980–"
        },
        {
          "12\nLi et al.": "1987. IEEE (2017) 6"
        },
        {
          "12\nLi et al.": "36. Zhang, T., Liu, C., Liu, X., Liu, Y., Meng, L., Sun, L., Jiang, W., Zhang, F.: Emo-"
        },
        {
          "12\nLi et al.": "tion recognition based on multi-task learning framework in the abaw4 challenge."
        },
        {
          "12\nLi et al.": "arXiv preprint arXiv:2207.09373 (2022) 8"
        },
        {
          "12\nLi et al.": "37. Zhang, W., Qiu, F., Wang, S., Zeng, H., Zhang, Z., An, R., Ma, B., Ding, Y.:"
        },
        {
          "12\nLi et al.": "Transformer-based multimodal\ninformation fusion for\nfacial\nexpression analysis."
        },
        {
          "12\nLi et al.": "In: Proc. CVPRW. pp. 2428–2437 (2022) 3"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Beit: Bert pre-training of image transformers",
      "authors": [
        "H Bao",
        "L Dong",
        "F Wei"
      ],
      "year": "2021",
      "venue": "Beit: Bert pre-training of image transformers",
      "arxiv": "arXiv:2106.08254"
    },
    {
      "citation_id": "2",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "3",
      "title": "Multiple emotion descriptors estimation at the abaw3 challenge",
      "authors": [
        "D Deng"
      ],
      "year": "2022",
      "venue": "Multiple emotion descriptors estimation at the abaw3 challenge"
    },
    {
      "citation_id": "4",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "5",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2022",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "6",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "K He",
        "H Fan",
        "Y Wu",
        "S Xie",
        "R Girshick"
      ],
      "year": "2020",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "7",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "8",
      "title": "Multitask emotion recognition model with knowledge distillation and task discriminator",
      "authors": [
        "E Jeong",
        "G Oh",
        "S Lim"
      ],
      "year": "2022",
      "venue": "Multitask emotion recognition model with knowledge distillation and task discriminator",
      "arxiv": "arXiv:2203.13072"
    },
    {
      "citation_id": "9",
      "title": "Facial expression recognition based on multi-head cross attention network",
      "authors": [
        "J Jeong",
        "Y Hong",
        "D Kim",
        "Y Jung",
        "J Jeong"
      ],
      "year": "2022",
      "venue": "Facial expression recognition based on multi-head cross attention network",
      "arxiv": "arXiv:2203.13235"
    },
    {
      "citation_id": "10",
      "title": "Learning from synthetic data: Facial expression classification based on ensemble of multi-task networks",
      "authors": [
        "J Jeong",
        "Y Hong",
        "J Oh",
        "S Hong",
        "J Jeong",
        "Y Jung"
      ],
      "year": "2022",
      "venue": "Learning from synthetic data: Facial expression classification based on ensemble of multi-task networks",
      "arxiv": "arXiv:2207.10025"
    },
    {
      "citation_id": "11",
      "title": "Facial expression recognition with swin transformer",
      "authors": [
        "J Kim",
        "N Kim",
        "C Won"
      ],
      "year": "2022",
      "venue": "Facial expression recognition with swin transformer",
      "arxiv": "arXiv:2203.13472"
    },
    {
      "citation_id": "12",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "13",
      "title": "Photorealistic facial synthesis in the dimensional affect space",
      "authors": [
        "D Kollias",
        "S Cheng",
        "M Pantic",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Proc. ECCVW"
    },
    {
      "citation_id": "14",
      "title": "Deep neural network augmentation: Generating faces for affect analysis",
      "authors": [
        "D Kollias",
        "S Cheng",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "IJCV"
    },
    {
      "citation_id": "15",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "Proc. CVPRW"
    },
    {
      "citation_id": "16",
      "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "17",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Deep affect prediction in-the-wild: Aff-wild database and challenge"
    },
    {
      "citation_id": "18",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "19",
      "title": "Va-stargan: Continuous affect generation",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Int. Conf. Adv. Conc. Intel. Vis. Sys"
    },
    {
      "citation_id": "20",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "21",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE TAC"
    },
    {
      "citation_id": "22",
      "title": "Large-scale celebfaces attributes (celeba) dataset",
      "authors": [
        "Z Liu",
        "P Luo",
        "X Wang",
        "X Tang"
      ],
      "year": "2018",
      "venue": "Large-scale celebfaces attributes (celeba) dataset"
    },
    {
      "citation_id": "23",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space",
      "authors": [
        "M Nicolaou",
        "H Gunes",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE TAC"
    },
    {
      "citation_id": "24",
      "title": "Multi-label co-regularization for semisupervised facial action unit recognition",
      "authors": [
        "X Niu",
        "H Han",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "25",
      "title": "Expression classification using concatenation of deep neural network for the 3rd abaw3 competition",
      "authors": [
        "K Phan",
        "H Nguyen",
        "V Huynh",
        "S Kim"
      ],
      "year": "2022",
      "venue": "Expression classification using concatenation of deep neural network for the 3rd abaw3 competition",
      "arxiv": "arXiv:2203.12899"
    },
    {
      "citation_id": "26",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "27",
      "title": "Designing network design spaces",
      "authors": [
        "I Radosavovic",
        "R Kosaraju",
        "R Girshick",
        "K He",
        "P Dollár"
      ],
      "year": "2020",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "28",
      "title": "Hse-nn team at the 4th abaw competition: Multi-task emotion recognition and learning from synthetic images",
      "authors": [
        "A Savchenko"
      ],
      "year": "2022",
      "venue": "Hse-nn team at the 4th abaw competition: Multi-task emotion recognition and learning from synthetic images",
      "arxiv": "arXiv:2207.09508"
    },
    {
      "citation_id": "29",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "C Szegedy",
        "V Vanhoucke",
        "S Ioffe",
        "J Shlens",
        "Z Wojna"
      ],
      "year": "2016",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "30",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "31",
      "title": "Tal emotionet challenge 2020 rethinking the model chosen problem in multi-task learning",
      "authors": [
        "P Wang",
        "Z Wang",
        "Z Ji",
        "X Liu",
        "S Yang",
        "Z Wu"
      ],
      "year": "2020",
      "venue": "Proc. CVPRW"
    },
    {
      "citation_id": "32",
      "title": "Distract your attention: multi-head cross attention network for facial expression recognition",
      "authors": [
        "Z Wen",
        "W Lin",
        "T Wang",
        "G Xu"
      ],
      "year": "2021",
      "venue": "Distract your attention: multi-head cross attention network for facial expression recognition",
      "arxiv": "arXiv:2109.07270"
    },
    {
      "citation_id": "33",
      "title": "Multi-label co-training",
      "authors": [
        "Y Xing",
        "G Yu",
        "C Domeniconi",
        "J Wang",
        "Z Zhang"
      ],
      "year": "2018",
      "venue": "Proc. IJCAI"
    },
    {
      "citation_id": "34",
      "title": "Coarse-to-fine cascaded networks with smooth predicting for video facial expression recognition",
      "authors": [
        "F Xue",
        "Z Tan",
        "Y Zhu",
        "Z Ma",
        "G Guo"
      ],
      "year": "2022",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "35",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Proc. CVPRW"
    },
    {
      "citation_id": "36",
      "title": "Emotion recognition based on multi-task learning framework in the abaw4 challenge",
      "authors": [
        "T Zhang",
        "C Liu",
        "X Liu",
        "Y Liu",
        "L Meng",
        "L Sun",
        "W Jiang",
        "F Zhang"
      ],
      "year": "2022",
      "venue": "Emotion recognition based on multi-task learning framework in the abaw4 challenge",
      "arxiv": "arXiv:2207.09373"
    },
    {
      "citation_id": "37",
      "title": "Transformer-based multimodal information fusion for facial expression analysis",
      "authors": [
        "W Zhang",
        "F Qiu",
        "S Wang",
        "H Zeng",
        "Z Zhang",
        "R An",
        "B Ma",
        "Y Ding"
      ],
      "year": "2022",
      "venue": "Proc. CVPRW"
    }
  ]
}