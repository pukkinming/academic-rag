{
  "paper_id": "2403.11440v1",
  "title": "Boosting Continuous Emotion Recognition With Self-Pretraining Using Masked Autoencoders, Temporal Convolutional Networks, And Transformers",
  "published": "2024-03-18T03:28:01Z",
  "authors": [
    "Weiwei Zhou",
    "Jiada Lu",
    "Chenkun Ling",
    "Weifeng Wang",
    "Shaowei Liu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human emotion recognition holds a pivotal role in facilitating seamless human-computer interaction. This paper delineates our methodology in tackling the Valence-Arousal (VA) Estimation Challenge, Expression (Expr) Classification Challenge, and Action Unit (AU) Detection Challenge within the ambit of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW). Our study advocates a novel approach aimed at refining continuous emotion recognition. We achieve this by initially harnessing pre-training with Masked Autoencoders (MAE) on facial datasets, followed by fine-tuning on the aff-wild2 dataset annotated with expression (Expr) labels. The pre-trained model serves as an adept visual feature extractor, thereby enhancing the model's robustness. Furthermore, we bolster the performance of continuous emotion recognition by integrating Temporal Convolutional Network (TCN) modules and Transformer Encoder modules into our framework.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial Expression Recognition (FER) holds immense potential across a spectrum of applications, ranging from discerning emotions in videos to bolstering security through facial recognition systems, and even enriching virtual reality experiences. While significant strides have been made in various facial-related tasks, such as face and attribute recognition, the nuanced realm of emotional comprehension remains a challenge.\n\nThe intricacies of emotional expressions often present subtle differentiations that can introduce ambiguity or uncertainty in accurately perceiving emotions. Consequently, this complexity poses hurdles in effectively assessing an individual's emotional state. One of the primary obstacles lies in the inadequacy of existing FER datasets to encapsulate the breadth and depth of human emotional expressions, hindering the development of robust models. Efforts to expand and diversify these datasets are imperative to enhance the efficacy and reliability of FER systems.\n\nThe appearance of AffWild and AffWild2 dataset and the corresponding challenges  [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] 20]  boost the development of affective recognition study. The Aff-Wild2 dataset contains about 600 videos with around 3M frames. The dataset is annotated with three different affect attributes: a) dimensional affect with valence and arousal; b) six basic categorical affect; c) action units of facial muscles. To facilitate the utilization of the Aff-Wild2 dataset, the 6th ABAW  [14]  competition was organized for affective behavior analysis in the wild.\n\nDue to the significant success achieved by pre-training models like MAE. In the past, we attempt to utilize the MAE pre-training method as a visual feature extractor on a facial expression dataset. Subsequently, we employ TCN and Transformer for continuous emotion recognition. Our approach results in a significant improvement in the evaluation accuracy of Valence-Arousal Estimation, Action Unit Detection, and Expression Classification.\n\nThe remaining parts of the paper are presented as follows: Sec 2 describe the study of facial emotion recognition. Sec 3 describes our methodology; Sec 4 describes the experiment details and the result; Sec 5 is the conclusion of the paper.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Previous studies have proposed some useful networks on the Aff-wild2 dataset. Kuhnke et al.  [15]  combined vision and audio information in the video and constructed a two-stream network for emotion recognition, achieving high performance. Yue Jin et al.  [2]  proposed a transformerbased model to merge audio and visual features.\n\nNetEase  [21]  utilized the visual information from a Masked Autoencoder (MAE) model that had been pretrained on a large-scale face image dataset in a selfsupervised manner. Next, the MAE encoder was fine-tuned on the image frames from the Aff-wild2 for AU, EXPR, and VA tasks, which could be regarded as static and uni-modal training. Additionally, multi-modal and temporal informa-tion from the videos were leveraged, and a transformerbased framework was implemented to fuse the multi-modal features.\n\nSituTech  [17]  utilized multi-modal feature combinations extracted by several different pre-trained models, which were applied to capture more effective emotional information.\n\nTemporal Convolutional Network (TCN) was proposed by Colin Lea et al.  [16] , which hierarchically captured relationships at low-, intermediate-, and high-level time scales. Jin Fan et al.  [1]  proposed a model with a spatial-temporal attention mechanism to catch dynamic internal correlations with stacked TCN backbones to extract features from different window sizes.\n\nThe Transformer mechanism proposed by Vaswani et al.  [19]  has achieved high performance in many tasks, so many researchers exploited the Transformer for affective behavior studies. Zhao et al.  [22]  proposed a model with spatial and temporal Transformers for facial expression analysis. Jacob et al.  [18]  proposed a network to learn the relationship between action units with a transformer correlation module.\n\nInspired by the previous work, in this paper, we propose to use MAE as a feature extractor and design a model consisting of TCN and Transformer to enhance the performance of emotion recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we describe in detail our proposed method for tackling the three challenging tasks of affective behavior analysis in the wild that are addressed by the 6th ABAW Competition: Valence-Arousal Estimation, Expr Classification, and AU Detection. We explain how we design our model architecture, data processing, and training strategy for each task.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mae Pre-Training",
      "text": "Inspired by Netease, we conduct pre-training of our MAE on a facial image dataset. To this end, we also curate a largescale dataset of facial expressions to learn facial features, consisting of AffectNet, RAF-DB, FER2013, and FER+. Subsequently, the MAE model is pre-trained on this dataset in a self-supervised manner. Specifically, our MAE consists of a ViT-Base encoder and a ViT decoder. The pre-training process of MAE follows a masked-reconstruction method, where images are first divided into a series of patches (16x16), with 75% of these patches randomly masked. These masked images are then fed into the MAE encoder, while the MAE decoder is tasked with reconstructing the complete image. The loss function for MAE pre-training is pixel-level L2 loss, aiming to minimize the difference between the reconstructed image and the target image. Once self-supervised learning is completed, the MAE decoder is removed and replaced with fully connected layers connected to the MAE encoder. Subsequently, Expr labels are fine-tuned to obtain a feature extractor more aligned with the distribution of aff-wild2 data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Temporal Convolutional Network",
      "text": "Videos are first split into segments with a window size w and stride s. Given the segment window w and stride s, a video with n frames would be split into [n/s] + 1 segments, where the i-th segment contains frames F (i-1) * s+1 , . . . , F (i-1) * s+w .\n\nIn other words, videos are cut into some overlapping chunks, each with a fixed number of frames. The purpose of doing this is to break down the video into smaller parts that are easier to process and analyze. Each chunk has some degree of overlap with the previous and next ones so that no information in the video is missed.\n\nWe denote visual features as f i corresponding to the i-th segment extracted by pre-trained and fine-tuned ViT-Base encoder.\n\nVisual feature is fed into a dedicated Temporal Convolutional Network (TCN) for temporal encoding, which can be formulated as follows:\n\nThis means that we use a special type of neural network that can capture the temporal patterns and dependencies of the features over time. The TCN takes the input feature vector and applies a series of convolutional layers with different kernel sizes and dilation rates to produce an output feature vector. The output feature vector has the same length as the input feature vector but contains more information about the temporal context. For example, the TCN can learn how the image changes over time in each segment of the video.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Temporal Encoder",
      "text": "We utilize a transformer encoder to model the temporal information in the video segment as well, which can be formulated as follows:\n\nThe Transformer encoder only models the context within a single segment, thereby ignoring the dependencies between frames across segments. To account for the context of different frames, overlapping between consecutive segments can be employed, thus enabling the capture of the dependencies between frames across segments, which means s ≤ w.\n\nWe use another type of neural network that can learn the relationships and interactions among the features within each segment. The transformer encoder takes the input feature vector and applies a series of self-attention layers and feed-forward layers to produce an output feature vector. The output feature vector has more semantic meaning and representation power than the input feature vector. For example, the transformer encoder can learn how different parts of the image relate to each other in each segment of the video. However, the transformer encoder does not consider how different segments of the video are connected or influenced by each other. To solve this problem, we can make some segments overlap with each other so that some frames are shared by two or more segments. This way, we can capture some information about how different segments affect each other. The degree of overlap is controlled by two parameters: s is the length of a segment and w is the sliding window size. If s is smaller than or equal to w, then there will be some overlap between consecutive segments.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Prediction",
      "text": "After the temporal encoder, the features h i are finally fed into MLP for regression, which can be formulated as follows:\n\nwhere y i are the predictions of i-th segment. For VA challenge, y i ∈ R l×2 . For Expr challenge, y i ∈ R l×8 . For AU challenge, y i ∈ R l×12 . The prediction vector contains the values we want to estimate for each segment. The MLP consists of several layers of neurons that can learn non-linear transformations of the input. The MLP can be trained to minimize the error between the prediction vector and the ground truth vector. The ground truth vector is the values we want to predict for each segment. Depending on what kind of challenge we are solving, we have different types of ground truth vectors and prediction vectors. For the VA challenge, we want to predict two values: valence and arousal. Valence measures how positive or negative an emotion is. Arousal measures how active or passive an emotion is. For the Expr challenge, we want to predict eight values: one for each basic expression (anger, disgust, fear, happiness, sadness, and surprise) plus neutral and other expressions. For the AU challenge, we want to predict twelve values: one for each action unit (AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23, AU24, AU25, AU26).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Loss Functions",
      "text": "VA challenge: We use the Concordance Correlation Coefficient (CCC) between the predictions and the ground truth labels as the measure, which is defined as in Eq 1. It measures the correlation between two sequences x and y and ranges between -1 and 1, where -1 means perfect anticorrelation, 0 means no correlation, and 1 means perfect correlation. The loss is calculated as Eq 2.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ccc(X, Y)",
      "text": "Expr challenge: We use the cross-entropy loss as the loss function, which is defined as in Eq 3.\n\nwhere y ic is a binary indicator (0 or 1) if class c is the correct classification for observation i. p ic is the predicted probability of observation i being in class c, M is the number of classes. The multiclass cross entropy loss function measures how well a model predicts the true probabilities of each class for a given observation. It penalizes wrong predictions by taking the logarithm of the predicted probabilities. The lower the loss, the better the model.\n\nAU challenge: We employ BCEWithLogitsLoss as the loss function, which integrates a sigmoid layer and binary cross-entropy, which is defined as in Eq 4.\n\n) where N is the number of samples, y i is the target label for sample i, x i is the input logits for sample i, σ is the sigmoid function The advantage of using BCEWithLogitsLoss over BCELoss with sigmoid is that it can avoid numerical instability and improve performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments Settings",
      "text": "All models were trained on two Nvidia GeForce GTX 3090 GPUs with each having 24GB of memory.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mae Pre-Training",
      "text": "We conducted an extensive pre-training of the MAE model on large-scale facial image datasets over 500 epochs, employing the AdamW optimizer. During this phase, we maintained a batch size of 1024 and set the learning rate to 0.0005. Subsequently, in the fine-tuning stage of MAE, we adjusted the batch size to 256 and lowered the learning rate to 0.0001, still leveraging the AdamW optimizer.   1 . Results for the five folds of three tasks",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task Trainging",
      "text": "We used the AdamW optimizer and cosine learning rate schedule with the first epoch warmup. The learning rate was set to 3e -5, the weight decay to 1e -5, the dropout probability to 0.3, and the batch size to 32. Videos were split using a segment window of w = 300 and a stride of s = 200 for all three challenges. This meant we divided each video into segments of 300 frames with an overlap of 100 frames between consecutive segments. This approach helped capture the temporal dynamics of facial expressions and emotions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Overall Results",
      "text": "Table  1  displays the experimental results of our proposed method on the validation set of the VA, Expr, and AU Challenge, where the Concordance Correlation Coefficient (CCC) is utilized as the evaluation metric for both valence and arousal prediction, and F1-score is used to evaluate the result of Expr and AU challenge. As demonstrated in the table, our proposed method outperforms the baseline significantly. These results show that our proposed approach using TCN and a Transformer-based model effectively integrates visual and audio information for improved accuracy in recognizing emotions on this dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "Our proposed approach utilizes a combination of a Temporal Convolutional Network (TCN) and a Transformerbased model to integrate visual and audio information for improved accuracy in recognizing emotions. The TCN captures relationships at low-, intermediate-, and high-level time scales, while the Transformer mechanism merges audio and visual features. We conducted our experiment on the Aff-Wild2 dataset, which is a widely used benchmark dataset for emotion recognition. Our results show that our method significantly outperforms the baseline.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chinatelecom Cloud": "{zhouweiwei,lujiada,lingchengk,wangweifeng,liusw12}@chinatelecom.cn"
        },
        {
          "Chinatelecom Cloud": "Abstract"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "Human emotion recognition holds a pivotal role in facil-"
        },
        {
          "Chinatelecom Cloud": "itating seamless human-computer interaction.\nThis paper"
        },
        {
          "Chinatelecom Cloud": "delineates our methodology in tackling the Valence-Arousal"
        },
        {
          "Chinatelecom Cloud": "(VA) Estimation Challenge, Expression (Expr) Classiﬁca-"
        },
        {
          "Chinatelecom Cloud": "tion Challenge, and Action Unit (AU) Detection Challenge"
        },
        {
          "Chinatelecom Cloud": "within the ambit of\nthe 6th Workshop and Competition on"
        },
        {
          "Chinatelecom Cloud": "Affective Behavior Analysis in-the-wild (ABAW). Our study"
        },
        {
          "Chinatelecom Cloud": "advocates a novel approach aimed at reﬁning continuous"
        },
        {
          "Chinatelecom Cloud": "emotion recognition. We achieve this by initially harnessing"
        },
        {
          "Chinatelecom Cloud": "pre-training with Masked Autoencoders\n(MAE) on facial"
        },
        {
          "Chinatelecom Cloud": "datasets,\nfollowed by ﬁne-tuning on the aff-wild2 dataset"
        },
        {
          "Chinatelecom Cloud": "annotated with expression (Expr)\nlabels.\nThe pre-trained"
        },
        {
          "Chinatelecom Cloud": "model serves as an adept visual\nfeature extractor,\nthereby"
        },
        {
          "Chinatelecom Cloud": "enhancing the model’s robustness. Furthermore, we bolster"
        },
        {
          "Chinatelecom Cloud": "the performance of continuous emotion recognition by in-"
        },
        {
          "Chinatelecom Cloud": "tegrating Temporal Convolutional Network (TCN) modules"
        },
        {
          "Chinatelecom Cloud": "and Transformer Encoder modules into our framework."
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "1. Introduction"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "Facial Expression Recognition (FER) holds immense po-"
        },
        {
          "Chinatelecom Cloud": "tential across a spectrum of applications, ranging from dis-"
        },
        {
          "Chinatelecom Cloud": "cerning emotions in videos to bolstering security through"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "facial recognition systems, and even enriching virtual real-"
        },
        {
          "Chinatelecom Cloud": "ity experiences. While signiﬁcant strides have been made in"
        },
        {
          "Chinatelecom Cloud": "various facial-related tasks, such as face and attribute recog-"
        },
        {
          "Chinatelecom Cloud": "nition,\nthe nuanced realm of emotional comprehension re-"
        },
        {
          "Chinatelecom Cloud": "mains a challenge."
        },
        {
          "Chinatelecom Cloud": "The intricacies of emotional expressions often present"
        },
        {
          "Chinatelecom Cloud": "subtle differentiations that can introduce ambiguity or un-"
        },
        {
          "Chinatelecom Cloud": "certainty in accurately perceiving emotions. Consequently,"
        },
        {
          "Chinatelecom Cloud": "this complexity poses hurdles in effectively assessing an in-"
        },
        {
          "Chinatelecom Cloud": "dividual’s emotional state.\nOne of\nthe primary obstacles"
        },
        {
          "Chinatelecom Cloud": "lies in the inadequacy of existing FER datasets to encapsu-"
        },
        {
          "Chinatelecom Cloud": "late the breadth and depth of human emotional expressions,"
        },
        {
          "Chinatelecom Cloud": "hindering the development of robust models. Efforts to ex-"
        },
        {
          "Chinatelecom Cloud": "pand and diversify these datasets are imperative to enhance"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tion from the videos were leveraged, and a transformer-": "based framework was implemented to fuse the multi-modal",
          "is removed and replaced with fully connected layers con-": "nected to the MAE encoder. Subsequently, Expr labels are"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "features.",
          "is removed and replaced with fully connected layers con-": "ﬁne-tuned to obtain a feature extractor more aligned with"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "SituTech [17] utilized multi-modal feature combinations",
          "is removed and replaced with fully connected layers con-": "the distribution of aff-wild2 data."
        },
        {
          "tion from the videos were leveraged, and a transformer-": "extracted by several different pre-trained models, which",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "3.2. Temporal Convolutional Network"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "were applied to capture more effective emotional informa-",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "tion.",
          "is removed and replaced with fully connected layers con-": "Videos\nare ﬁrst\nsplit\ninto segments with a window size"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "Temporal Convolutional Network (TCN) was proposed",
          "is removed and replaced with fully connected layers con-": "stride\nGiven\nthe\nw and\ns.\nsegment window w and"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "by Colin Lea et al. [16], which hierarchically captured rela-",
          "is removed and replaced with fully connected layers con-": "stride\na\nvideo with\nframes would\nbe\nsplit\ninto\ns,\nn"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "tionships at low-, intermediate-, and high-level time scales.",
          "is removed and replaced with fully connected layers con-": "segments, where\nthe\nsegment\ncontains\n[n/s] + 1\ni-th"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "Jin Fan et al.\n[1] proposed a model with a spatial-temporal",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "frames(cid:8)F(i−1)∗s+1, . . . , F(i−1)∗s+w(cid:9)."
        },
        {
          "tion from the videos were leveraged, and a transformer-": "attention mechanism to catch dynamic internal correlations",
          "is removed and replaced with fully connected layers con-": "In other words, videos are cut\ninto some overlapping"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "with stacked TCN backbones to extract features from dif-",
          "is removed and replaced with fully connected layers con-": "chunks, each with a ﬁxed number of frames. The purpose"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "ferent window sizes.",
          "is removed and replaced with fully connected layers con-": "of doing this is to break down the video into smaller parts"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "The Transformer mechanism proposed by Vaswani et al.",
          "is removed and replaced with fully connected layers con-": "that are easier to process and analyze. Each chunk has some"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "[19] has achieved high performance in many tasks, so many",
          "is removed and replaced with fully connected layers con-": "degree of overlap with the previous and next ones so that no"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "researchers exploited the Transformer for affective behavior",
          "is removed and replaced with fully connected layers con-": "information in the video is missed."
        },
        {
          "tion from the videos were leveraged, and a transformer-": "studies. Zhao et al.\n[22] proposed a model with spatial and",
          "is removed and replaced with fully connected layers con-": "We denote visual features as fi corresponding to the i-th"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "temporal Transformers for\nfacial expression analysis.\nJa-",
          "is removed and replaced with fully connected layers con-": "segment extracted by pre-trained and ﬁne-tuned ViT-Base"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "cob et al.\n[18] proposed a network to learn the relationship",
          "is removed and replaced with fully connected layers con-": "encoder."
        },
        {
          "tion from the videos were leveraged, and a transformer-": "between action units with a transformer correlation module.",
          "is removed and replaced with fully connected layers con-": "Visual feature is fed into a dedicated Temporal Convolu-"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "Inspired by the previous work, in this paper, we propose",
          "is removed and replaced with fully connected layers con-": "tional Network (TCN) for temporal encoding, which can be"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "to use MAE as a feature extractor and design a model con-",
          "is removed and replaced with fully connected layers con-": "formulated as follows:"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "sisting of TCN and Transformer to enhance the performance",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "gi = TCN (fi)"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "of emotion recognition.",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "This means that we use a special type of neural network"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "3. Methodology",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "that can capture the temporal patterns and dependencies of"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "the features over time. The TCN takes the input feature vec-"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "In this section, we describe in detail our proposed method",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "tor and applies a series of convolutional layers with different"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "for tackling the three challenging tasks of affective behav-",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "kernel sizes and dilation rates to produce an output feature"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "ior analysis in the wild that are addressed by the 6th ABAW",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "vector. The output feature vector has the same length as the"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "Competition: Valence-Arousal Estimation, Expr Classiﬁca-",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "input feature vector but contains more information about the"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "tion, and AU Detection. We explain how we design our",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "temporal context. For example, the TCN can learn how the"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "model architecture, data processing, and training strategy",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "image changes over time in each segment of the video."
        },
        {
          "tion from the videos were leveraged, and a transformer-": "for each task.",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "3.3. Temporal Encoder"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "3.1. MAE Pre training",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "We utilize a transformer encoder to model the temporal in-"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "Inspired by Netease, we conduct pre-training of our MAE",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "formation in the video segment as well, which can be for-"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "on a facial image dataset. To this end, we also curate a large-",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "mulated as follows:"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "scale dataset of\nfacial expressions to learn facial\nfeatures,",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "consisting of AffectNet, RAF-DB, FER2013, and FER+.",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "",
          "is removed and replaced with fully connected layers con-": "hi = TransformerEncoder (gi) ."
        },
        {
          "tion from the videos were leveraged, and a transformer-": "Subsequently, the MAE model is pre-trained on this dataset",
          "is removed and replaced with fully connected layers con-": ""
        },
        {
          "tion from the videos were leveraged, and a transformer-": "in a self-supervised manner. Speciﬁcally, our MAE consists",
          "is removed and replaced with fully connected layers con-": "The Transformer encoder only models the context within"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "of a ViT-Base encoder and a ViT decoder. The pre-training",
          "is removed and replaced with fully connected layers con-": "a single segment,\nthereby ignoring the dependencies be-"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "process of MAE follows a masked-reconstruction method,",
          "is removed and replaced with fully connected layers con-": "tween frames across segments. To account for the context"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "where\nimages\nare ﬁrst\ndivided into\na\nseries\nof patches",
          "is removed and replaced with fully connected layers con-": "of different frames, overlapping between consecutive seg-"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "(16x16), with\n75% of\nthese\npatches\nrandomly masked.",
          "is removed and replaced with fully connected layers con-": "ments can be employed, thus enabling the capture of the de-"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "These masked images are then fed into the MAE encoder,",
          "is removed and replaced with fully connected layers con-": "pendencies between frames across segments, which means"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "while the MAE decoder\nis tasked with reconstructing the",
          "is removed and replaced with fully connected layers con-": "s ≤ w."
        },
        {
          "tion from the videos were leveraged, and a transformer-": "complete image. The loss function for MAE pre-training is",
          "is removed and replaced with fully connected layers con-": "We use another\ntype of neural network that can learn"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "pixel-level L2 loss, aiming to minimize the difference be-",
          "is removed and replaced with fully connected layers con-": "the relationships and interactions among the features within"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "tween the reconstructed image and the target\nimage. Once",
          "is removed and replaced with fully connected layers con-": "each segment.\nThe transformer encoder\ntakes\nthe\ninput"
        },
        {
          "tion from the videos were leveraged, and a transformer-": "self-supervised learning is completed,\nthe MAE decoder",
          "is removed and replaced with fully connected layers con-": "feature vector and applies a series of self-attention layers"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and feed-forward layers to produce an output feature vec-": "tor. The output feature vector has more semantic meaning"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "and representation power than the input feature vector. For"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "example,\nthe transformer encoder can learn how different"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "parts of the image relate to each other in each segment of"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "the video. However, the transformer encoder does not con-"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "sider how different segments of the video are connected or"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "inﬂuenced by each other.\nTo solve this problem, we can"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "make some segments overlap with each other so that some"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "frames are shared by two or more segments. This way, we"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "can capture some information about how different segments"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "affect each other. The degree of overlap is controlled by two"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "parameters: s is the length of a segment and w is the sliding"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "window size.\nthen there\nIf s is smaller than or equal\nto w,"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "will be some overlap between consecutive segments."
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "3.3.1\nPrediction"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "After the temporal encoder,\nthe features hi are ﬁnally fed"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "into MLP for\nregression, which can be formulated as fol-"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "lows:"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "yi = MLP(hi)"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "where yi are the predictions of i-th segment. For VA chal-"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "lenge, yi ∈ Rl×2. For Expr challenge, yi ∈ Rl×8. For AU"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "challenge, yi ∈ Rl×12 ."
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "The prediction vector contains the values we want to es-"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "timate for each segment. The MLP consists of several lay-"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "ers of neurons that can learn non-linear transformations of"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "the input. The MLP can be trained to minimize the error"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "between the prediction vector and the ground truth vector."
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "The ground truth vector is the values we want to predict for"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "each segment. Depending on what kind of challenge we"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "are solving, we have different types of ground truth vectors"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "and prediction vectors. For the VA challenge, we want\nto"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "predict two values: valence and arousal. Valence measures"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "how positive or negative an emotion is. Arousal measures"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "how active or passive an emotion is. For the Expr challenge,"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "we want to predict eight values: one for each basic expres-"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "sion (anger, disgust, fear, happiness, sadness, and surprise)"
        },
        {
          "and feed-forward layers to produce an output feature vec-": ""
        },
        {
          "and feed-forward layers to produce an output feature vec-": "plus neutral and other expressions. For the AU challenge,"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "we want\nto predict\ntwelve values: one for each action unit"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "(AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23,"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "AU24, AU25, AU26)."
        },
        {
          "and feed-forward layers to produce an output feature vec-": "3.4. Loss Functions"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "VA challenge: We use the Concordance Correlation Coefﬁ-"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "cient (CCC) between the predictions and the ground truth"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "labels\nas\nthe measure, which is deﬁned as\nin Eq 1.\nIt"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "measures the correlation between two sequences x and y"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "and ranges between -1 and 1, where -1 means perfect anti-"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "correlation, 0 means no correlation, and 1 means perfect"
        },
        {
          "and feed-forward layers to produce an output feature vec-": "correlation. The loss is calculated as Eq 2."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: displays the experimental results of our proposed",
      "data": [
        {
          "Task": "",
          "Evaluation Metric": "",
          "Method": "Ours",
          "Fold 0": "0.5385",
          "Fold 1": "0.6404",
          "Fold 2": "0.4926",
          "Fold 3": "0.5863",
          "Fold 4": "0.5403"
        },
        {
          "Task": "Valence",
          "Evaluation Metric": "",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Method": "Baseline",
          "Fold 0": "0.24",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        },
        {
          "Task": "",
          "Evaluation Metric": "CCC",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Method": "Ours",
          "Fold 0": "0.6224",
          "Fold 1": "0.5651",
          "Fold 2": "0.6015",
          "Fold 3": "0.6812",
          "Fold 4": "0.6342"
        },
        {
          "Task": "Arousal",
          "Evaluation Metric": "",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Method": "Baseline",
          "Fold 0": "0.20",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Method": "Ours",
          "Fold 0": "0.4561",
          "Fold 1": "0.4478",
          "Fold 2": "0.4463",
          "Fold 3": "0.4583",
          "Fold 4": "0.4506"
        },
        {
          "Task": "Expr",
          "Evaluation Metric": "F1-score",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Method": "Baseline",
          "Fold 0": "0.23",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Method": "Ours",
          "Fold 0": "0.5762",
          "Fold 1": "0.5566",
          "Fold 2": "0.5018",
          "Fold 3": "0.5556",
          "Fold 4": "0.5819"
        },
        {
          "Task": "AU",
          "Evaluation Metric": "F1-score",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Method": "Baseline",
          "Fold 0": "0.39",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: displays the experimental results of our proposed",
      "data": [
        {
          "Table 1. Results for the ﬁve folds of three tasks": "4.1.2\nTask Trainging"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "We used the AdamW optimizer and cosine learning rate"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "schedule with the ﬁrst epoch warmup.\nThe learning rate"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "was set\nthe dropout\nto 3e − 5,\nthe weight decay to 1e − 5,"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "probability to 0.3, and the batch size to 32."
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "Videos were split using a segment window of w = 300"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "and a stride of s = 200 for all three challenges. This meant"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "we divided each video into segments of 300 frames with an"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "overlap of 100 frames between consecutive segments. This"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "approach helped capture the temporal dynamics of\nfacial"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "expressions and emotions."
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "4.2. Overall Results"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "Table 1 displays the experimental results of our proposed"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "method on the validation set of\nthe VA, Expr,\nand AU"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "Challenge, where the Concordance Correlation Coefﬁcient"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "(CCC) is utilized as the evaluation metric for both valence"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "and arousal prediction, and F1-score is used to evaluate the"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "result of Expr and AU challenge. As demonstrated in the"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "table, our proposed method outperforms the baseline sig-"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "niﬁcantly. These results show that our proposed approach"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "using TCN and a Transformer-based model effectively inte-"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "grates visual and audio information for improved accuracy"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "in recognizing emotions on this dataset."
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "5. Conclusion"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "Our proposed approach utilizes a combination of a Tem-"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "poral Convolutional Network (TCN)\nand a Transformer-"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "based model\nto integrate visual and audio information for"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "improved accuracy in recognizing emotions. The TCN cap-"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "tures\nrelationships at\nlow-,\nintermediate-,\nand high-level"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "time scales, while the Transformer mechanism merges au-"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "dio and visual features. We conducted our experiment on"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": ""
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "the Aff-Wild2 dataset, which is a widely used benchmark"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "dataset for emotion recognition. Our results show that our"
        },
        {
          "Table 1. Results for the ﬁve folds of three tasks": "method signiﬁcantly outperforms the baseline."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Zafeiriou.\nDistribution matching for heterogeneous multi-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "arXiv\npreprint\ntask\nlearning:\na\nlarge-scale\nface\nstudy."
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "arXiv:2105.03790, 2021."
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[12] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Cowen, and Stefanos Zafeiriou. Abaw: Valence-arousal esti-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "mation, expression recognition, action unit detection & emo-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "tional reaction intensity estimation challenges, 2023."
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[13] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Cowen, and Stefanos Zafeiriou. Abaw: Valence-arousal esti-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "mation, expression recognition, action unit detection & emo-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "tional reaction intensity estimation challenges.\nIn Proceed-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "ings of\nthe IEEE/CVF Conference on Computer Vision and"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Pattern Recognition, pages 5888–5897, 2023. 1"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[14] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "fanos Zafeiriou, Chunchang Shao, and Guanyu Hu. The 6th"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "affective behavior analysis in-the-wild (abaw) competition."
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "arXiv preprint arXiv:2402.19344, 2024. 1"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[15]\nFelix Kuhnke, Lars Rumberg, and J¨orn Ostermann.\nTwo-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "stream aural-visual affect analysis in the wild.\nIn 2020 15th"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "IEEE International Conference on Automatic Face and Ges-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "ture Recognition (FG 2020), pages 600–605. IEEE, 2020. 1"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[16] Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager."
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Temporal convolutional networks: A uniﬁed approach to ac-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "tion segmentation.\nIn Computer Vision–ECCV 2016 Work-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "shops: Amsterdam, The Netherlands, October 8-10 and 15-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "16, 2016, Proceedings, Part\nIII 14, pages 47–54. Springer,"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "2016. 2"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[17] Chuanhe Liu, Xinjie Zhang, Xiaolong Liu, Tenggan Zhang,"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Jiang. Multi-modal expression recognition with ensemble"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "method. arXiv preprint arXiv:2303.10033, 2023. 2"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[18] Geethu Miriam Jacob and Bj¨orn Stenger. Facial action unit"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "detection with transformers.\nIn 2021 IEEE/CVF Conference"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "on Computer Vision and Pattern Recognition (CVPR), pages"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "7676–7685, 2021. 2"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Polosukhin. Attention is all you need. Advances in neural"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "information processing systems, 30, 2017. 2"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[20]\nStefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Athanasios Papaioannou, Guoying Zhao,\nand\nIrene Kot-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "sia. Aff-wild: Valence and arousal\n‘in-the-wild’challenge."
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "and Pattern Recognition Workshops\nIn Computer Vision"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "(CVPRW), 2017 IEEE Conference on,\npages 1980–1987."
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "IEEE, 2017. 1"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[21] Wei Zhang, Bowen Ma, Feng Qiu,\nand Yu Ding. Multi-"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "modal facial affective analysis based on masked autoencoder."
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "the IEEE/CVF Conference on Computer\nIn Proceedings of"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Vision and Pattern Recognition, pages 5792–5801, 2023. 1"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[22] Zengqun Zhao and Qingshan Liu.\nFormer-dfer: Dynamic"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "facial expression recognition transformer.\nIn Proceedings"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "of\nthe 29th ACM International Conference on Multimedia,"
        },
        {
          "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "pages 1553–1561, 2021. 2"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Parallel spatio-temporal attention-based tcn for multivariate time series prediction",
      "authors": [
        "Jin Fan",
        "Ke Zhang",
        "Yipan Huang",
        "Yifei Zhu",
        "Baiping Chen"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "2",
      "title": "A multi-modal and multi-task learning method for action unit and expression recognition",
      "authors": [
        "Jin Yue",
        "Tianqing Zheng",
        "Chao Gao",
        "Guoqiang Xu"
      ],
      "venue": "A multi-modal and multi-task learning method for action unit and expression recognition",
      "arxiv": "arXiv:2107.04187,2021.1"
    },
    {
      "citation_id": "3",
      "title": "Learning from synthetic data & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias",
        "Abaw"
      ],
      "year": "2022",
      "venue": "Learning from synthetic data & multi-task learning challenges",
      "arxiv": "arXiv:2207.01138"
    },
    {
      "citation_id": "4",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "6",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "7",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "8",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "9",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "10",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "11",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "12",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges"
    },
    {
      "citation_id": "13",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "arxiv": "arXiv:2402.19344"
    },
    {
      "citation_id": "15",
      "title": "Twostream aural-visual affect analysis in the wild",
      "authors": [
        "Felix Kuhnke",
        "Lars Rumberg",
        "Jörn Ostermann"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "16",
      "title": "Temporal convolutional networks: A unified approach to action segmentation",
      "authors": [
        "Colin Lea",
        "Rene Vidal",
        "Austin Reiter",
        "Gregory Hager"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016 Workshops"
    },
    {
      "citation_id": "17",
      "title": "Multi-modal expression recognition with ensemble method",
      "authors": [
        "Chuanhe Liu",
        "Xinjie Zhang",
        "Xiaolong Liu",
        "Tenggan Zhang",
        "Liyu Meng",
        "Yuchen Liu",
        "Yuanyuan Deng",
        "Wenqiang Jiang"
      ],
      "year": "2023",
      "venue": "Multi-modal expression recognition with ensemble method",
      "arxiv": "arXiv:2303.10033"
    },
    {
      "citation_id": "18",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "Miriam Geethu",
        "Björn Jacob",
        "Stenger"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "19",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "20",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "21",
      "title": "Multimodal facial affective analysis based on masked autoencoder",
      "authors": [
        "Wei Zhang",
        "Bowen Ma",
        "Feng Qiu",
        "Yu Ding"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    }
  ]
}