{
  "paper_id": "2406.10741v1",
  "title": "Speech Emotion Recognition Using Cnn And Its Use Case In Digital Healthcare",
  "published": "2024-06-15T21:33:03Z",
  "authors": [
    "Nishargo Nigar"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The process of identifying human emotion and affective states from speech is known as speech emotion recognition (SER). This is based on the observation that tone and pitch in the voice frequently convey underlying emotion. Speech recognition includes the ability to recognize emotions, which is becoming increasingly popular and in high demand. With the help of appropriate factors (such modalities, emotions, intensities, repetitions, etc.)  found in the data, my research seeks to use the Convolutional Neural Network (CNN) to distinguish emotions from audio recordings and label them in accordance with the range of different emotions. I have developed a machine learning model to identify emotions from supplied audio files with the aid of machine learning methods. The evaluation is mostly focused on precision, recall, and F1 score, which are common machine learning metrics. To properly set up and train the machine learning framework, the main objective is to investigate the influence and cross-relation of all input and output parameters. To improve the ability to recognize intentions, a key condition for communication, I have evaluated emotions using my specialized machine learning algorithm via voice that would address the emotional state from voice with the help of digital healthcare, bridging the gap between human and artificial intelligence (AI). \n Lock Flag This work contains confidential and/or privileged information. Any unauthorized copying, disclosure or distribution of the material is strictly forbidden. This work is subject to copyright. All rights are reserved, whether the whole or parts of the material are concerned, specifically the rights of translation, reprinting, use of illustrations, recitations, broadcasting, reproduction on microfilm or any other storage media and in data banks. The use of general descriptive names, registered names, trademarks, etc.in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. Duplication of this publication or parts thereof is forbidden without written agreement permission by Nishargo Nigar, without any time limitations.",
      "page_start": 3,
      "page_end": 8
    },
    {
      "section_name": "Ii Table Of Contents",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Iii List Of Figures",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Introduction",
      "text": "The past few years have seen technological advancements in data and digitalization, and analytics have reshaped the entire world, increased productivity, and sparked the rise of cuttingedge solutions in a variety of industries, including health. The technical environment has now been enlarged to encompass many additional autonomous domains to build complicated models and solve them using sophisticated tools, software, and procedures. Now that artificial intelligence has advanced, this is both conceivable and essential. The first thing to consider when implementing machine learning is the platform on which the work will be done. Each step of the machine learning process can be automated with the right tools.\n\nSpeech Emotion Recognition (SER) is a field dedicated to the identification and interpretation of human emotions and affective states through the analysis of speech. This is facilitated by the understanding that voice carries valuable cues that reflect underlying emotions, expressed through variations in tone, pitch, and other vocal characteristics. Emotion recognition, as an integral component of speech recognition, has gained significant traction in recent years, owing to its potential applications and the growing recognition of its importance.\n\nThe human voice serves as a powerful medium for the expression of emotions, conveying a wealth of information beyond the mere words spoken. Through subtle variations in tone, pitch, and prosody, individuals imbue their speech with emotional nuances, providing valuable insights into their affective states. As such, the recognition and analysis of emotions embedded within speech have emerged as crucial areas of research, fostering the development of the field known as SER.\n\nWith advancements in speech and language processing technologies, the recognition of emotions from speech has gained prominence and garnered increased attention across various domains. Researchers and practitioners have recognized the potential of SER in fields such as human-computer interaction, virtual assistants, sentiment analysis, market research, and healthcare, among others. The growing popularity of SER can be attributed to its ability to provide rich emotional context, complementing the traditional focus on the linguistic aspects of speech recognition.\n\nThe goal of speech emotion recognition, an emerging field, is to identify and evaluate the emotional content of spoken language. In recent years, the emergence of deep learning techniques, particularly Convolutional Neural Networks (CNNs), has revolutionized the field of speech emotion recognition. CNNs excel at automatically extracting complex hierarchical features from input data, making them well-suited for analyzing speech signals. By leveraging large-scale datasets and powerful computing resources, CNN-based models have demonstrated superior performance in emotion recognition tasks compared to traditional approaches. Using CNN, a class of artificial neural networks that process input data through multiple layers of connected nodes to produce an output, is one well-liked method of voice emotion identification.\n\nThese networks may learn to identify speech patterns that correspond to various emotional states with the use of sizable, annotated datasets.\n\nOne of the key advantages of using CNNs for speech emotion recognition is their ability to learn discriminative features directly from the raw speech signal. Rather than relying solely on handcrafted features, CNNs can automatically learn representations that are more robust and informative for emotion classification. This data-driven approach has shown great promise in capturing subtle variations in speech patterns, thus enabling more accurate and reliable emotion recognition.\n\nMental healthcare is one significant area where feedforward neural networks for voice emotion identification are used. Speech patterns and emotional expression might change because of mental health conditions including depression and anxiety. Mental health experts may be able to identify early warning signals of mental health illnesses by studying speech patterns using speech emotion recognition technology, allowing for self-management. Speech emotion recognition can also be used to modify treatment plans in response to changes in the emotional state of the patient and track the effectiveness of therapy over time.\n\nSpeech emotion recognition can also be helpful for remote mental health monitoring, allowing medical professionals to keep an eye on their patients' emotional states. Patients who are unable to routinely attend in-person appointments or who are reluctant to seek therapy owing to stigma or other obstacles may find this technology to be especially helpful. Speech emotion recognition using feedforward neural networks has the potential to enhance patient outcomes by increasing accessibility and effectiveness of mental healthcare.\n\nThe efficiency of voice emotion identification using feedforward neural networks in mental healthcare has been shown in numerous studies. For instance,  Cummins et al. (2020)  achieved an accuracy of over 80% when classifying speech samples from depressed subjects and healthy controls using a feedforward neural network. An accuracy of over 90% was attained when  Zhao et al. (2021)  employed a similar method to classify speech samples from people with PTSD.\n\nTherefore, feedforward neural network-based speech emotion recognition has great promise for improving mental healthcare by giving doctors a potent tool for identifying and tracking mental health disorders.\n\nThere has been a growing interest in the field of speech emotion recognition due to its potential However, these methods often face challenges in capturing subtle emotional nuances and achieving high accuracy.\n\nThe implications of accurate speech emotion recognition in digital healthcare are profound. For example, we can imagine a scenario where individuals can engage in remote telehealth consultations with healthcare providers, and the system automatically analyzes their speech to assess their emotional well-being. By monitoring changes in emotional states over time, healthcare professionals can gain insights into a patient's mental health and detect early warning signs of conditions such as depression, anxiety, or stress.\n\nFurthermore, speech emotion recognition can be integrated into digital therapeutic interventions. For example, a virtual reality-based therapy system can adapt its content and interactions based on real-time emotion recognition, providing personalized and tailored interventions to patients. Such applications have the potential to enhance the effectiveness of mental health treatments and improve patient outcomes.\n\nIn this thesis book, I aim to explore the field of speech emotion recognition using Convolutional Neural Networks and investigate its use case in digital healthcare. I will delve into the fundamental concepts of speech processing, deep learning, and emotion recognition algorithms.\n\nMoreover, I will discuss the design and implementation of a CNN-based speech emotion recognition system and evaluate its performance on benchmark datasets. Finally, we will explore the applications of speech emotion recognition in digital healthcare, discussing potential challenges, ethical considerations, and future directions for research.",
      "page_start": 9,
      "page_end": 12
    },
    {
      "section_name": "Background Of The Study",
      "text": "A vital component of overall health and wellbeing is mental health, which is significant in the context of digital healthcare. The ability to understand and interpret human emotions has long been a subject of fascination and importance in various fields, ranging from psychology to human-computer interaction  (Picard, 1997) . Emotions play a crucial role in our daily lives, influencing our decisions, behaviors, and overall well-being. Traditionally, the recognition and understanding of emotions have heavily relied on facial expressions, body language, and verbal cues. However, speech, as a powerful medium of communication, holds a wealth of information that can provide valuable insights into human emotions  (Schuller et al., 2013) .\n\nSER is an interdisciplinary field that aims to develop computational techniques capable of automatically analyzing and deciphering emotions conveyed through speech signals  (Schuller et al., 2013) . By capturing and interpreting the underlying emotional content of speech, SER has the potential to revolutionize numerous domains, including digital healthcare. With the rise of telehealth and remote patient monitoring, there is an increasing need for non-intrusive and scalable methods to assess individuals' emotional well-being and mental health from a distance.\n\nHistorically, SER research predominantly relied on handcrafted acoustic features extracted from speech signals, such as pitch, energy, and formant frequencies  (Schuller et al., 2013) . While these features provided a foundation for emotion classification, they often fell short in capturing the intricate nuances and complexities of human emotions. Emotions are multifaceted, characterized by dynamic temporal patterns and a broad spectrum of expressiveness. Thus, the need for more sophisticated and data-driven approaches to SER became apparent.\n\nThe advent of deep learning, particularly Convolutional Neural Networks (CNNs), has opened up new possibilities for SER  (Hinton et al., 2012) . CNNs, inspired by the structure and functionality of the human visual cortex, excel at automatically learning hierarchical representations from raw input data. Their ability to capture intricate patterns and extract discriminative features has led to significant advancements in various fields, including computer vision and natural language processing  (Hinton et al., 2012) . Leveraging the power of CNNs for SER presents an opportunity to overcome the limitations of traditional handcrafted features and achieve more accurate and robust emotion recognition.\n\nFurthermore, the integration of speech emotion recognition with digital healthcare holds tremendous potential  (Schuller et al., 2013) . As healthcare systems evolve and embrace digital solutions, the collection and analysis of patient data become more accessible and feasible. By incorporating speech emotion recognition into digital healthcare platforms, healthcare professionals can gain real-time insights into patients' emotional states, enabling more personalized and effective care interventions. For instance, a virtual therapy system equipped with SER capabilities could adapt its content and interactions based on the detected emotions, tailoring the therapeutic experience to the individual's emotional needs.\n\nWhile research on speech emotion recognition using CNNs has made significant strides, there are still challenges and areas for improvement. The development of robust and generalizable models that can handle variations in accents, languages, and cultural contexts remains a priority  (Hinton et al., 2012) . Moreover, ethical considerations regarding data privacy, informed consent, and the responsible use of emotion-related information in healthcare settings need to be carefully addressed.\n\nIn this thesis book, I aim to contribute to the field of speech emotion recognition by focusing on the application of Convolutional Neural Networks and exploring their use case in digital healthcare. By investigating the potential of CNNs to automatically extract meaningful emotionrelated features from speech signals, I seek to advance the accuracy and reliability of emotion recognition systems. Moreover, I examine the practical implications and challenges of integrating speech emotion recognition into digital healthcare, with a focus on ensuring patient privacy, security, and ethical practices.\n\nBy shedding light on the intersection of speech emotion recognition, deep learning, and digital healthcare, this thesis book aims to pave the way for innovative applications that enhance our understanding of human emotions, revolutionize patient care, and contribute to the advancement of digital healthcare technologies.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Objective Of The Study",
      "text": "Speech Emotion Recognition is a burgeoning field that focuses on the identification and understanding of human emotions and affective states through speech. The objective of this project is to employ Convolutional Neural Networks (CNNs) as a means to recognize emotions from unseen data, specifically audio files, and subsequently label them according to various emotional ranges. The study seeks to explore the efficacy of CNNs in accurately detecting different emotions and examine the potential application of this technology in managing depression and anxiety within the realm of digital healthcare.\n\nResearch studies have demonstrated that capturing changes in vocal features, such as pitch and speech rate, can offer valuable insights into an individual's emotional state and aid in the diagnosis of mental health conditions like depression and anxiety  (Girardi et al., 2018)  Additionally, the pressing need for effective emotion recognition systems is driven by the everexpanding demand for personalized and context-aware interactions in various domains. From virtual assistants understanding user emotions to adaptive educational technologies tailoring content based on emotional cues, the applications of SER are broad and diverse. In fields like healthcare and mental health, accurate emotion recognition from speech can aid in the assessment, diagnosis, and monitoring of individuals' emotional well-being, providing valuable insights to healthcare professionals and enabling more targeted interventions.\n\nThe increasing recognition of the significance of emotion recognition has fueled a surge in research and development efforts in the field of SER. Researchers are exploring innovative techniques and approaches to improve the accuracy and robustness of emotion recognition systems, leveraging advancements in machine learning, deep learning, and signal processing.\n\nThese efforts aim to unlock the potential of speech as a rich source of emotional information, contributing to the ever-expanding field of speech and emotion analysis.",
      "page_start": 13,
      "page_end": 15
    },
    {
      "section_name": "Outline Of The Study",
      "text": "The structure of the thesis is organized as follows. In Section 2, an extensive literature review is presented, encompassing previous research and studies related to speech emotion recognition.\n\nThis section provides a comprehensive overview of the current state of the field, highlighting the limitations of traditional approaches and emphasizing the role of Convolutional Neural Networks (CNNs) in advancing SER.\n\nMoving on to Section 3, the focus shifts to the structure and setup of the proposed model. This The thesis culminates with Section 6, which encompasses the conclusion part. This section summarizes the key findings and contributions of the study, reflecting upon the research objectives and questions posed at the beginning. It offers a concise and insightful recapitulation of the overall significance and impact of the study, while also discussing any limitations or challenges encountered. Finally, the conclusion part may suggest future research directions, provide practical implications for the field, and offer final remarks that encapsulate the essence of the thesis.\n\nIn this manner, the thesis progresses coherently through the literature review, model structure and setup, implementation, testing and result analysis, ultimately culminating in the conclusive findings and implications in the conclusion part presented in Section 6.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Literature Review",
      "text": "The literature review provides a comprehensive overview of existing research and studies related to Speech Emotion Recognition and its applications in various domains. This section explores the historical context, theoretical foundations, and advancements in SER methodologies, highlighting the contributions made by researchers and practitioners in the field.   et al., 2013) . RNNs, on the other hand, have been employed to capture the sequential nature of speech signals and contextual information for improved emotion recognition  (Zhao et al., 2019) . The availability of annotated datasets has also contributed to the advancement of SER. Datasets such as the Berlin Emotional Speech Database (Emo-DB)  (Burkhardt et al., 2005) , the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset  (Busso et al., 2008) , and the Toronto emotional speech set (TESS)  (Dupuis and Friend, 2009)  have provided standardized benchmarks for evaluating and comparing different emotion recognition models. These datasets contain recordings of actors expressing various emotions, providing researchers with valuable resources for training and testing their algorithms. In summary, the historical development of SER has evolved from early studies on vocal expression of emotions to the application of statistical models and, more recently, deep learning approaches. The integration of emotion recognition into affective computing and human-computer interaction has paved the way for numerous applications in fields such as healthcare, virtual assistants, and entertainment. The availability of annotated datasets and the advancements in machine learning techniques have propelled the field forward, enabling more accurate and robust emotion recognition from speech.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Feature Extraction And Representation:",
      "text": "Feature extraction plays a crucial role in SER, as it aims to capture the discriminative aspects of speech that convey emotional information. Various acoustic features have been explored, including prosodic features (e.g., pitch, intensity, and duration), spectral features (e.g., Mel-frequency cepstral coefficients), and voice quality-related features (e.g., jitter and shimmer)  (Schuller et al., 2013) . These features provide Feature extraction is a crucial step in Speech Emotion Recognition as it aims to capture the distinctive aspects of speech that convey emotional information. Various acoustic features have been explored to represent the vocal characteristics associated with different emotions. One commonly used set of features is prosodic features, which capture the temporal and melodic aspects of speech. These include fundamental frequency (F0), also known as pitch, which reflects the variations in vocal cord vibrations and is associated with emotions such as excitement or sadness.\n\nIntensity, representing the energy level of the speech signal, is another important prosodic feature related to emotional intensity or arousal. Duration measures the length of speech segments and can indicate the emphasis or elongation of certain sounds, contributing to the expression of emotions  (Schuller et al., 2013) . Spectral features are another class of acoustic features that focus on the frequency content of speech. Melfrequency cepstral coefficients (MFCCs) are widely used in SER as they capture the spectral envelope of the speech signal. MFCCs represent the power spectrum of the speech signal on a mel-scale, which is perceptually more relevant than the linear scale.\n\nThese coefficients capture information related to the shape of vocal tract resonances and can be indicative of emotional content in speech  (Eyben et al., 2013) . Additionally, voice quality-related features have been investigated for SER. These features, including jitter and shimmer, measure the variations in pitch period and amplitude, respectively.\n\nThey provide insights into the stability and smoothness of the vocal production and have been found to correlate with certain emotional states  (Gomez et al., 2019) . Moreover, with advancements in technology, researchers have explored the use of higher-level representations such as prosody patterns, linguistic content, and contextual information.\n\nProsody patterns refer to the patterns of pitch, intensity, and timing across larger segments of speech, capturing the melodic and rhythmic aspects that contribute to emotional expression. Linguistic content refers to the words and semantic information conveyed in the speech signal, which can provide cues about the emotional context.\n\nContextual information takes into account the surrounding context, including the speaker's identity, cultural factors, and situational context, which can influence the interpretation of emotions in speech  (Schuller, 2018) . The selection of appropriate features for SER depends on the specific research objectives, dataset characteristics, and the emotions of interest. Feature engineering plays a crucial role in identifying discriminative features that effectively capture emotional information in speech.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Traditional Approaches To Ser:",
      "text": "Over the years, various traditional approaches have been employed for SER to analyze and classify emotional states from speech signals.\n\nThese approaches typically involve the utilization of machine learning algorithms and handcrafted features to model the relationship between acoustic properties and emotions. One commonly used traditional approach is based on the utilization of statistical models such as Hidden Markov Models (HMMs) and Gaussian Mixture Models (GMMs). These models capture the statistical patterns and temporal dynamics present in speech signals. HMMs have been particularly popular for modeling the transitions between different emotional states, while GMMs have been used for estimating the probability distributions of acoustic features corresponding to different emotions  (Wang and Narayanan, 2005) . Another approach in traditional SER involves the use of rule-based systems. These systems rely on predefined linguistic and acoustic rules to associate specific patterns in speech with different emotional categories. For example, certain linguistic cues, such as the presence of specific words or phrases, can indicate the expression of certain emotions. Acoustic cues, such as high pitch for excitement or slow speech rate for sadness, can also be used as rules for emotion classification  (Batliner et al., 2004) . Additionally, some traditional approaches focus on feature selection and dimensionality reduction techniques to improve the efficiency and effectiveness of emotion recognition. Feature selection methods aim to identify the most informative subset of features that contribute significantly to emotion classification.\n\nTechniques such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) have been employed to reduce the dimensionality of feature vectors while retaining the discriminative information  (Salam et al., 2016) . However, traditional approaches to SER often face limitations in capturing complex patterns and extracting high-level representations from speech data. They heavily rely on manually engineered features, which may not fully capture the rich and subtle cues of emotional expression in speech. Moreover, these approaches may struggle to generalize well to unseen data  Researchers and practitioners should work towards developing guidelines and frameworks to ensure the responsible and ethical use of SER, respecting user rights and fostering transparency  (Martin et al., 2019) .\n\nBy addressing these challenges and exploring the suggested directions, the field of SER can continue to evolve and contribute to numerous domains, including mental health, humancomputer interaction, and customer service, among others.\n\nIn summary, the literature review highlights the historical development, methodologies, and applications of SER. It underscores the shift from traditional approaches to deep learning techniques, emphasizing the role of CNNs and RNNs in capturing emotional cues from speech.\n\nThe review also emphasizes the importance of annotated datasets and evaluation metrics for benchmarking and comparing different SER models.\n\nIn Another Conv2D layer with 64 filters of size 3x3 is added, followed by another MaxPooling2D layer and a Dropout layer with the same configuration as before.\n\ne. Flatten Layer: The output of the second dropout layer is then flattened using the Flatten layer. This flattens the multidimensional feature maps into a one-dimensional vector, which can be fed into the dense layers of the network.\n\nf. Dense Layer: The next step is to add a fully connected Dense layer with 128 units and the ReLU activation function. This dense layer serves as a hidden layer and allows the model to learn more complex representations from the flattened input. The created model is then returned by the function as the output.",
      "page_start": 19,
      "page_end": 29
    },
    {
      "section_name": "Methodology",
      "text": "In the methodology section of this study, two key components were employed: Convolutional",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Used Tools",
      "text": "In the development of my model, I have used Python and Kaggle. More of the details are included below:\n\n1. Python: I employed the Python programming language as the primary tool for implementation. Python is a versatile and widely adopted language known for its simplicity, readability, and extensive range of libraries and frameworks. Its rich ecosystem of data science and machine learning libraries, such as TensorFlow, Keras, and scikit-learn, provided me with the necessary resources to efficiently build and train my model for speech emotion recognition. Python's syntax and structure enabled me to write clean and organized code, facilitating the implementation and maintenance of the model. The availability of various pre-built functions and modules allowed me to leverage existing solutions and implement complex operations with ease. Moreover, Python's interactive nature and vast community support made it convenient to troubleshoot issues and seek assistance whenever needed.",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Kaggle",
      "text": "",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Dataset",
      "text": "The dataset employed in this study is the RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song), which holds significant prominence and widespread usage in the domain of Speech SER. RAVDESS serves as a comprehensive collection of recordings comprising emotional speech and song, facilitating researchers in exploring and analyzing various facets of human emotional expression through vocal signals. The files included in this portion of the RAVDESS dataset encompass a total of 1440 files, resulting from 60 trials per actor multiplied by 24 actors. Within the RAVDESS, there are 24 professional actors, equally divided between 12 females and 12 males, who vocalize two lexically-matched statements in a neutral North American accent. The speech emotions covered in this dataset include calm, happy, sad, angry, fearful, surprise, and disgust expressions. Each expression is produced at two levels of emotional intensity: normal and strong, with an additional neutral expression.\n\nThe file naming convention for the 1440 files follows a specific structure, utilizing a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers serve to define the characteristics of each stimulus:\n\n• Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n\n• Vocal channel (01 = speech, 02 = song).\n\n• Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n\n• Emotional intensity (01 = normal, 02 = strong). Note: There is no strong intensity for the 'neutral' emotion.\n\n• Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n\n• Repetition (01 = 1st repetition, 02 = 2nd repetition).\n\n• Actor (01 to 24, with odd-numbered actors being male and even-numbered actors being female).\n\nAs an example, consider the filename 03-01-06-01-02-01-12.wav, which can be interpreted as follows:\n\n• Audio-only (03).\n\n• Speech (01).\n\n• Fearful (06).\n\n• Normal intensity (01).\n\n• Statement \"dogs\" (02).\n\n• 1st Repetition (01).\n\n• 12th Actor (12).\n\n• Female, as the actor ID number is even.\n\nThe recordings for this research were conducted in a professional recording studio at Ryerson University, ensuring a controlled and consistent environment. To maintain consistency among actors, specific guidelines were followed. Actors wore black t-shirts, had minimal makeup, were clean-shaven, wore contact lenses if necessary, and refrained from wearing distinctive jewelry.\n\nThroughout the recording process, actors remained standing, although a seat was provided for them to rest and prepare between different emotional conditions.\n\nTo ensure appropriate microphone levels, actors produced several highly angry expressions as a reference, allowing adjustments to be made accordingly. The recording session commenced with practice trials for each emotional expression in speech, followed by the completion of all speech trials. Afterward, actors were given a 60-minute break before proceeding to practice trials and subsequent recordings for the singing condition. The order of recording was carefully planned to conduct speech trials before singing trials, eliminating potential influences from the singing condition. Trials were organized in blocks based on emotions, starting with low-intensity emotions and progressing to their highly intense counterparts, enabling actors to immerse themselves in and sustain the desired emotional state for all productions within a specific emotional category.\n\nTo guide the actors' performances, a dialog script was provided. The description of each emotional condition incorporated emotional labels derived from the prototype model of emotion, ensuring that the actors understood the intended emotion. Additionally, a vignette depicting a scenario related to each intensity level of the emotion was given. Actors were given sufficient time to prepare their emotional state using their preferred induction technique. In the singing condition, actors were instructed to follow the basic notated pitches while having the freedom to vary acoustic characteristics to convey the desired emotion.\n\nDuring the recording process, actors had the flexibility to repeat a trial until they felt comfortable with their production. The performances of the actors were observed in a separate control room through video and audio feeds, allowing for real-time monitoring. Feedback was provided if a production was deemed ambiguous by both operators. However, actors were not given specific instructions on how to express each emotion, allowing them to exhibit their own interpretation. Multiple takes were recorded for each production, and subsequently, all takes were reviewed by the three investigators. Clips containing hand movements or gestures were excluded, as well as trials that contained lexical errors. After removing erroneous clips, the selection criteria for inclusion in the dataset were productions that clearly conveyed the specified emotion and intensity through both facial expressions and voice. Through consensus among the investigators, the two best takes were chosen for each production.\n\nFurthermore, the RAVDESS dataset offers additional attributes, such as gender, age, and native language, which provide opportunities to explore potential correlations between these factors and emotional expressions in speech. This rich metadata allows researchers to conduct more comprehensive investigations and gain insights into the interplay between emotions and various demographic characteristics.\n\nThe group of twenty-four professional actors selected for this study was specifically recruited for the purpose of creating stimuli. These actors, residing in Toronto, Ontario, Canada, had a mean age of 26.0 years with a standard deviation of 3.75, ranging from 21 to 33 years. The group consisted of an equal distribution of 12 male and 12 female actors. Among them, 20 identified themselves as Caucasian, 2 as East-Asian, and 2 as Mixed, with one being East-Asian\n\nCaucasian and the other Black-Canadian First Nations Caucasian. Eligible actors were required to have English as their first language, speak with a neutral North American accent, and not possess any distinctive physical features like beards, facial tattoos, hair colorings, or facial piercings. Additionally, participants underwent testing to assess their ability to identify text presented at a distance of 1.5 meters without wearing glasses.\n\nThe selection of professional actors for this study was based on several factors. Previous research has demonstrated that actors are more easily recognized for their portrayals of emotions compared to lay individuals  (Elfenbein et al., 2002) . While a recent study found minimal differences in vocal expression accuracy between actors and non-actors  (Anikin et al., 2019) , it remains uncertain whether the same holds true for facial expressions or dynamic audio-visual expressions. Moreover, the use of trained individuals is common in psychological tasks, such as music performance  (Juslin et al., 2004) . Actors are frequently recruited for studies involving emotional expression due to their extensive training in realistically conveying emotions  (Livingstone et al., 2018) .\n\nThe Toronto accent represents Standard North American English commonly encountered in Hollywood movies. However, it's important to note that the linguistic phenomenon called Canadian raising, where diphthongs are raised before voiceless consonants, is not prominent in the Toronto region and is absent in the stimulus statements of the RAVDESS dataset. Canadian raising can be found in various parts of Canada, northeastern New England, the Pacific Northwest, and the Upper Midwest.\n\nThe RAVDESS dataset offers high-quality audio recordings that ensure reliable and consistent data for training and evaluating SER models. It provides a balanced distribution of emotions across different actors, allowing for a fair representation of various emotional states in the collected samples.\n\nTo maintain the integrity and quality of the dataset, the RAVDESS recordings underwent rigorous acoustic analysis and validation by domain experts. This meticulous curation process guarantees the reliability and suitability of the data for SER research purposes.\n\nThe utilization of the RAVDESS dataset in this research not only serves as a standardized benchmark for evaluating the proposed SER methodologies but also enables comparison and benchmarking against other state-of-the-art approaches in the field. The availability of such a well-structured and diverse dataset contributes to the advancement of SER research and facilitates the development of more accurate and robust emotion recognition systems.\n\nI have incorporated my AI model into a voice emotion recognition application that analyzes and detects the emotions present in a user's voice. The application utilizes a set of predefined emotion labels, including sadness, happiness, anger, surprise, neutral, and others. By leveraging advanced algorithms and machine learning techniques, the app can accurately identify and classify the emotional state conveyed through the user's voice.\n\nThrough the integration of my AI model, the voice emotion recognition app provides users with valuable insights into their emotional expressions, allowing them to better understand and manage their emotions. By simply speaking into the app, users can receive real-time feedback on their emotional state, gaining awareness of their feelings and potentially facilitating selfreflection and emotional well-being.\n\nThe app's functionality extends beyond mere emotion detection. It offers a user-friendly interface that presents the detected emotions in an intuitive and visually appealing manner, enhancing the overall user experience. Additionally, the app may provide additional features such as historical emotion tracking, allowing users to monitor their emotional patterns over time and identify any potential trends or correlations.\n\nBy incorporating my AI model into this voice emotion recognition app, I aim to contribute to the field of emotion recognition technology and its practical applications. The ability to accurately detect emotions from voice recordings opens up opportunities for numerous domains, including mental health, virtual assistants, customer service, and entertainment. This integration represents a step forward in the development of intelligent systems that can understand and respond to human emotions, ultimately enhancing human-computer interaction and improving overall user satisfaction.",
      "page_start": 32,
      "page_end": 32
    },
    {
      "section_name": "Ui/Ux",
      "text": "In this section of the thesis, the user interface (UI) and user experience (UX) of the application are showcased. The app's design process involved the utilization of a powerful design tool known as Figma.\n\nFigma played a pivotal role in crafting an aesthetically pleasing and intuitive user interface for the application. As a collaborative design platform, Figma provided a comprehensive set of features and functionalities that facilitated the creation of a visually appealing and interactive app layout.\n\nBy leveraging Figma's capabilities, the app's UI was meticulously designed, taking into consideration key principles of user-centered design and usability. The goal was to create an interface that would not only capture the attention of users but also ensure a seamless and engaging user experience.\n\nThe Figma tool enabled the development of wireframes, prototypes, and mockups, allowing for iterative design improvements and feedback incorporation. Its collaborative nature allowed multiple team members to work concurrently on the app's UI, fostering efficient communication and coordination among designers, developers, and other stakeholders.\n\nThrough Figma's rich library of design elements, icons, and components, the app's visual elements were carefully selected and customized to align with the overall app concept and brand identity. The tool's versatility enabled the creation of a consistent and visually appealing UI across different screens and functionalities of the application.\n\nFurthermore, Figma's interactive prototyping capabilities provided a means to simulate user interactions and transitions within the app, enabling designers to test and refine the UX design.\n\nThis iterative process allowed for the identification and resolution of potential usability issues, ensuring that the final app design offered a smooth and intuitive user experience.\n\nOverall, the utilization of Figma as the design tool for the app's UI/UX exemplifies the commitment to delivering a well-crafted and user-centric application. By harnessing Figma's robust features, the design team was able to translate their creative vision into a visually stunning and user-friendly interface, enhancing the overall usability and appeal of the app.  can focus on and benefit from within the app. By offering these personalized choice options, the app demonstrates its commitment to meeting the diverse needs and goals of its users. This approach acknowledges that individuals may have different priorities and areas they wish to work on, and provides them with tailored content and features accordingly. By presenting users with these choices, the app empowers them to take an active role in their well-being journey.\n\nUsers have the freedom to select the option that resonates with them the most, aligning with their specific goals and preferences. Furthermore, the inclusion of reminders for meditation emphasizes the app's focus on promoting mindfulness and self-care. These reminders serve as gentle prompts to encourage users to engage in regular meditation practices, which have been shown to have numerous mental and physical health benefits. This thoughtful design approach enhances user engagement and satisfaction by tailoring the app's content and features to individual preferences. By empowering users to select their areas of focus and providing reminders for meditation, the app aims to support users in their journey towards improved wellbeing and a more fulfilling life. and discover what works best for them. This approach enhances user engagement and satisfaction, as individuals can personalize their meditation practice and adapt it to their evolving needs. These diverse choices cater to different meditation techniques and objectives, allowing users to select the approach that aligns with their preferences and goals.\n\nIn this section of the thesis, the focus shifts towards testing and analyzing the performance of the developed system. A comprehensive evaluation of the system's capabilities and effectiveness will be discussed, providing valuable insights into its performance in detecting and recognizing emotions in voice recordings.\n\nThe performance evaluation aims to assess the accuracy, reliability, and overall effectiveness of the AI model integrated into the voice emotion recognition system. Rigorous testing procedures have been employed to ensure the validity and robustness of the results obtained.\n\nThe evaluation process involves subjecting the system to a diverse range of voice samples, encompassing different emotions, speaking styles, and environmental conditions. This extensive testing enables a comprehensive assessment of the system's ability to accurately identify and classify various emotional states, including but not limited to sadness, happiness, anger, surprise, and neutral expressions.\n\nVarious performance metrics and evaluation techniques have been employed to analyze the system's performance. These metrics provide quantitative measures of accuracy, precision, recall, and F1 score, among others, allowing for a thorough evaluation of the system's performance in emotion recognition.\n\nAdditionally, the evaluation process includes comparing the system's results with humanlabeled ground truth data to assess the system's agreement with human perception. This comparison helps gauge the system's performance in relation to human judgment and provides insights into its ability to capture and interpret emotional cues in the voice accurately.\n\nFurthermore, the evaluation includes testing the system's robustness and generalizability by assessing its performance on unseen data, including recordings from different individuals, diverse linguistic backgrounds, and varying recording conditions. This analysis ensures that the system can effectively handle real-world scenarios and demonstrates its adaptability to different user profiles.\n\nThe discussion of the results obtained from the performance evaluation will provide an in-depth analysis of the system's strengths, limitations, and areas for improvement. Not only will the thesis present quantitative performance metrics, but it will also offer qualitative insights into the system's performance, highlighting specific cases, challenges, and successes encountered during the testing phase.\n\nOverall, this section of the thesis serves as a comprehensive exploration of the testing and results of the voice emotion recognition system. Through meticulous evaluation procedures and a thorough analysis of the system's performance, valuable insights will be gained, enabling a deeper understanding of the system's capabilities and paving the way for future enhancements and advancements in the field of voice emotion recognition.",
      "page_start": 38,
      "page_end": 45
    },
    {
      "section_name": "Performance Evaluation",
      "text": "The performance evaluation section involves a detailed analysis of the developed system's effectiveness, which includes assessing various parameters to gauge its performance accurately.\n\nTo ensure a comprehensive evaluation, several key metrics and comparisons with other models have been employed.\n\nDuring the performance evaluation, critical parameters such as training and validation accuracies, loss, and epoch are utilized to measure the system's performance. These metrics provide valuable insights into the model's ability to learn and generalize from the training data and its performance on unseen validation data. By examining these parameters, the system's accuracy and efficiency can be assessed, allowing for a thorough evaluation of its overall performance.\n\nIn addition to evaluating internal metrics, the developed model is compared with other wellestablished models like LSTM (Long Short-Term Memory) and  DNN (Deep Neural Network) .\n\nThis comparison aims to identify the strengths and weaknesses of each model and determine which one performs the best in terms of accuracy and reliability. By benchmarking against existing models, a fair assessment of the system's performance can be obtained, highlighting its comparative advantages and contributions.\n\nThe comparison with LSTM and DNN models provides valuable insights into the effectiveness of the developed system in relation to alternative approaches. This analysis helps researchers and practitioners understand the unique characteristics and capabilities of the proposed model, enabling them to make informed decisions regarding its deployment in real-world scenarios.\n\nFurthermore, the performance evaluation involves assessing the system's performance across various datasets and scenarios to ensure its robustness and generalizability. By testing the system on diverse datasets, including those with different emotional expressions and speaking styles, its ability to handle various real-world scenarios can be evaluated. This analysis ensures that the developed model is not limited to specific datasets or conditions, but rather exhibits consistent and reliable performance across different contexts.\n\nThrough a comprehensive performance evaluation, including parameter analysis and model comparisons, the strengths, weaknesses, and overall effectiveness of the developed system can be thoroughly examined. The findings derived from this evaluation provide valuable insights for further improvements and advancements in the field of voice emotion recognition, ultimately leading to more accurate and reliable systems for detecting and interpreting emotions from voice data.   In addition to precision, recall, and F1 score, accuracy is also used as a performance metric.\n\nAccuracy indicates the overall correctness of the model's predictions, measuring the proportion of correct predictions out of the total predictions made.\n\nBy comparing these metrics across the different models, Table  1  allows for a comprehensive assessment of their performance in emotion recognition. This comparison enables researchers and practitioners to identify the strengths and weaknesses of each model, as well as to determine which model demonstrates the highest level of precision, recall, F1 score, and accuracy in predicting emotions accurately.\n\nUltimately, this comparison serves as a valuable reference for selecting the most appropriate model for emotion recognition tasks, based on the specific requirements and priorities of the application or research endeavor at hand. Specifically, the CNN model achieved a precision, recall, F1-score, and accuracy of 0.5444, indicating a balanced performance in terms of correctly classifying positive instances and overall accuracy.\n\nFor the LSTM model, the precision, recall, F1-score, and accuracy values are reported as 0.4944, demonstrating its ability to accurately identify positive predictions. The recall, F1score, and accuracy metrics provide further insights into the model's performance in terms of correctly identifying positive instances, the balance between precision and recall, and the overall correctness of predictions.\n\nAs for the DNN model, it achieved a precision, recall, F1-score, and accuracy of 0.5583, indicating a relatively higher level of accuracy in correctly labeling positive predictions compared to the other models.\n\nComparing these metrics across the different models allows for a comprehensive evaluation of their performance in the emotion recognition task. These quantitative measures offer valuable information for researchers and practitioners to assess the strengths and weaknesses of each model and make informed decisions regarding their applicability in specific scenarios or research endeavors.",
      "page_start": 45,
      "page_end": 46
    },
    {
      "section_name": "Result Summary",
      "text": "Among the three compared models, the DNN model emerges as the top performer in terms of overall performance. It exhibits the highest precision, recall, F1-score, and accuracy values, indicating its superior ability to accurately classify and predict emotions.\n\nOn the other hand, the CNN model's performance falls in between the other two models. While it demonstrates reasonable precision, recall, F1-score, and accuracy, it does not outperform the DNN model in terms of overall effectiveness.\n\nInterestingly, the precision, recall, F1-score, and accuracy metrics for all the models show a similar pattern. This observation suggests that the number of false positives (incorrectly predicted positive instances) is roughly equal to the number of false negatives (incorrectly predicted negative instances). This balance between false positives and false negatives can influence the precision, recall, F1-score, and accuracy metrics, resulting in comparable values across the models.\n\nThe comparable patterns in performance metrics across the models indicate the presence of certain similarities and consistencies in their prediction capabilities. Further analysis and investigation would be necessary to understand the specific factors contributing to these similar performance patterns and to identify any distinguishing characteristics among the models.\n\nOverall, the DNN model stands out as the most effective and accurate among the compared models, while the CNN model exhibits a moderate level of performance. The similarity in performance metrics suggests a balanced trade-off between false positives and false negatives within the models' predictions.\n\nIn As further research and development progress in this field, the potential for speech emotion recognition to revolutionize the digital healthcare landscape is vast, promising more patientcentric, accessible, and efficient care delivery.",
      "page_start": 49,
      "page_end": 50
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: CNN Architecture Used in the Model ....................................................................... 28",
      "page": 5
    },
    {
      "caption": "Figure 2: RAVDESS Dataset Sample ...................................................................................... 33",
      "page": 5
    },
    {
      "caption": "Figure 3: The Sign Up & Sign In Options ................................................................................ 40",
      "page": 5
    },
    {
      "caption": "Figure 4: User Choices ............................................................................................................. 41",
      "page": 5
    },
    {
      "caption": "Figure 5: Meditation Options ................................................................................................... 42",
      "page": 5
    },
    {
      "caption": "Figure 6: Performance Evaluation for CNN Model ................................................................. 46",
      "page": 5
    },
    {
      "caption": "Figure 1: CNN Architecture Used in the Model",
      "page": 28
    },
    {
      "caption": "Figure 2: RAVDESS Dataset Sample (Livingstone et al., 2018)",
      "page": 33
    },
    {
      "caption": "Figure 2: demonstrates a sample example of RAVDESS dataset. The speech audio files utilized",
      "page": 33
    },
    {
      "caption": "Figure 3: The Sign Up & Sign In Options",
      "page": 40
    },
    {
      "caption": "Figure 3: illustrates the login and registration features within the application, providing users",
      "page": 40
    },
    {
      "caption": "Figure 4: User Choices",
      "page": 41
    },
    {
      "caption": "Figure 4: provides a visual representation of the app's welcome screen, presenting users with",
      "page": 41
    },
    {
      "caption": "Figure 5: Meditation Options",
      "page": 42
    },
    {
      "caption": "Figure 5: illustrates a diverse range of meditation options available within the app, catering to",
      "page": 42
    },
    {
      "caption": "Figure 6: Performance Evaluation for CNN Model",
      "page": 46
    },
    {
      "caption": "Figure 6: displays the performance evaluation results obtained for my CNN model. The",
      "page": 46
    },
    {
      "caption": "Figure 6: highlights its",
      "page": 47
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison Among Different Models",
      "data": [
        {
          "Model Name": "CNN",
          "Precision": "0.5444",
          "Recall": "0.5444",
          "F1-Score": "0.5444",
          "Accuracy": "0.5444"
        },
        {
          "Model Name": "LSTM",
          "Precision": "0.4944",
          "Recall": "0.4944",
          "F1-Score": "0.4944",
          "Accuracy": "0.4944"
        },
        {
          "Model Name": "DNN",
          "Precision": "0.5583",
          "Recall": "0.5583",
          "F1-Score": "0.5583",
          "Accuracy": "0.5583"
        }
      ],
      "page": 48
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition for depression diagnosis using deep neural network",
      "authors": [
        "N Cummins",
        "S Scherer",
        "J Krajewski",
        "S Schnieder"
      ],
      "year": "2020",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition with convolutional neural network and fine-grained temporal modeling for PTSD screening",
      "authors": [
        "Z Zhao",
        "X Li",
        "S Li",
        "B Yan"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Videoconferencing psychotherapy and depression: A systematic review",
      "authors": [
        "M Berryhill",
        "N Culmer",
        "N Williams",
        "A Halli-Tierney",
        "A Betancourt",
        "H Roberts",
        "M King"
      ],
      "year": "2019",
      "venue": "Telemedicine journal and e-health: the official journal of the American Telemedicine Association",
      "doi": "10.1089/tmj.2018.0058"
    },
    {
      "citation_id": "4",
      "title": "A randomized controlled trial of internet-delivered cognitive behaviour therapy and acceptance and commitment therapy for depression and anxiety disorders",
      "authors": [
        "D Richards",
        "T Richardson",
        "L Timulak",
        "J Mcelvaney",
        "P Gallagher"
      ],
      "year": "2018",
      "venue": "Journal of consulting and clinical psychology",
      "doi": "10.1037/ccp0000295"
    },
    {
      "citation_id": "5",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": "Affective computing"
    },
    {
      "citation_id": "6",
      "title": "Computational paralinguistics: emotion, affect and personality in speech and language processing",
      "authors": [
        "B Schuller",
        "A Batliner",
        "F Burkhardt"
      ],
      "year": "2013",
      "venue": "Computational paralinguistics: emotion, affect and personality in speech and language processing"
    },
    {
      "citation_id": "7",
      "title": "Imagenet: A largescale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "8",
      "title": "Opensmile: the Munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the international conference on multimedia"
    },
    {
      "citation_id": "9",
      "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "authors": [
        "G Hinton",
        "L Deng",
        "D Yu",
        "G Dahl",
        "A Mohamed",
        "N Jaitly",
        ". Kingsbury"
      ],
      "year": "2012",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "10",
      "title": "Do you see what I see? Sex differences in the discrimination of facial emotions during treatment with adjunctive deep-brain stimulation of the ventral striatum and the effects of depression severity",
      "authors": [
        "A Girardi",
        "A Kaczkurkin",
        "M Sala",
        "F Grist",
        "G Goodwin",
        "S Hollon"
      ],
      "year": "2018",
      "venue": "European Neuropsychopharmacology"
    },
    {
      "citation_id": "11",
      "title": "Vocal correlates of emotional states",
      "authors": [
        "K Scherer"
      ],
      "year": "1979",
      "venue": "Nonverbal Communication, Interaction, and Gesture"
    },
    {
      "citation_id": "12",
      "title": "Silent messages: Implicit communication of emotions and attitudes",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1971",
      "venue": "Silent messages: Implicit communication of emotions and attitudes"
    },
    {
      "citation_id": "13",
      "title": "Improved support vector machines for speech emotion classification",
      "authors": [
        "L Deng",
        "J Hansen"
      ],
      "year": "2003",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Deep learning-based approach for the recognition of emotion in speech",
      "authors": [
        "S Kim",
        "J Lee",
        "E Provost"
      ],
      "year": "2013",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition from speech using deep recurrent neural networks with time-frequency attention mechanism",
      "authors": [
        "L Zhao",
        "H Feng",
        "Y Xu",
        "B Xu"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proceedings of the 9th European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "17",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        ". Narayanan"
      ],
      "year": "2008",
      "venue": "Journal of Language Resources and Evaluation"
    },
    {
      "citation_id": "18",
      "title": "The Toronto emotional speech set (TESS): A large-scale corpus of expressive natural speech",
      "authors": [
        "B Dupuis",
        "M Friend"
      ],
      "year": "2009",
      "venue": "Proceedings of the 10th Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "19",
      "title": "Classification of emotional speech using Gaussian mixture models",
      "authors": [
        "Y Wang",
        "S Narayanan"
      ],
      "year": "2005",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "20",
      "title": "The relevance of feature type for the automatic classification of emotional user states: low level descriptors and functionals",
      "authors": [
        "A Batliner",
        "S Steidl",
        "B Schuller",
        "D Seppi",
        "T Vogt",
        "J Wagner",
        ". Eyben"
      ],
      "year": "2004",
      "venue": "Proceedings of the 9th European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "21",
      "title": "Speech emotion recognition using feature selection",
      "authors": [
        "H Salam",
        "P Shah",
        "N Rashid",
        "R Anwar"
      ],
      "year": "2016",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition using deep residual bidirectional long short-term memory networks",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev",
        "N Vasconcelos"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "The Toronto emotional speech set (TESS): A validated set of non-acted recordings of 245 statements spoken in a range of emotions from neutral to intense",
      "authors": [
        "M Dupuis",
        "M Allard",
        "P Ouellet"
      ],
      "year": "2019",
      "venue": "PloS one"
    },
    {
      "citation_id": "24",
      "title": "Towards speech-based automatic depression level detection in uncontrolled environments",
      "authors": [
        "D Girardi",
        "C Costa",
        "C Fernandes"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "26",
      "title": "Customer emotion analysis for call center improvement using speech emotion recognition",
      "authors": [
        "V Kumar",
        "F Sattar",
        "X Gao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "The INTERSPEECH 2011 Paralinguistic Challenge: Emotion, Speech, and Corpora",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi",
        "T Vogt",
        "J Wagner",
        ". Devillers"
      ],
      "year": "2011",
      "venue": "The INTERSPEECH 2011 Paralinguistic Challenge: Emotion, Speech, and Corpora"
    },
    {
      "citation_id": "28",
      "title": "On the acoustics of emotion in audio: What speech, music, and sound have in common",
      "authors": [
        "F Weninger",
        "F Eyben",
        "B Schuller",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2013",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "29",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        ". Wöllmer"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}