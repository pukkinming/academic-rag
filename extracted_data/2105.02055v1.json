{
  "paper_id": "2105.02055v1",
  "title": "Towards Interpretable And Transferable Speech Emotion Recognition: Latent Representation Based Analysis Of Features, Methods And Corpora",
  "published": "2021-05-05T13:47:39Z",
  "authors": [
    "Sneha Das",
    "Nicole Nadine Lønfeldt",
    "Anne Katrine Pagsberg",
    "Line H. Clemmensen"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Autoencoder",
    "Interpretability",
    "Knowledge transfer",
    "Transparency"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In recent years, speech emotion recognition (SER) has been used in wide ranging applications, from healthcare to the commercial sector. In addition to signal processing approaches, methods for SER now also use deep learning techniques. However, generalizing over languages, corpora and recording conditions is still an open challenge in the field. Furthermore, due to the black-box nature of deep learning algorithms, a newer challenge is the lack of interpretation and transparency in the models and the decision making process. This is critical when the SER systems are deployed in applications that influence human lives. In this work we address this gap by providing an in-depth analysis of the decision making process of the proposed SER system. Towards that end, we present low-complexity SER based on undercomplete-and denoising-autoencoders that achieve an average classification accuracy of over 55% for four-class emotion classification. Following this, we investigate the clustering of emotions in the latent space to understand the influence of the corpora on the model behavior and to obtain a physical interpretation of the latent embedding. Lastly, we explore the role of each input feature towards the performance of the SER. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) refers to a group of algorithms that deduce the emotional state of an individual from their speech utterances. SER combined with affect recognition, which uses other modalities like vision and physiological signals are deployed in a wide range of applications. For instance, in the detection and intervention of disorders in healthcare, monitoring the attentiveness of students in schools, risk assessment within the criminal justice system, and for commercial applications, like detecting customer satisfaction in call-centers and by employment agencies to find suitable candidates  [1, 2] .\n\nState-of-the-art SER techniques have evolved from the more conventional signal processing and machine learning based methods to deep neural network based solutions  [3] . Classical methods were based on hidden Markov models (HMM), Gaussian mixture models (GMM), support vector machines (SVM) and decision trees. Contributions based on HMM employed energy and pitch features  [4] , and logfrequency power coefficients from the spectrum  [5]  and showed high classification accuracy. Spectral, prosodic and energy features in tandem with a GMM and SVM were used to recog- 1 For reproducibility the code will be made available at this link by the time of publication: https://bit.ly/3fDWSbq",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Input-Layer",
      "text": "Output-layer Bottleneck nize emotions from Basque and Chinese datasets  [6, 7] , while pitch based input features were used to obtain a GMM and tested on more heterogeneous speech corpora  [8] . SVM is another popular method, either used as the primary classification tool, or in coordination with other techniques to predict the affect classes  [9] . Recently, more advanced methods that use deep learning have been used for SER. Long short-term memory (LSTM), bidirectional LSTM and recurrent neural networks (RNN) were used to predict the quadrant in the dimensional emotional model  [10] . Following this, denoising autoencoders (DAEs)were used to learn a lower dimensional latent representation for the emotions that were then employed at various levels to classify speech into emotional categories  [11] [12] [13] . Convolution neural networks, having had immense success in computer vision, are a common architecture choice for neural network based SERs.\n\nDespite the long history of research contributions in the domain, state-of-the-art methods often struggle to generalize, across corpora with different languages, recording conditions, cultures. Shortage of annotated data in lesser-spoken languages further amplifies issue. Furthermore, many deep learning based SERs are highly complex black-boxes whereby the models are not interpretable and they lack the transparency in their decision making process. This is a crucial aspect, especially when the technology is deployed in applications with consequences on people's lives and access to resources  [14] .\n\nUnsupervised learning is a promising approach to address label shortage in developing SERs. In addition, autoencoders (AEs), an unsupervised learning technique, and latent representation studies are useful tools in making models more interpretable. This can also lead to better knowledge transfer between data sets. However, despite the use of AEs for SER in existing literature, few methods provide insights beyond classi- fication accuracy. In this work: 1. we present a low-complexity undercomplete AE (UAE) and DAE for SER that achieves a performance similar to existing methods, 2. we show that the clusters in the latent space implicitly model the activation variable from the dimensional emotional model, 3. we study the robustness of the methods by investigating the differences that occur in the latent representations when the underlying data conditions are modified. In other words, how the differences in the language of the corpus impacts the latent space, hence the performance. 4. We introduce interpretation in the system by analysing the feature attributions towards emotion clustering. The DeepLIFT algorithm  [15]  is used to gain insights into the feature subsets that contribute most towards the discrimination of the emotion classes. Related Work: Unsupervised learning techniques for SER have received a lot of interest recently, and AEs are commonly used methods for unsupervised learning. DAE was one of the earliest deep learning based unsupervised learning techniques for SER  [12] . This was followed by the use of sparse AE for feature transfer  [11]  and for SER on spontaneous data set  [16] . Furthermore, end-to-end representation learning for affect recognition from speech was proposed and showed performance comparable to existing methods  [17, 18] . In recent years, techniques like variational and adversarial AEs and adversarial variational Bayes have been exploited to learn the latent representations of speech emotions with input features ranging from the raw signals to hand crafted features  [19] [20] [21] [22] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods And Experimental Setup",
      "text": "In this section, we provide an overview of the features, the data sets, the architecture of the proposed algorithms and briefly discuss the reference algorithms that are employed in this work. Data sets: We use the IEMOCAP data set, an audio-visual affect data set, to train and validate the models  [23] . The dataset comprises of annotations representing both the categorical and dimensional emotional model  [24] . To study how the latent representations are transferred between corpora, we use 1. the Surrey Audio-Visual Expresses Emotion (SAVEE) database that is primarily English and consists of male speakers only, 2. the Berlin Database of Emotional Speech (Emo-DB) recorded in German and, 3. the Canadian French Emotional (CaFE) speech database comprising of French audio samples  [25] [26] [27] . In this work, we utilize the audio modality only, and constrain the emotional categories to neutral (N), sad (S), happy (H), angry (A) as these are the emotions that are common over the data sets.\n\nTable  1 : Grouped features of the eGEMAPS  [28] . Features: We use the extended Geneva minimalistic acoustic parameter set (eGeMAPS) in this work  [28] . Since our objective is to explore the features relevant to the different emotional categories, we used the functionals of lower-level features because summary statistics provide relatively lower variability within a sample. Additionally, while raw speech features may provide higher freedom for the AE to discover representations  [17, 18] , in such a system it is inherently difficult to understand the association of individual paralinguistic markers to the emotions. Furthermore, emotional annotations in few datasets are provided sample-wise and not segment-wise. Each speech sample yields a feature vector comprising of 88 features and Tab. 1 illustrates a compressed version of the feature set. We use the OpenSmile toolkit to extract the features  [29] .\n\nAlgorithms and Evaluation Metric: In this paper, an UAE, that is an AE with the latent dimension much smaller relative to the input feature vector (z << xinput), and a DAE are presented for SER  [30] . While AEs have been employed for SER in existing literature, their main focus was to propose network architectures that provide better classification accuracy  [12, 16, 19, 20, 22] . Since our goal is to explore the crosscorpus transferability of the latent representations and gain insights into the feature markers for the emotional classes, we adhere to the simplest architecture that provides comparable results to previous works. For a fair comparison between the performance of the UAE and the DAE for SER, we design both systems with identical architectures. The input feature dimension is 88 and we maintain the latent dimension size at z ∈ R 2×1 . In relation to past works, the primary source of low-complexity in our proposed architecture is from the latent dimension size. The network architecture for the UAE is illustrated in Fig.  1   Preprocessing: Prior to using the data sets for training and testing, we remove the outliers by computing the z-score and eliminating the data samples that have a z-score, -10 > z > 10.\n\nWe chose a threshold of 10 instead of the standard value of 3 because the goal of this work is to understand the behavior of the models for both typical and atypical rendition of emotions in speech. Therefore, we only remove the extreme outliers. Following this, the data sets are standardized to obtain a normal feature distribution.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "We train and validate the models using 10-fold cross-validation on the IEMOCAP database while the transfer data sets are identical over the iterations. The models were trained over 50 epochs and a batch size of 64, and we used the Adam optimizer with the learning rate set to 1e-3. In the following parts, we evaluate the latent embedding by using them as the input features to classify the speech samples into emotional categories using the SVC. We first present the overall classification accuracy to understand how our presented systems compare to previously proposed methods. Following that, the separability of the emotional categories is investigated and we delve into the physical interpretation of the latent space. At this stage we also investigate how the latent embeddings transfer to unseen corpora. Lastly, we study the input features that pose as the main markers of the emotional categories and demonstrate why certain emotion categories are classified better. Classification Performance: From the classification accuracy presented in Fig.  2 , we observe that for the training and validation data sets, the performance of UAE and DAE is much lower than the accuracy of the reference SVC without any dimensionality reduction. PCA and UAE have similar performances, and the results from the DAE are the lowest on these data sets. The classification accuracy reduces over the transfer data sets, specifically for the German and French data sets.\n\nHowever, the advantage of transfer learning for the UAE and the DAE over the reference methods are visible, as they show the largest classification accuracy in these cases. While the presented architecture in this work is relatively simple (hidden layers and units, latent dimension, input feature size), we obtain an accuracy that is close to the results presented in existing works  [12, 16, 19, 20, 22] .\n\nAnalysing Emotions in Triads: The results above provide a generic understanding of the performance of the methods under investigation. To gain an insight on the separability of the latent embedding between the four emotions, we process the emotion categories in triads. In other words, we train the UAE and the DAE with samples from three emotional categories at a time. Since an AE operates by minimizing the MSE between the true and reconstructed samples, it is possible that features from an arbitrary emotional category dominates the magnitude of the loss. Therefore, the triad approach also aids in studying if a certain set of features or emotion dominates the training of the AEs. The resulting classification accuracy and the confusion matrices of the DAE, for a randomly selected fold in the crossvalidation, are presented in Fig.  3 . We observe that the best results are obtained for the N-S-A triad and the worst results are obtained for the S-H-A triad. Comparing the results over the data sets, results are similar for the English data sets, but the accuracy reduces for German and French. In addition, it can be observed that the emotion happy (H) is generally misclassified in all combinations. However, anger (A) is consistently classified with high accuracy over all combinations and data sets. Also, unlike for N-S-H, sad (S) is classified accurately for S-H-A over the English data sets. Following the above observations, we extend the analysis to understand why some emotions are more separable than others and the influence of the differences in corpora on the latent embedding. We do that by visually inspecting the latent representation of the DAE and building a physical interpretation of the latent space by employing the valence and activation la-Figure  4 : (a)-(f) Scatter plots and marginal distributions of the latent variables from the denoising autoencoder for train, validation and transfer data sets. (g)-(j) Feature attribution scores using deepLIFT  [15]  for the latent dimensions over the valid and transfer data sets.\n\nbels available in the IEMOCAP data set as the ideal reference for clustering emotions. This is illustrated in Fig.  4  (a) using a scatter plot of the valence and activation values in the training data set, color-coded with the emotion labels. Scatter plots in Fig.  4 (b-f ) represent the distribution of the latent embedding for the train, validation and transfer data sets. From Fig.  4 (b-f ), we observe that the samples representing anger form a separate cluster over all the data sets, whereas samples representing neutral and sad overlap and form a joint cluster in the latent space. In contrast the happy samples are spread over both of these clusters. Furthermore, the marginal distribution of the classes for the activation variable in Fig.  4  (a) resembles the marginal distribution of Dim 1 shown in Fig.  4 (b-f ). We therefore conclude that the DAE learns to cluster the samples in terms of their activation. Additionally, while the latent clusters of the training, validation and transfer data sets are similar, the orientation of the distributions are rotated for the German and French data sets relative to the English data sets. This explains the observed reduction in the classification accuracy over the German and French data sets in Figs.  2  and 3 .\n\nFeature Attribution for Emotion Clustering: In the final part of the analysis, we study the features that pose as the main markers for each emotional category. To that end, we utilize the DeepLIFT (Deep Learning Important FeaTures) algorithm that computes the influence of the input features on a specific neuron through the difference in the output, relative to the reference output when there is a difference in input between the reference and the considered input sample  [15] . We computed the mean input feature vector over the true positives from the neutral samples of the validation set and employed that as the reference. The resulting distributions of feature attribution scores for the latent dimensions are demonstrated as violin-plots in Fig.  4 (gj ) over the true positives from the validation set. Note that we have grouped similar features for effective visualization.\n\nThe attribution scores of the neutral class is similar over all features, as shown in Fig.  4  (g). This is expected since our reference is the neutral class. The classification accuracy and the confusion matrices have indicated that the clustering of anger is consistently accurate. This is also reflected in Fig.  4 (i)  where we observe that the features with high attribution scores are unique to this emotion class, specifically features related to loudness, spectral flux and spectral slope. Although we observe that the formants (F1, F2, F3), jitter, shimmer and harmonic-tonoise-ratio (HNR) have a relatively high feature attribution and are unique to sad, classes sad and happy seem to generally share features with high attribution scores as shown in Fig.  4(h, j ). Furthermore, the observation from the classification accuracy indicates that samples representative of the emotion class happy are particularly difficult to cluster and classify. This can be explained from the feature attribution scores in  Fig 4(j)  that are very similar to the scores of the neutral class. In other words, the proposed DAE architecture is unable to distinguish between features from neutral and happy classes. From this analysis we find the specific features that are directing the SER decisions in the proposed DAE and this understanding can be employed when the model is used on newer and unseen corpora, for instance on a cross-cultural emotion data set, to interpret the model and introduce transparency into the decision process.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we have proposed a low-complexity UAE and DAE that achieves accuracy rates similar to existing methods, at a relatively lower computation cost due to a highly compressed latent space. The goal of this work was to make the system interpretable and study the transferabilty of the latent representation, whereby we performed an in-depth analysis of the latent representations and their physical interpretation. We learned that the model implicitly learns to cluster the emotion classes according to their activation levels. Additionally, we observe that the orientation of the latent distributions for the German and French data sets is different to that of the English data sets. Following this, we conducted a study on the main feature markers for each emotion class. We discovered that the highest feature attribution scores are obtained for the class anger and are unique for this class, whereas the feature attributions for happy are very similar to the reference neutral category.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the autoencoder architecture.",
      "page": 1
    },
    {
      "caption": "Figure 2: Mean and 95% conﬁdence intervals of the 4-category",
      "page": 2
    },
    {
      "caption": "Figure 3: Mean and 95% conﬁdence intervals of the classiﬁcation accuracy from 10-fold cross-validation over train, valid and transfer",
      "page": 3
    },
    {
      "caption": "Figure 2: , we observe that for the training and val-",
      "page": 3
    },
    {
      "caption": "Figure 3: We observe that the best",
      "page": 3
    },
    {
      "caption": "Figure 4: (a)-(f) Scatter plots and marginal distributions of the latent variables from the denoising autoencoder for train, validation",
      "page": 4
    },
    {
      "caption": "Figure 4: (a) using a",
      "page": 4
    },
    {
      "caption": "Figure 4: (b-f) represent the distribution of the latent embedding",
      "page": 4
    },
    {
      "caption": "Figure 4: (a) resembles the marginal dis-",
      "page": 4
    },
    {
      "caption": "Figure 4: (b-f). We therefore conclude",
      "page": 4
    },
    {
      "caption": "Figure 4: (j) that are",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Pitch": "Segments/second",
          "Loudness": "Jitter",
          "Spectral ﬂux (U, UV)": "Shimmer"
        },
        {
          "Pitch": "Segment length (UV)",
          "Loudness": "Harmonics",
          "Spectral ﬂux (U, UV)": "Formant 1 (F1)"
        },
        {
          "Pitch": "Formant 2 (F2)",
          "Loudness": "Formant 3 (F3)",
          "Spectral ﬂux (U, UV)": "Alpha ratio (V, UV)"
        },
        {
          "Pitch": "Hammerberg\nIndex (V, UV)",
          "Loudness": "Spectral slope 1\n(0-0.5kHz)-(V, UV)",
          "Spectral ﬂux (U, UV)": "Spectral slope 2\n(0.5-1.5kHz)-(V, UV)"
        },
        {
          "Pitch": "Loudness\npeaks/second",
          "Loudness": "Mel-frequency cepstrum\nCoefﬁcient (MFCC)",
          "Spectral ﬂux (U, UV)": "Segment length(UV)"
        },
        {
          "Pitch": "Equivalent\nsound level",
          "Loudness": "Harmonic-to-\nnoise-ratio (HNR)",
          "Spectral ﬂux (U, UV)": ""
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Effect of Wearable Digital Intervention for Improving Socialization in Children With Autism Spectrum Disorder: A Randomized Clinical Trial",
      "authors": [
        "C Voss",
        "J Schwartz",
        "J Daniels",
        "A Kline",
        "N Haber",
        "P Washington",
        "Q Tariq",
        "T Robinson",
        "M Desai",
        "J Phillips",
        "C Feinstein",
        "T Winograd",
        "D Wall"
      ],
      "year": "2019",
      "venue": "JAMA Pediatrics",
      "doi": "10.1001/jamapediatrics.2019.0285"
    },
    {
      "citation_id": "3",
      "title": "New kairos facial recognition camera offers customer insights",
      "authors": [
        "P Luana"
      ],
      "year": "2019",
      "venue": "New kairos facial recognition camera offers customer insights"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "A Nogueiras",
        "A Moreno",
        "A Bonafonte",
        "J Mariño"
      ],
      "year": "2001",
      "venue": "Seventh European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "6",
      "title": "Detection of stress and emotion in speech using traditional and fft based log energy features",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Fourth International Conference on Information, Communications and Signal Processing, 2003 and the Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint"
    },
    {
      "citation_id": "7",
      "title": "Automatic emotion recognition using prosodic parameters",
      "authors": [
        "I Luengo",
        "E Navas",
        "I Hernáez",
        "J Sánchez"
      ],
      "year": "2005",
      "venue": "Ninth European conference on speech communication and technology"
    },
    {
      "citation_id": "8",
      "title": "Gmm supervector based svm with spectral features for speech emotion recognition",
      "authors": [
        "H Hu",
        "M Xu",
        "W Wu"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing -ICASSP '07"
    },
    {
      "citation_id": "9",
      "title": "Analysis of emotionally salient aspects of fundamental frequency for emotion detection",
      "authors": [
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "The elements of statistical learning",
      "authors": [
        "J Friedman",
        "T Hastie",
        "R Tibshirani"
      ],
      "year": "2001",
      "venue": "Springer series in statistics New York"
    },
    {
      "citation_id": "11",
      "title": "Combining long short-term memory and dynamic bayesian networks for incremental emotion-sensitive artificial listening",
      "authors": [
        "M Wöllmer",
        "B Schuller",
        "F Eyben",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Sparse autoencoder-based feature transfer learning for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "2013 humaine association conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "13",
      "title": "Using denoising autoencoder for emotion recognition",
      "authors": [
        "R Xia",
        "Y Liu"
      ],
      "year": "2013",
      "venue": "Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Autoencoder-based unsupervised domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "15",
      "title": "AI Now",
      "authors": [
        "K Crawford",
        "R Dobbe",
        "T Dryer",
        "G Fried",
        "B Green",
        "E Kaziunas",
        "A Kak",
        "V Mathur",
        "E Mcelroy",
        "A Sánchez",
        "D Raji",
        "J Rankin",
        "R Richardson",
        "J Schultz",
        "S West",
        "M Whittaker"
      ],
      "year": "2019",
      "venue": "AI Now"
    },
    {
      "citation_id": "16",
      "title": "Learning important features through propagating activation differences",
      "authors": [
        "A Shrikumar",
        "P Greenside",
        "A Kundaje"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition 'in the wild'using an autoencoder",
      "authors": [
        "V Dissanayake",
        "H Zhang",
        "M Billinghurst",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "18",
      "title": "Representation learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "E Laksana",
        "L.-P Morency",
        "S Scherer"
      ],
      "year": "2016",
      "venue": "Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Learning representations of affect from speech",
      "year": "2015",
      "venue": "Learning representations of affect from speech",
      "arxiv": "arXiv:1511.04747"
    },
    {
      "citation_id": "20",
      "title": "Variational autoencoders for learning latent representations of speech emotion: a preliminary study",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Interspeech 2018: Proceedings"
    },
    {
      "citation_id": "21",
      "title": "Improving emotion classification through variational inference of latent variables",
      "authors": [
        "S Parthasarathy",
        "V Rozgic",
        "M Sun",
        "C Wang"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Unsupervised learning approach to feature analysis for automatic speech emotion recognition",
      "authors": [
        "S Eskimez",
        "Z Duan",
        "W Heinzelman"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "25",
      "title": "Pleasure, arousal, dominance: Mehrabian and russell revisited",
      "authors": [
        "I Bakker",
        "T Van Der",
        "P Voordt",
        "J Vink",
        "De Boon"
      ],
      "year": "2014",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "26",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "27",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "28",
      "title": "A canadian french emotional speech dataset",
      "authors": [
        "P Gournay",
        "O Lahaie",
        "R Lefebvre"
      ],
      "year": "2018",
      "venue": "Proceedings of the 9th ACM Multimedia Systems Conference"
    },
    {
      "citation_id": "29",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "30",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "",
      "authors": [
        "I Goodfellow",
        "Y Bengio",
        "A Courville",
        "Deep Learning"
      ],
      "year": "2016",
      "venue": ""
    }
  ]
}