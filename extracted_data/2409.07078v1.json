{
  "paper_id": "2409.07078v1",
  "title": "Multimodal Emotion Recognition With Vision-Language Prompting And Modality Dropout",
  "published": "2024-09-11T08:06:47Z",
  "authors": [
    "Anbin QI",
    "Zhongliang Liu",
    "Xinyong Zhou",
    "Jinba Xiao",
    "Fengrun Zhang",
    "Qi Gan",
    "Ming Tao",
    "Gaozheng Zhang",
    "Lu Zhang"
  ],
  "keywords": [
    "MER 2024",
    "multimodal emotion recognition",
    "fine-tuning CLIP",
    "modality dropout",
    "semi-supervised learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we present our solution for the Second Multimodal Emotion Recognition Challenge Track 1 (MER2024-SEMI). To enhance the accuracy and generalization performance of emotion recognition, we propose several methods for Multimodal Emotion Recognition. Firstly, we introduce EmoVCLIP, a model fine-tuned based on CLIP using vision-language prompt learning, designed for video-based emotion recognition tasks. By leveraging prompt learning on CLIP, EmoVCLIP improves the performance of pretrained CLIP on emotional videos. Additionally, to address the issue of modality dependence in multimodal fusion, we employ modality dropout for robust information fusion. Furthermore, to aid Baichuan in better extracting emotional information, we suggest using GPT-4 as the prompt for Baichuan. Lastly, we utilize a self-training strategy to leverage unlabeled videos. In this process, we use unlabeled videos with high-confidence pseudo-labels generated by our model and incorporate them into the training set. Experimental results demonstrate that our model ranks 1st in the MER2024-SEMI track, achieving an accuracy of 90.15% on the test set. \n CCS CONCEPTS ‚Ä¢ Computing methodologies ‚Üí Computer vision; Natural language processing; ‚Ä¢ Human-centered computing ‚Üí HCI theory, concepts and models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As an important part of human-computer interactions (HCI), Multimodal Emotion Recognition (MER) has garnered increasing interest. Incorporating an understanding of human emotional states into HCI is critical not only elevates the quality of the interaction experience but also fosters a deeper connection between users and technology. There are many technologies for studying human emotions, including speech, image, text, video, and other modalities.\n\nAlthough multimodal can boost emotion recognition, collecting annotation data with high quality is challenging. The performance of the model degrades easily when only limited data is feasible. To address the above issue, methods that leverage pre-trained models predominate in current research. In Natural Language Processing (NLP) tasks, RoBERTa  [13]  outperforms BERT  [6]  on several benchmarks via a larger training dataset, longer training duration, and dynamically masking strategy. XLNet  [22]  incorporates the mechanisms of Transformer-XL  [5]  and integrates bidirectional context conditioning. ELECTRA  [4]  replaces the widely used mask prediction task with a new pre-training task called \"Replaced Token Detection\". By now, Baichuan  [21]  has achieved outperforming results on several benchmarks owing to the utilization of extensive high-quality data and effective training methodologies, especially in the Chinese language. GPT4  [1]  has better language understanding due to more languages and a wider training set. Therefore, we attempt to combine GPT4 and Baichuan to enhance emotional representation in language.\n\nIn Speech Signal Processing (SSP) tasks, Wav2Vec2.0  [2]  uses contrastive learning to learn robust audio representations from large-scale speech data pre-training. HuBERT  [7]  further improves the robustness of features through the pre-training method based on mask prediction. Emotion2Vec  [15]  significantly boosts the performance of speech emotion recognition. In MER2024  [11]  and MERBENCH  [12] , Hubert has been found to have excellent generalization performance in emotion recognition.In our preliminary experiments, we found that emotion2vec can improve the emotion recognition rate on the training set, but its generalization performance is lacking on the test set.\n\nIn Vision-Language (VL) tasks, Contrastive Language-Image Pre-Training (CLIP)  [17]  introduces contrastive learning to pretraining on large-scale image-text pairs and achieves cross-modal understanding and generation. However, we found that using CLIP to extract facial features from open-face videos results in not only inference training bias but also loss of temporal information of the video, which can impair emotion recognition ability. Inspired by  [18] , we propose using prompt learning to learn video time series correlations while preserving CLIP's own generalization ability.\n\nIn addition, there are many ways to fuse the extracted features after obtaining them  [19] [23]  [14] . However, fusing modalities can combine different information between different modalities and help with better understanding,  [9]  found that there is competition between modalities. We found modality dropout can relieve this phenomenon. On the other hand, in order to fully utilize unlabeled data, the self-training strategy is used  [3] . And we used the same method as them.\n\nIn summary, our contributions are as follows:\n\n‚Ä¢ We propose EmoVCLIP, a video understanding model on emotion recognition. Via fine-tuning on labeled video-text pairs with Vision-Language prompting learning, EmoVCLIP is capable of extracting video features with more emotional information.\n\n‚Ä¢ We propose modality dropout to enhance the robustness of multimodal fusion. This method effectively solves the problem of insufficient fusion caused by modal competition and modal dependence. We adopt a self-training method based on pseudo labels to leverage information from unlabeled data. ‚Ä¢ We propose to use a large language model to enhance the emotion of the text to extract text features with richer emotional information, which we name GPT4-Baichuan.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "Semi-Supervised Learning with Self-training: Self-training is a simple semi-supervised method. The main idea is to augment the labeled training dataset with unlabeled data  [3] [24]  [20] . Selftraining-based semi-supervised learning methods have demonstrated promising performance in MER2023 challenge. Both Unlabeled data with generated pseudo-labels and labeled data are fed into the model during training and models can benefit from increasing the amount of training data  [3] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modality Competition In Multimodal Fusion:",
      "text": "The superiority of multimodal over single-modal lies in its capability to integrate and exploit complementary information from multiple modalities, thereby enriching the representation for downstream tasks. However, due to modality competition  [9] , only a subset of modalities effectively learn and contribute to the final model, while others are underutilized within a multimodal network during training. In MER2023, participant  [26]  improves recognition performance by directly discarding the text modality.  [16]  proposes online gradient modulation to solve the problem of modality competition.  [8]  proposes modality dropout to improve zero-shot transfer capabilities between audio and video modalities.\n\nCLIP for video understanding tasks: In video-based tasks, videos are often treated as a temporal sequence of image frames. Methods in video processing typically leverage image models while incorporating temporal information to capture dynamic changes and contextual relationships. This integration allows video analysis to benefit from advancements in image processing. Specifically for CLIP-related models  [18] , video-based CLIP typically uses additional learnable components for temporal-spatial modeling, such as self-attention layers, additional prompts, and a dedicated video decoder. Among the mentioned components, additional prompts  [10, 25]  are widely adapted due to their efficiency when transferring to downstream tasks without re-training the model's parameters.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method 3.1 Model Architecture",
      "text": "The overall architecture of the proposed model is illustrated in Figure . 1, which consists of multiple single-modal feature extractors and a multimodal feature fusion network. In brief, the input ùë• ‚àà {ùë• ùëÜ , ùë• ùêº , ùë• ùëá , ùë• ùëâ } are fed into the corresponding single-modal feature extractor to obtain high-dimensional feature ùëì ‚àà {ùëì ùëÜ , ùëì ùêº , ùëì ùëá , ùëì ùëâ }. The meanings represented by the subscripts are as follows: ùëÜ for speech, ùêº for image, ùëá for text, and ùëâ for video. For video modality, we propose EmoVCLIP with a strong capacity for emotion extraction. For image modality, we use CLIP  [17]  to extract image features by slicing the video into frames of images. For speech modality, we extract speech features by feeding the raw speech signals into Hubert  [7] , which is widely adapted for speech-based tasks. For text modality, we propose to use Baichuan with input text augmented by GPT4 to explicitly obtain textual features along with the corresponding emotion states. Then two dimensional inputs ùëì ‚àà R ùêø,ùê∂ are pooled into corresponding single-modal embeddings ùëí ‚àà {ùëí ùëÜ , ùëí ùêº , ùëí ùëá , ùëí ùëâ } via temporal average pooling. Finally, multimodal embeddings are concentrated in channel dimensional and fed into the fusion networks to output the prediction of emotion labels in neutral, anger, happiness, sadness, worry, and surprise. Next, we will provide a detailed introduction to our proposed methods.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emovclip",
      "text": "As is discussed in  [18] , although simple fine-tuning of CLIP exhibits competitive performance, it is not always feasible, particularly in downstream tasks with little available data. Research  [12]  has found that fine-tuning CLIP does not improve the performance on the MER2023 dataset but instead degrades the generalization performance. We attribute this performance degradation to the lack of labeled data and the inefficiency of the fine-tuning strategy. Inspired by Vision-Language prompt learning  [18] , we use prompt learning to effectively improve the generalization ability of the model on specific emotion datasets instead of fine-tuning CLIP.  As shown in Figure  2 , specifically, we keep the structure and parameters of CLIP frozen, but add N prefix learnable prompt tokens at each layer of the image encoder and text encoder. For each frame of the video, the image is input into CLIP, and finally, all frames are pooled in the time dimension to calculate similarity with the text embedding",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modality Dropout",
      "text": "In u-HuBERT  [8] , modality dropout is proposed to learn modalityagnostic representations by randomly dropping certain modalities during training, thereby improving the model's generalization capabilities across various modalities.\n\nIn this paper, we propose random modality dropout to promote the integration of features from different modalities and alleviate modality competition. Specifically, in our model, each modality has its own extracted embeddings before multimodal fusion: ùëí ùëÜ for speech, ùëí ùêº for image, ùëí ùëá for text, and ùëí ùëâ for video. Embeddings are concentrated and then fed into fusion networks ùëî to predict the emotion labels. This process can be dropped as Eq. 1.\n\nùëùùëüùëíùëë = ùëî(concat(ùëí ùëÜ , ùëí ùêº , ùëí ùëá , ùëí ùëâ ))\n\n(1)\n\nIn our method, each modality's embedding is replaced by zero vectors with a certain probability before concentration. The real multimodal fusion can be described as Eq. 2.\n\nùëùùëüùëíùëë = ùëî(concat(ùëí ùëÜ , ùëí ùêº , ùëí ùëá , ùëí ùëâ ))\n\nwhere ùëù 1 denotes the modality dropout rates.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Gpt4-Baichuan",
      "text": "Although Baichuan excels in Chinese-based tasks, we found that GPT4  [1]  surpasses Baichuan in emotion recognition. To enhance the emotional feature extraction capabilities of Baichuan, we have integrated GPT4's emotion extraction ability with Baichuan's Chinese language processing ability via text augment. Considering that it is impossible to obtain the feature embeddings by GPT4, we input the text and prompt words into GPT4, let GPT4 pay attention to the emotional information in the text, and sort by the possibility of six emotion labels according to the text. Finally, the output of GPT4 is spliced with the text and input into Baichuan to obtain the text information and the corresponding emotional state more clearly.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Self-Training",
      "text": "Following  [3] , we adopt self-training as it exhibits promising capabilities in semi-supervised learning by effectively leveraging unlabeled data. Initially, we train a foundational multimodal emotion recognition model using labeled data. Then, we engage in an iterative self-training strategy which involves expanding the labeled dataset with pseudo-labeled data, ending with the retraining of the model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "MER2024-SEMI provides two sets, Train & Val with labels and Test without labels, containing 5030 and 115595 videos respectively. The final test set contains 1169 videos, including 115595 unlabeled videos. Since Train & val is not divided into train set and validation set, similar to the baseline system, we choose to perform five-fold cross-validation, and then weigh the best results on the five validation sets equally to obtain the final result. The evaluation indicator is the weighted average F-score(WAF) defined by the challenge organizers.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "We used four feature extractors, Chinese-Hubert-large 1  , CLIP 2  , EmoVCLIP, and GPT4-Baichuan. The 24k speech signals extracted from the video are fed into the Chinese-Hubert-large and the output of the last four layers are added as the frame-level features of the speech modality. We used the open-face toolkit  3  to extract the facial features of the main faces in each frame of the video and aligned them to 128*128 pixels, and finally obtained the frame-level features. We directly input each frame of the video into EmoVCLIP without using open-face to obtain video-level features. When fine-tuning EmoVCLIP, we set the number of learned prompts of each layer to 4 and the layers are the first 12 layers of CLIP. After inputting the text and prompt into GPT4, we obtained the probability ranking of the six categories of emotion labels and then concatenated the ranking and text into Baichuan  4  to obtain frame-level features. For the frame-level features, we calculated the mean and variance and concatenated them during training to obtain sentence-level features. Finally, we combined modality dropout with the attention used by the baseline to fuse different modalities. In the self-training process, we select top k = 10 pseudo-labeled samples from each class and set the maximum number of steps Nr = 10. In each fold of 5 folds, we set the maximum number of epochs to 30 and the batch size to 64. The Adam  [12]  optimizer is utilized with a learning rate of 3e-4. Dropout  [24]  and modality dropout rate are set to 0.3.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Results And Analysis",
      "text": "Table  1  shows our experiments in different modalities. First, in the single modal, we try to reproduce the baseline system under the same conditions. Then we show the model results obtained by all the methods we proposed. The experiment shows that our method exceeds the best baseline result by 3%. At the same time, to verify the effectiveness of our proposed method, we conducted an ablation experiment and compared the performance of EmoVClIP and CLIP single modal on the test set. It can be found that EmoVClIP has better emotional features than CLIP, and EmoVCLIP and CLIP have certain complementary characteristics, which means that although EmoVCLIP pays more attention to the time change information of the frame sequence after fine-tuning CLIP, it will lose some of its focus on facial features due to the lack of open-face processing. We will also show the comparison between GPT4-Baichuan and Baichuan, but there is no complementarity between GPT4-Baichuan features and Baichuan. As can be seen from the table, using modality dropout is more conducive to the fusion of different modalities than not using it, and alleviates the dependence and competition effects between modalities.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "We proposed a method to use prompt learning to fine-tune CLIP to adapt to emotion recognition in the video domain. We used GPT4 to strengthen Baichuan's attention to the order of sentiment classification of text. When fusing different modalities, we proposed using modality dropout to alleviate modal dependence and modal competition and used self-training to utilize unlabeled data. Finally, our model achieved a weighted F1 score of 90.15% in the emotion recognition results of the MER2024-SEMI challenge, and we verified the effectiveness of the proposed method through ablation experiments.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Framework of our proposed method.",
      "page": 3
    },
    {
      "caption": "Figure 2: Vision-Language prompt learning for EmoVCLIP.",
      "page": 3
    },
    {
      "caption": "Figure 2: , specifically, we keep the structure and",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Baseline",
          "Dropout": "0",
          "Train&Val": "80.34¬±0.10",
          "Test": "86.32¬±0.22"
        },
        {
          "Model": "Ours w/o\nEmoVCLIP",
          "Dropout": "0",
          "Train&Val": "82.64",
          "Test": "88.68"
        },
        {
          "Model": "Ours",
          "Dropout": "0",
          "Train&Val": "84.25",
          "Test": "89.52"
        },
        {
          "Model": "Ours",
          "Dropout": "0.15",
          "Train&Val": "84.04",
          "Test": "89.87"
        },
        {
          "Model": "Ours",
          "Dropout": "0.3",
          "Train&Val": "83.76",
          "Test": "90.15"
        },
        {
          "Model": "Ours",
          "Dropout": "0.5",
          "Train&Val": "82.34",
          "Test": "89.08"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "year": "2023",
      "venue": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "3",
      "title": "Semisupervised multimodal emotion recognition with class-balanced pseudo-labeling",
      "authors": [
        "Haifeng Chen",
        "Chujia Guo",
        "Yan Li",
        "Peng Zhang",
        "Dongmei Jiang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "4",
      "title": "Electra: Pre-training text encoders as discriminators rather than generators",
      "authors": [
        "Kevin Clark",
        "Minh-Thang Luong",
        "Quoc V Le",
        "Christopher Manning"
      ],
      "year": "2020",
      "venue": "Electra: Pre-training text encoders as discriminators rather than generators",
      "arxiv": "arXiv:2003.10555"
    },
    {
      "citation_id": "5",
      "title": "Transformer-xl: Attentive language models beyond a fixedlength context",
      "authors": [
        "Zihang Dai",
        "Zhilin Yang",
        "Yiming Yang",
        "Jaime Carbonell",
        "Ruslan Quoc V Le",
        "Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Transformer-xl: Attentive language models beyond a fixedlength context",
      "arxiv": "arXiv:1901.02860"
    },
    {
      "citation_id": "6",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "7",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "8",
      "title": "2022. u-hubert: Unified mixed-modal speech pretraining and zero-shot transfer to unlabeled modality",
      "authors": [
        "Wei-Ning Hsu",
        "Bowen Shi"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "9",
      "title": "Modality competition: What makes joint training of multi-modal network fail in deep learning?(provably)",
      "authors": [
        "Yu Huang",
        "Junyang Lin",
        "Chang Zhou",
        "Hongxia Yang",
        "Longbo Huang"
      ],
      "year": "2022",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "10",
      "title": "Maple: Multi-modal prompt learning",
      "authors": [
        "Muhammad Uzair Khattak",
        "Hanoona Rasheed",
        "Muhammad Maaz",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Hao Gu",
        "Jinming Zhao",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition",
      "arxiv": "arXiv:2404.17113"
    },
    {
      "citation_id": "12",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Yong Ren",
        "Hao Gu",
        "Haiyang Sun",
        "Lan Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "13",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "14",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Efficient low-rank multimodal fusion with modality-specific factors",
      "arxiv": "arXiv:1806.00064"
    },
    {
      "citation_id": "15",
      "title": "2023. emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Jiaxin Ye",
        "Jinchao Li",
        "Zhifu Gao",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "year": "2023",
      "venue": "2023. emotion2vec: Self-supervised pre-training for speech emotion representation",
      "arxiv": "arXiv:2312.15185"
    },
    {
      "citation_id": "16",
      "title": "Balanced multimodal learning via on-the-fly gradient modulation",
      "authors": [
        "Xiaokang Peng",
        "Yake Wei",
        "Andong Deng",
        "Dong Wang",
        "Di Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "17",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "18",
      "title": "Fine-tuned clip models are efficient video learners",
      "authors": [
        "Hanoona Rasheed",
        "Muhammad Khattak",
        "Muhammad Maaz",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for computational linguistics. Meeting"
    },
    {
      "citation_id": "20",
      "title": "Dash: Semi-supervised learning with dynamic thresholding",
      "authors": [
        "Yi Xu",
        "Lei Shang",
        "Jinxing Ye",
        "Qi Qian",
        "Yu-Feng Li",
        "Baigui Sun",
        "Hao Li",
        "Rong Jin"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "21",
      "title": "Open largescale language models",
      "authors": [
        "Aiyuan Yang",
        "Bin Xiao",
        "Bingning Wang",
        "Borong Zhang",
        "Ce Bian",
        "Chenxu Chao Yin",
        "Da Lv",
        "Dian Pan",
        "Dong Wang",
        "Yan"
      ],
      "year": "2023",
      "venue": "Open largescale language models",
      "arxiv": "arXiv:2309.10305"
    },
    {
      "citation_id": "22",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Russ Salakhutdinov",
        "Quoc V Le"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "24",
      "title": "Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling",
      "authors": [
        "Bowen Zhang",
        "Yidong Wang",
        "Wenxin Hou",
        "Hao Wu",
        "Jindong Wang",
        "Manabu Okumura",
        "Takahiro Shinozaki"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "25",
      "title": "Conditional prompt learning for vision-language models",
      "authors": [
        "Kaiyang Zhou",
        "Jingkang Yang",
        "Chen Loy",
        "Ziwei Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Building robust multimodal sentiment recognition via a simple yet effective multimodal transformer",
      "authors": [
        "Daoming Zong",
        "Chaoyue Ding",
        "Baoxiang Li",
        "Dinghao Zhou",
        "Jiakui Li",
        "Ken Zheng",
        "Qunyan Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    }
  ]
}