{
  "paper_id": "2404.12251v1",
  "title": "Dynamic Modality And View Selection For Multimodal Emotion Recognition With Missing Modalities",
  "published": "2024-04-18T15:18:14Z",
  "authors": [
    "Luciana Trinkaus Menon",
    "Luiz Carlos Ribeiro Neduziak",
    "Jean Paul Barddal",
    "Alessandro Lameiras Koerich",
    "Alceu de Souza Britto Jr"
  ],
  "keywords": [
    "Multimodal Emotion Recognition",
    "Missing Modalities",
    "Dynamic Modal Selection"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The study of human emotions, traditionally a cornerstone in fields like psychology and neuroscience, has been profoundly impacted by the advent of artificial intelligence (AI). Multiple channels, such as speech (voice) and facial expressions (image), are crucial in understanding human emotions. However, AI's journey in multimodal emotion recognition (MER) is marked by substantial technical challenges. One significant hurdle is how AI models manage the absence of a particular modality -a frequent occurrence in real-world situations. This study's central focus is assessing the performance and resilience of two strategies when confronted with the lack of one modality: a novel multimodal dynamic modality and view selection, and a cross-attention mechanism. Results on the RECOLA dataset show that dynamic selection-based methods are a promising approach for MER. In the missing modalities scenarios, all dynamic selection-based methods outperformed the baseline. The study concludes by emphasizing the intricate interplay between audio and video modalities in emotion prediction, showcasing the adaptability of dynamic selection methods in handling missing modalities.",
      "page_start": 1,
      "page_end": 13
    },
    {
      "section_name": "Introduction",
      "text": "The world we live in is multimodal. In this context, modality refers to how we perceive and interact with our environment. Such a concept is crucial in advancing artificial intelligence (AI), as it underlines the AI systems' need to understand and integrate these different modalities effectively. The study of the complementary relationships between modalities such as visual (video), auditory (audio), textual (text), and sensory signals (e.g., heart rate variability) is essential for developing more sophisticated and context-aware AI systems  [2, 11] .\n\nMost current multimodal approaches assume that multiple modalities are always available and they carry complementary information  [11] . This perspective is crucial in understanding how combining these modalities can lead to a richer and more comprehensive interpretation of underlying data. Each modality -visual, audio, textual, or otherwise -is believed to contribute unique insights. When merged, these insights form enhanced representations that are often more informative and accurate than any modality could achieve alone  [19] .\n\nHowever, the ideal scenario of having all expected modalities available for a given task is not always the case  [5, 7, 10, 25, 27] . In the context of multimodal AI systems, the absence or unavailability of one or more modalities can significantly impact their performance and decision-making capabilities. Therefore, addressing missing modalities in multimodal learning is critical, as it reflects the challenges of dealing with real-world data that may not always conform to ideal settings.\n\nThis work evaluates two possible strategies to model the interaction of modalities (video+audio) in the context of emotion recognition. The first is a novel approach based on dynamic selection across the multimodal space. The second employs a well-known strategy using a neural network with an attention-based mechanism to jointly learn the modalities, inspired by the method proposed in  [20] . In other words, we focus on assessing the impact of one modality's absence on model performance within these distinct multimodal AI approaches. Thus, two research questions guided our experiments. The first addresses the proposed dynamic selection method, as (RQ1) -\"Could dynamic selection of modalities and views be a promising approach for a multimodal AI method?\". The second research question concerns the impact of a missing modality on each evaluated multimodal approach, formulated as (RQ2) -\"What is the impact on emotion recognition performance when one modality (video or audio) is missing?\". To this end, we simulate the loss of a modality by replacing the corresponding features with zeros.\n\nThe contribution of this paper is twofold: (i) it introduces a novel multimodal method for emotion recognition based on dynamic modality and view selection, and (ii) it assesses how different approaches (dynamic selection vs. attention mechanism) respond to the absence of a specific modality. This work scrutinizes the challenges faced by AI in multimodal emotion recognition (MER). Also, it explores innovative methodologies, aiming to push the boundaries of what these sophisticated models can achieve in understanding the complex landscape of human emotions.\n\nThe remainder of this paper is structured into six sections. Section 2 discusses related works on MER and missing modalities. Section 3 outlines the proposed method for dynamic modality and view selection. Section 4 details the strategies employed to assess the impact of missing modalities on the proposed method and the baseline. Section 5 presents the experimental results and corresponding analysis. Finally, our conclusions and avenues for further research are presented in Section 6.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "The field of MER has been the subject of extensive research, given its applicability in various domains  [1, 3, 4, 6, 11, [17] [18] [19] [20] . This section highlights relevant works addressing the importance of emotion recognition through multiple modalities and dealing with missing modalities during inference.\n\nMultiple modalities are essential in emotion recognition because humans simultaneously express emotions through various channels. Human communication encompasses both verbal and non-verbal expressions, highlighting our dependence on multimodal cues. When pronounced with emphasis in a deep voice, a simple word can convey a more intense emotion to the listener -a nuance that eludes capture through the analysis of linguistic features alone. Complex emotions, such as fear, are more readily recognized through the interplay of facial expressions and variations in pitch than relying solely on the transcripts of spoken words  [6] .\n\nRelying on a single modality may lead to incomplete or inaccurate assessments of emotional states. In contrast, multimodal models are shown to enhance reliability and accuracy when compared to unimodal ones  [6, 11] . However, multimodal data are heterogeneous and complex. Some central technical challenges of using multimodal data in machine learning include choosing the most suitable data representations, addressing misalignment between elements of different modalities, and managing intramodal and cross-modal data correlation and fusion approaches  [6] . Dealing with missing data is one of the most critical challenges in multimodal fields  [11] .\n\nSeveral factors can contribute to the absence or unavailability of specific modalities in MER approaches at inference time  [1, 4, 10] , such as malfunction of cameras or microphones resulting in the inability to capture specific modalities, disabling or restricting certain sensors due to privacy concerns, poor lighting conditions can hinder the accurate detection of facial expressions, background noise that may affect the capture and interpretation of vocal expressions, invasiveness of sensor to capture physiological signals, etc. Understanding these causes is essential for developing strategies to handle missing modalities effectively.\n\nHandling missing modalities in multimodal approaches often involves three strategies to adapt to such situations  [1, 4, 9, 10, 25] : (i) imputing or filling in missing modalities using data imputation or leveraging information from other available modalities; (ii) designing models that can gracefully handle scenarios where certain modalities are missing, potentially by learning to rely more on available modalities; (iii) developing models that can adapt to varying modalities or different data distributions, allowing for better generalization when faced with missing modalities.\n\nSeveral works propose innovative approaches to the absence of modalities during inference. When dealing with missing modalities, common approaches often perform imputations to address the absence of modalities before proceeding with additional computations. Simple imputation methods, such as filling missing values with zeros, are straightforward but may lead to considerable inaccuracies.\n\nOther noteworthy contributions include the work of Da Silva-Filarder et al.  [23] , which studies multimodal variational autoencoders and states that a critical property of multimodal generative models is to have efficient approaches to deal with missing modalities and to enable cross-generation. Cross-generation involves using a subset of input modalities to generate output modalities missing from the input subset. The authors propose two methods: latent component dropout (LCD) and exhaustive cross-generation (ECG). LCD is based on randomization and simulates missing modalities by applying a dropout mask for each modality to individual elements of the latent variables. It then reverts to a prior expert for those elements the mask selects. ECG is a method based on brute force that considers all possible subsets of modalities.\n\nLi et al.  [7]  also address the issue of missing modalities. According to the authors, these missing data harm extracting features of multimodal data, resulting in a decline in model performance and inaccurate results. Therefore, a multiple multi-head attention network based on an encoder with missing modalities is proposed. The multi-head attention represents the modality based on the entire sequence, and the cross-mapping is used to obtain the relationship between the modalities. Random missing modalities are encoded and combined with an optimization module to enhance the association between missing and non-missing modalities. The encoder and decoder module obtain global information and map global information to multiple spaces.\n\nZhu et al.  [27]  proposed an invariant feature for a missing modality imagination network. This method relies on two encoders: the specificity encoder, responsible for extracting high-level features from raw input features, and the invariance encoder, which takes the modality-specific features obtained by the specificity encoder as input to extract modality-invariant features. Additionally, the method incorporates an invariant feature-based imagination module that predicts the modality-specific features of the missing modality based on the available modality and generates a joint representation.\n\nVazquez-Rodriguez et al.  [25]  proposed a transformer-based architecture for continuous prediction of arousal and valence, even with missing input modalities. A multimodal transformer is used as an encoder to obtain representations from the different modalities, and a transformer decoder is used to process those representations and make predictions. An encoder-decoder attention mechanism (cross-attention) of the transformer decoder is used to weigh the importance of different modalities. The transformer decoder is auto-regressive, considering past predicted values when doing the current inference, which is essential when performing time-continuous predictions.\n\nOther interesting works present computational multimodal frameworks based on transformer architecturea and attention mechanisms for MER with incomplete data  [4, 10, 12, 15] . Despite the impressive performance when a massive dataset is available, such an approach involves more complex operations, making interpretation less intuitive. In addition, attention mechanisms and Transformers can become computationally costly, even when some modalities are absent, due to the attention structure that may involve all pairs of elements in the sequence.\n\nDynamic selection can offer specific advantages compared to attention mechanisms and transformer architecture when dealing with missing modalities in multimodal approaches. Dynamic selection-based methods provide better interpretability, as the choice of specific regressors/classifiers in the absence of modalities can be easily analyzed. Additionally, dynamic selection can be more computationally efficient than the attention structure, which may involve all sequence elements. Finally, dynamic selection is less dependent on large datasets for training, as observed in our experiments.\n\nEach approach has advantages and challenges, and the choice should be based on the specific demands of the task. In this work, we will analyze the impact of missing modalities in solutions based on dynamic selection and attention mechanisms in the context of arousal-valence regression. Can more straightforward solutions based on the imputation method of filling missing values with zeros effectively address the problem of absent modalities?",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Proposed Dynamic Modality And View Selection Method",
      "text": "The proposed dynamic selection-based method considers features from AVEC'16  [24] , encompassing audio and video modalities. As shown in Fig.  1 , the audio features include acoustic features, MFCCs, and Mel spectrograms. On the video side, we incorporate appearance features and geometric features. Each regressor is trained independently, resulting in a pool of regressors denoted as F = f 1 , f 2 , . . . , f N , where N is the total number of regressors. Two LSTM layers with 256 cells each are employed to consider the temporal structure of the data. For the recurrent layers, the input is segmented into sequences of 6 seconds, corresponding to 150 time steps (frames) at a sampling rate of 16kHz. The motivation for using a simple 2-layer LSTM model is to evaluate how promising dynamic selection is considering weak regressors.\n\nInitially, the LSTM models are evaluated in the dynamic modal selection (DMS) phase. DMS is performed by training a meta-classifier with concatenated outputs from each regressor (a vector of dimension N). Training is conducted on the validation set, and the ideal output is defined as the modal (audio or video) with the best mean Concordance Correlation Coefficient (CCC), the modal whose predictions are closest to the proper labeling.\n\nAfter the modal is selected, intra-modal dynamic view selection (DVS) is performed. Each model from the selected modal receives a weight to assess each test case x j based on its performance in the competence region Ψ -set composed of the K-nearest neighbors of x j in the validation set. The DVS selects the regressor with the smallest accumulated error in the competence region or combines all the regressors or a subset using weighted averaging according to a calculated weight α i of regressor f i .\n\nThe impact of the absence of a modality was accessed using traditional dynamic selection techniques  [13] , adapted here for the multimodal problem: dy- namic selection (DS), dynamic weighting (DW) and dynamic weighting selection (DWS).\n\nIn the DS, we select the regressor from the previously chosen modal with the smallest accumulated error in the competence region. DW combines all regressors from a pre-selected modal using weighted averaging. For each test pattern x j , its competence region Ψ is calculated. For each item in Ψ , a weight d k is calculated using Eq. 1, where dist k is a distance measure between the item in the competence region t k ∈ Ψ and the test pattern x j . The vector d 1 , d 2 , ..., d k is used to calculate the weight α i of regressor f i using Eq. 2, where N is the size of the selected modal regressors pool, k represents the neighbor index, and sqe k,i is the squared error of regressor i calculated using the item t k ∈ Ψ . Finally, DWS combines a subset of regressors, and regressors with the accumulated error in the upper half of the error interval E i > (E max -E max )/2 are discarded. The method for calculating the weights of the regressors and the strategy for combining the models are the same as the DW algorithm (Eqs. 1 and 2).\n\nIt is important to emphasize that tests were conducted with the standard methods of dynamic selection, DS, DW, and DWS, with K varying from 5 to 150. All results presented in this work are with K = 100. In addition, as a baseline, we compute the simple average of the regressors' outputs and utilize the cross-attention architecture proposed by  [20] .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Missing Modality Analysis",
      "text": "The experimental approach was structured in three distinct phases. The first phase involved training the models using both audio and video modalities. In the second phase, the audio modality was disabled, and the relative contribution of the video modality to the regression task was assessed. Finally, in the third phase, the video modality was disabled, allowing for the evaluation of the audio modality's relative contribution to the regression task.\n\nThis approach was inspired by sensitivity analysis methods used in  [14, 16, 26] . In these studies, sensitivity analysis was conducted at the feature level, examining the impact of subsets of features on the overall performance of a machine learning model. In the present work, however, the sensitivity analysis was performed by generating a zero feature vector for the modality intended to be disabled. Subsequently, the model was tested with a fusion of this feature vector from the disabled modality and the active feature vector from the other modality.\n\nSuch analysis provided us valuable insights into how each modality independently influences the model's performance and how the strategies employed by dynamic selection and cross-attention handle the absence of specific modalities. By comparing the results from each phase, one could discern the individual and combined effects of audio and video modalities. Moreover, this analysis sheds light on the sensitivity of the proposed methods when confronted with missing modalities.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "The remote collaborative and affective interactions (RECOLA) dataset  [21]  represents an extensive source of multimodal data, encompassing extracted features and raw data from various modalities, including audio, video, and physiological recordings (electrocardiogram and electrodermal activity). The labeling of the first five minutes of interaction for 18 participants is available.\n\nThe data is labeled within the repository, adhering to a continuous emotional scale. This labeling is mapped into a two-dimensional space, a psychologically grounded method for describing emotions through the linear combination of arousal and valence. The concept of representing emotions in arousal and valence follows the circumplex model proposed by Russell  [22] .\n\nThe official metric for evaluating the performance of the problem is the CCC  [24] , which captures the co-variation relationship between predictions and ground truth and accounts for any deviation. As a result, it offers a more accurate representation of the alignment between predictions and ground truth  [8] . Higher CCC values signify excellent performance in terms of consistency and accuracy. The calculation process for CCC is as follows:\n\nwhere ρ is the Pearson correlation coefficient, σ y and σ ŷ are the standard deviations and µ y and µ ŷ are the means of actual and predicted emotional state. This experiment emphasized two primary modalities: audio and video. The eGeMAPS acoustic feature set was employed for the audio component, which was extracted using the OpenSmile software and is available within the RECOLA dataset. Additionally, feature sets based on Mel-frequency cepstral coefficients (MFCCs) and Mel spectrograms, both of which were extracted by the authors of this work, were utilized. The video component, on the other hand, has been focused solely on extracted features from the RECOLA dataset, including geometric features derived from 49 distinct facial landmarks and appearance features obtained by a principal component analysis (PCA) from 50,000 LGBP-TOP features.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Experimental Protocol",
      "text": "An experimental protocol based on the k-fold cross-validation method has been implemented to ensure the robustness and reliability of our findings.\n\nThe dataset comprised data from 18 individuals. To balance training and testing and ensure that our model was tested on unseen data, we allocated three individuals for testing and three for validation. The remaining participants were used for training. The experimental setup was repeated ten times, each time with a different configuration, to enhance the generalization of our results. In each iteration of the experiment, the participants were randomly shuffled.\n\nWe deliberately introduced a modality-absent condition to simulate a realworld scenario. These simulations are essential for assessing the robustness and adaptability of our model under less-than-ideal conditions. To simulate the absence of a modality a zero vector was used. In practical terms, this meant that for any given instance where a particular modality was supposed to be missing, its feature values were replaced with zeros. This approach effectively mimics scenarios where a modality's data is entirely unavailable, allowing us to observe how the model performs when deprived of information from one of the modalities.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "This section offers an in-depth analysis of the outcomes achieved by employing the proposed techniques of dynamic modal and view selection and cross-attention mechanism under ideal conditions and on a modality-absent condition.\n\nTable  1  displays the arousal and valence results, in terms of CCC, for the pool of regressors F . The findings reveal that the audio modality better represents the arousal dimension, with acoustic features, MFCCs, and Mel spectrograms, achieving CCC values of 0.69, 0.64, and 0.68. Conversely, valence is more accurately represented by the video modality, with its appearance features and geometric features representations, achieving CCC values of 0.48 and 0.56.\n\nFig.  2  shows the arousal prediction of all models for the same test case, considering scenarios where both modalities are available and when each modality is individually unavailable. Under ideal conditions, all models exhibit a consistent pattern with similar predictions. However, at certain moments, one model aligns more closely with the gold standard, while another performs better at other times. When a modality is absent, a noticeable decline in performance is observed among models relying on representations of that particular modality.\n\nFor arousal, under ideal conditions, with all modalities available -video and audio, the highest performance with CCC = 0.72 was observed when employing DW, DWS, and a simple mean of all regressors' outputs. DW and DWS yielded the best outcomes in the valence dimension with CCC = 0.54 and CCC = 0.53, surpassing DS and mean of regressors' outputs, which registered CCC = 0.46. Cross-attention results included CCC = 0.46 for arousal and CCC = 0.41 for valence. Detailed results are shown in Table  2 .\n\nRegarding arousal, methods based on dynamic selection (DW and DWS, CCC = 0.72) outperformed the top-performing regressor alone, relying solely on acoustic features, CCC = 0.69. It shows that dynamic selection of modalities can be a promising approach for a multimodal AI method. Valence achieved its peak performance with geometric features, CCC = 0.56, and none of the proposed methods managed to surpass this benchmark in valence prediction. Fig.  2 . Arousal gold standard and prediction of models based on acoustic features, MFCCs, Mel spectrograms, appearance features, and geometric features. The image was generated using test case T2 (second person from the test set) of the second cross-validation fold (k=2). From top to bottom, we have (i) prediction with all active modalities, (ii) prediction with the absence of audio modality, and (iii) prediction with the absence of video modality.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Impact Of Missing Modalities",
      "text": "Our second research question is related to how the different approaches (dynamic selection and cross-attention) respond to the absence of a specific modality. Tables 3 and 4 display the arousal and valence results, in terms of CCC, of all comparison methods -encompassing mean of the regressors' outputs, dynamic selection (DS, DW, DWS), and cross-attention. Fig.  3  compares the arousal gold standard, prediction with all active modalities, and with the absence of each modality.\n\nIn the context of arousal, the cross-attention method demonstrated heightened robustness in terms of sensitivity, exhibiting a 6.52% increase in CCC when the video modality was absent, compared to the ideal scenario where both audio and video modalities were available. Contrarily, the remaining methods either sustained their performance or experienced some loss. Among the dynamic selection-based methods, DS exhibited no performance lowering, while DW and DWS showed a minimal decrease of 1.39%. The mean method observed the most substantial decline, recording a significant loss of 15.28%. More pronounced performance losses were observed when the audio modality was absent. Several approaches witnessed a decline of over 50% in performance, which is understandable as the audio modality most effectively represents the arousal dimension. DW emerged as the most robust approach in scenarios without audio, experiencing a performance decline of 43.06% compared to the scenario with all available modalities. Following closely, DWS and DS demonstrated a CCC decline of 44.44% and 47.76%.\n\nA contrasting pattern was observed in valence, where disabling the audio modality yields superior results. When the video modality is absent, DS proves to be the least sensitive approach with a performance decline of 39.13%. In the same scenario, when the audio modality is missing, DW and DS emerged as the most robust methods, showcasing a 9.26% and 6.52% increase in CCC, respectively.\n\nIn the cross-attention method, concerning arousal, there was a notable 6.52% increase in CCC when the video modality was absent but a substantial 50% decline in performance when the audio modality was disabled. Regarding valence, promising outcomes were observed when the video modality was turned off, with a performance lowering of 2.44%. However, turning off the video modality resulted in a significant decline of 68.29% in performance compared to the ideal scenario where both audio and video modalities were available.\n\nConsidering the scenarios of missing modalities in arousal and valence dimensions, all dynamic selection-based methods (DS, DW, DWS) consistently outperformed the baselines, mean of all regressors' output, and cross-attentionbased method.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Discussion",
      "text": "The proposed approach performs better in arousal than valence, especially when the audio features are available. It may be related to the fact that arousal, which relates to the emotional intensity or activation level, might be more distinctly captured in tone of voice, volume, and speech rate, even without visual cues. For example, screams or high intonations may indicate a more excited emotional state. Elements like rhythm and timbre in speech also reflect emotional excitement; rapid rhythm or timbre changes can indicate more intense emotional states. Audio data carry significant information about the emotional state and can be quite effective in capturing the subtleties of arousal levels.\n\nAuditory cues might be less effective in conveying valence levels. Valence, associated with the positivity or negativity of emotions, is often reflected in facial expressions and might be more nuanced and complex to discern from audio alone. Visual cues are critical in identifying the valence levels, making video a more informative modality for this dimension.\n\nMER using dynamic modality and view selection appears to be an effective strategy for combining audio and video modalities, showing promising results under ideal conditions. The highest performances were observed when employing DW, with arousal CCC = 0.72 and valence CCC = 0.54.\n\nFurthermore, dynamic modality and view selection techniques exhibit notable robustness when confronted with the absence of specific modalities. In scenarios where the audio modality was absent DW also demonstrated heightened robustness in terms of sensitivity, exhibiting a performance decline of 43.06% in arousal and a increase of 9.26% in valence, compared to the ideal scenario where both audio and video modalities were available. The cross-attention method emerged as the most robust approach when the video modality was absent, exhibiting a 6.52% increase in CCC in arousal (DW demonstrated a decline of 1.39%). In valence dimension DS proves to be the least sensitive approach in scenarios without video with a performance decline of 39.13% (DW demonstrated a decline of 44.44%).\n\nFor (RQ1) -\"Could dynamic selection of modalities and views be a promising approach for a multimodal AI method?\", the results affirmatively show that dynamic selection-based methods are promising. However, the outcome of missing modalities revealed interesting nuances, addressing the research question (RQ2) -\"What is the impact on emotion recognition performance when one modality (video or audio) is missing?\", several approaches witnessed a decline of over 50% in performance when a modality is absent, emphasizing the importance of each modality in contributing to accurate predictions.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "Our investigation into the representation of time-continuous emotions, particularly in arousal and valence dimensions, through different dynamic selection approaches has yielded valuable insights. Even under less-than-ideal conditions, MER systems have demonstrated their versatility and reliability.\n\nThe findings reveal that DW show the highest performance in arousal and valence predictions under ideal conditions, with both modalities available. In the missing modalities scenarios, all dynamic selection-based methods (DS, DW, and DWS) outperformed the baselines, mean of all regressors' output, and crossattention-based method. The study concludes by emphasizing the intricate interplay between audio and video modalities in emotion prediction, showcasing the adaptability of dynamic selection methods in handling missing modalities.\n\nFinally, it is important to highlight that we have employed simple two-layer LSTMs for all modalities. Replacing such a model and handcrafted feature representations by pre-trained vision transformers and large language models must improve the performance for each modality and the final ensemble.",
      "page_start": 13,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Dynamic modality and view selection method. The audio features include",
      "page": 6
    },
    {
      "caption": "Figure 2: shows the arousal prediction of all models for the same test case, con-",
      "page": 8
    },
    {
      "caption": "Figure 2: Arousal gold standard and prediction of models based on acoustic features,",
      "page": 10
    },
    {
      "caption": "Figure 3: compares the arousal",
      "page": 10
    },
    {
      "caption": "Figure 3: Comparison of arousal gold standard, prediction with all active modalities,",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Regarding arousal, methods based on dynamic selection (DW and DWS,",
      "data": [
        {
          "Features": "Acoustic",
          "Arousal": "0.69±0.06",
          "Valence": "0.18±0.07"
        },
        {
          "Features": "MFCCs",
          "Arousal": "0.64±0.06",
          "Valence": "0.35±0.08"
        },
        {
          "Features": "Mel Spectrograms",
          "Arousal": "0.68±0.06",
          "Valence": "0.22±0.09"
        },
        {
          "Features": "Appearance",
          "Arousal": "0.42±0.09",
          "Valence": "0.48±0.06"
        },
        {
          "Features": "Geometric",
          "Arousal": "0.41±0.09",
          "Valence": "0.56±0.14"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 2: Regarding arousal, methods based on dynamic selection (DW and DWS,",
      "data": [
        {
          "Approach": "DS",
          "Arousal": "0.67±0.06",
          "Valence": "0.46±0.08"
        },
        {
          "Approach": "DW",
          "Arousal": "0.72±0.04 0.54±0.10",
          "Valence": ""
        },
        {
          "Approach": "DWS",
          "Arousal": "0.72±0.04",
          "Valence": "0.53±0.10"
        },
        {
          "Approach": "Cross-Attention",
          "Arousal": "0.46±0.13",
          "Valence": "0.41±0.17"
        },
        {
          "Approach": "Mean",
          "Arousal": "0.72±0.04",
          "Valence": "0.46±0.08"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Privileged knowledge distillation for dimensional emotion recognition in the wild",
      "authors": [
        "M Aslam",
        "O Zeeshan",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "E Granger"
      ],
      "year": "2023",
      "venue": "CVPRw 2023: IEEE/CVF CVPR"
    },
    {
      "citation_id": "2",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrusaitis",
        "C Ahuja",
        "L Morency"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "3",
      "title": "ETS system for AV+EC 2015 challenge",
      "authors": [
        "P Cardinal",
        "N Dehak",
        "A Koerich",
        "J Alam",
        "P Boucher"
      ],
      "year": "2015",
      "venue": "ETS system for AV+EC 2015 challenge"
    },
    {
      "citation_id": "4",
      "title": "A novel transformer autoencoder for multimodal emotion recognition with incomplete data",
      "authors": [
        "C Cheng",
        "Z Fan",
        "L Feng",
        "Z Jia"
      ],
      "year": "2024",
      "venue": "Neural Netw"
    },
    {
      "citation_id": "5",
      "title": "A review and meta-analysis of multimodal affect detection systems",
      "authors": [
        "S D'mello",
        "J Kory"
      ],
      "year": "2015",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "6",
      "title": "Survey on multimodal approaches to emotion recognition",
      "authors": [
        "A Gladys",
        "V Vetriselvi"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "7",
      "title": "Mman-m2: Multiple multi-head attentions network based on encoder with missing modalities",
      "authors": [
        "J Li",
        "L Li",
        "R Sun",
        "G Yuan",
        "S Wang",
        "S Sun"
      ],
      "year": "2024",
      "venue": "Patt Recogn Lett"
    },
    {
      "citation_id": "8",
      "title": "A survey of deep learningbased multimodal emotion recognition: Speech, text, and face",
      "authors": [
        "H Lian",
        "C Lu",
        "S Li",
        "Y Zhao",
        "C Tang",
        "Y Zong"
      ],
      "year": "2023",
      "venue": "Entropy"
    },
    {
      "citation_id": "9",
      "title": "MissModal: Increasing Robustness to Missing Modality in Multimodal Sentiment Analysis",
      "authors": [
        "R Lin",
        "H Hu"
      ],
      "year": "2023",
      "venue": "Trans Assoc Comput Ling"
    },
    {
      "citation_id": "10",
      "title": "Enhancing resilience to missing data in audiotext emotion recognition with multi-scale chunk regularization",
      "authors": [
        "W Lin",
        "L Goncalves",
        "C Busso"
      ],
      "year": "2023",
      "venue": "th ICMI"
    },
    {
      "citation_id": "11",
      "title": "Multi-modal fusion network with complementarity and importance for emotion recognition",
      "authors": [
        "S Liu",
        "P Gao",
        "Y Li",
        "W Fu",
        "W Ding"
      ],
      "year": "2023",
      "venue": "Inf. Sci"
    },
    {
      "citation_id": "12",
      "title": "Modality translation-based multimodal sentiment analysis under uncertain missing modalities",
      "authors": [
        "Z Liu",
        "B Zhou",
        "D Chu",
        "Y Sun",
        "L Meng"
      ],
      "year": "2024",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "13",
      "title": "Mine: A framework for dynamic regressor selection",
      "authors": [
        "T Moura",
        "G Cavalcanti",
        "L Oliveira"
      ],
      "year": "2021",
      "venue": "Inf. Sci"
    },
    {
      "citation_id": "14",
      "title": "A novel sensitivity-based method for feature selection",
      "authors": [
        "D Naik",
        "R Kiran"
      ],
      "year": "2021",
      "venue": "Journal of Big Data"
    },
    {
      "citation_id": "15",
      "title": "Multimodal learning with incompleteness towards multimodal sentiment analysis and emotion recognition task",
      "authors": [
        "C Nguyen",
        "H Nguyen",
        "D Le",
        "Q Ha"
      ],
      "year": "2023",
      "venue": "th Intl Conf on Knowledge Syst Eng"
    },
    {
      "citation_id": "16",
      "title": "An optimized hill climbing algorithm for feature subset selection: evaluation on handwritten character recognition",
      "authors": [
        "C Nunes",
        "A Britto",
        "C Kaestner",
        "R Sabourin"
      ],
      "year": "2004",
      "venue": "An optimized hill climbing algorithm for feature subset selection: evaluation on handwritten character recognition"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition using fusion of audio and video features",
      "authors": [
        "J Ortega",
        "P Cardinal",
        "A Koerich"
      ],
      "year": "2019",
      "venue": "IEEE Intl Conf SMC"
    },
    {
      "citation_id": "18",
      "title": "Multimodal fusion with deep neural networks for audio-video emotion recognition",
      "authors": [
        "J Ortega",
        "M Senoussaoui",
        "E Granger",
        "M Pedersoli",
        "P Cardinal",
        "A Koerich"
      ],
      "year": "2018",
      "venue": "Multimodal fusion with deep neural networks for audio-video emotion recognition"
    },
    {
      "citation_id": "19",
      "title": "Audio-visual fusion for emotion recognition in the valence-arousal space using joint cross-attention",
      "authors": [
        "R Praveen",
        "P Cardinal",
        "E Granger"
      ],
      "year": "2023",
      "venue": "IEEE Trans Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "20",
      "title": "A joint crossattention model for audio-visual fusion in dimensional emotion recognition",
      "authors": [
        "R Praveen",
        "W De Melo",
        "N Ullah",
        "H Aslam",
        "O Zeeshan",
        "T Denorme",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "P Cardinal",
        "E Granger"
      ],
      "year": "2022",
      "venue": "A joint crossattention model for audio-visual fusion in dimensional emotion recognition"
    },
    {
      "citation_id": "21",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "10th IEEE Intl Conf FG"
    },
    {
      "citation_id": "22",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "J Personality Soc Psychology"
    },
    {
      "citation_id": "23",
      "title": "Multimodal variational autoencoders for sensor fusion and cross generation",
      "authors": [
        "M Silva-Filarder",
        "A Ancora",
        "M Filippone",
        "P Michiardi"
      ],
      "year": "2021",
      "venue": "th IEEE ICMLA"
    },
    {
      "citation_id": "24",
      "title": "AVEC 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "F Ringeval",
        "D Lalanne",
        "M Torres Torres",
        "S Scherer",
        "G Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "6th AVEC"
    },
    {
      "citation_id": "25",
      "title": "Accommodating missing modalities in time-continuous multimodal emotion recognition",
      "authors": [
        "J Vazquez-Rodriguez",
        "G Lefebvre",
        "J Cumin",
        "J Crowley"
      ],
      "year": "2023",
      "venue": "Accommodating missing modalities in time-continuous multimodal emotion recognition"
    },
    {
      "citation_id": "26",
      "title": "Feature selection with neural networks",
      "authors": [
        "A Verikas",
        "M Bacauskiene"
      ],
      "year": "2002",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "27",
      "title": "Exploiting multi-modal fusion for robust face representation learning with missing modality",
      "authors": [
        "Y Zhu",
        "X Sun",
        "X Zhou"
      ],
      "year": "2023",
      "venue": "ICANN 2023"
    }
  ]
}