{
  "paper_id": "2410.08470v1",
  "title": "Dat: Dialogue-Aware Transformer With Modality-Group Fusion For Human Engagement Estimation",
  "published": "2024-10-11T02:43:45Z",
  "authors": [
    "Jia Li",
    "Yangchen Yu",
    "Yin Chen",
    "Yu Zhang",
    "Peng Jia",
    "Yunbo Xu",
    "Ziqiang Li",
    "Meng Wang",
    "Richang Hong"
  ],
  "keywords": [
    "Engagement Estimation",
    "Human Behavior",
    "Feature Group Fusion",
    "Multimodal Transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Engagement estimation plays a crucial role in understanding human social behaviors, attracting increasing research interests in fields such as affective computing and human-computer interaction. In this paper, we propose a Dialogue-Aware Transformer framework (DAT) with Modality-Group Fusion (MGF), which relies solely on audio-visual input and is language-independent, for estimating human engagement in conversations. Specifically, our method employs a modality-group fusion strategy that independently fuses audio and visual features within each modality for each person before inferring the entire audio-visual content. This strategy significantly enhances the model's performance and robustness. Additionally, to better estimate the target participant's engagement levels, the introduced Dialogue-Aware Transformer considers both the participant's behavior and cues from their conversational partners. Our method was rigorously tested in the Multi-Domain Engagement Estimation Challenge held by MultiMediate'24, demonstrating notable improvements in engagement-level regression precision over the baseline model. Notably, our approach achieves a CCC score of * Equally contribution.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In communication, engagement is a crucial indicator of participants' involvement and interaction levels within a conversation. It not only reflects the importance participants attribute to the interaction but also profoundly influences the quality and efficacy of the dialogue. The rapid advancements in deep learning technologies have led to the emergence of automating engagement estimation as a critical challenge and opportunity in affective computing and group behavior analysis  [16, 35] .\n\nTo further research in this field, the MultiMediate 2024 competition has organized the Multi-Domain Engagement Estimation track  [20] . Competitors are tasked with continuously predicting the engagement level of each individual in a conversation, frame by frame, with scores ranging from 0 to 1. Competitors are encouraged to explore multimodal data and reciprocal behaviors, and to evaluate the accuracy of their predictions using the Concordance Correlation Coefficient (CCC)  [18] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Vision",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Contextual Segment",
      "text": "Audio l s l Engagement Figure  1 : The task of engagement estimation  [20]  aims to predict the continuous values of a target participant's engagement level frame-by-frame in a conversation. Contextual segmentation is performed on the entire recording sequence to include more contextual information around a specific time period for prediction.\n\nExisting methods for engagement estimation typically rely on extracting features from video sequences or segments, followed by processing them with Recurrent Neural Networks (RNN)  [10] , Long Short-Term Memory networks (LSTM)  [14] , or Transformer  [29]  models. These approaches focus primarily on utilizing cues from the target participants, overlooking the conversational partner's behavior, which also plays a crucial role in evaluating the target participant's engagement. Moreover, in multimodal engagement estimation tasks, current methods often simply concatenate features extracted from video sequences or segments for engagement prediction, without deeply exploring the interplay between various features and modalities.\n\nTo address these limitations and challenges, we propose a flexible and effective framework, named DAT, which is designed to fully utilize the audio and visual information of relevant participants in the conversation to accurately estimate the engagement level of the target participant. Specifically, DAT is composed of several Transformer Layers, including the Modality-Group Fusion (MGF) module and the Dialogue-Aware Transformer Encoder (DAE). The MGF module aims to encode video and audio features using a modalitygroup fusion strategy to obtain deep and language-independent modality-specific representations. This unique processing of modalities aims to minimize redundancy and overlap within the datasets. Additionally, the DAE module, a straightforward Transformer with Cross-Attention, incorporates auxiliary information from other conversational partners to enrich the target participant's data. The enrichment of data sources enhances the precision in capturing the target participant's behavior throughout the conversation, thereby enhancing the accuracy of the engagement estimation.\n\nOur method was evaluated on the NoXi Base  [4, 20]  , NoXi-Add  [20] , and MPIIGI  [21]  test sets in the Multi-Domain Engagement Estimation Challenge of the MultiMediate 2024. The experimental results indicate that the proposed method achieved superior performance and surpassed baseline models  [20]  across all datasets in terms of CCC, confirming that the inclusion of conversational partners and the modality-group fusion strategy significantly enhances our model performance and robustness.\n\nThe remainder of this paper is organized as follows. Related works are introduced in Section 2. Section 3 describes the details of our DAT model and the whole workflow. Section 4 presents the implementation details, ablation studies and comparisons with other methods. We conclude our work in Section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Engagement Estimation. Estimating engagement  [1]  levels in human conversation remains a pivotal topic in human-computer interaction. This task entails predicting participants' engagement frame-by-frame, utilizing multimodal data from interactions involving two or more individuals. Several studies  [3, 13, 17, 26, 31]  have explored engagement estimation. Additionally, studies within the education sector  [12, 15, 25]  have also addressed this topic. Recent efforts have increasingly directed attention towards estimating engagement within conversational settings. Yang et al.  [30] , achieved commendable results by classifying and concatenating audio-video features using a straightforward network architecture. Similarly, Yu et al.  [32]  employed a sliding window approach to capture both local and global cues in video data, utilizing Transformer's capabilities for sequence modeling to depict participant engagement accurately. However, these methodologies insufficiently explore the multimodal data inherent in conversations, thus missing out on capturing the nuanced representations of interaction data. Our research, in contrast, emphasizes dialogue scenarios and has shown good performance across multiple engagement estimation datasets. Multimodal Feature Fusion. To enhance engagement estimation, it is crucial to employ multimodal feature fusion techniques to bridge the semantic gap between different modalities. Several approaches have been proposed to leverage different modalities. Commonly, self-attention  [29]  and cross-attention  [27]  mechanisms are employed. Beyond traditional self-attention and cross-attention mechanisms, more sophisticated techniques exist. Neural Multimodal Embeddings  [5]  learn a joint embedding space mapping different modalities to proximate, semantically similar points, facilitating tasks such as cross-modal retrieval, generation, and alignment. Tensor Fusion  [33]  addresses both inter-and intra-modality aspects by considering interactions within and between modalities. Inspired by prior work  [30]  , our strategy partitions features into video and audio groups before their subsequent fusion, achieving better results. Emotion Analysis. The problem of human engagement estimation is highly correlated with the popular research field of human emotion analysis, such as facial expression recognition in images or videos, audio-visual emotion recognition  [19, 23, 34] , and multimodal sentiment analysis  [7, 8, 28] . Emotion expressed by a person's facial expression, speech tone and spoken language is closely related to their level of engagement in a dialogue. Considering that large-scale unimodal or multimodal in-the-wild emotion datasets have been established, we believe that emotion-related audio, visual, and textual features (or representations) can benefit existing methods for engagement estimation if only the original multimodal data of conversations is available. However, in the MultiMediate'24 Our DAT consists of two main modules: Modality-Group Fusion and Dialogue-Aware Encoder. Firstly, the Modality-Group Fusion module processes audio and visual features for both the participant and partner. Each feature is processed through a Transformer before being fused together. Subsequently, the Dialogue-Aware Encoder utilizes cross-attention to combine and encode information from both participants, focusing on contextual interactions to enhance engagement prediction. Finally, an MLP predicts continuous engagement levels frame-by-frame by utilizing the encoded features.\n\nChallenge, only pre-extracted specific features are provided, and the original data is not available.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology 3.1 Problem Definition",
      "text": "Given a long video, the engagement estimation task involves predicting the continuous level of conversational engagement for each participant, ranging from 0 (minimal engagement) to 1 (maximum engagement). Since each frame in the video is individually labeled, this task is essentially a continuous regression problem. Given the inputs of target participant ğ‘¿ and the label of the target participant ğ‘Œ , this task is to learn a mapping function ğ‘“ ğœƒ (ğ‘¿ ) â†’ ğ’€ , where ğ‘“ ğœƒ defines the model and ğœƒ represents the learnable parameters.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Contextual Segment",
      "text": "For the engagement estimation problem, the ideal situation is that the engagement of the participants at the current moment can be predicted by combining the features of the current moment frame and the frames within a period of time around it, as illustraed in Figure  1 . This problem can be formalized as follows:\n\nwhere [ğ‘¥ ğ‘¡ -ğ‘™ , ğ‘¥ ğ‘¡ -ğ‘™+1 , ğ‘¥ ğ‘¡ -ğ‘™+2 , . . . , ğ‘¥ ğ‘¡ +ğ‘™ ] represents the multimodal features from ğ‘¡ -ğ‘™ to ğ‘¡ +ğ‘™ time, ğ‘¦ ğ‘¡ represents the estimated engagement of the participant at time ğ‘¡, ğ‘™ is the length of the surrounding frames considered.\n\nContextual segment-based prediction can significantly enhance performance beyond what is achievable through the sole use of multimodal features at any given moment. Engagement levels of participants are in constant flux throughout a conversation, where the contextual backdrop often holds essential insights crucial for accurately estimating engagement. To leverage this, we introduce context segmentation for the input feature sequence, i.e., we divide the video into several sub-sequences tailored for subsequent prediction tasks. our approach enables simultaneous engagement prediction across multiple frames, employing a step size ğ‘  to generate overlapping sub-sequences. These overlapping sub-sequences are processed using a Sequence-to-Sequence (Seq2Seq) Model, which is delineated as follows:\n\nHere, ğ‘  + 2ğ‘™ denotes the sequence length of each prediction input, where ğ‘  is the predicted sequence length (stride), and ğ‘¥ ğ‘– represents the ğ‘– multimodal feature in the sequence. A larger ğ‘™ incorporates more surrounding frames. When ğ‘™ is sufficiently large, the entire video can be approximately modeled.\n\nWe observe that certain situations within a conversation, such as unexpected phone calls, can cause significant fluctuations in participants' engagement levels. Additionally, longer sequences tend to contain more extraneous information, which can negatively impact model performance. Thus, selecting an appropriate sequence length is crucial for effectively capturing relevant features and enhancing prediction accuracy.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dat Model",
      "text": "As illustrated in Figure  2 , a Dialogue-Aware Transformer (DAT) framework has been developed specifically for engagement estimation. The DAT framework comprises Modality-Group Fusion (MGF) modules and a Dialogue-Aware Encoder (DAE). Each MGF module independently processes the video and audio features from both the target participant and their conversational partner, and the DAE combines these features, facilitating their effective integration. The resulting fused feature is then feed to MLP to predict the final engagement level of the target participant within the conversation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Modality-Grouping Fusion.",
      "text": "As depicted in Figure  2 , five features ( ğ‘¿ ğ¸ , ğ‘¿ ğ‘Š , ğ‘¿ ğ¶ , ğ‘¿ ğ‘‚ğ¹ and ğ‘¿ ğ‘‚ğ‘ƒ ) are extracted from the conventional participants using several specialized tools: OpenSmile  [11] , W2v-BERT 2.0  [9] , CLIP  [24] , OpenFace  [2]  and Open-Pose  [6] , respectively. Initially, these features are segmented into several contextual segments according to their timestamps. For each segment feature ğ’™ ğ‘ , ğ‘ âˆˆ {ğ¸,ğ‘Š , ğ¶, ğ‘‚ğ¹, ğ‘‚ğ‘ƒ }, it is first projected to a unified dimension ğ‘‘ then processed though a Transformer to derive deeper representations. The mathematical formulation of this process is as follows:\n\nwhere ğ‘ƒğ‘Ÿğ‘œ ğ‘— denotes the linear layer and the ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘  ğ‘“ ğ‘œğ‘Ÿğ‘šğ‘’ğ‘Ÿ represents a standard transformer layer.\n\nSubsequently, these deep features are grouped into audio features ğ’™ â€²â€² ğ¸ , ğ’™ â€²â€² ğ‘Š âˆˆ R (ğ‘ +2ğ‘™ ) Ã—ğ‘‘ and video feature ğ’™ â€²â€² ğ¶ , ğ’™ â€²â€² ğ‘‚ğ¹ , ğ’™ â€²â€² ğ‘‚ğ‘ƒ âˆˆ R (ğ‘ +2ğ‘™ ) Ã—ğ‘‘ , which are then concatenated in ğ’™ ğ´ âˆˆ R (ğ‘ +2ğ‘™ ) Ã—2ğ‘‘ , ğ’™ ğ‘‰ âˆˆ R (ğ‘ +2ğ‘™ ) Ã—3ğ‘‘ . This can be described as:\n\nwhere ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ denotes concatenation operation. Following this, the audio features and video features are further processed using a Transformer layer to derive deeper representations of audio and video modalities:\n\nwhere ğ’™ â€² ğ´ âˆˆ R (ğ‘ +2ğ‘™ ) Ã—2ğ‘‘ and ğ’™ â€² ğ‘‰ âˆˆ R (ğ‘ +2ğ‘™ ) Ã—3ğ‘‘ represent the processed audio and video modality features for the participant, respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dialogue-Aware Transformer Encoder.",
      "text": "To enhance the accuracy of estimating the participation level of individuals in a conversation, relying solely on the data from the target participant is insufficient and biased. Consequently, this paper advocates for the inclusion of the conversational partner's information as supplementary data. We designed a dialogue-aware transformer encoder to fuse the audio and video features from the target participant and involved partners. As illustrated in Figure  2 , the modality features ğ‘¨ ğ‘‡ , ğ‘½ ğ‘‡ for target participant and ğ‘¨ ğ¼ , ğ‘½ ğ¼ for involved partner are obtained from the Modality-Group Fusion Module, respectively. Upon extracting these features, the partner's features are used as the query, and the target participant's features as both the key and value, to perform cross-attention fusion on audio and video features independently. This is achieved through ğ‘ layers of Transformer to improve the data integration process. The dialogue-aware transformer encoder layer's operation is as follows:\n\nğ¶ğ‘Ÿğ‘œğ‘ ğ‘ -ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘¸, ğ‘², ğ‘½ ) = ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥\n\nwhere ğ‘ğ‘œğ‘Ÿğ‘š, ğ¹ ğ¹ ğ‘ , ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ denote the LayerNorm and FeedForward and softmax function, respectively. The final audio feature output is obtained as follows:\n\nSimilarly, the final video feature ğ‘½ ğ‘‚ ğ‘‡ is derived using dialogue-aware transformers.\n\nThis methodology not only helps to filter out irrelevant information but also allows the model to consider a broader and more comprehensive dataset, enabling a more accurate evaluation of the target participant's engagement level.\n\nFor all the encoders of the aforementioned models, we utilize fully connected layers to predict the final engagement level of the target participant. The final prediction ğ’š ğ‘ğ‘Ÿğ‘’ is calculated as follows:\n\nwhere ğ’™ ğ‘‚ ğ‘‡ âˆˆ R (ğ‘ +2ğ‘™ ) Ã—5ğ‘‘ and ğ‘€ğ¿ğ‘ƒ denotes fully connected layers. Finally, we supervise the model using MSE loss for the NoXi datasets and CCC loss for the MPIIGroupInteraction (MPIIGI) dataset. Different loss functions are employed due to the distinct characteristics of their labels. The labels in the NoXi dataset are continuous and fluctuate between adjacent frames, making MSE loss suitable for capturing these variations. In contrast, the MPIIGI dataset labels are discrete, categorized into 25 classes ranging from 0 to 1, resulting in many consecutive frames having identical labels. Therefore, CCC loss is more appropriate for MPIIGI as it effectively handles the stability and consistency of discrete labels across multiple frames.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "To demonstrate the efficacy of our proposed DAT framework in engagement estimation tasks, we conduct extensive evaluations on the dataset provided by the organizers. Moreover, ablation studies are conducted on Modality-Group fusion module and Dialogue-Aware Transformer encoder. This section is dedicated to detailing the datasets employed during both training and evaluation stages, outlining the experimental setup, presenting an ablation analysis, and highlighting the most favorable experimental results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "This challenge  [20]  includes three datasets, and the average CCC score  [18]  across these datasets is used as the final competition metric.\n\nNoXi Base  [4]  dataset includes 38 sets of training files, 10 sets of validation files, and 16 sets of test files. It primarily consists of one-on-one video interactions between experts and novices in English, French, and German, incorporating both audio and visual components. The sampling rate is 25 fps, although some features are sampled at 40 fps. Due to privacy concerns, only pre-extracted features are provided. These features are continuously recorded, with each frame labeled with an engagement value ranging from 0 to 1.\n\nNoXi Additional Language dataset (NoXi-Add) comprises only 12 sets of test files. Unlike the basic version of NoXi, the linguistic forms in its features are entirely different, including Arabic, Italian, Indonesian, and Spanish. However, other feature formats remain consistent with the basic version.\n\nMPIIGroupInteraction  [22]  dataset consists of 6 validation files and 6 test files. MPIIGI dataset records the engagement levels of four individuals interacting in real-time using German. Four cameras capture the individuals from different positions. A significant difference from the NoXi dataset is that participants in MPIIGI dataset are mostly recorded in a seated position, whereas those in NoXi dataset are recorded standing. The sampling rates of various features in MPIIGI dataset remain consistent with those in the NoXi dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "We utilized all official-provided features from the dataset, totaling 2477 dimensions, including: 88-dimensional audio features from OpenSmile  [11] , 1024-dimensional word features from W2v-BERT 2.0  [9] , 512-dimensional visual features from CLIP  [24] , 714dimensional facial features from OpenFace  [2] , and 139-dimensional pose features from OpenPose  [6] . These features are grouped into audio (first two) and visual (latter three) modalities.\n\nIn the contextual segment method, the total sequence length is 96, with a core sequence length of 32 and auxiliary sequence lengths of 32 on both sides. The step size ğ‘  for segment division is also 32. Additionally, all features in Modality-Group Fusion are uniformly mapped to a dimension ğ‘‘ of 512. As shown in Figure  2 , the number of layers ğ‘ for both modalities in the Dialogue-Aware Encoder is set to 1. To reduce overfitting and improve model generalization, the dropout rate for all network layers is set to 0.2.\n\nTo optimize the model, we utilize the Adam optimizer with a learning rate of 5e-5 and a batch size of 128. The training process incorporates different loss functions tailored to each dataset: Mean Squared Error (MSE) for the NoXi dataset and Concordance Correlation Coefficient (CCC) for the MPIIGI dataset. Additionally, an Exponential Moving Average (EMA) strategy is applied throughout the 50 training epochs. For the NoXi Base and NoXi-Add datasets, the NoXi train and validation sets are used as the training data, while the MPIIGI validation set serves as the training data for the MPIIGI dataset.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Ablation Study",
      "text": "This section investigates the impact of our proposed key modules, Modality-Group Fusion and Dialogue-Aware Encoder, on model performance through ablation experiments on the NoXi Base dataset. Notably, our baseline model does not benefit from these two modules. It consists of six Transformer encoders  [29] , where five features are sequentially enhanced through five encoders, concatenated, and integrated through one Transformer. Predictions are made through the MLP layers. This corresponds to the first row in Table  1 , representing our model's baseline. Ablation on Modality-Grouping Fusion Module. We have developed a feature fusion strategy and introduced the Modality-Group Fusion (MGF) module. Our hypothesis suggests that by grouping and fusing features based on their respective modalities, it is possible to minimize information redundancy within each modality, thereby optimizing inter-modality integration. Consequently, the MGF module was specifically designed to group and fuse audio and video features, thereby enhancing the overall feature fusion process. The results as presented in Table  1  clearly support our hypothesis. The integration of the MGF module has resulted in a performance enhancement by 0.014 on the validation set. This improvement further substantiates the effectiveness of the MGF  module in boosting the model's learning capabilities and enhancing its overall performance. Ablation on Dialogue-Aware Encoder. Regarding the performance enhancement attributed to our proposed Dialogue-Aware Encoder (DAE), an ablation study was meticulously executed, with results summarized in Table  1 . The data unequivocally indicate that the integration of DAE yielded an improvement of 0.008 in the CCC on the Noxi Base dataset, thereby validating the contribution of DAE to the model's predictive accuracy. Moreover, when DAE was synergistically paired with MGF, the model's performance was further elevated, achieving an additional gain of 0.020 in the CCC. This incremental enhancement not only underscores the effectiveness of the DAE module but also highlights its complementary benefits when integrated with MGF, thus reinforcing the holistic efficacy of our proposed framework. Additionally, to verify that the benefits of the DAE module are not due to an increase in parameter count, we compared models with and without the DAE module while increasing the model depth to keep the parameter count consistent. As shown in Table  2 , the model with the DAE module outperforms We have explored various sizes for the dimension ğ‘‘. As shown in Figure  3 , regarding the validation set CCC score for the NoXi Base data, the default size of 512 for ğ‘‘ in our model is found to be optimal. Dimensions that are too small result in inadequate expressive power for the model, whereas excessively large dimensions lead to overfitting. Therefore, dimensions that are either larger or smaller than 512 lead to a reduction in the model's performance.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Presentation Of Final Results",
      "text": "Finally, our model framework, adjusted to the optimal settings, was applied to the test sets of all three datasets and compared with the official baseline models  [20] . As shown in Table  3 , our model achieved a state-of-the-art result on the NoXi Base test set with a 0.76 CCC. Additionally, it demonstrated superior performance on the NoXi-Add (0.67) and MPIIGI (0.49) datasets, showcasing the robustness of our method. Furthermore, our approach surpasses the official baseline by 0.23 on the global average CCC, indicating its effectiveness. To visually highlight our model's performance, we present comparisons of predicted and true labels on the NoXi Base validation set, as shown in Figure  4 . These visualizations show that our predictions largely capture the fluctuations in participants' engagement over time.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduce a novel, language-independent Dialogue-Aware Transformer (DAT) framework that exclusively utilizes audiovisual inputs to estimate human engagement in conversations. The Modality-Group Fusion (MGF) employed in this study effectively fuses each participant's audio-visual features independently, significantly enhancing the model's performance and robustness. Furthermore, the inclusion of the Dialogue-Aware Encoder (DAE), which  integrates behavioral and conversational information from both the target participant and their conversational partners, significantly augments the target participant's data, effectively mitigating bias in the model. With integrating MGF and DAE, our approach not only achieves a CCC of 0.76 on the NoXi Base test set but also surpasses the established baseline by an average of 0.23 CCC across NoXi Base, NoXi-Add and MPIIGI test sets. We believe that DAT will serve as a solid baseline and contribute significantly to engagement estimation. In the future, we will explore more effective methods to leverage partner information to further enhance engagement estimation in dialogues.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The task of engagement estimation [20] aims to pre-",
      "page": 2
    },
    {
      "caption": "Figure 2: Overall architecture of the proposed method. Our DAT consists of two main modules: Modality-Group Fusion and Dialogue-",
      "page": 3
    },
    {
      "caption": "Figure 1: This problem can be formalized as follows:",
      "page": 3
    },
    {
      "caption": "Figure 2: , a Dialogue-Aware Transformer (DAT)",
      "page": 3
    },
    {
      "caption": "Figure 2: , the modality features",
      "page": 4
    },
    {
      "caption": "Figure 2: , the number",
      "page": 5
    },
    {
      "caption": "Figure 3: Ablation on the unified mapping space dimension",
      "page": 5
    },
    {
      "caption": "Figure 4: Using our method for real-time fitting, the selected interval length is fixed at 5000 samples.",
      "page": 6
    },
    {
      "caption": "Figure 3: , regarding the validation set CCC score for the NoXi Base",
      "page": 6
    },
    {
      "caption": "Figure 4: These visualizations show",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "E": "",
          "Column_2": "E",
          "C": "OF"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Bodily Behaviors in Social Interaction: Novel Annotations and State-of-the-Art Evaluation",
      "authors": [
        "Michal Balazia",
        "Philipp MÃ¼ller"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia",
      "doi": "10.1145/3503161.3548363"
    },
    {
      "citation_id": "2",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "Tadas BaltruÅ¡aitis",
        "Peter Robinson",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE"
    },
    {
      "citation_id": "3",
      "title": "Gaze and conversational engagement in multiparty video conversation: an annotation scheme and classification of high and low levels of engagement",
      "authors": [
        "Roman Bednarik",
        "Shahram Eivazi",
        "Michal Hradis"
      ],
      "year": "2012",
      "venue": "Proceedings of the 4th workshop on eye gaze in intelligent human machine interaction"
    },
    {
      "citation_id": "4",
      "title": "The NoXi database: multimodal recordings of mediated novice-expert interactions",
      "authors": [
        "Angelo Cafaro",
        "Johannes Wagner",
        "Tobias Baur",
        "Soumia Dermouche",
        "Mercedes Torres",
        "Catherine Pelachaud",
        "Elisabeth AndrÃ©",
        "Michel Valstar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "5",
      "title": "Multimodal Attention for Neural Machine Translation",
      "authors": [
        "Ozan Caglayan",
        "LoÃ¯c Barrault",
        "Fethi Bougares"
      ],
      "year": "2016",
      "venue": "Multimodal Attention for Neural Machine Translation",
      "arxiv": "arXiv:1609.03976[cs.CL"
    },
    {
      "citation_id": "6",
      "title": "Realtime multiperson 2d pose estimation using part affinity fields",
      "authors": [
        "Zhe Cao",
        "Tomas Simon",
        "Shih-En Wei",
        "Yaser Sheikh"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "7",
      "title": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "authors": [
        "Yin Chen",
        "Jia Li",
        "Shiguang Shan",
        "Meng Wang",
        "Richang Hong"
      ],
      "year": "2023",
      "venue": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "arxiv": "arXiv:2312.05447"
    },
    {
      "citation_id": "8",
      "title": "MMA-DFER: MultiModal Adaptation of unimodal models for Dynamic Facial Expression Recognition in-the-wild",
      "authors": [
        "Kateryna Chumachenko",
        "Alexandros Iosifidis",
        "Moncef Gabbouj"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Ruoming Pang, and Yonghui Wu. 2021. W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training",
      "authors": [
        "Yu-An Chung",
        "Yu Zhang",
        "Wei Han",
        "Chung-Cheng Chiu",
        "James Qin"
      ],
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "10",
      "title": "Finding structure in time",
      "authors": [
        "Jeffrey L Elman"
      ],
      "year": "1990",
      "venue": "Cognitive science"
    },
    {
      "citation_id": "11",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin WÃ¶llmer",
        "BjÃ¶rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Attentive or not? Toward a machine learning approach to assessing students' visible engagement in classroom instruction",
      "authors": [
        "Patricia Goldberg",
        "Ã–mer SÃ¼mer",
        "Kathleen StÃ¼rmer",
        "Wolfgang Wagner",
        "Richard GÃ¶llner",
        "Peter Gerjets",
        "Enkelejda Kasneci",
        "Ulrich Trautwein"
      ],
      "year": "2021",
      "venue": "Educational Psychology Review"
    },
    {
      "citation_id": "13",
      "title": "Developing an Effective and Automated Patient Engagement Estimator for Telehealth: A Machine Learning Approach",
      "authors": [
        "Pooja Guhan",
        "Naman Awasthi",
        "Kathryn Mcdonald",
        "Kristin Bussell",
        "Dinesh Manocha",
        "Gloria Reeves",
        "Aniket Bera"
      ],
      "year": "2023",
      "venue": "Developing an Effective and Automated Patient Engagement Estimator for Telehealth: A Machine Learning Approach",
      "arxiv": "arXiv:2011.08690[cs.CV"
    },
    {
      "citation_id": "14",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "JÃ¼rgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "15",
      "title": "Automatic engagement recognition for distance learning systems: A literature study of engagement datasets and methods",
      "authors": [
        "Nur Shofiyati",
        "Shinobu Karimah",
        "Hasegawa"
      ],
      "year": "2021",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "16",
      "title": "Prediction and localization of student engagement in the wild",
      "authors": [
        "Amanjot Kaur",
        "Aamir Mustafa",
        "Love Mehta",
        "Abhinav Dhall"
      ],
      "year": "2018",
      "venue": "Digital Image Computing: Techniques and Applications (DICTA). IEEE"
    },
    {
      "citation_id": "17",
      "title": "Detection of social signals for recognizing engagement in human-robot interaction",
      "authors": [
        "Divesh Lala",
        "Koji Inoue",
        "Pierrick Milhorat",
        "Tatsuya Kawahara"
      ],
      "year": "2017",
      "venue": "Detection of social signals for recognizing engagement in human-robot interaction",
      "arxiv": "arXiv:1709.10257[cs.HC"
    },
    {
      "citation_id": "18",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "Kuei Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "19",
      "title": "Emotion separation and recognition from a facial expression by generating the poker face with vision transformers",
      "authors": [
        "Jia Li",
        "Jiantao Nie",
        "Dan Guo",
        "Richang Hong",
        "Meng Wang"
      ],
      "year": "2022",
      "venue": "Emotion separation and recognition from a facial expression by generating the poker face with vision transformers",
      "arxiv": "arXiv:2207.11081"
    },
    {
      "citation_id": "20",
      "title": "MultiMediate'24: Multi-Domain Engagement Estimation",
      "authors": [
        "Philipp MÃ¼ller",
        "Michal Balazia",
        "Tobias Baur",
        "Michael Dietz",
        "Alexander Heimerl",
        "Anna Penzkofer",
        "Dominik Schiller",
        "FranÃ§ois BrÃ©mond"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia",
      "doi": "10.1145/3664647.3689004"
    },
    {
      "citation_id": "21",
      "title": "Detecting low rapport during natural interactions in small groups from non-verbal behaviour",
      "authors": [
        "Philipp MÃ¼ller",
        "Michael Huang",
        "Andreas Bulling"
      ],
      "year": "2018",
      "venue": "Proceedings of the 23rd International Conference on Intelligent User Interfaces"
    },
    {
      "citation_id": "22",
      "title": "MultiMediate: Multi-modal Group Behaviour Analysis for Artificial Mediation",
      "authors": [
        "Philipp MÃ¼ller",
        "Dominik Schiller",
        "Dominike Thomas",
        "Guanhua Zhang",
        "Michael Dietz",
        "Patrick Gebhard",
        "Elisabeth AndrÃ©",
        "Andreas Bulling"
      ],
      "year": "2021",
      "venue": "Proc. ACM Multimedia (MM)",
      "doi": "10.1145/3474085.3479219"
    },
    {
      "citation_id": "23",
      "title": "Representation Learning and Identity Adversarial Training for Facial Behavior Understanding",
      "authors": [
        "Mang Ning",
        "Albert Ali Salah",
        "Itir Onal"
      ],
      "year": "2024",
      "venue": "Representation Learning and Identity Adversarial Training for Facial Behavior Understanding",
      "arxiv": "arXiv:2407.11243"
    },
    {
      "citation_id": "24",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "25",
      "title": "Video-Based Student Engagement Estimation via Time Convolution Neural Networks for Remote Learning",
      "authors": [
        "Khaled Saleh",
        "Kun Yu",
        "Fang Chen"
      ],
      "year": "2022",
      "venue": "Australasian Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "26",
      "title": "Automatic analysis of affective postures and body motion to detect engagement with a game companion",
      "authors": [
        "Jyotirmay Sanghvi",
        "Ginevra Castellano",
        "Iolanda Leite",
        "AndrÃ© Pereira",
        "Peter Mcowan",
        "Ana Paiva"
      ],
      "year": "2011",
      "venue": "Proceedings of the 6th international conference on Human-robot interaction"
    },
    {
      "citation_id": "27",
      "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "authors": [
        "Hao Tan",
        "Mohit Bansal"
      ],
      "year": "2019",
      "venue": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "arxiv": "arXiv:1908.07490[cs.CL"
    },
    {
      "citation_id": "28",
      "title": "COLD fusion: Calibrated and ordinal latent distribution fusion for uncertainty-aware multimodal emotion recognition",
      "authors": [
        "Mani Kumar Tellamekala",
        "Shahin Amiriparian",
        "W BjÃ¶rn",
        "Elisabeth Schuller",
        "Timo AndrÃ©",
        "Michel Giesbrecht",
        "Valstar"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "29",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "30",
      "title": "MultiMediate 2023: Engagement Level Detection using Audio and Video Features",
      "authors": [
        "Chunxi Yang",
        "Kangzhong Wang",
        "Peter Chen",
        "Michael Cheung",
        "Youqian Zhang",
        "Eugene Fu",
        "Grace Ngai"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia",
      "doi": "10.1145/3581783.3612873"
    },
    {
      "citation_id": "31",
      "title": "Sliding Window Seq2seq Modeling for Engagement Estimation",
      "authors": [
        "Jun Yu",
        "Keda Lu",
        "Mohan Jing",
        "Ziqi Liang",
        "Bingyuan Zhang",
        "Jianqing Sun",
        "Jiaen Liang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "Sliding Window Seq2seq Modeling for Engagement Estimation",
      "authors": [
        "Jun Yu",
        "Keda Lu",
        "Mohan Jing",
        "Ziqi Liang",
        "Bingyuan Zhang",
        "Jianqing Sun",
        "Jiaen Liang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia",
      "doi": "10.1145/3581783.3612852"
    },
    {
      "citation_id": "33",
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "arxiv": "arXiv:1707.07250[cs.CL"
    },
    {
      "citation_id": "34",
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "35",
      "title": "Estimation of learners' engagement using face and body features by transfer learning",
      "authors": [
        "Xianwen Zheng",
        "Shinobu Hasegawa",
        "Minh-Tuan Tran",
        "Koichi Ota",
        "Teruhiko Unoki"
      ],
      "year": "2021",
      "venue": "International conference on human-computer interaction"
    }
  ]
}