{
  "paper_id": "2107.05677v1",
  "title": "Codified Audio Language Modeling Learns Useful Representations For Music Information Retrieval",
  "published": "2021-07-12T18:28:50Z",
  "authors": [
    "Rodrigo Castellon",
    "Chris Donahue",
    "Percy Liang"
  ],
  "keywords": [
    "Rodrigo Castellon",
    "Chris Donahue",
    "Percy Liang",
    "\"Codified audio language modeling learns useful representations for music information retrieval\"",
    "in Proc. of the 22nd Int. Society for Music Information Retrieval Conf.",
    "Online",
    "2021"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We demonstrate that language models pre-trained on codified (discretely-encoded) music audio learn representations that are useful for downstream MIR tasks. Specifically, we explore representations from Jukebox [1]: a music generation system containing a language model trained on codified audio from 1M songs. To determine if Jukebox's representations contain useful information for MIR, we use them as input features to train shallow models on several MIR tasks. Relative to representations from conventional MIR models which are pre-trained on tagging, we find that using representations from Jukebox as input features yields 30% stronger performance on average across four MIR tasks: tagging, genre classification, key detection, and emotion recognition. For key detection, we observe that representations from Jukebox are considerably stronger than those from models pre-trained on tagging, suggesting that pre-training via codified audio language modeling may address blind spots in conventional approaches. We interpret the strength of Jukebox's representations as evidence that modeling audio instead of tags provides richer representations for MIR. * Equal contribution 1 MIR has a broad definition, but in this paper \"MIR\" refers specifically to making discriminative predictions on music audio.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "It is conventional in MIR 1 to pre-train models on large labeled datasets for one or more tasks (commonly tagging), and reuse the learned representations for different downstream tasks  [2] [3] [4] [5] [6] [7] [8] [9] [10] . Such transfer learning approaches decrease the amount of labeled data needed to perform well on downstream tasks, which is particularly useful in MIR where labeled data for many important tasks is scarce  [11, 12] . Historically-speaking, improvement on downstream tasks is enabled by finding ever-larger sources of labels for pre-training-in chronological order: tags  [3] , metadata  [5, 7, 9, 10] , and recently, co-listening data  [9] . However, it stands to reason that directly modeling music audio (as opposed to labels) could yield richer repre-sentations. Recently, contrastive learning  [13]  has been proposed as an MIR pre-training strategy which learns representations from audio  [14] , but this paradigm has yet to exceed the performance of label-based pre-trained models on downstream tasks.\n\nOutside of the discriminative MIR landscape, a recent system called Jukebox  [1]  demonstrated promising performance for generating music audio. To achieve this result, Jukebox leverages recent architectural developments from natural language processing (NLP) by codifying audio-encoding high-rate continuous audio waveforms into lower-rate discrete sequences which can be fed in directly to NLP models. Specifically, Jukebox trains a Transformer  [15, 16]  language model, an autoregressive generative model, on codified audio from 1M songs. Purely for convenience, we refer to Jukebox's training procedure as codified audio language modeling (CALM).\n\nWhile Jukebox already demonstrates that CALM is useful for music generation, in this work we demonstrate that CALM is also useful as a pre-training procedure for discriminative MIR tasks. To this end, we repurpose Jukebox for MIR by first using it to extract audio feature representations, and then training shallow models (probes  [18, 19] ) on downstream tasks using these features as input (Figure  1 ). Relative to representations from models pre-trained with tagging, we find that representations from Jukebox are 30% more effective on average when used to train probes on four downstream MIR tasks: tagging, genre classification, key detection, and emotion recognition. We also observe that representations from Jukebox are much more useful for key detection than those from models pre-trained on tagging, which suggests that CALM pre-training may be particularly beneficial for tasks which have little to do with tagging. This simple setup of training shallow models on representations from Jukebox is even competitive with purpose-built state-of-the-art methods on several tasks.\n\nTo facilitate reproducibility and encourage further investigation of these representations and tasks  [11] , we release all of our code for this project, alongside images for Docker containers which provide full provenance for our experiments. 2 We note that, while CALM pre-training at the scale of Jukebox requires substantial computational resources, our post hoc experiments with Jukebox only require a single commodity GPU with 12 GB memory.  Conventional MIR pre-training (left) trains convolutional neural networks on audio spectrograms using manually-annotated labels from tagging datasets. In contrast, CALM MIR pre-training (middle) involves training a language model on codified audio, which has been previously explored for music generation  [17, 1] -here, we propose to use it for discriminative MIR tasks. To determine if CALM pre-training is effective for MIR, we probe for information about particular MIR tasks (right) in resultant representations. Specifically, we extract features from the learned language model for the audio in small, task-specific labeled datasets, and use these features to train shallow probing models on each task.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Calm Pre-Training",
      "text": "CALM was first proposed by van den Oord et al. and used for unconditional speech generation  [20] . As input, CALM takes a collection of raw audio waveforms (and optionally, conditioning metadata), and learns a distribution p(audio | metadata). To this end, CALM adopts a threestage approach: (1) codify a high-rate continuous audio signal into lower-rate discrete codes, (2) train a language model on the resulting codified audio and optional metadata, i.e., learn p(codified audio | metadata), and (3) decode sequences generated by the language model to raw audio. 3 The original paper  [20]  also proposed a strategy for codifying audio called the vector-quantized variational auto-encoder (VQ-VAE), and the language model was a WaveNet  [21] . Within music, CALM was first used by Dieleman et al. for unconditional piano music generation  [17] , and subsequently, Dhariwal et al. used CALM to build a music generation system called Jukebox  [1]  with conditioning on genre, artist, and optionally, lyrics.\n\nDespite promising results on music audio generation, CALM has not yet been explored as a pre-training strategy for discriminative MIR. We suspect that effective music audio generation necessitates intermediate representations that would also contain useful information for MIR. This hypothesis is further motivated by an abundance of previous work in NLP suggesting that generative and selfsupervised pre-training can yield powerful representations for discriminative tasks  [22] [23] [24] [25] .\n\nTo explore this potential, we repurpose Jukebox for MIR. While Jukebox was designed only for generation, its internal language model was trained on codified audio from a corpus of 1.2M songs from many genres and 3 This third stage is not necessary for transfer learning. artists, making its representations potentially suitable for a multitude of downstream MIR tasks. Jukebox consists of two components-the first is a small (2M parameters) VQ-VAE model  [20]  that learns to codify high-rate (44.1 kHz), continuous audio waveforms into lower-rate (∼345 Hz), discrete code sequences with a vocabulary size of 2048 (11 bits). The second component is a large (5B parameters) language model that learns to generate codified audio using a Transformer decoder-an architecture originally designed for modeling natural language  [15, 16] . By training on codified audio (as in  [17, 1] ) instead of raw audio (as in  [21, 16] ), language models are (empirically) able to learn longer-term structure in music, while simultaneously using significantly less memory to model the same amount of audio.\n\nLike conventional MIR models which pre-train on tagging and/or metadata, Jukebox also makes use of genre and artist labels during training, providing them as conditioning information to allow for increased user control over the music generation process. Hence, while CALM in general is an unsupervised strategy that does not require labels, transfer learning from Jukebox specifically should not be considered an unsupervised approach (especially for downstream tasks like genre detection). However, by modeling the audio itself instead of modeling the labels (as in conventional MIR pre-training), we hypothesize that Jukebox learns richer representations for MIR tasks than conventional strategies.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Extracting Suitable Representations From Jukebox",
      "text": "Here we describe how we extract audio representations from Jukebox which are suitable as input features for  training shallow models. While several pre-trained Jukebox models exist with different sizes and conditioning information, here we use the 5B-parameter model without lyrics conditioning (named \"5b\"), which is a sparse transformer  [15, 16]  containing 72 layers. Each layer yields 4800-dimensional activations for each element in the codified audio sequence, i.e., approximately 345 times per second. To extract representations from this model for a particular audio waveform, we (1) resample the waveform to 44.1kHz, (2) normalize it, (3) codify it using the Jukebox VQ-VAE model, and (4) input the codified audio into the language model, interpreting its layer-wise activations as representations. Jukebox was trained on ∼24-second audio clips (codified audio sequences of length 8192)-we feed in this same amount of audio at a time when extracting representations. In addition to the genre and artist conditioning fields mentioned previously, Jukebox expects two additional fields: total song length and clip offset-to ensure that representations only depend on the input audio, we simply pass in \"unknown\" for artist and genre, one minute for song length, and zero seconds for clip offset.  4 The Jukebox language model yields an unwieldy amount of data-for every 24-second audio clip, it emits 24 × 345 × 72 × 4800 numbers, i.e., over 10GB if stored naively as 32-bit floating point. We reduce the amount of data by mean pooling across time, a common strategy in MIR transfer learning  [4, 8] , which aggregates more than 10GB of activations to around 1MB (72 × 4800).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Layer Selection",
      "text": "While pooling across time dramatically reduced the dimensionality of Jukebox's outputs, training shallow classifiers on 72 × 4800 features is still computationally expensive. To further reduce the dimensionality, we use only one of the layers from Jukebox-the middle layer (36)yielding a total of 4800 features per 24 second audio clip.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task",
      "text": "Size Metrics #Out\n\nTagging  [31]  25860 AUC/AP 50 Genre classification  [32]  930 Accuracy 10 Key detection  [33]  1763 Score 24 Emotion recognition  [34]  744\n\nTable  1 . Basic information about the four tasks we consider in this work, including the size of each task-specific dataset in terms of number of labeled examples, relevant metrics for each task, and the number of model outputs required for each dataset.\n\nUnlike conventional pre-training, where the strongest representations for transfer learning typically lie at the end of the model  [26] , the strongest representations from pretrained language models tend to lie towards the middle of the network  [27] [28] [29] [30] . To confirm this observation in our context, we trained linear models using representations from different layers of Jukebox on our downstream MIR tasks-average performance indeed peaked at the middle layers (Figure  2 ). In addition to using the middle layer, we experimented with two other layer selection strategies: (1) sub-sampling layers across the network, and (2) selecting relevant layers in a task-specific fashion.  5  We found that the simplest strategy of using only the middle layer was equally effective and more computationally practical  6  than the other two layer selection strategies.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Downstream Task Descriptions",
      "text": "We select four downstream MIR tasks to constitute a benchmark for comparing different audio feature representations: (1) tagging, (2) genre classification, (3) key detection, and (4) emotion recognition. A summary of the datasets used for each task appears in Table  1 . These tasks were selected to cover a wide range of dataset sizes (744 examples for emotion recognition vs. 26k examples for tagging) and subjectivity (emotion recognition is more subjective vs. key detection is more objective). Additionally, each task has an easily-accessible dataset with standard evaluation criteria. We describe each of these tasks and metrics below.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Tagging",
      "text": "Tagging involves determining which tags from a fixed set of tags apply to a particular song. Categories of tags include genre (e.g., jazz), instrumentation (e.g., violin), emotions (e.g., happy), and characteristics (e.g., fast). There are two large datasets for tagging, which both contain human-annotated tags for 30-second clips: MagnaTa-gATune  [31]  (MTT) which contains around 26k clips, and a tagged subset of 240k clips from the Million Song Dataset  [35]  (MSD). While both datasets contain a large vocabulary of tags, typical usage involves limiting the vocabulary to the 50 most common tags in each.\n\nBecause it is the largest non-proprietary MIR dataset, MSD is commonly used for pre-training models for transfer learning. To mitigate an unfair advantage of methods which pre-train on MSD, we use MTT instead of MSD to benchmark representations on tagging performance. While both datasets are superficially similar (choosing from 50 tags for 30-second clips), their label distributions are quite different: MSD is skewed towards genre tags, while MTT is skewed towards instrumentation tags.\n\nWe use the standard (12:1:3) train, validation, and test split for MTT  [3] . Additionally, we report both common metrics (both are macro-averaged over tags as is conventional): area under the receiver operating characteristic curve (MTT AUC ), and average precision (MTT AP ).  7  We note that inconsistencies in handling unlabeled examples for past work on MTT have been observed  [36] -some work discards examples without top-50 tags during training, evaluation, or both. In this work, we do not discard any examples.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Genre Classification",
      "text": "Genre classification involves assigning the most appropriate genre from a fixed list for a given song. For this task, we report accuracy on the GTZAN dataset  [37] , which contains 30-second clips from 10 distinct genres. We adopt the \"fault-filtered\" split from  [32]  which addresses some of the reported issues with this dataset  [38] . We note that this task has a high degree of overlap with tagging, as tagging datasets typically have a number of genres within their tag vocabulary. In fact, seven of ten genres in GTZAN are present in the tag list of MSD.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Key Detection",
      "text": "Key detection involves predicting both the scale and tonic pitch class for the underlying key of a song. We investigate the Giantsteps-MTG and Giantsteps datasets  [33]  which include songs in major and minor scales for all pitch classes, i.e., a 24-way classification task. As in past work  [39] , we use the former for training and the latter for testing. Because no standard validation split exists for Giantsteps-MTG, we follow  [32]  and create an artiststratified 4:1 split for training and validation, which we include in our codebase for reproducibility. The music in this dataset is all electronic dance music, and the clips are two minutes in length. We report the typical weighted score metric for Giantsteps (GS): an accuracy measure which gives partial credit for reasonable mistakes such as predicting the relative minor key for the major ground truth  [40] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Emotion recognition involves predicting human emotional response to a song. Data is collected by asking hu-Representation Pre-training strategy Dimensions CHROMA N/A 72 MFCC N/A 120 CHOI  [4]  MSD Tagging  [3]  160 MUSICNN  [8]  MSD Tagging  [3]  4194 CLMR  [14]  Contrastive  [13]  512 JUKEBOX  [1]  CALM  [20]  4800\n\nTable  2 . Basic statistics about the six representations we examine in this work.\n\nmans to report their emotional response on a two dimensional valence-arousal plane  [41] , where valence indicates positive versus negative emotional response, and arousal indicates emotional intensity. We use the Emomusic dataset  [34] , which contains 744 clips of 45 seconds in length. We investigate the static version of this task where original time-varying annotations are averaged together to constitute a clip-level annotation. Because this dataset does not have a standard split, it is difficult to directly compare with past work. To simplify comparison going forward, we created an artist-stratified split of Emomusic, which is released in our codebase. We take the highest reported numbers from past work to characterize \"state-of-the-art\" performance, though we note that these numbers are not directly comparable to our own due to differing splits. We report the coefficient of determination between the model predictions and human annotations for arousal (Emo A ) and valence (Emo V ).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Probing Experiments",
      "text": "Here we describe our protocol for probing for information about MIR tasks in representations from Jukebox and other pre-trained models, i.e., measuring performance of shallow models trained on these tasks using different representations as input features. We borrow the term \"probing\" from analogous investigations in NLP  [19, 42, 43] , however such methodology is common in transfer learning for MIR [2-5, 7-10].",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Descriptions Of Representations",
      "text": "In addition to probing representations from Jukebox (an exemplar of CALM pre-training), we probe four additional representations which are emblematic of three other MIR pre-training strategies (Table  2 ). Before pre-training, handcrafted features were commonplace in MIR-as archetypal examples, we probe constant-Q chromagrams (CHROMA) and Mel-frequency cepstral coefficients (MFCC), extracted with librosa  [49]  using the default settings. As in  [4] , we concatenate the mean and standard deviation across time of both the features and their first-and secondorder discrete differences. We also probe two examples of the current conventional paradigm which pre-trains on tagging using MSD: a convolutional model proposed by  Choi et     3 ) key detection (GS), and (4) emotion recognition (Emo A /Emo V ). For all six metrics, the max score is and higher is better-see Section 4 for a full description of tasks/metrics. For each metric, the best probing-based approach and the best approach overall are bolded. We also report an average score across all four tasks; tasks with multiple evaluation metrics are averaged beforehand. On all metrics, probing JUKEBOX is more effective than probing representations from other pre-trained models. Probing JUKEBOX is competitive with task-specific state-of-the-art approaches for all tasks/metrics except key detection (GS). Note that the ordering of citations in the bottom section corresponds to respective column ordering. * indicates that past work on Emomusic evaluates on different subsets of the dataset than our work and hence numbers are not directly comparable-see Section 4.4 for details.\n\npare to a recently-proposed strategy for MIR pre-training called contrastive learning of musical representations  [14]  (CLMR), though we note that the only available pretrained model from this work was trained on far less audio (a few thousand songs) than the other pre-trained models (CHOI, MUSICNN, and JUKEBOX).\n\nAll of these strategies operate at different frame rates, i.e., they produce a different number of representation vectors for a fixed amount of input audio. To handle this, we follow common practice of mean pooling representations across time  [4, 8] . While CHROMA, MFCC, and CLMR produce a single canonical representation per frame, we note that the other three produce multiple representations per frame, i.e., the outputs of individual layers in each model. For CHOI, we concatenate all layer representations together, which was shown to have strong performance on all downstream tasks in  [4] . For MUSICNN, we concatenate together the mean and max pool of threesecond windows (before mean pooling across these windows), i.e., the default configuration for that approach. For JUKEBOX, we use the middle layer of the network as motivated in Section 3.1. By using a single layer, we also mitigate the potential of a superficial dimensionality advantage for JUKEBOX, as this induces a dimensionality similar to that of MUSICNN (4800 and 4194 respectively; see Table  2 ).\n\nUnlike other representations which operate on short context windows, CHOI and JUKEBOX were trained on long windows of 29 seconds and 24 seconds of audio respectively. Accordingly, for the three datasets with short clips (tagging, genre classification, and emotion recognition all have clips between 30 and 45 seconds in length), we adopt the policy from  [4]  and simply truncate the clips to the first window when computing representations for CHOI and JUKEBOX. Because clips from the key detection dataset are much longer (two minutes), we split the clips into 30-second windows for all methods and train probes on these shorter windows. At test time, we ensemble window-level predictions into clip-level predictions before computing the score.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Probing Protocol",
      "text": "To probe representations for relevant information about downstream MIR tasks, we train shallow supervised models (linear models and one-layer MLPs) on each task using these representations as input features. As some representations may require different hyperparameter configurations for successful training, we run a grid search over the following hyperparameters (216 total configurations) for each representation and task (24 total grid searches), using early stopping based on task-specific metrics computed on the validation set of each task:\n\n• Feature standardization: {off, on} • Model: {Linear, one-layer MLP with 512 hidden units} • Batch size: {64, 256} • Learning rate: {1e-5, 1e-4, 1e-3} • Dropout probability: {0.25, 0.5, 0.75} • L2 regularization: {0, 1e-4, 1e-3} While we use this same hyperparameter grid for all tasks, the learning objective varies by task (cross-entropy for genre classification and key detection, independent binary cross-entropy per tag for tagging, and mean squared error for emotion recognition) as does the number of probe outputs (Table  1 ). Some tasks have multiple metrics-we early stop on MTT AUC for tagging as it is a more com-mon metric than MTT AP , and on the average of Emo A and Emo V for emotion recognition. We take the model with the best early stopping performance from each grid search and compute its performance on the task-specific test set.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Discussion",
      "text": "In Table  3 , we report performance of all representations on all tasks and metrics, as well as average performance across all tasks. Results are indicative that CALM is a promising paradigm for MIR pre-training. Specifically, we observe that probing the representations from JUKEBOX (learned through CALM pre-training) achieves an average of 69.9, which is 30% higher relative to the average of the best representation pre-trained with tagging (MUSICNN achieves an average of 53.7). Performance of JUKEBOX on all individual metrics is also higher than that of any other representation. Additionally, JUKEBOX achieves an average performance that is 38% higher than that of CLMR. Representations from all pre-trained models outperform hand-crafted features (CHROMA and MFCC) on average. Note that these results are holistic comparisons across different model architectures, model sizes, and amounts of pre-training data (e.g., CLMR was trained on far less data than JUKEBOX), and hence not sufficient evidence to claim that CALM is the \"best\" music pre-training strategy in gen-\n\nWe also observe that JUKEBOX contains substantially more information relevant for key detection than other representations. While CHROMA (spectrogram projected onto musical pitch classes) contains information relevant to key detection by design, all other representations besides JUKEBOX yield performance on par with that of a majority classifier (outputting \"F minor\" for every example scores 15.0)-hence, these representations contain almost no information about this task. For models pre-trained with tagging (CHOI and MUSICNN), intuition suggests that this is because none of the tags in MSD relate to key signature. For CLMR, we speculate that the use of transposition as a data augmentation strategy also results in a model that contains little useful information about key signature. While tagging and CLMR were not designed with the intention of supporting transfer to key detection, we argue that it is generally desirable to have a unified music representation which performs well on a multitude of downstream MIR tasks. Hence, we interpret the comparatively stronger performance of JUKEBOX on key detection as evidence that CALM pre-training addresses blind spots present in other MIR pre-training paradigms.\n\nIn the bottom section of Table  3 , we also report state-ofthe-art performance for purpose-built methods on all tasks, which is further broken down by models which use any form of pre-training (including pre-training on additional task-specific data as in  [47] ) vs. ones that are trained from scratch. Surprisingly, we observe that probing JUKEBOX is competitive with state-of-the-art for all tasks except for key detection, and achieves an average only 4% lower relative to that of state-of-the-art. On tagging, probing JUKEBOX achieves similar MTT AUC to a strategy which pre-trains on a proprietary dataset of 10M songs using supervision  [9] . We interpret the strong performance of this simple probing setup as evidence that CALM pre-training is a promising path towards models that are useful for many MIR tasks.\n\nWe believe that CALM pre-training is promising for MIR not just because of the strong performance of an existing pre-trained model (Jukebox), but also because there are numerous avenues which may yield further improvements for those with the data and computational resources to explore them. Firstly, CALM could be scaled up to pre-train even larger models on more data (Jukebox was trained on 1M songs, while Spotify has an estimated 70M songs in its catalog). In  [50] , it is observed that increasing model and dataset size yields predictable improvements to cross-entropy for language modeling in NLP, an insight which may also hold for CALM pre-training for MIR. Secondly, we anticipate that fine-tuning a model pre-trained with CALM would outperform our probing setup. Finally, taking a cue from related findings in NLP, we speculate that CALM pre-training with a bidirectional model and masked language modeling (as in BERT  [23] ) would outperform the generative setup of Jukebox (that of GPT  [51] ).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Related Work",
      "text": "Transfer learning has been an active area of study in MIR for over a decade. An early effort seeking to replace hand-crafted features used neural networks to automatically extract context-independent features from unlabeled audio  [52]  and used those features for a supervised learning task. Other early efforts focused on learning shared embedding spaces between audio and metadata  [53, 2]  or directly using outputs from pre-trained tagging models for music similarity judgements  [54] .\n\nThe predominant strategy for MIR pre-training using large tagging datasets was first proposed by van den Oord et al. 2014  [3] . This work pre-trained deep neural networks on MSD and demonstrated promising performance on other tagging and genre classification tasks. Choi et al. 2017  [4]  pre-trained on MSD but using a convolutional neural network and also explored a more diverse array of downstream tasks-we use their pre-trained model as one of our baselines. More recent improvements use the same approach with different architectures  [6, 8] , the latest of which is another one of our baselines.\n\nOther strategies for MIR transfer learning have been proposed. Some work pre-trains on music metadata (e.g., artist, album) instead of tags  [5, 7] . In contrast to the manual annotations required for tagging-based pre-training, metadata is much cheaper to obtain, but performance of pre-training on metadata is comparable to that of pretraining on tagging. Kim et al. 2020  [10]  improve over  Choi et al. 2017 [4]  using a multi-task approach that pretrains on both tags and metadata. Huang et al.  [9]  demonstrate that metadata can be combined with proprietary colistening data for pre-training on 10M songs to achieve state-of-the-art performance on MTT-probing representations from CALM pre-training on 1M songs achieves comparable performance on MTT (Table  3 ). Finally, con-trastive learning  [13]  has been proposed as a strategy for MIR pre-training  [55, 56, 14] -we compare to such a model from Spijkervet and Burgoyne 2021  [14] .\n\nWhile CALM has not previously been explored for MIR transfer learning, it has been explored for other purposes. van den Oord et al. 2017  [20]  first proposed CALM and used it for unconditional speech generation. Variations of CALM have been used as pre-training for speech recognition  [57, 58]  and urban sound classification  [59] . CALM has also been explored for music generation  [17, 1] . CALM is related to past work on language modeling of raw (i.e., not codified) waveforms  [21, 60, 61] , which tends to be less effective for capturing long-term dependencies compared to modeling codified audio. Language models have also been used extensively for modeling symbolic music  [62] [63] [64] , including some work on pre-training on large corpora of scores for transfer learning  [65, 66] .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this work we demonstrated that CALM is a promising pre-training strategy for MIR. Compared to conventional approaches, CALM learns richer representations by modeling audio instead of labels. Moreover, CALM allows MIR researchers to repurpose NLP methodologyhistorically, repurposing methodology from another field (computer vision) has provided considerable leverage for MIR. Finally, CALM suggests a direction for MIR research where enormous models pre-trained on large music catalogs break new ground on MIR tasks, analogous to ongoing paradigm shifts in other areas of machine learning.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Conventional MIR pre-training (left) trains convolutional neural networks on audio spectrograms using",
      "page": 2
    },
    {
      "caption": "Figure 2: Normalized validation performance of linear",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "6 7\n…\n8 9 7 2 0 0 6 5",
          "Column_3": "0 6 5 2 5 2 0 6"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Basic information about the four tasks we con-",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "Ta",
          "Column_4": "gging (",
          "Column_5": "Magnat",
          "Column_6": "agatune",
          "Column_7": ")",
          "Column_8": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "G\nE",
          "Column_4": "enre cla\nmotion r",
          "Column_5": "ssificati\necogniti",
          "Column_6": "on (GTZ\non (Em",
          "Column_7": "AN)\nomusic)",
          "Column_8": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "Ke\nAv",
          "Column_4": "y detec\nerage",
          "Column_5": "tion (Gi",
          "Column_6": "antstep",
          "Column_7": "s)",
          "Column_8": ""
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Jukebox: A generative model for music",
      "authors": [
        "P Dhariwal",
        "H Jun",
        "C Payne",
        "J Kim",
        "A Radford",
        "I Sutskever"
      ],
      "year": "2020",
      "venue": "Jukebox: A generative model for music",
      "arxiv": "arXiv:2005.00341"
    },
    {
      "citation_id": "2",
      "title": "Transfer learning in MIR: Sharing learned latent representations for music audio classification and similarity",
      "authors": [
        "P Hamel",
        "M Davies",
        "K Yoshii",
        "M Goto"
      ],
      "year": "2013",
      "venue": "Transfer learning in MIR: Sharing learned latent representations for music audio classification and similarity"
    },
    {
      "citation_id": "3",
      "title": "Transfer learning by supervised pre-training for audio-based music classification",
      "authors": [
        "A Van Den Oord",
        "S Dieleman",
        "B Schrauwen"
      ],
      "year": "2014",
      "venue": "ISMIR"
    },
    {
      "citation_id": "4",
      "title": "Transfer learning for music classification and regression tasks",
      "authors": [
        "K Choi",
        "G Fazekas",
        "M Sandler",
        "K Cho"
      ],
      "year": "2017",
      "venue": "ISMIR"
    },
    {
      "citation_id": "5",
      "title": "Representation learning of music using artist labels",
      "authors": [
        "J Park",
        "J Lee",
        "J Park",
        "J.-W Ha",
        "J Nam"
      ],
      "year": "2017",
      "venue": "Representation learning of music using artist labels",
      "arxiv": "arXiv:1710.06648"
    },
    {
      "citation_id": "6",
      "title": "SampleCNN: End-to-end deep convolutional neural networks using very small filters for music classification",
      "authors": [
        "J Lee",
        "J Park",
        "K Kim",
        "J Nam"
      ],
      "year": "2018",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "7",
      "title": "Representation learning of music using artist, album, and track information",
      "authors": [
        "J Lee",
        "J Park",
        "J Nam"
      ],
      "year": "2019",
      "venue": "Representation learning of music using artist, album, and track information",
      "arxiv": "arXiv:1906.11783"
    },
    {
      "citation_id": "8",
      "title": "musicnn: Pre-trained convolutional neural networks for music audio tagging",
      "authors": [
        "J Pons",
        "X Serra"
      ],
      "year": "2019",
      "venue": "ISMIR Late-breaking Demos"
    },
    {
      "citation_id": "9",
      "title": "Large-scale weaklysupervised content embeddings for music recommendation and tagging",
      "authors": [
        "Q Huang",
        "A Jansen",
        "L Zhang",
        "D Ellis",
        "R Saurous",
        "J Anderson"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "10",
      "title": "One deep music representation to rule them all? A comparative analysis of different representation learning strategies",
      "authors": [
        "J Kim",
        "J Urbano",
        "C Liem",
        "A Hanjalic"
      ],
      "year": "2020",
      "venue": "One deep music representation to rule them all? A comparative analysis of different representation learning strategies"
    },
    {
      "citation_id": "11",
      "title": "Open-source practices for music signal processing research: Recommendations for transparent, sustainable, and reproducible audio research",
      "authors": [
        "B Mcfee",
        "J Kim",
        "M Cartwright",
        "J Salamon",
        "R Bittner",
        "J Bello"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "12",
      "title": "Data usage in MIR: history & future recommendations",
      "authors": [
        "W Chen",
        "J Keast",
        "J Moody",
        "C Moriarty",
        "F Villalobos",
        "V Winter",
        "X Zhang",
        "X Lyu",
        "E Freeman",
        "J Wang"
      ],
      "year": "2019",
      "venue": "ISMIR"
    },
    {
      "citation_id": "13",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "ICML"
    },
    {
      "citation_id": "14",
      "title": "Contrastive learning of musical representations",
      "authors": [
        "J Spijkervet",
        "J Burgoyne"
      ],
      "year": "2021",
      "venue": "Contrastive learning of musical representations",
      "arxiv": "arXiv:2103.09410"
    },
    {
      "citation_id": "15",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need",
      "arxiv": "arXiv:1706.03762"
    },
    {
      "citation_id": "16",
      "title": "Generating long sequences with sparse transformers",
      "authors": [
        "R Child",
        "S Gray",
        "A Radford",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "Generating long sequences with sparse transformers",
      "arxiv": "arXiv:1904.10509"
    },
    {
      "citation_id": "17",
      "title": "The challenge of realistic music generation: modelling raw audio at scale",
      "authors": [
        "S Dieleman",
        "A Van Den Oord",
        "K Simonyan"
      ],
      "year": "2018",
      "venue": "NIPS"
    },
    {
      "citation_id": "18",
      "title": "Understanding intermediate layers using linear classifier probes",
      "authors": [
        "G Alain",
        "Y Bengio"
      ],
      "year": "2016",
      "venue": "Understanding intermediate layers using linear classifier probes",
      "arxiv": "arXiv:1610.01644"
    },
    {
      "citation_id": "19",
      "title": "Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure",
      "authors": [
        "D Hupkes",
        "S Veldhoen",
        "W Zuidema"
      ],
      "year": "2018",
      "venue": "Journal of Artificial Intelligence Research"
    },
    {
      "citation_id": "20",
      "title": "Neural discrete representation learning",
      "authors": [
        "A Van Den Oord",
        "O Vinyals",
        "K Kavukcuoglu"
      ],
      "year": "2017",
      "venue": "Neural discrete representation learning",
      "arxiv": "arXiv:1711.00937"
    },
    {
      "citation_id": "21",
      "title": "WaveNet: A generative model for raw audio",
      "authors": [
        "A Van Den Oord",
        "S Dieleman",
        "H Zen",
        "K Simonyan",
        "O Vinyals",
        "A Graves",
        "N Kalchbrenner",
        "A Senior",
        "K Kavukcuoglu"
      ],
      "year": "2016",
      "venue": "WaveNet: A generative model for raw audio",
      "arxiv": "arXiv:1609.03499"
    },
    {
      "citation_id": "22",
      "title": "Deep contextualized word representations",
      "authors": [
        "M Peters",
        "M Neumann",
        "M Iyyer",
        "M Gardner",
        "C Clark",
        "K Lee",
        "L Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Deep contextualized word representations"
    },
    {
      "citation_id": "23",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "BERT: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "24",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI Blog"
    },
    {
      "citation_id": "25",
      "title": "GPT understands, too",
      "authors": [
        "X Liu",
        "Y Zheng",
        "Z Du",
        "M Ding",
        "Y Qian",
        "Z Yang",
        "J Tang"
      ],
      "year": "2021",
      "venue": "GPT understands, too",
      "arxiv": "arXiv:2103.10385"
    },
    {
      "citation_id": "26",
      "title": "Visualizing and understanding convolutional networks",
      "authors": [
        "M Zeiler",
        "R Fergus"
      ],
      "year": "2014",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "27",
      "title": "Linguistic knowledge and transferability of contextual representations",
      "authors": [
        "N Liu",
        "M Gardner",
        "Y Belinkov",
        "M Peters",
        "N Smith"
      ],
      "year": "2019",
      "venue": "Linguistic knowledge and transferability of contextual representations",
      "arxiv": "arXiv:1903.08855"
    },
    {
      "citation_id": "28",
      "title": "Generative pretraining from pixels",
      "authors": [
        "M Chen",
        "A Radford",
        "R Child",
        "J Wu",
        "H Jun",
        "D Luan",
        "I Sutskever"
      ],
      "year": "2020",
      "venue": "ICML"
    },
    {
      "citation_id": "29",
      "title": "Finding universal grammatical relations in multilingual bert",
      "authors": [
        "E Chi",
        "J Hewitt",
        "C Manning"
      ],
      "year": "2020",
      "venue": "Finding universal grammatical relations in multilingual bert"
    },
    {
      "citation_id": "30",
      "title": "A primer in BERTology: What we know about how BERT works",
      "authors": [
        "A Rogers",
        "O Kovaleva",
        "A Rumshisky"
      ],
      "year": "2020",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "31",
      "title": "Evaluation of algorithms using games: The case of music tagging",
      "authors": [
        "E Law",
        "K West",
        "M Mandel",
        "M Bay",
        "J Downie"
      ],
      "year": "2009",
      "venue": "ISMIR"
    },
    {
      "citation_id": "32",
      "title": "Deep learning and music adversaries",
      "authors": [
        "C Kereliuk",
        "B Sturm",
        "J Larsen"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Two data sets for tempo estimation and key detection in electronic dance music annotated from user corrections",
      "authors": [
        "P Knees",
        "Á Faraldo Pérez",
        "H Boyer",
        "R Vogl",
        "S Böck",
        "F Hörschläger",
        "M Goff"
      ],
      "year": "2015",
      "venue": "Two data sets for tempo estimation and key detection in electronic dance music annotated from user corrections"
    },
    {
      "citation_id": "34",
      "title": "1000 songs for emotional analysis of music",
      "authors": [
        "M Soleymani",
        "M Caro",
        "E Schmidt",
        "C.-Y Sha",
        "Y.-H Yang"
      ],
      "year": "2013",
      "venue": "ACM International Workshop on Crowdsourcing for Multimedia"
    },
    {
      "citation_id": "35",
      "title": "The Million Song Dataset",
      "authors": [
        "T Bertin-Mahieux",
        "D Ellis",
        "B Whitman",
        "P Lamere"
      ],
      "year": "2011",
      "venue": "ISMIR"
    },
    {
      "citation_id": "36",
      "title": "Evaluation of CNN-based automatic music tagging models",
      "authors": [
        "M Won",
        "A Ferraro",
        "D Bogdanov",
        "X Serra"
      ],
      "year": "2020",
      "venue": "Evaluation of CNN-based automatic music tagging models",
      "arxiv": "arXiv:2006.00751"
    },
    {
      "citation_id": "37",
      "title": "Musical genre classification of audio signals",
      "authors": [
        "G Tzanetakis",
        "P Cook"
      ],
      "year": "2002",
      "venue": "IEEE Transactions on Speech and Audio Processing"
    },
    {
      "citation_id": "38",
      "title": "The GTZAN dataset: its contents, its faults, their effects on evaluation, and its future use",
      "authors": [
        "B Sturm"
      ],
      "year": "2013",
      "venue": "The GTZAN dataset: its contents, its faults, their effects on evaluation, and its future use",
      "arxiv": "arXiv:1306.1461"
    },
    {
      "citation_id": "39",
      "title": "End-to-end musical key estimation using a convolutional neural network",
      "authors": [
        "F Korzeniowski",
        "G Widmer"
      ],
      "year": "2017",
      "venue": "European Signal Processing Conference"
    },
    {
      "citation_id": "40",
      "title": "mir_eval: A transparent implementation of common mir metrics",
      "authors": [
        "C Raffel",
        "B Mcfee",
        "E Humphrey",
        "J Salamon",
        "O Nieto",
        "D Liang",
        "D Ellis"
      ],
      "year": "2014",
      "venue": "mir_eval: A transparent implementation of common mir metrics"
    },
    {
      "citation_id": "41",
      "title": "Automated music emotion recognition: A systematic evaluation",
      "authors": [
        "A Huq",
        "J Bello",
        "R Rowe"
      ],
      "year": "2010",
      "venue": "Journal of New Music Research"
    },
    {
      "citation_id": "42",
      "title": "What you can cram into a single vector: Probing sentence embeddings for linguistic properties",
      "authors": [
        "A Conneau",
        "G Kruszewski",
        "G Lample",
        "L Barrault",
        "M Baroni"
      ],
      "year": "2018",
      "venue": "What you can cram into a single vector: Probing sentence embeddings for linguistic properties",
      "arxiv": "arXiv:1805.01070"
    },
    {
      "citation_id": "43",
      "title": "A structural probe for finding syntax in word representations",
      "authors": [
        "J Hewitt",
        "C Manning"
      ],
      "year": "2019",
      "venue": "North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "44",
      "title": "On-line continuous-time music mood regression with deep recurrent neural networks",
      "authors": [
        "F Weninger",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "ICASSP"
    },
    {
      "citation_id": "45",
      "title": "Comparison and analysis of deep audio embeddings for music emotion recognition",
      "authors": [
        "E Koh",
        "S Dubnov"
      ],
      "year": "2021",
      "venue": "Comparison and analysis of deep audio embeddings for music emotion recognition",
      "arxiv": "arXiv:2104.06517"
    },
    {
      "citation_id": "46",
      "title": "",
      "authors": [
        "Pioneer"
      ],
      "venue": ""
    },
    {
      "citation_id": "47",
      "title": "MIREX 2019 submission: Crowd annotation for audio key estimation",
      "authors": [
        "J Jiang",
        "G Xia",
        "D Carlton"
      ],
      "year": "2019",
      "venue": "MIREX"
    },
    {
      "citation_id": "48",
      "title": "Masked conditional neural networks for audio classification",
      "authors": [
        "F Medhat",
        "D Chesmore",
        "J Robinson"
      ],
      "year": "2017",
      "venue": "International Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "49",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "librosa: Audio and music signal analysis in python"
    },
    {
      "citation_id": "50",
      "title": "Scaling laws for neural language models",
      "authors": [
        "J Kaplan",
        "S Mccandlish",
        "T Henighan",
        "T Brown",
        "B Chess",
        "R Child",
        "S Gray",
        "A Radford",
        "J Wu",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Scaling laws for neural language models",
      "arxiv": "arXiv:2001.08361"
    },
    {
      "citation_id": "51",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "A Radford",
        "K Narasimhan",
        "T Salimans",
        "I Sutskever"
      ],
      "year": "2018",
      "venue": "OpenAI Blog"
    },
    {
      "citation_id": "52",
      "title": "Learning features from music audio with deep belief networks",
      "authors": [
        "P Hamel",
        "D Eck"
      ],
      "year": "2010",
      "venue": "ISMIR"
    },
    {
      "citation_id": "53",
      "title": "Multi-tasking with joint semantic spaces for large-scale music annotation and retrieval",
      "authors": [
        "J Weston",
        "S Bengio",
        "P Hamel"
      ],
      "year": "2011",
      "venue": "Journal of New Music Research"
    },
    {
      "citation_id": "54",
      "title": "From improved auto-taggers to improved music similarity measures",
      "authors": [
        "K Seyerlehner",
        "M Schedl",
        "R Sonnleitner",
        "D Hauger",
        "B Ionescu"
      ],
      "year": "2012",
      "venue": "International Workshop on Adaptive Multimedia Retrieval"
    },
    {
      "citation_id": "55",
      "title": "Learning contextual tag embeddings for cross-modal alignment of audio and tags",
      "authors": [
        "X Favory",
        "K Drossos",
        "T Virtanen",
        "X Serra"
      ],
      "year": "2020",
      "venue": "Learning contextual tag embeddings for cross-modal alignment of audio and tags",
      "arxiv": "arXiv:2010.14171"
    },
    {
      "citation_id": "56",
      "title": "Enriched music representations with multiple cross-modal contrastive learning",
      "authors": [
        "A Ferraro",
        "X Favory",
        "K Drossos",
        "Y Kim",
        "D Bogdanov"
      ],
      "year": "2021",
      "venue": "Enriched music representations with multiple cross-modal contrastive learning"
    },
    {
      "citation_id": "57",
      "title": "Effectiveness of self-supervised pre-training for speech recognition",
      "authors": [
        "A Baevski",
        "M Auli",
        "A Mohamed"
      ],
      "year": "2019",
      "venue": "Effectiveness of self-supervised pre-training for speech recognition",
      "arxiv": "arXiv:1911.03912"
    },
    {
      "citation_id": "58",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "arxiv": "arXiv:2006.11477"
    },
    {
      "citation_id": "59",
      "title": "A framework for contrastive and generative learning of audio representations",
      "authors": [
        "P Verma",
        "J Smith"
      ],
      "year": "2020",
      "venue": "A framework for contrastive and generative learning of audio representations",
      "arxiv": "arXiv:2010.11459"
    },
    {
      "citation_id": "60",
      "title": "SampleRNN: An unconditional end-to-end neural audio generation model",
      "authors": [
        "S Mehri",
        "K Kumar",
        "I Gulrajani",
        "R Kumar",
        "S Jain",
        "J Sotelo",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "ICLR"
    },
    {
      "citation_id": "61",
      "title": "Efficient neural audio synthesis",
      "authors": [
        "N Kalchbrenner",
        "E Elsen",
        "K Simonyan",
        "S Noury",
        "N Casagrande",
        "E Lockhart",
        "F Stimberg",
        "A Van Den Oord",
        "S Dieleman",
        "K Kavukcuoglu"
      ],
      "year": "2018",
      "venue": "ICML"
    },
    {
      "citation_id": "62",
      "title": "Finding temporal structure in music: Blues improvisation with LSTM recurrent networks",
      "authors": [
        "D Eck",
        "J Schmidhuber"
      ],
      "year": "2002",
      "venue": "IEEE Workshop on Neural Networks for Signal Processing"
    },
    {
      "citation_id": "63",
      "title": "Performance RNN: Generating music with expressive timing and dynamics",
      "authors": [
        "I Simon",
        "S Oore"
      ],
      "year": "2017",
      "venue": "Performance RNN: Generating music with expressive timing and dynamics"
    },
    {
      "citation_id": "64",
      "title": "Music transformer",
      "authors": [
        "C.-Z Huang",
        "A Vaswani",
        "J Uszkoreit",
        "N Shazeer",
        "I Simon",
        "C Hawthorne",
        "A Dai",
        "M Hoffman",
        "M Dinculescu",
        "D Eck"
      ],
      "year": "2019",
      "venue": "Music transformer"
    },
    {
      "citation_id": "65",
      "title": "LakhNES: Improving multi-instrumental music generation with cross-domain pre-training",
      "authors": [
        "C Donahue",
        "H Mao",
        "Y Li",
        "G Cottrell",
        "J Mcauley"
      ],
      "year": "2019",
      "venue": "LakhNES: Improving multi-instrumental music generation with cross-domain pre-training"
    },
    {
      "citation_id": "66",
      "title": "Improving automatic jazz melody generation by transfer learning techniques",
      "authors": [
        "H.-T Hung",
        "C.-Y Wang",
        "Y.-H Yang",
        "H.-M Wang"
      ],
      "year": "2019",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    }
  ]
}