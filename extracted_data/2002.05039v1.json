{
  "paper_id": "2002.05039v1",
  "title": "X-Vectors Meet Emotions: A Study On Dependencies Between Emotion And Speaker Recognition",
  "published": "2020-02-12T15:13:07Z",
  "authors": [
    "Raghavendra Pappagari",
    "Tianzi Wang",
    "Jesus Villalba",
    "Nanxin Chen",
    "Najim Dehak"
  ],
  "keywords": [
    "emotion recognition",
    "speaker verification",
    "xvector",
    "transfer learning",
    "pre-trained"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this work, we explore the dependencies between speaker recognition and emotion recognition. We first show that knowledge learned for speaker recognition can be reused for emotion recognition through transfer learning. Then, we show the effect of emotion on speaker recognition. For emotion recognition, we show that using a simple linear model is enough to obtain good performance on the features extracted from pre-trained models such as the x-vector model. Then, we improve emotion recognition performance by finetuning for emotion classification. We evaluated our experiments on three different types of datasets: IEMOCAP, MSP-Podcast, and Crema-D. By fine-tuning, we obtained 30.40%, 7.99%, and 8.61% absolute improvement on IEMOCAP, MSP-Podcast, and Crema-D respectively over baseline model with no pre-training. Finally, we present results on the effect of emotion on speaker verification. We observed that speaker verification performance is prone to changes in test speaker emotions. We found that trials with angry utterances performed worst in all three datasets. We hope our analysis will initiate a new line of research in the speaker recognition community.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In this work, we explore the dependencies between speaker recognition and speech emotion recognition (SER). Speaker verification, a more general task of speaker recognition, deals with verifying speaker identity in a pair of utterances. The goal of the SER task is to recognize the emotional state of a speaker in a speech recording. For both tasks, acoustic parameters such as pitch, fundamental frequencies, acoustic energy play a crucial role in obtaining better performance. Hence, we hypothesize that models trained to discriminate speakers can be reused for SER. Some of the applications of speaker verification include voicebased authentication, security systems, and personal assistants. SER is useful in applications such as detecting hate speech in social media, detecting patient's emotions, call routing based on emotion, actors analysis in the entertainment industry, mental health analysis, and human-machine interaction.\n\nSeveral works in the past have tried to improve SER by using various feature representations and models. In  [1, 2, 3]  feature learning from raw-waveform or spectrogram using CNN, LSTM based models is explored. In  [4, 5, 6, 7] , CNN and LSTM based models are explored from feature representations such as MFCC and OpenS-MILE  [8]  features. In  [9, 10, 11, 12] , adversarial learning paradigm is explored for robust recognition. In  [13, 14] , transfer learning approach is explored.\n\nIn this work, we follow the transfer learning approach. Our work is motivated by several previous works  [14, 15, 16] . It is shown in  [14]  that reusing an ASR model trained to predict phonemes is helpful for the SER task. In  [15] , authors studied the applicability of speaker based utterance representations such as i-vectors and x-vectors for several downstream tasks related to speech, speaker, and utterance meta information. However, they did not study for emotion-related tasks.\n\nAuthors in  [16]  show that speaker-based utterance-level representations i-vectors and x-vectors encode speaking-style information and emotion. However, their experimental setup included overlapping speakers between training and testing data splits. We believe that speaker overlap should be avoided in SER tasks, especially when using speaker-specific representations as input. In this paper, we present results using pre-trained as well as fine-tuned models which is not studied in  [16] .\n\nIn this paper, we explore transfer learning for SER task from neural networks trained to discriminate speakers such as the x-vector model. First, we show that emotion-related information is encoded in x-vectors, and then we show that fine-tuning for emotion targets further improves the performance. We use two pre-trained models for this study-one trained with augmentation and another without augmentation. We also experiment with augmenting the emotion data for better performance. Then, we present results of speaker verification on emotion datasets and show the effect of emotion on its performance, which could potentially initiate a new line of research in the speaker recognition community.\n\nThe main contributions of this work are:\n\n• Exploring pre-trained models trained to discriminate speakers for emotion tasks on 3 different types of datasets • Fine-tuned models for SER task\n\n• Results with data augmentation on emotion datasets • Analysis of the effect of emotion on speaker verification results\n\nRest of the paper is organized as follows. In Section 2, we present our method followed by experimental setup in Section 3. Then, we discuss results in Section 4 and finally in Section 5, we present conclusion and future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Our Approach",
      "text": "In this section, we present details of the x-vector model reused for the SER task. Then, we explain the transfer learning approach followed to perform SER. It is shown in the literature that i-vectors   [17] , speaker diarization  [18, 19, 20, 21] . In this work, we only exploit the x-vector model because of its superiority over i-vectors  [22]  and also because it is easy to adapt for down-stream tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "X-Vector Model",
      "text": "In this paper, we used state-of-the-art ResNet x-vector model reported in  [17]  for utterance level speaker embedding extraction. The network consisted of three parts: frame-level representation learning network, pooling network, and utterance-level classifier. Framelevel representation learning network uses ResNet-34  [23]  structure, which consists of several 2D convolutional layers with short-cut connections between them. After that, we used a multi-head attention layer to summarize the whole utterance into a large embedding. This layer takes ResNet outputs xt as input and computes its own attention scores w h,t for each head h:\n\nAttention scores w h,t are normalized along time axis. Output embedding for head h is the weighted average over its inputs:\n\nDifferent heads are designed to capture different aspects of input signal. Embedding from different heads are concatenated and projected by an affine transformation into the final embedding. From the pooling layer to output, there are two fully connected layers, and it predicts speaker identity in the training set. Angular softmax  [24]  loss was used to train the network. The whole network structure is illustrated in Table  1 . For more details, please refer to  [17] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition",
      "text": "From a pre-trained x-vector model, we can transfer knowledge to achieve SER in two ways:\n\n• Extract x-vectors and apply a simple linear model like logistic regression (LR) • Replace the speaker-discriminative output layer with emotiondiscriminative layer and fine-tune\n\nIn this paper, we show experiments with both methods. We compare these two methods with widely used OpenSMILE features. We also experiment with two versions of pre-trained x-vector models: one trained with augmentation, referred to as ResNet-aug, and another trained with only clean data, referred to as ResNet-clean.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "We validate our experiments on three different types of datasets: IEMOCAP (acted and no restriction on spoken content), MSP-Podcast (natural and no restriction on spoken content), and Crema-D (acted and restricted to 12 sentences). The details of each dataset are as follows.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iemocap",
      "text": "IEMOCAP dataset is a multimodal dyadic conversational dataset recorded with 5 female and 5 male actors  [25] . It contains conversations from 5 sessions wherein each session one male and female actor converse about a pre-defined topic. Each session is segmented into utterances manually, and each utterance is annotated by at least 3 annotators to categorize into one of 8 emotion classes (angry, happy, neutral, sad, disgust, fear, excited). Conversations are scripted and improvisational in nature.\n\nIn this work, we followed previous works in choosing data for our experiments. We combined happy and excited emotions into one class. We choose a subset of data consisting of 4 emotions: angry, sad, neutral, happy. As the number of speakers and utterances in this dataset is low, we opted for 5-fold cross-validation (CV) to obtain reliable results. As it was shown in  [15]  that speaker verification models capture session variability along with speaker characteristics; we did leave-one-session-out training for 5-fold CV to avoid overlapping of speakers and sessions between training and testing. In each fold, we used weighted f-score as our metric, and hence, we reported an average of weighted f-scores of 5-fold CV for each experiment.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Msp-Podcast Dataset",
      "text": "MSP-Podcast corpus 1  is collected from podcast recordings. The recordings were processed to remove segments with SNR less than 20dB, background music, telephone quality speech, and overlapping speech. For more information on this dataset, please refer to  [26] . In this work, we used 5 emotions: angry, happy, sad, neutral, disgust for classification as in  [27] . We used the standard splits in Release 1.4 for training, development, and testing. This dataset has 610 speakers in the training split, 30 in the development, and 50 speakers in the test split.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Crema-D Dataset",
      "text": "Crema-D dataset 2  is a multimodal dataset (audio and visual) with 91 professional actors enacting a target emotion for a pre-defined list of",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Extraction",
      "text": "In this work, we extracted 23-dim MFCC with a 10ms frame shift and 25ms frame size. We used a simple energy-based speech activity detector to remove silence segments from the utterances. Our pretrained models were trained with MFCC. For OpenSMILE features, referred to as GeMAPS, we extracted 88-dim features as suggested in  [28]  with a 10ms frame shift and 25ms frame size.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Table  2  presents results of SER task with ResNet architecture on all three datasets. As noted in Section 2.2, ResNet-clean and ResNetaug denotes unaugmented and augmented x-vector models. In the second row, Clean denotes emotion classification training is only on clean data, and Clean+aug denotes clean data is augmented with noisy data for the respective datasets. Augmentation is done with additive noise and music using MUSAN corpus  [29] .\n\nComparison of 3rd and 4th rows suggests GeMAPS perform better than MFCC in most cases, but as our pre-trained models were trained with MFCC, we did not consider GeMAPS for further experiments. Significant improvements were obtained on all the datasets by using pre-trained models compared to random initialization suggesting that pre-training is helpful. In Clean setting i.e., when using only clean data for emotion classification, pre-trained ResNet-clean performed 2.94%, 4.17% and 3.38% better than pre-trained ResNetaug on IEMOCAP, MSP-Podcast and Crema-D respectively. A similar conclusion was reported in  [15]  for tasks such as prediction of the session, utterance length, gender, etc.. Having observed the good performance of pre-trained ResNet models, which are trained to discriminate speakers, we proceeded to fine-tune the pre-trained models for emotion recognition. By fine-tuning, we obtained improvements in all cases except for a 3.03% drop on Crema-D when using ResNetclean.\n\nFrom comparison of Clean+aug column with Clean, we can observe that augmenting data for emotion classification helped on IEMOCAP and Crema-D datasets except when using ResNet-clean suggesting that it is not robust to noise. For the MSP-Podcast dataset, improvements are not clear using data augmentation. We obtained improvements with fixed pre-trained models but not when fine-tuning for emotion task.\n\nOverall, fine-tuned ResNet-aug model worked best on IEMO-CAP and Crema-D in Clean+aug setting with 70.30% and 81.54% respectively. On MSP-Podcast dataset, fine-tuned ResNet-aug worked best with 58.46% on clean training data. In terms of absolute improvement over randomly initialized ResNet (MFCC) baseline, we obtained 30.40%, 7.99%, and 8.61% on IEMOCAP, MSP-Podcast, and Crema-D, respectively. It is difficult to compare our results with previous works as there are no standard splits for IEMOCAP and Crema-D. In the case of MSP-Podcast, the dataset collection is an ongoing effort, and we did not find previous works on the current release yet.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speaker Verification",
      "text": "In this section, we show the effect of emotion on the performance of the speaker verification system. For this experiment, we have formed speaker verification trials by comparing every utterance against each EERs on IEMOCAP are very high because utterances are very short and because of domain mismatch. Histogram of IEMOCAP dataset utterance duration is presented in Fig.  1(b ). The majority of the utterances in the dataset are less than 4 seconds. EERs for MSP-Podcast dataset are better than IEMOCAP but still above 10%, which can be attributed to the short utterances in the dataset. Histogram of MSP-Podcast dataset utterance duration is presented in Fig.  1 (a). It can be observed that most utterances are short but longer than IEMOCAP. Even though utterances in Crema-D are shorter than IEMOCAP (see Fig.  1(c )), EERs are better for the former, which could be because phonetic content variability limited to only 12 sentences. For comparison, the authors in  [30]  report EER increasing from 2.5% to more than 20% when going from full-length recordings to 5sec versus 5sec trials on NIST 2010 corpora.\n\nAlso, it should be noted that EERs are worse when the test utterance emotion is different from enroll utterance emotion, suggesting that speaker verification systems are sensitive to change of emotion. It could be a very serious problem in real scenarios because humans can easily change their emotions according to the situation. In sameemotion trials, Neutral vs. Neutral performed best on IEMOCAP and MSP-Podcast while the same pair performed worst in Crema-D. Sad vs. Sad is best on Crema-D. Angry vs. Angry on IEMOCAP and Happy vs. Happy on MSP-Podcast are the worst same-emotion trials. In cross-emotion trials, Angry vs. Sad/Neutral is the worst performing emotion pair on IEMOCAP, Angry vs. Happy on MSP-Podcast and Angry vs. Sad on Crema-D. It can be observed that emotion Angry is common in worst performing cross-emotion trials across datasets. Except on MSP-Podcast, all cross-emotion trails performed worst compared to same-emotion trails.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In this work, we study the connections between speaker recognition and emotion recognition. We first show that emotion recognition performance can be improved using speaker recognition models Table  5 : EER for Speaker Verification on Crema-D such as x-vectors through transfer learning. Then, we show the effect of emotion on speaker verification performance. For emotion recognition, we observed that features extracted from pre-trained models performed better than the features curated for emotion recognition tasks such as GeMAPS. We noticed that the unaugmented x-vector model features perform better than the augmented x-vector model features for emotion recognition. Best emotion recognition performance on all 3 datasets is obtained by fine-tuning the pre-trained x-vector models. Data augmentation for emotion classification provided consistent improvements on 2 out of 3 datasets. We observed that the unaugmented x-vector model is not robust to noise. In terms of absolute improvement, we obtained 30.40%, 7.99%, and 8.61% on IEMOCAP, MSP-Podcast, and Crema-D, respectively, over the baseline model with no pre-training. Finally, analysis of the effect of emotion on speaker verification models revealed that the latter is highly sensitive to change in the emotion of test speakers. We observed that same-emotion trials perform better than cross-emotion trials. Among worst-performing cross-emotion trials, angry was common across all datasets. As part of future work, we will focus on emotion-invariant speaker verification models. We hope our work will initiate a new line of research in the speaker recognition community.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Histograms of utterance durations",
      "page": 3
    },
    {
      "caption": "Figure 1: (b). The majority",
      "page": 4
    },
    {
      "caption": "Figure 1: (a). It can be observed that most utterances are short but longer",
      "page": 4
    },
    {
      "caption": "Figure 1: (c)), EERs are better for the former, which",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Component": "Frame-level\nRepresentation\nLearning",
          "Layer": "7 × 7, 16",
          "Output Size": "T × 23"
        },
        {
          "Component": "",
          "Layer": "3 × 3, 16\n× 3\n3 × 3, 16(cid:21)\n(cid:20)",
          "Output Size": "T × 23"
        },
        {
          "Component": "",
          "Layer": "3 × 3, 32\n× 4, stride 2\n(cid:20)\n3 × 3, 32(cid:21)",
          "Output Size": "T2\n× 12"
        },
        {
          "Component": "",
          "Layer": "3 × 3, 64\n× 6, stride 2\n(cid:20)\n3 × 3, 64(cid:21)",
          "Output Size": "T4\n× 6"
        },
        {
          "Component": "",
          "Layer": "3 × 3, 128\n× 3, stride 2\n3 × 3, 128(cid:21)\n(cid:20)",
          "Output Size": "T8\n× 3"
        },
        {
          "Component": "",
          "Layer": "average pool 1 × 3",
          "Output Size": "T8"
        },
        {
          "Component": "Pooling",
          "Layer": "32 heads attention",
          "Output Size": "32 × 128"
        },
        {
          "Component": "Utterance-level\nClassiﬁer",
          "Layer": "FC",
          "Output Size": "400"
        },
        {
          "Component": "",
          "Layer": "FC",
          "Output Size": "#spk:12,872"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "2",
      "title": "Emotion identification from raw speech signals using dnns",
      "authors": [
        "Mousmita Sarma",
        "Pegah Ghahremani",
        "Daniel Povey",
        "Kumar Nagendra",
        "Kandarpa Goel",
        "Najim Sarma",
        "Dehak"
      ],
      "year": "2018",
      "venue": "Emotion identification from raw speech signals using dnns"
    },
    {
      "citation_id": "3",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "George Trigeorgis",
        "Fabien Ringeval",
        "Raymond Brueckner",
        "Erik Marchi",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "4",
      "title": "Deep neural networks for emotion recognition combining audio and transcripts",
      "authors": [
        "Jaejin Cho",
        "Raghavendra Pappagari",
        "Purva Kulkarni",
        "Jesús Villalba",
        "Yishay Carmiel",
        "Najim Dehak"
      ],
      "year": "2018",
      "venue": "Deep neural networks for emotion recognition combining audio and transcripts"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "Jianfeng Zhao",
        "Xia Mao",
        "Lijiang Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "Zhengwei Huang",
        "Ming Dong",
        "Qirong Mao",
        "Yongzhao Zhan"
      ],
      "year": "2014",
      "venue": "Proceed-ings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks",
      "authors": [
        "Wootaek Lim",
        "Daeyoung Jang",
        "Taejin Lee"
      ],
      "year": "2016",
      "venue": "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "8",
      "title": "Recent developments in opensmile, the munich open-source multimedia feature extractor",
      "authors": [
        "Florian Eyben",
        "Felix Weninger",
        "Florian Gross",
        "Björn Schuller"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM international conference on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Junaid Qadir"
      ],
      "year": "2018",
      "venue": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "arxiv": "arXiv:1811.11402"
    },
    {
      "citation_id": "10",
      "title": "Towards conditional adversarial training for predicting emotions from speech",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "Zhao Ren",
        "Fabien Ringeval",
        "Björn Schuller"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Improving emotion classification through variational inference of latent variables",
      "authors": [
        "Srinivas Parthasarathy",
        "Viktor Rozgic",
        "Ming Sun",
        "Chao Wang"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "On enhancing speech emotion recognition using generative adversarial networks",
      "authors": [
        "Saurabh Sahu",
        "Rahul Gupta",
        "Carol Espy-Wilson"
      ],
      "year": "2018",
      "venue": "On enhancing speech emotion recognition using generative adversarial networks",
      "arxiv": "arXiv:1806.06626"
    },
    {
      "citation_id": "13",
      "title": "Transfer learning for improving speech emotion classification accuracy",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Shahzad Younis",
        "Junaid Qadir",
        "Julien Epps"
      ],
      "year": "2018",
      "venue": "Transfer learning for improving speech emotion classification accuracy",
      "arxiv": "arXiv:1801.06353"
    },
    {
      "citation_id": "14",
      "title": "Reusing neural speech representations for auditory emotion recognition",
      "authors": [
        "Egor Lakomkin",
        "Cornelius Weber",
        "Sven Magg",
        "Stefan Wermter"
      ],
      "year": "2018",
      "venue": "Reusing neural speech representations for auditory emotion recognition",
      "arxiv": "arXiv:1803.11508"
    },
    {
      "citation_id": "15",
      "title": "Probing the information encoded in x-vectors",
      "authors": [
        "Desh Raj",
        "David Snyder",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2019",
      "venue": "Probing the information encoded in x-vectors",
      "arxiv": "arXiv:1909.06351"
    },
    {
      "citation_id": "16",
      "title": "Disentangling style factors from speaker representations",
      "authors": [
        "Jennifer Williams",
        "Simon King"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Stateof-the-art speaker recognition for telephone and video speech: The jhu-mit submission for nist sre18",
      "authors": [
        "Jesús Villalba",
        "Nanxin Chen",
        "David Snyder",
        "Daniel Garcia-Romero",
        "Alan Mccree",
        "Gregory Sell",
        "Jonas Borgstrom",
        "Fred Richardson",
        "Suwon Shon",
        "Franc ¸ois Grondin"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Unsupervised methods for speaker diarization: An integrated and iterative approach",
      "authors": [
        "Najim Stephen H Shum",
        "Réda Dehak",
        "James Dehak",
        "Glass"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Speaker diarization with plda i-vector scoring and unsupervised calibration",
      "authors": [
        "Gregory Sell",
        "Daniel Garcia-Romero"
      ],
      "year": "2014",
      "venue": "2014 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "20",
      "title": "Characterizing performance of speaker diarization systems on far-field speech using standard methods",
      "authors": [
        "Matthew Maciejewski",
        "David Snyder",
        "Vimal Manohar",
        "Najim Dehak",
        "Sanjeev Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Diarization is hard: Some experiences and lessons learned for the jhu team in the inaugural dihard challenge",
      "authors": [
        "Gregory Sell",
        "David Snyder",
        "Alan Mccree",
        "Daniel Garcia-Romero",
        "Jesús Villalba",
        "Matthew Maciejewski",
        "Vimal Manohar",
        "Najim Dehak",
        "Daniel Povey",
        "Shinji Watanabe"
      ],
      "year": "2018",
      "venue": "Diarization is hard: Some experiences and lessons learned for the jhu team in the inaugural dihard challenge"
    },
    {
      "citation_id": "22",
      "title": "X-vectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "David Snyder",
        "Daniel Garcia-Romero",
        "Gregory Sell",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "24",
      "title": "Sphereface: Deep hypersphere embedding for face recognition",
      "authors": [
        "Weiyang Liu",
        "Yandong Wen",
        "Zhiding Yu",
        "Ming Li",
        "Bhiksha Raj",
        "Le Song"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "25",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "26",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Curriculum learning for speech emotion recognition from crowdsourced labels",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)"
    },
    {
      "citation_id": "28",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Björn Schuller",
        "Johan Sundberg",
        "Elisabeth André",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Musan: A music, speech, and noise corpus",
      "authors": [
        "David Snyder",
        "Guoguo Chen",
        "Daniel Povey"
      ],
      "year": "2015",
      "venue": "Musan: A music, speech, and noise corpus",
      "arxiv": "arXiv:1510.08484"
    },
    {
      "citation_id": "30",
      "title": "A study of x-vector based speaker recognition on short utterances",
      "authors": [
        "Ahilan Kanagasundaram",
        "Sridha Sridharan",
        "Sriram Ganapathy",
        "Prachi Singh",
        "Clinton Fookes"
      ],
      "year": "2019",
      "venue": "A study of x-vector based speaker recognition on short utterances"
    }
  ]
}