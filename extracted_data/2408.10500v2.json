{
  "paper_id": "2408.10500v2",
  "title": "Sztu-Cmu At Mer2024: Improving Emotion-Llama With Conv-Attention For Multimodal Emotion Recognition",
  "published": "2024-08-20T02:46:03Z",
  "authors": [
    "Zebang Cheng",
    "Shuyuan Tu",
    "Dawei Huang",
    "Minghan Li",
    "Xiaojiang Peng",
    "Zhi-Qi Cheng",
    "Alexander G. Hauptmann"
  ],
  "keywords": [
    "MER2024",
    "Noise Robustness",
    "Open-Vocabulary Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents our winning approach for the MER-NOISE and MER-OV tracks of the MER2024 Challenge on multimodal emotion recognition. Our system leverages the advanced emotional understanding capabilities of Emotion-LLaMA to generate high-quality annotations for unlabeled samples, addressing the challenge of limited labeled data. To enhance multimodal fusion while mitigating modality-specific noise, we introduce Conv-Attention, a lightweight and efficient hybrid framework. Extensive experimentation validates the effectiveness of our approach. In the MER-NOISE track, our system achieves a state-of-the-art weighted average F-score of 85.30%, surpassing the second and third-place teams by 1.47% and 1.65%, respectively. For the MER-OV track, our utilization of Emotion-LLaMA for open-vocabulary annotation yields an 8.52% improvement in average accuracy and recall compared to GPT-4V, securing the highest score among all participating large multimodal models. The code and model for Emotion-LLaMA are available at https://github.com/ZebangCheng/Emotion-LLaMA. \n CCS Concepts ‚Ä¢ Computing methodologies ‚Üí Artificial intelligence; ‚Ä¢ Computer vision; ‚Ä¢ Humancentered computing; ‚Ä¢ HCI design and evaluation methods.; Z. Cheng (Emotion-LLaMA), S. Tu (Conv-Attention), D. Huang (feature engineering) contrib. equally. M. Li (replicated last year's champ solution). X. Peng & Z-Q. Cheng (corresponding authors, guided research/system, organized and rewrote the paper). A. Hauptmann (valuable insights & suggestions).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal Emotion Recognition (MER) aims to integrate information from various modalities-such as text, speech, and visual cues-to accurately identify and understand human emotions. This field has shown great promise in applications ranging from humancomputer interaction to mental health care and education. However, achieving robust performance in real-world scenarios remains a significant challenge. The MER2024 Challenge addresses these challenges through two specialized tracks: MER-NOISE and MER-OV.\n\nThe MER-NOISE track focuses on enhancing noise robustness in emotion recognition systems. In practical settings, noise is pervasive, making it difficult to ensure that audio streams are free of distortions and video frames maintain high resolution. This track targets the two most prevalent types of noise: audio additive noise and image blur. Participants are encouraged to employ data augmentation techniques  [48]  and other innovative methods  [38]  to improve the resilience of emotion recognition systems against these noise factors.\n\nThe MER-OV track introduces the concept of open-vocabulary emotion recognition, addressing the inherent subjectivity and ambiguity in emotion labeling. Traditional datasets often constrain label spaces to a few discrete categories, relying on multiple annotators and majority voting to determine the most likely label. This approach can overlook correct but non-candidate or minority For further details, refer to the original Emotion-LLaMA paper  [7] .\n\nlabels, leading to potential inaccuracies. The MER-OV track challenges participants to generate any number of labels across diverse categories, striving for a more nuanced and precise description of emotional states  [32] .\n\nTo address these challenges, we propose a robust system that integrates the advanced capabilities of Emotion-LLaMA for generating high-quality labels with a Conv-Attention model designed for efficient multimodal feature fusion. Our approach is detailed in Figure  2 , where we outline the workflow and demonstrate how each component contributes to overcoming the limitations of existing methods. A key limitation of previous approaches  [8]  lies in their reliance on models to generate pseudo-labels for unlabeled data, which are then used to augment training datasets. The effectiveness of this strategy depends heavily on the initial model's quality-if the model lacks robustness, it can produce low-quality pseudolabels, which may introduce errors in subsequent training phases. To mitigate this issue, we introduce Emotion-LLaMA  [7] , a model specifically designed to generate high-quality pseudo-labels for the unlabeled samples in the MER2024 dataset. As illustrated in Figure  1 , Emotion-LLaMA processes inputs from multiple modalities, utilizing visual and auditory features as contextual information to enhance the interpretation of text-based emotions. This approach ensures robust multimodal emotion understanding, even in the presence of loss or noise of modality.\n\nIn the feature extraction stage, we leverage high-performing unimodal models referenced in the official baseline papers  [32, 34] , such as HuBERT  [22]  and CLIP  [50] . Our experiments revealed that visual modality models are particularly vulnerable to noise, prompting us to pre-train MAE  [19]  and VideoMAE  [56]  on the unlabeled samples from the MER2024 dataset. This pre-training effectively captures both static and dynamic visual expression features. Additionally, to enhance the accuracy of textual feature extraction, we employed prompt-based strategies for models like Qwen  [3, 55]  and Baichuan  [63] , which were carefully evaluated for their effectiveness in capturing emotion-related information.\n\nDespite the strong performance of Emotion-LLaMA in MER, its substantial computational overhead and slow iteration cycle present challenges. To address these, we propose Conv-Attention, a lightweight and efficient hybrid framework that combines convolutional and global attention mechanisms for feature fusion. Conv-Attention leverages the inductive biases inherent in convolutional operations, allowing the model to perform effectively even with limited data. By integrating a simple attention mechanism with multiple convolutional blocks, the model can prioritize critical features while minimizing the impact of noise. The attention mechanism excels in querying features from a global perspective, while the convolutional operation focuses on capturing fine-grained semantic details within a limited receptive field. This combined approach mitigates the disadvantages of each individual method, enhancing overall model performance.\n\nIn summary, our team makes the following contributions:   [20] , MAE  [19] , VideoMAE  [56] , BERT  [13] , and HuBERT  [22] , followed by basic linear fusion layers  [8, 31, 68] . However, these simplistic models struggled to capture the complexity of multimodal data, prompting the development of more sophisticated cross-attention-based fusion models  [5, 14, 16] . Despite their advancements, these fusion techniques  [30, 38, 44, 53]  often lead to competition between modalities, where dominant modalities disproportionately influence the results.\n\nTo address these challenges, PMR  [41]  introduced a common message hub to better capture cross-modal dynamics. Subsequent research has focused on pre-fusion alignment, as seen in Emo-tionCLIP  [66] , which aligns temporal visual and textual data, and VAT  [15] , which aligns visual with audio features. However, these models still face challenges, including the need for large datasets and the difficulty of capturing fine-grained emotional features due to a reliance on global attention mechanisms.\n\nOur work overcomes these limitations by leveraging the advanced capabilities of Emotion-LLaMA for generating high-quality pseudo-labels and introducing a Conv-Attention model for efficient multimodal feature fusion, significantly improving the robustness and accuracy of emotion recognition in noisy environments.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Large Models In Emotion Understanding",
      "text": "The advent of large multimodal models (LMMs) has revolutionized emotion understanding, providing unprecedented inferential capabilities  [2, 6, 23, 47, 57] . Instruction-tuning techniques, pioneered by models such as InstructionGPT  [46] , FLAN  [11] , and OPT-IML  [24] , have further expanded the practical applications of these models  [58, 59] . In the context of emotion recognition, Instruc-tERC  [26]  has advanced conversation-based emotion recognition by introducing additional emotion alignment tasks. DFER-CLIP  [67] , built on the CLIP model  [50] , has shown promise in dynamic facial expression recognition. MER-MCE  [9]  has pushed the boundaries by inferring the causes of emotional triggers in conversations through multimodal inputs. Notably, GPT-4V  [35]  has demonstrated strong capabilities in generalized emotion recognition tasks.\n\nDespite these advances, most approaches rely on single emotion labels, often neglecting non-candidate or minority yet correct labels. Addressing the need for more nuanced emotion descriptions in real-world contexts, AffectGPT  [36]  proposes a multimodal, explainable, open-vocabulary emotion recognition approach, using GPT-4V to generate visual and acoustic signals and extract reliable labels. EmoVIT  [62]  further contributes by generating visual emotion instruction data using paired annotations.\n\nBuilding on these developments, our approach utilizes Emotion-LLaMA  [7]  to generate detailed multimodal emotional descriptions, resulting in comprehensive open-vocabulary labels. Emotion-LLaMA's capability to align multimodal features within a semantic space allows it to maintain robust emotion understanding even in the presence of modality loss or noise, significantly enhancing the accuracy and robustness of emotion recognition systems in real-world applications.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "This section presents our method that secured the highest performance in Track 2: MER-NOISE at the MER 2024 contest. First, we detail our feature extraction process (Sec. 3.1). Next, we describe how Emotion-LLaMA generates multimodal emotional descriptions and derives high-quality emotion labels (Sec. 3.2). Finally, we explain the Conv-Attention model used for feature fusion (Sec. 3.3).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multimodal Feature Engineering",
      "text": "We employed domain-specific models to extract unimodal features from auditory, visual, and textual data, with each model leveraging prior knowledge tailored to its respective domain.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Auditory Modality.",
      "text": "The MER2024 dataset contains exclusively Mandarin speech dialogues, prompting the selection of audio encoders that support the Chinese language. We prioritized Chinese-Hubert  [22]  and emotion2vec  [42] , with Chinese-Hubert being a variant of Hubert pre-trained on Chinese datasets. This model excels in processing Mandarin, producing high-quality embeddings suitable for complex emotion detection, making it ideal for the challenges presented in MER2024. To enhance the robustness and accuracy of our auditory emotion recognition pipeline, we also incorporated multilingual models such as Whisper  [51] , VGGish  [21] , and eGeMAPS  [18] . Whisper, a significant advancement in Automatic Speech Recognition (ASR), combined with VGGish and eGeMAPS, provided a comprehensive audio processing framework.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Visual Modality.",
      "text": "Building on the experience and results from the MER2023 competition, we selected a series of high-performing visual encoders for comparative analysis, including CLIP  [50] , MAE  [19] , VideoMAE  [56] , and MANet  [37] . CLIP, pre-trained on largescale image-text pairs, excels at associating images with textual descriptions, making it particularly effective for affective computing tasks. MAE, designed for the visual domain, reconstructs obscured segments of input images, compelling the model to learn both global and local features, which is advantageous for facial recognition and emotion analysis. VideoMAE extends MAE's concept to video by applying random masking to video frames and training the model to reconstruct missing parts, effectively leveraging spatiotemporal characteristics for tasks such as video classification. Both MAE and VideoMAE were further fine-tuned to enhance their performance in noisy environments.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Textual Modality.",
      "text": "Given that the subtitles extracted from audio are primarily in Chinese, we focused on models with strong Chinese language proficiency, including ChatGLM2  [17] , Qwen  [3] , and Baichuan  [63] . These models, pre-trained on extensive Chinese corpora, are particularly effective for handling Chinese text inputs. We also utilized multilingual models such as RoBERTa  [39] , MacBERT  [12] , and BLOOM  [61] . Inspired by In-Context Learning  [4]  and Chain-of-Thought (CoT)  [60]  techniques, we employed a prompt strategy to enhance feature extraction. This approach involved generating emotion-associated descriptions by appending a designed prompt before the text input, as formalized in the following equation:\n\nHere, ùëá ‚Ä≤ represents the augmented text input, ‚äï denotes the concatenation operation, and ùëÄ is the language model used for feature extraction. This prompt-based method guides the model to capture emotional cues more effectively, resulting in richer and more accurate emotional descriptions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Emotion-Llama Pseudo-Labeling",
      "text": "Emotion-LLaMA  [7] , developed in our previous work, is a multimodal emotion recognition model that supports inputs across text, audio, and visual domains. By aligning audio and visual features within a shared semantic space as contextual information for the text modality, Emotion-LLaMA excels in multimodal emotion recognition and reasoning tasks. We leveraged Emotion-LLaMA's capabilities for the MER2024 competition.\n\nTo address the challenge of limited labeled data, especially for the MER-NOISE track, we used Emotion-LLaMA to generate pseudolabels. By performing multimodal emotion recognition on 20,000 unlabeled samples, we significantly augmented the training set with pseudo-labeled data. This approach not only increased the volume of training data but also introduced greater diversity, thereby enhancing the model's generalization capabilities.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Prompt Design And Data",
      "text": "Processing. We designed specific prompts for Emotion-LLaMA and LLaMA-3 to extract detailed emotion-related descriptions and labels. These prompts guide the models to focus on relevant aspects of the input data, improving the quality and relevance of the generated labels. Table  1  provides examples of these prompts, illustrating their effectiveness in eliciting precise and informative responses. We processed the data by feeding it into Emotion-LLaMA with a simple instruction prompt to obtain emotion-related descriptions and category labels, as formalized below:\n\nwhere ùê∏ represents Emotion-LLaMA, Tùëëùëíùë†ùëêùëüùëñùëùùë°ùëñùëúùëõ is the emotionrelated description generated for the MER-OV track, L denotes the pseudo-label set, and A ‚Ä≤ ùë¢ , V ‚Ä≤ ùë¢ , T ‚Ä≤ ùë¢ are the data in the audio, visual, and textual modalities, respectively. The prompts P ‚Ä† and P ‚Ä° are used for multimodal emotion classification and reasoning, respectively.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Keyword Extraction And Dataset",
      "text": "Augmentation. We employed LLaMA-3 as a keyword extractor to convert emotional descriptions into labels, which were then used as the final prediction results for MER-OV. These pseudo-labeled samples were combined with the Train&Val dataset from MER2024 to create the training set for the multimodal fusion model:\n\nwhere L ùëÇùëâ represents the open vocabulary labels, D ùëô is the labeled dataset from MER2024, D ‚Ä≤ ùë¢ is the unlabeled dataset, and D ùëéùë¢ùëî is the augmented dataset. By leveraging the capabilities of Emotion-LLaMA, our methodology significantly enhances the volume and quality of data available for training, effectively addressing the issue of sample scarcity.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Multimodal Feature Fusion",
      "text": "To address the limitations of pure attention mechanisms, we employed the Conv-Attention structure, as illustrated in Figure  2(b) . This structure integrates convolutional blocks with attention mechanisms to introduce inductive biases, improving performance on limited-scale data. The convolutional branch captures semantic details due to its limited receptive fields, while the attention branch focuses on global, emotionally salient features.\n\nWe began by using a multilayer perceptron (MLP) to standardize the channel depth of features from different modalities (audio, visual, text) to a common dimension. Each feature is represented as (batch, depth). These features were then concatenated along their embedding depths and sequence lengths to obtain hybrid features ùë≠ ùëë and ùë≠ ùë† :\n\nwhere ùë® = {ùë® ‚Ñéùë¢ùëèùëíùëüùë° }, ùëΩ = ùëΩ ùëêùëôùëñùëù , ùëΩ ùë£ùëñùëëùëíùëúùëöùëéùëí , ùëΩ ùëöùëéùëí , ùëΩ ùëöùëéùëõùëíùë° , and ùëª = ùëª ùëûùë§ùëíùëõ , ùëª ùëèùëéùëñùëê‚Ñéùë¢ùëéùëõ refer to audio, visual, and text features, respectively.\n\nIn the attention branch, we applied Attn_MLP(‚Ä¢) to ùë≠ ùëë , downsampling its embedding depth to align with the sequence length of ùë≠ ùë† . We then performed a matrix product between the downsampled ùë≠ ùëë and ùë≠ ùë† to obtain the attention-based fusion feature ùë≠ ùëéùë°ùë°ùëõ , which enhances the model's ability to identify emotionally salient components across different modalities:\n\nwhere √ó denotes the matrix product, and Unsqueeze(‚Ä¢) denotes the unsqueeze operation.\n\nIn the convolutional branch, we designed a lightweight convolution block consisting of a convolution layer Conv1d(‚Ä¢), batch normalization BN(‚Ä¢), and an activation function Swish(‚Ä¢). The convolutional branch includes ùëµ convolution blocks and an adaptive average pooling layer Pool(‚Ä¢), which reshapes the ultimate fusion features. The convolutional operators' inductive bias and limited receptive fields encourage the model to focus on fine-grained details, enhancing robustness, particularly when trained on limited-scale datasets:\n\nwhere ùëò (ùëò = 1, 2, ..., ùëÅ ) refers to the index of the convolution block, and ùë≠ ùëêùëúùëõùë£ indicates the final convolution-based fusion feature. Note that ùë≠ 0 ùëêùëúùëõùë£ = ùë≠ ùë† . Finally, we employed a residual connection to combine ùë≠ ùëêùëúùëõùë£ and ùë≠ ùëéùë°ùë°ùëõ into the final fusion feature ùë≠ ùëì ùë¢ùë†ùëñùëúùëõ , which is then fed into a linear classification head FC ùëúùë¢ùë° (‚Ä¢) for emotion prediction:\n\nThe pseudo-labels generated by Emotion-LLaMA were integrated with the 5030 Train&Val datasets from MER2024 to form the training set for our Conv-Attention model, as depicted in Figure  2(b) . This integration through data augmentation significantly improves the performance and robustness of our emotion recognition system.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "This section presents a comprehensive evaluation of our approach on Track 2 (MER-NOISE) and Track 3 (MER-OV) of the MER2024 competition. We begin by analyzing the performance of singlemodal models, followed by multimodal fusion experiments, and conclude with ablation studies. Our goal is to provide detailed insights that can inform future research and practical applications in multimodal emotion recognition, particularly in noisy and openvocabulary environments.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Single-Modal Performance On Track 2: Mer-Noise",
      "text": "We assessed several single-modal models on the MER-NOISE track to analyze the contributions of auditory, visual, and textual modalities to emotion recognition performance. The results are summarized in Table  2 . Given the limited availability of audio extraction models tailored for the Chinese language, we evaluated five models: eGeMAPS  [18] , VGGish  [21] , Whisper  [51] , emo-tion2vec  [42] , and Chinese-Hubert  [22] . The Chinese-Hubert model emerged as the top performer with a Weighted Average F-score (WAF) of 72.67%. This superior performance can be attributed to its pre-training on Chinese datasets, which enables it to capture contextual representations more effectively than other models.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Visual Modality.",
      "text": "In the visual modality, we evaluated four models: MANet  [37] , MAE  [19] , VideoMAE  [56] , and CLIP  [50] , along with versions fine-tuned for MER2024. CLIP achieved the highest WAF of 58.80%, likely due to its extensive pre-training and multimodal learning capabilities. VideoMAE was further enhanced by domain-specific fine-tuning for emotion recognition tasks.\n\n4.1.3 Textual Modality. For the textual modality, we focused on models with strong Chinese language proficiency. Baichuan-13B, when used with carefully designed prompts, attained the highest WAF of 59.32%, closely followed by Qwen1.5-32B with a WAF of 58.88%. The strong performance of these models can be attributed to their large-scale pre-training on Chinese corpora and the effective use of prompts, which significantly enhanced their ability to recognize and classify emotions in textual data. Further analysis of prompt designs, as shown in Table  3 , revealed that Prompt 1 provided the best performance across both single-modal and multimodal fusion scenarios.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Multimodal Fusion On Track 2: Mer-Noise",
      "text": "Leveraging the findings from the single-modal evaluations, we conducted multimodal fusion experiments by integrating features from the best-performing models in each modality. The results, as presented in Table  4 , demonstrate the effectiveness of our proposed Conv-Attention model. The configuration combining Hu-BERT, CLIP, VideoMAE, Qwen, and Baichuan features yielded the highest WAF and ACC scores of 81.59% and 81.71% on the Train&Val dataset. On the MER-NOISE track, the optimal setup, which also included additional visual models (MAE and MANet), achieved a WAF of 80.10%. These results underscore the effectiveness of multimodal fusion, particularly when employing our Conv-Attention model, which consistently outperformed other fusion strategies across all metrics (Table  5 ) when trained on the augmented dataset.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Performance On Track 3: Mer-Ov",
      "text": "For Track 3 (MER-OV), which addresses open-vocabulary emotion recognition, we evaluated various large language models using Accuracy ùë† , Recall ùë† , and their average (Avg). The results are detailed in Table  6 . Emotion-LLaMA outperformed other models in terms   3 ) an open-vocabulary design, which is well-suited for handling diverse and complex emotion descriptions. The trade-off observed between accuracy and recall suggests that while more detailed emotion descriptions enhance recall, they may also introduce a higher risk of misclassification.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ablation Studies",
      "text": "To gain deeper insights into the effectiveness of our proposed Conv-Attention model, we conducted a series of ablation studies to evaluate the impact of various components and hyperparameters.  yielding the highest scores across all metrics. This suggests that pseudo-labeling effectively augments the training data, enabling the model to learn from a larger and more diverse dataset. However, it is important to consider that while using 100% of pseudo-labeled data can enhance performance, it may also introduce some noise.\n\nThe optimal ratio may depend on the quality of the pseudo-labels.\n\nRegarding learning rates, a rate of 1e-3 provided the best balance between convergence speed and model accuracy, achieving the highest Noise WAF. Lower learning rates (e.g., 1e-4) resulted in slower convergence, while higher rates (e.g., 5e-3 and above) caused unstable training and poor generalization, especially in noisy environments. These findings highlight the critical importance of proper hyperparameter tuning in achieving optimal performance, particularly in challenging multimodal and noisy settings.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Modality Alignment.",
      "text": "Building on previous work  [69] , we examined the impact of modality alignment on MER, with results presented in Table  10 . Interesting phenomena emerged from these experiments: (1) Post-alignment, the scores of the previously weaker visual and textual modalities improved. However, this came at the cost of a performance decline in the best-performing audio modality, suggesting that alignment may sometimes dilute the complementary strengths of individual modalities.  (2)  The performance of the multimodal fusion dropped significantly after alignment. This indicates that while alignment may homogenize features across modalities, it can reduce the benefits derived from the diversity of information carried by different modalities.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we presented our winning approach for enhancing multimodal emotion recognition in the MER2024 Challenge, specifically targeting the MER-NOISE and MER-OV tracks. By leveraging the advanced capabilities of Emotion-LLaMA to generate highquality pseudo-labels and introducing a Conv-Attention mechanism for efficient feature fusion, we significantly improved the robustness and accuracy of emotion recognition. Our method delivered stateof-the-art performance in the MER-NOISE track with a weighted average F-score of 85.30% and achieved top results in the MER-OV track, enhancing average accuracy and recall by 8.52% compared to GPT-4V. The integration of Emotion-LLaMA was pivotal in achieving these results, underscoring its potential to advance the field of multimodal emotion recognition.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the Emotion-LLaMA architecture, which integrates audio, visual, and text inputs for advanced multimodal",
      "page": 2
    },
    {
      "caption": "Figure 2: , where we outline the workflow and demonstrate how each",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview of our framework for MER2024. In the feature extraction phase, frozen encoders extract features from",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "Emotion-LLaMA‚Ä†",
          "Language": "English",
          "Prompt": "Please determine which emotion label\nin the video represents: happy, sad,\nneutral, angry, worried, surprise."
        },
        {
          "Models": "Emotion-LLaMA‚Ä°",
          "Language": "English",
          "Prompt": "Please analyze all the clues in the video and reason out the emotional label of\nthe person in the video."
        },
        {
          "Models": "LLaMA-3",
          "Language": "English",
          "Prompt": "You are an emotion analysis expert. Please analyze the input multimodal emo-\ntion description and output keywords related to the emotion description.\nInput: [Multimodal Emotion Description]\nOutput:"
        },
        {
          "Models": "Qwen1.5-32B",
          "Language": "Chinese",
          "Prompt": "Please analyze the provided text content and classify emotions into six cate-\ngories: [neutral, angry, happy, sad, worried, surprise], and explain the specific\nreasons: <Text>"
        },
        {
          "Models": "Baichuan-13B (prompt 1)",
          "Language": "Chinese",
          "Prompt": "Please analyze the provided text content and classify emotions into six cate-\ngories: [neutral, angry, happy, sad, worried, surprise], and explain the specific\nreasons: <Text>"
        },
        {
          "Models": "Baichuan-13B (prompt 2)",
          "Language": "Chinese",
          "Prompt": "Please analyze the provided text content and classify emotions into six cate-\ngories: [neutral, angry, happy, sad, worried, surprise]: <Text>"
        },
        {
          "Models": "Baichuan-13B (prompt 3)",
          "Language": "Chinese",
          "Prompt": "Please analyze the provided text content: <Text>"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An assessment of in-thewild datasets for multimodal emotion recognition",
      "authors": [
        "Ana Aguilera",
        "Diego Mellado",
        "Felipe Rojas"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "2",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "Jean-Baptiste Alayrac",
        "Jeff Donahue",
        "Pauline Luc",
        "Antoine Miech",
        "Iain Barr",
        "Yana Hasson",
        "Karel Lenc",
        "Arthur Mensch",
        "Katherine Millican",
        "Malcolm Reynolds"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang",
        "Binyuan Hui",
        "Luo Ji",
        "Mei Li",
        "Junyang Lin",
        "Runji Lin",
        "Dayiheng Liu",
        "Gao Liu",
        "Chengqiang Lu",
        "Keming Lu",
        "Jianxin Ma",
        "Rui Men",
        "Xingzhang Ren",
        "Xuancheng Ren",
        "Chuanqi Tan",
        "Sinan Tan",
        "Jianhong Tu",
        "Peng Wang",
        "Shijie Wang",
        "Wei Wang",
        "Shengguang Wu",
        "Benfeng Xu",
        "Jin Xu",
        "An Yang",
        "Hao Yang",
        "Jian Yang",
        "Shusheng Yang",
        "Yang Yao",
        "Bowen Yu",
        "Hongyi Yuan",
        "Zheng Yuan",
        "Jianwei Zhang",
        "Xingxuan Zhang",
        "Yichang Zhang",
        "Zhenru Zhang",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "",
      "arxiv": "arXiv:2309.16609[cs.CL]"
    },
    {
      "citation_id": "4",
      "title": "Language Models are Few-Shot Learners",
      "authors": [
        "B Tom",
        "Benjamin Brown",
        "Nick Mann",
        "Melanie Ryder",
        "Jared Subbiah",
        "Prafulla Kaplan",
        "Arvind Dhariwal",
        "Pranav Neelakantan",
        "Girish Shyam",
        "Amanda Sastry",
        "Sandhini Askell",
        "Ariel Agarwal",
        "Gretchen Herbert-Voss",
        "Tom Krueger",
        "Rewon Henighan",
        "Aditya Child",
        "Daniel Ramesh",
        "Jeffrey Ziegler",
        "Clemens Wu",
        "Christopher Winter",
        "Mark Hesse",
        "Eric Chen",
        "Mateusz Sigler",
        "Scott Litwin",
        "Benjamin Gray",
        "Jack Chess",
        "Christopher Clark",
        "Sam Berner",
        "Alec Mccandlish",
        "Ilya Radford",
        "Dario Sutskever",
        "Amodei"
      ],
      "year": "2020",
      "venue": "Language Models are Few-Shot Learners",
      "arxiv": "arXiv:2005.14165"
    },
    {
      "citation_id": "5",
      "title": "Crossvit: Crossattention multi-scale vision transformer for image classification",
      "authors": [
        "Chun-Fu Richard Chen",
        "Quanfu Fan",
        "Rameswar Panda"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "6",
      "title": "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic",
      "authors": [
        "Keqin Chen",
        "Zhao Zhang",
        "Weili Zeng",
        "Richong Zhang",
        "Feng Zhu",
        "Rui Zhao"
      ],
      "year": "2023",
      "venue": "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic",
      "arxiv": "arXiv:2306.15195"
    },
    {
      "citation_id": "7",
      "title": "Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Jingdong Sun",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning",
      "arxiv": "arXiv:2406.11161"
    },
    {
      "citation_id": "8",
      "title": "Semi-supervised multimodal emotion recognition with expression mae",
      "authors": [
        "Zebang Cheng",
        "Yuxiang Lin",
        "Zhaoru Chen",
        "Xiang Li",
        "Shuyi Mao",
        "Fan Zhang",
        "Daijun Ding",
        "Bowen Zhang",
        "Xiaojiang Peng"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Mips at semeval-2024 task 3: Multimodal emotion-cause pair extraction in conversations with multimodal language models",
      "authors": [
        "Zebang Cheng",
        "Fuqiang Niu",
        "Yuxiang Lin",
        "Zhi-Qi Cheng",
        "Bowen Zhang",
        "Xiaojiang Peng"
      ],
      "year": "2024",
      "venue": "Mips at semeval-2024 task 3: Multimodal emotion-cause pair extraction in conversations with multimodal language models",
      "arxiv": "arXiv:2404.00511"
    },
    {
      "citation_id": "10",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "11",
      "title": "Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Eric Li",
        "Xuezhi Wang",
        "Mostafa Dehghani"
      ],
      "year": "2022",
      "venue": "Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models",
      "arxiv": "arXiv:2210.11416"
    },
    {
      "citation_id": "12",
      "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing",
      "authors": [
        "Yiming Cui",
        "Wanxiang Che",
        "Ting Liu",
        "Bing Qin",
        "Shijin Wang",
        "Guoping Hu"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "13",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "arxiv": "arXiv:1810.04805[cs.CL"
    },
    {
      "citation_id": "14",
      "title": "Stable Speech Emotion Recognition with Head-k-Pooling Loss",
      "authors": [
        "Chaoyue Ding",
        "Jiakui Li",
        "Daoming Zong",
        "Baoxiang Li",
        "Tianhao Zhang",
        "Qunyan Zhou"
      ],
      "year": "2023",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "15",
      "title": "Learning aligned audiovisual representations for multimodal sentiment analysis",
      "authors": [
        "Chaoyue Ding",
        "Daoming Zong",
        "Baoxiang Li",
        "Ken Zheng",
        "Dinghao Zhou",
        "Jiakui Li",
        "Qunyan Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the 1st International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "LETR: A lightweight and efficient transformer for keyword spotting",
      "authors": [
        "Kevin Ding",
        "Martin Zong",
        "Jiakui Li",
        "Baoxiang Li"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
      "authors": [
        "Zhengxiao Du",
        "Yujie Qian",
        "Xiao Liu",
        "Ming Ding",
        "Jiezhong Qiu",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "18",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Bj√∂rn Schuller",
        "Johan Sundberg",
        "Elisabeth Andr√©",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr Doll√°r",
        "Ross Girshick"
      ],
      "year": "2021",
      "venue": "Masked Autoencoders Are Scalable Vision Learners",
      "arxiv": "arXiv:2111.06377[cs.CV"
    },
    {
      "citation_id": "20",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "21",
      "title": "CNN architectures for large-scale audio classification",
      "authors": [
        "Shawn Hershey",
        "Sourish Chaudhuri",
        "P Daniel",
        "Ellis",
        "Aren Jort F Gemmeke",
        "R Channing Jansen",
        "Manoj Moore",
        "Devin Plakal",
        "Rif Platt",
        "Bryan Saurous",
        "Seybold"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "23",
      "title": "From Large Language Models to Large Multimodal Models: A Literature Review",
      "authors": [
        "Dawei Huang",
        "Chuan Yan",
        "Qing Li",
        "Xiaojiang Peng"
      ],
      "year": "2024",
      "venue": "Applied Sciences",
      "doi": "10.3390/app14125068"
    },
    {
      "citation_id": "24",
      "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
      "authors": [
        "Srinivasan Iyer",
        "Xi Victoria Lin",
        "Ramakanth Pasunuru",
        "Todor Mihaylov",
        "D√°niel Simig",
        "Ping Yu",
        "Kurt Shuster",
        "Tianlu Wang",
        "Qing Liu",
        "Punit Singh Koura"
      ],
      "year": "2022",
      "venue": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
      "arxiv": "arXiv:2212.12017"
    },
    {
      "citation_id": "25",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "26",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "27",
      "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning",
      "authors": [
        "Bo Li",
        "Yuanhan Zhang",
        "Liangyu Chen",
        "Jinghao Wang",
        "Jingkang Yang",
        "Ziwei Liu"
      ],
      "year": "2023",
      "venue": "Otter: A Multi-Modal Model with In-Context Instruction Tuning",
      "arxiv": "arXiv:2305.03726"
    },
    {
      "citation_id": "28",
      "title": "Videochat: Chat-centric video understanding",
      "authors": [
        "Kunchang Li",
        "Yinan He",
        "Yi Wang",
        "Yizhuo Li",
        "Wenhai Wang",
        "Ping Luo",
        "Yali Wang",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Videochat: Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "29",
      "title": "MVBench: A Comprehensive Multi-modal Video Understanding Benchmark",
      "authors": [
        "Kunchang Li",
        "Yali Wang",
        "Yinan He",
        "Yizhuo Li",
        "Yi Wang",
        "Yi Liu",
        "Zun Wang",
        "Jilan Xu",
        "Guo Chen",
        "Ping Luo",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts",
      "authors": [
        "Yunxin Li",
        "Shenyuan Jiang",
        "Baotian Hu",
        "Longyue Wang",
        "Wanqi Zhong",
        "Wenhan Luo",
        "Lin Ma",
        "Min Zhang"
      ],
      "year": "2024",
      "venue": "Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts",
      "arxiv": "arXiv:2405.11273"
    },
    {
      "citation_id": "31",
      "title": "Mer 2023: Multi-label learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Kang Chen",
        "Mngyu Xu",
        "Kexin Wang",
        "Ke Xu",
        "Yu He",
        "Ying Li",
        "Jinming Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Hao Gu",
        "Jinming Zhao",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition",
      "arxiv": "arXiv:2404.17113"
    },
    {
      "citation_id": "33",
      "title": "AffectGPT: Dataset and Framework for Explainable Multimodal Emotion Recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Jiangyan Yi",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "AffectGPT: Dataset and Framework for Explainable Multimodal Emotion Recognition",
      "arxiv": "arXiv:2407.07653"
    },
    {
      "citation_id": "34",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Yong Ren",
        "Hao Gu",
        "Haiyang Sun",
        "Lan Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "35",
      "title": "GPT-4V with emotion: A zero-shot benchmark for Generalized Emotion Recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Haiyang Sun",
        "Kang Chen",
        "Zhuofan Wen",
        "Hao Gu",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "36",
      "title": "Explainable multimodal emotion reasoning",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Mingyu Xu",
        "Haiyang Sun",
        "Ke Xu",
        "Zhuofan Wen",
        "Shun Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Explainable multimodal emotion reasoning",
      "arxiv": "arXiv:2306.15401"
    },
    {
      "citation_id": "37",
      "title": "Luc Van Gool, and Radu Timofte. 2021. Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution",
      "authors": [
        "Jingyun Liang",
        "Guolei Sun",
        "Kai Zhang"
      ],
      "venue": "Luc Van Gool, and Radu Timofte. 2021. Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution",
      "arxiv": "arXiv:2108.05302[cs.CV"
    },
    {
      "citation_id": "38",
      "title": "Umt: Unified multi-modal transformers for joint video moment retrieval and highlight detection",
      "authors": [
        "Ye Liu",
        "Siyuan Li",
        "Yang Wu",
        "Chang-Wen Chen",
        "Ying Shan",
        "Xiaohu Qie"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "39",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "40",
      "title": "Valley: Video assistant with large language model enhanced ability",
      "authors": [
        "Ruipu Luo",
        "Ziwang Zhao",
        "Min Yang",
        "Junwei Dong",
        "Da Li",
        "Pengcheng Lu",
        "Tao Wang",
        "Linmei Hu",
        "Minghui Qiu",
        "Zhongyu Wei"
      ],
      "year": "2023",
      "venue": "Valley: Video assistant with large language model enhanced ability",
      "arxiv": "arXiv:2306.07207"
    },
    {
      "citation_id": "41",
      "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
      "authors": [
        "Fengmao Lv",
        "Xiang Chen",
        "Yanyong Huang",
        "Lixin Duan",
        "Guosheng Lin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "42",
      "title": "2023. emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Jiaxin Ye",
        "Jinchao Li",
        "Zhifu Gao",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "year": "2023",
      "venue": "2023. emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation",
      "arxiv": "arXiv:2312.15185"
    },
    {
      "citation_id": "43",
      "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
      "authors": [
        "Muhammad Maaz",
        "Hanoona Rasheed",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "year": "2023",
      "venue": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
      "arxiv": "arXiv:2306.05424"
    },
    {
      "citation_id": "44",
      "title": "Attention bottlenecks for multimodal fusion",
      "authors": [
        "Arsha Nagrani",
        "Shan Yang",
        "Anurag Arnab",
        "Aren Jansen",
        "Cordelia Schmid",
        "Chen Sun"
      ],
      "year": "2021",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "45",
      "title": "GPT-4V(ision) system card",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "GPT-4V(ision) system card"
    },
    {
      "citation_id": "46",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "47",
      "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
      "authors": [
        "Zhiliang Peng",
        "Wenhui Wang",
        "Li Dong",
        "Yaru Hao",
        "Shaohan Huang",
        "Shuming Ma",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
      "arxiv": "arXiv:2306.14824"
    },
    {
      "citation_id": "48",
      "title": "A survey of semi-supervised learning methods",
      "authors": [
        "Nitin Namdeo",
        "Parag Kulkarni"
      ],
      "year": "2008",
      "venue": "2008 International conference on computational intelligence and security"
    },
    {
      "citation_id": "49",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "50",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "51",
      "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "arxiv": "arXiv:2212.04356[eess.AS"
    },
    {
      "citation_id": "52",
      "title": "PandaGPT: One Model To Instruction-Follow Them All",
      "authors": [
        "Yixuan Su",
        "Tian Lan",
        "Huayang Li",
        "Jialu Xu",
        "Yan Wang",
        "Deng Cai"
      ],
      "year": "2023",
      "venue": "Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants"
    },
    {
      "citation_id": "53",
      "title": "Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Chao Ma Zejun",
        "Zhang"
      ],
      "year": "2023",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "55",
      "title": "Introducing Qwen1.5",
      "authors": [
        "Qwen Team"
      ],
      "year": "2024",
      "venue": "Introducing Qwen1.5"
    },
    {
      "citation_id": "56",
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "authors": [
        "Zhan Tong",
        "Yibing Song",
        "Jue Wang",
        "Limin Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "57",
      "title": "Visionllm: Large language model is also an open-ended decoder for vision-centric tasks",
      "authors": [
        "Wenhai Wang",
        "Zhe Chen",
        "Xiaokang Chen",
        "Jiannan Wu",
        "Xizhou Zhu",
        "Gang Zeng",
        "Ping Luo",
        "Tong Lu",
        "Jie Zhou",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Visionllm: Large language model is also an open-ended decoder for vision-centric tasks",
      "arxiv": "arXiv:2305.11175"
    },
    {
      "citation_id": "58",
      "title": "Self-Instruct: Aligning Language Model with Self Generated Instructions",
      "authors": [
        "Yizhong Wang",
        "Yeganeh Kordi",
        "Swaroop Mishra",
        "Alisa Liu",
        "Noah Smith",
        "Daniel Khashabi",
        "Hannaneh Hajishirzi"
      ],
      "year": "2022",
      "venue": "Self-Instruct: Aligning Language Model with Self Generated Instructions",
      "arxiv": "arXiv:2212.10560"
    },
    {
      "citation_id": "59",
      "title": "Benchmarking generalization via incontext instructions on 1,600+ language tasks",
      "authors": [
        "Yizhong Wang",
        "Swaroop Mishra",
        "Pegah Alipoormolabashi",
        "Yeganeh Kordi",
        "Amirreza Mirzaei",
        "Anjana Arunkumar",
        "Arjun Ashok",
        "Arut Selvan Dhanasekaran",
        "Atharva Naik",
        "David Stap"
      ],
      "year": "2022",
      "venue": "Benchmarking generalization via incontext instructions on 1,600+ language tasks",
      "arxiv": "arXiv:2204.07705"
    },
    {
      "citation_id": "60",
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Brian Ichter",
        "Fei Xia",
        "Ed Chi",
        "Quoc Le",
        "Denny Zhou"
      ],
      "year": "2023",
      "venue": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "arxiv": "arXiv:2201.11903[cs.CL"
    },
    {
      "citation_id": "61",
      "title": "Bloom: A 176b-parameter open-access multilingual language model",
      "authors": [
        "Bigscience Workshop",
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Iliƒá",
        "Roman Hesslow",
        "Alexandra Castagn√©",
        "Fran√ßois Sasha Luccioni",
        "Yvon"
      ],
      "year": "2022",
      "venue": "Bloom: A 176b-parameter open-access multilingual language model",
      "arxiv": "arXiv:2211.05100"
    },
    {
      "citation_id": "62",
      "title": "EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning",
      "authors": [
        "Hongxia Xie",
        "Chu-Jun Peng",
        "Yu-Wen Tseng",
        "Hung-Jen Chen",
        "Chan-Feng Hsu",
        "Hong-Han Shuai",
        "Wen-Huang Cheng"
      ],
      "year": "2024",
      "venue": "EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning",
      "arxiv": "arXiv:2404.16670"
    },
    {
      "citation_id": "63",
      "title": "Open largescale language models",
      "authors": [
        "Aiyuan Yang",
        "Bin Xiao",
        "Bingning Wang",
        "Borong Zhang",
        "Ce Bian",
        "Chenxu Chao Yin",
        "Da Lv",
        "Dian Pan",
        "Dong Wang",
        "Yan"
      ],
      "year": "2023",
      "venue": "Open largescale language models",
      "arxiv": "arXiv:2309.10305"
    },
    {
      "citation_id": "64",
      "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
      "authors": [
        "Qinghao Ye",
        "Haiyang Xu",
        "Guohai Xu",
        "Jiabo Ye",
        "Ming Yan",
        "Yiyang Zhou",
        "Junyang Wang",
        "Anwen Hu",
        "Pengcheng Shi",
        "Yaya Shi",
        "Chaoya Jiang",
        "Chenliang Li",
        "Yuanhong Xu",
        "Hehong Chen",
        "Junfeng Tian",
        "Qian Qi",
        "Ji Zhang",
        "Fei Huang"
      ],
      "year": "2023",
      "venue": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
      "arxiv": "arXiv:2304.14178[cs.CL]"
    },
    {
      "citation_id": "65",
      "title": "Video-llama: An instructiontuned audio-visual language model for video understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "Video-llama: An instructiontuned audio-visual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    },
    {
      "citation_id": "66",
      "title": "Learning emotion representations from verbal and nonverbal communication",
      "authors": [
        "Sitao Zhang",
        "Yimu Pan",
        "James Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "67",
      "title": "Prompting visual-language models for dynamic facial expression recognition",
      "authors": [
        "Zengqun Zhao",
        "Ioannis Patras"
      ],
      "year": "2023",
      "venue": "Prompting visual-language models for dynamic facial expression recognition",
      "arxiv": "arXiv:2308.13382"
    },
    {
      "citation_id": "68",
      "title": "Exploring emotion features and fusion strategies for audio-video emotion recognition",
      "authors": [
        "Hengshun Zhou",
        "Debin Meng",
        "Yuanyuan Zhang",
        "Xiaojiang Peng",
        "Jun Du",
        "Kai Wang",
        "Yu Qiao"
      ],
      "year": "2019",
      "venue": "Exploring emotion features and fusion strategies for audio-video emotion recognition"
    },
    {
      "citation_id": "69",
      "title": "AcFormer: An Aligned and Compact Transformer for Multimodal Sentiment Analysis",
      "authors": [
        "Daoming Zong",
        "Chaoyue Ding",
        "Baoxiang Li",
        "Jiakui Li",
        "Ken Zheng",
        "Qunyan Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    }
  ]
}