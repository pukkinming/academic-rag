{
  "paper_id": "2310.04481v1",
  "title": "Acoustic And Linguistic Representations For Speech Continuous Emotion Recognition In Call Center Conversations",
  "published": "2023-10-06T10:22:51Z",
  "authors": [
    "Manon Macary",
    "Marie Tahon",
    "Yannick Estève",
    "Daniel Luzzati"
  ],
  "keywords": [
    "Continuous Speech Emotion Recognition",
    "Pretrained Features",
    "Multi-modalities",
    "AlloSat"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The goal of our research is to automatically retrieve the satisfaction and the frustration in real-life call-center conversations. This study focuses an industrial application in which the customer satisfaction is continuously tracked down to improve customer services. To compensate the lack of large annotated emotional databases, we explore the use of pre-trained speech representations as a form of transfer learning towards AlloSat corpus. Moreover, several studies have pointed out that emotion can be detected not only in speech but also in facial trait, in biological response or in textual information. In the context of telephone conversations, we can break down the audio information into acoustic and linguistic by using the speech signal and its transcription. Our experiments confirms the large gain in performance obtained with the use of pre-trained features. Surprisingly, we found that the linguistic content is clearly the major contributor for the prediction of satisfaction and best generalizes to unseen data. Our experiments conclude to the definitive advantage of using CamemBERT representations, however the benefit of the fusion of acoustic and linguistic modalities is not as obvious. With models learnt on individual annotations, we found that fusion approaches are more robust to the subjectivity of the annotation task. This study also tackles the problem of performances variability and intends to estimate this variability from different views: weights initialization, confidence intervals and annotation subjectivity. A deep analysis on the linguistic content investigates interpretable factors able to explain the high contribution of the linguistic modality for this task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "N OWADAYS, relations between customers and companies are increasingly based on call centers  [1] . Within these structures, massive speech data is collected and automatically processed everyday by companies, since such data contains crucial information for these companies to improve their commercial relations with customers. With the huge improvements in Automatic Speech Recognition and Spoken Language Understanding processing, it is now possible to extract automatically linguistic and semantic information for speech analytics. In addition, paralinguistic cues can be useful to evaluate the customer level of commitment or attention to the agent discourse. One of the main paralinguistic cue of interest in such speech data is the emotional state of the speaker. In particular, frustration and satisfaction hold key factors of the customer relationship, and more precisely their M. Macary, M. Tahon and D. Luzzati are with the LIUM, Le Mans Université, France.\n\nY. Estève is with LIA, Avignon Université, France. Manon Macary is also employed with Allomedia, Paris, France.\n\nevolution according time during the conversation. In this paper, we focus on the automatic continuous extraction of such factors in the whole speech conversation. Emotional states have been extensively studied and many theories exist  [2] ,  [3] . Among these, the continuous theory, also called dimensional theory, has been introduced by Wundt et al.  [4]  and Scholsberg  [5] , and consider that all affective states arise from independent fundamental neurophysiological systems. According to this authors, these systems can be defined by three independent dimensions characterized by their extremum values: pleasant-unpleasant, tension-relaxation, and excitation-calm. These three dimensions were soon-to-be found overlapping. Russell  [2]  introduced the circumplex model in which emotion categories are arranged on a circle controlled by two dimensions: valence (positive-negative) and arousal (weak-strong). Consequently, each emotion category can be understood as a linear combination of these two dimensions, or as varying degrees of both valence and arousal. While most emotional theories consider affective states from the point of view of psychology and psychiatry, machine learning systems usually takes one input among speech, vision, or physiological signals. More precisely, a Speech Emotion Recognition (SER) system consider that emotion in speech is conveyed by both linguistic and acoustic modalities. For example, Alva et al.  [6]  proved that arousal is better recognized from acoustic features and valence from linguistic features. Considering these facts, we investigated the fusion of both acoustic and linguistic modalities in our work as many studies have proven its utility in comprehension related domain  [7] ,  [8] ,  [9] ,  [10] . In a previous work  [11] , we defined a new axis within the circumplex model that goes from satisfaction to frustration through a neutral state in the middle. This axis has been proposed for the specific analysis of customer relationships in the context of call-center conversations.\n\nSER systems are subject to different forms of variability which make commercial applications difficult to set up. The first variability lies in the references used to train the models: Emotion perception is highly subjective, and several manual annotations are required to reach a kind of \"ground truth\". This variability is usually measured with annotator agreements (kappa values or correlation coefficients). Second, the reliability of the performances increases with the number of audio samples used to evaluate the models. In SER, this number is relatively small due to the high data collection and annotation cost, therefore, confidence intervals for the performances of the systems are highly required. Finally, the third variability comes from the initialization of the parameters of the models, and possible shuffle of the data during the training stage. In this paper, we intend to bring some insights to these three forms of variability with the investigation of individual annotations, the systematic addition of confidence intervals to the regression performances, and the evaluation of initialization impact on the performances.\n\nThe different experiments detailed in this article conclude that the linguistic modality is the major contributor for satisfaction recognition in call-center conversations. A deep analysis on the linguistic content of some conversations is carried on to investigate intrepretable factors able to explain the high contribution of the linguistic modality for this specific task.\n\nThe main contributions of our study are the followings:\n\n• The use of pre-trained models for satisfaction recognition • The fusion of acoustic and linguistic modalities • The addition of protocols to evaluate performance variabilities (annotation, initialization, confidence intervals). • The proposition of interpretable linguistic cues which explain the performances of our model The paper is organized as follows: Section II presents the related works, followed by our motivations in Section III and the global overview of our experimental protocols in Section IV. Satisfaction recognition using either acoustic or linguistic modalities experiments and results detailed in Section V, while the fusion experiments and results are described in Section VI. Section VII presents a complete analysis regarding annotation and linguistic content. The conclusion is drawn in the last Section.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Features For Speech Emotion Recognition",
      "text": "Looking for speech cues that gives the best emotion recognition model has always been a \"holy grail\"  [12] ,  [13] ,  [14] . However most studies agree that emotion mainly lies in prosody which is a combination of different factors such as intensity, intonation, rhythm and voice quality. These high level factors are usually estimated from low level descriptors: pitch, spectral features, MFCCs, energy, etc... Therefore, to analyze emotion in speech, researchers usually rely on various voice parameters set that are related to emotion  [15] ,  [16] ,  [17]  including fundamental frequency, speech rate, pauses, voice intensity, voice onset time, jitter (pitch perturbations), shimmer (loudness perturbations), voice breaks, pitch jumps, and measures of voice quality. Para-linguistic sets used in Speech Emotion Recognition (SER) such as ComParE  [18] , and GeMAPS  [13]  used in Interspeech Emotion Challenges  [19] , are designed to capture prosody. Other features like spectral ones can also be extracted: among them, mel frequency cepstral coefficients (MFCCs) are clearly the most often used as they are robust to noisy signals, even if they have not been designed to retrieve prosodic information nor emotion as concluded in Tahon et al.  [14] .\n\nFor a while, SER has been dominated by the acoustic modality. However, emotions are not only conveyed by prosody but also by words. While automatic speech recognition systems (ASR) are more and more efficient, linguistic features can be extracted with high reliability. In the field of Sentiment Analysis (SA) where the goal is to find emotion in written text, different features were proposed such as POS-tagged (Part-Of-Speech-tagged) words  [20] ,  [21] , polarity dictionaries (Sen-ticNet  [22] , FAN  [23] ) or features extracted with the GloVe representation  [24]  n-grams/bag-of-words  [25] . It should be noticed however, that spoken language differs from written text in the grammatical correctness, disfluences and non-verbal vocalizations such as laughter, breathing, and so on  [26] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Modality Fusion",
      "text": "Due to the small amounts of training data, SER has late moved to the neural paradigm. First studies have used RNN (Recurrent Neural Networks), especially with LSTMs (Long Short Term memory) to retrieve emotional categories  [27]  or continuous dimensions  [28] ,  [29] . CNNs (Convolutional networks) have also been used to predict SEWA continuous dimensions  [30]  however LSTMs seems to better generalize on call-center data  [31] .\n\nIn order to take advantage of the linguistic content in SER, the fusion of both textual and audio information gains on popularity  [32] ,  [33] ,  [34] . Three strategies are usually applied for multi-modal fusion: (a) at the feature level by concatenating the inputs of different modalities, (b) at the decision level with majority voting, or (c) at the model level by merging intermediate representations  [7] ,  [10] ,  [35] ,  [36] . More precisely, the fused model (c) is done by concatenated outputs of two distinct networks corresponding to each modality to feed next layers  [37] . Many other modalities can be used in SER to better represent affective states. For example, audio representation, facial cues from video, textual information are used in the work of Chen et al.  [38]  and Poria et al.  [39]  while Wu et al.  [40]  focuses on semantics labels and audio features. Modality fusion always improve the performances obtained on speech only.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Pre-Trained Features For Nlp",
      "text": "Expert features have the advantage to convey human understandable information but there are not the only way to represent data. From other research domains such as SA, we assist to the rising of pre-trained self-supervised feature to represent the data, especially with word embeddings such as GloVe  [24]  or Word2Vec  [41] . As there are trained on a massive amount of data, they tend to be able to efficiently represent data, without the need of human annotation. Very recently, these pre-trained features spread in SER. Atmaja et al.  [42]  uses acoustic features consisting mostly in time and spectral domain features, and Word2Vec embedding for the linguistic part by performing a feature level fusion. While Yenigalla  [43]  et al. uses spectrogram and phoneme embeddings merged at the model level.\n\nThe self-supervised learning of speech or language representations has been proposed these last few years, for instance with the BERT system  [44] , used for textual representation. Such representations, computed by neural models trained on huge amounts of unlabeled data, have shown their effectiveness on some tasks under certain conditions, for instance in ASR  [45] ,  [46] , or speech translation  [47] . Recently Wav2Vec  [48] , Mockingjay  [46]  and Audio AlBERT  [49]  were introduced in ASR and speaker identification as one of the first pre-trained approaches to extract context dependent features from raw signals for ASR tasks but they have not been used for SER yet. Very recently a BERT-like model for French has been developed  [50] . To the best of the authors' knowledge, such pre-trained features have not been yet used for SER.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Motivation",
      "text": "The goal of our research is to continuously recognize satisfaction and frustration in real-life call-center conversations. To do so, we are using AlloSat  [11]  French corpus to train speech emotion recognition network.\n\nMoreover, several studies point out that emotion information can be detected not only in speech but also in facial traits, biological responses or linguistic and semantic information. Traditionally, emotion recognition models use only the acoustic modality  [51] , even if some works have shown that linguistic modality also convey important information  [52] . In our work, we investigate the use of the acoustic signal and its linguistic transcription, separately or jointly. To compensate the lack of training data dedicated to the targeted task, we also explore the benefit of using models pre-trained on huge amount of data for both modalities such as Wav2Vec  [48] , Word2Vec  [53]  or BERT  [54] .\n\nTo design application for real industrial end users, one of the main concern is to be able to reproduce the results on multiple GPU clusters, thus to reduce all possible variabilities during the evaluation process. In the scope of neural networks paradigm, weights initialization has always been pointed out as crucial as it impacts both the training time and the phenomena of being stuck in a local minima  [55] .\n\nTherefore we will estimate how much the weight initialization affects the performances of the satisfaction recognition. Because the Test set is, of course, not representative of all possible realizations, we decided to include an confidence interval to our scores. This aims at given an idea of how much the performances could vary when evaluating on different conversations, considering that all non-deterministic sources are fixed for that matter. In the field of continuous emotion recognition, the reference generally consists of the averaged value over all annotators. In our study, we tackle the problem of the subjectivity of the annotation by considering individual annotation instead of the averaged reference.\n\nOur major conclusion is that models learnt on features extracted from the transcripts only are very accurate in the prediction of satisfaction and frustration. Therefore, this work analyses the linguistic content, and proposes relevant linguistic clues which are strongly related with the perception of the emotional state.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Global Overview",
      "text": "This section presents the speech material used to train and evaluate the models and the general architecture of the neural network used for SER.\n\nA. Speech emotional data : AlloSat corpus While past emotional speech corpora were annotated with discrete emotion categories  [56] ,  [57] , the current trend is to move towards continuous annotations of affective dimensions. Among the most popular corpora annotated continuously, we can cite SEMAINE  [58]  composed of English interactions with virtual or human operators, or RECOLA  [59]  targeting French dyadic online conversations. Both corpora are annotated at least according to arousal and valence dimensions. The recent cross-cultural Emotion Database SEWA  [60]  was presented for the 2018 Audio/Visual Emotion Challenge  [61]  which aimed to retrieve arousal, valence and liking dimensions from semisupervised dyadic conversations.\n\nIn order to fit with our target task, we choose to carry on our experiments on AlloSat corpus  [11]  composed of reallife call-center conversations, annotated along the satisfaction axis. AlloSat was precisely built to continuously predict the evolution of the customer satisfaction on call-centers audio recordings of French speaking adult callers (i.e. customers). Various information are asked by the callers: contract information, global details on the company, or complains.\n\nAll conversations were recorded at 8kHz between July 2017 and November 2018 in call-centers located in Frenchspeaking countries. The agents are employees of various companies in different domains, mainly energy, travel agency, real estate agency and insurance. The two telephone channels were recorded separately. Due to commercial constraints, we discarded the part of the receiver (i.e agent). Consequently, there is no overlap in the conversations.\n\nAlloSat contains 303 conversations for a total duration of 37h 23' as summarized in Table  I . There is generally one single speaker per conversation even if some conversations can involve multiple speakers, for instance when the caller gives the telephone to someone else. In order to preserve the speakers' privacy, all personal information were obfuscated with a jazzy sound letting the annotator knows that there was private information at this very moment. This anonymization process ensures to respect the General Data Protection Regulation (GDPR) recommendation. Because we removed the agent speech, there can be long moments of silence in the remaining caller speech. To minimize the annotator effort, we decided to replace these silences by 2 seconds of white noise, allowing the annotators to identify these moments of silence. In order to avoid collecting too many conversations with poor emotional content, we decided to apply three selection criterion based on prosodic and linguistic content.\n\n1) Speech duration: conversations longer than 30 seconds containing more than three speech turns; 2) Intonation: standard deviation of the fundamental frequency (F 0 ) over 40 Hz. F 0 is extracted with YAPPT algorithm  [62]  which is adapted to telephone signals; 3) Linguistic valence: the valence score computed on the transcriptions is below 4.98 (negative) or above 5.02 (positive). Word scores are given by FAN French dictionary  [23]  and unknown words are at 5.00. Emotion annotation is known to be a highly subjective task. To compensate for the subjectivity of the annotation task, three annotators rated continuously the 303 conversations along the satisfaction axis. This axis range from frustration to satisfaction with a neutral state in the middle and is sampled every 0.25 seconds. Individual annotations were averaged to get a gold reference, used in the prediction task. For more details about the coherence of the annotations, please refer to our previous work  [11] . An automatic transcription were provided by Allo-Media for each conversation.\n\nThe corpus has been divided into three subsets: The train set contains 201 conversations corresponding to about 25h of audio signal and 16h of speech; The development set is composed of 42 conversations; and the test set contains 60 conversations. Both Development and Test sets are composed of about 6h of audio signal and 3h of speech.\n\nB. SER neural network model 1) Baseline architecture: We designed a regressive baseline neurol network to continuously predict the satisfaction along the conversation. To do so, a recurrent network, inspired from  [30] , is used for the prediction task using bidirectionnal Long Short-Term Memory units (biLSTM).\n\nThe sizes of the different layers have been optimized in our previous work, and the final architecture is composed of 4 biLSTM layers of respectively 200, 64, 32, 32 units with a tanh activation as shown on Figure  1 . A single output neuron is also used to predict the regression value each 250 ms at the emotional segment level. Neither dropout nor batch normalisation is used in this approach.\n\nThe baseline network is fed with expert acoustic, respectively linguistic, feature sets of low dimension (40, respectively 48) described in the next section V. When moving to pre-train features, the input dimension explodes up to hundreds as they intend to represent huge amounts of speech data.\n\nA mean and variance normalization of the input features is done over the training data for all experiments.\n\n2) Loss and evaluation function: The concordance correlation coefficient (CCC)  [63]  goes from 0 (chance level) to 1 (perfect) and is calculated according to eq. 1, where x is the prediction and y the reference. µ x and µ y are the means for the two variables and σ x and σ y their corresponding variances. ρ is the correlation coefficient between the two variables x and y.\n\nIn previous experiments on the prediction of emotional dimensions  [28] ,  [30] , the loss function to be minimized during the training phase is defined according to eq. 2, where the CCC is computed over all concatenated conversations within a batch.\n\nThe CCC is also used as the evaluation metric on the Development and Test subsets. The score is computed at once on all the concatenated conversations of a given data subset, as described in AVEC challenges  [61] .\n\n3) Confidence interval for CCC score: As mentioned previously, our work also intend to assess the robustness of the models from an industrial perspective. More precisely, as the number of samples used to evaluate the models is relatively small, we need to estimate how reliable is the final CCC score with a confidence interval  [64] ,  [65] . The definition of the confidence interval for CCC is given in Appendix A. On AlloSat evaluations, the confidence interval widths for the CCC are between 0.006 (lower CCC) and 0.002 (high CCC). In the following experiments, a difference in performance will be judged as consistent if the two confidence intervals do not overlap.\n\n4) Hyper-parameters: All networks are implemented under Pytorch framework  [66] . Preliminary experiments on the development set, helped to settle the baseline network architecture (number of biLSTM layers and number of neurons per layer) and the following hyper-parameters: training is done on batches from 8 to 20 conversations using the Adam optimiser, depending on the size of the input embedding and memory constraints. All the conversations are kept without any padding. The learning rate is optimized at 0.001 by empirical method, tested on a range from 0.001 to 0.02 by a 0.005 step.\n\nAfter preliminary experiments, we noticed that networks were not improving after the first 400 epochs, so the maximum number of epochs is set to 500. For each training process, the final model is the one extracted from the epoch that gets the best score on the Development set. This final model is then evaluated on the Test set.\n\n5) Initialization: the initialization of the model can have a huge impact on both the execution time and the accuracy of the resulting system. To handle with this hypothesis, 5 random initializations are tested on our best decision fusion system. In additional experiments  1  , the final CCC score of one of the fusion approaches varies from .873 to .911 depending on the seed used for the initialization. It is a high variability which is considered to be relevant if we refer to the confidence interval, allowing us to conclude that the initialization is crucial. In such a situation, if a new model is trained with same data and same architecture, there is a significant uncertainty on the final performances. This will not be investigated in the reste of the article.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "V. Acoustic And Linguistic Features",
      "text": "This section describes features used in input of the network. While acoustic features are extracted directly from the speech signal, linguistic features are obtained from the transcription. Baseline features consists of the traditional inputs used to represent the signal, i.e. Mel Frequency Cepstral Coefficents (MFCCs), or textual information, i.e. Word2Vec. Pre-trained features are indeed embeddings which are learnt on huge amount of data for an external task, here automatic speech recognition.\n\nA. Acoustic modality 1) Baseline features: MFCCs: Two baseline acoustic sets are used as input of the network: MFCCs and the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS). While eGeMAPS intend to precisely capture and represent prosody in speech, MFCCs are known to be robust to low quality audio signals such as telephone. In previous experiments, we have shown that MFCCs better achieve to predict satisfaction than eGeMAPS features  [67] , therefore only MFCCs are considered in the remainings. In speech processing, the spectral content is considered as constant on small audio segments of around 30 ms. Our signal is sampled at 8 kHz, therefore MFCC 1-12 and their delta values are extracted on 30 ms frames each 10 ms with torchaudio toolkit  2  .\n\nMean and standard deviation of each coefficient are computed over the emotional segment in order to get a 48 dimensional vector each 250 ms.\n\n2) Pre-trained features : Wav2Vec: Self-supervised learning approaches have been designed in order to take benefit of huge amount of unlabelled data. Wav2Vec (1.0)  [48]  is a neural model trained through self-supervision to compute speech representations from raw audio. This model is composed of two distinct convolutional neural networks. A first encoder network converts the audio signal into a new representation that is given to the second network, the \"context network\", which takes care of the context by aggregating multiple time step representations into a contextualized tensor that matches to a receptive field of about 210 ms. Both are then used to minimize a contrastive loss function. The resulting embedding is a 512-dimensional feature vector. As the training of such model demands a lot of data and calculation power, we use the large pre-trained model provided by Schneider et al. in  [48] , trained on Librispeech corpus  [68]  consisting of 960 hours of English audio book samples at 16 kHz. Our features were extracted on an upsampled version of AlloSat 3 . In order to investigate the influence of the acoustic context on Wav2Vec representations, embeddings are extracted either on the current 250 ms emotional segment (without context) or on the whole conversation input (with context).\n\nIn the end, each emotional segment is represented by a 512dimensional vector which consists of the averaged values of obtained embeddings over each segment of 250 ms.\n\nB. Linguistic modality 1) Baseline features : Word2Vec: Word2Vec embeddings have been extensively used for sentiment analysis or opinion mining from text  [41] ,  [42] , this motivated us to use such representation for the prediction of satisfaction. In the following experiments, a Word2Vec model has been trained with the toolkit GENSIM  [69] , using private data owned by Allo-Media composed of manual call transcriptions received by call centers, totaling over 500 hours of speech, with CBoW algorithm  [53] . No stop list is used before extracting the embeddings. In a first step, the output size embedding is fixed to 40 in order to have similar dimension with baseline MFCC features (i.e 48). It is also motivated with empirical results showing that in the range between 20 and 60, the dimension 40 gave the best results. We also did the experiment with a more standardized output size, fixed at 100.\n\n2) Pre-trained features : CamemBERT: Inspired by RoBERTa  [70]  and BERT, CamemBERT  [54]  is a multilayer bidirectional Transformer. CamemBERT is trained on the Masked Language Modeling (MLM) task which consists of replacing some tokens by either the token <MASK> or a random token and asking the model to correct the tokens. The network uses a cross-entropy loss. The input consists of a mix of whole words and sub-words in order to take advantage of the context.\n\nWe use the \"camemBERT-base\" pre-trained model delivered by the authors and trained on the French part of OSCAR corpus  [71]  consisting of a set of monolingual corpora extracted from Common Crawl snapshot and totaling 138GB of raw text and 32.7B tokens after sub-word tokenization. Text representations were extracted on Allosat by using this pretrained model, and we summarized the results by averaging the continuous representations of sub-words occurring in the current emotional segment. In total, we use a 768-dimensional feature vector. In order to investigate the influence of the linguistic context on CamemBERT representations, embeddings are extracted either on the words pronounced during the current emotional segment (without context) or on the whole conversation input (with context).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Results",
      "text": "All results on acoustic and linguistic modalities are reported in Table  II . We confirm that pre-trained features are achieving awesome results in comparison to baseline features. Especially, the performance impressively increases on the Test set (+23.8%) when using Wav2Vec pre-trained features extracted without context (CCC=.806) instead of MFCCs features (CCC=.651). The relative improvement on Test set (+7.3%) obtained when using CamemBERT pretrained features extracted with context instead of Word2Vec is not as spectacular as the one obtained on acoustics because Word2Vec (CCC=.861) features already reach good results in comparison to MFCCs (CCC=.651). However this modality seems more robust as it improves for both Dev and Test sets.\n\nTo confirm the reliability of our results, we can notice that the performance obtained by our models trained on acoustic features computed by the English Wav2Vec1.0 model is consistent, and even better, to the one obtained on the same data and presented in a recent study  [72]  that used a Wav2Vec2.0 model to extract acoustic features to feed smaller neural models.\n\nDeeper experiments on the number of features used to train Word2Vec representations confirm that the best performance on Dev and Test set are obtained with a size of 40. Increasing the number of features to 100 degrades the score on Dev (-3.5%) and Test (-5.7%) sets. Regarding to confidence intervals detailed in appendix B, we confirm that all mentioned improvements are significant.\n\nA lot of differences exist by nature between CamemBERT and Word2Vec: complexity of the neural architecture, contextdependent dynamic embeddings vs. static embeddings, subwords vs. words, . . . The computation of CamemBERT needs a lot of GPUs, data and time. However, we do not have the means to train such a model on specialized data with call-center conversations. Fortunately with the help of the pre-trained model kindly distributed by the authors and it is possible to get very good results on the targeted SER task without owning such amount of resources.\n\nAs described in Table  II , the different acoustic and linguistic representations of the speech signal have different sizes which can impact the training of the network. We previously investigated the impact of this dimension gap on system performances  [67] , by comparing the network presented in Figure  1  with another one designed to reduce the dimension of input features. This reduction was done by adding an optional dense layer after the inputs and before the first biLSTM layer in order to reduce the input size to 40, resp. 48, for linguistic, resp. acoustic, modalities. We concluded that both architectures achieved comparable results and the addition of a dense layer was not necessary and that the input size does not significantly affect the results.\n\nTo conclude from Table  II , we confirm the relevance of using pre-trained features for satisfaction recognition. Surprisingly, we also found that linguistic embeddings, are able to capture a lot of emotional information directly from the transcribed speech as it performs slightly better than the acoustic one, especially when using CamemBERT features extracted with context. At this point, we should notice that pre-trained linguistic features are extracted from textual transcriptions, however Word2Vec and CamemBERT models are trained on speech signals. Therefore some acoustic information (mainly phonetics) is, in a sense, also included in these linguistic features. However, we do not know at this stage how prosodic and para-linguistic information is captured by pre-trained linguistic features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Modality Fusion",
      "text": "As discussed in Section II, many studies confirm that emotion is conveyed by many modalities, especially acoustic and linguistic modalities as presented in Section II. However, there is no consensus on the independence of acoustic and linguistic modalities, or there synchronicity with time. To address this problem, we experiment three types of fusion : feature, model and decision fusion. In our case, the output value is return each 250 ms. Therefore acoustic and linguistic vectors must be aligned together with respect to time.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Feature Fusion",
      "text": "Feature fusion methods enable a new representation of the speech signal which is the concatenation of individual modality features from the two modalities (Fig.  2a ). A single model is then trained with a unique vector corresponding to a joint representation of the acoustic and linguistic features. The input size is therefore the sum of the two acoustic and linguistic feature sizes. Good fusion performances at the feature level would probably mean that acoustic and linguistic modalities are synchronously used to perceive the satisfaction.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Model Fusion",
      "text": "We experiment two types of model fusion :\n\n• Early fusion: Outputs of the first layers of acoustic and linguistic modalities are concatenated to feed the second layer (Fig.  2b ). • Late fusion: Outputs of the third layers of acoustic and linguistic modalities are concatenated to feed the last biLSTM layer (Fig.  2c ).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Decision Fusion",
      "text": "To perform a decision fusion, two models are trained independently on each modality and the predicted numerical values are averaged to compute new predictions (Fig.  2d ). In this configuration, it can be relevant to computed the global prediction (CCC G ) as the weighted average (Eq. 3) of the individual acoustic CCC a and the linguistic CCC b scores, in order to give more importance to one of the two modalities.\n\nWe optimize the weights of each modality from 0.1 to 0.9 with a step of 0.01. The final configuration is the one which gives the better score on the development set. Good fusion performances at the decision levels would probably mean that synchronicity is useless for the perception of satisfaction on a 250 ms frame.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Results",
      "text": "The CCC scores obtained on Dev and Test sets with baseline (resp. pre-trained) features are summarized in Table III (resp. Table  IV ). The relative differences between Test and Dev results are given in the last column to estimate the generalization power of the model. Relative improvements are also included with the best single model as reference, i.e. Word2Vec or CamemBERT. Detailed scores with confidence interval can be found in appendix B.\n\nTable  III  shows that whatever the fusion level, fusion performs better than Word2Vec only on the Dev set and lower on the Test set. The poor performances on the Test set can be explained by the very small CCC obtained with MFCCs (CCC=.651). The best improvement on the Dev set is obtained when using the late model fusion (+3.9%), however this is the configuration that less generalizes on the Test set (-11.1%).\n\nTable  IV  shows that the addition of the acoustic modality to CamemBERT embeddings does not improve performances on Dev set with feature fusion but with model or decision fusion. We confirm the fact that acoustic modality alone does  Unexpectedly, our results concludes that the linguistic modality (without the addition of acoustic features) best generalize to unseen data. They also confirms the relevance of pre-trained features such as CamemBERT, to a lesser extent Wav2Vec, for satisfaction recognition in call-center converations. While the model late fusion does not reach the best results in Test set, it significantly outperforms single linguistic modality on Dev, confirming the multi-modal aspects of emotion. The advantage of this fusion method is that it requires less computing ressources to be trained. Therefore, acoustic information is still useful but is less robust to unseen data.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vii. Analysis And Discussion",
      "text": "This section deeper analysis our results in order to better understand the importance of the linguistic modality. We investigates two axes: Annotator subjectivity and linguistic content.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Influence Of Annotation Subjectivity",
      "text": "Our first analysis interrogates the subjectivity of the annotation task regarding acoustic and linguistic modalities. To do so, we modify the reference: Instead of training a single model on the averaged value over the three annotators, we train three different models per annotator, in which each reference is the single values for this annotator. The predictions of these models are evaluated regarding individual annotations (top part of Table  V ) or the ground truth defined as the average of the three individual annotations (bottom part of Table  V ). The AVG column gives the average performance over the three individual models. The CV column gives the coefficient of variation (standard deviation over mean) over the three individual models. Diff1 is the relative difference between linguistic and acoustic taken independently and gives an idea of the gain per annotator.\n\nIndividual annotations: From the upper part of Table  V , we can notice that the coefficient of variation (CV) for single features, is higher with acoustic features than with linguistic features when the references are individual annotations, especially on the Test set. More precisely, regarding annotator a 3 , the performance of the acoustic modality severally drops on the Test set (CCC=.597). Our hypothesis is that the variability in the acoustic space is highly diverse, and the same acoustic realization might be perceived with different satisfaction levels by the same annotator, what produces bad performances on the acoustic modality. In the previous Section VI, we have shown that the fusion of the modalities improves performances on Dev but degrades on Test. This is not true when models are train and evaluated on individual annotations: fusion improves performances in most configurations and the best performance in average is reached with the model early fusion (CCC=.854 on Test set). The improvement on Test is highest with annotator a 2 (+3.7% with model early fusion). This can be explained by the very small difference between the performances obtained on independent modalities for this annotator (+6.2%), maybe indicating that both modalities carry different information for this specific annotator.\n\nFrom these results, we hypothesize that, at the annotator level, acoustic and linguistic modalities convey complementary emotional information, however, while the linguistic part is well shared among annotators, the perception of the acoustic part seems quite individual. Of course additional experiments with cross-annotations are needed to confirm this hypothesis.\n\nAveraged annotations: Regarding individual models evaluated with averaged annotations (bottom part of Table  V ), we notice that annotator a 2 has the lowest performances when using only linguistic features. The model built upon this annotator reaches the lowest performances using any type of fusion on Dev and Test sets. Thus confirming the importance of high linguistic performances for the general evaluation. This result can be explained by the fact that among the three annotators, we have shown that a 2 had the lowest intraannotator agreement (see  [11] ). We also confirm the fact that the fusion helps to improve the performances per annotator in all cases. The early model fusion has the advantage of having higher averaged performances than CamemBERT and of being the model less affected by individual annotations (CV =0.020 on Test set).\n\nFrom these experiments, we conclude that while the fusion approaches degrade the global performances in comparison to CamemBERT only (see Table  IV ), it seems that they are more robust to the subjectivity of the annotation task. We found that the early model fusion was the best compromise between performance and robustness. Our insights also interrogates the evaluation process using the average values of the three annotators: averaged values have no perceptive reality, but individual values do.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Linguistic Analysis",
      "text": "In the context of call-center conversations, the experiments described below conclude that the satisfaction-frustration axis is more supported by linguistic than acoustic content. Regarding to the circumflex model, this axis is very close to the valence axis, what could explain in some extend the importance of word for the detection of satisfaction. In this section, we intend to provide elements that could explain the importance of linguistics to retrieve the satisfaction. This analysis have been done on 13 conversations selected in order to cover different dynamics of the satisfaction dimension: Globally flat, occurrences of high frustration (ground truth < 4) and occurrences of strongly decreasing satisfaction (frustration drops). The analysis has been done using the automatic transcription, the reference satisfaction annotation and tags corresponding to high frustration and frustration drop.\n\nOur hypothesis is that frustrated speech mainly correspond to the accentuation of the oral phenomena. Consequently, we specifically investigated the following orality clues:\n\n• Amount of disfluencies, • Hesitations, repairs, repetitions, babbling, • Importance of self-breaks defined as \"the points where the utterance flow is broken\"  [73] ,\n\n• Usage of interrogations and negations, • Semantic evidences of frustration or unhappiness, • Amount of meaningfull segments vs. semantically empty segments.\n\nBased on these clues, the analysis concludes to different observations. There are semantic evidences of frustration in the conversations such as the usage of the negation (c ¸a ne m'amuse pas, c'est inadmissible), strong markers (c'est gonflé, putain de ...) and weak markers (quand même, franchement). It seems also that the amount of meaningful segments, selfbreaks and disfluencies, are generally correlated with high frustration or satisfaction drops. The syntactic structure of interrogative utterances seems also correlated with frustration.\n\nIn a second step, we intend to go further in this analysis with the automatic extraction of orality clues. Of course, moving from manual to automated extraction implies to do some choices in the definition of the clues. Trying to model the amount of meaningful segments, we extract POS tags using MACAON  [74]  directly from automatic transcriptions and compute the number of verbs and nouns with respect to time. To capture the other orality clues, we decided to extract automatically the seven features mentioned in Table  VII .\n\nThe idea is not to provide an exhaustive analysis on the whole dataset but to provide some explainable clues. We focus here on the deep analysis of a single conversation about a certified letter. All the occurrences of features summarized in Table VII are synchronized in time together with the annotated satisfaction reference. The number of verbs and nouns does not give relevant information and is not represented here. The dynamic linguistic analysis of each conversation is shown on Fig.  3 . This conversation has been annotated with a strong drop of satisfaction before 200 sec. The automatic transcription obtained just before this drop is given in Table  VI . Just before the drop, the occurrences of single words repetition and c'est are important, whereas after the drop, the number of filled pauses and negation marker (pas) increases. We also notice that a strong marker (réclamation) happen just before the drop, probably meaning that this specific word induces the perception of noticeable frustration.\n\nIn the context of AlloSat speech data, emotional information seems to lies more in the words than in the prosodic and acoustic content. In such data, the expression of frustration is mainly related to the accentuation of the oral phenomena: semantic content and above all self-breaks, disfluencies, hesitations, repairs and repetitions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Viii. Conclusion",
      "text": "This paper present the independent use of acoustic and linguistic pre-trained features and the fusion of these two modalities for the continuous recognition of satisfaction in call-center conversations. We also present a further analysis on the influence of annotation subjectivity on the performances. We also investigate possible linguistic clues able to explain the supremacy of linguistic features for this task.\n\nConducted on the AlloSat corpus, built for the recognition of satisfaction and frustration in real-life call-center conversations, we observe that Wav2Vec acoustic and CamemBERT linguistic pre-trained features, better represent satisfaction than TABLE VI: Extract (137 -166 sec.) from a conversation about a certified letter. Disfluencies: italic; Hesitations, repairs, babbling: underline; Semantic evidences of frustration: bold; self-breaks: // French English translation voilà et la deuxième lettre // c'est pareil mais bon cette lettre // elle est où maintenant. . . pas comprendre pourquoi on n'a pas retiré la lettre... la deuxième lettre // c'est pareil mais elle venait d'où // cette lettre... c'était qui // qui a envoyé cette lettre... parce que c'est important // on est une société // nous. . . quand on sait pas qui c'est // ... comment on peut savoir qui c'est ouais mais c ¸a va pas du tout hein c ¸a va pas du tout // c ¸a -there we are and the second letter // it is the same but yes this letter // where is it now ... not understand why no one removed this letter ... the second letter // it is the same but where does it come from // this letter ... it is who // who sent this letter ... because it is important // we are a society // we ... when we don't know who it is // ... how can we know who it is yeah but it's not ok eh it's not ok // it  In our experiments, we found that linguistic representations clearly outperform acoustic representations, thus questioning the need for acoustic in such task. However, linguistic pre-trained features are extracted on automatic transcriptions directly obtained from the acoustic signals. So we definitely need acoustic and we do not know at this stage how prosodic and para-linguistic information is captured by these pre-trained features.\n\nOur results clearly affirm the advantage of using Camem-BERT representations, however the benefit of the fusion of acoustic and linguistic modalities is not as obvious. With models learnt on individual annotations, we found that fusion approaches are more robust to the differences in annotations. This article also investigates the robustness of the proposed approach towards industrial applications. We pointed out the fact that the initialization process induces a large variability in the performances of the network. Further investigations are needed to cope with this issue. We demonstrate that the use of fused models improves the robustness of the models regarding annotation subjectivity. Finally a deep linguistic analysis allows us to propose relevant linguistic clues (negation and semantic markers, repetitions, filled pauses, etc.) that somewhat explains why the linguistic content is so important for this task. We conclude that para-linguistic information is mainly included in words and their syntax.\n\nIn a future work, we intend to develop some approaches in order to cope with the initialization issue in order to provide reproducible experiments. Additional experiments with crossannotations approaches are needed in order to investigate the differences in perception due to the linguistic and acoustic modalities. This work raises the question of the place of acoustic cues, especially prosody features. To pursue our investigation, we aim at applying the presented protocol on additional speech data, for instance broadcast news, political debates, etc.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A single output",
      "page": 4
    },
    {
      "caption": "Figure 1: Baseline network architecture. Number of neurons of",
      "page": 4
    },
    {
      "caption": "Figure 1: with another one designed to reduce the dimension of",
      "page": 6
    },
    {
      "caption": "Figure 2: a). A single",
      "page": 6
    },
    {
      "caption": "Figure 2: Description of the four used fusions.",
      "page": 7
    },
    {
      "caption": "Figure 3: This conversation has been annotated with a strong",
      "page": 9
    },
    {
      "caption": "Figure 3: Dynamic analysis of frustration of a conversation about a certified letter. Number of occurrences of the seven linguistic",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reference": "Individual\nannotations",
          "Annotator\nFusion level": "SINGLE\nWav2Vec\nCamemBERT\nDiff1 (%)",
          "a1\nDev\nTest": ".834\n.734\n.898\n.877\n7.7\n19.5",
          "a2\nDev\nTest": ".731\n.785\n.833\n.834\n14.0\n6.2",
          "a3\nDev\nTest": ".841\n.597\n.900\n.804\n7.0\n34.7",
          "AVG\nDev\nTest": ".802\n.705\n.877\n.838\n-\n-",
          "CV": ".077\n.138\n.043\n.044\n-\n-"
        },
        {
          "Reference": "",
          "Annotator\nFusion level": "FEATURE",
          "a1\nDev\nTest": ".884\n.870",
          "a2\nDev\nTest": ".815\n.753",
          "a3\nDev\nTest": ".834\n.883",
          "AVG\nDev\nTest": ".861\n.819",
          "CV": ".046\n.073"
        },
        {
          "Reference": "",
          "Annotator\nFusion level": "MODEL\nEarly\nLate",
          "a1\nDev\nTest": ".883\n.870\n.911\n.875",
          "a2\nDev\nTest": ".855\n.865\n.814\n.837",
          "a3\nDev\nTest": ".888\n.826\n.921\n.799",
          "AVG\nDev\nTest": ".854\n.875\n.882\n.837",
          "CV": ".020\n.028\n.067\n.045"
        },
        {
          "Reference": "",
          "Annotator\nFusion level": "DECISION",
          "a1\nDev\nTest": ".913\n.882",
          "a2\nDev\nTest": ".840\n.849",
          "a3\nDev\nTest": ".916\n.793",
          "AVG\nDev\nTest": ".890\n.841",
          "CV": ".048\n.053"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Averagedan-\nnotations": "",
          "SINGLE\nWav2Vec\nCamemBERT\nDiff1 (%)": "FEATURE",
          ".862\n.736\n.916\n.878\n6.3\n19.3": ".896\n.845",
          ".774\n.731\n.755\n.793\n-2.5\n8.5": ".741\n.688",
          ".779\n.710\n.851\n.833\n9.2\n17.3": ".868\n.861",
          ".805\n.726\n.841\n.835\n-\n-": ".835\n.798",
          ".061\n.019\n.096\n.051": ".099\n.120"
        },
        {
          "Averagedan-\nnotations": "",
          "SINGLE\nWav2Vec\nCamemBERT\nDiff1 (%)": "MODEL\nEarly\nLate",
          ".862\n.736\n.916\n.878\n6.3\n19.3": ".911\n.833\n.899\n.914",
          ".774\n.731\n.755\n.793\n-2.5\n8.5": ".809\n.824\n.763\n.784",
          ".779\n.710\n.851\n.833\n9.2\n17.3": ".879\n.856\n.844\n.841",
          ".805\n.726\n.841\n.835\n-\n-": ".866\n.838\n.840\n.841",
          ".061\n.019\n.096\n.051": ".060\n.020\n.090\n.068"
        },
        {
          "Averagedan-\nnotations": "",
          "SINGLE\nWav2Vec\nCamemBERT\nDiff1 (%)": "DECISION",
          ".862\n.736\n.916\n.878\n6.3\n19.3": ".938\n.882",
          ".774\n.731\n.755\n.793\n-2.5\n8.5": ".795\n.778",
          ".779\n.710\n.851\n.833\n9.2\n17.3": ".874\n.868",
          ".805\n.726\n.841\n.835\n-\n-": ".867\n.845",
          ".061\n.019\n.096\n.051": ".082\n.069"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A study of strategic call center management: relationship between key performance indicators and customer satisfaction",
      "authors": [
        "K Cheong",
        "J Kim",
        "S So"
      ],
      "year": "2008",
      "venue": "European Journal of Social Sciences"
    },
    {
      "citation_id": "2",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "3",
      "title": "What are emotions? and how can they be measured?",
      "authors": [
        "K Scherer"
      ],
      "year": "2005",
      "venue": "Social science information"
    },
    {
      "citation_id": "4",
      "title": "Outlines of psychology",
      "authors": [
        "W Wundt"
      ],
      "venue": "Outlines of psychology"
    },
    {
      "citation_id": "5",
      "title": "Three dimensions of emotion",
      "authors": [
        "H Schlosberg"
      ],
      "year": "1954",
      "venue": "Psychological review"
    },
    {
      "citation_id": "6",
      "title": "A comprehensive survey on features and methods for speech emotion detection",
      "authors": [
        "Y Alva",
        "N Muthuraman",
        "J Paulose"
      ],
      "year": "2015",
      "venue": "Proc. of IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)"
    },
    {
      "citation_id": "7",
      "title": "LSTMmodeling of continuous emotions in an audiovisual affect recognition framework",
      "authors": [
        "M Wöllmer",
        "M Kaiser",
        "B Schuller",
        "G Rigoll"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "8",
      "title": "Fusion of acoustic, linguistic and psycholinguistic features for speaker personality traits recognition",
      "authors": [
        "F Alam",
        "G Riccardi"
      ],
      "year": "2014",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "9",
      "title": "Multimodal fusion for multimedia analysis: A survey",
      "authors": [
        "P Atrey",
        "M Hossain",
        "A Saddik",
        "M Kankanhalli"
      ],
      "year": "2010",
      "venue": "Multimedia Syst"
    },
    {
      "citation_id": "10",
      "title": "Efficient low-rank multimodal fusion with modalityspecific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "11",
      "title": "AlloSat: A new call center french corpus for satisfaction and frustration analysis",
      "authors": [
        "M Macary",
        "M Tahon",
        "Y Estève",
        "A Rousseau"
      ],
      "year": "2020",
      "venue": "Proc. of Language Resources and Evaluation Conference (LREC), Virtual Conference"
    },
    {
      "citation_id": "12",
      "title": "Whodunnit -searching for the most important feature types signalling emotion-related user states in speech",
      "authors": [
        "A Batliner",
        "S Steidl",
        "B Schuller",
        "D Seppi",
        "T Vogt",
        "J Wagner",
        "L Devillers",
        "L Vidrascu",
        "V Aharonson",
        "L Kessous",
        "N Amir"
      ],
      "year": "2011",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "13",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Towards a small set of robust acoustic features for emotion recognition: Challenges",
      "authors": [
        "M Tahon",
        "L Devillers"
      ],
      "year": "2016",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Automatic emotional speech classification",
      "authors": [
        "D Ververidis",
        "C Kotropoulos",
        "I Pitas"
      ],
      "year": "2004",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "16",
      "title": "Comparing feature sets for acted and spontaneous speech in view of automatic emotion recognition",
      "authors": [
        "T Vogt",
        "E Andre"
      ],
      "year": "2005",
      "venue": "International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition by speech signals",
      "authors": [
        "O Kwon",
        "K Chan",
        "J Hao",
        "T Lee"
      ],
      "year": "2003",
      "venue": "European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "18",
      "title": "The INTERSPEECH 2014 computational paralinguistics challenge: Cognitive & physical load",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "J Krajewski",
        "J Epps",
        "F Eyben",
        "F Ringeval"
      ],
      "year": "2014",
      "venue": "Proc. of INTER-SPEECH"
    },
    {
      "citation_id": "19",
      "title": "The INTERSPEECH 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "A Vinciarelli",
        "K Scherer"
      ],
      "year": "2013",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "20",
      "title": "Aspect-level sentiment analysis on ecommerce data",
      "authors": [
        "S Vanaja",
        "M Belwal"
      ],
      "year": "2018",
      "venue": "Proc. of International Conference on Inventive Research in Computing Applications (ICIRCA)"
    },
    {
      "citation_id": "21",
      "title": "Sentiment analysis using neural networks: A new approach",
      "authors": [
        "S Dhar",
        "S Pednekar",
        "K Borad",
        "A Save"
      ],
      "year": "2018",
      "venue": "Proc. of International Conference on Inventive Communication and Computational Technologies (ICICCT)"
    },
    {
      "citation_id": "22",
      "title": "Enhanced SenticNet with affective labels for concept-based opinion mining",
      "authors": [
        "S Poria",
        "A Gelbukh",
        "A Hussain",
        "D Das",
        "S Bandyopadhyay"
      ],
      "year": "2013",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "23",
      "title": "Affective norms for French words (FAN)",
      "authors": [
        "C Monnier",
        "A Syssau"
      ],
      "year": "2014",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "24",
      "title": "TCS research at SemEval-2018 task 1: Learning robust representations using multi-attention architecture",
      "authors": [
        "H Meisheri",
        "L Dey"
      ],
      "year": "2018",
      "venue": "Proc. of The International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "25",
      "title": "Using a heterogeneous dataset for emotion analysis in text",
      "authors": [
        "S Chaffar",
        "D Inkpen"
      ],
      "year": "2011",
      "venue": "Proc of Advances in Artificial Intelligence"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communication of ACM"
    },
    {
      "citation_id": "27",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "Proc. of INTER-SPEECH"
    },
    {
      "citation_id": "28",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou"
      ],
      "year": "2016",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "29",
      "title": "Deep recurrent neural networks for emotion recognition in speech",
      "authors": [
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Deep recurrent neural networks for emotion recognition in speech"
    },
    {
      "citation_id": "30",
      "title": "Continuous emotion recognition in speech -do we need recurrence?",
      "authors": [
        "M Schmitt",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proc. of INTER-SPEECH"
    },
    {
      "citation_id": "31",
      "title": "Multi-corpus experiment on continuous speech emotion recognition: convolution or recurrence?",
      "authors": [
        "M Macary",
        "M Lebourdais",
        "M Tahon",
        "Y Estève",
        "A Rousseau"
      ],
      "year": "2020",
      "venue": "Proc. of Conference on Speech and Computer (SPECOM), Virtual Conference"
    },
    {
      "citation_id": "32",
      "title": "Dimensional speech emotion recognition from speech features and word embeddings by using multitask learning",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "APSIPA Transactions on Signal and Information Processing"
    },
    {
      "citation_id": "33",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "Proc. of Spoken Language Technologies Workshop (SLT)"
    },
    {
      "citation_id": "34",
      "title": "Multi-modal learning for speech emotion recognition: An analysis and comparison of asr outputs with ground truth transcription",
      "authors": [
        "S Sahu",
        "V Mitra",
        "N Seneviratne",
        "C Espy-Wilson"
      ],
      "year": "2019",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "35",
      "title": "Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts",
      "authors": [
        "J Sebastian",
        "P Pierucci"
      ],
      "year": "2019",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "36",
      "title": "Comparison between decision-level and feature-level fusion of acoustic and linguistic features for spontaneous emotion recognition",
      "authors": [
        "S Planet",
        "I Iriondo"
      ],
      "year": "2012",
      "venue": "Proc. of the Iberian Conference on Information Systems and Technologies (CISTI)"
    },
    {
      "citation_id": "37",
      "title": "Feature-level and model-level audiovisual fusion for emotion recognition in the wild",
      "authors": [
        "J Cai",
        "Z Meng",
        "A Khan",
        "Z Li",
        "J O'reilly",
        "S Han",
        "P Liu",
        "M Chen",
        "Y Tong"
      ],
      "year": "2019",
      "venue": "Proc. of Multimedia Information Processing and Retrieval (MIPR)"
    },
    {
      "citation_id": "38",
      "title": "Multimodal multi-task learning for dimensional and continuous emotion recognition",
      "authors": [
        "S Chen",
        "Q Jin",
        "J Zhao",
        "S Wang"
      ],
      "year": "2017",
      "venue": "Proc. of the Audio/Visual Emotion Challenge and Workshop (AVEC)"
    },
    {
      "citation_id": "39",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "40",
      "title": "Emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information and semantic labels",
      "authors": [
        "C Wu",
        "W Liang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Toward qualitative evaluation of embeddings for Arabic sentiment analysis",
      "authors": [
        "A Barhoumi",
        "N Camelin",
        "C Aloulou",
        "Y Estève",
        "L Belguith"
      ],
      "year": "2020",
      "venue": "Proc. of Language Resources and Evaluation Conference (LREC), Virtual Conference"
    },
    {
      "citation_id": "42",
      "title": "Speech emotion recognition using speech feature and word embedding",
      "authors": [
        "B Atmaja",
        "K Shirai",
        "M Akagi"
      ],
      "year": "2019",
      "venue": "Proc. of Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)"
    },
    {
      "citation_id": "43",
      "title": "Speech emotion recognition using spectrogram and phoneme embedding",
      "authors": [
        "P Yenigalla",
        "A Kumar",
        "S Tripathi",
        "C Singh",
        "S Kar"
      ],
      "year": "2018",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "44",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. of the North American Chapter"
    },
    {
      "citation_id": "45",
      "title": "Librilight: A benchmark for ASR with limited or no supervision",
      "authors": [
        "J Kahn",
        "M Rivière",
        "W Zheng",
        "E Kharitonov",
        "Q Xu"
      ],
      "year": "2020",
      "venue": "Proc. of ICASSP, Virtual Conference"
    },
    {
      "citation_id": "46",
      "title": "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
      "authors": [
        "A Liu",
        "S Yang",
        "P Chi",
        "P Hsu",
        "H Lee"
      ],
      "year": "2020",
      "venue": "Proc. of ICASSP, Virtual Conference"
    },
    {
      "citation_id": "47",
      "title": "Investigating self-supervised pre-training for end-to-end speech translation",
      "authors": [
        "H Nguyen",
        "F Bougares",
        "N Tomashenko",
        "Y Estève"
      ],
      "year": "2020",
      "venue": "Proc. of the workshop on Self-supervision in Audio and Speech at the International Conference on Machine Learning (ICML), Virtual Conference"
    },
    {
      "citation_id": "48",
      "title": "Wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Proc. of INTER-SPEECH"
    },
    {
      "citation_id": "49",
      "title": "Audio AlBERT: A lite BERT for self-supervised learning of audio representation",
      "authors": [
        "P Chi",
        "P Chung",
        "T Wu",
        "C Hsieh",
        "S Li"
      ],
      "year": "2020",
      "venue": "Audio AlBERT: A lite BERT for self-supervised learning of audio representation"
    },
    {
      "citation_id": "50",
      "title": "FlauBERT: Unsupervised language model pre-training for French",
      "authors": [
        "H Le",
        "L Vial",
        "J Frej",
        "V Segonne",
        "M Coavoux"
      ],
      "year": "2020",
      "venue": "Proc. of Language Resources and Evaluation Conference (LREC), Virtual Conference"
    },
    {
      "citation_id": "51",
      "title": "",
      "authors": [
        "S Patel",
        "K Scherer"
      ],
      "year": "2013",
      "venue": ""
    },
    {
      "citation_id": "52",
      "title": "Real-life emotions detection with lexical and paralinguistic cues on human-human call center dialogs",
      "authors": [
        "L Devillers",
        "L Vidrascu"
      ],
      "year": "2006",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "53",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space"
    },
    {
      "citation_id": "54",
      "title": "CamemBERT: a tasty French language model",
      "authors": [
        "L Martin",
        "B Muller",
        "P Ortiz Suárez",
        "Y Dupont",
        "L Romary"
      ],
      "year": "2020",
      "venue": "Proc. of ACL, Virtual Conference"
    },
    {
      "citation_id": "55",
      "title": "Benchmarking neural network robustness to common corruptions and perturbations",
      "authors": [
        "D Hendrycks",
        "T Dietterich"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "56",
      "title": "Real-life emotion-related states detection in call centers: a cross-corpora study",
      "authors": [
        "L Devillers",
        "C Vaudable",
        "C Chasatgnol"
      ],
      "year": "2010",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "57",
      "title": "Natural resources, aid, and democratization: A bestcase scenario",
      "authors": [
        "K Morrison"
      ],
      "year": "2007",
      "venue": "Public Choice"
    },
    {
      "citation_id": "58",
      "title": "The SEMAINE database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "G Mckeown",
        "M Valstar",
        "R Cowie",
        "M Pantic",
        "M Schröoder"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "59",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "60",
      "title": "SEWA DB: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "61",
      "title": "AVEC 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "R Cowie",
        "H Kaya"
      ],
      "year": "2018",
      "venue": "Proc. of the Audio/Visual Emotion Challenge and Workshop (AVEC)"
    },
    {
      "citation_id": "62",
      "title": "A spectral/temporal method for robust fundamental frequency tracking",
      "authors": [
        "S Zahorian",
        "H Hu"
      ],
      "year": "2008",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "63",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "L.-K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "64",
      "title": "A note on concordance correlation coefficient",
      "authors": [
        "J Liao",
        "J Lewis"
      ],
      "year": "2000",
      "venue": "PDA Journal of Pharmaceutical Science and Technology"
    },
    {
      "citation_id": "65",
      "title": "A Proposal for Strength-of-Agreement Criteria for Lin's Concordance Correlation Coefficient",
      "authors": [
        "G Mcbride"
      ],
      "year": "2005",
      "venue": "A Proposal for Strength-of-Agreement Criteria for Lin's Concordance Correlation Coefficient"
    },
    {
      "citation_id": "66",
      "title": "PyTorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "67",
      "title": "On the use of Selfsupervised Pre-trained Acoustic and Linguistic Features for Continuous Speech Emotion Recognition",
      "authors": [
        "M Macary",
        "M Tahon",
        "Y Estève",
        "A Rousseau"
      ],
      "year": "2021",
      "venue": "IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "68",
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "69",
      "title": "Software framework for topic modelling with large corpora",
      "authors": [
        "R Řehůřek",
        "P Sojka"
      ],
      "year": "2010",
      "venue": "Proc. of the LREC Workshop on New Challenges for NLP Frameworks"
    },
    {
      "citation_id": "70",
      "title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi"
      ],
      "year": "2019",
      "venue": "RoBERTa: A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "71",
      "title": "Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures",
      "authors": [
        "P Ortiz Suárez",
        "B Sagot",
        "L Romary"
      ],
      "year": "2019",
      "venue": "Workshop on the Challenges in the Management of Large Corpora (CMLC-7)"
    },
    {
      "citation_id": "72",
      "title": "Task Agnostic and Task Specific Self-Supervised Learning from Speech with LeBenchmark",
      "authors": [
        "S Evain",
        "H Nguyen",
        "H Le",
        "M Boito",
        "S Mdhaffar",
        "S Alisamir",
        "Z Tong",
        "N Tomashenko",
        "M Dinarelli",
        "T Parcollet",
        "A Allauzen",
        "Y Estève",
        "B Lecouteux",
        "F Portet",
        "S Rossato",
        "F Ringeval",
        "D Schwab",
        "Laurent Besacier"
      ],
      "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "73",
      "title": "Fluency and Disfluency across Languages and Language Varieties, ser. Corpora and Language in use",
      "authors": [
        "B Pallaud",
        "R Bertrand",
        "P Blache",
        "L Prévot",
        "S Rauzy"
      ],
      "year": "2019",
      "venue": "P. U. de Louvain"
    },
    {
      "citation_id": "74",
      "title": "Macaon: An nlp tool suite for processing word lattices",
      "authors": [
        "A Nasr",
        "F Béchet",
        "J.-F Rey",
        "B Favre",
        "J Roux"
      ],
      "year": "2011",
      "venue": "Proc. of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Systems Demonstrations (HLT)"
    }
  ]
}