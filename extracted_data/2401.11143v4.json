{
  "paper_id": "2401.11143v4",
  "title": "Density Adaptive Attention Is All You Need: Robust Parameter-Efficient Fine-Tuning Across Multiple Modalities",
  "published": "2024-01-20T06:42:32Z",
  "authors": [
    "Georgios Ioannides",
    "Aman Chadha",
    "Aaron Elkins"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We propose the Multi-Head Density Adaptive Attention Mechanism (DAAM), a novel probabilistic attention framework that can be used for Parameter-Efficient Fine-tuning (PEFT), and the Density Adaptive Transformer (DAT), designed to enhance information aggregation across multiple modalities, including Speech, Text, and Vision. DAAM integrates learnable mean and variance into its attention mechanism, implemented in a multi-head framework, enabling it to collectively model any probability distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance, up to approximately +20% (abs.) in accuracy. Empirically, DAAM exhibits superior adaptability and efficacy across a diverse range of tasks, including emotion recognition in speech, image classification, and text classification, thereby establishing its robustness and versatility in handling data across multiple modalities. Furthermore, we introduce the Importance Factor, a new learning-based metric that enhances the explainability of models trained with DAAM-based methods † .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "Advantages of Learning with Multi-Head DAAM DAAM leverages additive and multiplicative Gaussian parameters -mean offsets and variance scaling factors, respectively -to dynamically adjust attention. The mean offset shifts the Gaussian's focus based on input context, enhancing responsiveness to deviations. In parallel, the variance scaling adapts the distribution's spread, ensuring attention is not only accurately centered but also suitably scaled to the task's specificity. The multi-head design allows each head to address different data aspects, enhancing the model's adaptability to non-Gaussian traits  [10] .\n\nBelow, we analyze the theoretical aspects of entropy for both traditional self-attention mechanisms and DAAM.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Density Adaptive Attention With N Gaussians Per Head, H",
      "text": "Each head, h, in Density Adaptive Attention Mechanism (DAAM) processes input using Gaussian normalization, which is controlled by learnable parameters µ i,h and σ i,h . The transformation is defined by the formula y norm = y-(mean+mean_offset) √ var+ϵ , where ϵ is a small constant ensuring numerical stability. This normalized input is then applied to a Gaussian function, f (h) (x) = exp -y 2 norm 2c 2 , with c as a learnable parameter that controls the spread of the Gaussian function. The overall transformation for each head approximates a Gaussian distribution, where the variance σ 2 prod is a function of the aggregated variances and mean adjustments within the head, represented by σ 2 prod = N i=1\n\nAdditionally, the effective mean is given by\n\nThe entropy for each head, denoted as H(X h ), is calculated using the formula 1  2 log(2πeσ 2 prod ). This entropy value reflects how the data is spread, influenced by parameters such as c, the mean offset, and the computed variance from the downsampled data. To capture the overall system entropy, including potential interactions among multiple heads, it is represented by the formula H(X) = H h=1 H(X h ) + ∆. Here, ∆ accounts for additional entropy arising from the diversity and interactions across different heads, highlighting the ensemble effect of the multi-head Gaussian transformations. This approach allows DAAM to modulate attention distribution adaptively, balancing between broad and focused attention based on input characteristics.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Traditional Self-Attention",
      "text": "Traditional self-attention mechanisms are mathematically represented as Attention\n\nIn this framework, the softmax function is applied to the scaled dot products of queries (Q) and keys (K), producing attention weights. Consider a vector z = {z 1 , z 2 , . . . , z n }, derived from these scaled dot products. Let S = n j=1 e zj denote the sum of exponential terms. The softmax values are represented as e z 1\n\nS , e z 2 S , . . . , e zn",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "S",
      "text": ", with entropy\n\nS log e z i S . Typically, this entropy is low unless the z values are nearly identical, leading to a uniform softmax output. This low entropy results from the exponential nature of the softmax function, which tends to emphasize larger dot product values, thereby focusing attention on specific parts of the input.\n\nIn practical terms, the output of traditional self-attention often skews towards dominant features, concentrating attention and leading to lower entropy. Higher entropy, indicating a more uniform attention distribution, can be beneficial for tasks requiring a comprehensive view of all input data. Achieving higher entropy theoretically demands near-uniformity in the elements of z (the inputs to the softmax function). However, without modifications to the architecture-such as designing or constraining the weight matrices W Q and W K to produce similar outputs across different inputs-traditional self-attention mechanisms inherently produce lower entropy. This characteristic makes them less adaptable in scenarios demanding sensitivity to diverse and dynamic data elements, highlighting a limitation in their design for certain applications.\n\nHigher entropy in an attention mechanism signifies a more balanced distribution of attention across various parts of the input data. This balance is crucial for ensuring that the model does not overly focus on a few features but instead considers a broader array of information. Such capability is vital in complex tasks where the input data is highly non-stationary. In contrast to traditional self-attention, the DAAM architecture dynamically adjusts its entropy in response to input characteristics. It provides both broad (high entropy) and focused (low entropy) attention distributions as needed, which is essential for effectively handling both highly non-stationary and stationary data environments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Attention Mechanisms & Parameter Efficient Fine-Tuning",
      "text": "The Multi-Head Attention (MHA) mechanism in Transformer architectures uses parallel attention heads to enhance sequence modeling. Each head computes attention scores independently using the scaled dot-product attention formula:\n\nwhere d k is the dimensionality of the keys. This enables each attention head to focus on different parts of the input, capturing diverse relationships within the data. The overall MHA is expressed as:\n\nwith each head calculated as:\n\nThis architecture allows the model to capture complex patterns and dependencies, improving performance in various tasks, such as machine translation and text summarization.\n\nThe Grouped Query Attention (GQA) mechanism serves as an intermediary between MHA and Multi-Query Attention (MQA). In GQA, query heads are grouped into G groups, with each group sharing a single key and value. This can be formulated as:\n\nGQA provides a balance between computational efficiency and the model's capacity to learn complex relationships, making it a valuable tool for scaling attention mechanisms in large models.\n\nLow-Rank Matrix Adaptation (LoRA) is a technique designed for efficient model adaptation, commonly used in the Natural Language Processing domain. LoRA leverages low-rank decomposition to fine-tune pre-trained models by introducing low-rank updates to the weight matrices in neural networks. This is parameterized as ∆W = AB T , where A ∈ R d×r and B ∈ R d×r with r ≪ d. This approach significantly reduces the number of parameters while maintaining the model's expressive power. The overall weight update in the model is represented as\n\nwhere W is the original weight matrix.\n\nBy adjusting the rank r, LoRA balances between model complexity and computational efficiency, proving particularly effective for scenarios requiring rapid adaptation to new tasks or domains. This method has demonstrated competitive performance with significantly reduced resource requirements, as noted in recent research  [11, 12, 13] .\n\nAdditionally, LoRA+ extends the original LoRA technique by introducing separate learning rates for the matrices A and B, thereby allowing more fine-grained control over the model adaptation process. This modification enhances the flexibility of LoRA by decoupling the optimization dynamics of the two matrices. Specifically, instead of a shared learning rate for both A and B, LoRA+ applies different learning rates, η A and η B , to each matrix during the training process, enabling improved convergence behavior and potentially better adaptation to the target task. This innovation has been explored in more recent literature  [14] , demonstrating that separate learning rates can further optimize performance while maintaining LoRA's benefits of parameter efficiency and adaptability.\n\nDespite these efficiencies, LoRA still requires a relatively higher number of parameters-at least 24 times more than DAAM across the models used and at least 2 times for than DAT in this study, even when assuming the lowest rank possible, r = 4. This parameter difference highlights the efficiency of DAAM and DAT in comparison, as illustrated in Table  1 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "General Purpose Pre-Trained Models",
      "text": "WavLM  [5]  is a large-scale pre-trained model designed to enhance speech processing capabilities by utilizing 94,000 hours of diverse audio inputs. Building upon the Hidden Unit BERT (HuBERT) framework  [6] , which primarily focuses on masked speech prediction, WavLM incorporates an additional denoising modeling component. This dual approach allows WavLM to handle a broader range of speech-related tasks effectively. At its core, WavLM utilizes self-supervised learning techniques, enabling the model to predict masked portions of audio inputs. Through this process, the model acquires a deeper understanding of speech patterns, nuances, and contextual information, thereby improving its performance in various speech processing applications.\n\nLlama family of models  [7]  mark a significant advancement in Large Language Models (LLMs), leveraging an optimized auto-regressive transformer architecture.\n\nBidirectional Encoder Representations from Image Transformers (BEiT)  [8]  represents a breakthrough in self-supervised learning for vision tasks, drawing inspiration from the BERT  [15]  approach in natural language processing. BEiT utilizes Masked Image Modeling (MIM) as a pre-training strategy for vision transformers. In this approach, images are tokenized into discrete visual tokens, and a blockwise masking strategy is applied. The model then predicts the original visual tokens from these masked patches, focusing on learning high-level semantic representations directly from raw pixel data. This methodology allows BEiT to excel in downstream tasks such as image classification and semantic segmentation, surpassing other pre-training methods in both performance and fine-tuning stability.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "The frozen encoder of each of the three PTM implementations (as described in Section 1) is used to train and evaluate a decoder on the IEMOCAP  [16] , AG News  [17]  and CIFAR100  [18]  datasets to assess the applicability of the newly proposed attention mechanisms across speech, text and image modalities. For the IEMOCAP dataset, we employ 5-fold cross-validation, training on 4 sessions and validating on 1. We focus on the emotion categories neutral, happiness (merging happiness and excited), anger, and sadness. AG News dataset is employed in our study. Our dataset construction focuses solely on the title and description fields of these articles. In terms of data distribution, each category (out of four) contributed 30,000 articles to the training set and 1,900 articles to the validation set. For our analysis, we use the following division of the CIFAR-100 dataset: 50,000 images for training and 10,000 for validation. In the Multi-head DAAM, as outlined in Algorithm 1, the attention weights are determined utilizing multiple Gaussian probability density functions. In this formulation, the scaled variances are treated as learnable parameters, while the means are adjusted through learnable offsets. This methodology enables the model to dynamically adapt its attention focus in response to the distribution of the input data.\n\nIn a multi-head configuration, the DAAM process is independently applied across different heads, each focusing on distinct subspaces of the original input features. The final output (of all heads combined via stacking) is computed as an element-wise multiplication (Hadamard product) of the original input features and the Density attention weights.\n\nThis process enhances the model's ability to focus on contextually relevant information in the input sequence. All head outputs are stacked vertically, forming the Density Attention modulated Tensor (X ′ ). An even more parameter-efficient solution of the DAAM -termed as the Mixture of Densities Adaptive Attention Mechanism, can be found in the Appendix (see Section B). Following the integration of Multi-Head DAAM, we investigate its compatibility with dot-product-based attention mechanisms (e.g., MHA, MQA, and GQA). Our focus on GQA is driven by its comparable performance to MHA, superior computational efficiency  [19]  and advantages of its hierarchical learning structure  [20] . We refer to this method as the Grouped Query Density Adaptive Attention Mechanism (GQDAAM). The objective here is to showcase that DAAM can benefit PTMs across multiple modalities as a parameter-efficient fine-tuning method. The Computational complexity of DAAM can be analyzed as follows: O(n • m), where n is the batch size and m is the dimension size along normAxis. O(h • n • m), with h as numHeads, allowing for parallelization.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Encoder And Decoder Models",
      "text": "We apply different attention mechanisms on SSL-based PTMs acting as PTMs to extract embeddings from. Specifically, we utilize the pre-trained model weights from three distinct PTMs: (i) WavLM-Large, (ii) Llama2-13B, and (iii) BEiT-Large.\n\nLibri-light  [21]  GigaSpeech  [22]  and English parts of VoxPopuli  [23]  have been used for pre-training (i). ImageNet-1k  [24]  has been used for pre-training (iii). Conversely, while (ii) has undergone pre-training on undisclosed, publicly sourced textual data, this aspect does not impact our research. to be on training and subsequently evaluating the performance of the newly proposed decoder on the designated downstream task. This approach ensures that the intrinsic properties and learned representations of the PTMs are preserved, while the decoder adapts and fine-tunes to the specific requirements of the task at hand  [25] . The output from each transformer layer (in the encoder) undergoes mean pooling across the time dimension (sequence length), followed by concatenation of these pooled outputs. These concatenated outputs then serve as input embeddings for the Attention Module, which employs either (i) Multi-Head Self-Attention, (ii) Multi-Head Density Adaptive Attention, or (iii) Multi-Head Grouped Query Density Adaptive Attention where (ii) and (iii) are contributions of this work. When (ii) or (iii) are used, the decoder network is termed as DAT.\n\nIn mathematical terms, the embeddings are represented as X ∈ R N ×d , where each x i is a vector in a d-dimensional space, with d taking values in the set {1024, 5120}. Here, N signifies the total count of transformer layers in the encoder, which are kept in a static (frozen) state. The attention mechanism of the module then produces a new, contextualized representation C ∈ R N ×d for the input sequence. Subsequently, convolutional layers are utilized to distill features (pertaining to speech, text, or image data) from the context matrix generated by the attention mechanism. By employing 2-dimensional convolution layers (with kernel_size = (3, 3), stride = 1, and padding = 1), the model adeptly processes the array of context tensor outputs from each transformer layer.\n\nTable  1  lists the attention mechanism parameters for the proposed DAAM-based decoders of WavLM-Large, Llama2-13B, and BEiT-Large encoders. Here, g denotes DAAM-based head count, with higher values indicating a higher number of learned Gaussian Distributions. q and kv are the counts of query and key-value heads, respectively. The embedding dimensions are 1024 for WavLM and BEiT, and 5120 for Llama2. We also benchmark against LoRA-based methods downstream task by fine-tuning the query and key projections modules.\n\nAll decoder network models are trained for 35 epochs and their layer weights (except their respective attention module) are initialized using Xavier initialization  [26] . Adam  [27]  is used as the optimizer, with both weight decay factor of 0.1 and an initial learning rate of 10 -4 (except for when Llama 2 is used as an encoder, in which case it is 5 × 10 -5 ). For the SER experiments, Focal Loss  [28]  is used, where γ = 2.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "In this study, the primary evaluation metric is Accuracy (Acc.), calculated as the percentage of correct predictions to total predictions. Additionally, the Importance Factor (IF) is introduced, calculated using Density Attention weights (DA) from the attention module. IF is DAij -min(DA) max(DA)-min(DA)\n\nwith IF ∈ [0, 1], indicating the relative importance of features in the model's decision process. Higher IF values indicate more significant features and vice versa. IF-based heatmaps are created by taking the arithmetic average of the generated Density Attention maps during validation and then applying the IF formula. They visually depict feature importance. Unlike traditional self-attention, where attention might skew towards a few dominant features, DAAM's Gaussian-based attention provides a more balanced spread. This helps in capturing a broader range of features, reducing the bias towards overly frequent features and focusing more on features that contribute meaningfully to the task. All experiments have been carried out on two A100 80GB NVIDIA Graphical Processing Units (GPUs).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Discussion",
      "text": "The motivation for this research arises from the broad range of downstream applications that could benefit from an improved attention mechanism, addressing the limitations inherent in the self-attention mechanism (or other dot-product attention mechanism variations) found in Transformer models, which rely on normalized dot-products. This presents an opportunity to explore more robust and explainable approaches.\n\nDespite their widespread adoption, self-attention mechanisms face several limitations that can impact their performance. One significant challenge is the fixed-length context window, which can lead to sub-optimal outcomes, particularly for long sequences where distant elements may lack relevance  [29] . Additionally, without inductive biases like locality  [30] , self-attention layers may require more data to learn patterns that other methods can capture more efficiently. Although self-attention is theoretically capable of capturing long-term dependencies, it can struggle in practice as sequence length increases  [31] . Furthermore, the interpretability of self-attention mechanisms is limited; it primarily relies on correlation-based activations, which focus on pairwise similarities and may not effectively capture the most relevant context  [32] . This makes it difficult to understand why certain parts of the input are prioritized, underscoring the need for more transparent and interpretable attention mechanisms/frameworks.\n\nIn our work, we introduce a significant enhancement to the Transformer model's attention mechanism: the (Multi-Head) Density Adaptive Attention Mechanism (DAAM). DAAM is designed to improve upon the standard self-attention mechanism in Transformers. Unlike conventional attention in the Transformer, which calculates weights based on dot-product between different weight matrices, DAAM employs a Gaussian-based modulation of input features instead. This approach enables the model to concentrate on the most pertinent features in a context-sensitive manner, thereby improving its capability to interpret and process sequential and spatial data. DAAM's attention mechanism, can be applied in various domains like multimedia recommendation (as in  [33] ), image classification (aligning with Patrick et al.'s  [34]  robustness strategies), and text classification (enhancing accuracy in contexts like e-commerce as shown by Yıldırım et al.  [35] ), and can significantly enhance model performance. Its ability to dynamically recalibrate feature significance based on Gaussian parameters proves particularly beneficial, offering improved accuracy, robustness, and user experience across diverse and challenging real-world applications. Furthermore, DAAM's Gaussian-based modulation offers a more interpretable framework for Artificial Intelligence (AI), addressing the critical need for transparency and trustworthiness in real-world AI systems  [36] .\n\nOur proposed DAAM mechanism learns multiple means and variances of input features in a Multi-Headed setting. This mechanism operates independently across different heads, each focusing on distinct, non-overlapping subspaces of the input features. By employing Gaussian modulation, DAAM assigns varying levels of importance to each feature, effectively generating local attention outputs from each head. These outputs are then combined to construct a comprehensive Global Attention map. Each head independently adjusts its means and variances, allowing for a focused approach to different skewness aspects in data subsets capturing a broader range of data characteristics, including asymmetries, and collectively, non-Gaussian traits. Unlike other approaches in the literature wherein no parameters of the Gaussian distribution are learned, and are thus hard-coded making them non-specific to the data they are used on  [37, 38] , only multiplicative parameters like the scaled variance are learned  [39, 40] , a pre-defined amplitude that is updated during training  [41]  which are all approaches that are limited because their attention framework can only explicitly model Gaussian traits behavior  [42, 43] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Current Benchmarks & Analysis Of Encoder Layer Contribution",
      "text": "In Table  2 , we compare DAAM-based methods with those utilizing Multi-Head Self-Attention with and without Batch Normalization (BN); LoRA-based methods (rank of 4 and 8). BN assumes independent and identical distribution across mini-batches  [44]  and is applied immediately after MHA to maintain the normalization order consistent with the original Transformer architecture. DAAM, due to its multi-head structure, effectively handles variations in feature distribution, including shifts in mean offset and variance, as well as the ability to model a mixture of density distributions with varying attention weights. This capability is demonstrated in Table  2  and Figure  2 . Such adaptability enables DAAM to outperform methods dependent on static feature distributions, such as BN. Unlike methods that assume data follows a single Gaussian distribution, DAAM can model multiple Gaussian distributions with varying parameters, allowing it to approximate any probability distribution. The results indicate that DAAM demonstrates superior performance in scenarios characterized by significant variations in Gaussian parameters, which suggests a need for modeling non-stationary data and potentially non-Gaussian characteristics. Conversely, in cases where such parameter variations are minimal, DAAM outperforms MHA.\n\nObserving the data in Table  2 , we note that speech data exhibits high variability in both central tendency (mean offset, µ) and spread (scaled variance, σ 2 ). This variability reflects the highly nonstationary nature of speech  [45] , necessitating attention mechanisms that can dynamically adjust both focus (µ) and width (σ) to capture the rapidly changing features essential for tasks such as emotion recognition. Text data, on the other hand, shows high mean variation, possibly due to changing semantic contexts, while the variance remains moderately stable, which aligns with the structured nature of language. In text processing, attention mechanisms need to focus primarily on tracking the shifting mean to match the changing semantic and syntactic focal points. In contrast, vision tasks show low variations in both mean and variance, indicating that features are relatively stable and consistent in their locations and spreads. This stability suggests that simpler attention mechanisms can be effective, maintaining a consistent focus and width suitable for tasks with minimal feature variability.\n\nTo understand the contribution of encoder layers, we employ heatmap visualizations of the Importance Factor (IF) to reveal how features within the frozen pre-trained models drive decision-making. We analyze the IF heatmap of the best-performing multi-head attention mechanism across each data modality, as indicated in Tables 2. For instance, in the context of Speech Processing with WavLM-Large using DAAM, Figure  2a  shows a dense population of higher IF values at the lower layers, indicating these layers' significant role in modulating the input sequence. This suggests that foundational speech features are captured early on, with upper layers refining these features into more abstract representations. Conversely, Text Processing with Llama2-13B using GQDAAM, illustrated in Figure  2b , displays a more uniform distribution of IF across all layers, with a slight concentration in earlier layers. This pattern reflects a balanced approach to hierarchical feature extraction, where both lower and higher-level features are crucial, particularly those derived from the early to middle layers.\n\nSimilarly, Digital Image Processing with BEiT-Large using GQDAAM in Figure  2c  emphasizes the importance of lower layer features, which is consistent with the need for early-stage feature extraction in visual tasks, such as identifying edges and textures. These variations in IF distribution highlight the unique information processing needs of each modality. While speech and image processing rely heavily on primary feature extraction, text processing requires a combination of fundamental and more complex feature identification. The insights gained from IF analysis not only enhance the explainability of the models but also provide a quantifiable measure of feature significance, facilitating more informed decisions in model refinement and adaptation.        attention weights primarily indicate the level of correlation between different parts of the input sequence  [46] . Each element's weight reflects its relevance to every other element within the same sequence. However, this approach does not directly translate to the performance on downstream tasks. For instance, the authors in  [25]  derive normalized self-attention weights for SER on IEMOCAP using WavLM-Large, identifying layer 23 as pivotal. While useful for their use case, this only indicates inter-layer correlation, and not a direct link to better or worse performance and would be misleading to use these weights for that use case (as shown in Table  2  using DAAM for the same task). In contrast, DAAM-based learning dynamically adjusts attention weights tailoring attention to improve feature representation aligned with the model's end goal. Analysis of Figure  3  indicates earlier layers, exhibit more meaningful features, and contribute more to model performance, suggesting potential overparameterization in later layers  [47] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Limitations & Future Work",
      "text": "DAAM's fixed number of Gaussians can limit its adaptability across different datasets and tasks. This can be improved by adopting a Bayesian approach to dynamically select the optimal number of Gaussians. Utilizing criteria like the Bayesian Information Criterion (BIC) or a neural network to predict the number based on input characteristics can enhance performance and efficiency, allowing the model to better adapt to varying data distributions and complexities. Future work should explore DAAM in additional tasks, datasets, grounding experiments  [48] , and beyond feature extraction, including model compression using attention weights during training (crucial for resource-limited applications)  [49] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we introduce Multi-Head DAAM and the Density Adaptive Transformer. We demonstrate their effectiveness in enhancing model performance, particularly with highly non-stationary data such as Speech and Vision. Results show that combining learnable mean and variance for every Gaussian Distribution enables dynamic feature significance recalibration and approximation of any Probability Distribution across multiple modalities. Combining this mechanism with the dot-product attention mechanism enhances performance with a minimal increase in parameters (0.016%-0.08% compared to GQA models) and at least 44% less total parameters than LoRA. Finally, we introduce the Importance Factor for improved model explainability.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A Reproducibility",
      "text": "The source code has been provided in the following GitHub repository: https://github.com/ gioannides/DAAM-PEFT-paper-code",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "B Multi-Head Mixture Of Densities Adaptive Attention Mechanism Extension",
      "text": "This section presents an extension of the Multi-Head Density Adaptive Attention Mechanism (DAAM), focusing on enhancing the stability of the training process and the model's efficiency by significantly reducing the number of learnable parameters even further. The proposed method integrates multi-head attention mechanisms with Gaussian mixtures and skip connections to provide a more refined and adaptable approach to handling complex datasets.\n\nThe extended DAAM incorporates multiple attention heads, each with its Gaussian mixture model, to process different segments of the input tensor in parallel. This approach allows for a more diverse and comprehensive understanding of the data, leading to increased model robustness and efficiency.\n\nAdditionally, as illustrated in Algorithm 4 then add the original input features (X) to the augmented one (X ′ ) for enhanced stability during training (i.e. X ′ ← X ′ + X).",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "B.1 Algorithmic Details",
      "text": "This algorithm which forms the core of the extended DAAM, implementing the Gaussian mixture model within each attention head can be found in Algorithm 2. Key elements include initialization of Gaussian parameters and mean offsets, and a forward pass handling. x ← layer(x) + x 3: end for 4: return x",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Extended Results",
      "text": "We repeat all experiments previously carried out but with the Mixture of DAAM instead (see Table  2 ). It is evident that Mixture of DAAM not only outperforms DAAM but it also reduces its overall",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed model architecture showcasing a pre-trained model (i.e., the encoder) for feature",
      "page": 5
    },
    {
      "caption": "Figure 1: , is crucial during",
      "page": 5
    },
    {
      "caption": "Figure 2: a shows a dense population of higher IF values at the lower",
      "page": 8
    },
    {
      "caption": "Figure 2: b, displays a more uniform distribution of IF across all layers, with a slight concentration",
      "page": 8
    },
    {
      "caption": "Figure 2: c emphasizes the",
      "page": 9
    },
    {
      "caption": "Figure 2: IF values for different processing tasks with their respective models (with output feature",
      "page": 9
    },
    {
      "caption": "Figure 3: Percentage contribution of each layer to attention weights in different downstream tasks",
      "page": 11
    },
    {
      "caption": "Figure 3: indicates earlier layers,",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison of min and max learnable parameters (in millions) for various PEFT methods.",
      "page": 6
    },
    {
      "caption": "Table 1: lists the attention mechanism parameters for the proposed DAAM-based decoders of WavLM-",
      "page": 6
    },
    {
      "caption": "Table 2: , we compare DAAM-based methods with those utilizing Multi-Head Self-Attention",
      "page": 8
    },
    {
      "caption": "Table 2: and Figure 2. Such",
      "page": 8
    },
    {
      "caption": "Table 2: , we note that speech data exhibits high variability in both central",
      "page": 8
    },
    {
      "caption": "Table 2: Comparison of results from IEMOCAP, CIFAR100, modality summary, and AG News",
      "page": 10
    },
    {
      "caption": "Table 2: ). For the Text and Vision downstream",
      "page": 10
    },
    {
      "caption": "Table 2: ), layers with higher IF achieve better performance, especially in Vision. In MHA,",
      "page": 10
    },
    {
      "caption": "Table 2: using DAAM for the same task). In",
      "page": 11
    },
    {
      "caption": "Table 2: ). It is evident that Mixture of DAAM not only outperforms DAAM but it also reduces its overall",
      "page": 15
    },
    {
      "caption": "Table 2: inside the parentheses where only the parameters",
      "page": 16
    },
    {
      "caption": "Table 3: Number of learnable parameters for Mixture of Densities Aadaptive Attention Mechanism.",
      "page": 16
    },
    {
      "caption": "Table 4: IEMOCAP 5-fold validation (F1-F5) using WavLM-Large.",
      "page": 16
    },
    {
      "caption": "Table 5: CIFAR100 5 Run validation (R1-R5) using BEiT-Large.",
      "page": 16
    },
    {
      "caption": "Table 6: AG News 3 Run validation (R1-R3) using Llama2-13B.",
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "2",
      "title": "Rrwkv: Capturing long-range dependencies in rwkv",
      "authors": [
        "L Wang"
      ],
      "year": "2023",
      "venue": "Rrwkv: Capturing long-range dependencies in rwkv",
      "arxiv": "arXiv:2306.05176"
    },
    {
      "citation_id": "3",
      "title": "Long-range sequence modeling with predictable sparse attention",
      "authors": [
        "Y Zhuang",
        "J Zhang",
        "M Tu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "4",
      "title": "A unified view of long-sequence models towards million-scale dependencies",
      "authors": [
        "H He"
      ],
      "year": "2023",
      "venue": "A unified view of long-sequence models towards million-scale dependencies",
      "arxiv": "arXiv:2307.03172"
    },
    {
      "citation_id": "5",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units"
    },
    {
      "citation_id": "7",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "H Touvron"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models"
    },
    {
      "citation_id": "8",
      "title": "Beit: Bert pre-training of image transformers",
      "authors": [
        "H Bao",
        "L Dong",
        "S Piao",
        "F Wei"
      ],
      "year": "2022",
      "venue": "Beit: Bert pre-training of image transformers"
    },
    {
      "citation_id": "9",
      "title": "Fearless: Feature refinement loss for ensembling selfsupervised learning features in robust end-to-end speech recognition",
      "authors": [
        "S Chen",
        "J Xie",
        "J Hansen"
      ],
      "year": "2022",
      "venue": "Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "10",
      "title": "Full wcdm analysis of kids-1000 weak lensing maps using deep learning",
      "authors": [
        "J Fluri",
        "T Kacprzak",
        "A Lucchi",
        "A Schneider",
        "A Réfrégier",
        "T Hofmann"
      ],
      "year": "2022",
      "venue": "Physical Review D"
    },
    {
      "citation_id": "11",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "L Wang",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "arxiv": "arXiv:2106.09685"
    },
    {
      "citation_id": "12",
      "title": "Parameter-efficient fine-tuning by low-rank adaptation",
      "authors": [
        "M Ding",
        "W Zheng",
        "X Liu",
        "W Hong",
        "X Tian",
        "J Tang"
      ],
      "year": "2022",
      "venue": "Parameter-efficient fine-tuning by low-rank adaptation",
      "arxiv": "arXiv:2203.08275"
    },
    {
      "citation_id": "13",
      "title": "Towards parameter-efficient transfer learning for natural language processing",
      "authors": [
        "X Li",
        "M Liang",
        "Y Shen",
        "L Wang"
      ],
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "14",
      "title": "Lora+: Efficient low rank adaptation of large models",
      "authors": [
        "S Hayou",
        "N Ghosh",
        "B Yu"
      ],
      "year": "2024",
      "venue": "Lora+: Efficient low rank adaptation of large models"
    },
    {
      "citation_id": "15",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "16",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database"
    },
    {
      "citation_id": "17",
      "title": "Character-level convolutional networks for text classification",
      "authors": [
        "X Zhang",
        "J Zhao",
        "Y Lecun"
      ],
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "Learning multiple layers of features from tiny images",
      "authors": [
        "A Krizhevsky"
      ],
      "year": "2009",
      "venue": "Learning multiple layers of features from tiny images"
    },
    {
      "citation_id": "19",
      "title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints",
      "authors": [
        "J Ainslie"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Towards Understanding Hierarchical Learning: Benefits of Neural Representations",
      "authors": [
        "M Chen",
        "Y Bai"
      ],
      "year": "2020",
      "venue": "Towards Understanding Hierarchical Learning: Benefits of Neural Representations"
    },
    {
      "citation_id": "21",
      "title": "Libri-light: A benchmark for asr with limited or no supervision",
      "authors": [
        "J Kahn"
      ],
      "year": "2020",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio",
      "authors": [
        "G Chen"
      ],
      "year": "2021",
      "venue": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio"
    },
    {
      "citation_id": "23",
      "title": "VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation",
      "authors": [
        "C Wang",
        "M Riviere",
        "A Lee",
        "A Wu",
        "C Talnikar",
        "D Haziza",
        "M Williamson",
        "J Pino",
        "E Dupoux"
      ],
      "year": "2021",
      "venue": "International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "24",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "O Russakovsky"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Vision",
      "doi": "10.1007/s11263-015-0816-y"
    },
    {
      "citation_id": "25",
      "title": "Towards Paralinguistic-Only Speech Representations for End-to-End Speech Emotion Recognition",
      "authors": [
        "G Ioannides",
        "M Owen",
        "A Fletcher",
        "V Rozgic",
        "C Wang"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "26",
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "authors": [
        "X Glorot",
        "Y Bengio"
      ],
      "year": "2010",
      "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, ser. Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "27",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2017",
      "venue": "Adam: A method for stochastic optimization"
    },
    {
      "citation_id": "28",
      "title": "Focal loss for dense object detection",
      "authors": [
        "T.-Y Lin",
        "P Goyal",
        "R Girshick",
        "K He",
        "P Dollár"
      ],
      "year": "2018",
      "venue": "Focal loss for dense object detection"
    },
    {
      "citation_id": "29",
      "title": "Unlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering",
      "authors": [
        "Y Li"
      ],
      "year": "2023",
      "venue": "Unlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering",
      "doi": "10.48550/arXiv.2304.12102",
      "arxiv": "arXiv:2304.12102"
    },
    {
      "citation_id": "30",
      "title": "Inductive biases and variable creation in self-attention mechanisms",
      "authors": [
        "B Edelman",
        "S Goel",
        "S Kakade",
        "C Zhang"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "31",
      "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models",
      "authors": [
        "M Hahn"
      ],
      "year": "2020",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": "10.1162/tacl_a_00306"
    },
    {
      "citation_id": "32",
      "title": "Evaluating self-attention interpretability through human-grounded experimental protocol",
      "authors": [
        "M Bhan",
        "N Achache",
        "V Legrand",
        "A Blangero",
        "N Chesneau"
      ],
      "year": "2023",
      "venue": "Explainable Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Self-supervised learning for multimedia recommendation",
      "authors": [
        "Z Tao",
        "X Liu",
        "Y Xia",
        "X Wang",
        "L Yang",
        "X Huang",
        "T.-S Chua"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2022.3187556"
    },
    {
      "citation_id": "34",
      "title": "Reconstructive training for real-world robustness in image classification",
      "authors": [
        "D Patrick",
        "M Geyer",
        "R Tran",
        "A Fernandez"
      ],
      "year": "2022",
      "venue": "IEEE Winter Conference on Applications of Computer Vision Workshops (WACVW)",
      "doi": "10.1109/WACVW54805.2022.00031"
    },
    {
      "citation_id": "35",
      "title": "A real-world text classification application for an e-commerce platform",
      "authors": [
        "F Yıldırım",
        "A Kaya",
        "S Öztürk",
        "D Kılınç"
      ],
      "year": "2019",
      "venue": "International Symposium on Advanced Electrical and Communication Technologies (ISAECT)",
      "doi": "10.1109/ASYU48272.2019.8946337"
    },
    {
      "citation_id": "36",
      "title": "Rethinking ai explainability and plausibility",
      "authors": [
        "W Jin",
        "X Li",
        "G Hamarneh"
      ],
      "year": "2023",
      "venue": "Rethinking ai explainability and plausibility",
      "arxiv": "arXiv:2303.17707"
    },
    {
      "citation_id": "37",
      "title": "Hard-coded gaussian attention for neural machine translation",
      "authors": [
        "W You",
        "S Sun",
        "M Iyyer"
      ],
      "year": "2020",
      "venue": "Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "38",
      "title": "Gaussian transformer: A lightweight approach for natural language inference",
      "authors": [
        "M Guo",
        "Y Zhang",
        "T Liu"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Gaussian context transformer",
      "authors": [
        "D Ruan",
        "D Wang",
        "Y Zheng",
        "N Zheng",
        "M Zheng"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "40",
      "title": "T-gsa: Transformer with gaussian-weighted self-attention for speech enhancement",
      "authors": [
        "J Kim",
        "M El-Khamy",
        "J Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "41",
      "title": "Gaflow: Incorporating gaussian attention into optical flow",
      "authors": [
        "A Luo",
        "F Yang",
        "X Li",
        "L Nie",
        "C Lin",
        "H Fan",
        "S Liu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "42",
      "title": "An interpretable channelwise attention mechanism based on asymmetric and skewed gaussian distribution",
      "authors": [
        "C Chen",
        "B Li"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "43",
      "title": "Gpca: A probabilistic framework for gaussian process embedded channel attention",
      "authors": [
        "J Xie",
        "Z Ma",
        "D Chang",
        "G Zhang",
        "J Guo"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "44",
      "title": "Batch normalization damages federated learning on noniid data: Analysis and remedy",
      "authors": [
        "Y Wang",
        "Q Shi",
        "T.-H Chang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "45",
      "title": "Implicit wiener filtering for speech enhancement in non-stationary noise",
      "authors": [
        "R Jaiswal",
        "D Romero"
      ],
      "venue": "2021 11th International Conference on Information Science and Technology (ICIST), 2021"
    },
    {
      "citation_id": "46",
      "title": "Give me your attention: Dot-product attention considered harmful for adversarial patch robustness",
      "authors": [
        "G Lovisotto",
        "N Finnie",
        "M Muñoz",
        "C Mummadi",
        "J Metzen"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "47",
      "title": "Platon: Pruning large transformer models with upper confidence bound of weight importance",
      "authors": [
        "Q Zhang",
        "S Zuo",
        "C Liang",
        "A Bukharin",
        "P He",
        "W Chen",
        "T Zhao"
      ],
      "year": "2022",
      "venue": "Platon: Pruning large transformer models with upper confidence bound of weight importance"
    },
    {
      "citation_id": "48",
      "title": "Transformer-based visual grounding with cross-modality interaction",
      "authors": [
        "K Li",
        "J Li",
        "D Guo",
        "X Yang",
        "M Wang"
      ],
      "year": "2023",
      "venue": "ACM Trans. Multimedia Comput. Commun. Appl",
      "doi": "10.1145/3587251"
    },
    {
      "citation_id": "49",
      "title": "Magnitude attention-based dynamic pruning",
      "authors": [
        "J Back",
        "N Ahn",
        "J Kim"
      ],
      "year": "2023",
      "venue": "Magnitude attention-based dynamic pruning"
    }
  ]
}